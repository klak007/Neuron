{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn import datasets\n",
    "from IPython.display import display\n",
    "le = LabelEncoder()\n",
    "seed = np.random.seed\n",
    "hotone = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.35721204\n",
      "Iteration 2, loss = 1.63603705\n",
      "Iteration 3, loss = 1.93882652\n",
      "Iteration 4, loss = 1.26094722\n",
      "Iteration 5, loss = 0.70787225\n",
      "Iteration 6, loss = 0.88739333\n",
      "Iteration 7, loss = 0.76889081\n",
      "Iteration 8, loss = 0.55110124\n",
      "Iteration 9, loss = 0.60850024\n",
      "Iteration 10, loss = 0.50377267\n",
      "Iteration 11, loss = 0.54514560\n",
      "Iteration 12, loss = 0.45028719\n",
      "Iteration 13, loss = 0.41710675\n",
      "Iteration 14, loss = 0.42833162\n",
      "Iteration 15, loss = 0.39651726\n",
      "Iteration 16, loss = 0.36351265\n",
      "Iteration 17, loss = 0.33624602\n",
      "Iteration 18, loss = 0.30414803\n",
      "Iteration 19, loss = 0.29021965\n",
      "Iteration 20, loss = 0.28394947\n",
      "Iteration 21, loss = 0.26640358\n",
      "Iteration 22, loss = 0.25626798\n",
      "Iteration 23, loss = 0.24288436\n",
      "Iteration 24, loss = 0.22333325\n",
      "Iteration 25, loss = 0.21934740\n",
      "Iteration 26, loss = 0.19578285\n",
      "Iteration 27, loss = 0.19289828\n",
      "Iteration 28, loss = 0.17518830\n",
      "Iteration 29, loss = 0.17052570\n",
      "Iteration 30, loss = 0.16073473\n",
      "Iteration 31, loss = 0.15252600\n",
      "Iteration 32, loss = 0.14918515\n",
      "Iteration 33, loss = 0.13924380\n",
      "Iteration 34, loss = 0.13815875\n",
      "Iteration 35, loss = 0.13010022\n",
      "Iteration 36, loss = 0.12820384\n",
      "Iteration 37, loss = 0.12322578\n",
      "Iteration 38, loss = 0.11973222\n",
      "Iteration 39, loss = 0.11765225\n",
      "Iteration 40, loss = 0.11315392\n",
      "Iteration 41, loss = 0.11264788\n",
      "Iteration 42, loss = 0.10850581\n",
      "Iteration 43, loss = 0.10822259\n",
      "Iteration 44, loss = 0.10518424\n",
      "Iteration 45, loss = 0.10450215\n",
      "Iteration 46, loss = 0.10257329\n",
      "Iteration 47, loss = 0.10152370\n",
      "Iteration 48, loss = 0.10047888\n",
      "Iteration 49, loss = 0.09908380\n",
      "Iteration 50, loss = 0.09858135\n",
      "Iteration 51, loss = 0.09714209\n",
      "Iteration 52, loss = 0.09684855\n",
      "Iteration 53, loss = 0.09545874\n",
      "Iteration 54, loss = 0.09527730\n",
      "Iteration 55, loss = 0.09401489\n",
      "Iteration 56, loss = 0.09382281\n",
      "Iteration 57, loss = 0.09272938\n",
      "Iteration 58, loss = 0.09254772\n",
      "Iteration 59, loss = 0.09159223\n",
      "Iteration 60, loss = 0.09139389\n",
      "Iteration 61, loss = 0.09058171\n",
      "Iteration 62, loss = 0.09039194\n",
      "Iteration 63, loss = 0.08967984\n",
      "Iteration 64, loss = 0.08948359\n",
      "Iteration 65, loss = 0.08886757\n",
      "Iteration 66, loss = 0.08867306\n",
      "Iteration 67, loss = 0.08812773\n",
      "Iteration 68, loss = 0.08791834\n",
      "Iteration 69, loss = 0.08744364\n",
      "Iteration 70, loss = 0.08722007\n",
      "Iteration 71, loss = 0.08681076\n",
      "Iteration 72, loss = 0.08656105\n",
      "Iteration 73, loss = 0.08621700\n",
      "Iteration 74, loss = 0.08594342\n",
      "Iteration 75, loss = 0.08566247\n",
      "Iteration 76, loss = 0.08536871\n",
      "Iteration 77, loss = 0.08513793\n",
      "Iteration 78, loss = 0.08483630\n",
      "Iteration 79, loss = 0.08463785\n",
      "Iteration 80, loss = 0.08434866\n",
      "Iteration 81, loss = 0.08415995\n",
      "Iteration 82, loss = 0.08390068\n",
      "Iteration 83, loss = 0.08370167\n",
      "Iteration 84, loss = 0.08348267\n",
      "Iteration 85, loss = 0.08326819\n",
      "Iteration 86, loss = 0.08308319\n",
      "Iteration 87, loss = 0.08286352\n",
      "Iteration 88, loss = 0.08269347\n",
      "Iteration 89, loss = 0.08248688\n",
      "Iteration 90, loss = 0.08231226\n",
      "Iteration 91, loss = 0.08213091\n",
      "Iteration 92, loss = 0.08194665\n",
      "Iteration 93, loss = 0.08178523\n",
      "Iteration 94, loss = 0.08160286\n",
      "Iteration 95, loss = 0.08144544\n",
      "Iteration 96, loss = 0.08127847\n",
      "Iteration 97, loss = 0.08111531\n",
      "Iteration 98, loss = 0.08096443\n",
      "Iteration 99, loss = 0.08080184\n",
      "Iteration 100, loss = 0.08065487\n",
      "Iteration 101, loss = 0.08050402\n",
      "Iteration 102, loss = 0.08035394\n",
      "Iteration 103, loss = 0.08021393\n",
      "Iteration 104, loss = 0.08006746\n",
      "Iteration 105, loss = 0.07992888\n",
      "Iteration 106, loss = 0.07979194\n",
      "Iteration 107, loss = 0.07965242\n",
      "Iteration 108, loss = 0.07952068\n",
      "Iteration 109, loss = 0.07938699\n",
      "Iteration 110, loss = 0.07925460\n",
      "Iteration 111, loss = 0.07912749\n",
      "Iteration 112, loss = 0.07899817\n",
      "Iteration 113, loss = 0.07887184\n",
      "Iteration 114, loss = 0.07874869\n",
      "Iteration 115, loss = 0.07862415\n",
      "Iteration 116, loss = 0.07850282\n",
      "Iteration 117, loss = 0.07838355\n",
      "Iteration 118, loss = 0.07826365\n",
      "Iteration 119, loss = 0.07814660\n",
      "Iteration 120, loss = 0.07803124\n",
      "Iteration 121, loss = 0.07791576\n",
      "Iteration 122, loss = 0.07780257\n",
      "Iteration 123, loss = 0.07769098\n",
      "Iteration 124, loss = 0.07757954\n",
      "Iteration 125, loss = 0.07746989\n",
      "Iteration 126, loss = 0.07736190\n",
      "Iteration 127, loss = 0.07725425\n",
      "Iteration 128, loss = 0.07714788\n",
      "Iteration 129, loss = 0.07704321\n",
      "Iteration 130, loss = 0.07693916\n",
      "Iteration 131, loss = 0.07683599\n",
      "Iteration 132, loss = 0.07673435\n",
      "Iteration 133, loss = 0.07663367\n",
      "Iteration 134, loss = 0.07653371\n",
      "Iteration 135, loss = 0.07643497\n",
      "Iteration 136, loss = 0.07633744\n",
      "Iteration 137, loss = 0.07624067\n",
      "Iteration 138, loss = 0.07614470\n",
      "Iteration 139, loss = 0.07604986\n",
      "Iteration 140, loss = 0.07595600\n",
      "Iteration 141, loss = 0.07586285\n",
      "Iteration 142, loss = 0.07577056\n",
      "Iteration 143, loss = 0.07567930\n",
      "Iteration 144, loss = 0.07558891\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36148097\n",
      "Iteration 2, loss = 1.64520827\n",
      "Iteration 3, loss = 1.95495568\n",
      "Iteration 4, loss = 1.32473910\n",
      "Iteration 5, loss = 0.70812378\n",
      "Iteration 6, loss = 0.93758302\n",
      "Iteration 7, loss = 0.77334959\n",
      "Iteration 8, loss = 0.55812748\n",
      "Iteration 9, loss = 0.59926122\n",
      "Iteration 10, loss = 0.49833634\n",
      "Iteration 11, loss = 0.54375739\n",
      "Iteration 12, loss = 0.44792328\n",
      "Iteration 13, loss = 0.40432591\n",
      "Iteration 14, loss = 0.41576066\n",
      "Iteration 15, loss = 0.39230111\n",
      "Iteration 16, loss = 0.35765759\n",
      "Iteration 17, loss = 0.32419809\n",
      "Iteration 18, loss = 0.29261385\n",
      "Iteration 19, loss = 0.27567179\n",
      "Iteration 20, loss = 0.26815267\n",
      "Iteration 21, loss = 0.25609971\n",
      "Iteration 22, loss = 0.24498696\n",
      "Iteration 23, loss = 0.23020261\n",
      "Iteration 24, loss = 0.21161994\n",
      "Iteration 25, loss = 0.20353204\n",
      "Iteration 26, loss = 0.18400852\n",
      "Iteration 27, loss = 0.17838641\n",
      "Iteration 28, loss = 0.16136661\n",
      "Iteration 29, loss = 0.15870733\n",
      "Iteration 30, loss = 0.14506915\n",
      "Iteration 31, loss = 0.14352940\n",
      "Iteration 32, loss = 0.13218521\n",
      "Iteration 33, loss = 0.13068117\n",
      "Iteration 34, loss = 0.12189161\n",
      "Iteration 35, loss = 0.12048101\n",
      "Iteration 36, loss = 0.11401238\n",
      "Iteration 37, loss = 0.11207184\n",
      "Iteration 38, loss = 0.10767458\n",
      "Iteration 39, loss = 0.10583667\n",
      "Iteration 40, loss = 0.10190422\n",
      "Iteration 41, loss = 0.10069379\n",
      "Iteration 42, loss = 0.09770880\n",
      "Iteration 43, loss = 0.09682165\n",
      "Iteration 44, loss = 0.09416945\n",
      "Iteration 45, loss = 0.09385923\n",
      "Iteration 46, loss = 0.09140505\n",
      "Iteration 47, loss = 0.09140818\n",
      "Iteration 48, loss = 0.08950815\n",
      "Iteration 49, loss = 0.08891861\n",
      "Iteration 50, loss = 0.08837523\n",
      "Iteration 51, loss = 0.08679545\n",
      "Iteration 52, loss = 0.08679365\n",
      "Iteration 53, loss = 0.08606718\n",
      "Iteration 54, loss = 0.08487322\n",
      "Iteration 55, loss = 0.08473698\n",
      "Iteration 56, loss = 0.08453309\n",
      "Iteration 57, loss = 0.08366868\n",
      "Iteration 58, loss = 0.08285572\n",
      "Iteration 59, loss = 0.08268054\n",
      "Iteration 60, loss = 0.08282334\n",
      "Iteration 61, loss = 0.08272090\n",
      "Iteration 62, loss = 0.08245120\n",
      "Iteration 63, loss = 0.08168284\n",
      "Iteration 64, loss = 0.08097641\n",
      "Iteration 65, loss = 0.08042664\n",
      "Iteration 66, loss = 0.08010920\n",
      "Iteration 67, loss = 0.07998214\n",
      "Iteration 68, loss = 0.08013598\n",
      "Iteration 69, loss = 0.08121261\n",
      "Iteration 70, loss = 0.08351335\n",
      "Iteration 71, loss = 0.08780634\n",
      "Iteration 72, loss = 0.08215477\n",
      "Iteration 73, loss = 0.07855064\n",
      "Iteration 74, loss = 0.08108866\n",
      "Iteration 75, loss = 0.08159062\n",
      "Iteration 76, loss = 0.07904603\n",
      "Iteration 77, loss = 0.07818868\n",
      "Iteration 78, loss = 0.08021292\n",
      "Iteration 79, loss = 0.07998548\n",
      "Iteration 80, loss = 0.07746552\n",
      "Iteration 81, loss = 0.07889095\n",
      "Iteration 82, loss = 0.08010908\n",
      "Iteration 83, loss = 0.07730489\n",
      "Iteration 84, loss = 0.07772637\n",
      "Iteration 85, loss = 0.07934949\n",
      "Iteration 86, loss = 0.07716264\n",
      "Iteration 87, loss = 0.07675488\n",
      "Iteration 88, loss = 0.07809074\n",
      "Iteration 89, loss = 0.07694547\n",
      "Iteration 90, loss = 0.07605593\n",
      "Iteration 91, loss = 0.07672407\n",
      "Iteration 92, loss = 0.07661317\n",
      "Iteration 93, loss = 0.07580561\n",
      "Iteration 94, loss = 0.07564869\n",
      "Iteration 95, loss = 0.07603164\n",
      "Iteration 96, loss = 0.07589873\n",
      "Iteration 97, loss = 0.07521492\n",
      "Iteration 98, loss = 0.07517000\n",
      "Iteration 99, loss = 0.07551071\n",
      "Iteration 100, loss = 0.07526035\n",
      "Iteration 101, loss = 0.07477580\n",
      "Iteration 102, loss = 0.07455375\n",
      "Iteration 103, loss = 0.07469218\n",
      "Iteration 104, loss = 0.07482454\n",
      "Iteration 105, loss = 0.07456566\n",
      "Iteration 106, loss = 0.07420280\n",
      "Iteration 107, loss = 0.07395677\n",
      "Iteration 108, loss = 0.07390494\n",
      "Iteration 109, loss = 0.07396044\n",
      "Iteration 110, loss = 0.07398565\n",
      "Iteration 111, loss = 0.07396224\n",
      "Iteration 112, loss = 0.07380225\n",
      "Iteration 113, loss = 0.07363824\n",
      "Iteration 114, loss = 0.07343786\n",
      "Iteration 115, loss = 0.07328185\n",
      "Iteration 116, loss = 0.07314302\n",
      "Iteration 117, loss = 0.07304971\n",
      "Iteration 118, loss = 0.07298777\n",
      "Iteration 119, loss = 0.07301494\n",
      "Iteration 120, loss = 0.07316171\n",
      "Iteration 121, loss = 0.07370828\n",
      "Iteration 122, loss = 0.07471330\n",
      "Iteration 123, loss = 0.07763663\n",
      "Iteration 124, loss = 0.07960031\n",
      "Iteration 125, loss = 0.08373473\n",
      "Iteration 126, loss = 0.07693610\n",
      "Iteration 127, loss = 0.07251432\n",
      "Iteration 128, loss = 0.07280613\n",
      "Iteration 129, loss = 0.07580790\n",
      "Iteration 130, loss = 0.07750949\n",
      "Iteration 131, loss = 0.07309916\n",
      "Iteration 132, loss = 0.07193160\n",
      "Iteration 133, loss = 0.07439290\n",
      "Iteration 134, loss = 0.07433965\n",
      "Iteration 135, loss = 0.07250259\n",
      "Iteration 136, loss = 0.07152190\n",
      "Iteration 137, loss = 0.07284763\n",
      "Iteration 138, loss = 0.07374380\n",
      "Iteration 139, loss = 0.07197213\n",
      "Iteration 140, loss = 0.07129048\n",
      "Iteration 141, loss = 0.07227914\n",
      "Iteration 142, loss = 0.07244157\n",
      "Iteration 143, loss = 0.07161308\n",
      "Iteration 144, loss = 0.07099635\n",
      "Iteration 145, loss = 0.07146998\n",
      "Iteration 146, loss = 0.07197443\n",
      "Iteration 147, loss = 0.07137424\n",
      "Iteration 148, loss = 0.07078616\n",
      "Iteration 149, loss = 0.07087719\n",
      "Iteration 150, loss = 0.07123177\n",
      "Iteration 151, loss = 0.07124433\n",
      "Iteration 152, loss = 0.07075640\n",
      "Iteration 153, loss = 0.07046377\n",
      "Iteration 154, loss = 0.07058467\n",
      "Iteration 155, loss = 0.07077919\n",
      "Iteration 156, loss = 0.07078226\n",
      "Iteration 157, loss = 0.07047150\n",
      "Iteration 158, loss = 0.07020879\n",
      "Iteration 159, loss = 0.07015796\n",
      "Iteration 160, loss = 0.07026199\n",
      "Iteration 161, loss = 0.07036351\n",
      "Iteration 162, loss = 0.07030715\n",
      "Iteration 163, loss = 0.07017697\n",
      "Iteration 164, loss = 0.06998252\n",
      "Iteration 165, loss = 0.06983721\n",
      "Iteration 166, loss = 0.06976501\n",
      "Iteration 167, loss = 0.06975735\n",
      "Iteration 168, loss = 0.06978933\n",
      "Iteration 169, loss = 0.06982928\n",
      "Iteration 170, loss = 0.06990503\n",
      "Iteration 171, loss = 0.06996696\n",
      "Iteration 172, loss = 0.07015201\n",
      "Iteration 173, loss = 0.07034331\n",
      "Iteration 174, loss = 0.07086694\n",
      "Iteration 175, loss = 0.07140698\n",
      "Iteration 176, loss = 0.07284579\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35303276\n",
      "Iteration 2, loss = 1.62579625\n",
      "Iteration 3, loss = 1.92004172\n",
      "Iteration 4, loss = 1.24625045\n",
      "Iteration 5, loss = 0.69858285\n",
      "Iteration 6, loss = 0.88047414\n",
      "Iteration 7, loss = 0.75578185\n",
      "Iteration 8, loss = 0.54001266\n",
      "Iteration 9, loss = 0.59446964\n",
      "Iteration 10, loss = 0.49030851\n",
      "Iteration 11, loss = 0.52949937\n",
      "Iteration 12, loss = 0.43364632\n",
      "Iteration 13, loss = 0.39843760\n",
      "Iteration 14, loss = 0.40669817\n",
      "Iteration 15, loss = 0.37244418\n",
      "Iteration 16, loss = 0.33878240\n",
      "Iteration 17, loss = 0.30972273\n",
      "Iteration 18, loss = 0.27553319\n",
      "Iteration 19, loss = 0.25995758\n",
      "Iteration 20, loss = 0.25024366\n",
      "Iteration 21, loss = 0.23115848\n",
      "Iteration 22, loss = 0.22162107\n",
      "Iteration 23, loss = 0.20771854\n",
      "Iteration 24, loss = 0.18972032\n",
      "Iteration 25, loss = 0.17952391\n",
      "Iteration 26, loss = 0.16575608\n",
      "Iteration 27, loss = 0.15765176\n",
      "Iteration 28, loss = 0.14524776\n",
      "Iteration 29, loss = 0.13624340\n",
      "Iteration 30, loss = 0.12639888\n",
      "Iteration 31, loss = 0.12046397\n",
      "Iteration 32, loss = 0.11447235\n",
      "Iteration 33, loss = 0.10680675\n",
      "Iteration 34, loss = 0.10480696\n",
      "Iteration 35, loss = 0.09673563\n",
      "Iteration 36, loss = 0.09672952\n",
      "Iteration 37, loss = 0.08995512\n",
      "Iteration 38, loss = 0.08926283\n",
      "Iteration 39, loss = 0.08573684\n",
      "Iteration 40, loss = 0.08309268\n",
      "Iteration 41, loss = 0.08244855\n",
      "Iteration 42, loss = 0.07885152\n",
      "Iteration 43, loss = 0.07904344\n",
      "Iteration 44, loss = 0.07628242\n",
      "Iteration 45, loss = 0.07568275\n",
      "Iteration 46, loss = 0.07442143\n",
      "Iteration 47, loss = 0.07292394\n",
      "Iteration 48, loss = 0.07272481\n",
      "Iteration 49, loss = 0.07083939\n",
      "Iteration 50, loss = 0.07094611\n",
      "Iteration 51, loss = 0.06928453\n",
      "Iteration 52, loss = 0.06925679\n",
      "Iteration 53, loss = 0.06801909\n",
      "Iteration 54, loss = 0.06774312\n",
      "Iteration 55, loss = 0.06682265\n",
      "Iteration 56, loss = 0.06647061\n",
      "Iteration 57, loss = 0.06574690\n",
      "Iteration 58, loss = 0.06536511\n",
      "Iteration 59, loss = 0.06470150\n",
      "Iteration 60, loss = 0.06442845\n",
      "Iteration 61, loss = 0.06375912\n",
      "Iteration 62, loss = 0.06359471\n",
      "Iteration 63, loss = 0.06288652\n",
      "Iteration 64, loss = 0.06279286\n",
      "Iteration 65, loss = 0.06214252\n",
      "Iteration 66, loss = 0.06200123\n",
      "Iteration 67, loss = 0.06154122\n",
      "Iteration 68, loss = 0.06118242\n",
      "Iteration 69, loss = 0.06099539\n",
      "Iteration 70, loss = 0.06049015\n",
      "Iteration 71, loss = 0.06033218\n",
      "Iteration 72, loss = 0.06001220\n",
      "Iteration 73, loss = 0.05965185\n",
      "Iteration 74, loss = 0.05951344\n",
      "Iteration 75, loss = 0.05919864\n",
      "Iteration 76, loss = 0.05891311\n",
      "Iteration 77, loss = 0.05877599\n",
      "Iteration 78, loss = 0.05853068\n",
      "Iteration 79, loss = 0.05825802\n",
      "Iteration 80, loss = 0.05810584\n",
      "Iteration 81, loss = 0.05795354\n",
      "Iteration 82, loss = 0.05773382\n",
      "Iteration 83, loss = 0.05753230\n",
      "Iteration 84, loss = 0.05739045\n",
      "Iteration 85, loss = 0.05726792\n",
      "Iteration 86, loss = 0.05712155\n",
      "Iteration 87, loss = 0.05695480\n",
      "Iteration 88, loss = 0.05679783\n",
      "Iteration 89, loss = 0.05665539\n",
      "Iteration 90, loss = 0.05652690\n",
      "Iteration 91, loss = 0.05647211\n",
      "Iteration 92, loss = 0.05702420\n",
      "Iteration 93, loss = 0.05879010\n",
      "Iteration 94, loss = 0.06126124\n",
      "Iteration 95, loss = 0.05811466\n",
      "Iteration 96, loss = 0.05586124\n",
      "Iteration 97, loss = 0.05756367\n",
      "Iteration 98, loss = 0.05696762\n",
      "Iteration 99, loss = 0.05562119\n",
      "Iteration 100, loss = 0.05666633\n",
      "Iteration 101, loss = 0.05605012\n",
      "Iteration 102, loss = 0.05539899\n",
      "Iteration 103, loss = 0.05607066\n",
      "Iteration 104, loss = 0.05539568\n",
      "Iteration 105, loss = 0.05519307\n",
      "Iteration 106, loss = 0.05557069\n",
      "Iteration 107, loss = 0.05494560\n",
      "Iteration 108, loss = 0.05497396\n",
      "Iteration 109, loss = 0.05514295\n",
      "Iteration 110, loss = 0.05462096\n",
      "Iteration 111, loss = 0.05473311\n",
      "Iteration 112, loss = 0.05479869\n",
      "Iteration 113, loss = 0.05436167\n",
      "Iteration 114, loss = 0.05446185\n",
      "Iteration 115, loss = 0.05450280\n",
      "Iteration 116, loss = 0.05413680\n",
      "Iteration 117, loss = 0.05415655\n",
      "Iteration 118, loss = 0.05420690\n",
      "Iteration 119, loss = 0.05392052\n",
      "Iteration 120, loss = 0.05386290\n",
      "Iteration 121, loss = 0.05392349\n",
      "Iteration 122, loss = 0.05371043\n",
      "Iteration 123, loss = 0.05359217\n",
      "Iteration 124, loss = 0.05363665\n",
      "Iteration 125, loss = 0.05351688\n",
      "Iteration 126, loss = 0.05335691\n",
      "Iteration 127, loss = 0.05334358\n",
      "Iteration 128, loss = 0.05331445\n",
      "Iteration 129, loss = 0.05318097\n",
      "Iteration 130, loss = 0.05308127\n",
      "Iteration 131, loss = 0.05306627\n",
      "Iteration 132, loss = 0.05301975\n",
      "Iteration 133, loss = 0.05290358\n",
      "Iteration 134, loss = 0.05281414\n",
      "Iteration 135, loss = 0.05278136\n",
      "Iteration 136, loss = 0.05275251\n",
      "Iteration 137, loss = 0.05267507\n",
      "Iteration 138, loss = 0.05257549\n",
      "Iteration 139, loss = 0.05249424\n",
      "Iteration 140, loss = 0.05244699\n",
      "Iteration 141, loss = 0.05241906\n",
      "Iteration 142, loss = 0.05237727\n",
      "Iteration 143, loss = 0.05231194\n",
      "Iteration 144, loss = 0.05223068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35165820\n",
      "Iteration 2, loss = 1.63726219\n",
      "Iteration 3, loss = 1.93757501\n",
      "Iteration 4, loss = 1.24883043\n",
      "Iteration 5, loss = 0.70228831\n",
      "Iteration 6, loss = 0.88977336\n",
      "Iteration 7, loss = 0.76460150\n",
      "Iteration 8, loss = 0.54573048\n",
      "Iteration 9, loss = 0.60116468\n",
      "Iteration 10, loss = 0.49300700\n",
      "Iteration 11, loss = 0.53371396\n",
      "Iteration 12, loss = 0.43936585\n",
      "Iteration 13, loss = 0.40197016\n",
      "Iteration 14, loss = 0.41367249\n",
      "Iteration 15, loss = 0.37866755\n",
      "Iteration 16, loss = 0.34021342\n",
      "Iteration 17, loss = 0.31278669\n",
      "Iteration 18, loss = 0.28081768\n",
      "Iteration 19, loss = 0.26476012\n",
      "Iteration 20, loss = 0.25796202\n",
      "Iteration 21, loss = 0.23962728\n",
      "Iteration 22, loss = 0.22721135\n",
      "Iteration 23, loss = 0.21443018\n",
      "Iteration 24, loss = 0.19393056\n",
      "Iteration 25, loss = 0.18377139\n",
      "Iteration 26, loss = 0.16586470\n",
      "Iteration 27, loss = 0.15575878\n",
      "Iteration 28, loss = 0.14565915\n",
      "Iteration 29, loss = 0.13413015\n",
      "Iteration 30, loss = 0.12870006\n",
      "Iteration 31, loss = 0.11933528\n",
      "Iteration 32, loss = 0.11451265\n",
      "Iteration 33, loss = 0.10888979\n",
      "Iteration 34, loss = 0.10288173\n",
      "Iteration 35, loss = 0.09992949\n",
      "Iteration 36, loss = 0.09447641\n",
      "Iteration 37, loss = 0.09170405\n",
      "Iteration 38, loss = 0.08825575\n",
      "Iteration 39, loss = 0.08472749\n",
      "Iteration 40, loss = 0.08290262\n",
      "Iteration 41, loss = 0.07964068\n",
      "Iteration 42, loss = 0.07798145\n",
      "Iteration 43, loss = 0.07598490\n",
      "Iteration 44, loss = 0.07393996\n",
      "Iteration 45, loss = 0.07292265\n",
      "Iteration 46, loss = 0.07104307\n",
      "Iteration 47, loss = 0.07010879\n",
      "Iteration 48, loss = 0.06893626\n",
      "Iteration 49, loss = 0.06773296\n",
      "Iteration 50, loss = 0.06708811\n",
      "Iteration 51, loss = 0.06589786\n",
      "Iteration 52, loss = 0.06528567\n",
      "Iteration 53, loss = 0.06442876\n",
      "Iteration 54, loss = 0.06362525\n",
      "Iteration 55, loss = 0.06307405\n",
      "Iteration 56, loss = 0.06221979\n",
      "Iteration 57, loss = 0.06174734\n",
      "Iteration 58, loss = 0.06105005\n",
      "Iteration 59, loss = 0.06050287\n",
      "Iteration 60, loss = 0.06001536\n",
      "Iteration 61, loss = 0.05942141\n",
      "Iteration 62, loss = 0.05905020\n",
      "Iteration 63, loss = 0.05851015\n",
      "Iteration 64, loss = 0.05814316\n",
      "Iteration 65, loss = 0.05771816\n",
      "Iteration 66, loss = 0.05732011\n",
      "Iteration 67, loss = 0.05699159\n",
      "Iteration 68, loss = 0.05658793\n",
      "Iteration 69, loss = 0.05630087\n",
      "Iteration 70, loss = 0.05593069\n",
      "Iteration 71, loss = 0.05564464\n",
      "Iteration 72, loss = 0.05532562\n",
      "Iteration 73, loss = 0.05502858\n",
      "Iteration 74, loss = 0.05475560\n",
      "Iteration 75, loss = 0.05445791\n",
      "Iteration 76, loss = 0.05421415\n",
      "Iteration 77, loss = 0.05393130\n",
      "Iteration 78, loss = 0.05370090\n",
      "Iteration 79, loss = 0.05344351\n",
      "Iteration 80, loss = 0.05321882\n",
      "Iteration 81, loss = 0.05298771\n",
      "Iteration 82, loss = 0.05276743\n",
      "Iteration 83, loss = 0.05255839\n",
      "Iteration 84, loss = 0.05234479\n",
      "Iteration 85, loss = 0.05215174\n",
      "Iteration 86, loss = 0.05194709\n",
      "Iteration 87, loss = 0.05176437\n",
      "Iteration 88, loss = 0.05157045\n",
      "Iteration 89, loss = 0.05139551\n",
      "Iteration 90, loss = 0.05121268\n",
      "Iteration 91, loss = 0.05104340\n",
      "Iteration 92, loss = 0.05087018\n",
      "Iteration 93, loss = 0.05070541\n",
      "Iteration 94, loss = 0.05054075\n",
      "Iteration 95, loss = 0.05038057\n",
      "Iteration 96, loss = 0.05022343\n",
      "Iteration 97, loss = 0.05006808\n",
      "Iteration 98, loss = 0.04991747\n",
      "Iteration 99, loss = 0.04976714\n",
      "Iteration 100, loss = 0.04962223\n",
      "Iteration 101, loss = 0.04947690\n",
      "Iteration 102, loss = 0.04933703\n",
      "Iteration 103, loss = 0.04919653\n",
      "Iteration 104, loss = 0.04906112\n",
      "Iteration 105, loss = 0.04892516\n",
      "Iteration 106, loss = 0.04879377\n",
      "Iteration 107, loss = 0.04866202\n",
      "Iteration 108, loss = 0.04853430\n",
      "Iteration 109, loss = 0.04840644\n",
      "Iteration 110, loss = 0.04828210\n",
      "Iteration 111, loss = 0.04815785\n",
      "Iteration 112, loss = 0.04803669\n",
      "Iteration 113, loss = 0.04791580\n",
      "Iteration 114, loss = 0.04779766\n",
      "Iteration 115, loss = 0.04767995\n",
      "Iteration 116, loss = 0.04756469\n",
      "Iteration 117, loss = 0.04744998\n",
      "Iteration 118, loss = 0.04733747\n",
      "Iteration 119, loss = 0.04722563\n",
      "Iteration 120, loss = 0.04711576\n",
      "Iteration 121, loss = 0.04700665\n",
      "Iteration 122, loss = 0.04689931\n",
      "Iteration 123, loss = 0.04679280\n",
      "Iteration 124, loss = 0.04668789\n",
      "Iteration 125, loss = 0.04658387\n",
      "Iteration 126, loss = 0.04648128\n",
      "Iteration 127, loss = 0.04637965\n",
      "Iteration 128, loss = 0.04627930\n",
      "Iteration 129, loss = 0.04617996\n",
      "Iteration 130, loss = 0.04608180\n",
      "Iteration 131, loss = 0.04598467\n",
      "Iteration 132, loss = 0.04588861\n",
      "Iteration 133, loss = 0.04579362\n",
      "Iteration 134, loss = 0.04569960\n",
      "Iteration 135, loss = 0.04560668\n",
      "Iteration 136, loss = 0.04551466\n",
      "Iteration 137, loss = 0.04542374\n",
      "Iteration 138, loss = 0.04533369\n",
      "Iteration 139, loss = 0.04524471\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35771515\n",
      "Iteration 2, loss = 1.62612155\n",
      "Iteration 3, loss = 1.92755262\n",
      "Iteration 4, loss = 1.23194013\n",
      "Iteration 5, loss = 0.69896170\n",
      "Iteration 6, loss = 0.88010305\n",
      "Iteration 7, loss = 0.74552400\n",
      "Iteration 8, loss = 0.54415391\n",
      "Iteration 9, loss = 0.58917165\n",
      "Iteration 10, loss = 0.48990720\n",
      "Iteration 11, loss = 0.52470553\n",
      "Iteration 12, loss = 0.43607263\n",
      "Iteration 13, loss = 0.40994819\n",
      "Iteration 14, loss = 0.41353957\n",
      "Iteration 15, loss = 0.37882989\n",
      "Iteration 16, loss = 0.35019742\n",
      "Iteration 17, loss = 0.32145194\n",
      "Iteration 18, loss = 0.29090634\n",
      "Iteration 19, loss = 0.27873093\n",
      "Iteration 20, loss = 0.26845942\n",
      "Iteration 21, loss = 0.25221172\n",
      "Iteration 22, loss = 0.24437740\n",
      "Iteration 23, loss = 0.22933358\n",
      "Iteration 24, loss = 0.21587975\n",
      "Iteration 25, loss = 0.20453350\n",
      "Iteration 26, loss = 0.18932208\n",
      "Iteration 27, loss = 0.18162208\n",
      "Iteration 28, loss = 0.16879440\n",
      "Iteration 29, loss = 0.16201676\n",
      "Iteration 30, loss = 0.15334931\n",
      "Iteration 31, loss = 0.14704148\n",
      "Iteration 32, loss = 0.14171110\n",
      "Iteration 33, loss = 0.13538514\n",
      "Iteration 34, loss = 0.13212144\n",
      "Iteration 35, loss = 0.12665199\n",
      "Iteration 36, loss = 0.12439362\n",
      "Iteration 37, loss = 0.11994120\n",
      "Iteration 38, loss = 0.11784276\n",
      "Iteration 39, loss = 0.11461731\n",
      "Iteration 40, loss = 0.11246846\n",
      "Iteration 41, loss = 0.11023940\n",
      "Iteration 42, loss = 0.10808340\n",
      "Iteration 43, loss = 0.10658999\n",
      "Iteration 44, loss = 0.10455186\n",
      "Iteration 45, loss = 0.10353460\n",
      "Iteration 46, loss = 0.10173695\n",
      "Iteration 47, loss = 0.10097344\n",
      "Iteration 48, loss = 0.09945976\n",
      "Iteration 49, loss = 0.09883817\n",
      "Iteration 50, loss = 0.09758436\n",
      "Iteration 51, loss = 0.09702996\n",
      "Iteration 52, loss = 0.09600028\n",
      "Iteration 53, loss = 0.09548509\n",
      "Iteration 54, loss = 0.09461454\n",
      "Iteration 55, loss = 0.09412591\n",
      "Iteration 56, loss = 0.09337532\n",
      "Iteration 57, loss = 0.09291000\n",
      "Iteration 58, loss = 0.09223864\n",
      "Iteration 59, loss = 0.09179874\n",
      "Iteration 60, loss = 0.09119233\n",
      "Iteration 61, loss = 0.09078274\n",
      "Iteration 62, loss = 0.09022544\n",
      "Iteration 63, loss = 0.08984738\n",
      "Iteration 64, loss = 0.08933851\n",
      "Iteration 65, loss = 0.08899404\n",
      "Iteration 66, loss = 0.08852775\n",
      "Iteration 67, loss = 0.08821099\n",
      "Iteration 68, loss = 0.08778905\n",
      "Iteration 69, loss = 0.08749588\n",
      "Iteration 70, loss = 0.08711444\n",
      "Iteration 71, loss = 0.08683453\n",
      "Iteration 72, loss = 0.08649409\n",
      "Iteration 73, loss = 0.08622289\n",
      "Iteration 74, loss = 0.08592013\n",
      "Iteration 75, loss = 0.08565204\n",
      "Iteration 76, loss = 0.08538268\n",
      "Iteration 77, loss = 0.08511730\n",
      "Iteration 78, loss = 0.08487540\n",
      "Iteration 79, loss = 0.08461617\n",
      "Iteration 80, loss = 0.08439447\n",
      "Iteration 81, loss = 0.08414707\n",
      "Iteration 82, loss = 0.08393702\n",
      "Iteration 83, loss = 0.08370758\n",
      "Iteration 84, loss = 0.08350313\n",
      "Iteration 85, loss = 0.08329437\n",
      "Iteration 86, loss = 0.08309305\n",
      "Iteration 87, loss = 0.08290306\n",
      "Iteration 88, loss = 0.08270699\n",
      "Iteration 89, loss = 0.08252975\n",
      "Iteration 90, loss = 0.08234346\n",
      "Iteration 91, loss = 0.08217241\n",
      "Iteration 92, loss = 0.08199890\n",
      "Iteration 93, loss = 0.08183057\n",
      "Iteration 94, loss = 0.08166899\n",
      "Iteration 95, loss = 0.08150448\n",
      "Iteration 96, loss = 0.08135045\n",
      "Iteration 97, loss = 0.08119313\n",
      "Iteration 98, loss = 0.08104253\n",
      "Iteration 99, loss = 0.08089406\n",
      "Iteration 100, loss = 0.08074598\n",
      "Iteration 101, loss = 0.08060437\n",
      "Iteration 102, loss = 0.08046106\n",
      "Iteration 103, loss = 0.08032298\n",
      "Iteration 104, loss = 0.08018628\n",
      "Iteration 105, loss = 0.08005065\n",
      "Iteration 106, loss = 0.07991945\n",
      "Iteration 107, loss = 0.07978782\n",
      "Iteration 108, loss = 0.07965971\n",
      "Iteration 109, loss = 0.07953328\n",
      "Iteration 110, loss = 0.07940762\n",
      "Iteration 111, loss = 0.07928529\n",
      "Iteration 112, loss = 0.07916333\n",
      "Iteration 113, loss = 0.07904344\n",
      "Iteration 114, loss = 0.07892559\n",
      "Iteration 115, loss = 0.07880826\n",
      "Iteration 116, loss = 0.07869331\n",
      "Iteration 117, loss = 0.07857945\n",
      "Iteration 118, loss = 0.07846665\n",
      "Iteration 119, loss = 0.07835589\n",
      "Iteration 120, loss = 0.07824589\n",
      "Iteration 121, loss = 0.07813732\n",
      "Iteration 122, loss = 0.07803032\n",
      "Iteration 123, loss = 0.07792411\n",
      "Iteration 124, loss = 0.07781940\n",
      "Iteration 125, loss = 0.07771596\n",
      "Iteration 126, loss = 0.07761340\n",
      "Iteration 127, loss = 0.07751225\n",
      "Iteration 128, loss = 0.07741219\n",
      "Iteration 129, loss = 0.07731307\n",
      "Iteration 130, loss = 0.07721525\n",
      "Iteration 131, loss = 0.07711843\n",
      "Iteration 132, loss = 0.07702256\n",
      "Iteration 133, loss = 0.07692787\n",
      "Iteration 134, loss = 0.07683414\n",
      "Iteration 135, loss = 0.07674133\n",
      "Iteration 136, loss = 0.07664961\n",
      "Iteration 137, loss = 0.07655881\n",
      "Iteration 138, loss = 0.07646890\n",
      "Iteration 139, loss = 0.07638001\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35721204\n",
      "Iteration 2, loss = 1.74611452\n",
      "Iteration 3, loss = 2.25607157\n",
      "Iteration 4, loss = 0.81103117\n",
      "Iteration 5, loss = 0.70520789\n",
      "Iteration 6, loss = 0.62683946\n",
      "Iteration 7, loss = 0.55559087\n",
      "Iteration 8, loss = 0.50180878\n",
      "Iteration 9, loss = 0.44681033\n",
      "Iteration 10, loss = 0.41370850\n",
      "Iteration 11, loss = 0.38751443\n",
      "Iteration 12, loss = 0.38334437\n",
      "Iteration 13, loss = 0.58228081\n",
      "Iteration 14, loss = 1.70088146\n",
      "Iteration 15, loss = 0.52263764\n",
      "Iteration 16, loss = 0.57009265\n",
      "Iteration 17, loss = 0.40606152\n",
      "Iteration 18, loss = 0.39718730\n",
      "Iteration 19, loss = 0.34433320\n",
      "Iteration 20, loss = 0.28724144\n",
      "Iteration 21, loss = 0.24413513\n",
      "Iteration 22, loss = 0.21670587\n",
      "Iteration 23, loss = 0.21546748\n",
      "Iteration 24, loss = 0.34348772\n",
      "Iteration 25, loss = 1.20392288\n",
      "Iteration 26, loss = 2.37709306\n",
      "Iteration 27, loss = 0.46460359\n",
      "Iteration 28, loss = 0.58101832\n",
      "Iteration 29, loss = 0.56208869\n",
      "Iteration 30, loss = 0.50218889\n",
      "Iteration 31, loss = 0.47778004\n",
      "Iteration 32, loss = 0.46416111\n",
      "Iteration 33, loss = 0.46085829\n",
      "Iteration 34, loss = 0.45124523\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36148097\n",
      "Iteration 2, loss = 1.73643596\n",
      "Iteration 3, loss = 2.17567420\n",
      "Iteration 4, loss = 0.79795878\n",
      "Iteration 5, loss = 0.71635699\n",
      "Iteration 6, loss = 0.64037064\n",
      "Iteration 7, loss = 0.55524331\n",
      "Iteration 8, loss = 0.48606317\n",
      "Iteration 9, loss = 0.44429341\n",
      "Iteration 10, loss = 0.40692315\n",
      "Iteration 11, loss = 0.37729929\n",
      "Iteration 12, loss = 0.35129985\n",
      "Iteration 13, loss = 0.34372729\n",
      "Iteration 14, loss = 0.63384852\n",
      "Iteration 15, loss = 2.34064976\n",
      "Iteration 16, loss = 0.32631214\n",
      "Iteration 17, loss = 0.56943901\n",
      "Iteration 18, loss = 0.45356293\n",
      "Iteration 19, loss = 0.44996298\n",
      "Iteration 20, loss = 0.43915185\n",
      "Iteration 21, loss = 0.39328150\n",
      "Iteration 22, loss = 0.36877797\n",
      "Iteration 23, loss = 0.32943385\n",
      "Iteration 24, loss = 0.31032801\n",
      "Iteration 25, loss = 0.30157229\n",
      "Iteration 26, loss = 0.34289706\n",
      "Iteration 27, loss = 0.69240874\n",
      "Iteration 28, loss = 1.70135164\n",
      "Iteration 29, loss = 3.11960644\n",
      "Iteration 30, loss = 0.52126770\n",
      "Iteration 31, loss = 0.58178722\n",
      "Iteration 32, loss = 0.60668031\n",
      "Iteration 33, loss = 0.59676958\n",
      "Iteration 34, loss = 0.54868485\n",
      "Iteration 35, loss = 0.45362418\n",
      "Iteration 36, loss = 0.39540691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35303276\n",
      "Iteration 2, loss = 1.69721183\n",
      "Iteration 3, loss = 2.07663100\n",
      "Iteration 4, loss = 0.77501764\n",
      "Iteration 5, loss = 0.71697587\n",
      "Iteration 6, loss = 0.64066717\n",
      "Iteration 7, loss = 0.57112884\n",
      "Iteration 8, loss = 0.52739669\n",
      "Iteration 9, loss = 0.49349083\n",
      "Iteration 10, loss = 0.42258252\n",
      "Iteration 11, loss = 0.39125488\n",
      "Iteration 12, loss = 0.36296598\n",
      "Iteration 13, loss = 0.37622954\n",
      "Iteration 14, loss = 0.69808306\n",
      "Iteration 15, loss = 1.56721413\n",
      "Iteration 16, loss = 1.04165900\n",
      "Iteration 17, loss = 0.35560586\n",
      "Iteration 18, loss = 0.41055184\n",
      "Iteration 19, loss = 0.37151966\n",
      "Iteration 20, loss = 0.32204708\n",
      "Iteration 21, loss = 0.26353601\n",
      "Iteration 22, loss = 0.21639168\n",
      "Iteration 23, loss = 0.18719900\n",
      "Iteration 24, loss = 0.17218689\n",
      "Iteration 25, loss = 0.18175182\n",
      "Iteration 26, loss = 0.32499313\n",
      "Iteration 27, loss = 1.35186452\n",
      "Iteration 28, loss = 2.73028352\n",
      "Iteration 29, loss = 0.40824345\n",
      "Iteration 30, loss = 0.65883544\n",
      "Iteration 31, loss = 0.57790794\n",
      "Iteration 32, loss = 0.47113496\n",
      "Iteration 33, loss = 0.41861494\n",
      "Iteration 34, loss = 0.40416199\n",
      "Iteration 35, loss = 0.40937926\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35165820\n",
      "Iteration 2, loss = 1.71786270\n",
      "Iteration 3, loss = 2.12602753\n",
      "Iteration 4, loss = 0.79224175\n",
      "Iteration 5, loss = 0.71788835\n",
      "Iteration 6, loss = 0.64069355\n",
      "Iteration 7, loss = 0.56474153\n",
      "Iteration 8, loss = 0.49976716\n",
      "Iteration 9, loss = 0.44903283\n",
      "Iteration 10, loss = 0.40978637\n",
      "Iteration 11, loss = 0.38617853\n",
      "Iteration 12, loss = 0.41049890\n",
      "Iteration 13, loss = 0.77849549\n",
      "Iteration 14, loss = 1.71882135\n",
      "Iteration 15, loss = 0.34316064\n",
      "Iteration 16, loss = 0.33362881\n",
      "Iteration 17, loss = 0.32743728\n",
      "Iteration 18, loss = 0.29723853\n",
      "Iteration 19, loss = 0.25857249\n",
      "Iteration 20, loss = 0.26673518\n",
      "Iteration 21, loss = 0.38526152\n",
      "Iteration 22, loss = 1.04883153\n",
      "Iteration 23, loss = 0.28366976\n",
      "Iteration 24, loss = 0.30107483\n",
      "Iteration 25, loss = 0.24594129\n",
      "Iteration 26, loss = 0.20291767\n",
      "Iteration 27, loss = 0.19735958\n",
      "Iteration 28, loss = 0.34940441\n",
      "Iteration 29, loss = 0.80266675\n",
      "Iteration 30, loss = 1.67184397\n",
      "Iteration 31, loss = 0.40866222\n",
      "Iteration 32, loss = 0.52554152\n",
      "Iteration 33, loss = 0.47287122\n",
      "Iteration 34, loss = 0.42871743\n",
      "Iteration 35, loss = 0.43305927\n",
      "Iteration 36, loss = 0.43460188\n",
      "Iteration 37, loss = 0.42018012\n",
      "Iteration 38, loss = 0.39960748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35771515\n",
      "Iteration 2, loss = 1.71998789\n",
      "Iteration 3, loss = 2.06683061\n",
      "Iteration 4, loss = 0.76387600\n",
      "Iteration 5, loss = 0.70679143\n",
      "Iteration 6, loss = 0.62492841\n",
      "Iteration 7, loss = 0.55510594\n",
      "Iteration 8, loss = 0.50163689\n",
      "Iteration 9, loss = 0.44684755\n",
      "Iteration 10, loss = 0.40780330\n",
      "Iteration 11, loss = 0.38299248\n",
      "Iteration 12, loss = 0.38742480\n",
      "Iteration 13, loss = 0.66112506\n",
      "Iteration 14, loss = 1.72991915\n",
      "Iteration 15, loss = 0.38644524\n",
      "Iteration 16, loss = 0.36508215\n",
      "Iteration 17, loss = 0.35347713\n",
      "Iteration 18, loss = 0.29318144\n",
      "Iteration 19, loss = 0.26583641\n",
      "Iteration 20, loss = 0.24345795\n",
      "Iteration 21, loss = 0.27103200\n",
      "Iteration 22, loss = 0.58928365\n",
      "Iteration 23, loss = 1.24035660\n",
      "Iteration 24, loss = 1.30642846\n",
      "Iteration 25, loss = 0.44743758\n",
      "Iteration 26, loss = 0.49876979\n",
      "Iteration 27, loss = 0.53849692\n",
      "Iteration 28, loss = 0.52839631\n",
      "Iteration 29, loss = 0.49233334\n",
      "Iteration 30, loss = 0.45880156\n",
      "Iteration 31, loss = 0.44741753\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35721204\n",
      "Iteration 2, loss = 1.10881881\n",
      "Iteration 3, loss = 0.92189471\n",
      "Iteration 4, loss = 0.80397058\n",
      "Iteration 5, loss = 0.75264896\n",
      "Iteration 6, loss = 0.74995610\n",
      "Iteration 7, loss = 0.76274982\n",
      "Iteration 8, loss = 0.77002771\n",
      "Iteration 9, loss = 0.76544293\n",
      "Iteration 10, loss = 0.74995365\n",
      "Iteration 11, loss = 0.72660538\n",
      "Iteration 12, loss = 0.69836352\n",
      "Iteration 13, loss = 0.66777574\n",
      "Iteration 14, loss = 0.63731303\n",
      "Iteration 15, loss = 0.60913715\n",
      "Iteration 16, loss = 0.58468719\n",
      "Iteration 17, loss = 0.56502523\n",
      "Iteration 18, loss = 0.55004478\n",
      "Iteration 19, loss = 0.53902593\n",
      "Iteration 20, loss = 0.53203667\n",
      "Iteration 21, loss = 0.52753027\n",
      "Iteration 22, loss = 0.52117705\n",
      "Iteration 23, loss = 0.51285645\n",
      "Iteration 24, loss = 0.50327578\n",
      "Iteration 25, loss = 0.49280687\n",
      "Iteration 26, loss = 0.48188366\n",
      "Iteration 27, loss = 0.47166194\n",
      "Iteration 28, loss = 0.46333518\n",
      "Iteration 29, loss = 0.45620842\n",
      "Iteration 30, loss = 0.44959417\n",
      "Iteration 31, loss = 0.44335037\n",
      "Iteration 32, loss = 0.43717104\n",
      "Iteration 33, loss = 0.43115011\n",
      "Iteration 34, loss = 0.42566837\n",
      "Iteration 35, loss = 0.42090974\n",
      "Iteration 36, loss = 0.41681556\n",
      "Iteration 37, loss = 0.41323062\n",
      "Iteration 38, loss = 0.40956117\n",
      "Iteration 39, loss = 0.40551904\n",
      "Iteration 40, loss = 0.40103214\n",
      "Iteration 41, loss = 0.39621728\n",
      "Iteration 42, loss = 0.39130716\n",
      "Iteration 43, loss = 0.38656403\n",
      "Iteration 44, loss = 0.38227730\n",
      "Iteration 45, loss = 0.37821384\n",
      "Iteration 46, loss = 0.37419822\n",
      "Iteration 47, loss = 0.37016716\n",
      "Iteration 48, loss = 0.36613637\n",
      "Iteration 49, loss = 0.36216177\n",
      "Iteration 50, loss = 0.35829101\n",
      "Iteration 51, loss = 0.35456314\n",
      "Iteration 52, loss = 0.35098740\n",
      "Iteration 53, loss = 0.34744013\n",
      "Iteration 54, loss = 0.34378966\n",
      "Iteration 55, loss = 0.34004017\n",
      "Iteration 56, loss = 0.33625298\n",
      "Iteration 57, loss = 0.33250898\n",
      "Iteration 58, loss = 0.32874289\n",
      "Iteration 59, loss = 0.32477382\n",
      "Iteration 60, loss = 0.32054758\n",
      "Iteration 61, loss = 0.31613401\n",
      "Iteration 62, loss = 0.31168007\n",
      "Iteration 63, loss = 0.30729550\n",
      "Iteration 64, loss = 0.30291268\n",
      "Iteration 65, loss = 0.29834587\n",
      "Iteration 66, loss = 0.29369123\n",
      "Iteration 67, loss = 0.28910930\n",
      "Iteration 68, loss = 0.28445468\n",
      "Iteration 69, loss = 0.27959058\n",
      "Iteration 70, loss = 0.27457399\n",
      "Iteration 71, loss = 0.26956551\n",
      "Iteration 72, loss = 0.26458594\n",
      "Iteration 73, loss = 0.25951455\n",
      "Iteration 74, loss = 0.25435719\n",
      "Iteration 75, loss = 0.24924853\n",
      "Iteration 76, loss = 0.24415483\n",
      "Iteration 77, loss = 0.23898037\n",
      "Iteration 78, loss = 0.23386720\n",
      "Iteration 79, loss = 0.22885210\n",
      "Iteration 80, loss = 0.22384771\n",
      "Iteration 81, loss = 0.21892862\n",
      "Iteration 82, loss = 0.21413520\n",
      "Iteration 83, loss = 0.20939278\n",
      "Iteration 84, loss = 0.20471853\n",
      "Iteration 85, loss = 0.20019232\n",
      "Iteration 86, loss = 0.19567618\n",
      "Iteration 87, loss = 0.19118431\n",
      "Iteration 88, loss = 0.18676736\n",
      "Iteration 89, loss = 0.18249848\n",
      "Iteration 90, loss = 0.17818563\n",
      "Iteration 91, loss = 0.17402141\n",
      "Iteration 92, loss = 0.16991963\n",
      "Iteration 93, loss = 0.16588442\n",
      "Iteration 94, loss = 0.16201491\n",
      "Iteration 95, loss = 0.15830025\n",
      "Iteration 96, loss = 0.15467226\n",
      "Iteration 97, loss = 0.15111072\n",
      "Iteration 98, loss = 0.14771200\n",
      "Iteration 99, loss = 0.14443772\n",
      "Iteration 100, loss = 0.14129095\n",
      "Iteration 101, loss = 0.13824265\n",
      "Iteration 102, loss = 0.13532682\n",
      "Iteration 103, loss = 0.13253372\n",
      "Iteration 104, loss = 0.12985709\n",
      "Iteration 105, loss = 0.12731756\n",
      "Iteration 106, loss = 0.12488386\n",
      "Iteration 107, loss = 0.12259018\n",
      "Iteration 108, loss = 0.12039272\n",
      "Iteration 109, loss = 0.11829551\n",
      "Iteration 110, loss = 0.11631724\n",
      "Iteration 111, loss = 0.11442794\n",
      "Iteration 112, loss = 0.11264401\n",
      "Iteration 113, loss = 0.11096245\n",
      "Iteration 114, loss = 0.10935419\n",
      "Iteration 115, loss = 0.10783447\n",
      "Iteration 116, loss = 0.10640382\n",
      "Iteration 117, loss = 0.10504453\n",
      "Iteration 118, loss = 0.10375278\n",
      "Iteration 119, loss = 0.10253683\n",
      "Iteration 120, loss = 0.10138638\n",
      "Iteration 121, loss = 0.10029152\n",
      "Iteration 122, loss = 0.09925342\n",
      "Iteration 123, loss = 0.09827399\n",
      "Iteration 124, loss = 0.09734769\n",
      "Iteration 125, loss = 0.09646700\n",
      "Iteration 126, loss = 0.09562977\n",
      "Iteration 127, loss = 0.09483561\n",
      "Iteration 128, loss = 0.09408224\n",
      "Iteration 129, loss = 0.09336706\n",
      "Iteration 130, loss = 0.09268875\n",
      "Iteration 131, loss = 0.09204474\n",
      "Iteration 132, loss = 0.09143260\n",
      "Iteration 133, loss = 0.09085109\n",
      "Iteration 134, loss = 0.09030166\n",
      "Iteration 135, loss = 0.08978718\n",
      "Iteration 136, loss = 0.08929299\n",
      "Iteration 137, loss = 0.08881676\n",
      "Iteration 138, loss = 0.08833481\n",
      "Iteration 139, loss = 0.08787168\n",
      "Iteration 140, loss = 0.08744859\n",
      "Iteration 141, loss = 0.08706020\n",
      "Iteration 142, loss = 0.08668295\n",
      "Iteration 143, loss = 0.08630041\n",
      "Iteration 144, loss = 0.08592439\n",
      "Iteration 145, loss = 0.08556740\n",
      "Iteration 146, loss = 0.08523745\n",
      "Iteration 147, loss = 0.08492286\n",
      "Iteration 148, loss = 0.08461288\n",
      "Iteration 149, loss = 0.08429716\n",
      "Iteration 150, loss = 0.08398712\n",
      "Iteration 151, loss = 0.08369513\n",
      "Iteration 152, loss = 0.08342116\n",
      "Iteration 153, loss = 0.08315290\n",
      "Iteration 154, loss = 0.08288296\n",
      "Iteration 155, loss = 0.08261528\n",
      "Iteration 156, loss = 0.08235914\n",
      "Iteration 157, loss = 0.08211586\n",
      "Iteration 158, loss = 0.08188046\n",
      "Iteration 159, loss = 0.08164737\n",
      "Iteration 160, loss = 0.08141769\n",
      "Iteration 161, loss = 0.08119098\n",
      "Iteration 162, loss = 0.08097203\n",
      "Iteration 163, loss = 0.08076240\n",
      "Iteration 164, loss = 0.08056086\n",
      "Iteration 165, loss = 0.08036505\n",
      "Iteration 166, loss = 0.08017363\n",
      "Iteration 167, loss = 0.07998715\n",
      "Iteration 168, loss = 0.07980431\n",
      "Iteration 169, loss = 0.07962502\n",
      "Iteration 170, loss = 0.07945061\n",
      "Iteration 171, loss = 0.07928209\n",
      "Iteration 172, loss = 0.07911958\n",
      "Iteration 173, loss = 0.07896272\n",
      "Iteration 174, loss = 0.07881083\n",
      "Iteration 175, loss = 0.07866385\n",
      "Iteration 176, loss = 0.07852114\n",
      "Iteration 177, loss = 0.07838384\n",
      "Iteration 178, loss = 0.07825073\n",
      "Iteration 179, loss = 0.07812088\n",
      "Iteration 180, loss = 0.07799675\n",
      "Iteration 181, loss = 0.07787458\n",
      "Iteration 182, loss = 0.07775236\n",
      "Iteration 183, loss = 0.07762858\n",
      "Iteration 184, loss = 0.07750281\n",
      "Iteration 185, loss = 0.07737844\n",
      "Iteration 186, loss = 0.07725976\n",
      "Iteration 187, loss = 0.07714857\n",
      "Iteration 188, loss = 0.07704482\n",
      "Iteration 189, loss = 0.07694497\n",
      "Iteration 190, loss = 0.07684577\n",
      "Iteration 191, loss = 0.07674552\n",
      "Iteration 192, loss = 0.07664479\n",
      "Iteration 193, loss = 0.07654297\n",
      "Iteration 194, loss = 0.07644212\n",
      "Iteration 195, loss = 0.07634315\n",
      "Iteration 196, loss = 0.07624791\n",
      "Iteration 197, loss = 0.07615624\n",
      "Iteration 198, loss = 0.07606773\n",
      "Iteration 199, loss = 0.07598055\n",
      "Iteration 200, loss = 0.07589548\n",
      "Iteration 201, loss = 0.07581186\n",
      "Iteration 202, loss = 0.07572912\n",
      "Iteration 203, loss = 0.07564719\n",
      "Iteration 204, loss = 0.07556615\n",
      "Iteration 205, loss = 0.07548613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36148097\n",
      "Iteration 2, loss = 1.11413451\n",
      "Iteration 3, loss = 0.92793012\n",
      "Iteration 4, loss = 0.80998178\n",
      "Iteration 5, loss = 0.75750312\n",
      "Iteration 6, loss = 0.75388362\n",
      "Iteration 7, loss = 0.76720327\n",
      "Iteration 8, loss = 0.77489683\n",
      "Iteration 9, loss = 0.77005324\n",
      "Iteration 10, loss = 0.75376704\n",
      "Iteration 11, loss = 0.72951577\n",
      "Iteration 12, loss = 0.70055247\n",
      "Iteration 13, loss = 0.66952323\n",
      "Iteration 14, loss = 0.63893205\n",
      "Iteration 15, loss = 0.61069653\n",
      "Iteration 16, loss = 0.58639254\n",
      "Iteration 17, loss = 0.56675859\n",
      "Iteration 18, loss = 0.55177650\n",
      "Iteration 19, loss = 0.54062279\n",
      "Iteration 20, loss = 0.53292497\n",
      "Iteration 21, loss = 0.52806697\n",
      "Iteration 22, loss = 0.52141290\n",
      "Iteration 23, loss = 0.51269107\n",
      "Iteration 24, loss = 0.50265596\n",
      "Iteration 25, loss = 0.49162669\n",
      "Iteration 26, loss = 0.48029758\n",
      "Iteration 27, loss = 0.46984836\n",
      "Iteration 28, loss = 0.46103213\n",
      "Iteration 29, loss = 0.45349407\n",
      "Iteration 30, loss = 0.44655707\n",
      "Iteration 31, loss = 0.43992528\n",
      "Iteration 32, loss = 0.43354344\n",
      "Iteration 33, loss = 0.42772568\n",
      "Iteration 34, loss = 0.42241949\n",
      "Iteration 35, loss = 0.41783145\n",
      "Iteration 36, loss = 0.41368252\n",
      "Iteration 37, loss = 0.40966511\n",
      "Iteration 38, loss = 0.40548177\n",
      "Iteration 39, loss = 0.40091922\n",
      "Iteration 40, loss = 0.39598220\n",
      "Iteration 41, loss = 0.39083784\n",
      "Iteration 42, loss = 0.38573698\n",
      "Iteration 43, loss = 0.38085822\n",
      "Iteration 44, loss = 0.37628300\n",
      "Iteration 45, loss = 0.37203743\n",
      "Iteration 46, loss = 0.36784176\n",
      "Iteration 47, loss = 0.36364653\n",
      "Iteration 48, loss = 0.35944285\n",
      "Iteration 49, loss = 0.35534096\n",
      "Iteration 50, loss = 0.35144169\n",
      "Iteration 51, loss = 0.34764923\n",
      "Iteration 52, loss = 0.34389719\n",
      "Iteration 53, loss = 0.34013761\n",
      "Iteration 54, loss = 0.33627329\n",
      "Iteration 55, loss = 0.33239094\n",
      "Iteration 56, loss = 0.32852046\n",
      "Iteration 57, loss = 0.32468844\n",
      "Iteration 58, loss = 0.32088983\n",
      "Iteration 59, loss = 0.31712692\n",
      "Iteration 60, loss = 0.31338097\n",
      "Iteration 61, loss = 0.30962893\n",
      "Iteration 62, loss = 0.30588213\n",
      "Iteration 63, loss = 0.30219502\n",
      "Iteration 64, loss = 0.29858095\n",
      "Iteration 65, loss = 0.29500180\n",
      "Iteration 66, loss = 0.29142725\n",
      "Iteration 67, loss = 0.28785175\n",
      "Iteration 68, loss = 0.28429674\n",
      "Iteration 69, loss = 0.28077208\n",
      "Iteration 70, loss = 0.27728288\n",
      "Iteration 71, loss = 0.27382222\n",
      "Iteration 72, loss = 0.27037211\n",
      "Iteration 73, loss = 0.26694690\n",
      "Iteration 74, loss = 0.26355031\n",
      "Iteration 75, loss = 0.26019384\n",
      "Iteration 76, loss = 0.25687960\n",
      "Iteration 77, loss = 0.25359778\n",
      "Iteration 78, loss = 0.25033535\n",
      "Iteration 79, loss = 0.24709653\n",
      "Iteration 80, loss = 0.24388118\n",
      "Iteration 81, loss = 0.24069180\n",
      "Iteration 82, loss = 0.23752897\n",
      "Iteration 83, loss = 0.23439115\n",
      "Iteration 84, loss = 0.23127672\n",
      "Iteration 85, loss = 0.22819248\n",
      "Iteration 86, loss = 0.22513712\n",
      "Iteration 87, loss = 0.22211970\n",
      "Iteration 88, loss = 0.21911952\n",
      "Iteration 89, loss = 0.21609410\n",
      "Iteration 90, loss = 0.21309684\n",
      "Iteration 91, loss = 0.21006197\n",
      "Iteration 92, loss = 0.20699339\n",
      "Iteration 93, loss = 0.20387602\n",
      "Iteration 94, loss = 0.20070416\n",
      "Iteration 95, loss = 0.19747954\n",
      "Iteration 96, loss = 0.19420179\n",
      "Iteration 97, loss = 0.19087227\n",
      "Iteration 98, loss = 0.18750282\n",
      "Iteration 99, loss = 0.18409578\n",
      "Iteration 100, loss = 0.18065928\n",
      "Iteration 101, loss = 0.17720161\n",
      "Iteration 102, loss = 0.17373346\n",
      "Iteration 103, loss = 0.17026128\n",
      "Iteration 104, loss = 0.16678804\n",
      "Iteration 105, loss = 0.16333066\n",
      "Iteration 106, loss = 0.15990301\n",
      "Iteration 107, loss = 0.15652356\n",
      "Iteration 108, loss = 0.15319354\n",
      "Iteration 109, loss = 0.14993929\n",
      "Iteration 110, loss = 0.14673970\n",
      "Iteration 111, loss = 0.14364599\n",
      "Iteration 112, loss = 0.14059985\n",
      "Iteration 113, loss = 0.13766323\n",
      "Iteration 114, loss = 0.13479895\n",
      "Iteration 115, loss = 0.13204031\n",
      "Iteration 116, loss = 0.12937176\n",
      "Iteration 117, loss = 0.12682570\n",
      "Iteration 118, loss = 0.12437089\n",
      "Iteration 119, loss = 0.12201103\n",
      "Iteration 120, loss = 0.11975307\n",
      "Iteration 121, loss = 0.11758528\n",
      "Iteration 122, loss = 0.11552882\n",
      "Iteration 123, loss = 0.11355690\n",
      "Iteration 124, loss = 0.11169937\n",
      "Iteration 125, loss = 0.10990936\n",
      "Iteration 126, loss = 0.10822646\n",
      "Iteration 127, loss = 0.10662820\n",
      "Iteration 128, loss = 0.10510036\n",
      "Iteration 129, loss = 0.10366871\n",
      "Iteration 130, loss = 0.10230988\n",
      "Iteration 131, loss = 0.10102170\n",
      "Iteration 132, loss = 0.09981317\n",
      "Iteration 133, loss = 0.09866069\n",
      "Iteration 134, loss = 0.09757206\n",
      "Iteration 135, loss = 0.09655030\n",
      "Iteration 136, loss = 0.09557329\n",
      "Iteration 137, loss = 0.09465383\n",
      "Iteration 138, loss = 0.09378990\n",
      "Iteration 139, loss = 0.09296373\n",
      "Iteration 140, loss = 0.09218099\n",
      "Iteration 141, loss = 0.09144655\n",
      "Iteration 142, loss = 0.09074964\n",
      "Iteration 143, loss = 0.09008669\n",
      "Iteration 144, loss = 0.08945733\n",
      "Iteration 145, loss = 0.08886422\n",
      "Iteration 146, loss = 0.08829860\n",
      "Iteration 147, loss = 0.08775930\n",
      "Iteration 148, loss = 0.08725160\n",
      "Iteration 149, loss = 0.08676892\n",
      "Iteration 150, loss = 0.08630643\n",
      "Iteration 151, loss = 0.08586663\n",
      "Iteration 152, loss = 0.08544749\n",
      "Iteration 153, loss = 0.08504882\n",
      "Iteration 154, loss = 0.08466849\n",
      "Iteration 155, loss = 0.08430565\n",
      "Iteration 156, loss = 0.08395628\n",
      "Iteration 157, loss = 0.08362246\n",
      "Iteration 158, loss = 0.08330550\n",
      "Iteration 159, loss = 0.08300145\n",
      "Iteration 160, loss = 0.08270765\n",
      "Iteration 161, loss = 0.08242454\n",
      "Iteration 162, loss = 0.08215320\n",
      "Iteration 163, loss = 0.08189367\n",
      "Iteration 164, loss = 0.08164547\n",
      "Iteration 165, loss = 0.08141464\n",
      "Iteration 166, loss = 0.08120039\n",
      "Iteration 167, loss = 0.08098680\n",
      "Iteration 168, loss = 0.08075942\n",
      "Iteration 169, loss = 0.08052448\n",
      "Iteration 170, loss = 0.08032380\n",
      "Iteration 171, loss = 0.08014907\n",
      "Iteration 172, loss = 0.07995536\n",
      "Iteration 173, loss = 0.07975606\n",
      "Iteration 174, loss = 0.07958043\n",
      "Iteration 175, loss = 0.07941698\n",
      "Iteration 176, loss = 0.07924677\n",
      "Iteration 177, loss = 0.07907301\n",
      "Iteration 178, loss = 0.07892237\n",
      "Iteration 179, loss = 0.07877786\n",
      "Iteration 180, loss = 0.07861108\n",
      "Iteration 181, loss = 0.07846727\n",
      "Iteration 182, loss = 0.07834545\n",
      "Iteration 183, loss = 0.07818970\n",
      "Iteration 184, loss = 0.07804206\n",
      "Iteration 185, loss = 0.07792010\n",
      "Iteration 186, loss = 0.07778954\n",
      "Iteration 187, loss = 0.07765332\n",
      "Iteration 188, loss = 0.07752929\n",
      "Iteration 189, loss = 0.07741383\n",
      "Iteration 190, loss = 0.07729340\n",
      "Iteration 191, loss = 0.07716981\n",
      "Iteration 192, loss = 0.07705771\n",
      "Iteration 193, loss = 0.07694870\n",
      "Iteration 194, loss = 0.07683671\n",
      "Iteration 195, loss = 0.07672635\n",
      "Iteration 196, loss = 0.07662379\n",
      "Iteration 197, loss = 0.07652351\n",
      "Iteration 198, loss = 0.07641638\n",
      "Iteration 199, loss = 0.07631685\n",
      "Iteration 200, loss = 0.07622173\n",
      "Iteration 201, loss = 0.07612073\n",
      "Iteration 202, loss = 0.07602533\n",
      "Iteration 203, loss = 0.07593477\n",
      "Iteration 204, loss = 0.07584011\n",
      "Iteration 205, loss = 0.07574841\n",
      "Iteration 206, loss = 0.07566162\n",
      "Iteration 207, loss = 0.07557231\n",
      "Iteration 208, loss = 0.07548413\n",
      "Iteration 209, loss = 0.07540051\n",
      "Iteration 210, loss = 0.07531623\n",
      "Iteration 211, loss = 0.07523179\n",
      "Iteration 212, loss = 0.07515079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35303276\n",
      "Iteration 2, loss = 1.10504854\n",
      "Iteration 3, loss = 0.91906857\n",
      "Iteration 4, loss = 0.80154579\n",
      "Iteration 5, loss = 0.74947201\n",
      "Iteration 6, loss = 0.74474179\n",
      "Iteration 7, loss = 0.75706005\n",
      "Iteration 8, loss = 0.76416864\n",
      "Iteration 9, loss = 0.75936111\n",
      "Iteration 10, loss = 0.74363884\n",
      "Iteration 11, loss = 0.72022758\n",
      "Iteration 12, loss = 0.69217705\n",
      "Iteration 13, loss = 0.66196998\n",
      "Iteration 14, loss = 0.63202970\n",
      "Iteration 15, loss = 0.60429842\n",
      "Iteration 16, loss = 0.58019552\n",
      "Iteration 17, loss = 0.56067207\n",
      "Iteration 18, loss = 0.54574477\n",
      "Iteration 19, loss = 0.53459834\n",
      "Iteration 20, loss = 0.52597742\n",
      "Iteration 21, loss = 0.52062233\n",
      "Iteration 22, loss = 0.51422579\n",
      "Iteration 23, loss = 0.50564855\n",
      "Iteration 24, loss = 0.49575231\n",
      "Iteration 25, loss = 0.48506747\n",
      "Iteration 26, loss = 0.47379896\n",
      "Iteration 27, loss = 0.46355012\n",
      "Iteration 28, loss = 0.45482538\n",
      "Iteration 29, loss = 0.44699226\n",
      "Iteration 30, loss = 0.43976602\n",
      "Iteration 31, loss = 0.43295537\n",
      "Iteration 32, loss = 0.42664882\n",
      "Iteration 33, loss = 0.42070179\n",
      "Iteration 34, loss = 0.41541353\n",
      "Iteration 35, loss = 0.41082599\n",
      "Iteration 36, loss = 0.40657706\n",
      "Iteration 37, loss = 0.40232410\n",
      "Iteration 38, loss = 0.39782745\n",
      "Iteration 39, loss = 0.39296547\n",
      "Iteration 40, loss = 0.38778506\n",
      "Iteration 41, loss = 0.38243855\n",
      "Iteration 42, loss = 0.37716160\n",
      "Iteration 43, loss = 0.37209264\n",
      "Iteration 44, loss = 0.36727114\n",
      "Iteration 45, loss = 0.36273103\n",
      "Iteration 46, loss = 0.35831413\n",
      "Iteration 47, loss = 0.35394786\n",
      "Iteration 48, loss = 0.34962969\n",
      "Iteration 49, loss = 0.34537884\n",
      "Iteration 50, loss = 0.34119934\n",
      "Iteration 51, loss = 0.33705221\n",
      "Iteration 52, loss = 0.33299037\n",
      "Iteration 53, loss = 0.32894099\n",
      "Iteration 54, loss = 0.32476090\n",
      "Iteration 55, loss = 0.32058923\n",
      "Iteration 56, loss = 0.31639600\n",
      "Iteration 57, loss = 0.31216674\n",
      "Iteration 58, loss = 0.30797845\n",
      "Iteration 59, loss = 0.30374433\n",
      "Iteration 60, loss = 0.29937106\n",
      "Iteration 61, loss = 0.29488307\n",
      "Iteration 62, loss = 0.29029158\n",
      "Iteration 63, loss = 0.28559319\n",
      "Iteration 64, loss = 0.28080892\n",
      "Iteration 65, loss = 0.27594244\n",
      "Iteration 66, loss = 0.27095922\n",
      "Iteration 67, loss = 0.26584132\n",
      "Iteration 68, loss = 0.26063354\n",
      "Iteration 69, loss = 0.25533364\n",
      "Iteration 70, loss = 0.24993255\n",
      "Iteration 71, loss = 0.24445641\n",
      "Iteration 72, loss = 0.23912273\n",
      "Iteration 73, loss = 0.23372351\n",
      "Iteration 74, loss = 0.22846082\n",
      "Iteration 75, loss = 0.22302568\n",
      "Iteration 76, loss = 0.21755629\n",
      "Iteration 77, loss = 0.21220403\n",
      "Iteration 78, loss = 0.20679616\n",
      "Iteration 79, loss = 0.20154747\n",
      "Iteration 80, loss = 0.19633944\n",
      "Iteration 81, loss = 0.19112867\n",
      "Iteration 82, loss = 0.18604045\n",
      "Iteration 83, loss = 0.18105623\n",
      "Iteration 84, loss = 0.17615238\n",
      "Iteration 85, loss = 0.17137816\n",
      "Iteration 86, loss = 0.16671100\n",
      "Iteration 87, loss = 0.16213053\n",
      "Iteration 88, loss = 0.15771097\n",
      "Iteration 89, loss = 0.15342444\n",
      "Iteration 90, loss = 0.14927299\n",
      "Iteration 91, loss = 0.14528765\n",
      "Iteration 92, loss = 0.14141281\n",
      "Iteration 93, loss = 0.13771089\n",
      "Iteration 94, loss = 0.13415193\n",
      "Iteration 95, loss = 0.13071897\n",
      "Iteration 96, loss = 0.12745821\n",
      "Iteration 97, loss = 0.12430201\n",
      "Iteration 98, loss = 0.12132015\n",
      "Iteration 99, loss = 0.11846705\n",
      "Iteration 100, loss = 0.11574618\n",
      "Iteration 101, loss = 0.11317351\n",
      "Iteration 102, loss = 0.11070859\n",
      "Iteration 103, loss = 0.10838520\n",
      "Iteration 104, loss = 0.10614979\n",
      "Iteration 105, loss = 0.10406681\n",
      "Iteration 106, loss = 0.10204650\n",
      "Iteration 107, loss = 0.10016095\n",
      "Iteration 108, loss = 0.09835190\n",
      "Iteration 109, loss = 0.09665597\n",
      "Iteration 110, loss = 0.09502348\n",
      "Iteration 111, loss = 0.09349068\n",
      "Iteration 112, loss = 0.09202714\n",
      "Iteration 113, loss = 0.09063514\n",
      "Iteration 114, loss = 0.08931595\n",
      "Iteration 115, loss = 0.08805902\n",
      "Iteration 116, loss = 0.08687275\n",
      "Iteration 117, loss = 0.08572945\n",
      "Iteration 118, loss = 0.08465542\n",
      "Iteration 119, loss = 0.08362083\n",
      "Iteration 120, loss = 0.08264336\n",
      "Iteration 121, loss = 0.08170580\n",
      "Iteration 122, loss = 0.08081303\n",
      "Iteration 123, loss = 0.07996390\n",
      "Iteration 124, loss = 0.07914429\n",
      "Iteration 125, loss = 0.07836792\n",
      "Iteration 126, loss = 0.07761971\n",
      "Iteration 127, loss = 0.07690805\n",
      "Iteration 128, loss = 0.07622214\n",
      "Iteration 129, loss = 0.07556680\n",
      "Iteration 130, loss = 0.07493718\n",
      "Iteration 131, loss = 0.07433287\n",
      "Iteration 132, loss = 0.07375456\n",
      "Iteration 133, loss = 0.07319688\n",
      "Iteration 134, loss = 0.07266235\n",
      "Iteration 135, loss = 0.07214551\n",
      "Iteration 136, loss = 0.07165300\n",
      "Iteration 137, loss = 0.07117429\n",
      "Iteration 138, loss = 0.07071696\n",
      "Iteration 139, loss = 0.07027408\n",
      "Iteration 140, loss = 0.06985058\n",
      "Iteration 141, loss = 0.06943881\n",
      "Iteration 142, loss = 0.06904282\n",
      "Iteration 143, loss = 0.06865875\n",
      "Iteration 144, loss = 0.06828944\n",
      "Iteration 145, loss = 0.06793056\n",
      "Iteration 146, loss = 0.06758585\n",
      "Iteration 147, loss = 0.06725001\n",
      "Iteration 148, loss = 0.06692558\n",
      "Iteration 149, loss = 0.06661139\n",
      "Iteration 150, loss = 0.06630670\n",
      "Iteration 151, loss = 0.06601061\n",
      "Iteration 152, loss = 0.06572359\n",
      "Iteration 153, loss = 0.06544470\n",
      "Iteration 154, loss = 0.06517418\n",
      "Iteration 155, loss = 0.06491208\n",
      "Iteration 156, loss = 0.06465633\n",
      "Iteration 157, loss = 0.06440831\n",
      "Iteration 158, loss = 0.06416691\n",
      "Iteration 159, loss = 0.06393252\n",
      "Iteration 160, loss = 0.06370371\n",
      "Iteration 161, loss = 0.06348166\n",
      "Iteration 162, loss = 0.06326506\n",
      "Iteration 163, loss = 0.06305372\n",
      "Iteration 164, loss = 0.06284826\n",
      "Iteration 165, loss = 0.06264813\n",
      "Iteration 166, loss = 0.06245224\n",
      "Iteration 167, loss = 0.06226191\n",
      "Iteration 168, loss = 0.06207594\n",
      "Iteration 169, loss = 0.06189397\n",
      "Iteration 170, loss = 0.06171603\n",
      "Iteration 171, loss = 0.06154228\n",
      "Iteration 172, loss = 0.06137365\n",
      "Iteration 173, loss = 0.06120781\n",
      "Iteration 174, loss = 0.06104635\n",
      "Iteration 175, loss = 0.06088895\n",
      "Iteration 176, loss = 0.06073489\n",
      "Iteration 177, loss = 0.06058395\n",
      "Iteration 178, loss = 0.06043619\n",
      "Iteration 179, loss = 0.06029186\n",
      "Iteration 180, loss = 0.06015100\n",
      "Iteration 181, loss = 0.06001232\n",
      "Iteration 182, loss = 0.05987686\n",
      "Iteration 183, loss = 0.05974442\n",
      "Iteration 184, loss = 0.05961474\n",
      "Iteration 185, loss = 0.05948747\n",
      "Iteration 186, loss = 0.05936264\n",
      "Iteration 187, loss = 0.05924031\n",
      "Iteration 188, loss = 0.05912027\n",
      "Iteration 189, loss = 0.05900253\n",
      "Iteration 190, loss = 0.05888723\n",
      "Iteration 191, loss = 0.05877389\n",
      "Iteration 192, loss = 0.05866275\n",
      "Iteration 193, loss = 0.05855363\n",
      "Iteration 194, loss = 0.05844647\n",
      "Iteration 195, loss = 0.05834143\n",
      "Iteration 196, loss = 0.05823810\n",
      "Iteration 197, loss = 0.05813661\n",
      "Iteration 198, loss = 0.05803699\n",
      "Iteration 199, loss = 0.05793907\n",
      "Iteration 200, loss = 0.05784279\n",
      "Iteration 201, loss = 0.05774810\n",
      "Iteration 202, loss = 0.05765496\n",
      "Iteration 203, loss = 0.05756335\n",
      "Iteration 204, loss = 0.05747321\n",
      "Iteration 205, loss = 0.05738450\n",
      "Iteration 206, loss = 0.05729720\n",
      "Iteration 207, loss = 0.05721125\n",
      "Iteration 208, loss = 0.05712663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35165820\n",
      "Iteration 2, loss = 1.10424458\n",
      "Iteration 3, loss = 0.91840542\n",
      "Iteration 4, loss = 0.80151386\n",
      "Iteration 5, loss = 0.75021197\n",
      "Iteration 6, loss = 0.74679324\n",
      "Iteration 7, loss = 0.76000234\n",
      "Iteration 8, loss = 0.76731795\n",
      "Iteration 9, loss = 0.76224539\n",
      "Iteration 10, loss = 0.74596423\n",
      "Iteration 11, loss = 0.72185971\n",
      "Iteration 12, loss = 0.69314822\n",
      "Iteration 13, loss = 0.66237883\n",
      "Iteration 14, loss = 0.63206771\n",
      "Iteration 15, loss = 0.60415735\n",
      "Iteration 16, loss = 0.58000455\n",
      "Iteration 17, loss = 0.56076632\n",
      "Iteration 18, loss = 0.54613165\n",
      "Iteration 19, loss = 0.53517805\n",
      "Iteration 20, loss = 0.52701369\n",
      "Iteration 21, loss = 0.52217093\n",
      "Iteration 22, loss = 0.51555171\n",
      "Iteration 23, loss = 0.50680143\n",
      "Iteration 24, loss = 0.49632411\n",
      "Iteration 25, loss = 0.48511068\n",
      "Iteration 26, loss = 0.47334176\n",
      "Iteration 27, loss = 0.46284285\n",
      "Iteration 28, loss = 0.45432110\n",
      "Iteration 29, loss = 0.44704591\n",
      "Iteration 30, loss = 0.44019304\n",
      "Iteration 31, loss = 0.43352964\n",
      "Iteration 32, loss = 0.42709673\n",
      "Iteration 33, loss = 0.42105014\n",
      "Iteration 34, loss = 0.41543048\n",
      "Iteration 35, loss = 0.41060473\n",
      "Iteration 36, loss = 0.40629637\n",
      "Iteration 37, loss = 0.40214774\n",
      "Iteration 38, loss = 0.39786130\n",
      "Iteration 39, loss = 0.39318639\n",
      "Iteration 40, loss = 0.38807375\n",
      "Iteration 41, loss = 0.38273962\n",
      "Iteration 42, loss = 0.37746428\n",
      "Iteration 43, loss = 0.37243023\n",
      "Iteration 44, loss = 0.36765280\n",
      "Iteration 45, loss = 0.36313361\n",
      "Iteration 46, loss = 0.35876079\n",
      "Iteration 47, loss = 0.35437798\n",
      "Iteration 48, loss = 0.34998914\n",
      "Iteration 49, loss = 0.34565060\n",
      "Iteration 50, loss = 0.34142281\n",
      "Iteration 51, loss = 0.33737705\n",
      "Iteration 52, loss = 0.33336075\n",
      "Iteration 53, loss = 0.32930436\n",
      "Iteration 54, loss = 0.32517013\n",
      "Iteration 55, loss = 0.32096965\n",
      "Iteration 56, loss = 0.31680222\n",
      "Iteration 57, loss = 0.31273844\n",
      "Iteration 58, loss = 0.30869905\n",
      "Iteration 59, loss = 0.30448588\n",
      "Iteration 60, loss = 0.30005534\n",
      "Iteration 61, loss = 0.29548704\n",
      "Iteration 62, loss = 0.29085544\n",
      "Iteration 63, loss = 0.28624340\n",
      "Iteration 64, loss = 0.28160534\n",
      "Iteration 65, loss = 0.27678925\n",
      "Iteration 66, loss = 0.27180458\n",
      "Iteration 67, loss = 0.26679399\n",
      "Iteration 68, loss = 0.26176336\n",
      "Iteration 69, loss = 0.25659783\n",
      "Iteration 70, loss = 0.25130222\n",
      "Iteration 71, loss = 0.24597763\n",
      "Iteration 72, loss = 0.24072342\n",
      "Iteration 73, loss = 0.23537467\n",
      "Iteration 74, loss = 0.23005482\n",
      "Iteration 75, loss = 0.22474443\n",
      "Iteration 76, loss = 0.21936312\n",
      "Iteration 77, loss = 0.21397045\n",
      "Iteration 78, loss = 0.20864724\n",
      "Iteration 79, loss = 0.20334677\n",
      "Iteration 80, loss = 0.19811089\n",
      "Iteration 81, loss = 0.19294749\n",
      "Iteration 82, loss = 0.18782047\n",
      "Iteration 83, loss = 0.18277225\n",
      "Iteration 84, loss = 0.17782642\n",
      "Iteration 85, loss = 0.17296017\n",
      "Iteration 86, loss = 0.16820699\n",
      "Iteration 87, loss = 0.16355360\n",
      "Iteration 88, loss = 0.15902103\n",
      "Iteration 89, loss = 0.15463236\n",
      "Iteration 90, loss = 0.15038486\n",
      "Iteration 91, loss = 0.14628286\n",
      "Iteration 92, loss = 0.14230692\n",
      "Iteration 93, loss = 0.13846456\n",
      "Iteration 94, loss = 0.13477391\n",
      "Iteration 95, loss = 0.13122281\n",
      "Iteration 96, loss = 0.12780373\n",
      "Iteration 97, loss = 0.12454823\n",
      "Iteration 98, loss = 0.12141726\n",
      "Iteration 99, loss = 0.11842072\n",
      "Iteration 100, loss = 0.11557374\n",
      "Iteration 101, loss = 0.11284123\n",
      "Iteration 102, loss = 0.11023761\n",
      "Iteration 103, loss = 0.10775959\n",
      "Iteration 104, loss = 0.10539865\n",
      "Iteration 105, loss = 0.10314804\n",
      "Iteration 106, loss = 0.10100157\n",
      "Iteration 107, loss = 0.09895649\n",
      "Iteration 108, loss = 0.09700979\n",
      "Iteration 109, loss = 0.09515567\n",
      "Iteration 110, loss = 0.09338741\n",
      "Iteration 111, loss = 0.09170096\n",
      "Iteration 112, loss = 0.09009253\n",
      "Iteration 113, loss = 0.08855745\n",
      "Iteration 114, loss = 0.08709181\n",
      "Iteration 115, loss = 0.08569172\n",
      "Iteration 116, loss = 0.08435435\n",
      "Iteration 117, loss = 0.08307480\n",
      "Iteration 118, loss = 0.08185467\n",
      "Iteration 119, loss = 0.08068037\n",
      "Iteration 120, loss = 0.07955904\n",
      "Iteration 121, loss = 0.07848526\n",
      "Iteration 122, loss = 0.07745564\n",
      "Iteration 123, loss = 0.07646681\n",
      "Iteration 124, loss = 0.07551795\n",
      "Iteration 125, loss = 0.07460299\n",
      "Iteration 126, loss = 0.07372028\n",
      "Iteration 127, loss = 0.07286422\n",
      "Iteration 128, loss = 0.07203229\n",
      "Iteration 129, loss = 0.07121984\n",
      "Iteration 130, loss = 0.07041879\n",
      "Iteration 131, loss = 0.06962379\n",
      "Iteration 132, loss = 0.06882839\n",
      "Iteration 133, loss = 0.06803621\n",
      "Iteration 134, loss = 0.06725276\n",
      "Iteration 135, loss = 0.06647361\n",
      "Iteration 136, loss = 0.06570772\n",
      "Iteration 137, loss = 0.06496152\n",
      "Iteration 138, loss = 0.06422863\n",
      "Iteration 139, loss = 0.06351577\n",
      "Iteration 140, loss = 0.06281265\n",
      "Iteration 141, loss = 0.06211301\n",
      "Iteration 142, loss = 0.06141693\n",
      "Iteration 143, loss = 0.06076050\n",
      "Iteration 144, loss = 0.06005723\n",
      "Iteration 145, loss = 0.05939998\n",
      "Iteration 146, loss = 0.05877309\n",
      "Iteration 147, loss = 0.05815787\n",
      "Iteration 148, loss = 0.05767850\n",
      "Iteration 149, loss = 0.05726784\n",
      "Iteration 150, loss = 0.05661363\n",
      "Iteration 151, loss = 0.05605616\n",
      "Iteration 152, loss = 0.05578629\n",
      "Iteration 153, loss = 0.05513907\n",
      "Iteration 154, loss = 0.05487849\n",
      "Iteration 155, loss = 0.05437947\n",
      "Iteration 156, loss = 0.05402909\n",
      "Iteration 157, loss = 0.05368535\n",
      "Iteration 158, loss = 0.05327106\n",
      "Iteration 159, loss = 0.05300777\n",
      "Iteration 160, loss = 0.05257949\n",
      "Iteration 161, loss = 0.05234698\n",
      "Iteration 162, loss = 0.05195357\n",
      "Iteration 163, loss = 0.05171422\n",
      "Iteration 164, loss = 0.05137729\n",
      "Iteration 165, loss = 0.05110930\n",
      "Iteration 166, loss = 0.05082855\n",
      "Iteration 167, loss = 0.05053644\n",
      "Iteration 168, loss = 0.05030274\n",
      "Iteration 169, loss = 0.05000801\n",
      "Iteration 170, loss = 0.04979583\n",
      "Iteration 171, loss = 0.04951907\n",
      "Iteration 172, loss = 0.04930252\n",
      "Iteration 173, loss = 0.04906136\n",
      "Iteration 174, loss = 0.04883339\n",
      "Iteration 175, loss = 0.04862856\n",
      "Iteration 176, loss = 0.04839633\n",
      "Iteration 177, loss = 0.04820755\n",
      "Iteration 178, loss = 0.04799041\n",
      "Iteration 179, loss = 0.04779993\n",
      "Iteration 180, loss = 0.04761078\n",
      "Iteration 181, loss = 0.04741498\n",
      "Iteration 182, loss = 0.04724435\n",
      "Iteration 183, loss = 0.04705713\n",
      "Iteration 184, loss = 0.04688833\n",
      "Iteration 185, loss = 0.04672102\n",
      "Iteration 186, loss = 0.04654799\n",
      "Iteration 187, loss = 0.04639295\n",
      "Iteration 188, loss = 0.04623026\n",
      "Iteration 189, loss = 0.04607476\n",
      "Iteration 190, loss = 0.04592707\n",
      "Iteration 191, loss = 0.04577332\n",
      "Iteration 192, loss = 0.04563038\n",
      "Iteration 193, loss = 0.04549019\n",
      "Iteration 194, loss = 0.04534819\n",
      "Iteration 195, loss = 0.04521508\n",
      "Iteration 196, loss = 0.04508148\n",
      "Iteration 197, loss = 0.04494883\n",
      "Iteration 198, loss = 0.04482336\n",
      "Iteration 199, loss = 0.04469777\n",
      "Iteration 200, loss = 0.04457367\n",
      "Iteration 201, loss = 0.04445518\n",
      "Iteration 202, loss = 0.04433688\n",
      "Iteration 203, loss = 0.04421988\n",
      "Iteration 204, loss = 0.04410733\n",
      "Iteration 205, loss = 0.04399568\n",
      "Iteration 206, loss = 0.04388537\n",
      "Iteration 207, loss = 0.04377877\n",
      "Iteration 208, loss = 0.04367392\n",
      "Iteration 209, loss = 0.04356955\n",
      "Iteration 210, loss = 0.04346767\n",
      "Iteration 211, loss = 0.04336832\n",
      "Iteration 212, loss = 0.04327017\n",
      "Iteration 213, loss = 0.04317336\n",
      "Iteration 214, loss = 0.04307892\n",
      "Iteration 215, loss = 0.04298603\n",
      "Iteration 216, loss = 0.04289425\n",
      "Iteration 217, loss = 0.04280410\n",
      "Iteration 218, loss = 0.04271593\n",
      "Iteration 219, loss = 0.04262912\n",
      "Iteration 220, loss = 0.04254329\n",
      "Iteration 221, loss = 0.04245901\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35771515\n",
      "Iteration 2, loss = 1.10699475\n",
      "Iteration 3, loss = 0.91863603\n",
      "Iteration 4, loss = 0.79961857\n",
      "Iteration 5, loss = 0.74649225\n",
      "Iteration 6, loss = 0.74174011\n",
      "Iteration 7, loss = 0.75543673\n",
      "Iteration 8, loss = 0.76407826\n",
      "Iteration 9, loss = 0.76006199\n",
      "Iteration 10, loss = 0.74434368\n",
      "Iteration 11, loss = 0.72042874\n",
      "Iteration 12, loss = 0.69165981\n",
      "Iteration 13, loss = 0.66074814\n",
      "Iteration 14, loss = 0.63027899\n",
      "Iteration 15, loss = 0.60220760\n",
      "Iteration 16, loss = 0.57783076\n",
      "Iteration 17, loss = 0.55826890\n",
      "Iteration 18, loss = 0.54339804\n",
      "Iteration 19, loss = 0.53236738\n",
      "Iteration 20, loss = 0.52498518\n",
      "Iteration 21, loss = 0.52025717\n",
      "Iteration 22, loss = 0.51378762\n",
      "Iteration 23, loss = 0.50551124\n",
      "Iteration 24, loss = 0.49592355\n",
      "Iteration 25, loss = 0.48607422\n",
      "Iteration 26, loss = 0.47584552\n",
      "Iteration 27, loss = 0.46569928\n",
      "Iteration 28, loss = 0.45598851\n",
      "Iteration 29, loss = 0.44811514\n",
      "Iteration 30, loss = 0.44163571\n",
      "Iteration 31, loss = 0.43538036\n",
      "Iteration 32, loss = 0.42920033\n",
      "Iteration 33, loss = 0.42301584\n",
      "Iteration 34, loss = 0.41744580\n",
      "Iteration 35, loss = 0.41271729\n",
      "Iteration 36, loss = 0.40855932\n",
      "Iteration 37, loss = 0.40466424\n",
      "Iteration 38, loss = 0.40077971\n",
      "Iteration 39, loss = 0.39658584\n",
      "Iteration 40, loss = 0.39201534\n",
      "Iteration 41, loss = 0.38719735\n",
      "Iteration 42, loss = 0.38232817\n",
      "Iteration 43, loss = 0.37760923\n",
      "Iteration 44, loss = 0.37316185\n",
      "Iteration 45, loss = 0.36887648\n",
      "Iteration 46, loss = 0.36466588\n",
      "Iteration 47, loss = 0.36044641\n",
      "Iteration 48, loss = 0.35624778\n",
      "Iteration 49, loss = 0.35217943\n",
      "Iteration 50, loss = 0.34833421\n",
      "Iteration 51, loss = 0.34465625\n",
      "Iteration 52, loss = 0.34107240\n",
      "Iteration 53, loss = 0.33744151\n",
      "Iteration 54, loss = 0.33366609\n",
      "Iteration 55, loss = 0.32981794\n",
      "Iteration 56, loss = 0.32603877\n",
      "Iteration 57, loss = 0.32236064\n",
      "Iteration 58, loss = 0.31870912\n",
      "Iteration 59, loss = 0.31495017\n",
      "Iteration 60, loss = 0.31102963\n",
      "Iteration 61, loss = 0.30698901\n",
      "Iteration 62, loss = 0.30294038\n",
      "Iteration 63, loss = 0.29893008\n",
      "Iteration 64, loss = 0.29482149\n",
      "Iteration 65, loss = 0.29070229\n",
      "Iteration 66, loss = 0.28654929\n",
      "Iteration 67, loss = 0.28229130\n",
      "Iteration 68, loss = 0.27784550\n",
      "Iteration 69, loss = 0.27323991\n",
      "Iteration 70, loss = 0.26857165\n",
      "Iteration 71, loss = 0.26387936\n",
      "Iteration 72, loss = 0.25912126\n",
      "Iteration 73, loss = 0.25425972\n",
      "Iteration 74, loss = 0.24940212\n",
      "Iteration 75, loss = 0.24456895\n",
      "Iteration 76, loss = 0.23969175\n",
      "Iteration 77, loss = 0.23481546\n",
      "Iteration 78, loss = 0.23000444\n",
      "Iteration 79, loss = 0.22518200\n",
      "Iteration 80, loss = 0.22043822\n",
      "Iteration 81, loss = 0.21566345\n",
      "Iteration 82, loss = 0.21097802\n",
      "Iteration 83, loss = 0.20636383\n",
      "Iteration 84, loss = 0.20184423\n",
      "Iteration 85, loss = 0.19740856\n",
      "Iteration 86, loss = 0.19305432\n",
      "Iteration 87, loss = 0.18881105\n",
      "Iteration 88, loss = 0.18467859\n",
      "Iteration 89, loss = 0.18066679\n",
      "Iteration 90, loss = 0.17675430\n",
      "Iteration 91, loss = 0.17291986\n",
      "Iteration 92, loss = 0.16918761\n",
      "Iteration 93, loss = 0.16553305\n",
      "Iteration 94, loss = 0.16197851\n",
      "Iteration 95, loss = 0.15847309\n",
      "Iteration 96, loss = 0.15507273\n",
      "Iteration 97, loss = 0.15173079\n",
      "Iteration 98, loss = 0.14842633\n",
      "Iteration 99, loss = 0.14518169\n",
      "Iteration 100, loss = 0.14205400\n",
      "Iteration 101, loss = 0.13902345\n",
      "Iteration 102, loss = 0.13604379\n",
      "Iteration 103, loss = 0.13320443\n",
      "Iteration 104, loss = 0.13043119\n",
      "Iteration 105, loss = 0.12773165\n",
      "Iteration 106, loss = 0.12514369\n",
      "Iteration 107, loss = 0.12269297\n",
      "Iteration 108, loss = 0.12027555\n",
      "Iteration 109, loss = 0.11799985\n",
      "Iteration 110, loss = 0.11583431\n",
      "Iteration 111, loss = 0.11375128\n",
      "Iteration 112, loss = 0.11176735\n",
      "Iteration 113, loss = 0.10989737\n",
      "Iteration 114, loss = 0.10812639\n",
      "Iteration 115, loss = 0.10644098\n",
      "Iteration 116, loss = 0.10483862\n",
      "Iteration 117, loss = 0.10333295\n",
      "Iteration 118, loss = 0.10191266\n",
      "Iteration 119, loss = 0.10056648\n",
      "Iteration 120, loss = 0.09929310\n",
      "Iteration 121, loss = 0.09810222\n",
      "Iteration 122, loss = 0.09699664\n",
      "Iteration 123, loss = 0.09603032\n",
      "Iteration 124, loss = 0.09510056\n",
      "Iteration 125, loss = 0.09414574\n",
      "Iteration 126, loss = 0.09321035\n",
      "Iteration 127, loss = 0.09250319\n",
      "Iteration 128, loss = 0.09178194\n",
      "Iteration 129, loss = 0.09097886\n",
      "Iteration 130, loss = 0.09037797\n",
      "Iteration 131, loss = 0.08977646\n",
      "Iteration 132, loss = 0.08910921\n",
      "Iteration 133, loss = 0.08860848\n",
      "Iteration 134, loss = 0.08808370\n",
      "Iteration 135, loss = 0.08752883\n",
      "Iteration 136, loss = 0.08710850\n",
      "Iteration 137, loss = 0.08664591\n",
      "Iteration 138, loss = 0.08618415\n",
      "Iteration 139, loss = 0.08582216\n",
      "Iteration 140, loss = 0.08541482\n",
      "Iteration 141, loss = 0.08502568\n",
      "Iteration 142, loss = 0.08470809\n",
      "Iteration 143, loss = 0.08435198\n",
      "Iteration 144, loss = 0.08401785\n",
      "Iteration 145, loss = 0.08373603\n",
      "Iteration 146, loss = 0.08342548\n",
      "Iteration 147, loss = 0.08313255\n",
      "Iteration 148, loss = 0.08288111\n",
      "Iteration 149, loss = 0.08261057\n",
      "Iteration 150, loss = 0.08234868\n",
      "Iteration 151, loss = 0.08212226\n",
      "Iteration 152, loss = 0.08188665\n",
      "Iteration 153, loss = 0.08165065\n",
      "Iteration 154, loss = 0.08144330\n",
      "Iteration 155, loss = 0.08123722\n",
      "Iteration 156, loss = 0.08102399\n",
      "Iteration 157, loss = 0.08082976\n",
      "Iteration 158, loss = 0.08064640\n",
      "Iteration 159, loss = 0.08045628\n",
      "Iteration 160, loss = 0.08027293\n",
      "Iteration 161, loss = 0.08010461\n",
      "Iteration 162, loss = 0.07993691\n",
      "Iteration 163, loss = 0.07976737\n",
      "Iteration 164, loss = 0.07960730\n",
      "Iteration 165, loss = 0.07945546\n",
      "Iteration 166, loss = 0.07930504\n",
      "Iteration 167, loss = 0.07915508\n",
      "Iteration 168, loss = 0.07901203\n",
      "Iteration 169, loss = 0.07887424\n",
      "Iteration 170, loss = 0.07873635\n",
      "Iteration 171, loss = 0.07859894\n",
      "Iteration 172, loss = 0.07846616\n",
      "Iteration 173, loss = 0.07833800\n",
      "Iteration 174, loss = 0.07821129\n",
      "Iteration 175, loss = 0.07808498\n",
      "Iteration 176, loss = 0.07796254\n",
      "Iteration 177, loss = 0.07784383\n",
      "Iteration 178, loss = 0.07772807\n",
      "Iteration 179, loss = 0.07761357\n",
      "Iteration 180, loss = 0.07749990\n",
      "Iteration 181, loss = 0.07738798\n",
      "Iteration 182, loss = 0.07727860\n",
      "Iteration 183, loss = 0.07717162\n",
      "Iteration 184, loss = 0.07706630\n",
      "Iteration 185, loss = 0.07696217\n",
      "Iteration 186, loss = 0.07685909\n",
      "Iteration 187, loss = 0.07675855\n",
      "Iteration 188, loss = 0.07665943\n",
      "Iteration 189, loss = 0.07656180\n",
      "Iteration 190, loss = 0.07646552\n",
      "Iteration 191, loss = 0.07637035\n",
      "Iteration 192, loss = 0.07627651\n",
      "Iteration 193, loss = 0.07618446\n",
      "Iteration 194, loss = 0.07609349\n",
      "Iteration 195, loss = 0.07600354\n",
      "Iteration 196, loss = 0.07591467\n",
      "Iteration 197, loss = 0.07582685\n",
      "Iteration 198, loss = 0.07574010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35721204\n",
      "Iteration 2, loss = 1.03891745\n",
      "Iteration 3, loss = 0.83917146\n",
      "Iteration 4, loss = 0.77333751\n",
      "Iteration 5, loss = 0.75961469\n",
      "Iteration 6, loss = 0.76529405\n",
      "Iteration 7, loss = 0.74912679\n",
      "Iteration 8, loss = 0.71361679\n",
      "Iteration 9, loss = 0.67372739\n",
      "Iteration 10, loss = 0.63932786\n",
      "Iteration 11, loss = 0.61466974\n",
      "Iteration 12, loss = 0.59880927\n",
      "Iteration 13, loss = 0.58701298\n",
      "Iteration 14, loss = 0.57505826\n",
      "Iteration 15, loss = 0.56122584\n",
      "Iteration 16, loss = 0.54632944\n",
      "Iteration 17, loss = 0.53190532\n",
      "Iteration 18, loss = 0.51920328\n",
      "Iteration 19, loss = 0.50846125\n",
      "Iteration 20, loss = 0.49926207\n",
      "Iteration 21, loss = 0.49086139\n",
      "Iteration 22, loss = 0.48286064\n",
      "Iteration 23, loss = 0.47508968\n",
      "Iteration 24, loss = 0.46763812\n",
      "Iteration 25, loss = 0.46064106\n",
      "Iteration 26, loss = 0.45413540\n",
      "Iteration 27, loss = 0.44809128\n",
      "Iteration 28, loss = 0.44235274\n",
      "Iteration 29, loss = 0.43684694\n",
      "Iteration 30, loss = 0.43157091\n",
      "Iteration 31, loss = 0.42652390\n",
      "Iteration 32, loss = 0.42165811\n",
      "Iteration 33, loss = 0.41692826\n",
      "Iteration 34, loss = 0.41236757\n",
      "Iteration 35, loss = 0.40794393\n",
      "Iteration 36, loss = 0.40364881\n",
      "Iteration 37, loss = 0.39945252\n",
      "Iteration 38, loss = 0.39534733\n",
      "Iteration 39, loss = 0.39132530\n",
      "Iteration 40, loss = 0.38737844\n",
      "Iteration 41, loss = 0.38349915\n",
      "Iteration 42, loss = 0.37968068\n",
      "Iteration 43, loss = 0.37591729\n",
      "Iteration 44, loss = 0.37220595\n",
      "Iteration 45, loss = 0.36854164\n",
      "Iteration 46, loss = 0.36492131\n",
      "Iteration 47, loss = 0.36134789\n",
      "Iteration 48, loss = 0.35782417\n",
      "Iteration 49, loss = 0.35433920\n",
      "Iteration 50, loss = 0.35089174\n",
      "Iteration 51, loss = 0.34748078\n",
      "Iteration 52, loss = 0.34412020\n",
      "Iteration 53, loss = 0.34079359\n",
      "Iteration 54, loss = 0.33749535\n",
      "Iteration 55, loss = 0.33423015\n",
      "Iteration 56, loss = 0.33099723\n",
      "Iteration 57, loss = 0.32779647\n",
      "Iteration 58, loss = 0.32463144\n",
      "Iteration 59, loss = 0.32150157\n",
      "Iteration 60, loss = 0.31840435\n",
      "Iteration 61, loss = 0.31533979\n",
      "Iteration 62, loss = 0.31230457\n",
      "Iteration 63, loss = 0.30929801\n",
      "Iteration 64, loss = 0.30632470\n",
      "Iteration 65, loss = 0.30338255\n",
      "Iteration 66, loss = 0.30046474\n",
      "Iteration 67, loss = 0.29758048\n",
      "Iteration 68, loss = 0.29473424\n",
      "Iteration 69, loss = 0.29192642\n",
      "Iteration 70, loss = 0.28915876\n",
      "Iteration 71, loss = 0.28642830\n",
      "Iteration 72, loss = 0.28374344\n",
      "Iteration 73, loss = 0.28109322\n",
      "Iteration 74, loss = 0.27848074\n",
      "Iteration 75, loss = 0.27591037\n",
      "Iteration 76, loss = 0.27337569\n",
      "Iteration 77, loss = 0.27087427\n",
      "Iteration 78, loss = 0.26841261\n",
      "Iteration 79, loss = 0.26598397\n",
      "Iteration 80, loss = 0.26358900\n",
      "Iteration 81, loss = 0.26123341\n",
      "Iteration 82, loss = 0.25891471\n",
      "Iteration 83, loss = 0.25662826\n",
      "Iteration 84, loss = 0.25437260\n",
      "Iteration 85, loss = 0.25215167\n",
      "Iteration 86, loss = 0.24996642\n",
      "Iteration 87, loss = 0.24781984\n",
      "Iteration 88, loss = 0.24571169\n",
      "Iteration 89, loss = 0.24364013\n",
      "Iteration 90, loss = 0.24160214\n",
      "Iteration 91, loss = 0.23959742\n",
      "Iteration 92, loss = 0.23762726\n",
      "Iteration 93, loss = 0.23569108\n",
      "Iteration 94, loss = 0.23378958\n",
      "Iteration 95, loss = 0.23191996\n",
      "Iteration 96, loss = 0.23008179\n",
      "Iteration 97, loss = 0.22827425\n",
      "Iteration 98, loss = 0.22649755\n",
      "Iteration 99, loss = 0.22475107\n",
      "Iteration 100, loss = 0.22303443\n",
      "Iteration 101, loss = 0.22134748\n",
      "Iteration 102, loss = 0.21969003\n",
      "Iteration 103, loss = 0.21806215\n",
      "Iteration 104, loss = 0.21646346\n",
      "Iteration 105, loss = 0.21489288\n",
      "Iteration 106, loss = 0.21334989\n",
      "Iteration 107, loss = 0.21183391\n",
      "Iteration 108, loss = 0.21034472\n",
      "Iteration 109, loss = 0.20888142\n",
      "Iteration 110, loss = 0.20744387\n",
      "Iteration 111, loss = 0.20603161\n",
      "Iteration 112, loss = 0.20464421\n",
      "Iteration 113, loss = 0.20328108\n",
      "Iteration 114, loss = 0.20194197\n",
      "Iteration 115, loss = 0.20062634\n",
      "Iteration 116, loss = 0.19933398\n",
      "Iteration 117, loss = 0.19806395\n",
      "Iteration 118, loss = 0.19681627\n",
      "Iteration 119, loss = 0.19559050\n",
      "Iteration 120, loss = 0.19438842\n",
      "Iteration 121, loss = 0.19320753\n",
      "Iteration 122, loss = 0.19204727\n",
      "Iteration 123, loss = 0.19090715\n",
      "Iteration 124, loss = 0.18978674\n",
      "Iteration 125, loss = 0.18868569\n",
      "Iteration 126, loss = 0.18760361\n",
      "Iteration 127, loss = 0.18654010\n",
      "Iteration 128, loss = 0.18549477\n",
      "Iteration 129, loss = 0.18446725\n",
      "Iteration 130, loss = 0.18345718\n",
      "Iteration 131, loss = 0.18246422\n",
      "Iteration 132, loss = 0.18148796\n",
      "Iteration 133, loss = 0.18052810\n",
      "Iteration 134, loss = 0.17958430\n",
      "Iteration 135, loss = 0.17865620\n",
      "Iteration 136, loss = 0.17774348\n",
      "Iteration 137, loss = 0.17684582\n",
      "Iteration 138, loss = 0.17596295\n",
      "Iteration 139, loss = 0.17509445\n",
      "Iteration 140, loss = 0.17424010\n",
      "Iteration 141, loss = 0.17339960\n",
      "Iteration 142, loss = 0.17257266\n",
      "Iteration 143, loss = 0.17175897\n",
      "Iteration 144, loss = 0.17095857\n",
      "Iteration 145, loss = 0.17017081\n",
      "Iteration 146, loss = 0.16939580\n",
      "Iteration 147, loss = 0.16863346\n",
      "Iteration 148, loss = 0.16788340\n",
      "Iteration 149, loss = 0.16714490\n",
      "Iteration 150, loss = 0.16641768\n",
      "Iteration 151, loss = 0.16570169\n",
      "Iteration 152, loss = 0.16499692\n",
      "Iteration 153, loss = 0.16430280\n",
      "Iteration 154, loss = 0.16361948\n",
      "Iteration 155, loss = 0.16294669\n",
      "Iteration 156, loss = 0.16228398\n",
      "Iteration 157, loss = 0.16163128\n",
      "Iteration 158, loss = 0.16098823\n",
      "Iteration 159, loss = 0.16035489\n",
      "Iteration 160, loss = 0.15973095\n",
      "Iteration 161, loss = 0.15911604\n",
      "Iteration 162, loss = 0.15851033\n",
      "Iteration 163, loss = 0.15791330\n",
      "Iteration 164, loss = 0.15732504\n",
      "Iteration 165, loss = 0.15674540\n",
      "Iteration 166, loss = 0.15617435\n",
      "Iteration 167, loss = 0.15561125\n",
      "Iteration 168, loss = 0.15505621\n",
      "Iteration 169, loss = 0.15450912\n",
      "Iteration 170, loss = 0.15396965\n",
      "Iteration 171, loss = 0.15343767\n",
      "Iteration 172, loss = 0.15291310\n",
      "Iteration 173, loss = 0.15239579\n",
      "Iteration 174, loss = 0.15188557\n",
      "Iteration 175, loss = 0.15138233\n",
      "Iteration 176, loss = 0.15088593\n",
      "Iteration 177, loss = 0.15039623\n",
      "Iteration 178, loss = 0.14991310\n",
      "Iteration 179, loss = 0.14943645\n",
      "Iteration 180, loss = 0.14896610\n",
      "Iteration 181, loss = 0.14850196\n",
      "Iteration 182, loss = 0.14804396\n",
      "Iteration 183, loss = 0.14759188\n",
      "Iteration 184, loss = 0.14714570\n",
      "Iteration 185, loss = 0.14670528\n",
      "Iteration 186, loss = 0.14627050\n",
      "Iteration 187, loss = 0.14584126\n",
      "Iteration 188, loss = 0.14541749\n",
      "Iteration 189, loss = 0.14499904\n",
      "Iteration 190, loss = 0.14458584\n",
      "Iteration 191, loss = 0.14417782\n",
      "Iteration 192, loss = 0.14377484\n",
      "Iteration 193, loss = 0.14337683\n",
      "Iteration 194, loss = 0.14298369\n",
      "Iteration 195, loss = 0.14259536\n",
      "Iteration 196, loss = 0.14221173\n",
      "Iteration 197, loss = 0.14183272\n",
      "Iteration 198, loss = 0.14145828\n",
      "Iteration 199, loss = 0.14108826\n",
      "Iteration 200, loss = 0.14072266\n",
      "Iteration 201, loss = 0.14036137\n",
      "Iteration 202, loss = 0.14000430\n",
      "Iteration 203, loss = 0.13965140\n",
      "Iteration 204, loss = 0.13930264\n",
      "Iteration 205, loss = 0.13895799\n",
      "Iteration 206, loss = 0.13861725\n",
      "Iteration 207, loss = 0.13828037\n",
      "Iteration 208, loss = 0.13794739\n",
      "Iteration 209, loss = 0.13761816\n",
      "Iteration 210, loss = 0.13729261\n",
      "Iteration 211, loss = 0.13697076\n",
      "Iteration 212, loss = 0.13665244\n",
      "Iteration 213, loss = 0.13633774\n",
      "Iteration 214, loss = 0.13602641\n",
      "Iteration 215, loss = 0.13571856\n",
      "Iteration 216, loss = 0.13541404\n",
      "Iteration 217, loss = 0.13511284\n",
      "Iteration 218, loss = 0.13481493\n",
      "Iteration 219, loss = 0.13452017\n",
      "Iteration 220, loss = 0.13422862\n",
      "Iteration 221, loss = 0.13394018\n",
      "Iteration 222, loss = 0.13365487\n",
      "Iteration 223, loss = 0.13337256\n",
      "Iteration 224, loss = 0.13309324\n",
      "Iteration 225, loss = 0.13281684\n",
      "Iteration 226, loss = 0.13254335\n",
      "Iteration 227, loss = 0.13227266\n",
      "Iteration 228, loss = 0.13200474\n",
      "Iteration 229, loss = 0.13173959\n",
      "Iteration 230, loss = 0.13147710\n",
      "Iteration 231, loss = 0.13121728\n",
      "Iteration 232, loss = 0.13096006\n",
      "Iteration 233, loss = 0.13070543\n",
      "Iteration 234, loss = 0.13045337\n",
      "Iteration 235, loss = 0.13020377\n",
      "Iteration 236, loss = 0.12995665\n",
      "Iteration 237, loss = 0.12971197\n",
      "Iteration 238, loss = 0.12946967\n",
      "Iteration 239, loss = 0.12922977\n",
      "Iteration 240, loss = 0.12899216\n",
      "Iteration 241, loss = 0.12875688\n",
      "Iteration 242, loss = 0.12852388\n",
      "Iteration 243, loss = 0.12829310\n",
      "Iteration 244, loss = 0.12806450\n",
      "Iteration 245, loss = 0.12783811\n",
      "Iteration 246, loss = 0.12761381\n",
      "Iteration 247, loss = 0.12739165\n",
      "Iteration 248, loss = 0.12717158\n",
      "Iteration 249, loss = 0.12695354\n",
      "Iteration 250, loss = 0.12673751\n",
      "Iteration 251, loss = 0.12652348\n",
      "Iteration 252, loss = 0.12631146\n",
      "Iteration 253, loss = 0.12610131\n",
      "Iteration 254, loss = 0.12589313\n",
      "Iteration 255, loss = 0.12568683\n",
      "Iteration 256, loss = 0.12548238\n",
      "Iteration 257, loss = 0.12527977\n",
      "Iteration 258, loss = 0.12507897\n",
      "Iteration 259, loss = 0.12487997\n",
      "Iteration 260, loss = 0.12468274\n",
      "Iteration 261, loss = 0.12448725\n",
      "Iteration 262, loss = 0.12429348\n",
      "Iteration 263, loss = 0.12410140\n",
      "Iteration 264, loss = 0.12391103\n",
      "Iteration 265, loss = 0.12372229\n",
      "Iteration 266, loss = 0.12353518\n",
      "Iteration 267, loss = 0.12334968\n",
      "Iteration 268, loss = 0.12316579\n",
      "Iteration 269, loss = 0.12298346\n",
      "Iteration 270, loss = 0.12280268\n",
      "Iteration 271, loss = 0.12262346\n",
      "Iteration 272, loss = 0.12244570\n",
      "Iteration 273, loss = 0.12226946\n",
      "Iteration 274, loss = 0.12209478\n",
      "Iteration 275, loss = 0.12192166\n",
      "Iteration 276, loss = 0.12175002\n",
      "Iteration 277, loss = 0.12157979\n",
      "Iteration 278, loss = 0.12141098\n",
      "Iteration 279, loss = 0.12124359\n",
      "Iteration 280, loss = 0.12107752\n",
      "Iteration 281, loss = 0.12091283\n",
      "Iteration 282, loss = 0.12074947\n",
      "Iteration 283, loss = 0.12058745\n",
      "Iteration 284, loss = 0.12042673\n",
      "Iteration 285, loss = 0.12026730\n",
      "Iteration 286, loss = 0.12010913\n",
      "Iteration 287, loss = 0.11995223\n",
      "Iteration 288, loss = 0.11979657\n",
      "Iteration 289, loss = 0.11964215\n",
      "Iteration 290, loss = 0.11948893\n",
      "Iteration 291, loss = 0.11933692\n",
      "Iteration 292, loss = 0.11918608\n",
      "Iteration 293, loss = 0.11903642\n",
      "Iteration 294, loss = 0.11888792\n",
      "Iteration 295, loss = 0.11874058\n",
      "Iteration 296, loss = 0.11859435\n",
      "Iteration 297, loss = 0.11844924\n",
      "Iteration 298, loss = 0.11830523\n",
      "Iteration 299, loss = 0.11816232\n",
      "Iteration 300, loss = 0.11802048\n",
      "Iteration 301, loss = 0.11787971\n",
      "Iteration 302, loss = 0.11774003\n",
      "Iteration 303, loss = 0.11760133\n",
      "Iteration 304, loss = 0.11746370\n",
      "Iteration 305, loss = 0.11732717\n",
      "Iteration 306, loss = 0.11719163\n",
      "Iteration 307, loss = 0.11705707\n",
      "Iteration 308, loss = 0.11692347\n",
      "Iteration 309, loss = 0.11679084\n",
      "Iteration 310, loss = 0.11665914\n",
      "Iteration 311, loss = 0.11652848\n",
      "Iteration 312, loss = 0.11639873\n",
      "Iteration 313, loss = 0.11626992\n",
      "Iteration 314, loss = 0.11614203\n",
      "Iteration 315, loss = 0.11601505\n",
      "Iteration 316, loss = 0.11588896\n",
      "Iteration 317, loss = 0.11576377\n",
      "Iteration 318, loss = 0.11563946\n",
      "Iteration 319, loss = 0.11551604\n",
      "Iteration 320, loss = 0.11539347\n",
      "Iteration 321, loss = 0.11527181\n",
      "Iteration 322, loss = 0.11515093\n",
      "Iteration 323, loss = 0.11503093\n",
      "Iteration 324, loss = 0.11491174\n",
      "Iteration 325, loss = 0.11479336\n",
      "Iteration 326, loss = 0.11467581\n",
      "Iteration 327, loss = 0.11455907\n",
      "Iteration 328, loss = 0.11444311\n",
      "Iteration 329, loss = 0.11432794\n",
      "Iteration 330, loss = 0.11421357\n",
      "Iteration 331, loss = 0.11409995\n",
      "Iteration 332, loss = 0.11398711\n",
      "Iteration 333, loss = 0.11387507\n",
      "Iteration 334, loss = 0.11376378\n",
      "Iteration 335, loss = 0.11365322\n",
      "Iteration 336, loss = 0.11354340\n",
      "Iteration 337, loss = 0.11343433\n",
      "Iteration 338, loss = 0.11332595\n",
      "Iteration 339, loss = 0.11321829\n",
      "Iteration 340, loss = 0.11311135\n",
      "Iteration 341, loss = 0.11300510\n",
      "Iteration 342, loss = 0.11289954\n",
      "Iteration 343, loss = 0.11279466\n",
      "Iteration 344, loss = 0.11269047\n",
      "Iteration 345, loss = 0.11258694\n",
      "Iteration 346, loss = 0.11248407\n",
      "Iteration 347, loss = 0.11238188\n",
      "Iteration 348, loss = 0.11228032\n",
      "Iteration 349, loss = 0.11217940\n",
      "Iteration 350, loss = 0.11207915\n",
      "Iteration 351, loss = 0.11197950\n",
      "Iteration 352, loss = 0.11188048\n",
      "Iteration 353, loss = 0.11178211\n",
      "Iteration 354, loss = 0.11168433\n",
      "Iteration 355, loss = 0.11158715\n",
      "Iteration 356, loss = 0.11149059\n",
      "Iteration 357, loss = 0.11139463\n",
      "Iteration 358, loss = 0.11129925\n",
      "Iteration 359, loss = 0.11120445\n",
      "Iteration 360, loss = 0.11111025\n",
      "Iteration 361, loss = 0.11101660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36148097\n",
      "Iteration 2, loss = 1.04594581\n",
      "Iteration 3, loss = 0.84704508\n",
      "Iteration 4, loss = 0.78052835\n",
      "Iteration 5, loss = 0.76511094\n",
      "Iteration 6, loss = 0.76931331\n",
      "Iteration 7, loss = 0.75294903\n",
      "Iteration 8, loss = 0.71688566\n",
      "Iteration 9, loss = 0.67641070\n",
      "Iteration 10, loss = 0.64163133\n",
      "Iteration 11, loss = 0.61706966\n",
      "Iteration 12, loss = 0.60116979\n",
      "Iteration 13, loss = 0.58899206\n",
      "Iteration 14, loss = 0.57633866\n",
      "Iteration 15, loss = 0.56170033\n",
      "Iteration 16, loss = 0.54626608\n",
      "Iteration 17, loss = 0.53145672\n",
      "Iteration 18, loss = 0.51840700\n",
      "Iteration 19, loss = 0.50736574\n",
      "Iteration 20, loss = 0.49781331\n",
      "Iteration 21, loss = 0.48898704\n",
      "Iteration 22, loss = 0.48052493\n",
      "Iteration 23, loss = 0.47230650\n",
      "Iteration 24, loss = 0.46443305\n",
      "Iteration 25, loss = 0.45703472\n",
      "Iteration 26, loss = 0.45021803\n",
      "Iteration 27, loss = 0.44389943\n",
      "Iteration 28, loss = 0.43792114\n",
      "Iteration 29, loss = 0.43215424\n",
      "Iteration 30, loss = 0.42657045\n",
      "Iteration 31, loss = 0.42115218\n",
      "Iteration 32, loss = 0.41588954\n",
      "Iteration 33, loss = 0.41078009\n",
      "Iteration 34, loss = 0.40587526\n",
      "Iteration 35, loss = 0.40114685\n",
      "Iteration 36, loss = 0.39653032\n",
      "Iteration 37, loss = 0.39201367\n",
      "Iteration 38, loss = 0.38761191\n",
      "Iteration 39, loss = 0.38330607\n",
      "Iteration 40, loss = 0.37907936\n",
      "Iteration 41, loss = 0.37492529\n",
      "Iteration 42, loss = 0.37083812\n",
      "Iteration 43, loss = 0.36681303\n",
      "Iteration 44, loss = 0.36284604\n",
      "Iteration 45, loss = 0.35893384\n",
      "Iteration 46, loss = 0.35507381\n",
      "Iteration 47, loss = 0.35129475\n",
      "Iteration 48, loss = 0.34756304\n",
      "Iteration 49, loss = 0.34387664\n",
      "Iteration 50, loss = 0.34023434\n",
      "Iteration 51, loss = 0.33663556\n",
      "Iteration 52, loss = 0.33309985\n",
      "Iteration 53, loss = 0.32959507\n",
      "Iteration 54, loss = 0.32611957\n",
      "Iteration 55, loss = 0.32267664\n",
      "Iteration 56, loss = 0.31926866\n",
      "Iteration 57, loss = 0.31589837\n",
      "Iteration 58, loss = 0.31257134\n",
      "Iteration 59, loss = 0.30928601\n",
      "Iteration 60, loss = 0.30604181\n",
      "Iteration 61, loss = 0.30283743\n",
      "Iteration 62, loss = 0.29967191\n",
      "Iteration 63, loss = 0.29654657\n",
      "Iteration 64, loss = 0.29345835\n",
      "Iteration 65, loss = 0.29040524\n",
      "Iteration 66, loss = 0.28739180\n",
      "Iteration 67, loss = 0.28441962\n",
      "Iteration 68, loss = 0.28148479\n",
      "Iteration 69, loss = 0.27858965\n",
      "Iteration 70, loss = 0.27573846\n",
      "Iteration 71, loss = 0.27292902\n",
      "Iteration 72, loss = 0.27016339\n",
      "Iteration 73, loss = 0.26744336\n",
      "Iteration 74, loss = 0.26476506\n",
      "Iteration 75, loss = 0.26213123\n",
      "Iteration 76, loss = 0.25953777\n",
      "Iteration 77, loss = 0.25699003\n",
      "Iteration 78, loss = 0.25448267\n",
      "Iteration 79, loss = 0.25201164\n",
      "Iteration 80, loss = 0.24958099\n",
      "Iteration 81, loss = 0.24719064\n",
      "Iteration 82, loss = 0.24484983\n",
      "Iteration 83, loss = 0.24254778\n",
      "Iteration 84, loss = 0.24028660\n",
      "Iteration 85, loss = 0.23806585\n",
      "Iteration 86, loss = 0.23588872\n",
      "Iteration 87, loss = 0.23375429\n",
      "Iteration 88, loss = 0.23165597\n",
      "Iteration 89, loss = 0.22959658\n",
      "Iteration 90, loss = 0.22757736\n",
      "Iteration 91, loss = 0.22559697\n",
      "Iteration 92, loss = 0.22365325\n",
      "Iteration 93, loss = 0.22174461\n",
      "Iteration 94, loss = 0.21987137\n",
      "Iteration 95, loss = 0.21803285\n",
      "Iteration 96, loss = 0.21622737\n",
      "Iteration 97, loss = 0.21445498\n",
      "Iteration 98, loss = 0.21271524\n",
      "Iteration 99, loss = 0.21100760\n",
      "Iteration 100, loss = 0.20933273\n",
      "Iteration 101, loss = 0.20768818\n",
      "Iteration 102, loss = 0.20607387\n",
      "Iteration 103, loss = 0.20448924\n",
      "Iteration 104, loss = 0.20293374\n",
      "Iteration 105, loss = 0.20140697\n",
      "Iteration 106, loss = 0.19990962\n",
      "Iteration 107, loss = 0.19844023\n",
      "Iteration 108, loss = 0.19699811\n",
      "Iteration 109, loss = 0.19558263\n",
      "Iteration 110, loss = 0.19419336\n",
      "Iteration 111, loss = 0.19282983\n",
      "Iteration 112, loss = 0.19149227\n",
      "Iteration 113, loss = 0.19017936\n",
      "Iteration 114, loss = 0.18889071\n",
      "Iteration 115, loss = 0.18762584\n",
      "Iteration 116, loss = 0.18638453\n",
      "Iteration 117, loss = 0.18516599\n",
      "Iteration 118, loss = 0.18396976\n",
      "Iteration 119, loss = 0.18279537\n",
      "Iteration 120, loss = 0.18164235\n",
      "Iteration 121, loss = 0.18051027\n",
      "Iteration 122, loss = 0.17939871\n",
      "Iteration 123, loss = 0.17830730\n",
      "Iteration 124, loss = 0.17723548\n",
      "Iteration 125, loss = 0.17618294\n",
      "Iteration 126, loss = 0.17514912\n",
      "Iteration 127, loss = 0.17413366\n",
      "Iteration 128, loss = 0.17313619\n",
      "Iteration 129, loss = 0.17215768\n",
      "Iteration 130, loss = 0.17119729\n",
      "Iteration 131, loss = 0.17025468\n",
      "Iteration 132, loss = 0.16932860\n",
      "Iteration 133, loss = 0.16841827\n",
      "Iteration 134, loss = 0.16752344\n",
      "Iteration 135, loss = 0.16664403\n",
      "Iteration 136, loss = 0.16577963\n",
      "Iteration 137, loss = 0.16492992\n",
      "Iteration 138, loss = 0.16409458\n",
      "Iteration 139, loss = 0.16327334\n",
      "Iteration 140, loss = 0.16246597\n",
      "Iteration 141, loss = 0.16167214\n",
      "Iteration 142, loss = 0.16089157\n",
      "Iteration 143, loss = 0.16012425\n",
      "Iteration 144, loss = 0.15936974\n",
      "Iteration 145, loss = 0.15862777\n",
      "Iteration 146, loss = 0.15789788\n",
      "Iteration 147, loss = 0.15717975\n",
      "Iteration 148, loss = 0.15647317\n",
      "Iteration 149, loss = 0.15577789\n",
      "Iteration 150, loss = 0.15509368\n",
      "Iteration 151, loss = 0.15442035\n",
      "Iteration 152, loss = 0.15375784\n",
      "Iteration 153, loss = 0.15310610\n",
      "Iteration 154, loss = 0.15246454\n",
      "Iteration 155, loss = 0.15183300\n",
      "Iteration 156, loss = 0.15121120\n",
      "Iteration 157, loss = 0.15059894\n",
      "Iteration 158, loss = 0.14999603\n",
      "Iteration 159, loss = 0.14940229\n",
      "Iteration 160, loss = 0.14881751\n",
      "Iteration 161, loss = 0.14824160\n",
      "Iteration 162, loss = 0.14767429\n",
      "Iteration 163, loss = 0.14711541\n",
      "Iteration 164, loss = 0.14656478\n",
      "Iteration 165, loss = 0.14602222\n",
      "Iteration 166, loss = 0.14548757\n",
      "Iteration 167, loss = 0.14496069\n",
      "Iteration 168, loss = 0.14444141\n",
      "Iteration 169, loss = 0.14392957\n",
      "Iteration 170, loss = 0.14342504\n",
      "Iteration 171, loss = 0.14292766\n",
      "Iteration 172, loss = 0.14243729\n",
      "Iteration 173, loss = 0.14195380\n",
      "Iteration 174, loss = 0.14147705\n",
      "Iteration 175, loss = 0.14100690\n",
      "Iteration 176, loss = 0.14054323\n",
      "Iteration 177, loss = 0.14008592\n",
      "Iteration 178, loss = 0.13963483\n",
      "Iteration 179, loss = 0.13918986\n",
      "Iteration 180, loss = 0.13875085\n",
      "Iteration 181, loss = 0.13831773\n",
      "Iteration 182, loss = 0.13789038\n",
      "Iteration 183, loss = 0.13746867\n",
      "Iteration 184, loss = 0.13705251\n",
      "Iteration 185, loss = 0.13664181\n",
      "Iteration 186, loss = 0.13623644\n",
      "Iteration 187, loss = 0.13583632\n",
      "Iteration 188, loss = 0.13544133\n",
      "Iteration 189, loss = 0.13505141\n",
      "Iteration 190, loss = 0.13466643\n",
      "Iteration 191, loss = 0.13428632\n",
      "Iteration 192, loss = 0.13391100\n",
      "Iteration 193, loss = 0.13354037\n",
      "Iteration 194, loss = 0.13317434\n",
      "Iteration 195, loss = 0.13281284\n",
      "Iteration 196, loss = 0.13245579\n",
      "Iteration 197, loss = 0.13210310\n",
      "Iteration 198, loss = 0.13175470\n",
      "Iteration 199, loss = 0.13141052\n",
      "Iteration 200, loss = 0.13107048\n",
      "Iteration 201, loss = 0.13073465\n",
      "Iteration 202, loss = 0.13040291\n",
      "Iteration 203, loss = 0.13007512\n",
      "Iteration 204, loss = 0.12975132\n",
      "Iteration 205, loss = 0.12943133\n",
      "Iteration 206, loss = 0.12911509\n",
      "Iteration 207, loss = 0.12880252\n",
      "Iteration 208, loss = 0.12849357\n",
      "Iteration 209, loss = 0.12818817\n",
      "Iteration 210, loss = 0.12788627\n",
      "Iteration 211, loss = 0.12758779\n",
      "Iteration 212, loss = 0.12729269\n",
      "Iteration 213, loss = 0.12700091\n",
      "Iteration 214, loss = 0.12671240\n",
      "Iteration 215, loss = 0.12642709\n",
      "Iteration 216, loss = 0.12614495\n",
      "Iteration 217, loss = 0.12586591\n",
      "Iteration 218, loss = 0.12558992\n",
      "Iteration 219, loss = 0.12531695\n",
      "Iteration 220, loss = 0.12504693\n",
      "Iteration 221, loss = 0.12477982\n",
      "Iteration 222, loss = 0.12451558\n",
      "Iteration 223, loss = 0.12425415\n",
      "Iteration 224, loss = 0.12399550\n",
      "Iteration 225, loss = 0.12373959\n",
      "Iteration 226, loss = 0.12348635\n",
      "Iteration 227, loss = 0.12323577\n",
      "Iteration 228, loss = 0.12298779\n",
      "Iteration 229, loss = 0.12274238\n",
      "Iteration 230, loss = 0.12249954\n",
      "Iteration 231, loss = 0.12225928\n",
      "Iteration 232, loss = 0.12202164\n",
      "Iteration 233, loss = 0.12178642\n",
      "Iteration 234, loss = 0.12155358\n",
      "Iteration 235, loss = 0.12132308\n",
      "Iteration 236, loss = 0.12109488\n",
      "Iteration 237, loss = 0.12086896\n",
      "Iteration 238, loss = 0.12064533\n",
      "Iteration 239, loss = 0.12042391\n",
      "Iteration 240, loss = 0.12020466\n",
      "Iteration 241, loss = 0.11998756\n",
      "Iteration 242, loss = 0.11977258\n",
      "Iteration 243, loss = 0.11955968\n",
      "Iteration 244, loss = 0.11934884\n",
      "Iteration 245, loss = 0.11914003\n",
      "Iteration 246, loss = 0.11893321\n",
      "Iteration 247, loss = 0.11872837\n",
      "Iteration 248, loss = 0.11852548\n",
      "Iteration 249, loss = 0.11832450\n",
      "Iteration 250, loss = 0.11812541\n",
      "Iteration 251, loss = 0.11792819\n",
      "Iteration 252, loss = 0.11773286\n",
      "Iteration 253, loss = 0.11753941\n",
      "Iteration 254, loss = 0.11734773\n",
      "Iteration 255, loss = 0.11715780\n",
      "Iteration 256, loss = 0.11696961\n",
      "Iteration 257, loss = 0.11678312\n",
      "Iteration 258, loss = 0.11659831\n",
      "Iteration 259, loss = 0.11641517\n",
      "Iteration 260, loss = 0.11623367\n",
      "Iteration 261, loss = 0.11605383\n",
      "Iteration 262, loss = 0.11587560\n",
      "Iteration 263, loss = 0.11569894\n",
      "Iteration 264, loss = 0.11552383\n",
      "Iteration 265, loss = 0.11535025\n",
      "Iteration 266, loss = 0.11517823\n",
      "Iteration 267, loss = 0.11500769\n",
      "Iteration 268, loss = 0.11483862\n",
      "Iteration 269, loss = 0.11467102\n",
      "Iteration 270, loss = 0.11450486\n",
      "Iteration 271, loss = 0.11434013\n",
      "Iteration 272, loss = 0.11417680\n",
      "Iteration 273, loss = 0.11401486\n",
      "Iteration 274, loss = 0.11385429\n",
      "Iteration 275, loss = 0.11369506\n",
      "Iteration 276, loss = 0.11353718\n",
      "Iteration 277, loss = 0.11338060\n",
      "Iteration 278, loss = 0.11322535\n",
      "Iteration 279, loss = 0.11307136\n",
      "Iteration 280, loss = 0.11291864\n",
      "Iteration 281, loss = 0.11276719\n",
      "Iteration 282, loss = 0.11261696\n",
      "Iteration 283, loss = 0.11246797\n",
      "Iteration 284, loss = 0.11232016\n",
      "Iteration 285, loss = 0.11217358\n",
      "Iteration 286, loss = 0.11202815\n",
      "Iteration 287, loss = 0.11188390\n",
      "Iteration 288, loss = 0.11174080\n",
      "Iteration 289, loss = 0.11159883\n",
      "Iteration 290, loss = 0.11145798\n",
      "Iteration 291, loss = 0.11131825\n",
      "Iteration 292, loss = 0.11117961\n",
      "Iteration 293, loss = 0.11104206\n",
      "Iteration 294, loss = 0.11090558\n",
      "Iteration 295, loss = 0.11077016\n",
      "Iteration 296, loss = 0.11063578\n",
      "Iteration 297, loss = 0.11050244\n",
      "Iteration 298, loss = 0.11037012\n",
      "Iteration 299, loss = 0.11023881\n",
      "Iteration 300, loss = 0.11010850\n",
      "Iteration 301, loss = 0.10997918\n",
      "Iteration 302, loss = 0.10985083\n",
      "Iteration 303, loss = 0.10972345\n",
      "Iteration 304, loss = 0.10959703\n",
      "Iteration 305, loss = 0.10947154\n",
      "Iteration 306, loss = 0.10934700\n",
      "Iteration 307, loss = 0.10922337\n",
      "Iteration 308, loss = 0.10910066\n",
      "Iteration 309, loss = 0.10897884\n",
      "Iteration 310, loss = 0.10885793\n",
      "Iteration 311, loss = 0.10873789\n",
      "Iteration 312, loss = 0.10861873\n",
      "Iteration 313, loss = 0.10850043\n",
      "Iteration 314, loss = 0.10838299\n",
      "Iteration 315, loss = 0.10826638\n",
      "Iteration 316, loss = 0.10815062\n",
      "Iteration 317, loss = 0.10803569\n",
      "Iteration 318, loss = 0.10792157\n",
      "Iteration 319, loss = 0.10780826\n",
      "Iteration 320, loss = 0.10769574\n",
      "Iteration 321, loss = 0.10758403\n",
      "Iteration 322, loss = 0.10747310\n",
      "Iteration 323, loss = 0.10736294\n",
      "Iteration 324, loss = 0.10725354\n",
      "Iteration 325, loss = 0.10714491\n",
      "Iteration 326, loss = 0.10703704\n",
      "Iteration 327, loss = 0.10692990\n",
      "Iteration 328, loss = 0.10682350\n",
      "Iteration 329, loss = 0.10671783\n",
      "Iteration 330, loss = 0.10661287\n",
      "Iteration 331, loss = 0.10650865\n",
      "Iteration 332, loss = 0.10640511\n",
      "Iteration 333, loss = 0.10630227\n",
      "Iteration 334, loss = 0.10620013\n",
      "Iteration 335, loss = 0.10609868\n",
      "Iteration 336, loss = 0.10599790\n",
      "Iteration 337, loss = 0.10589780\n",
      "Iteration 338, loss = 0.10579836\n",
      "Iteration 339, loss = 0.10569957\n",
      "Iteration 340, loss = 0.10560144\n",
      "Iteration 341, loss = 0.10550395\n",
      "Iteration 342, loss = 0.10540711\n",
      "Iteration 343, loss = 0.10531089\n",
      "Iteration 344, loss = 0.10521530\n",
      "Iteration 345, loss = 0.10512033\n",
      "Iteration 346, loss = 0.10502598\n",
      "Iteration 347, loss = 0.10493223\n",
      "Iteration 348, loss = 0.10483908\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35303276\n",
      "Iteration 2, loss = 1.03698609\n",
      "Iteration 3, loss = 0.83678868\n",
      "Iteration 4, loss = 0.76876249\n",
      "Iteration 5, loss = 0.75555596\n",
      "Iteration 6, loss = 0.76020450\n",
      "Iteration 7, loss = 0.74463866\n",
      "Iteration 8, loss = 0.70916466\n",
      "Iteration 9, loss = 0.66885924\n",
      "Iteration 10, loss = 0.63352907\n",
      "Iteration 11, loss = 0.60826978\n",
      "Iteration 12, loss = 0.59212185\n",
      "Iteration 13, loss = 0.58020821\n",
      "Iteration 14, loss = 0.56794042\n",
      "Iteration 15, loss = 0.55370861\n",
      "Iteration 16, loss = 0.53826978\n",
      "Iteration 17, loss = 0.52347568\n",
      "Iteration 18, loss = 0.51043793\n",
      "Iteration 19, loss = 0.49935048\n",
      "Iteration 20, loss = 0.48983062\n",
      "Iteration 21, loss = 0.48109183\n",
      "Iteration 22, loss = 0.47271596\n",
      "Iteration 23, loss = 0.46455809\n",
      "Iteration 24, loss = 0.45670749\n",
      "Iteration 25, loss = 0.44928101\n",
      "Iteration 26, loss = 0.44241858\n",
      "Iteration 27, loss = 0.43599578\n",
      "Iteration 28, loss = 0.42984201\n",
      "Iteration 29, loss = 0.42389910\n",
      "Iteration 30, loss = 0.41813139\n",
      "Iteration 31, loss = 0.41253548\n",
      "Iteration 32, loss = 0.40709318\n",
      "Iteration 33, loss = 0.40182855\n",
      "Iteration 34, loss = 0.39674783\n",
      "Iteration 35, loss = 0.39185340\n",
      "Iteration 36, loss = 0.38706150\n",
      "Iteration 37, loss = 0.38236341\n",
      "Iteration 38, loss = 0.37776334\n",
      "Iteration 39, loss = 0.37324906\n",
      "Iteration 40, loss = 0.36881014\n",
      "Iteration 41, loss = 0.36443967\n",
      "Iteration 42, loss = 0.36013179\n",
      "Iteration 43, loss = 0.35588190\n",
      "Iteration 44, loss = 0.35168643\n",
      "Iteration 45, loss = 0.34754249\n",
      "Iteration 46, loss = 0.34344875\n",
      "Iteration 47, loss = 0.33941093\n",
      "Iteration 48, loss = 0.33541951\n",
      "Iteration 49, loss = 0.33147116\n",
      "Iteration 50, loss = 0.32756519\n",
      "Iteration 51, loss = 0.32370119\n",
      "Iteration 52, loss = 0.31987858\n",
      "Iteration 53, loss = 0.31609529\n",
      "Iteration 54, loss = 0.31235612\n",
      "Iteration 55, loss = 0.30866139\n",
      "Iteration 56, loss = 0.30500944\n",
      "Iteration 57, loss = 0.30140014\n",
      "Iteration 58, loss = 0.29783353\n",
      "Iteration 59, loss = 0.29431534\n",
      "Iteration 60, loss = 0.29083823\n",
      "Iteration 61, loss = 0.28740417\n",
      "Iteration 62, loss = 0.28400940\n",
      "Iteration 63, loss = 0.28065307\n",
      "Iteration 64, loss = 0.27733915\n",
      "Iteration 65, loss = 0.27406934\n",
      "Iteration 66, loss = 0.27084108\n",
      "Iteration 67, loss = 0.26765823\n",
      "Iteration 68, loss = 0.26452262\n",
      "Iteration 69, loss = 0.26143624\n",
      "Iteration 70, loss = 0.25839964\n",
      "Iteration 71, loss = 0.25541237\n",
      "Iteration 72, loss = 0.25247404\n",
      "Iteration 73, loss = 0.24958884\n",
      "Iteration 74, loss = 0.24674956\n",
      "Iteration 75, loss = 0.24395503\n",
      "Iteration 76, loss = 0.24120769\n",
      "Iteration 77, loss = 0.23850764\n",
      "Iteration 78, loss = 0.23585078\n",
      "Iteration 79, loss = 0.23323739\n",
      "Iteration 80, loss = 0.23066517\n",
      "Iteration 81, loss = 0.22813666\n",
      "Iteration 82, loss = 0.22566156\n",
      "Iteration 83, loss = 0.22322585\n",
      "Iteration 84, loss = 0.22083288\n",
      "Iteration 85, loss = 0.21848784\n",
      "Iteration 86, loss = 0.21619810\n",
      "Iteration 87, loss = 0.21395247\n",
      "Iteration 88, loss = 0.21175095\n",
      "Iteration 89, loss = 0.20959597\n",
      "Iteration 90, loss = 0.20748646\n",
      "Iteration 91, loss = 0.20541768\n",
      "Iteration 92, loss = 0.20338993\n",
      "Iteration 93, loss = 0.20139963\n",
      "Iteration 94, loss = 0.19944772\n",
      "Iteration 95, loss = 0.19753411\n",
      "Iteration 96, loss = 0.19565739\n",
      "Iteration 97, loss = 0.19381697\n",
      "Iteration 98, loss = 0.19201226\n",
      "Iteration 99, loss = 0.19024287\n",
      "Iteration 100, loss = 0.18850819\n",
      "Iteration 101, loss = 0.18680756\n",
      "Iteration 102, loss = 0.18514032\n",
      "Iteration 103, loss = 0.18350593\n",
      "Iteration 104, loss = 0.18190349\n",
      "Iteration 105, loss = 0.18033247\n",
      "Iteration 106, loss = 0.17879239\n",
      "Iteration 107, loss = 0.17728277\n",
      "Iteration 108, loss = 0.17580537\n",
      "Iteration 109, loss = 0.17435749\n",
      "Iteration 110, loss = 0.17293884\n",
      "Iteration 111, loss = 0.17154788\n",
      "Iteration 112, loss = 0.17018380\n",
      "Iteration 113, loss = 0.16884695\n",
      "Iteration 114, loss = 0.16753590\n",
      "Iteration 115, loss = 0.16625076\n",
      "Iteration 116, loss = 0.16499111\n",
      "Iteration 117, loss = 0.16375622\n",
      "Iteration 118, loss = 0.16254506\n",
      "Iteration 119, loss = 0.16135945\n",
      "Iteration 120, loss = 0.16019510\n",
      "Iteration 121, loss = 0.15905356\n",
      "Iteration 122, loss = 0.15793404\n",
      "Iteration 123, loss = 0.15683517\n",
      "Iteration 124, loss = 0.15575688\n",
      "Iteration 125, loss = 0.15469924\n",
      "Iteration 126, loss = 0.15366130\n",
      "Iteration 127, loss = 0.15264300\n",
      "Iteration 128, loss = 0.15164364\n",
      "Iteration 129, loss = 0.15066272\n",
      "Iteration 130, loss = 0.14969995\n",
      "Iteration 131, loss = 0.14875464\n",
      "Iteration 132, loss = 0.14782654\n",
      "Iteration 133, loss = 0.14691529\n",
      "Iteration 134, loss = 0.14602049\n",
      "Iteration 135, loss = 0.14514191\n",
      "Iteration 136, loss = 0.14427892\n",
      "Iteration 137, loss = 0.14343117\n",
      "Iteration 138, loss = 0.14259855\n",
      "Iteration 139, loss = 0.14178038\n",
      "Iteration 140, loss = 0.14097661\n",
      "Iteration 141, loss = 0.14018673\n",
      "Iteration 142, loss = 0.13941062\n",
      "Iteration 143, loss = 0.13864762\n",
      "Iteration 144, loss = 0.13789794\n",
      "Iteration 145, loss = 0.13716064\n",
      "Iteration 146, loss = 0.13643604\n",
      "Iteration 147, loss = 0.13572339\n",
      "Iteration 148, loss = 0.13502282\n",
      "Iteration 149, loss = 0.13433369\n",
      "Iteration 150, loss = 0.13365595\n",
      "Iteration 151, loss = 0.13298940\n",
      "Iteration 152, loss = 0.13233356\n",
      "Iteration 153, loss = 0.13168835\n",
      "Iteration 154, loss = 0.13105354\n",
      "Iteration 155, loss = 0.13042893\n",
      "Iteration 156, loss = 0.12981414\n",
      "Iteration 157, loss = 0.12920906\n",
      "Iteration 158, loss = 0.12861347\n",
      "Iteration 159, loss = 0.12802720\n",
      "Iteration 160, loss = 0.12744999\n",
      "Iteration 161, loss = 0.12688172\n",
      "Iteration 162, loss = 0.12632208\n",
      "Iteration 163, loss = 0.12577096\n",
      "Iteration 164, loss = 0.12522817\n",
      "Iteration 165, loss = 0.12469364\n",
      "Iteration 166, loss = 0.12416708\n",
      "Iteration 167, loss = 0.12364835\n",
      "Iteration 168, loss = 0.12313725\n",
      "Iteration 169, loss = 0.12263366\n",
      "Iteration 170, loss = 0.12213740\n",
      "Iteration 171, loss = 0.12164833\n",
      "Iteration 172, loss = 0.12116631\n",
      "Iteration 173, loss = 0.12069119\n",
      "Iteration 174, loss = 0.12022282\n",
      "Iteration 175, loss = 0.11976109\n",
      "Iteration 176, loss = 0.11930588\n",
      "Iteration 177, loss = 0.11885699\n",
      "Iteration 178, loss = 0.11841434\n",
      "Iteration 179, loss = 0.11797782\n",
      "Iteration 180, loss = 0.11754726\n",
      "Iteration 181, loss = 0.11712259\n",
      "Iteration 182, loss = 0.11670368\n",
      "Iteration 183, loss = 0.11629040\n",
      "Iteration 184, loss = 0.11588266\n",
      "Iteration 185, loss = 0.11548050\n",
      "Iteration 186, loss = 0.11508372\n",
      "Iteration 187, loss = 0.11469217\n",
      "Iteration 188, loss = 0.11430574\n",
      "Iteration 189, loss = 0.11392433\n",
      "Iteration 190, loss = 0.11354784\n",
      "Iteration 191, loss = 0.11317620\n",
      "Iteration 192, loss = 0.11280929\n",
      "Iteration 193, loss = 0.11244703\n",
      "Iteration 194, loss = 0.11208936\n",
      "Iteration 195, loss = 0.11173616\n",
      "Iteration 196, loss = 0.11138736\n",
      "Iteration 197, loss = 0.11104290\n",
      "Iteration 198, loss = 0.11070267\n",
      "Iteration 199, loss = 0.11036662\n",
      "Iteration 200, loss = 0.11003465\n",
      "Iteration 201, loss = 0.10970672\n",
      "Iteration 202, loss = 0.10938274\n",
      "Iteration 203, loss = 0.10906265\n",
      "Iteration 204, loss = 0.10874637\n",
      "Iteration 205, loss = 0.10843383\n",
      "Iteration 206, loss = 0.10812498\n",
      "Iteration 207, loss = 0.10781975\n",
      "Iteration 208, loss = 0.10751808\n",
      "Iteration 209, loss = 0.10721989\n",
      "Iteration 210, loss = 0.10692516\n",
      "Iteration 211, loss = 0.10663379\n",
      "Iteration 212, loss = 0.10634575\n",
      "Iteration 213, loss = 0.10606097\n",
      "Iteration 214, loss = 0.10577941\n",
      "Iteration 215, loss = 0.10550100\n",
      "Iteration 216, loss = 0.10522570\n",
      "Iteration 217, loss = 0.10495346\n",
      "Iteration 218, loss = 0.10468422\n",
      "Iteration 219, loss = 0.10441800\n",
      "Iteration 220, loss = 0.10415467\n",
      "Iteration 221, loss = 0.10389419\n",
      "Iteration 222, loss = 0.10363652\n",
      "Iteration 223, loss = 0.10338161\n",
      "Iteration 224, loss = 0.10312956\n",
      "Iteration 225, loss = 0.10288018\n",
      "Iteration 226, loss = 0.10263344\n",
      "Iteration 227, loss = 0.10238930\n",
      "Iteration 228, loss = 0.10214789\n",
      "Iteration 229, loss = 0.10190908\n",
      "Iteration 230, loss = 0.10167276\n",
      "Iteration 231, loss = 0.10143889\n",
      "Iteration 232, loss = 0.10120744\n",
      "Iteration 233, loss = 0.10097836\n",
      "Iteration 234, loss = 0.10075165\n",
      "Iteration 235, loss = 0.10052724\n",
      "Iteration 236, loss = 0.10030510\n",
      "Iteration 237, loss = 0.10008519\n",
      "Iteration 238, loss = 0.09986749\n",
      "Iteration 239, loss = 0.09965196\n",
      "Iteration 240, loss = 0.09943857\n",
      "Iteration 241, loss = 0.09922728\n",
      "Iteration 242, loss = 0.09901807\n",
      "Iteration 243, loss = 0.09881091\n",
      "Iteration 244, loss = 0.09860575\n",
      "Iteration 245, loss = 0.09840259\n",
      "Iteration 246, loss = 0.09820139\n",
      "Iteration 247, loss = 0.09800210\n",
      "Iteration 248, loss = 0.09780472\n",
      "Iteration 249, loss = 0.09760923\n",
      "Iteration 250, loss = 0.09741556\n",
      "Iteration 251, loss = 0.09722373\n",
      "Iteration 252, loss = 0.09703369\n",
      "Iteration 253, loss = 0.09684542\n",
      "Iteration 254, loss = 0.09665890\n",
      "Iteration 255, loss = 0.09647410\n",
      "Iteration 256, loss = 0.09629100\n",
      "Iteration 257, loss = 0.09610957\n",
      "Iteration 258, loss = 0.09592979\n",
      "Iteration 259, loss = 0.09575166\n",
      "Iteration 260, loss = 0.09557511\n",
      "Iteration 261, loss = 0.09540016\n",
      "Iteration 262, loss = 0.09522678\n",
      "Iteration 263, loss = 0.09505493\n",
      "Iteration 264, loss = 0.09488475\n",
      "Iteration 265, loss = 0.09471605\n",
      "Iteration 266, loss = 0.09454885\n",
      "Iteration 267, loss = 0.09438312\n",
      "Iteration 268, loss = 0.09421884\n",
      "Iteration 269, loss = 0.09405598\n",
      "Iteration 270, loss = 0.09389453\n",
      "Iteration 271, loss = 0.09373447\n",
      "Iteration 272, loss = 0.09357579\n",
      "Iteration 273, loss = 0.09341846\n",
      "Iteration 274, loss = 0.09326246\n",
      "Iteration 275, loss = 0.09310779\n",
      "Iteration 276, loss = 0.09295442\n",
      "Iteration 277, loss = 0.09280235\n",
      "Iteration 278, loss = 0.09265155\n",
      "Iteration 279, loss = 0.09250200\n",
      "Iteration 280, loss = 0.09235370\n",
      "Iteration 281, loss = 0.09220662\n",
      "Iteration 282, loss = 0.09206075\n",
      "Iteration 283, loss = 0.09191608\n",
      "Iteration 284, loss = 0.09177259\n",
      "Iteration 285, loss = 0.09163026\n",
      "Iteration 286, loss = 0.09148909\n",
      "Iteration 287, loss = 0.09134906\n",
      "Iteration 288, loss = 0.09121015\n",
      "Iteration 289, loss = 0.09107236\n",
      "Iteration 290, loss = 0.09093566\n",
      "Iteration 291, loss = 0.09080005\n",
      "Iteration 292, loss = 0.09066550\n",
      "Iteration 293, loss = 0.09053202\n",
      "Iteration 294, loss = 0.09039959\n",
      "Iteration 295, loss = 0.09026818\n",
      "Iteration 296, loss = 0.09013780\n",
      "Iteration 297, loss = 0.09000844\n",
      "Iteration 298, loss = 0.08988007\n",
      "Iteration 299, loss = 0.08975268\n",
      "Iteration 300, loss = 0.08962627\n",
      "Iteration 301, loss = 0.08950083\n",
      "Iteration 302, loss = 0.08937634\n",
      "Iteration 303, loss = 0.08925279\n",
      "Iteration 304, loss = 0.08913017\n",
      "Iteration 305, loss = 0.08900848\n",
      "Iteration 306, loss = 0.08888769\n",
      "Iteration 307, loss = 0.08876781\n",
      "Iteration 308, loss = 0.08864881\n",
      "Iteration 309, loss = 0.08853070\n",
      "Iteration 310, loss = 0.08841345\n",
      "Iteration 311, loss = 0.08829707\n",
      "Iteration 312, loss = 0.08818154\n",
      "Iteration 313, loss = 0.08806685\n",
      "Iteration 314, loss = 0.08795299\n",
      "Iteration 315, loss = 0.08783995\n",
      "Iteration 316, loss = 0.08772775\n",
      "Iteration 317, loss = 0.08761638\n",
      "Iteration 318, loss = 0.08750579\n",
      "Iteration 319, loss = 0.08739598\n",
      "Iteration 320, loss = 0.08728696\n",
      "Iteration 321, loss = 0.08717871\n",
      "Iteration 322, loss = 0.08707121\n",
      "Iteration 323, loss = 0.08696450\n",
      "Iteration 324, loss = 0.08685851\n",
      "Iteration 325, loss = 0.08675326\n",
      "Iteration 326, loss = 0.08664875\n",
      "Iteration 327, loss = 0.08654496\n",
      "Iteration 328, loss = 0.08644190\n",
      "Iteration 329, loss = 0.08633953\n",
      "Iteration 330, loss = 0.08623787\n",
      "Iteration 331, loss = 0.08613693\n",
      "Iteration 332, loss = 0.08603670\n",
      "Iteration 333, loss = 0.08593715\n",
      "Iteration 334, loss = 0.08583828\n",
      "Iteration 335, loss = 0.08574010\n",
      "Iteration 336, loss = 0.08564256\n",
      "Iteration 337, loss = 0.08554567\n",
      "Iteration 338, loss = 0.08544945\n",
      "Iteration 339, loss = 0.08535386\n",
      "Iteration 340, loss = 0.08525889\n",
      "Iteration 341, loss = 0.08516458\n",
      "Iteration 342, loss = 0.08507087\n",
      "Iteration 343, loss = 0.08497777\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35165820\n",
      "Iteration 2, loss = 1.03698554\n",
      "Iteration 3, loss = 0.83797852\n",
      "Iteration 4, loss = 0.77150908\n",
      "Iteration 5, loss = 0.75843887\n",
      "Iteration 6, loss = 0.76303549\n",
      "Iteration 7, loss = 0.74651920\n",
      "Iteration 8, loss = 0.71049224\n",
      "Iteration 9, loss = 0.67024327\n",
      "Iteration 10, loss = 0.63537479\n",
      "Iteration 11, loss = 0.61091170\n",
      "Iteration 12, loss = 0.59488892\n",
      "Iteration 13, loss = 0.58263562\n",
      "Iteration 14, loss = 0.56994528\n",
      "Iteration 15, loss = 0.55540474\n",
      "Iteration 16, loss = 0.53999877\n",
      "Iteration 17, loss = 0.52511571\n",
      "Iteration 18, loss = 0.51193797\n",
      "Iteration 19, loss = 0.50077064\n",
      "Iteration 20, loss = 0.49106360\n",
      "Iteration 21, loss = 0.48210900\n",
      "Iteration 22, loss = 0.47348623\n",
      "Iteration 23, loss = 0.46509036\n",
      "Iteration 24, loss = 0.45704810\n",
      "Iteration 25, loss = 0.44946481\n",
      "Iteration 26, loss = 0.44238929\n",
      "Iteration 27, loss = 0.43585047\n",
      "Iteration 28, loss = 0.42961737\n",
      "Iteration 29, loss = 0.42357942\n",
      "Iteration 30, loss = 0.41767826\n",
      "Iteration 31, loss = 0.41192492\n",
      "Iteration 32, loss = 0.40633418\n",
      "Iteration 33, loss = 0.40089893\n",
      "Iteration 34, loss = 0.39566768\n",
      "Iteration 35, loss = 0.39066703\n",
      "Iteration 36, loss = 0.38577688\n",
      "Iteration 37, loss = 0.38099908\n",
      "Iteration 38, loss = 0.37633013\n",
      "Iteration 39, loss = 0.37174584\n",
      "Iteration 40, loss = 0.36723980\n",
      "Iteration 41, loss = 0.36280559\n",
      "Iteration 42, loss = 0.35843794\n",
      "Iteration 43, loss = 0.35413155\n",
      "Iteration 44, loss = 0.34988305\n",
      "Iteration 45, loss = 0.34568961\n",
      "Iteration 46, loss = 0.34154895\n",
      "Iteration 47, loss = 0.33745893\n",
      "Iteration 48, loss = 0.33341811\n",
      "Iteration 49, loss = 0.32942501\n",
      "Iteration 50, loss = 0.32547305\n",
      "Iteration 51, loss = 0.32156466\n",
      "Iteration 52, loss = 0.31770228\n",
      "Iteration 53, loss = 0.31388582\n",
      "Iteration 54, loss = 0.31011488\n",
      "Iteration 55, loss = 0.30638786\n",
      "Iteration 56, loss = 0.30270282\n",
      "Iteration 57, loss = 0.29906627\n",
      "Iteration 58, loss = 0.29547033\n",
      "Iteration 59, loss = 0.29191606\n",
      "Iteration 60, loss = 0.28839803\n",
      "Iteration 61, loss = 0.28491618\n",
      "Iteration 62, loss = 0.28148366\n",
      "Iteration 63, loss = 0.27809760\n",
      "Iteration 64, loss = 0.27475780\n",
      "Iteration 65, loss = 0.27147311\n",
      "Iteration 66, loss = 0.26824528\n",
      "Iteration 67, loss = 0.26507324\n",
      "Iteration 68, loss = 0.26195062\n",
      "Iteration 69, loss = 0.25887561\n",
      "Iteration 70, loss = 0.25585194\n",
      "Iteration 71, loss = 0.25287730\n",
      "Iteration 72, loss = 0.24995138\n",
      "Iteration 73, loss = 0.24707056\n",
      "Iteration 74, loss = 0.24423136\n",
      "Iteration 75, loss = 0.24143886\n",
      "Iteration 76, loss = 0.23870235\n",
      "Iteration 77, loss = 0.23602342\n",
      "Iteration 78, loss = 0.23339525\n",
      "Iteration 79, loss = 0.23081296\n",
      "Iteration 80, loss = 0.22827435\n",
      "Iteration 81, loss = 0.22578141\n",
      "Iteration 82, loss = 0.22333312\n",
      "Iteration 83, loss = 0.22092779\n",
      "Iteration 84, loss = 0.21856527\n",
      "Iteration 85, loss = 0.21624497\n",
      "Iteration 86, loss = 0.21396660\n",
      "Iteration 87, loss = 0.21172958\n",
      "Iteration 88, loss = 0.20953426\n",
      "Iteration 89, loss = 0.20738031\n",
      "Iteration 90, loss = 0.20526618\n",
      "Iteration 91, loss = 0.20319136\n",
      "Iteration 92, loss = 0.20115632\n",
      "Iteration 93, loss = 0.19915970\n",
      "Iteration 94, loss = 0.19720008\n",
      "Iteration 95, loss = 0.19527773\n",
      "Iteration 96, loss = 0.19339188\n",
      "Iteration 97, loss = 0.19154204\n",
      "Iteration 98, loss = 0.18972862\n",
      "Iteration 99, loss = 0.18794963\n",
      "Iteration 100, loss = 0.18620451\n",
      "Iteration 101, loss = 0.18449304\n",
      "Iteration 102, loss = 0.18281415\n",
      "Iteration 103, loss = 0.18116757\n",
      "Iteration 104, loss = 0.17955204\n",
      "Iteration 105, loss = 0.17796705\n",
      "Iteration 106, loss = 0.17641203\n",
      "Iteration 107, loss = 0.17488656\n",
      "Iteration 108, loss = 0.17339023\n",
      "Iteration 109, loss = 0.17192215\n",
      "Iteration 110, loss = 0.17048173\n",
      "Iteration 111, loss = 0.16906846\n",
      "Iteration 112, loss = 0.16768157\n",
      "Iteration 113, loss = 0.16632049\n",
      "Iteration 114, loss = 0.16498472\n",
      "Iteration 115, loss = 0.16367386\n",
      "Iteration 116, loss = 0.16238713\n",
      "Iteration 117, loss = 0.16112419\n",
      "Iteration 118, loss = 0.15988424\n",
      "Iteration 119, loss = 0.15866799\n",
      "Iteration 120, loss = 0.15747386\n",
      "Iteration 121, loss = 0.15630132\n",
      "Iteration 122, loss = 0.15515003\n",
      "Iteration 123, loss = 0.15401929\n",
      "Iteration 124, loss = 0.15290888\n",
      "Iteration 125, loss = 0.15181812\n",
      "Iteration 126, loss = 0.15074670\n",
      "Iteration 127, loss = 0.14969414\n",
      "Iteration 128, loss = 0.14866023\n",
      "Iteration 129, loss = 0.14764540\n",
      "Iteration 130, loss = 0.14664838\n",
      "Iteration 131, loss = 0.14566858\n",
      "Iteration 132, loss = 0.14470571\n",
      "Iteration 133, loss = 0.14375921\n",
      "Iteration 134, loss = 0.14282897\n",
      "Iteration 135, loss = 0.14191436\n",
      "Iteration 136, loss = 0.14101527\n",
      "Iteration 137, loss = 0.14013112\n",
      "Iteration 138, loss = 0.13926176\n",
      "Iteration 139, loss = 0.13840674\n",
      "Iteration 140, loss = 0.13756575\n",
      "Iteration 141, loss = 0.13673858\n",
      "Iteration 142, loss = 0.13592524\n",
      "Iteration 143, loss = 0.13512518\n",
      "Iteration 144, loss = 0.13433800\n",
      "Iteration 145, loss = 0.13356341\n",
      "Iteration 146, loss = 0.13280111\n",
      "Iteration 147, loss = 0.13205081\n",
      "Iteration 148, loss = 0.13131228\n",
      "Iteration 149, loss = 0.13058526\n",
      "Iteration 150, loss = 0.12986951\n",
      "Iteration 151, loss = 0.12916476\n",
      "Iteration 152, loss = 0.12847079\n",
      "Iteration 153, loss = 0.12778737\n",
      "Iteration 154, loss = 0.12711430\n",
      "Iteration 155, loss = 0.12645137\n",
      "Iteration 156, loss = 0.12579841\n",
      "Iteration 157, loss = 0.12515512\n",
      "Iteration 158, loss = 0.12452129\n",
      "Iteration 159, loss = 0.12389675\n",
      "Iteration 160, loss = 0.12328121\n",
      "Iteration 161, loss = 0.12267459\n",
      "Iteration 162, loss = 0.12207671\n",
      "Iteration 163, loss = 0.12148744\n",
      "Iteration 164, loss = 0.12090650\n",
      "Iteration 165, loss = 0.12033421\n",
      "Iteration 166, loss = 0.11977037\n",
      "Iteration 167, loss = 0.11921448\n",
      "Iteration 168, loss = 0.11866633\n",
      "Iteration 169, loss = 0.11812581\n",
      "Iteration 170, loss = 0.11759272\n",
      "Iteration 171, loss = 0.11706692\n",
      "Iteration 172, loss = 0.11654826\n",
      "Iteration 173, loss = 0.11603661\n",
      "Iteration 174, loss = 0.11553184\n",
      "Iteration 175, loss = 0.11503384\n",
      "Iteration 176, loss = 0.11454254\n",
      "Iteration 177, loss = 0.11405761\n",
      "Iteration 178, loss = 0.11357907\n",
      "Iteration 179, loss = 0.11310675\n",
      "Iteration 180, loss = 0.11264062\n",
      "Iteration 181, loss = 0.11218043\n",
      "Iteration 182, loss = 0.11172615\n",
      "Iteration 183, loss = 0.11127769\n",
      "Iteration 184, loss = 0.11083487\n",
      "Iteration 185, loss = 0.11039765\n",
      "Iteration 186, loss = 0.10996590\n",
      "Iteration 187, loss = 0.10953950\n",
      "Iteration 188, loss = 0.10911837\n",
      "Iteration 189, loss = 0.10870246\n",
      "Iteration 190, loss = 0.10829158\n",
      "Iteration 191, loss = 0.10788572\n",
      "Iteration 192, loss = 0.10748475\n",
      "Iteration 193, loss = 0.10708858\n",
      "Iteration 194, loss = 0.10669719\n",
      "Iteration 195, loss = 0.10631048\n",
      "Iteration 196, loss = 0.10592833\n",
      "Iteration 197, loss = 0.10555069\n",
      "Iteration 198, loss = 0.10517754\n",
      "Iteration 199, loss = 0.10480870\n",
      "Iteration 200, loss = 0.10444410\n",
      "Iteration 201, loss = 0.10408372\n",
      "Iteration 202, loss = 0.10372747\n",
      "Iteration 203, loss = 0.10337529\n",
      "Iteration 204, loss = 0.10302707\n",
      "Iteration 205, loss = 0.10268273\n",
      "Iteration 206, loss = 0.10234233\n",
      "Iteration 207, loss = 0.10200566\n",
      "Iteration 208, loss = 0.10167272\n",
      "Iteration 209, loss = 0.10134345\n",
      "Iteration 210, loss = 0.10101782\n",
      "Iteration 211, loss = 0.10069591\n",
      "Iteration 212, loss = 0.10037749\n",
      "Iteration 213, loss = 0.10006257\n",
      "Iteration 214, loss = 0.09975099\n",
      "Iteration 215, loss = 0.09944283\n",
      "Iteration 216, loss = 0.09913787\n",
      "Iteration 217, loss = 0.09883617\n",
      "Iteration 218, loss = 0.09853762\n",
      "Iteration 219, loss = 0.09824224\n",
      "Iteration 220, loss = 0.09794989\n",
      "Iteration 221, loss = 0.09766064\n",
      "Iteration 222, loss = 0.09737431\n",
      "Iteration 223, loss = 0.09709096\n",
      "Iteration 224, loss = 0.09681047\n",
      "Iteration 225, loss = 0.09653286\n",
      "Iteration 226, loss = 0.09625804\n",
      "Iteration 227, loss = 0.09598604\n",
      "Iteration 228, loss = 0.09571677\n",
      "Iteration 229, loss = 0.09545018\n",
      "Iteration 230, loss = 0.09518620\n",
      "Iteration 231, loss = 0.09492489\n",
      "Iteration 232, loss = 0.09466607\n",
      "Iteration 233, loss = 0.09440988\n",
      "Iteration 234, loss = 0.09415608\n",
      "Iteration 235, loss = 0.09390481\n",
      "Iteration 236, loss = 0.09365593\n",
      "Iteration 237, loss = 0.09340940\n",
      "Iteration 238, loss = 0.09316530\n",
      "Iteration 239, loss = 0.09292346\n",
      "Iteration 240, loss = 0.09268395\n",
      "Iteration 241, loss = 0.09244665\n",
      "Iteration 242, loss = 0.09221161\n",
      "Iteration 243, loss = 0.09197875\n",
      "Iteration 244, loss = 0.09174803\n",
      "Iteration 245, loss = 0.09151957\n",
      "Iteration 246, loss = 0.09129326\n",
      "Iteration 247, loss = 0.09106903\n",
      "Iteration 248, loss = 0.09084689\n",
      "Iteration 249, loss = 0.09062676\n",
      "Iteration 250, loss = 0.09040861\n",
      "Iteration 251, loss = 0.09019249\n",
      "Iteration 252, loss = 0.08997833\n",
      "Iteration 253, loss = 0.08976607\n",
      "Iteration 254, loss = 0.08955570\n",
      "Iteration 255, loss = 0.08934721\n",
      "Iteration 256, loss = 0.08914054\n",
      "Iteration 257, loss = 0.08893568\n",
      "Iteration 258, loss = 0.08873261\n",
      "Iteration 259, loss = 0.08853134\n",
      "Iteration 260, loss = 0.08833179\n",
      "Iteration 261, loss = 0.08813397\n",
      "Iteration 262, loss = 0.08793785\n",
      "Iteration 263, loss = 0.08774344\n",
      "Iteration 264, loss = 0.08755067\n",
      "Iteration 265, loss = 0.08735954\n",
      "Iteration 266, loss = 0.08717006\n",
      "Iteration 267, loss = 0.08698213\n",
      "Iteration 268, loss = 0.08679588\n",
      "Iteration 269, loss = 0.08661108\n",
      "Iteration 270, loss = 0.08642786\n",
      "Iteration 271, loss = 0.08624615\n",
      "Iteration 272, loss = 0.08606598\n",
      "Iteration 273, loss = 0.08588725\n",
      "Iteration 274, loss = 0.08571001\n",
      "Iteration 275, loss = 0.08553421\n",
      "Iteration 276, loss = 0.08535983\n",
      "Iteration 277, loss = 0.08518687\n",
      "Iteration 278, loss = 0.08501532\n",
      "Iteration 279, loss = 0.08484514\n",
      "Iteration 280, loss = 0.08467632\n",
      "Iteration 281, loss = 0.08450885\n",
      "Iteration 282, loss = 0.08434271\n",
      "Iteration 283, loss = 0.08417787\n",
      "Iteration 284, loss = 0.08401434\n",
      "Iteration 285, loss = 0.08385209\n",
      "Iteration 286, loss = 0.08369114\n",
      "Iteration 287, loss = 0.08353140\n",
      "Iteration 288, loss = 0.08337292\n",
      "Iteration 289, loss = 0.08321566\n",
      "Iteration 290, loss = 0.08305962\n",
      "Iteration 291, loss = 0.08290477\n",
      "Iteration 292, loss = 0.08275111\n",
      "Iteration 293, loss = 0.08259862\n",
      "Iteration 294, loss = 0.08244729\n",
      "Iteration 295, loss = 0.08229710\n",
      "Iteration 296, loss = 0.08214805\n",
      "Iteration 297, loss = 0.08200011\n",
      "Iteration 298, loss = 0.08185328\n",
      "Iteration 299, loss = 0.08170755\n",
      "Iteration 300, loss = 0.08156289\n",
      "Iteration 301, loss = 0.08141930\n",
      "Iteration 302, loss = 0.08127677\n",
      "Iteration 303, loss = 0.08113529\n",
      "Iteration 304, loss = 0.08099484\n",
      "Iteration 305, loss = 0.08085541\n",
      "Iteration 306, loss = 0.08071700\n",
      "Iteration 307, loss = 0.08057959\n",
      "Iteration 308, loss = 0.08044317\n",
      "Iteration 309, loss = 0.08030772\n",
      "Iteration 310, loss = 0.08017325\n",
      "Iteration 311, loss = 0.08003974\n",
      "Iteration 312, loss = 0.07990718\n",
      "Iteration 313, loss = 0.07977556\n",
      "Iteration 314, loss = 0.07964487\n",
      "Iteration 315, loss = 0.07951509\n",
      "Iteration 316, loss = 0.07938623\n",
      "Iteration 317, loss = 0.07925826\n",
      "Iteration 318, loss = 0.07913119\n",
      "Iteration 319, loss = 0.07900500\n",
      "Iteration 320, loss = 0.07887969\n",
      "Iteration 321, loss = 0.07875524\n",
      "Iteration 322, loss = 0.07863165\n",
      "Iteration 323, loss = 0.07850890\n",
      "Iteration 324, loss = 0.07838699\n",
      "Iteration 325, loss = 0.07826591\n",
      "Iteration 326, loss = 0.07814565\n",
      "Iteration 327, loss = 0.07802621\n",
      "Iteration 328, loss = 0.07790757\n",
      "Iteration 329, loss = 0.07778972\n",
      "Iteration 330, loss = 0.07767267\n",
      "Iteration 331, loss = 0.07755639\n",
      "Iteration 332, loss = 0.07744089\n",
      "Iteration 333, loss = 0.07732615\n",
      "Iteration 334, loss = 0.07721217\n",
      "Iteration 335, loss = 0.07709894\n",
      "Iteration 336, loss = 0.07698645\n",
      "Iteration 337, loss = 0.07687470\n",
      "Iteration 338, loss = 0.07676368\n",
      "Iteration 339, loss = 0.07665338\n",
      "Iteration 340, loss = 0.07654379\n",
      "Iteration 341, loss = 0.07643491\n",
      "Iteration 342, loss = 0.07632674\n",
      "Iteration 343, loss = 0.07621925\n",
      "Iteration 344, loss = 0.07611246\n",
      "Iteration 345, loss = 0.07600635\n",
      "Iteration 346, loss = 0.07590091\n",
      "Iteration 347, loss = 0.07579615\n",
      "Iteration 348, loss = 0.07569204\n",
      "Iteration 349, loss = 0.07558859\n",
      "Iteration 350, loss = 0.07548579\n",
      "Iteration 351, loss = 0.07538363\n",
      "Iteration 352, loss = 0.07528211\n",
      "Iteration 353, loss = 0.07518122\n",
      "Iteration 354, loss = 0.07508095\n",
      "Iteration 355, loss = 0.07498132\n",
      "Iteration 356, loss = 0.07488232\n",
      "Iteration 357, loss = 0.07478393\n",
      "Iteration 358, loss = 0.07468615\n",
      "Iteration 359, loss = 0.07458896\n",
      "Iteration 360, loss = 0.07449236\n",
      "Iteration 361, loss = 0.07439636\n",
      "Iteration 362, loss = 0.07430093\n",
      "Iteration 363, loss = 0.07420608\n",
      "Iteration 364, loss = 0.07411181\n",
      "Iteration 365, loss = 0.07401810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35771515\n",
      "Iteration 2, loss = 1.03402933\n",
      "Iteration 3, loss = 0.83141386\n",
      "Iteration 4, loss = 0.76622790\n",
      "Iteration 5, loss = 0.75630545\n",
      "Iteration 6, loss = 0.76115373\n",
      "Iteration 7, loss = 0.74386663\n",
      "Iteration 8, loss = 0.70644493\n",
      "Iteration 9, loss = 0.66535736\n",
      "Iteration 10, loss = 0.63035475\n",
      "Iteration 11, loss = 0.60626673\n",
      "Iteration 12, loss = 0.59095265\n",
      "Iteration 13, loss = 0.57916585\n",
      "Iteration 14, loss = 0.56682602\n",
      "Iteration 15, loss = 0.55254506\n",
      "Iteration 16, loss = 0.53728625\n",
      "Iteration 17, loss = 0.52267212\n",
      "Iteration 18, loss = 0.50990656\n",
      "Iteration 19, loss = 0.49918605\n",
      "Iteration 20, loss = 0.48989954\n",
      "Iteration 21, loss = 0.48133570\n",
      "Iteration 22, loss = 0.47313011\n",
      "Iteration 23, loss = 0.46516567\n",
      "Iteration 24, loss = 0.45754044\n",
      "Iteration 25, loss = 0.45033470\n",
      "Iteration 26, loss = 0.44362695\n",
      "Iteration 27, loss = 0.43736216\n",
      "Iteration 28, loss = 0.43147229\n",
      "Iteration 29, loss = 0.42577095\n",
      "Iteration 30, loss = 0.42020625\n",
      "Iteration 31, loss = 0.41478350\n",
      "Iteration 32, loss = 0.40955710\n",
      "Iteration 33, loss = 0.40453280\n",
      "Iteration 34, loss = 0.39965984\n",
      "Iteration 35, loss = 0.39490702\n",
      "Iteration 36, loss = 0.39032585\n",
      "Iteration 37, loss = 0.38586619\n",
      "Iteration 38, loss = 0.38149576\n",
      "Iteration 39, loss = 0.37721091\n",
      "Iteration 40, loss = 0.37300558\n",
      "Iteration 41, loss = 0.36887278\n",
      "Iteration 42, loss = 0.36480667\n",
      "Iteration 43, loss = 0.36081075\n",
      "Iteration 44, loss = 0.35687243\n",
      "Iteration 45, loss = 0.35299440\n",
      "Iteration 46, loss = 0.34916909\n",
      "Iteration 47, loss = 0.34539207\n",
      "Iteration 48, loss = 0.34166286\n",
      "Iteration 49, loss = 0.33797659\n",
      "Iteration 50, loss = 0.33433178\n",
      "Iteration 51, loss = 0.33073118\n",
      "Iteration 52, loss = 0.32717436\n",
      "Iteration 53, loss = 0.32366103\n",
      "Iteration 54, loss = 0.32019165\n",
      "Iteration 55, loss = 0.31676386\n",
      "Iteration 56, loss = 0.31337811\n",
      "Iteration 57, loss = 0.31003699\n",
      "Iteration 58, loss = 0.30673252\n",
      "Iteration 59, loss = 0.30347030\n",
      "Iteration 60, loss = 0.30024919\n",
      "Iteration 61, loss = 0.29706497\n",
      "Iteration 62, loss = 0.29392421\n",
      "Iteration 63, loss = 0.29082979\n",
      "Iteration 64, loss = 0.28778355\n",
      "Iteration 65, loss = 0.28478474\n",
      "Iteration 66, loss = 0.28183421\n",
      "Iteration 67, loss = 0.27893058\n",
      "Iteration 68, loss = 0.27607126\n",
      "Iteration 69, loss = 0.27325937\n",
      "Iteration 70, loss = 0.27049055\n",
      "Iteration 71, loss = 0.26776185\n",
      "Iteration 72, loss = 0.26507818\n",
      "Iteration 73, loss = 0.26243580\n",
      "Iteration 74, loss = 0.25984160\n",
      "Iteration 75, loss = 0.25728924\n",
      "Iteration 76, loss = 0.25478112\n",
      "Iteration 77, loss = 0.25231708\n",
      "Iteration 78, loss = 0.24989877\n",
      "Iteration 79, loss = 0.24752696\n",
      "Iteration 80, loss = 0.24519679\n",
      "Iteration 81, loss = 0.24290930\n",
      "Iteration 82, loss = 0.24066682\n",
      "Iteration 83, loss = 0.23846769\n",
      "Iteration 84, loss = 0.23631190\n",
      "Iteration 85, loss = 0.23419498\n",
      "Iteration 86, loss = 0.23211947\n",
      "Iteration 87, loss = 0.23008181\n",
      "Iteration 88, loss = 0.22808270\n",
      "Iteration 89, loss = 0.22612141\n",
      "Iteration 90, loss = 0.22419693\n",
      "Iteration 91, loss = 0.22230916\n",
      "Iteration 92, loss = 0.22045763\n",
      "Iteration 93, loss = 0.21864168\n",
      "Iteration 94, loss = 0.21686145\n",
      "Iteration 95, loss = 0.21511540\n",
      "Iteration 96, loss = 0.21340342\n",
      "Iteration 97, loss = 0.21172449\n",
      "Iteration 98, loss = 0.21007890\n",
      "Iteration 99, loss = 0.20846560\n",
      "Iteration 100, loss = 0.20688352\n",
      "Iteration 101, loss = 0.20533255\n",
      "Iteration 102, loss = 0.20381257\n",
      "Iteration 103, loss = 0.20232190\n",
      "Iteration 104, loss = 0.20086074\n",
      "Iteration 105, loss = 0.19942783\n",
      "Iteration 106, loss = 0.19802304\n",
      "Iteration 107, loss = 0.19664552\n",
      "Iteration 108, loss = 0.19529484\n",
      "Iteration 109, loss = 0.19397039\n",
      "Iteration 110, loss = 0.19267169\n",
      "Iteration 111, loss = 0.19139812\n",
      "Iteration 112, loss = 0.19014910\n",
      "Iteration 113, loss = 0.18892418\n",
      "Iteration 114, loss = 0.18772268\n",
      "Iteration 115, loss = 0.18654433\n",
      "Iteration 116, loss = 0.18538827\n",
      "Iteration 117, loss = 0.18425460\n",
      "Iteration 118, loss = 0.18314216\n",
      "Iteration 119, loss = 0.18205099\n",
      "Iteration 120, loss = 0.18098017\n",
      "Iteration 121, loss = 0.17992957\n",
      "Iteration 122, loss = 0.17889824\n",
      "Iteration 123, loss = 0.17788630\n",
      "Iteration 124, loss = 0.17689301\n",
      "Iteration 125, loss = 0.17591821\n",
      "Iteration 126, loss = 0.17496113\n",
      "Iteration 127, loss = 0.17402152\n",
      "Iteration 128, loss = 0.17309904\n",
      "Iteration 129, loss = 0.17219318\n",
      "Iteration 130, loss = 0.17130394\n",
      "Iteration 131, loss = 0.17043051\n",
      "Iteration 132, loss = 0.16957295\n",
      "Iteration 133, loss = 0.16873047\n",
      "Iteration 134, loss = 0.16790290\n",
      "Iteration 135, loss = 0.16708997\n",
      "Iteration 136, loss = 0.16629117\n",
      "Iteration 137, loss = 0.16550629\n",
      "Iteration 138, loss = 0.16473491\n",
      "Iteration 139, loss = 0.16397699\n",
      "Iteration 140, loss = 0.16323199\n",
      "Iteration 141, loss = 0.16249973\n",
      "Iteration 142, loss = 0.16177994\n",
      "Iteration 143, loss = 0.16107223\n",
      "Iteration 144, loss = 0.16037647\n",
      "Iteration 145, loss = 0.15969226\n",
      "Iteration 146, loss = 0.15901939\n",
      "Iteration 147, loss = 0.15835774\n",
      "Iteration 148, loss = 0.15770684\n",
      "Iteration 149, loss = 0.15706663\n",
      "Iteration 150, loss = 0.15643688\n",
      "Iteration 151, loss = 0.15581722\n",
      "Iteration 152, loss = 0.15520754\n",
      "Iteration 153, loss = 0.15460761\n",
      "Iteration 154, loss = 0.15401718\n",
      "Iteration 155, loss = 0.15343609\n",
      "Iteration 156, loss = 0.15286418\n",
      "Iteration 157, loss = 0.15230116\n",
      "Iteration 158, loss = 0.15174691\n",
      "Iteration 159, loss = 0.15120160\n",
      "Iteration 160, loss = 0.15066543\n",
      "Iteration 161, loss = 0.15013762\n",
      "Iteration 162, loss = 0.14961789\n",
      "Iteration 163, loss = 0.14910603\n",
      "Iteration 164, loss = 0.14860188\n",
      "Iteration 165, loss = 0.14810529\n",
      "Iteration 166, loss = 0.14761612\n",
      "Iteration 167, loss = 0.14713420\n",
      "Iteration 168, loss = 0.14665937\n",
      "Iteration 169, loss = 0.14619150\n",
      "Iteration 170, loss = 0.14573043\n",
      "Iteration 171, loss = 0.14527603\n",
      "Iteration 172, loss = 0.14482818\n",
      "Iteration 173, loss = 0.14438672\n",
      "Iteration 174, loss = 0.14395154\n",
      "Iteration 175, loss = 0.14352251\n",
      "Iteration 176, loss = 0.14309950\n",
      "Iteration 177, loss = 0.14268240\n",
      "Iteration 178, loss = 0.14227109\n",
      "Iteration 179, loss = 0.14186546\n",
      "Iteration 180, loss = 0.14146539\n",
      "Iteration 181, loss = 0.14107078\n",
      "Iteration 182, loss = 0.14068152\n",
      "Iteration 183, loss = 0.14029750\n",
      "Iteration 184, loss = 0.13991864\n",
      "Iteration 185, loss = 0.13954507\n",
      "Iteration 186, loss = 0.13917654\n",
      "Iteration 187, loss = 0.13881292\n",
      "Iteration 188, loss = 0.13845411\n",
      "Iteration 189, loss = 0.13809997\n",
      "Iteration 190, loss = 0.13775058\n",
      "Iteration 191, loss = 0.13740553\n",
      "Iteration 192, loss = 0.13706488\n",
      "Iteration 193, loss = 0.13672859\n",
      "Iteration 194, loss = 0.13639654\n",
      "Iteration 195, loss = 0.13606873\n",
      "Iteration 196, loss = 0.13574504\n",
      "Iteration 197, loss = 0.13542544\n",
      "Iteration 198, loss = 0.13510981\n",
      "Iteration 199, loss = 0.13479807\n",
      "Iteration 200, loss = 0.13449018\n",
      "Iteration 201, loss = 0.13418605\n",
      "Iteration 202, loss = 0.13388562\n",
      "Iteration 203, loss = 0.13358881\n",
      "Iteration 204, loss = 0.13329560\n",
      "Iteration 205, loss = 0.13300592\n",
      "Iteration 206, loss = 0.13271966\n",
      "Iteration 207, loss = 0.13243677\n",
      "Iteration 208, loss = 0.13215725\n",
      "Iteration 209, loss = 0.13188099\n",
      "Iteration 210, loss = 0.13160791\n",
      "Iteration 211, loss = 0.13133809\n",
      "Iteration 212, loss = 0.13107128\n",
      "Iteration 213, loss = 0.13080769\n",
      "Iteration 214, loss = 0.13054717\n",
      "Iteration 215, loss = 0.13028955\n",
      "Iteration 216, loss = 0.13003490\n",
      "Iteration 217, loss = 0.12978305\n",
      "Iteration 218, loss = 0.12953407\n",
      "Iteration 219, loss = 0.12928785\n",
      "Iteration 220, loss = 0.12904433\n",
      "Iteration 221, loss = 0.12880350\n",
      "Iteration 222, loss = 0.12856535\n",
      "Iteration 223, loss = 0.12832973\n",
      "Iteration 224, loss = 0.12809669\n",
      "Iteration 225, loss = 0.12786614\n",
      "Iteration 226, loss = 0.12763811\n",
      "Iteration 227, loss = 0.12741246\n",
      "Iteration 228, loss = 0.12718921\n",
      "Iteration 229, loss = 0.12696832\n",
      "Iteration 230, loss = 0.12674975\n",
      "Iteration 231, loss = 0.12653348\n",
      "Iteration 232, loss = 0.12631943\n",
      "Iteration 233, loss = 0.12610760\n",
      "Iteration 234, loss = 0.12589794\n",
      "Iteration 235, loss = 0.12569043\n",
      "Iteration 236, loss = 0.12548503\n",
      "Iteration 237, loss = 0.12528171\n",
      "Iteration 238, loss = 0.12508044\n",
      "Iteration 239, loss = 0.12488118\n",
      "Iteration 240, loss = 0.12468392\n",
      "Iteration 241, loss = 0.12448860\n",
      "Iteration 242, loss = 0.12429522\n",
      "Iteration 243, loss = 0.12410373\n",
      "Iteration 244, loss = 0.12391413\n",
      "Iteration 245, loss = 0.12372635\n",
      "Iteration 246, loss = 0.12354040\n",
      "Iteration 247, loss = 0.12335624\n",
      "Iteration 248, loss = 0.12317384\n",
      "Iteration 249, loss = 0.12299317\n",
      "Iteration 250, loss = 0.12281422\n",
      "Iteration 251, loss = 0.12263696\n",
      "Iteration 252, loss = 0.12246137\n",
      "Iteration 253, loss = 0.12228740\n",
      "Iteration 254, loss = 0.12211507\n",
      "Iteration 255, loss = 0.12194432\n",
      "Iteration 256, loss = 0.12177514\n",
      "Iteration 257, loss = 0.12160752\n",
      "Iteration 258, loss = 0.12144142\n",
      "Iteration 259, loss = 0.12127683\n",
      "Iteration 260, loss = 0.12111372\n",
      "Iteration 261, loss = 0.12095208\n",
      "Iteration 262, loss = 0.12079188\n",
      "Iteration 263, loss = 0.12063314\n",
      "Iteration 264, loss = 0.12047586\n",
      "Iteration 265, loss = 0.12031996\n",
      "Iteration 266, loss = 0.12016543\n",
      "Iteration 267, loss = 0.12001225\n",
      "Iteration 268, loss = 0.11986040\n",
      "Iteration 269, loss = 0.11970986\n",
      "Iteration 270, loss = 0.11956062\n",
      "Iteration 271, loss = 0.11941266\n",
      "Iteration 272, loss = 0.11926596\n",
      "Iteration 273, loss = 0.11912050\n",
      "Iteration 274, loss = 0.11897628\n",
      "Iteration 275, loss = 0.11883326\n",
      "Iteration 276, loss = 0.11869144\n",
      "Iteration 277, loss = 0.11855080\n",
      "Iteration 278, loss = 0.11841133\n",
      "Iteration 279, loss = 0.11827302\n",
      "Iteration 280, loss = 0.11813586\n",
      "Iteration 281, loss = 0.11799982\n",
      "Iteration 282, loss = 0.11786488\n",
      "Iteration 283, loss = 0.11773104\n",
      "Iteration 284, loss = 0.11759827\n",
      "Iteration 285, loss = 0.11746656\n",
      "Iteration 286, loss = 0.11733592\n",
      "Iteration 287, loss = 0.11720630\n",
      "Iteration 288, loss = 0.11707772\n",
      "Iteration 289, loss = 0.11695015\n",
      "Iteration 290, loss = 0.11682359\n",
      "Iteration 291, loss = 0.11669802\n",
      "Iteration 292, loss = 0.11657343\n",
      "Iteration 293, loss = 0.11644979\n",
      "Iteration 294, loss = 0.11632712\n",
      "Iteration 295, loss = 0.11620539\n",
      "Iteration 296, loss = 0.11608460\n",
      "Iteration 297, loss = 0.11596472\n",
      "Iteration 298, loss = 0.11584575\n",
      "Iteration 299, loss = 0.11572768\n",
      "Iteration 300, loss = 0.11561050\n",
      "Iteration 301, loss = 0.11549420\n",
      "Iteration 302, loss = 0.11537876\n",
      "Iteration 303, loss = 0.11526418\n",
      "Iteration 304, loss = 0.11515045\n",
      "Iteration 305, loss = 0.11503756\n",
      "Iteration 306, loss = 0.11492549\n",
      "Iteration 307, loss = 0.11481425\n",
      "Iteration 308, loss = 0.11470381\n",
      "Iteration 309, loss = 0.11459418\n",
      "Iteration 310, loss = 0.11448533\n",
      "Iteration 311, loss = 0.11437727\n",
      "Iteration 312, loss = 0.11426998\n",
      "Iteration 313, loss = 0.11416345\n",
      "Iteration 314, loss = 0.11405768\n",
      "Iteration 315, loss = 0.11395266\n",
      "Iteration 316, loss = 0.11384837\n",
      "Iteration 317, loss = 0.11374482\n",
      "Iteration 318, loss = 0.11364199\n",
      "Iteration 319, loss = 0.11353987\n",
      "Iteration 320, loss = 0.11343846\n",
      "Iteration 321, loss = 0.11333775\n",
      "Iteration 322, loss = 0.11323773\n",
      "Iteration 323, loss = 0.11313840\n",
      "Iteration 324, loss = 0.11303975\n",
      "Iteration 325, loss = 0.11294177\n",
      "Iteration 326, loss = 0.11284445\n",
      "Iteration 327, loss = 0.11274782\n",
      "Iteration 328, loss = 0.11265187\n",
      "Iteration 329, loss = 0.11255657\n",
      "Iteration 330, loss = 0.11246190\n",
      "Iteration 331, loss = 0.11236787\n",
      "Iteration 332, loss = 0.11227445\n",
      "Iteration 333, loss = 0.11218165\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35721204\n",
      "Iteration 2, loss = 1.33009489\n",
      "Iteration 3, loss = 1.30346572\n",
      "Iteration 4, loss = 1.27732700\n",
      "Iteration 5, loss = 1.25170132\n",
      "Iteration 6, loss = 1.22666135\n",
      "Iteration 7, loss = 1.20223979\n",
      "Iteration 8, loss = 1.17843107\n",
      "Iteration 9, loss = 1.15515594\n",
      "Iteration 10, loss = 1.13244415\n",
      "Iteration 11, loss = 1.11038284\n",
      "Iteration 12, loss = 1.08898304\n",
      "Iteration 13, loss = 1.06829344\n",
      "Iteration 14, loss = 1.04828722\n",
      "Iteration 15, loss = 1.02900234\n",
      "Iteration 16, loss = 1.01043707\n",
      "Iteration 17, loss = 0.99260728\n",
      "Iteration 18, loss = 0.97547184\n",
      "Iteration 19, loss = 0.95903481\n",
      "Iteration 20, loss = 0.94330243\n",
      "Iteration 21, loss = 0.92826029\n",
      "Iteration 22, loss = 0.91390221\n",
      "Iteration 23, loss = 0.90022123\n",
      "Iteration 24, loss = 0.88724220\n",
      "Iteration 25, loss = 0.87494733\n",
      "Iteration 26, loss = 0.86335129\n",
      "Iteration 27, loss = 0.85238961\n",
      "Iteration 28, loss = 0.84207495\n",
      "Iteration 29, loss = 0.83235934\n",
      "Iteration 30, loss = 0.82323805\n",
      "Iteration 31, loss = 0.81468567\n",
      "Iteration 32, loss = 0.80664261\n",
      "Iteration 33, loss = 0.79911997\n",
      "Iteration 34, loss = 0.79212227\n",
      "Iteration 35, loss = 0.78562358\n",
      "Iteration 36, loss = 0.77959123\n",
      "Iteration 37, loss = 0.77398808\n",
      "Iteration 38, loss = 0.76879598\n",
      "Iteration 39, loss = 0.76395967\n",
      "Iteration 40, loss = 0.75951701\n",
      "Iteration 41, loss = 0.75548287\n",
      "Iteration 42, loss = 0.75171875\n",
      "Iteration 43, loss = 0.74823026\n",
      "Iteration 44, loss = 0.74491026\n",
      "Iteration 45, loss = 0.74187379\n",
      "Iteration 46, loss = 0.73901306\n",
      "Iteration 47, loss = 0.73623077\n",
      "Iteration 48, loss = 0.73352461\n",
      "Iteration 49, loss = 0.73089149\n",
      "Iteration 50, loss = 0.72830036\n",
      "Iteration 51, loss = 0.72573704\n",
      "Iteration 52, loss = 0.72319944\n",
      "Iteration 53, loss = 0.72068324\n",
      "Iteration 54, loss = 0.71814857\n",
      "Iteration 55, loss = 0.71559897\n",
      "Iteration 56, loss = 0.71303736\n",
      "Iteration 57, loss = 0.71047106\n",
      "Iteration 58, loss = 0.70789241\n",
      "Iteration 59, loss = 0.70529200\n",
      "Iteration 60, loss = 0.70265787\n",
      "Iteration 61, loss = 0.70002010\n",
      "Iteration 62, loss = 0.69736787\n",
      "Iteration 63, loss = 0.69469631\n",
      "Iteration 64, loss = 0.69199054\n",
      "Iteration 65, loss = 0.68921282\n",
      "Iteration 66, loss = 0.68634574\n",
      "Iteration 67, loss = 0.68343410\n",
      "Iteration 68, loss = 0.68052875\n",
      "Iteration 69, loss = 0.67763173\n",
      "Iteration 70, loss = 0.67471576\n",
      "Iteration 71, loss = 0.67182971\n",
      "Iteration 72, loss = 0.66895361\n",
      "Iteration 73, loss = 0.66611459\n",
      "Iteration 74, loss = 0.66335415\n",
      "Iteration 75, loss = 0.66064013\n",
      "Iteration 76, loss = 0.65800497\n",
      "Iteration 77, loss = 0.65538176\n",
      "Iteration 78, loss = 0.65278038\n",
      "Iteration 79, loss = 0.65017749\n",
      "Iteration 80, loss = 0.64759967\n",
      "Iteration 81, loss = 0.64506119\n",
      "Iteration 82, loss = 0.64251046\n",
      "Iteration 83, loss = 0.63996469\n",
      "Iteration 84, loss = 0.63741479\n",
      "Iteration 85, loss = 0.63488393\n",
      "Iteration 86, loss = 0.63236299\n",
      "Iteration 87, loss = 0.62983461\n",
      "Iteration 88, loss = 0.62730932\n",
      "Iteration 89, loss = 0.62479435\n",
      "Iteration 90, loss = 0.62228398\n",
      "Iteration 91, loss = 0.61978091\n",
      "Iteration 92, loss = 0.61728805\n",
      "Iteration 93, loss = 0.61480916\n",
      "Iteration 94, loss = 0.61234249\n",
      "Iteration 95, loss = 0.60988870\n",
      "Iteration 96, loss = 0.60743178\n",
      "Iteration 97, loss = 0.60498672\n",
      "Iteration 98, loss = 0.60255649\n",
      "Iteration 99, loss = 0.60014195\n",
      "Iteration 100, loss = 0.59774445\n",
      "Iteration 101, loss = 0.59536417\n",
      "Iteration 102, loss = 0.59300252\n",
      "Iteration 103, loss = 0.59065932\n",
      "Iteration 104, loss = 0.58833453\n",
      "Iteration 105, loss = 0.58602914\n",
      "Iteration 106, loss = 0.58374401\n",
      "Iteration 107, loss = 0.58147943\n",
      "Iteration 108, loss = 0.57923365\n",
      "Iteration 109, loss = 0.57700610\n",
      "Iteration 110, loss = 0.57479705\n",
      "Iteration 111, loss = 0.57260755\n",
      "Iteration 112, loss = 0.57043683\n",
      "Iteration 113, loss = 0.56828449\n",
      "Iteration 114, loss = 0.56615035\n",
      "Iteration 115, loss = 0.56407985\n",
      "Iteration 116, loss = 0.56216624\n",
      "Iteration 117, loss = 0.56034768\n",
      "Iteration 118, loss = 0.55855105\n",
      "Iteration 119, loss = 0.55682340\n",
      "Iteration 120, loss = 0.55521350\n",
      "Iteration 121, loss = 0.55363915\n",
      "Iteration 122, loss = 0.55212648\n",
      "Iteration 123, loss = 0.55062174\n",
      "Iteration 124, loss = 0.54910416\n",
      "Iteration 125, loss = 0.54756860\n",
      "Iteration 126, loss = 0.54600446\n",
      "Iteration 127, loss = 0.54441934\n",
      "Iteration 128, loss = 0.54282692\n",
      "Iteration 129, loss = 0.54124770\n",
      "Iteration 130, loss = 0.53968233\n",
      "Iteration 131, loss = 0.53814848\n",
      "Iteration 132, loss = 0.53663082\n",
      "Iteration 133, loss = 0.53515648\n",
      "Iteration 134, loss = 0.53370827\n",
      "Iteration 135, loss = 0.53228539\n",
      "Iteration 136, loss = 0.53089242\n",
      "Iteration 137, loss = 0.52951242\n",
      "Iteration 138, loss = 0.52814333\n",
      "Iteration 139, loss = 0.52678797\n",
      "Iteration 140, loss = 0.52545076\n",
      "Iteration 141, loss = 0.52411862\n",
      "Iteration 142, loss = 0.52279218\n",
      "Iteration 143, loss = 0.52147314\n",
      "Iteration 144, loss = 0.52016186\n",
      "Iteration 145, loss = 0.51885866\n",
      "Iteration 146, loss = 0.51756678\n",
      "Iteration 147, loss = 0.51628689\n",
      "Iteration 148, loss = 0.51501622\n",
      "Iteration 149, loss = 0.51375896\n",
      "Iteration 150, loss = 0.51251271\n",
      "Iteration 151, loss = 0.51127635\n",
      "Iteration 152, loss = 0.51005007\n",
      "Iteration 153, loss = 0.50883884\n",
      "Iteration 154, loss = 0.50763823\n",
      "Iteration 155, loss = 0.50644864\n",
      "Iteration 156, loss = 0.50527035\n",
      "Iteration 157, loss = 0.50410110\n",
      "Iteration 158, loss = 0.50293823\n",
      "Iteration 159, loss = 0.50178179\n",
      "Iteration 160, loss = 0.50063181\n",
      "Iteration 161, loss = 0.49948834\n",
      "Iteration 162, loss = 0.49835144\n",
      "Iteration 163, loss = 0.49722266\n",
      "Iteration 164, loss = 0.49610426\n",
      "Iteration 165, loss = 0.49499876\n",
      "Iteration 166, loss = 0.49390180\n",
      "Iteration 167, loss = 0.49281283\n",
      "Iteration 168, loss = 0.49172945\n",
      "Iteration 169, loss = 0.49065176\n",
      "Iteration 170, loss = 0.48957988\n",
      "Iteration 171, loss = 0.48851387\n",
      "Iteration 172, loss = 0.48745433\n",
      "Iteration 173, loss = 0.48640623\n",
      "Iteration 174, loss = 0.48536612\n",
      "Iteration 175, loss = 0.48433445\n",
      "Iteration 176, loss = 0.48331050\n",
      "Iteration 177, loss = 0.48229336\n",
      "Iteration 178, loss = 0.48128262\n",
      "Iteration 179, loss = 0.48027822\n",
      "Iteration 180, loss = 0.47928082\n",
      "Iteration 181, loss = 0.47828960\n",
      "Iteration 182, loss = 0.47730488\n",
      "Iteration 183, loss = 0.47632712\n",
      "Iteration 184, loss = 0.47535650\n",
      "Iteration 185, loss = 0.47439169\n",
      "Iteration 186, loss = 0.47343258\n",
      "Iteration 187, loss = 0.47247913\n",
      "Iteration 188, loss = 0.47153139\n",
      "Iteration 189, loss = 0.47058982\n",
      "Iteration 190, loss = 0.46965384\n",
      "Iteration 191, loss = 0.46872382\n",
      "Iteration 192, loss = 0.46779923\n",
      "Iteration 193, loss = 0.46688081\n",
      "Iteration 194, loss = 0.46596729\n",
      "Iteration 195, loss = 0.46505861\n",
      "Iteration 196, loss = 0.46415473\n",
      "Iteration 197, loss = 0.46325558\n",
      "Iteration 198, loss = 0.46236113\n",
      "Iteration 199, loss = 0.46147132\n",
      "Iteration 200, loss = 0.46058638\n",
      "Iteration 201, loss = 0.45970604\n",
      "Iteration 202, loss = 0.45882989\n",
      "Iteration 203, loss = 0.45795782\n",
      "Iteration 204, loss = 0.45709042\n",
      "Iteration 205, loss = 0.45622722\n",
      "Iteration 206, loss = 0.45536814\n",
      "Iteration 207, loss = 0.45451303\n",
      "Iteration 208, loss = 0.45366192\n",
      "Iteration 209, loss = 0.45281470\n",
      "Iteration 210, loss = 0.45197131\n",
      "Iteration 211, loss = 0.45113169\n",
      "Iteration 212, loss = 0.45029577\n",
      "Iteration 213, loss = 0.44946551\n",
      "Iteration 214, loss = 0.44863941\n",
      "Iteration 215, loss = 0.44781874\n",
      "Iteration 216, loss = 0.44700229\n",
      "Iteration 217, loss = 0.44618927\n",
      "Iteration 218, loss = 0.44538125\n",
      "Iteration 219, loss = 0.44457689\n",
      "Iteration 220, loss = 0.44377535\n",
      "Iteration 221, loss = 0.44297681\n",
      "Iteration 222, loss = 0.44218121\n",
      "Iteration 223, loss = 0.44138881\n",
      "Iteration 224, loss = 0.44060031\n",
      "Iteration 225, loss = 0.43981499\n",
      "Iteration 226, loss = 0.43903306\n",
      "Iteration 227, loss = 0.43825396\n",
      "Iteration 228, loss = 0.43747889\n",
      "Iteration 229, loss = 0.43670682\n",
      "Iteration 230, loss = 0.43593775\n",
      "Iteration 231, loss = 0.43517253\n",
      "Iteration 232, loss = 0.43441050\n",
      "Iteration 233, loss = 0.43365126\n",
      "Iteration 234, loss = 0.43289470\n",
      "Iteration 235, loss = 0.43214077\n",
      "Iteration 236, loss = 0.43138981\n",
      "Iteration 237, loss = 0.43064145\n",
      "Iteration 238, loss = 0.42989556\n",
      "Iteration 239, loss = 0.42915214\n",
      "Iteration 240, loss = 0.42841135\n",
      "Iteration 241, loss = 0.42767379\n",
      "Iteration 242, loss = 0.42693890\n",
      "Iteration 243, loss = 0.42620635\n",
      "Iteration 244, loss = 0.42547647\n",
      "Iteration 245, loss = 0.42474904\n",
      "Iteration 246, loss = 0.42402397\n",
      "Iteration 247, loss = 0.42330124\n",
      "Iteration 248, loss = 0.42258083\n",
      "Iteration 249, loss = 0.42186272\n",
      "Iteration 250, loss = 0.42114688\n",
      "Iteration 251, loss = 0.42043335\n",
      "Iteration 252, loss = 0.41972198\n",
      "Iteration 253, loss = 0.41901322\n",
      "Iteration 254, loss = 0.41830699\n",
      "Iteration 255, loss = 0.41760297\n",
      "Iteration 256, loss = 0.41690102\n",
      "Iteration 257, loss = 0.41620154\n",
      "Iteration 258, loss = 0.41550447\n",
      "Iteration 259, loss = 0.41480945\n",
      "Iteration 260, loss = 0.41411643\n",
      "Iteration 261, loss = 0.41342540\n",
      "Iteration 262, loss = 0.41273632\n",
      "Iteration 263, loss = 0.41204914\n",
      "Iteration 264, loss = 0.41136385\n",
      "Iteration 265, loss = 0.41068038\n",
      "Iteration 266, loss = 0.40999872\n",
      "Iteration 267, loss = 0.40931884\n",
      "Iteration 268, loss = 0.40864069\n",
      "Iteration 269, loss = 0.40796443\n",
      "Iteration 270, loss = 0.40729019\n",
      "Iteration 271, loss = 0.40661762\n",
      "Iteration 272, loss = 0.40594668\n",
      "Iteration 273, loss = 0.40527733\n",
      "Iteration 274, loss = 0.40460955\n",
      "Iteration 275, loss = 0.40394331\n",
      "Iteration 276, loss = 0.40327856\n",
      "Iteration 277, loss = 0.40261529\n",
      "Iteration 278, loss = 0.40195347\n",
      "Iteration 279, loss = 0.40129306\n",
      "Iteration 280, loss = 0.40063402\n",
      "Iteration 281, loss = 0.39997634\n",
      "Iteration 282, loss = 0.39931999\n",
      "Iteration 283, loss = 0.39866493\n",
      "Iteration 284, loss = 0.39801114\n",
      "Iteration 285, loss = 0.39735860\n",
      "Iteration 286, loss = 0.39670728\n",
      "Iteration 287, loss = 0.39605741\n",
      "Iteration 288, loss = 0.39540937\n",
      "Iteration 289, loss = 0.39476356\n",
      "Iteration 290, loss = 0.39411883\n",
      "Iteration 291, loss = 0.39347519\n",
      "Iteration 292, loss = 0.39283265\n",
      "Iteration 293, loss = 0.39219121\n",
      "Iteration 294, loss = 0.39155088\n",
      "Iteration 295, loss = 0.39091167\n",
      "Iteration 296, loss = 0.39027357\n",
      "Iteration 297, loss = 0.38963657\n",
      "Iteration 298, loss = 0.38900067\n",
      "Iteration 299, loss = 0.38836586\n",
      "Iteration 300, loss = 0.38773215\n",
      "Iteration 301, loss = 0.38709950\n",
      "Iteration 302, loss = 0.38646847\n",
      "Iteration 303, loss = 0.38583823\n",
      "Iteration 304, loss = 0.38520869\n",
      "Iteration 305, loss = 0.38457984\n",
      "Iteration 306, loss = 0.38395227\n",
      "Iteration 307, loss = 0.38332580\n",
      "Iteration 308, loss = 0.38270024\n",
      "Iteration 309, loss = 0.38207558\n",
      "Iteration 310, loss = 0.38145181\n",
      "Iteration 311, loss = 0.38082906\n",
      "Iteration 312, loss = 0.38020748\n",
      "Iteration 313, loss = 0.37958678\n",
      "Iteration 314, loss = 0.37896697\n",
      "Iteration 315, loss = 0.37834801\n",
      "Iteration 316, loss = 0.37772991\n",
      "Iteration 317, loss = 0.37711265\n",
      "Iteration 318, loss = 0.37649621\n",
      "Iteration 319, loss = 0.37588059\n",
      "Iteration 320, loss = 0.37526578\n",
      "Iteration 321, loss = 0.37465249\n",
      "Iteration 322, loss = 0.37403859\n",
      "Iteration 323, loss = 0.37342619\n",
      "Iteration 324, loss = 0.37281453\n",
      "Iteration 325, loss = 0.37220361\n",
      "Iteration 326, loss = 0.37159340\n",
      "Iteration 327, loss = 0.37098406\n",
      "Iteration 328, loss = 0.37037572\n",
      "Iteration 329, loss = 0.36976847\n",
      "Iteration 330, loss = 0.36916234\n",
      "Iteration 331, loss = 0.36855690\n",
      "Iteration 332, loss = 0.36795214\n",
      "Iteration 333, loss = 0.36734806\n",
      "Iteration 334, loss = 0.36674490\n",
      "Iteration 335, loss = 0.36614241\n",
      "Iteration 336, loss = 0.36554060\n",
      "Iteration 337, loss = 0.36493944\n",
      "Iteration 338, loss = 0.36433893\n",
      "Iteration 339, loss = 0.36373905\n",
      "Iteration 340, loss = 0.36313978\n",
      "Iteration 341, loss = 0.36254110\n",
      "Iteration 342, loss = 0.36194295\n",
      "Iteration 343, loss = 0.36134528\n",
      "Iteration 344, loss = 0.36074784\n",
      "Iteration 345, loss = 0.36014793\n",
      "Iteration 346, loss = 0.35952676\n",
      "Iteration 347, loss = 0.35886063\n",
      "Iteration 348, loss = 0.35816189\n",
      "Iteration 349, loss = 0.35754969\n",
      "Iteration 350, loss = 0.35695687\n",
      "Iteration 351, loss = 0.35632937\n",
      "Iteration 352, loss = 0.35567392\n",
      "Iteration 353, loss = 0.35499652\n",
      "Iteration 354, loss = 0.35430903\n",
      "Iteration 355, loss = 0.35364008\n",
      "Iteration 356, loss = 0.35303495\n",
      "Iteration 357, loss = 0.35234822\n",
      "Iteration 358, loss = 0.35164602\n",
      "Iteration 359, loss = 0.35098858\n",
      "Iteration 360, loss = 0.35032916\n",
      "Iteration 361, loss = 0.34965665\n",
      "Iteration 362, loss = 0.34896952\n",
      "Iteration 363, loss = 0.34826914\n",
      "Iteration 364, loss = 0.34756079\n",
      "Iteration 365, loss = 0.34685009\n",
      "Iteration 366, loss = 0.34614282\n",
      "Iteration 367, loss = 0.34544534\n",
      "Iteration 368, loss = 0.34472350\n",
      "Iteration 369, loss = 0.34399457\n",
      "Iteration 370, loss = 0.34327081\n",
      "Iteration 371, loss = 0.34254576\n",
      "Iteration 372, loss = 0.34181284\n",
      "Iteration 373, loss = 0.34107215\n",
      "Iteration 374, loss = 0.34032415\n",
      "Iteration 375, loss = 0.33957058\n",
      "Iteration 376, loss = 0.33881219\n",
      "Iteration 377, loss = 0.33805014\n",
      "Iteration 378, loss = 0.33728477\n",
      "Iteration 379, loss = 0.33651585\n",
      "Iteration 380, loss = 0.33574194\n",
      "Iteration 381, loss = 0.33496379\n",
      "Iteration 382, loss = 0.33418043\n",
      "Iteration 383, loss = 0.33339288\n",
      "Iteration 384, loss = 0.33260161\n",
      "Iteration 385, loss = 0.33180713\n",
      "Iteration 386, loss = 0.33100898\n",
      "Iteration 387, loss = 0.33020952\n",
      "Iteration 388, loss = 0.32940668\n",
      "Iteration 389, loss = 0.32859610\n",
      "Iteration 390, loss = 0.32778212\n",
      "Iteration 391, loss = 0.32696674\n",
      "Iteration 392, loss = 0.32614807\n",
      "Iteration 393, loss = 0.32532628\n",
      "Iteration 394, loss = 0.32450223\n",
      "Iteration 395, loss = 0.32367417\n",
      "Iteration 396, loss = 0.32284411\n",
      "Iteration 397, loss = 0.32201278\n",
      "Iteration 398, loss = 0.32117780\n",
      "Iteration 399, loss = 0.32034157\n",
      "Iteration 400, loss = 0.31950362\n",
      "Iteration 401, loss = 0.31866310\n",
      "Iteration 402, loss = 0.31782144\n",
      "Iteration 403, loss = 0.31697819\n",
      "Iteration 404, loss = 0.31613368\n",
      "Iteration 405, loss = 0.31528765\n",
      "Iteration 406, loss = 0.31444040\n",
      "Iteration 407, loss = 0.31359182\n",
      "Iteration 408, loss = 0.31274207\n",
      "Iteration 409, loss = 0.31189125\n",
      "Iteration 410, loss = 0.31104061\n",
      "Iteration 411, loss = 0.31018831\n",
      "Iteration 412, loss = 0.30933540\n",
      "Iteration 413, loss = 0.30848259\n",
      "Iteration 414, loss = 0.30762965\n",
      "Iteration 415, loss = 0.30677488\n",
      "Iteration 416, loss = 0.30592096\n",
      "Iteration 417, loss = 0.30506707\n",
      "Iteration 418, loss = 0.30421292\n",
      "Iteration 419, loss = 0.30335755\n",
      "Iteration 420, loss = 0.30251104\n",
      "Iteration 421, loss = 0.30165404\n",
      "Iteration 422, loss = 0.30079669\n",
      "Iteration 423, loss = 0.29994528\n",
      "Iteration 424, loss = 0.29909312\n",
      "Iteration 425, loss = 0.29823971\n",
      "Iteration 426, loss = 0.29738423\n",
      "Iteration 427, loss = 0.29653127\n",
      "Iteration 428, loss = 0.29567854\n",
      "Iteration 429, loss = 0.29482519\n",
      "Iteration 430, loss = 0.29396881\n",
      "Iteration 431, loss = 0.29311491\n",
      "Iteration 432, loss = 0.29226247\n",
      "Iteration 433, loss = 0.29140748\n",
      "Iteration 434, loss = 0.29055387\n",
      "Iteration 435, loss = 0.28969856\n",
      "Iteration 436, loss = 0.28884479\n",
      "Iteration 437, loss = 0.28799082\n",
      "Iteration 438, loss = 0.28713687\n",
      "Iteration 439, loss = 0.28628398\n",
      "Iteration 440, loss = 0.28543044\n",
      "Iteration 441, loss = 0.28458193\n",
      "Iteration 442, loss = 0.28372714\n",
      "Iteration 443, loss = 0.28287874\n",
      "Iteration 444, loss = 0.28202863\n",
      "Iteration 445, loss = 0.28117811\n",
      "Iteration 446, loss = 0.28033117\n",
      "Iteration 447, loss = 0.27948360\n",
      "Iteration 448, loss = 0.27863481\n",
      "Iteration 449, loss = 0.27778790\n",
      "Iteration 450, loss = 0.27694260\n",
      "Iteration 451, loss = 0.27609745\n",
      "Iteration 452, loss = 0.27525276\n",
      "Iteration 453, loss = 0.27441039\n",
      "Iteration 454, loss = 0.27357065\n",
      "Iteration 455, loss = 0.27272916\n",
      "Iteration 456, loss = 0.27189190\n",
      "Iteration 457, loss = 0.27105505\n",
      "Iteration 458, loss = 0.27021909\n",
      "Iteration 459, loss = 0.26938546\n",
      "Iteration 460, loss = 0.26855319\n",
      "Iteration 461, loss = 0.26772289\n",
      "Iteration 462, loss = 0.26689443\n",
      "Iteration 463, loss = 0.26606829\n",
      "Iteration 464, loss = 0.26524333\n",
      "Iteration 465, loss = 0.26442052\n",
      "Iteration 466, loss = 0.26359960\n",
      "Iteration 467, loss = 0.26278063\n",
      "Iteration 468, loss = 0.26196365\n",
      "Iteration 469, loss = 0.26114884\n",
      "Iteration 470, loss = 0.26033598\n",
      "Iteration 471, loss = 0.25952543\n",
      "Iteration 472, loss = 0.25871718\n",
      "Iteration 473, loss = 0.25791229\n",
      "Iteration 474, loss = 0.25710851\n",
      "Iteration 475, loss = 0.25630805\n",
      "Iteration 476, loss = 0.25551007\n",
      "Iteration 477, loss = 0.25471497\n",
      "Iteration 478, loss = 0.25392250\n",
      "Iteration 479, loss = 0.25313308\n",
      "Iteration 480, loss = 0.25234647\n",
      "Iteration 481, loss = 0.25156254\n",
      "Iteration 482, loss = 0.25078135\n",
      "Iteration 483, loss = 0.25000291\n",
      "Iteration 484, loss = 0.24922991\n",
      "Iteration 485, loss = 0.24845782\n",
      "Iteration 486, loss = 0.24769139\n",
      "Iteration 487, loss = 0.24692769\n",
      "Iteration 488, loss = 0.24616680\n",
      "Iteration 489, loss = 0.24540879\n",
      "Iteration 490, loss = 0.24465374\n",
      "Iteration 491, loss = 0.24390172\n",
      "Iteration 492, loss = 0.24315279\n",
      "Iteration 493, loss = 0.24240702\n",
      "Iteration 494, loss = 0.24166463\n",
      "Iteration 495, loss = 0.24092573\n",
      "Iteration 496, loss = 0.24019064\n",
      "Iteration 497, loss = 0.23945876\n",
      "Iteration 498, loss = 0.23873007\n",
      "Iteration 499, loss = 0.23800461\n",
      "Iteration 500, loss = 0.23728240\n",
      "Iteration 501, loss = 0.23656347\n",
      "Iteration 502, loss = 0.23584790\n",
      "Iteration 503, loss = 0.23513571\n",
      "Iteration 504, loss = 0.23442688\n",
      "Iteration 505, loss = 0.23372167\n",
      "Iteration 506, loss = 0.23301991\n",
      "Iteration 507, loss = 0.23232169\n",
      "Iteration 508, loss = 0.23162685\n",
      "Iteration 509, loss = 0.23093536\n",
      "Iteration 510, loss = 0.23024731\n",
      "Iteration 511, loss = 0.22956263\n",
      "Iteration 512, loss = 0.22888131\n",
      "Iteration 513, loss = 0.22820334\n",
      "Iteration 514, loss = 0.22752871\n",
      "Iteration 515, loss = 0.22685742\n",
      "Iteration 516, loss = 0.22618946\n",
      "Iteration 517, loss = 0.22552481\n",
      "Iteration 518, loss = 0.22486366\n",
      "Iteration 519, loss = 0.22420550\n",
      "Iteration 520, loss = 0.22355131\n",
      "Iteration 521, loss = 0.22290050\n",
      "Iteration 522, loss = 0.22225290\n",
      "Iteration 523, loss = 0.22160851\n",
      "Iteration 524, loss = 0.22096734\n",
      "Iteration 525, loss = 0.22032941\n",
      "Iteration 526, loss = 0.21969470\n",
      "Iteration 527, loss = 0.21906322\n",
      "Iteration 528, loss = 0.21843496\n",
      "Iteration 529, loss = 0.21780994\n",
      "Iteration 530, loss = 0.21718836\n",
      "Iteration 531, loss = 0.21657011\n",
      "Iteration 532, loss = 0.21595497\n",
      "Iteration 533, loss = 0.21534293\n",
      "Iteration 534, loss = 0.21473415\n",
      "Iteration 535, loss = 0.21412854\n",
      "Iteration 536, loss = 0.21352635\n",
      "Iteration 537, loss = 0.21292734\n",
      "Iteration 538, loss = 0.21233160\n",
      "Iteration 539, loss = 0.21173907\n",
      "Iteration 540, loss = 0.21114963\n",
      "Iteration 541, loss = 0.21056328\n",
      "Iteration 542, loss = 0.20998119\n",
      "Iteration 543, loss = 0.20940118\n",
      "Iteration 544, loss = 0.20882414\n",
      "Iteration 545, loss = 0.20825114\n",
      "Iteration 546, loss = 0.20768110\n",
      "Iteration 547, loss = 0.20711460\n",
      "Iteration 548, loss = 0.20655048\n",
      "Iteration 549, loss = 0.20598942\n",
      "Iteration 550, loss = 0.20543118\n",
      "Iteration 551, loss = 0.20487623\n",
      "Iteration 552, loss = 0.20432451\n",
      "Iteration 553, loss = 0.20377522\n",
      "Iteration 554, loss = 0.20322870\n",
      "Iteration 555, loss = 0.20268679\n",
      "Iteration 556, loss = 0.20214685\n",
      "Iteration 557, loss = 0.20160917\n",
      "Iteration 558, loss = 0.20107610\n",
      "Iteration 559, loss = 0.20054535\n",
      "Iteration 560, loss = 0.20001642\n",
      "Iteration 561, loss = 0.19948985\n",
      "Iteration 562, loss = 0.19896819\n",
      "Iteration 563, loss = 0.19844853\n",
      "Iteration 564, loss = 0.19793057\n",
      "Iteration 565, loss = 0.19741669\n",
      "Iteration 566, loss = 0.19690567\n",
      "Iteration 567, loss = 0.19639721\n",
      "Iteration 568, loss = 0.19589197\n",
      "Iteration 569, loss = 0.19538934\n",
      "Iteration 570, loss = 0.19488935\n",
      "Iteration 571, loss = 0.19439174\n",
      "Iteration 572, loss = 0.19389791\n",
      "Iteration 573, loss = 0.19340658\n",
      "Iteration 574, loss = 0.19291660\n",
      "Iteration 575, loss = 0.19243004\n",
      "Iteration 576, loss = 0.19194610\n",
      "Iteration 577, loss = 0.19146511\n",
      "Iteration 578, loss = 0.19098655\n",
      "Iteration 579, loss = 0.19051061\n",
      "Iteration 580, loss = 0.19003777\n",
      "Iteration 581, loss = 0.18956706\n",
      "Iteration 582, loss = 0.18909862\n",
      "Iteration 583, loss = 0.18863284\n",
      "Iteration 584, loss = 0.18816992\n",
      "Iteration 585, loss = 0.18771026\n",
      "Iteration 586, loss = 0.18725163\n",
      "Iteration 587, loss = 0.18679602\n",
      "Iteration 588, loss = 0.18634323\n",
      "Iteration 589, loss = 0.18589313\n",
      "Iteration 590, loss = 0.18544505\n",
      "Iteration 591, loss = 0.18499902\n",
      "Iteration 592, loss = 0.18455533\n",
      "Iteration 593, loss = 0.18411412\n",
      "Iteration 594, loss = 0.18367669\n",
      "Iteration 595, loss = 0.18324046\n",
      "Iteration 596, loss = 0.18280589\n",
      "Iteration 597, loss = 0.18237467\n",
      "Iteration 598, loss = 0.18194596\n",
      "Iteration 599, loss = 0.18151938\n",
      "Iteration 600, loss = 0.18109490\n",
      "Iteration 601, loss = 0.18067254\n",
      "Iteration 602, loss = 0.18025236\n",
      "Iteration 603, loss = 0.17983441\n",
      "Iteration 604, loss = 0.17941869\n",
      "Iteration 605, loss = 0.17900523\n",
      "Iteration 606, loss = 0.17859494\n",
      "Iteration 607, loss = 0.17818623\n",
      "Iteration 608, loss = 0.17777884\n",
      "Iteration 609, loss = 0.17737463\n",
      "Iteration 610, loss = 0.17697267\n",
      "Iteration 611, loss = 0.17657277\n",
      "Iteration 612, loss = 0.17617492\n",
      "Iteration 613, loss = 0.17577909\n",
      "Iteration 614, loss = 0.17538530\n",
      "Iteration 615, loss = 0.17499355\n",
      "Iteration 616, loss = 0.17460394\n",
      "Iteration 617, loss = 0.17421623\n",
      "Iteration 618, loss = 0.17383060\n",
      "Iteration 619, loss = 0.17344684\n",
      "Iteration 620, loss = 0.17306508\n",
      "Iteration 621, loss = 0.17268547\n",
      "Iteration 622, loss = 0.17230775\n",
      "Iteration 623, loss = 0.17193226\n",
      "Iteration 624, loss = 0.17155879\n",
      "Iteration 625, loss = 0.17118717\n",
      "Iteration 626, loss = 0.17081761\n",
      "Iteration 627, loss = 0.17044997\n",
      "Iteration 628, loss = 0.17008424\n",
      "Iteration 629, loss = 0.16972042\n",
      "Iteration 630, loss = 0.16935849\n",
      "Iteration 631, loss = 0.16899843\n",
      "Iteration 632, loss = 0.16864026\n",
      "Iteration 633, loss = 0.16828398\n",
      "Iteration 634, loss = 0.16792958\n",
      "Iteration 635, loss = 0.16757702\n",
      "Iteration 636, loss = 0.16722631\n",
      "Iteration 637, loss = 0.16687772\n",
      "Iteration 638, loss = 0.16653042\n",
      "Iteration 639, loss = 0.16618532\n",
      "Iteration 640, loss = 0.16584184\n",
      "Iteration 641, loss = 0.16550026\n",
      "Iteration 642, loss = 0.16516049\n",
      "Iteration 643, loss = 0.16482250\n",
      "Iteration 644, loss = 0.16448625\n",
      "Iteration 645, loss = 0.16415174\n",
      "Iteration 646, loss = 0.16381894\n",
      "Iteration 647, loss = 0.16348786\n",
      "Iteration 648, loss = 0.16315825\n",
      "Iteration 649, loss = 0.16283017\n",
      "Iteration 650, loss = 0.16250374\n",
      "Iteration 651, loss = 0.16217896\n",
      "Iteration 652, loss = 0.16185588\n",
      "Iteration 653, loss = 0.16153466\n",
      "Iteration 654, loss = 0.16121490\n",
      "Iteration 655, loss = 0.16089680\n",
      "Iteration 656, loss = 0.16058058\n",
      "Iteration 657, loss = 0.16026607\n",
      "Iteration 658, loss = 0.15995296\n",
      "Iteration 659, loss = 0.15964127\n",
      "Iteration 660, loss = 0.15933102\n",
      "Iteration 661, loss = 0.15902141\n",
      "Iteration 662, loss = 0.15871277\n",
      "Iteration 663, loss = 0.15840546\n",
      "Iteration 664, loss = 0.15809951\n",
      "Iteration 665, loss = 0.15779501\n",
      "Iteration 666, loss = 0.15749186\n",
      "Iteration 667, loss = 0.15718938\n",
      "Iteration 668, loss = 0.15688834\n",
      "Iteration 669, loss = 0.15658864\n",
      "Iteration 670, loss = 0.15629052\n",
      "Iteration 671, loss = 0.15599408\n",
      "Iteration 672, loss = 0.15569902\n",
      "Iteration 673, loss = 0.15540552\n",
      "Iteration 674, loss = 0.15511334\n",
      "Iteration 675, loss = 0.15482306\n",
      "Iteration 676, loss = 0.15453473\n",
      "Iteration 677, loss = 0.15424821\n",
      "Iteration 678, loss = 0.15396262\n",
      "Iteration 679, loss = 0.15367867\n",
      "Iteration 680, loss = 0.15339658\n",
      "Iteration 681, loss = 0.15311564\n",
      "Iteration 682, loss = 0.15283583\n",
      "Iteration 683, loss = 0.15255735\n",
      "Iteration 684, loss = 0.15228149\n",
      "Iteration 685, loss = 0.15200694\n",
      "Iteration 686, loss = 0.15173562\n",
      "Iteration 687, loss = 0.15146566\n",
      "Iteration 688, loss = 0.15119758\n",
      "Iteration 689, loss = 0.15093125\n",
      "Iteration 690, loss = 0.15066634\n",
      "Iteration 691, loss = 0.15040286\n",
      "Iteration 692, loss = 0.15014078\n",
      "Iteration 693, loss = 0.14988012\n",
      "Iteration 694, loss = 0.14962084\n",
      "Iteration 695, loss = 0.14936290\n",
      "Iteration 696, loss = 0.14910629\n",
      "Iteration 697, loss = 0.14885108\n",
      "Iteration 698, loss = 0.14859702\n",
      "Iteration 699, loss = 0.14834436\n",
      "Iteration 700, loss = 0.14809297\n",
      "Iteration 701, loss = 0.14784285\n",
      "Iteration 702, loss = 0.14759401\n",
      "Iteration 703, loss = 0.14734635\n",
      "Iteration 704, loss = 0.14709996\n",
      "Iteration 705, loss = 0.14685477\n",
      "Iteration 706, loss = 0.14661075\n",
      "Iteration 707, loss = 0.14636711\n",
      "Iteration 708, loss = 0.14612375\n",
      "Iteration 709, loss = 0.14588128\n",
      "Iteration 710, loss = 0.14563975\n",
      "Iteration 711, loss = 0.14539946\n",
      "Iteration 712, loss = 0.14516050\n",
      "Iteration 713, loss = 0.14492242\n",
      "Iteration 714, loss = 0.14468563\n",
      "Iteration 715, loss = 0.14444983\n",
      "Iteration 716, loss = 0.14421495\n",
      "Iteration 717, loss = 0.14398132\n",
      "Iteration 718, loss = 0.14374867\n",
      "Iteration 719, loss = 0.14351667\n",
      "Iteration 720, loss = 0.14328595\n",
      "Iteration 721, loss = 0.14305623\n",
      "Iteration 722, loss = 0.14282743\n",
      "Iteration 723, loss = 0.14259972\n",
      "Iteration 724, loss = 0.14237307\n",
      "Iteration 725, loss = 0.14214910\n",
      "Iteration 726, loss = 0.14192645\n",
      "Iteration 727, loss = 0.14170458\n",
      "Iteration 728, loss = 0.14148335\n",
      "Iteration 729, loss = 0.14126310\n",
      "Iteration 730, loss = 0.14104389\n",
      "Iteration 731, loss = 0.14082581\n",
      "Iteration 732, loss = 0.14060889\n",
      "Iteration 733, loss = 0.14039285\n",
      "Iteration 734, loss = 0.14017780\n",
      "Iteration 735, loss = 0.13996387\n",
      "Iteration 736, loss = 0.13975098\n",
      "Iteration 737, loss = 0.13953914\n",
      "Iteration 738, loss = 0.13932827\n",
      "Iteration 739, loss = 0.13911861\n",
      "Iteration 740, loss = 0.13891072\n",
      "Iteration 741, loss = 0.13870385\n",
      "Iteration 742, loss = 0.13849797\n",
      "Iteration 743, loss = 0.13829309\n",
      "Iteration 744, loss = 0.13808891\n",
      "Iteration 745, loss = 0.13788560\n",
      "Iteration 746, loss = 0.13768305\n",
      "Iteration 747, loss = 0.13748174\n",
      "Iteration 748, loss = 0.13728124\n",
      "Iteration 749, loss = 0.13708181\n",
      "Iteration 750, loss = 0.13688325\n",
      "Iteration 751, loss = 0.13668601\n",
      "Iteration 752, loss = 0.13649009\n",
      "Iteration 753, loss = 0.13629519\n",
      "Iteration 754, loss = 0.13610114\n",
      "Iteration 755, loss = 0.13590796\n",
      "Iteration 756, loss = 0.13571562\n",
      "Iteration 757, loss = 0.13552433\n",
      "Iteration 758, loss = 0.13533378\n",
      "Iteration 759, loss = 0.13514422\n",
      "Iteration 760, loss = 0.13495558\n",
      "Iteration 761, loss = 0.13476781\n",
      "Iteration 762, loss = 0.13458095\n",
      "Iteration 763, loss = 0.13439504\n",
      "Iteration 764, loss = 0.13420981\n",
      "Iteration 765, loss = 0.13402555\n",
      "Iteration 766, loss = 0.13384208\n",
      "Iteration 767, loss = 0.13365925\n",
      "Iteration 768, loss = 0.13347721\n",
      "Iteration 769, loss = 0.13329603\n",
      "Iteration 770, loss = 0.13311582\n",
      "Iteration 771, loss = 0.13293605\n",
      "Iteration 772, loss = 0.13275729\n",
      "Iteration 773, loss = 0.13257934\n",
      "Iteration 774, loss = 0.13240210\n",
      "Iteration 775, loss = 0.13222557\n",
      "Iteration 776, loss = 0.13204985\n",
      "Iteration 777, loss = 0.13187498\n",
      "Iteration 778, loss = 0.13170088\n",
      "Iteration 779, loss = 0.13152752\n",
      "Iteration 780, loss = 0.13135496\n",
      "Iteration 781, loss = 0.13118312\n",
      "Iteration 782, loss = 0.13101200\n",
      "Iteration 783, loss = 0.13084160\n",
      "Iteration 784, loss = 0.13067197\n",
      "Iteration 785, loss = 0.13050307\n",
      "Iteration 786, loss = 0.13033496\n",
      "Iteration 787, loss = 0.13016748\n",
      "Iteration 788, loss = 0.13000076\n",
      "Iteration 789, loss = 0.12983471\n",
      "Iteration 790, loss = 0.12966939\n",
      "Iteration 791, loss = 0.12950471\n",
      "Iteration 792, loss = 0.12934085\n",
      "Iteration 793, loss = 0.12917752\n",
      "Iteration 794, loss = 0.12901517\n",
      "Iteration 795, loss = 0.12885332\n",
      "Iteration 796, loss = 0.12869194\n",
      "Iteration 797, loss = 0.12853139\n",
      "Iteration 798, loss = 0.12837159\n",
      "Iteration 799, loss = 0.12821239\n",
      "Iteration 800, loss = 0.12805399\n",
      "Iteration 801, loss = 0.12789612\n",
      "Iteration 802, loss = 0.12773881\n",
      "Iteration 803, loss = 0.12758225\n",
      "Iteration 804, loss = 0.12742636\n",
      "Iteration 805, loss = 0.12727109\n",
      "Iteration 806, loss = 0.12711647\n",
      "Iteration 807, loss = 0.12696245\n",
      "Iteration 808, loss = 0.12680918\n",
      "Iteration 809, loss = 0.12665649\n",
      "Iteration 810, loss = 0.12650426\n",
      "Iteration 811, loss = 0.12635306\n",
      "Iteration 812, loss = 0.12620223\n",
      "Iteration 813, loss = 0.12605176\n",
      "Iteration 814, loss = 0.12590232\n",
      "Iteration 815, loss = 0.12575343\n",
      "Iteration 816, loss = 0.12560477\n",
      "Iteration 817, loss = 0.12545708\n",
      "Iteration 818, loss = 0.12531010\n",
      "Iteration 819, loss = 0.12516345\n",
      "Iteration 820, loss = 0.12501729\n",
      "Iteration 821, loss = 0.12487203\n",
      "Iteration 822, loss = 0.12472734\n",
      "Iteration 823, loss = 0.12458291\n",
      "Iteration 824, loss = 0.12443932\n",
      "Iteration 825, loss = 0.12429628\n",
      "Iteration 826, loss = 0.12415374\n",
      "Iteration 827, loss = 0.12401183\n",
      "Iteration 828, loss = 0.12387047\n",
      "Iteration 829, loss = 0.12372976\n",
      "Iteration 830, loss = 0.12358962\n",
      "Iteration 831, loss = 0.12344996\n",
      "Iteration 832, loss = 0.12331098\n",
      "Iteration 833, loss = 0.12317244\n",
      "Iteration 834, loss = 0.12303461\n",
      "Iteration 835, loss = 0.12289714\n",
      "Iteration 836, loss = 0.12276038\n",
      "Iteration 837, loss = 0.12262417\n",
      "Iteration 838, loss = 0.12248840\n",
      "Iteration 839, loss = 0.12235315\n",
      "Iteration 840, loss = 0.12221872\n",
      "Iteration 841, loss = 0.12208463\n",
      "Iteration 842, loss = 0.12195098\n",
      "Iteration 843, loss = 0.12181802\n",
      "Iteration 844, loss = 0.12168544\n",
      "Iteration 845, loss = 0.12155336\n",
      "Iteration 846, loss = 0.12142193\n",
      "Iteration 847, loss = 0.12129103\n",
      "Iteration 848, loss = 0.12116081\n",
      "Iteration 849, loss = 0.12103070\n",
      "Iteration 850, loss = 0.12090131\n",
      "Iteration 851, loss = 0.12077253\n",
      "Iteration 852, loss = 0.12064413\n",
      "Iteration 853, loss = 0.12051633\n",
      "Iteration 854, loss = 0.12038889\n",
      "Iteration 855, loss = 0.12026214\n",
      "Iteration 856, loss = 0.12013585\n",
      "Iteration 857, loss = 0.12001011\n",
      "Iteration 858, loss = 0.11988474\n",
      "Iteration 859, loss = 0.11975970\n",
      "Iteration 860, loss = 0.11963530\n",
      "Iteration 861, loss = 0.11951135\n",
      "Iteration 862, loss = 0.11938803\n",
      "Iteration 863, loss = 0.11926509\n",
      "Iteration 864, loss = 0.11914236\n",
      "Iteration 865, loss = 0.11902028\n",
      "Iteration 866, loss = 0.11889872\n",
      "Iteration 867, loss = 0.11877755\n",
      "Iteration 868, loss = 0.11865661\n",
      "Iteration 869, loss = 0.11853612\n",
      "Iteration 870, loss = 0.11841604\n",
      "Iteration 871, loss = 0.11829641\n",
      "Iteration 872, loss = 0.11817734\n",
      "Iteration 873, loss = 0.11805852\n",
      "Iteration 874, loss = 0.11794036\n",
      "Iteration 875, loss = 0.11782262\n",
      "Iteration 876, loss = 0.11770548\n",
      "Iteration 877, loss = 0.11758854\n",
      "Iteration 878, loss = 0.11747181\n",
      "Iteration 879, loss = 0.11735593\n",
      "Iteration 880, loss = 0.11724047\n",
      "Iteration 881, loss = 0.11712527\n",
      "Iteration 882, loss = 0.11701034\n",
      "Iteration 883, loss = 0.11689594\n",
      "Iteration 884, loss = 0.11678212\n",
      "Iteration 885, loss = 0.11666863\n",
      "Iteration 886, loss = 0.11655546\n",
      "Iteration 887, loss = 0.11644290\n",
      "Iteration 888, loss = 0.11633097\n",
      "Iteration 889, loss = 0.11621934\n",
      "Iteration 890, loss = 0.11610826\n",
      "Iteration 891, loss = 0.11599754\n",
      "Iteration 892, loss = 0.11588719\n",
      "Iteration 893, loss = 0.11577773\n",
      "Iteration 894, loss = 0.11566878\n",
      "Iteration 895, loss = 0.11556012\n",
      "Iteration 896, loss = 0.11545176\n",
      "Iteration 897, loss = 0.11534384\n",
      "Iteration 898, loss = 0.11523650\n",
      "Iteration 899, loss = 0.11512950\n",
      "Iteration 900, loss = 0.11502284\n",
      "Iteration 901, loss = 0.11491665\n",
      "Iteration 902, loss = 0.11481096\n",
      "Iteration 903, loss = 0.11470551\n",
      "Iteration 904, loss = 0.11460052\n",
      "Iteration 905, loss = 0.11449596\n",
      "Iteration 906, loss = 0.11439178\n",
      "Iteration 907, loss = 0.11428795\n",
      "Iteration 908, loss = 0.11418454\n",
      "Iteration 909, loss = 0.11408153\n",
      "Iteration 910, loss = 0.11397888\n",
      "Iteration 911, loss = 0.11387658\n",
      "Iteration 912, loss = 0.11377463\n",
      "Iteration 913, loss = 0.11367311\n",
      "Iteration 914, loss = 0.11357194\n",
      "Iteration 915, loss = 0.11347115\n",
      "Iteration 916, loss = 0.11337073\n",
      "Iteration 917, loss = 0.11327068\n",
      "Iteration 918, loss = 0.11317098\n",
      "Iteration 919, loss = 0.11307163\n",
      "Iteration 920, loss = 0.11297279\n",
      "Iteration 921, loss = 0.11287415\n",
      "Iteration 922, loss = 0.11277588\n",
      "Iteration 923, loss = 0.11267805\n",
      "Iteration 924, loss = 0.11258054\n",
      "Iteration 925, loss = 0.11248337\n",
      "Iteration 926, loss = 0.11238653\n",
      "Iteration 927, loss = 0.11229002\n",
      "Iteration 928, loss = 0.11219385\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36148097\n",
      "Iteration 2, loss = 1.33447200\n",
      "Iteration 3, loss = 1.30795436\n",
      "Iteration 4, loss = 1.28193138\n",
      "Iteration 5, loss = 1.25642138\n",
      "Iteration 6, loss = 1.23147408\n",
      "Iteration 7, loss = 1.20713579\n",
      "Iteration 8, loss = 1.18341619\n",
      "Iteration 9, loss = 1.16024621\n",
      "Iteration 10, loss = 1.13764362\n",
      "Iteration 11, loss = 1.11568987\n",
      "Iteration 12, loss = 1.09439915\n",
      "Iteration 13, loss = 1.07380354\n",
      "Iteration 14, loss = 1.05389406\n",
      "Iteration 15, loss = 1.03469235\n",
      "Iteration 16, loss = 1.01617501\n",
      "Iteration 17, loss = 0.99837495\n",
      "Iteration 18, loss = 0.98126395\n",
      "Iteration 19, loss = 0.96486348\n",
      "Iteration 20, loss = 0.94916245\n",
      "Iteration 21, loss = 0.93416830\n",
      "Iteration 22, loss = 0.91986348\n",
      "Iteration 23, loss = 0.90622404\n",
      "Iteration 24, loss = 0.89327890\n",
      "Iteration 25, loss = 0.88099813\n",
      "Iteration 26, loss = 0.86939625\n",
      "Iteration 27, loss = 0.85841180\n",
      "Iteration 28, loss = 0.84804436\n",
      "Iteration 29, loss = 0.83825873\n",
      "Iteration 30, loss = 0.82906359\n",
      "Iteration 31, loss = 0.82040915\n",
      "Iteration 32, loss = 0.81225520\n",
      "Iteration 33, loss = 0.80460337\n",
      "Iteration 34, loss = 0.79747052\n",
      "Iteration 35, loss = 0.79082224\n",
      "Iteration 36, loss = 0.78464901\n",
      "Iteration 37, loss = 0.77892238\n",
      "Iteration 38, loss = 0.77360873\n",
      "Iteration 39, loss = 0.76866538\n",
      "Iteration 40, loss = 0.76412003\n",
      "Iteration 41, loss = 0.75996739\n",
      "Iteration 42, loss = 0.75611761\n",
      "Iteration 43, loss = 0.75255812\n",
      "Iteration 44, loss = 0.74918366\n",
      "Iteration 45, loss = 0.74607968\n",
      "Iteration 46, loss = 0.74309653\n",
      "Iteration 47, loss = 0.74022581\n",
      "Iteration 48, loss = 0.73747102\n",
      "Iteration 49, loss = 0.73477969\n",
      "Iteration 50, loss = 0.73213564\n",
      "Iteration 51, loss = 0.72953012\n",
      "Iteration 52, loss = 0.72695714\n",
      "Iteration 53, loss = 0.72441710\n",
      "Iteration 54, loss = 0.72186197\n",
      "Iteration 55, loss = 0.71927948\n",
      "Iteration 56, loss = 0.71669157\n",
      "Iteration 57, loss = 0.71409428\n",
      "Iteration 58, loss = 0.71146377\n",
      "Iteration 59, loss = 0.70880643\n",
      "Iteration 60, loss = 0.70613214\n",
      "Iteration 61, loss = 0.70344037\n",
      "Iteration 62, loss = 0.70071234\n",
      "Iteration 63, loss = 0.69794292\n",
      "Iteration 64, loss = 0.69512198\n",
      "Iteration 65, loss = 0.69222778\n",
      "Iteration 66, loss = 0.68924679\n",
      "Iteration 67, loss = 0.68624343\n",
      "Iteration 68, loss = 0.68326737\n",
      "Iteration 69, loss = 0.68029540\n",
      "Iteration 70, loss = 0.67731169\n",
      "Iteration 71, loss = 0.67438715\n",
      "Iteration 72, loss = 0.67149582\n",
      "Iteration 73, loss = 0.66866938\n",
      "Iteration 74, loss = 0.66589594\n",
      "Iteration 75, loss = 0.66318258\n",
      "Iteration 76, loss = 0.66053223\n",
      "Iteration 77, loss = 0.65793088\n",
      "Iteration 78, loss = 0.65535284\n",
      "Iteration 79, loss = 0.65279857\n",
      "Iteration 80, loss = 0.65025721\n",
      "Iteration 81, loss = 0.64772911\n",
      "Iteration 82, loss = 0.64516857\n",
      "Iteration 83, loss = 0.64260824\n",
      "Iteration 84, loss = 0.64003171\n",
      "Iteration 85, loss = 0.63747702\n",
      "Iteration 86, loss = 0.63492885\n",
      "Iteration 87, loss = 0.63236022\n",
      "Iteration 88, loss = 0.62979068\n",
      "Iteration 89, loss = 0.62722921\n",
      "Iteration 90, loss = 0.62467067\n",
      "Iteration 91, loss = 0.62211754\n",
      "Iteration 92, loss = 0.61957237\n",
      "Iteration 93, loss = 0.61703866\n",
      "Iteration 94, loss = 0.61451745\n",
      "Iteration 95, loss = 0.61200915\n",
      "Iteration 96, loss = 0.60950100\n",
      "Iteration 97, loss = 0.60700233\n",
      "Iteration 98, loss = 0.60451958\n",
      "Iteration 99, loss = 0.60205289\n",
      "Iteration 100, loss = 0.59960329\n",
      "Iteration 101, loss = 0.59717148\n",
      "Iteration 102, loss = 0.59475700\n",
      "Iteration 103, loss = 0.59236018\n",
      "Iteration 104, loss = 0.58998211\n",
      "Iteration 105, loss = 0.58762505\n",
      "Iteration 106, loss = 0.58528623\n",
      "Iteration 107, loss = 0.58296513\n",
      "Iteration 108, loss = 0.58066227\n",
      "Iteration 109, loss = 0.57837872\n",
      "Iteration 110, loss = 0.57611350\n",
      "Iteration 111, loss = 0.57386632\n",
      "Iteration 112, loss = 0.57163718\n",
      "Iteration 113, loss = 0.56942692\n",
      "Iteration 114, loss = 0.56723503\n",
      "Iteration 115, loss = 0.56509413\n",
      "Iteration 116, loss = 0.56305885\n",
      "Iteration 117, loss = 0.56113317\n",
      "Iteration 118, loss = 0.55925270\n",
      "Iteration 119, loss = 0.55748573\n",
      "Iteration 120, loss = 0.55585400\n",
      "Iteration 121, loss = 0.55426275\n",
      "Iteration 122, loss = 0.55271463\n",
      "Iteration 123, loss = 0.55117376\n",
      "Iteration 124, loss = 0.54961953\n",
      "Iteration 125, loss = 0.54804535\n",
      "Iteration 126, loss = 0.54644869\n",
      "Iteration 127, loss = 0.54483199\n",
      "Iteration 128, loss = 0.54320779\n",
      "Iteration 129, loss = 0.54158939\n",
      "Iteration 130, loss = 0.53998358\n",
      "Iteration 131, loss = 0.53840264\n",
      "Iteration 132, loss = 0.53685516\n",
      "Iteration 133, loss = 0.53532818\n",
      "Iteration 134, loss = 0.53383658\n",
      "Iteration 135, loss = 0.53236583\n",
      "Iteration 136, loss = 0.53091914\n",
      "Iteration 137, loss = 0.52949501\n",
      "Iteration 138, loss = 0.52808652\n",
      "Iteration 139, loss = 0.52669624\n",
      "Iteration 140, loss = 0.52533035\n",
      "Iteration 141, loss = 0.52397166\n",
      "Iteration 142, loss = 0.52261779\n",
      "Iteration 143, loss = 0.52126821\n",
      "Iteration 144, loss = 0.51992347\n",
      "Iteration 145, loss = 0.51858408\n",
      "Iteration 146, loss = 0.51725054\n",
      "Iteration 147, loss = 0.51592811\n",
      "Iteration 148, loss = 0.51461549\n",
      "Iteration 149, loss = 0.51332120\n",
      "Iteration 150, loss = 0.51204081\n",
      "Iteration 151, loss = 0.51077237\n",
      "Iteration 152, loss = 0.50951898\n",
      "Iteration 153, loss = 0.50827432\n",
      "Iteration 154, loss = 0.50703825\n",
      "Iteration 155, loss = 0.50581065\n",
      "Iteration 156, loss = 0.50459198\n",
      "Iteration 157, loss = 0.50338457\n",
      "Iteration 158, loss = 0.50218592\n",
      "Iteration 159, loss = 0.50099548\n",
      "Iteration 160, loss = 0.49981312\n",
      "Iteration 161, loss = 0.49863874\n",
      "Iteration 162, loss = 0.49747222\n",
      "Iteration 163, loss = 0.49631345\n",
      "Iteration 164, loss = 0.49516232\n",
      "Iteration 165, loss = 0.49401872\n",
      "Iteration 166, loss = 0.49288309\n",
      "Iteration 167, loss = 0.49175984\n",
      "Iteration 168, loss = 0.49064493\n",
      "Iteration 169, loss = 0.48953946\n",
      "Iteration 170, loss = 0.48844287\n",
      "Iteration 171, loss = 0.48735379\n",
      "Iteration 172, loss = 0.48627208\n",
      "Iteration 173, loss = 0.48519743\n",
      "Iteration 174, loss = 0.48412903\n",
      "Iteration 175, loss = 0.48306865\n",
      "Iteration 176, loss = 0.48201586\n",
      "Iteration 177, loss = 0.48097152\n",
      "Iteration 178, loss = 0.47993506\n",
      "Iteration 179, loss = 0.47890547\n",
      "Iteration 180, loss = 0.47788157\n",
      "Iteration 181, loss = 0.47686327\n",
      "Iteration 182, loss = 0.47585045\n",
      "Iteration 183, loss = 0.47484305\n",
      "Iteration 184, loss = 0.47384373\n",
      "Iteration 185, loss = 0.47285321\n",
      "Iteration 186, loss = 0.47186817\n",
      "Iteration 187, loss = 0.47088840\n",
      "Iteration 188, loss = 0.46991397\n",
      "Iteration 189, loss = 0.46894533\n",
      "Iteration 190, loss = 0.46798189\n",
      "Iteration 191, loss = 0.46702514\n",
      "Iteration 192, loss = 0.46607873\n",
      "Iteration 193, loss = 0.46513811\n",
      "Iteration 194, loss = 0.46420261\n",
      "Iteration 195, loss = 0.46327216\n",
      "Iteration 196, loss = 0.46234670\n",
      "Iteration 197, loss = 0.46142617\n",
      "Iteration 198, loss = 0.46051050\n",
      "Iteration 199, loss = 0.45959965\n",
      "Iteration 200, loss = 0.45869374\n",
      "Iteration 201, loss = 0.45779338\n",
      "Iteration 202, loss = 0.45689772\n",
      "Iteration 203, loss = 0.45600667\n",
      "Iteration 204, loss = 0.45512018\n",
      "Iteration 205, loss = 0.45423820\n",
      "Iteration 206, loss = 0.45336067\n",
      "Iteration 207, loss = 0.45248755\n",
      "Iteration 208, loss = 0.45161878\n",
      "Iteration 209, loss = 0.45075455\n",
      "Iteration 210, loss = 0.44989440\n",
      "Iteration 211, loss = 0.44903807\n",
      "Iteration 212, loss = 0.44818609\n",
      "Iteration 213, loss = 0.44733805\n",
      "Iteration 214, loss = 0.44649390\n",
      "Iteration 215, loss = 0.44565424\n",
      "Iteration 216, loss = 0.44481895\n",
      "Iteration 217, loss = 0.44398741\n",
      "Iteration 218, loss = 0.44315958\n",
      "Iteration 219, loss = 0.44233540\n",
      "Iteration 220, loss = 0.44151485\n",
      "Iteration 221, loss = 0.44069786\n",
      "Iteration 222, loss = 0.43988440\n",
      "Iteration 223, loss = 0.43907440\n",
      "Iteration 224, loss = 0.43826783\n",
      "Iteration 225, loss = 0.43746462\n",
      "Iteration 226, loss = 0.43666471\n",
      "Iteration 227, loss = 0.43586806\n",
      "Iteration 228, loss = 0.43507461\n",
      "Iteration 229, loss = 0.43428430\n",
      "Iteration 230, loss = 0.43349712\n",
      "Iteration 231, loss = 0.43271299\n",
      "Iteration 232, loss = 0.43193183\n",
      "Iteration 233, loss = 0.43115360\n",
      "Iteration 234, loss = 0.43037825\n",
      "Iteration 235, loss = 0.42960578\n",
      "Iteration 236, loss = 0.42883595\n",
      "Iteration 237, loss = 0.42806889\n",
      "Iteration 238, loss = 0.42730450\n",
      "Iteration 239, loss = 0.42654271\n",
      "Iteration 240, loss = 0.42578348\n",
      "Iteration 241, loss = 0.42502676\n",
      "Iteration 242, loss = 0.42427249\n",
      "Iteration 243, loss = 0.42352063\n",
      "Iteration 244, loss = 0.42277113\n",
      "Iteration 245, loss = 0.42202394\n",
      "Iteration 246, loss = 0.42128018\n",
      "Iteration 247, loss = 0.42053922\n",
      "Iteration 248, loss = 0.41980095\n",
      "Iteration 249, loss = 0.41906502\n",
      "Iteration 250, loss = 0.41833122\n",
      "Iteration 251, loss = 0.41759957\n",
      "Iteration 252, loss = 0.41687004\n",
      "Iteration 253, loss = 0.41614338\n",
      "Iteration 254, loss = 0.41541885\n",
      "Iteration 255, loss = 0.41469643\n",
      "Iteration 256, loss = 0.41397611\n",
      "Iteration 257, loss = 0.41325786\n",
      "Iteration 258, loss = 0.41254165\n",
      "Iteration 259, loss = 0.41182746\n",
      "Iteration 260, loss = 0.41111527\n",
      "Iteration 261, loss = 0.41040504\n",
      "Iteration 262, loss = 0.40969766\n",
      "Iteration 263, loss = 0.40899250\n",
      "Iteration 264, loss = 0.40828928\n",
      "Iteration 265, loss = 0.40758764\n",
      "Iteration 266, loss = 0.40688798\n",
      "Iteration 267, loss = 0.40619009\n",
      "Iteration 268, loss = 0.40549397\n",
      "Iteration 269, loss = 0.40479960\n",
      "Iteration 270, loss = 0.40410695\n",
      "Iteration 271, loss = 0.40341600\n",
      "Iteration 272, loss = 0.40272672\n",
      "Iteration 273, loss = 0.40203908\n",
      "Iteration 274, loss = 0.40135305\n",
      "Iteration 275, loss = 0.40066862\n",
      "Iteration 276, loss = 0.39998571\n",
      "Iteration 277, loss = 0.39930434\n",
      "Iteration 278, loss = 0.39862512\n",
      "Iteration 279, loss = 0.39794745\n",
      "Iteration 280, loss = 0.39727126\n",
      "Iteration 281, loss = 0.39659652\n",
      "Iteration 282, loss = 0.39592320\n",
      "Iteration 283, loss = 0.39525127\n",
      "Iteration 284, loss = 0.39458071\n",
      "Iteration 285, loss = 0.39391147\n",
      "Iteration 286, loss = 0.39324384\n",
      "Iteration 287, loss = 0.39257777\n",
      "Iteration 288, loss = 0.39191289\n",
      "Iteration 289, loss = 0.39124929\n",
      "Iteration 290, loss = 0.39058739\n",
      "Iteration 291, loss = 0.38992698\n",
      "Iteration 292, loss = 0.38926780\n",
      "Iteration 293, loss = 0.38860985\n",
      "Iteration 294, loss = 0.38795308\n",
      "Iteration 295, loss = 0.38729752\n",
      "Iteration 296, loss = 0.38664318\n",
      "Iteration 297, loss = 0.38599019\n",
      "Iteration 298, loss = 0.38533819\n",
      "Iteration 299, loss = 0.38468738\n",
      "Iteration 300, loss = 0.38403819\n",
      "Iteration 301, loss = 0.38339014\n",
      "Iteration 302, loss = 0.38274315\n",
      "Iteration 303, loss = 0.38209720\n",
      "Iteration 304, loss = 0.38145227\n",
      "Iteration 305, loss = 0.38080835\n",
      "Iteration 306, loss = 0.38016541\n",
      "Iteration 307, loss = 0.37952347\n",
      "Iteration 308, loss = 0.37888248\n",
      "Iteration 309, loss = 0.37824244\n",
      "Iteration 310, loss = 0.37760355\n",
      "Iteration 311, loss = 0.37696581\n",
      "Iteration 312, loss = 0.37632941\n",
      "Iteration 313, loss = 0.37569431\n",
      "Iteration 314, loss = 0.37505991\n",
      "Iteration 315, loss = 0.37442684\n",
      "Iteration 316, loss = 0.37379609\n",
      "Iteration 317, loss = 0.37316609\n",
      "Iteration 318, loss = 0.37253686\n",
      "Iteration 319, loss = 0.37190845\n",
      "Iteration 320, loss = 0.37128085\n",
      "Iteration 321, loss = 0.37065410\n",
      "Iteration 322, loss = 0.37002821\n",
      "Iteration 323, loss = 0.36940318\n",
      "Iteration 324, loss = 0.36877903\n",
      "Iteration 325, loss = 0.36815576\n",
      "Iteration 326, loss = 0.36753338\n",
      "Iteration 327, loss = 0.36691188\n",
      "Iteration 328, loss = 0.36629126\n",
      "Iteration 329, loss = 0.36567152\n",
      "Iteration 330, loss = 0.36505266\n",
      "Iteration 331, loss = 0.36443465\n",
      "Iteration 332, loss = 0.36381754\n",
      "Iteration 333, loss = 0.36320121\n",
      "Iteration 334, loss = 0.36258576\n",
      "Iteration 335, loss = 0.36197112\n",
      "Iteration 336, loss = 0.36135730\n",
      "Iteration 337, loss = 0.36074427\n",
      "Iteration 338, loss = 0.36013203\n",
      "Iteration 339, loss = 0.35952055\n",
      "Iteration 340, loss = 0.35890984\n",
      "Iteration 341, loss = 0.35829986\n",
      "Iteration 342, loss = 0.35769071\n",
      "Iteration 343, loss = 0.35708230\n",
      "Iteration 344, loss = 0.35647462\n",
      "Iteration 345, loss = 0.35586763\n",
      "Iteration 346, loss = 0.35526134\n",
      "Iteration 347, loss = 0.35465593\n",
      "Iteration 348, loss = 0.35405120\n",
      "Iteration 349, loss = 0.35344715\n",
      "Iteration 350, loss = 0.35284376\n",
      "Iteration 351, loss = 0.35224102\n",
      "Iteration 352, loss = 0.35163892\n",
      "Iteration 353, loss = 0.35103745\n",
      "Iteration 354, loss = 0.35043660\n",
      "Iteration 355, loss = 0.34983638\n",
      "Iteration 356, loss = 0.34923674\n",
      "Iteration 357, loss = 0.34863771\n",
      "Iteration 358, loss = 0.34803928\n",
      "Iteration 359, loss = 0.34744165\n",
      "Iteration 360, loss = 0.34684471\n",
      "Iteration 361, loss = 0.34624837\n",
      "Iteration 362, loss = 0.34565262\n",
      "Iteration 363, loss = 0.34505745\n",
      "Iteration 364, loss = 0.34446380\n",
      "Iteration 365, loss = 0.34387075\n",
      "Iteration 366, loss = 0.34327831\n",
      "Iteration 367, loss = 0.34268647\n",
      "Iteration 368, loss = 0.34209524\n",
      "Iteration 369, loss = 0.34150465\n",
      "Iteration 370, loss = 0.34091558\n",
      "Iteration 371, loss = 0.34032899\n",
      "Iteration 372, loss = 0.33974282\n",
      "Iteration 373, loss = 0.33915730\n",
      "Iteration 374, loss = 0.33857223\n",
      "Iteration 375, loss = 0.33798754\n",
      "Iteration 376, loss = 0.33740300\n",
      "Iteration 377, loss = 0.33681657\n",
      "Iteration 378, loss = 0.33621648\n",
      "Iteration 379, loss = 0.33558812\n",
      "Iteration 380, loss = 0.33497346\n",
      "Iteration 381, loss = 0.33437179\n",
      "Iteration 382, loss = 0.33374116\n",
      "Iteration 383, loss = 0.33311045\n",
      "Iteration 384, loss = 0.33249698\n",
      "Iteration 385, loss = 0.33185214\n",
      "Iteration 386, loss = 0.33122089\n",
      "Iteration 387, loss = 0.33058223\n",
      "Iteration 388, loss = 0.32993207\n",
      "Iteration 389, loss = 0.32927535\n",
      "Iteration 390, loss = 0.32861990\n",
      "Iteration 391, loss = 0.32795465\n",
      "Iteration 392, loss = 0.32727893\n",
      "Iteration 393, loss = 0.32660456\n",
      "Iteration 394, loss = 0.32591975\n",
      "Iteration 395, loss = 0.32522325\n",
      "Iteration 396, loss = 0.32451862\n",
      "Iteration 397, loss = 0.32381547\n",
      "Iteration 398, loss = 0.32310121\n",
      "Iteration 399, loss = 0.32238531\n",
      "Iteration 400, loss = 0.32166090\n",
      "Iteration 401, loss = 0.32092778\n",
      "Iteration 402, loss = 0.32019174\n",
      "Iteration 403, loss = 0.31945078\n",
      "Iteration 404, loss = 0.31870277\n",
      "Iteration 405, loss = 0.31794784\n",
      "Iteration 406, loss = 0.31718659\n",
      "Iteration 407, loss = 0.31641881\n",
      "Iteration 408, loss = 0.31564660\n",
      "Iteration 409, loss = 0.31486856\n",
      "Iteration 410, loss = 0.31408417\n",
      "Iteration 411, loss = 0.31329358\n",
      "Iteration 412, loss = 0.31249698\n",
      "Iteration 413, loss = 0.31169580\n",
      "Iteration 414, loss = 0.31089137\n",
      "Iteration 415, loss = 0.31008096\n",
      "Iteration 416, loss = 0.30926671\n",
      "Iteration 417, loss = 0.30844575\n",
      "Iteration 418, loss = 0.30763099\n",
      "Iteration 419, loss = 0.30680035\n",
      "Iteration 420, loss = 0.30596627\n",
      "Iteration 421, loss = 0.30513521\n",
      "Iteration 422, loss = 0.30429647\n",
      "Iteration 423, loss = 0.30345035\n",
      "Iteration 424, loss = 0.30260547\n",
      "Iteration 425, loss = 0.30175748\n",
      "Iteration 426, loss = 0.30090619\n",
      "Iteration 427, loss = 0.30005150\n",
      "Iteration 428, loss = 0.29919492\n",
      "Iteration 429, loss = 0.29833572\n",
      "Iteration 430, loss = 0.29747340\n",
      "Iteration 431, loss = 0.29660883\n",
      "Iteration 432, loss = 0.29574119\n",
      "Iteration 433, loss = 0.29487049\n",
      "Iteration 434, loss = 0.29399786\n",
      "Iteration 435, loss = 0.29312309\n",
      "Iteration 436, loss = 0.29224677\n",
      "Iteration 437, loss = 0.29136582\n",
      "Iteration 438, loss = 0.29048331\n",
      "Iteration 439, loss = 0.28959934\n",
      "Iteration 440, loss = 0.28871359\n",
      "Iteration 441, loss = 0.28782887\n",
      "Iteration 442, loss = 0.28694210\n",
      "Iteration 443, loss = 0.28605179\n",
      "Iteration 444, loss = 0.28516046\n",
      "Iteration 445, loss = 0.28426960\n",
      "Iteration 446, loss = 0.28337876\n",
      "Iteration 447, loss = 0.28248755\n",
      "Iteration 448, loss = 0.28159396\n",
      "Iteration 449, loss = 0.28069930\n",
      "Iteration 450, loss = 0.27980332\n",
      "Iteration 451, loss = 0.27890781\n",
      "Iteration 452, loss = 0.27801289\n",
      "Iteration 453, loss = 0.27711702\n",
      "Iteration 454, loss = 0.27622151\n",
      "Iteration 455, loss = 0.27532595\n",
      "Iteration 456, loss = 0.27443473\n",
      "Iteration 457, loss = 0.27353984\n",
      "Iteration 458, loss = 0.27264123\n",
      "Iteration 459, loss = 0.27175190\n",
      "Iteration 460, loss = 0.27086091\n",
      "Iteration 461, loss = 0.26996705\n",
      "Iteration 462, loss = 0.26908044\n",
      "Iteration 463, loss = 0.26819343\n",
      "Iteration 464, loss = 0.26730559\n",
      "Iteration 465, loss = 0.26642271\n",
      "Iteration 466, loss = 0.26553855\n",
      "Iteration 467, loss = 0.26465323\n",
      "Iteration 468, loss = 0.26377244\n",
      "Iteration 469, loss = 0.26289283\n",
      "Iteration 470, loss = 0.26201616\n",
      "Iteration 471, loss = 0.26114034\n",
      "Iteration 472, loss = 0.26026731\n",
      "Iteration 473, loss = 0.25939651\n",
      "Iteration 474, loss = 0.25852755\n",
      "Iteration 475, loss = 0.25766063\n",
      "Iteration 476, loss = 0.25679580\n",
      "Iteration 477, loss = 0.25593321\n",
      "Iteration 478, loss = 0.25507282\n",
      "Iteration 479, loss = 0.25421522\n",
      "Iteration 480, loss = 0.25336050\n",
      "Iteration 481, loss = 0.25250799\n",
      "Iteration 482, loss = 0.25165851\n",
      "Iteration 483, loss = 0.25081098\n",
      "Iteration 484, loss = 0.24996622\n",
      "Iteration 485, loss = 0.24912499\n",
      "Iteration 486, loss = 0.24828863\n",
      "Iteration 487, loss = 0.24745424\n",
      "Iteration 488, loss = 0.24662354\n",
      "Iteration 489, loss = 0.24579563\n",
      "Iteration 490, loss = 0.24497078\n",
      "Iteration 491, loss = 0.24414881\n",
      "Iteration 492, loss = 0.24333348\n",
      "Iteration 493, loss = 0.24251604\n",
      "Iteration 494, loss = 0.24170519\n",
      "Iteration 495, loss = 0.24089726\n",
      "Iteration 496, loss = 0.24009245\n",
      "Iteration 497, loss = 0.23929159\n",
      "Iteration 498, loss = 0.23849306\n",
      "Iteration 499, loss = 0.23769854\n",
      "Iteration 500, loss = 0.23690722\n",
      "Iteration 501, loss = 0.23611980\n",
      "Iteration 502, loss = 0.23533560\n",
      "Iteration 503, loss = 0.23455510\n",
      "Iteration 504, loss = 0.23377780\n",
      "Iteration 505, loss = 0.23300622\n",
      "Iteration 506, loss = 0.23223551\n",
      "Iteration 507, loss = 0.23147116\n",
      "Iteration 508, loss = 0.23071006\n",
      "Iteration 509, loss = 0.22995154\n",
      "Iteration 510, loss = 0.22919578\n",
      "Iteration 511, loss = 0.22844335\n",
      "Iteration 512, loss = 0.22769841\n",
      "Iteration 513, loss = 0.22695445\n",
      "Iteration 514, loss = 0.22621198\n",
      "Iteration 515, loss = 0.22547653\n",
      "Iteration 516, loss = 0.22474536\n",
      "Iteration 517, loss = 0.22401639\n",
      "Iteration 518, loss = 0.22328979\n",
      "Iteration 519, loss = 0.22256627\n",
      "Iteration 520, loss = 0.22184790\n",
      "Iteration 521, loss = 0.22113399\n",
      "Iteration 522, loss = 0.22042144\n",
      "Iteration 523, loss = 0.21971541\n",
      "Iteration 524, loss = 0.21901343\n",
      "Iteration 525, loss = 0.21831385\n",
      "Iteration 526, loss = 0.21761671\n",
      "Iteration 527, loss = 0.21692291\n",
      "Iteration 528, loss = 0.21623485\n",
      "Iteration 529, loss = 0.21554987\n",
      "Iteration 530, loss = 0.21486691\n",
      "Iteration 531, loss = 0.21419057\n",
      "Iteration 532, loss = 0.21351698\n",
      "Iteration 533, loss = 0.21284555\n",
      "Iteration 534, loss = 0.21217675\n",
      "Iteration 535, loss = 0.21151247\n",
      "Iteration 536, loss = 0.21085327\n",
      "Iteration 537, loss = 0.21019601\n",
      "Iteration 538, loss = 0.20954405\n",
      "Iteration 539, loss = 0.20889468\n",
      "Iteration 540, loss = 0.20824790\n",
      "Iteration 541, loss = 0.20760423\n",
      "Iteration 542, loss = 0.20696838\n",
      "Iteration 543, loss = 0.20633062\n",
      "Iteration 544, loss = 0.20569952\n",
      "Iteration 545, loss = 0.20507370\n",
      "Iteration 546, loss = 0.20444941\n",
      "Iteration 547, loss = 0.20382662\n",
      "Iteration 548, loss = 0.20320797\n",
      "Iteration 549, loss = 0.20259652\n",
      "Iteration 550, loss = 0.20198480\n",
      "Iteration 551, loss = 0.20137551\n",
      "Iteration 552, loss = 0.20077377\n",
      "Iteration 553, loss = 0.20017430\n",
      "Iteration 554, loss = 0.19957683\n",
      "Iteration 555, loss = 0.19898204\n",
      "Iteration 556, loss = 0.19839594\n",
      "Iteration 557, loss = 0.19780795\n",
      "Iteration 558, loss = 0.19722205\n",
      "Iteration 559, loss = 0.19664324\n",
      "Iteration 560, loss = 0.19606628\n",
      "Iteration 561, loss = 0.19549124\n",
      "Iteration 562, loss = 0.19491990\n",
      "Iteration 563, loss = 0.19435505\n",
      "Iteration 564, loss = 0.19378921\n",
      "Iteration 565, loss = 0.19323016\n",
      "Iteration 566, loss = 0.19267454\n",
      "Iteration 567, loss = 0.19211974\n",
      "Iteration 568, loss = 0.19156688\n",
      "Iteration 569, loss = 0.19101952\n",
      "Iteration 570, loss = 0.19047493\n",
      "Iteration 571, loss = 0.18993144\n",
      "Iteration 572, loss = 0.18939384\n",
      "Iteration 573, loss = 0.18885789\n",
      "Iteration 574, loss = 0.18832596\n",
      "Iteration 575, loss = 0.18779640\n",
      "Iteration 576, loss = 0.18726982\n",
      "Iteration 577, loss = 0.18674677\n",
      "Iteration 578, loss = 0.18622666\n",
      "Iteration 579, loss = 0.18570913\n",
      "Iteration 580, loss = 0.18519426\n",
      "Iteration 581, loss = 0.18468325\n",
      "Iteration 582, loss = 0.18417506\n",
      "Iteration 583, loss = 0.18366898\n",
      "Iteration 584, loss = 0.18316555\n",
      "Iteration 585, loss = 0.18266579\n",
      "Iteration 586, loss = 0.18216868\n",
      "Iteration 587, loss = 0.18167415\n",
      "Iteration 588, loss = 0.18118306\n",
      "Iteration 589, loss = 0.18069434\n",
      "Iteration 590, loss = 0.18020794\n",
      "Iteration 591, loss = 0.17972489\n",
      "Iteration 592, loss = 0.17924441\n",
      "Iteration 593, loss = 0.17876657\n",
      "Iteration 594, loss = 0.17829170\n",
      "Iteration 595, loss = 0.17781951\n",
      "Iteration 596, loss = 0.17735054\n",
      "Iteration 597, loss = 0.17688330\n",
      "Iteration 598, loss = 0.17641851\n",
      "Iteration 599, loss = 0.17595700\n",
      "Iteration 600, loss = 0.17549807\n",
      "Iteration 601, loss = 0.17504176\n",
      "Iteration 602, loss = 0.17458793\n",
      "Iteration 603, loss = 0.17413658\n",
      "Iteration 604, loss = 0.17368769\n",
      "Iteration 605, loss = 0.17324129\n",
      "Iteration 606, loss = 0.17279802\n",
      "Iteration 607, loss = 0.17235731\n",
      "Iteration 608, loss = 0.17191832\n",
      "Iteration 609, loss = 0.17148273\n",
      "Iteration 610, loss = 0.17104931\n",
      "Iteration 611, loss = 0.17061822\n",
      "Iteration 612, loss = 0.17018943\n",
      "Iteration 613, loss = 0.16976299\n",
      "Iteration 614, loss = 0.16933976\n",
      "Iteration 615, loss = 0.16891764\n",
      "Iteration 616, loss = 0.16849933\n",
      "Iteration 617, loss = 0.16808296\n",
      "Iteration 618, loss = 0.16766892\n",
      "Iteration 619, loss = 0.16725634\n",
      "Iteration 620, loss = 0.16684750\n",
      "Iteration 621, loss = 0.16644085\n",
      "Iteration 622, loss = 0.16603496\n",
      "Iteration 623, loss = 0.16563248\n",
      "Iteration 624, loss = 0.16523282\n",
      "Iteration 625, loss = 0.16483484\n",
      "Iteration 626, loss = 0.16443769\n",
      "Iteration 627, loss = 0.16404385\n",
      "Iteration 628, loss = 0.16365278\n",
      "Iteration 629, loss = 0.16326252\n",
      "Iteration 630, loss = 0.16287532\n",
      "Iteration 631, loss = 0.16249125\n",
      "Iteration 632, loss = 0.16210806\n",
      "Iteration 633, loss = 0.16172688\n",
      "Iteration 634, loss = 0.16134771\n",
      "Iteration 635, loss = 0.16097227\n",
      "Iteration 636, loss = 0.16059814\n",
      "Iteration 637, loss = 0.16022493\n",
      "Iteration 638, loss = 0.15985472\n",
      "Iteration 639, loss = 0.15948719\n",
      "Iteration 640, loss = 0.15912102\n",
      "Iteration 641, loss = 0.15875621\n",
      "Iteration 642, loss = 0.15839368\n",
      "Iteration 643, loss = 0.15803395\n",
      "Iteration 644, loss = 0.15767550\n",
      "Iteration 645, loss = 0.15731931\n",
      "Iteration 646, loss = 0.15696543\n",
      "Iteration 647, loss = 0.15661306\n",
      "Iteration 648, loss = 0.15626262\n",
      "Iteration 649, loss = 0.15591425\n",
      "Iteration 650, loss = 0.15556739\n",
      "Iteration 651, loss = 0.15522262\n",
      "Iteration 652, loss = 0.15488001\n",
      "Iteration 653, loss = 0.15453889\n",
      "Iteration 654, loss = 0.15420037\n",
      "Iteration 655, loss = 0.15386265\n",
      "Iteration 656, loss = 0.15352666\n",
      "Iteration 657, loss = 0.15319299\n",
      "Iteration 658, loss = 0.15286092\n",
      "Iteration 659, loss = 0.15253018\n",
      "Iteration 660, loss = 0.15220159\n",
      "Iteration 661, loss = 0.15187444\n",
      "Iteration 662, loss = 0.15154904\n",
      "Iteration 663, loss = 0.15122540\n",
      "Iteration 664, loss = 0.15090350\n",
      "Iteration 665, loss = 0.15058352\n",
      "Iteration 666, loss = 0.15026495\n",
      "Iteration 667, loss = 0.14994789\n",
      "Iteration 668, loss = 0.14963137\n",
      "Iteration 669, loss = 0.14931652\n",
      "Iteration 670, loss = 0.14900263\n",
      "Iteration 671, loss = 0.14869084\n",
      "Iteration 672, loss = 0.14838052\n",
      "Iteration 673, loss = 0.14807141\n",
      "Iteration 674, loss = 0.14776364\n",
      "Iteration 675, loss = 0.14745781\n",
      "Iteration 676, loss = 0.14715350\n",
      "Iteration 677, loss = 0.14685065\n",
      "Iteration 678, loss = 0.14654958\n",
      "Iteration 679, loss = 0.14624998\n",
      "Iteration 680, loss = 0.14595182\n",
      "Iteration 681, loss = 0.14565521\n",
      "Iteration 682, loss = 0.14536069\n",
      "Iteration 683, loss = 0.14506800\n",
      "Iteration 684, loss = 0.14477712\n",
      "Iteration 685, loss = 0.14448778\n",
      "Iteration 686, loss = 0.14420003\n",
      "Iteration 687, loss = 0.14391385\n",
      "Iteration 688, loss = 0.14362923\n",
      "Iteration 689, loss = 0.14334550\n",
      "Iteration 690, loss = 0.14306304\n",
      "Iteration 691, loss = 0.14278157\n",
      "Iteration 692, loss = 0.14250303\n",
      "Iteration 693, loss = 0.14222672\n",
      "Iteration 694, loss = 0.14195199\n",
      "Iteration 695, loss = 0.14167920\n",
      "Iteration 696, loss = 0.14140809\n",
      "Iteration 697, loss = 0.14113855\n",
      "Iteration 698, loss = 0.14087056\n",
      "Iteration 699, loss = 0.14060407\n",
      "Iteration 700, loss = 0.14033905\n",
      "Iteration 701, loss = 0.14007549\n",
      "Iteration 702, loss = 0.13981338\n",
      "Iteration 703, loss = 0.13955272\n",
      "Iteration 704, loss = 0.13929336\n",
      "Iteration 705, loss = 0.13903527\n",
      "Iteration 706, loss = 0.13877855\n",
      "Iteration 707, loss = 0.13852223\n",
      "Iteration 708, loss = 0.13826700\n",
      "Iteration 709, loss = 0.13801224\n",
      "Iteration 710, loss = 0.13775811\n",
      "Iteration 711, loss = 0.13750505\n",
      "Iteration 712, loss = 0.13725323\n",
      "Iteration 713, loss = 0.13700300\n",
      "Iteration 714, loss = 0.13675403\n",
      "Iteration 715, loss = 0.13650654\n",
      "Iteration 716, loss = 0.13626027\n",
      "Iteration 717, loss = 0.13601514\n",
      "Iteration 718, loss = 0.13577100\n",
      "Iteration 719, loss = 0.13552806\n",
      "Iteration 720, loss = 0.13528636\n",
      "Iteration 721, loss = 0.13504738\n",
      "Iteration 722, loss = 0.13481103\n",
      "Iteration 723, loss = 0.13457578\n",
      "Iteration 724, loss = 0.13434188\n",
      "Iteration 725, loss = 0.13410940\n",
      "Iteration 726, loss = 0.13387817\n",
      "Iteration 727, loss = 0.13364843\n",
      "Iteration 728, loss = 0.13342039\n",
      "Iteration 729, loss = 0.13319356\n",
      "Iteration 730, loss = 0.13296787\n",
      "Iteration 731, loss = 0.13274336\n",
      "Iteration 732, loss = 0.13252005\n",
      "Iteration 733, loss = 0.13229863\n",
      "Iteration 734, loss = 0.13207869\n",
      "Iteration 735, loss = 0.13185993\n",
      "Iteration 736, loss = 0.13164229\n",
      "Iteration 737, loss = 0.13142547\n",
      "Iteration 738, loss = 0.13120975\n",
      "Iteration 739, loss = 0.13099514\n",
      "Iteration 740, loss = 0.13078141\n",
      "Iteration 741, loss = 0.13056836\n",
      "Iteration 742, loss = 0.13035633\n",
      "Iteration 743, loss = 0.13014554\n",
      "Iteration 744, loss = 0.12993601\n",
      "Iteration 745, loss = 0.12972761\n",
      "Iteration 746, loss = 0.12952045\n",
      "Iteration 747, loss = 0.12931442\n",
      "Iteration 748, loss = 0.12910943\n",
      "Iteration 749, loss = 0.12890547\n",
      "Iteration 750, loss = 0.12870252\n",
      "Iteration 751, loss = 0.12850060\n",
      "Iteration 752, loss = 0.12829974\n",
      "Iteration 753, loss = 0.12810001\n",
      "Iteration 754, loss = 0.12790132\n",
      "Iteration 755, loss = 0.12770358\n",
      "Iteration 756, loss = 0.12750686\n",
      "Iteration 757, loss = 0.12731108\n",
      "Iteration 758, loss = 0.12711623\n",
      "Iteration 759, loss = 0.12692234\n",
      "Iteration 760, loss = 0.12672946\n",
      "Iteration 761, loss = 0.12653758\n",
      "Iteration 762, loss = 0.12634662\n",
      "Iteration 763, loss = 0.12615661\n",
      "Iteration 764, loss = 0.12596752\n",
      "Iteration 765, loss = 0.12577933\n",
      "Iteration 766, loss = 0.12559216\n",
      "Iteration 767, loss = 0.12540596\n",
      "Iteration 768, loss = 0.12522065\n",
      "Iteration 769, loss = 0.12503621\n",
      "Iteration 770, loss = 0.12485265\n",
      "Iteration 771, loss = 0.12466999\n",
      "Iteration 772, loss = 0.12448819\n",
      "Iteration 773, loss = 0.12430725\n",
      "Iteration 774, loss = 0.12412725\n",
      "Iteration 775, loss = 0.12394807\n",
      "Iteration 776, loss = 0.12376972\n",
      "Iteration 777, loss = 0.12359220\n",
      "Iteration 778, loss = 0.12341550\n",
      "Iteration 779, loss = 0.12323962\n",
      "Iteration 780, loss = 0.12306456\n",
      "Iteration 781, loss = 0.12289032\n",
      "Iteration 782, loss = 0.12271690\n",
      "Iteration 783, loss = 0.12254428\n",
      "Iteration 784, loss = 0.12237246\n",
      "Iteration 785, loss = 0.12220136\n",
      "Iteration 786, loss = 0.12203101\n",
      "Iteration 787, loss = 0.12186146\n",
      "Iteration 788, loss = 0.12169276\n",
      "Iteration 789, loss = 0.12152483\n",
      "Iteration 790, loss = 0.12135764\n",
      "Iteration 791, loss = 0.12119125\n",
      "Iteration 792, loss = 0.12102546\n",
      "Iteration 793, loss = 0.12086033\n",
      "Iteration 794, loss = 0.12069585\n",
      "Iteration 795, loss = 0.12053204\n",
      "Iteration 796, loss = 0.12036895\n",
      "Iteration 797, loss = 0.12020655\n",
      "Iteration 798, loss = 0.12004485\n",
      "Iteration 799, loss = 0.11988383\n",
      "Iteration 800, loss = 0.11972353\n",
      "Iteration 801, loss = 0.11956394\n",
      "Iteration 802, loss = 0.11940502\n",
      "Iteration 803, loss = 0.11924683\n",
      "Iteration 804, loss = 0.11908930\n",
      "Iteration 805, loss = 0.11893250\n",
      "Iteration 806, loss = 0.11877634\n",
      "Iteration 807, loss = 0.11862091\n",
      "Iteration 808, loss = 0.11846615\n",
      "Iteration 809, loss = 0.11831208\n",
      "Iteration 810, loss = 0.11815868\n",
      "Iteration 811, loss = 0.11800614\n",
      "Iteration 812, loss = 0.11785417\n",
      "Iteration 813, loss = 0.11770323\n",
      "Iteration 814, loss = 0.11755288\n",
      "Iteration 815, loss = 0.11740302\n",
      "Iteration 816, loss = 0.11725414\n",
      "Iteration 817, loss = 0.11710587\n",
      "Iteration 818, loss = 0.11695830\n",
      "Iteration 819, loss = 0.11681153\n",
      "Iteration 820, loss = 0.11666554\n",
      "Iteration 821, loss = 0.11652019\n",
      "Iteration 822, loss = 0.11637564\n",
      "Iteration 823, loss = 0.11623180\n",
      "Iteration 824, loss = 0.11608861\n",
      "Iteration 825, loss = 0.11594597\n",
      "Iteration 826, loss = 0.11580400\n",
      "Iteration 827, loss = 0.11566272\n",
      "Iteration 828, loss = 0.11552202\n",
      "Iteration 829, loss = 0.11538194\n",
      "Iteration 830, loss = 0.11524251\n",
      "Iteration 831, loss = 0.11510373\n",
      "Iteration 832, loss = 0.11496571\n",
      "Iteration 833, loss = 0.11482801\n",
      "Iteration 834, loss = 0.11469108\n",
      "Iteration 835, loss = 0.11455468\n",
      "Iteration 836, loss = 0.11441876\n",
      "Iteration 837, loss = 0.11428358\n",
      "Iteration 838, loss = 0.11414880\n",
      "Iteration 839, loss = 0.11401475\n",
      "Iteration 840, loss = 0.11388128\n",
      "Iteration 841, loss = 0.11374824\n",
      "Iteration 842, loss = 0.11361573\n",
      "Iteration 843, loss = 0.11348393\n",
      "Iteration 844, loss = 0.11335264\n",
      "Iteration 845, loss = 0.11322186\n",
      "Iteration 846, loss = 0.11309151\n",
      "Iteration 847, loss = 0.11296163\n",
      "Iteration 848, loss = 0.11283222\n",
      "Iteration 849, loss = 0.11270339\n",
      "Iteration 850, loss = 0.11257504\n",
      "Iteration 851, loss = 0.11244718\n",
      "Iteration 852, loss = 0.11231987\n",
      "Iteration 853, loss = 0.11219314\n",
      "Iteration 854, loss = 0.11206692\n",
      "Iteration 855, loss = 0.11194110\n",
      "Iteration 856, loss = 0.11181571\n",
      "Iteration 857, loss = 0.11169080\n",
      "Iteration 858, loss = 0.11156643\n",
      "Iteration 859, loss = 0.11144250\n",
      "Iteration 860, loss = 0.11131907\n",
      "Iteration 861, loss = 0.11119613\n",
      "Iteration 862, loss = 0.11107375\n",
      "Iteration 863, loss = 0.11095193\n",
      "Iteration 864, loss = 0.11083058\n",
      "Iteration 865, loss = 0.11070964\n",
      "Iteration 866, loss = 0.11058924\n",
      "Iteration 867, loss = 0.11046936\n",
      "Iteration 868, loss = 0.11035004\n",
      "Iteration 869, loss = 0.11023119\n",
      "Iteration 870, loss = 0.11011281\n",
      "Iteration 871, loss = 0.10999496\n",
      "Iteration 872, loss = 0.10987759\n",
      "Iteration 873, loss = 0.10976023\n",
      "Iteration 874, loss = 0.10964326\n",
      "Iteration 875, loss = 0.10952677\n",
      "Iteration 876, loss = 0.10941072\n",
      "Iteration 877, loss = 0.10929523\n",
      "Iteration 878, loss = 0.10918023\n",
      "Iteration 879, loss = 0.10906570\n",
      "Iteration 880, loss = 0.10895159\n",
      "Iteration 881, loss = 0.10883793\n",
      "Iteration 882, loss = 0.10872474\n",
      "Iteration 883, loss = 0.10861198\n",
      "Iteration 884, loss = 0.10849968\n",
      "Iteration 885, loss = 0.10838779\n",
      "Iteration 886, loss = 0.10827636\n",
      "Iteration 887, loss = 0.10816536\n",
      "Iteration 888, loss = 0.10805481\n",
      "Iteration 889, loss = 0.10794469\n",
      "Iteration 890, loss = 0.10783501\n",
      "Iteration 891, loss = 0.10772573\n",
      "Iteration 892, loss = 0.10761703\n",
      "Iteration 893, loss = 0.10750870\n",
      "Iteration 894, loss = 0.10740082\n",
      "Iteration 895, loss = 0.10729339\n",
      "Iteration 896, loss = 0.10718624\n",
      "Iteration 897, loss = 0.10707957\n",
      "Iteration 898, loss = 0.10697329\n",
      "Iteration 899, loss = 0.10686741\n",
      "Iteration 900, loss = 0.10676201\n",
      "Iteration 901, loss = 0.10665699\n",
      "Iteration 902, loss = 0.10655241\n",
      "Iteration 903, loss = 0.10644817\n",
      "Iteration 904, loss = 0.10634443\n",
      "Iteration 905, loss = 0.10624097\n",
      "Iteration 906, loss = 0.10613789\n",
      "Iteration 907, loss = 0.10603511\n",
      "Iteration 908, loss = 0.10593291\n",
      "Iteration 909, loss = 0.10583096\n",
      "Iteration 910, loss = 0.10572923\n",
      "Iteration 911, loss = 0.10562827\n",
      "Iteration 912, loss = 0.10552813\n",
      "Iteration 913, loss = 0.10542911\n",
      "Iteration 914, loss = 0.10533031\n",
      "Iteration 915, loss = 0.10523246\n",
      "Iteration 916, loss = 0.10513486\n",
      "Iteration 917, loss = 0.10503747\n",
      "Iteration 918, loss = 0.10494030\n",
      "Iteration 919, loss = 0.10484339\n",
      "Iteration 920, loss = 0.10474734\n",
      "Iteration 921, loss = 0.10465152\n",
      "Iteration 922, loss = 0.10455589\n",
      "Iteration 923, loss = 0.10446077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35303276\n",
      "Iteration 2, loss = 1.32593075\n",
      "Iteration 3, loss = 1.29931942\n",
      "Iteration 4, loss = 1.27320119\n",
      "Iteration 5, loss = 1.24759210\n",
      "Iteration 6, loss = 1.22257009\n",
      "Iteration 7, loss = 1.19816746\n",
      "Iteration 8, loss = 1.17437665\n",
      "Iteration 9, loss = 1.15114735\n",
      "Iteration 10, loss = 1.12853993\n",
      "Iteration 11, loss = 1.10658865\n",
      "Iteration 12, loss = 1.08531855\n",
      "Iteration 13, loss = 1.06474805\n",
      "Iteration 14, loss = 1.04486888\n",
      "Iteration 15, loss = 1.02570883\n",
      "Iteration 16, loss = 1.00723940\n",
      "Iteration 17, loss = 0.98949685\n",
      "Iteration 18, loss = 0.97246602\n",
      "Iteration 19, loss = 0.95614953\n",
      "Iteration 20, loss = 0.94053662\n",
      "Iteration 21, loss = 0.92562381\n",
      "Iteration 22, loss = 0.91139502\n",
      "Iteration 23, loss = 0.89782857\n",
      "Iteration 24, loss = 0.88494378\n",
      "Iteration 25, loss = 0.87270108\n",
      "Iteration 26, loss = 0.86112299\n",
      "Iteration 27, loss = 0.85016694\n",
      "Iteration 28, loss = 0.83983702\n",
      "Iteration 29, loss = 0.83011009\n",
      "Iteration 30, loss = 0.82093505\n",
      "Iteration 31, loss = 0.81233203\n",
      "Iteration 32, loss = 0.80425081\n",
      "Iteration 33, loss = 0.79666166\n",
      "Iteration 34, loss = 0.78959179\n",
      "Iteration 35, loss = 0.78301744\n",
      "Iteration 36, loss = 0.77687764\n",
      "Iteration 37, loss = 0.77114880\n",
      "Iteration 38, loss = 0.76580676\n",
      "Iteration 39, loss = 0.76083203\n",
      "Iteration 40, loss = 0.75627977\n",
      "Iteration 41, loss = 0.75209367\n",
      "Iteration 42, loss = 0.74817328\n",
      "Iteration 43, loss = 0.74453853\n",
      "Iteration 44, loss = 0.74110838\n",
      "Iteration 45, loss = 0.73792378\n",
      "Iteration 46, loss = 0.73485162\n",
      "Iteration 47, loss = 0.73189856\n",
      "Iteration 48, loss = 0.72906016\n",
      "Iteration 49, loss = 0.72629502\n",
      "Iteration 50, loss = 0.72358639\n",
      "Iteration 51, loss = 0.72095585\n",
      "Iteration 52, loss = 0.71837734\n",
      "Iteration 53, loss = 0.71581030\n",
      "Iteration 54, loss = 0.71327262\n",
      "Iteration 55, loss = 0.71071676\n",
      "Iteration 56, loss = 0.70813082\n",
      "Iteration 57, loss = 0.70552613\n",
      "Iteration 58, loss = 0.70290798\n",
      "Iteration 59, loss = 0.70026682\n",
      "Iteration 60, loss = 0.69760516\n",
      "Iteration 61, loss = 0.69493166\n",
      "Iteration 62, loss = 0.69224621\n",
      "Iteration 63, loss = 0.68952912\n",
      "Iteration 64, loss = 0.68677833\n",
      "Iteration 65, loss = 0.68395838\n",
      "Iteration 66, loss = 0.68108072\n",
      "Iteration 67, loss = 0.67815100\n",
      "Iteration 68, loss = 0.67520185\n",
      "Iteration 69, loss = 0.67227842\n",
      "Iteration 70, loss = 0.66936551\n",
      "Iteration 71, loss = 0.66646278\n",
      "Iteration 72, loss = 0.66360708\n",
      "Iteration 73, loss = 0.66080377\n",
      "Iteration 74, loss = 0.65810407\n",
      "Iteration 75, loss = 0.65546919\n",
      "Iteration 76, loss = 0.65286365\n",
      "Iteration 77, loss = 0.65033282\n",
      "Iteration 78, loss = 0.64789028\n",
      "Iteration 79, loss = 0.64552162\n",
      "Iteration 80, loss = 0.64325066\n",
      "Iteration 81, loss = 0.64106114\n",
      "Iteration 82, loss = 0.63890186\n",
      "Iteration 83, loss = 0.63674905\n",
      "Iteration 84, loss = 0.63461162\n",
      "Iteration 85, loss = 0.63247505\n",
      "Iteration 86, loss = 0.63035348\n",
      "Iteration 87, loss = 0.62818321\n",
      "Iteration 88, loss = 0.62598479\n",
      "Iteration 89, loss = 0.62375693\n",
      "Iteration 90, loss = 0.62150605\n",
      "Iteration 91, loss = 0.61922333\n",
      "Iteration 92, loss = 0.61688852\n",
      "Iteration 93, loss = 0.61453929\n",
      "Iteration 94, loss = 0.61215732\n",
      "Iteration 95, loss = 0.60977194\n",
      "Iteration 96, loss = 0.60738889\n",
      "Iteration 97, loss = 0.60501233\n",
      "Iteration 98, loss = 0.60264326\n",
      "Iteration 99, loss = 0.60028198\n",
      "Iteration 100, loss = 0.59792980\n",
      "Iteration 101, loss = 0.59558838\n",
      "Iteration 102, loss = 0.59326073\n",
      "Iteration 103, loss = 0.59094478\n",
      "Iteration 104, loss = 0.58864083\n",
      "Iteration 105, loss = 0.58633271\n",
      "Iteration 106, loss = 0.58403490\n",
      "Iteration 107, loss = 0.58175037\n",
      "Iteration 108, loss = 0.57947805\n",
      "Iteration 109, loss = 0.57721789\n",
      "Iteration 110, loss = 0.57497019\n",
      "Iteration 111, loss = 0.57273572\n",
      "Iteration 112, loss = 0.57051446\n",
      "Iteration 113, loss = 0.56830610\n",
      "Iteration 114, loss = 0.56611087\n",
      "Iteration 115, loss = 0.56392899\n",
      "Iteration 116, loss = 0.56176167\n",
      "Iteration 117, loss = 0.55960867\n",
      "Iteration 118, loss = 0.55746999\n",
      "Iteration 119, loss = 0.55534588\n",
      "Iteration 120, loss = 0.55323658\n",
      "Iteration 121, loss = 0.55114723\n",
      "Iteration 122, loss = 0.54909611\n",
      "Iteration 123, loss = 0.54718864\n",
      "Iteration 124, loss = 0.54540364\n",
      "Iteration 125, loss = 0.54365888\n",
      "Iteration 126, loss = 0.54201989\n",
      "Iteration 127, loss = 0.54051379\n",
      "Iteration 128, loss = 0.53903374\n",
      "Iteration 129, loss = 0.53755068\n",
      "Iteration 130, loss = 0.53604042\n",
      "Iteration 131, loss = 0.53449880\n",
      "Iteration 132, loss = 0.53292889\n",
      "Iteration 133, loss = 0.53133963\n",
      "Iteration 134, loss = 0.52973754\n",
      "Iteration 135, loss = 0.52813980\n",
      "Iteration 136, loss = 0.52654584\n",
      "Iteration 137, loss = 0.52498489\n",
      "Iteration 138, loss = 0.52345631\n",
      "Iteration 139, loss = 0.52198161\n",
      "Iteration 140, loss = 0.52054481\n",
      "Iteration 141, loss = 0.51914087\n",
      "Iteration 142, loss = 0.51776337\n",
      "Iteration 143, loss = 0.51639269\n",
      "Iteration 144, loss = 0.51502649\n",
      "Iteration 145, loss = 0.51366501\n",
      "Iteration 146, loss = 0.51231064\n",
      "Iteration 147, loss = 0.51096342\n",
      "Iteration 148, loss = 0.50962280\n",
      "Iteration 149, loss = 0.50828814\n",
      "Iteration 150, loss = 0.50695979\n",
      "Iteration 151, loss = 0.50563812\n",
      "Iteration 152, loss = 0.50432345\n",
      "Iteration 153, loss = 0.50301609\n",
      "Iteration 154, loss = 0.50172364\n",
      "Iteration 155, loss = 0.50044530\n",
      "Iteration 156, loss = 0.49918864\n",
      "Iteration 157, loss = 0.49794677\n",
      "Iteration 158, loss = 0.49671643\n",
      "Iteration 159, loss = 0.49549393\n",
      "Iteration 160, loss = 0.49427785\n",
      "Iteration 161, loss = 0.49306658\n",
      "Iteration 162, loss = 0.49185942\n",
      "Iteration 163, loss = 0.49065778\n",
      "Iteration 164, loss = 0.48946232\n",
      "Iteration 165, loss = 0.48827801\n",
      "Iteration 166, loss = 0.48710488\n",
      "Iteration 167, loss = 0.48594789\n",
      "Iteration 168, loss = 0.48480281\n",
      "Iteration 169, loss = 0.48367068\n",
      "Iteration 170, loss = 0.48254482\n",
      "Iteration 171, loss = 0.48142520\n",
      "Iteration 172, loss = 0.48031295\n",
      "Iteration 173, loss = 0.47920900\n",
      "Iteration 174, loss = 0.47811124\n",
      "Iteration 175, loss = 0.47702001\n",
      "Iteration 176, loss = 0.47593632\n",
      "Iteration 177, loss = 0.47486285\n",
      "Iteration 178, loss = 0.47379835\n",
      "Iteration 179, loss = 0.47274320\n",
      "Iteration 180, loss = 0.47169319\n",
      "Iteration 181, loss = 0.47064718\n",
      "Iteration 182, loss = 0.46960530\n",
      "Iteration 183, loss = 0.46856795\n",
      "Iteration 184, loss = 0.46754656\n",
      "Iteration 185, loss = 0.46653040\n",
      "Iteration 186, loss = 0.46551866\n",
      "Iteration 187, loss = 0.46451154\n",
      "Iteration 188, loss = 0.46350896\n",
      "Iteration 189, loss = 0.46251119\n",
      "Iteration 190, loss = 0.46152122\n",
      "Iteration 191, loss = 0.46053837\n",
      "Iteration 192, loss = 0.45956100\n",
      "Iteration 193, loss = 0.45858952\n",
      "Iteration 194, loss = 0.45762252\n",
      "Iteration 195, loss = 0.45666068\n",
      "Iteration 196, loss = 0.45570446\n",
      "Iteration 197, loss = 0.45475325\n",
      "Iteration 198, loss = 0.45380694\n",
      "Iteration 199, loss = 0.45286545\n",
      "Iteration 200, loss = 0.45192871\n",
      "Iteration 201, loss = 0.45099664\n",
      "Iteration 202, loss = 0.45006917\n",
      "Iteration 203, loss = 0.44914670\n",
      "Iteration 204, loss = 0.44822930\n",
      "Iteration 205, loss = 0.44731621\n",
      "Iteration 206, loss = 0.44640742\n",
      "Iteration 207, loss = 0.44550282\n",
      "Iteration 208, loss = 0.44460244\n",
      "Iteration 209, loss = 0.44370619\n",
      "Iteration 210, loss = 0.44281402\n",
      "Iteration 211, loss = 0.44192588\n",
      "Iteration 212, loss = 0.44104172\n",
      "Iteration 213, loss = 0.44016148\n",
      "Iteration 214, loss = 0.43928509\n",
      "Iteration 215, loss = 0.43841286\n",
      "Iteration 216, loss = 0.43754429\n",
      "Iteration 217, loss = 0.43667909\n",
      "Iteration 218, loss = 0.43581732\n",
      "Iteration 219, loss = 0.43495943\n",
      "Iteration 220, loss = 0.43410496\n",
      "Iteration 221, loss = 0.43325394\n",
      "Iteration 222, loss = 0.43240677\n",
      "Iteration 223, loss = 0.43156257\n",
      "Iteration 224, loss = 0.43072133\n",
      "Iteration 225, loss = 0.42988461\n",
      "Iteration 226, loss = 0.42905200\n",
      "Iteration 227, loss = 0.42822199\n",
      "Iteration 228, loss = 0.42739539\n",
      "Iteration 229, loss = 0.42657244\n",
      "Iteration 230, loss = 0.42575253\n",
      "Iteration 231, loss = 0.42493561\n",
      "Iteration 232, loss = 0.42412160\n",
      "Iteration 233, loss = 0.42331045\n",
      "Iteration 234, loss = 0.42250211\n",
      "Iteration 235, loss = 0.42169650\n",
      "Iteration 236, loss = 0.42089358\n",
      "Iteration 237, loss = 0.42009330\n",
      "Iteration 238, loss = 0.41929578\n",
      "Iteration 239, loss = 0.41850134\n",
      "Iteration 240, loss = 0.41770960\n",
      "Iteration 241, loss = 0.41692022\n",
      "Iteration 242, loss = 0.41613338\n",
      "Iteration 243, loss = 0.41534908\n",
      "Iteration 244, loss = 0.41456832\n",
      "Iteration 245, loss = 0.41378975\n",
      "Iteration 246, loss = 0.41301346\n",
      "Iteration 247, loss = 0.41223959\n",
      "Iteration 248, loss = 0.41146793\n",
      "Iteration 249, loss = 0.41069861\n",
      "Iteration 250, loss = 0.40993145\n",
      "Iteration 251, loss = 0.40916648\n",
      "Iteration 252, loss = 0.40840375\n",
      "Iteration 253, loss = 0.40764306\n",
      "Iteration 254, loss = 0.40688492\n",
      "Iteration 255, loss = 0.40612905\n",
      "Iteration 256, loss = 0.40537523\n",
      "Iteration 257, loss = 0.40462353\n",
      "Iteration 258, loss = 0.40387376\n",
      "Iteration 259, loss = 0.40312591\n",
      "Iteration 260, loss = 0.40237995\n",
      "Iteration 261, loss = 0.40163595\n",
      "Iteration 262, loss = 0.40089368\n",
      "Iteration 263, loss = 0.40015330\n",
      "Iteration 264, loss = 0.39941470\n",
      "Iteration 265, loss = 0.39867788\n",
      "Iteration 266, loss = 0.39794272\n",
      "Iteration 267, loss = 0.39720929\n",
      "Iteration 268, loss = 0.39647751\n",
      "Iteration 269, loss = 0.39574734\n",
      "Iteration 270, loss = 0.39501922\n",
      "Iteration 271, loss = 0.39429290\n",
      "Iteration 272, loss = 0.39356817\n",
      "Iteration 273, loss = 0.39284509\n",
      "Iteration 274, loss = 0.39212395\n",
      "Iteration 275, loss = 0.39140434\n",
      "Iteration 276, loss = 0.39068622\n",
      "Iteration 277, loss = 0.38996958\n",
      "Iteration 278, loss = 0.38925440\n",
      "Iteration 279, loss = 0.38854063\n",
      "Iteration 280, loss = 0.38782823\n",
      "Iteration 281, loss = 0.38711755\n",
      "Iteration 282, loss = 0.38640836\n",
      "Iteration 283, loss = 0.38570051\n",
      "Iteration 284, loss = 0.38499396\n",
      "Iteration 285, loss = 0.38428868\n",
      "Iteration 286, loss = 0.38358467\n",
      "Iteration 287, loss = 0.38288187\n",
      "Iteration 288, loss = 0.38218028\n",
      "Iteration 289, loss = 0.38147987\n",
      "Iteration 290, loss = 0.38078062\n",
      "Iteration 291, loss = 0.38008249\n",
      "Iteration 292, loss = 0.37938547\n",
      "Iteration 293, loss = 0.37869148\n",
      "Iteration 294, loss = 0.37799860\n",
      "Iteration 295, loss = 0.37730645\n",
      "Iteration 296, loss = 0.37661522\n",
      "Iteration 297, loss = 0.37592497\n",
      "Iteration 298, loss = 0.37523561\n",
      "Iteration 299, loss = 0.37454731\n",
      "Iteration 300, loss = 0.37386018\n",
      "Iteration 301, loss = 0.37317514\n",
      "Iteration 302, loss = 0.37249240\n",
      "Iteration 303, loss = 0.37181058\n",
      "Iteration 304, loss = 0.37112968\n",
      "Iteration 305, loss = 0.37044971\n",
      "Iteration 306, loss = 0.36977068\n",
      "Iteration 307, loss = 0.36909259\n",
      "Iteration 308, loss = 0.36841545\n",
      "Iteration 309, loss = 0.36773926\n",
      "Iteration 310, loss = 0.36706406\n",
      "Iteration 311, loss = 0.36638983\n",
      "Iteration 312, loss = 0.36571660\n",
      "Iteration 313, loss = 0.36504436\n",
      "Iteration 314, loss = 0.36437312\n",
      "Iteration 315, loss = 0.36370288\n",
      "Iteration 316, loss = 0.36303363\n",
      "Iteration 317, loss = 0.36236592\n",
      "Iteration 318, loss = 0.36169888\n",
      "Iteration 319, loss = 0.36103241\n",
      "Iteration 320, loss = 0.36036670\n",
      "Iteration 321, loss = 0.35970297\n",
      "Iteration 322, loss = 0.35903874\n",
      "Iteration 323, loss = 0.35837608\n",
      "Iteration 324, loss = 0.35771424\n",
      "Iteration 325, loss = 0.35705327\n",
      "Iteration 326, loss = 0.35639314\n",
      "Iteration 327, loss = 0.35573382\n",
      "Iteration 328, loss = 0.35507529\n",
      "Iteration 329, loss = 0.35441756\n",
      "Iteration 330, loss = 0.35376061\n",
      "Iteration 331, loss = 0.35310443\n",
      "Iteration 332, loss = 0.35244902\n",
      "Iteration 333, loss = 0.35179436\n",
      "Iteration 334, loss = 0.35114044\n",
      "Iteration 335, loss = 0.35048758\n",
      "Iteration 336, loss = 0.34983535\n",
      "Iteration 337, loss = 0.34918375\n",
      "Iteration 338, loss = 0.34853297\n",
      "Iteration 339, loss = 0.34788294\n",
      "Iteration 340, loss = 0.34723366\n",
      "Iteration 341, loss = 0.34658531\n",
      "Iteration 342, loss = 0.34593762\n",
      "Iteration 343, loss = 0.34529058\n",
      "Iteration 344, loss = 0.34464419\n",
      "Iteration 345, loss = 0.34399842\n",
      "Iteration 346, loss = 0.34335343\n",
      "Iteration 347, loss = 0.34270911\n",
      "Iteration 348, loss = 0.34206540\n",
      "Iteration 349, loss = 0.34142229\n",
      "Iteration 350, loss = 0.34077978\n",
      "Iteration 351, loss = 0.34013811\n",
      "Iteration 352, loss = 0.33949732\n",
      "Iteration 353, loss = 0.33885711\n",
      "Iteration 354, loss = 0.33821758\n",
      "Iteration 355, loss = 0.33757885\n",
      "Iteration 356, loss = 0.33694072\n",
      "Iteration 357, loss = 0.33630333\n",
      "Iteration 358, loss = 0.33566820\n",
      "Iteration 359, loss = 0.33503439\n",
      "Iteration 360, loss = 0.33440090\n",
      "Iteration 361, loss = 0.33376782\n",
      "Iteration 362, loss = 0.33313517\n",
      "Iteration 363, loss = 0.33250298\n",
      "Iteration 364, loss = 0.33187130\n",
      "Iteration 365, loss = 0.33124013\n",
      "Iteration 366, loss = 0.33060953\n",
      "Iteration 367, loss = 0.32997951\n",
      "Iteration 368, loss = 0.32935007\n",
      "Iteration 369, loss = 0.32872125\n",
      "Iteration 370, loss = 0.32809370\n",
      "Iteration 371, loss = 0.32746745\n",
      "Iteration 372, loss = 0.32684154\n",
      "Iteration 373, loss = 0.32621606\n",
      "Iteration 374, loss = 0.32559115\n",
      "Iteration 375, loss = 0.32496673\n",
      "Iteration 376, loss = 0.32434302\n",
      "Iteration 377, loss = 0.32372119\n",
      "Iteration 378, loss = 0.32310052\n",
      "Iteration 379, loss = 0.32248043\n",
      "Iteration 380, loss = 0.32186089\n",
      "Iteration 381, loss = 0.32124189\n",
      "Iteration 382, loss = 0.32062299\n",
      "Iteration 383, loss = 0.31999661\n",
      "Iteration 384, loss = 0.31933831\n",
      "Iteration 385, loss = 0.31863359\n",
      "Iteration 386, loss = 0.31797937\n",
      "Iteration 387, loss = 0.31736008\n",
      "Iteration 388, loss = 0.31671462\n",
      "Iteration 389, loss = 0.31604735\n",
      "Iteration 390, loss = 0.31536140\n",
      "Iteration 391, loss = 0.31466457\n",
      "Iteration 392, loss = 0.31398763\n",
      "Iteration 393, loss = 0.31332050\n",
      "Iteration 394, loss = 0.31261556\n",
      "Iteration 395, loss = 0.31193392\n",
      "Iteration 396, loss = 0.31124776\n",
      "Iteration 397, loss = 0.31054892\n",
      "Iteration 398, loss = 0.30983608\n",
      "Iteration 399, loss = 0.30911463\n",
      "Iteration 400, loss = 0.30839098\n",
      "Iteration 401, loss = 0.30767698\n",
      "Iteration 402, loss = 0.30694090\n",
      "Iteration 403, loss = 0.30619739\n",
      "Iteration 404, loss = 0.30545957\n",
      "Iteration 405, loss = 0.30471404\n",
      "Iteration 406, loss = 0.30395706\n",
      "Iteration 407, loss = 0.30318979\n",
      "Iteration 408, loss = 0.30241897\n",
      "Iteration 409, loss = 0.30164612\n",
      "Iteration 410, loss = 0.30086664\n",
      "Iteration 411, loss = 0.30008000\n",
      "Iteration 412, loss = 0.29928674\n",
      "Iteration 413, loss = 0.29848879\n",
      "Iteration 414, loss = 0.29768644\n",
      "Iteration 415, loss = 0.29687889\n",
      "Iteration 416, loss = 0.29606442\n",
      "Iteration 417, loss = 0.29524482\n",
      "Iteration 418, loss = 0.29442056\n",
      "Iteration 419, loss = 0.29359150\n",
      "Iteration 420, loss = 0.29275766\n",
      "Iteration 421, loss = 0.29191975\n",
      "Iteration 422, loss = 0.29107708\n",
      "Iteration 423, loss = 0.29023247\n",
      "Iteration 424, loss = 0.28938253\n",
      "Iteration 425, loss = 0.28852815\n",
      "Iteration 426, loss = 0.28767134\n",
      "Iteration 427, loss = 0.28681161\n",
      "Iteration 428, loss = 0.28594865\n",
      "Iteration 429, loss = 0.28508252\n",
      "Iteration 430, loss = 0.28421353\n",
      "Iteration 431, loss = 0.28334204\n",
      "Iteration 432, loss = 0.28246836\n",
      "Iteration 433, loss = 0.28159370\n",
      "Iteration 434, loss = 0.28071706\n",
      "Iteration 435, loss = 0.27983595\n",
      "Iteration 436, loss = 0.27895484\n",
      "Iteration 437, loss = 0.27807203\n",
      "Iteration 438, loss = 0.27718768\n",
      "Iteration 439, loss = 0.27630195\n",
      "Iteration 440, loss = 0.27541546\n",
      "Iteration 441, loss = 0.27452830\n",
      "Iteration 442, loss = 0.27363967\n",
      "Iteration 443, loss = 0.27275037\n",
      "Iteration 444, loss = 0.27186120\n",
      "Iteration 445, loss = 0.27097199\n",
      "Iteration 446, loss = 0.27008242\n",
      "Iteration 447, loss = 0.26919282\n",
      "Iteration 448, loss = 0.26830302\n",
      "Iteration 449, loss = 0.26741319\n",
      "Iteration 450, loss = 0.26652071\n",
      "Iteration 451, loss = 0.26563143\n",
      "Iteration 452, loss = 0.26474677\n",
      "Iteration 453, loss = 0.26385472\n",
      "Iteration 454, loss = 0.26296883\n",
      "Iteration 455, loss = 0.26208127\n",
      "Iteration 456, loss = 0.26119144\n",
      "Iteration 457, loss = 0.26030817\n",
      "Iteration 458, loss = 0.25941985\n",
      "Iteration 459, loss = 0.25853609\n",
      "Iteration 460, loss = 0.25765056\n",
      "Iteration 461, loss = 0.25676756\n",
      "Iteration 462, loss = 0.25588547\n",
      "Iteration 463, loss = 0.25500457\n",
      "Iteration 464, loss = 0.25412325\n",
      "Iteration 465, loss = 0.25324517\n",
      "Iteration 466, loss = 0.25236659\n",
      "Iteration 467, loss = 0.25148674\n",
      "Iteration 468, loss = 0.25060958\n",
      "Iteration 469, loss = 0.24973167\n",
      "Iteration 470, loss = 0.24885692\n",
      "Iteration 471, loss = 0.24798486\n",
      "Iteration 472, loss = 0.24711029\n",
      "Iteration 473, loss = 0.24623634\n",
      "Iteration 474, loss = 0.24536202\n",
      "Iteration 475, loss = 0.24449492\n",
      "Iteration 476, loss = 0.24362406\n",
      "Iteration 477, loss = 0.24275943\n",
      "Iteration 478, loss = 0.24189532\n",
      "Iteration 479, loss = 0.24103106\n",
      "Iteration 480, loss = 0.24016845\n",
      "Iteration 481, loss = 0.23930876\n",
      "Iteration 482, loss = 0.23845130\n",
      "Iteration 483, loss = 0.23759361\n",
      "Iteration 484, loss = 0.23673762\n",
      "Iteration 485, loss = 0.23588358\n",
      "Iteration 486, loss = 0.23503246\n",
      "Iteration 487, loss = 0.23418261\n",
      "Iteration 488, loss = 0.23333341\n",
      "Iteration 489, loss = 0.23248635\n",
      "Iteration 490, loss = 0.23164180\n",
      "Iteration 491, loss = 0.23079912\n",
      "Iteration 492, loss = 0.22995831\n",
      "Iteration 493, loss = 0.22911960\n",
      "Iteration 494, loss = 0.22828309\n",
      "Iteration 495, loss = 0.22744888\n",
      "Iteration 496, loss = 0.22661699\n",
      "Iteration 497, loss = 0.22578756\n",
      "Iteration 498, loss = 0.22496052\n",
      "Iteration 499, loss = 0.22413620\n",
      "Iteration 500, loss = 0.22331431\n",
      "Iteration 501, loss = 0.22249487\n",
      "Iteration 502, loss = 0.22167870\n",
      "Iteration 503, loss = 0.22086488\n",
      "Iteration 504, loss = 0.22005442\n",
      "Iteration 505, loss = 0.21924712\n",
      "Iteration 506, loss = 0.21844215\n",
      "Iteration 507, loss = 0.21763984\n",
      "Iteration 508, loss = 0.21684152\n",
      "Iteration 509, loss = 0.21604632\n",
      "Iteration 510, loss = 0.21525354\n",
      "Iteration 511, loss = 0.21446347\n",
      "Iteration 512, loss = 0.21367656\n",
      "Iteration 513, loss = 0.21289422\n",
      "Iteration 514, loss = 0.21211454\n",
      "Iteration 515, loss = 0.21133864\n",
      "Iteration 516, loss = 0.21056618\n",
      "Iteration 517, loss = 0.20979846\n",
      "Iteration 518, loss = 0.20903343\n",
      "Iteration 519, loss = 0.20827099\n",
      "Iteration 520, loss = 0.20751152\n",
      "Iteration 521, loss = 0.20675587\n",
      "Iteration 522, loss = 0.20600498\n",
      "Iteration 523, loss = 0.20525636\n",
      "Iteration 524, loss = 0.20451193\n",
      "Iteration 525, loss = 0.20377138\n",
      "Iteration 526, loss = 0.20303405\n",
      "Iteration 527, loss = 0.20229997\n",
      "Iteration 528, loss = 0.20156937\n",
      "Iteration 529, loss = 0.20084449\n",
      "Iteration 530, loss = 0.20012219\n",
      "Iteration 531, loss = 0.19940261\n",
      "Iteration 532, loss = 0.19868671\n",
      "Iteration 533, loss = 0.19797569\n",
      "Iteration 534, loss = 0.19726797\n",
      "Iteration 535, loss = 0.19656337\n",
      "Iteration 536, loss = 0.19586356\n",
      "Iteration 537, loss = 0.19516726\n",
      "Iteration 538, loss = 0.19447374\n",
      "Iteration 539, loss = 0.19378406\n",
      "Iteration 540, loss = 0.19309901\n",
      "Iteration 541, loss = 0.19241724\n",
      "Iteration 542, loss = 0.19173887\n",
      "Iteration 543, loss = 0.19106466\n",
      "Iteration 544, loss = 0.19039370\n",
      "Iteration 545, loss = 0.18972678\n",
      "Iteration 546, loss = 0.18906356\n",
      "Iteration 547, loss = 0.18840380\n",
      "Iteration 548, loss = 0.18774751\n",
      "Iteration 549, loss = 0.18709483\n",
      "Iteration 550, loss = 0.18644587\n",
      "Iteration 551, loss = 0.18580055\n",
      "Iteration 552, loss = 0.18515885\n",
      "Iteration 553, loss = 0.18452070\n",
      "Iteration 554, loss = 0.18388611\n",
      "Iteration 555, loss = 0.18325506\n",
      "Iteration 556, loss = 0.18262757\n",
      "Iteration 557, loss = 0.18200368\n",
      "Iteration 558, loss = 0.18138339\n",
      "Iteration 559, loss = 0.18076657\n",
      "Iteration 560, loss = 0.18015338\n",
      "Iteration 561, loss = 0.17954489\n",
      "Iteration 562, loss = 0.17893858\n",
      "Iteration 563, loss = 0.17833496\n",
      "Iteration 564, loss = 0.17773634\n",
      "Iteration 565, loss = 0.17714131\n",
      "Iteration 566, loss = 0.17654968\n",
      "Iteration 567, loss = 0.17596139\n",
      "Iteration 568, loss = 0.17537638\n",
      "Iteration 569, loss = 0.17479464\n",
      "Iteration 570, loss = 0.17421640\n",
      "Iteration 571, loss = 0.17364182\n",
      "Iteration 572, loss = 0.17307045\n",
      "Iteration 573, loss = 0.17250275\n",
      "Iteration 574, loss = 0.17193854\n",
      "Iteration 575, loss = 0.17137763\n",
      "Iteration 576, loss = 0.17081991\n",
      "Iteration 577, loss = 0.17026557\n",
      "Iteration 578, loss = 0.16971516\n",
      "Iteration 579, loss = 0.16916777\n",
      "Iteration 580, loss = 0.16862301\n",
      "Iteration 581, loss = 0.16808191\n",
      "Iteration 582, loss = 0.16754410\n",
      "Iteration 583, loss = 0.16700938\n",
      "Iteration 584, loss = 0.16647754\n",
      "Iteration 585, loss = 0.16594930\n",
      "Iteration 586, loss = 0.16542433\n",
      "Iteration 587, loss = 0.16490208\n",
      "Iteration 588, loss = 0.16438269\n",
      "Iteration 589, loss = 0.16386740\n",
      "Iteration 590, loss = 0.16335492\n",
      "Iteration 591, loss = 0.16284505\n",
      "Iteration 592, loss = 0.16233773\n",
      "Iteration 593, loss = 0.16183446\n",
      "Iteration 594, loss = 0.16133422\n",
      "Iteration 595, loss = 0.16083665\n",
      "Iteration 596, loss = 0.16034178\n",
      "Iteration 597, loss = 0.15984984\n",
      "Iteration 598, loss = 0.15936048\n",
      "Iteration 599, loss = 0.15887423\n",
      "Iteration 600, loss = 0.15839136\n",
      "Iteration 601, loss = 0.15791097\n",
      "Iteration 602, loss = 0.15743367\n",
      "Iteration 603, loss = 0.15695909\n",
      "Iteration 604, loss = 0.15648726\n",
      "Iteration 605, loss = 0.15601866\n",
      "Iteration 606, loss = 0.15555213\n",
      "Iteration 607, loss = 0.15508901\n",
      "Iteration 608, loss = 0.15462852\n",
      "Iteration 609, loss = 0.15417064\n",
      "Iteration 610, loss = 0.15371537\n",
      "Iteration 611, loss = 0.15326270\n",
      "Iteration 612, loss = 0.15281266\n",
      "Iteration 613, loss = 0.15236526\n",
      "Iteration 614, loss = 0.15192047\n",
      "Iteration 615, loss = 0.15147828\n",
      "Iteration 616, loss = 0.15103868\n",
      "Iteration 617, loss = 0.15060167\n",
      "Iteration 618, loss = 0.15016725\n",
      "Iteration 619, loss = 0.14973552\n",
      "Iteration 620, loss = 0.14930613\n",
      "Iteration 621, loss = 0.14887933\n",
      "Iteration 622, loss = 0.14845499\n",
      "Iteration 623, loss = 0.14803310\n",
      "Iteration 624, loss = 0.14761380\n",
      "Iteration 625, loss = 0.14719688\n",
      "Iteration 626, loss = 0.14678265\n",
      "Iteration 627, loss = 0.14637062\n",
      "Iteration 628, loss = 0.14596166\n",
      "Iteration 629, loss = 0.14555419\n",
      "Iteration 630, loss = 0.14514956\n",
      "Iteration 631, loss = 0.14474725\n",
      "Iteration 632, loss = 0.14434713\n",
      "Iteration 633, loss = 0.14394923\n",
      "Iteration 634, loss = 0.14355381\n",
      "Iteration 635, loss = 0.14316123\n",
      "Iteration 636, loss = 0.14276990\n",
      "Iteration 637, loss = 0.14238117\n",
      "Iteration 638, loss = 0.14199599\n",
      "Iteration 639, loss = 0.14161078\n",
      "Iteration 640, loss = 0.14123045\n",
      "Iteration 641, loss = 0.14085162\n",
      "Iteration 642, loss = 0.14047368\n",
      "Iteration 643, loss = 0.14009731\n",
      "Iteration 644, loss = 0.13972341\n",
      "Iteration 645, loss = 0.13935398\n",
      "Iteration 646, loss = 0.13898294\n",
      "Iteration 647, loss = 0.13861620\n",
      "Iteration 648, loss = 0.13825193\n",
      "Iteration 649, loss = 0.13788901\n",
      "Iteration 650, loss = 0.13752747\n",
      "Iteration 651, loss = 0.13716777\n",
      "Iteration 652, loss = 0.13681076\n",
      "Iteration 653, loss = 0.13645510\n",
      "Iteration 654, loss = 0.13610159\n",
      "Iteration 655, loss = 0.13575010\n",
      "Iteration 656, loss = 0.13540045\n",
      "Iteration 657, loss = 0.13505294\n",
      "Iteration 658, loss = 0.13470719\n",
      "Iteration 659, loss = 0.13436286\n",
      "Iteration 660, loss = 0.13402046\n",
      "Iteration 661, loss = 0.13367986\n",
      "Iteration 662, loss = 0.13334136\n",
      "Iteration 663, loss = 0.13300506\n",
      "Iteration 664, loss = 0.13266987\n",
      "Iteration 665, loss = 0.13233733\n",
      "Iteration 666, loss = 0.13200566\n",
      "Iteration 667, loss = 0.13167543\n",
      "Iteration 668, loss = 0.13134833\n",
      "Iteration 669, loss = 0.13102156\n",
      "Iteration 670, loss = 0.13069673\n",
      "Iteration 671, loss = 0.13037460\n",
      "Iteration 672, loss = 0.13005340\n",
      "Iteration 673, loss = 0.12973440\n",
      "Iteration 674, loss = 0.12941793\n",
      "Iteration 675, loss = 0.12910224\n",
      "Iteration 676, loss = 0.12878945\n",
      "Iteration 677, loss = 0.12847864\n",
      "Iteration 678, loss = 0.12816960\n",
      "Iteration 679, loss = 0.12786235\n",
      "Iteration 680, loss = 0.12755658\n",
      "Iteration 681, loss = 0.12725270\n",
      "Iteration 682, loss = 0.12695074\n",
      "Iteration 683, loss = 0.12665067\n",
      "Iteration 684, loss = 0.12635194\n",
      "Iteration 685, loss = 0.12605655\n",
      "Iteration 686, loss = 0.12576279\n",
      "Iteration 687, loss = 0.12547012\n",
      "Iteration 688, loss = 0.12517946\n",
      "Iteration 689, loss = 0.12489109\n",
      "Iteration 690, loss = 0.12460319\n",
      "Iteration 691, loss = 0.12431782\n",
      "Iteration 692, loss = 0.12403428\n",
      "Iteration 693, loss = 0.12375143\n",
      "Iteration 694, loss = 0.12347036\n",
      "Iteration 695, loss = 0.12319107\n",
      "Iteration 696, loss = 0.12291338\n",
      "Iteration 697, loss = 0.12263718\n",
      "Iteration 698, loss = 0.12236276\n",
      "Iteration 699, loss = 0.12208966\n",
      "Iteration 700, loss = 0.12181798\n",
      "Iteration 701, loss = 0.12154770\n",
      "Iteration 702, loss = 0.12127942\n",
      "Iteration 703, loss = 0.12101202\n",
      "Iteration 704, loss = 0.12074616\n",
      "Iteration 705, loss = 0.12048195\n",
      "Iteration 706, loss = 0.12021857\n",
      "Iteration 707, loss = 0.11995547\n",
      "Iteration 708, loss = 0.11969259\n",
      "Iteration 709, loss = 0.11943108\n",
      "Iteration 710, loss = 0.11917073\n",
      "Iteration 711, loss = 0.11891151\n",
      "Iteration 712, loss = 0.11865331\n",
      "Iteration 713, loss = 0.11839645\n",
      "Iteration 714, loss = 0.11814096\n",
      "Iteration 715, loss = 0.11788696\n",
      "Iteration 716, loss = 0.11763372\n",
      "Iteration 717, loss = 0.11738141\n",
      "Iteration 718, loss = 0.11713006\n",
      "Iteration 719, loss = 0.11688028\n",
      "Iteration 720, loss = 0.11663144\n",
      "Iteration 721, loss = 0.11638616\n",
      "Iteration 722, loss = 0.11614252\n",
      "Iteration 723, loss = 0.11589988\n",
      "Iteration 724, loss = 0.11565794\n",
      "Iteration 725, loss = 0.11541690\n",
      "Iteration 726, loss = 0.11517798\n",
      "Iteration 727, loss = 0.11494077\n",
      "Iteration 728, loss = 0.11470501\n",
      "Iteration 729, loss = 0.11447027\n",
      "Iteration 730, loss = 0.11423693\n",
      "Iteration 731, loss = 0.11400560\n",
      "Iteration 732, loss = 0.11377511\n",
      "Iteration 733, loss = 0.11354566\n",
      "Iteration 734, loss = 0.11331773\n",
      "Iteration 735, loss = 0.11309046\n",
      "Iteration 736, loss = 0.11286417\n",
      "Iteration 737, loss = 0.11263920\n",
      "Iteration 738, loss = 0.11241559\n",
      "Iteration 739, loss = 0.11219362\n",
      "Iteration 740, loss = 0.11197335\n",
      "Iteration 741, loss = 0.11175459\n",
      "Iteration 742, loss = 0.11153712\n",
      "Iteration 743, loss = 0.11132076\n",
      "Iteration 744, loss = 0.11110579\n",
      "Iteration 745, loss = 0.11089253\n",
      "Iteration 746, loss = 0.11068036\n",
      "Iteration 747, loss = 0.11046909\n",
      "Iteration 748, loss = 0.11025880\n",
      "Iteration 749, loss = 0.11004960\n",
      "Iteration 750, loss = 0.10984215\n",
      "Iteration 751, loss = 0.10963486\n",
      "Iteration 752, loss = 0.10942926\n",
      "Iteration 753, loss = 0.10922485\n",
      "Iteration 754, loss = 0.10902144\n",
      "Iteration 755, loss = 0.10881902\n",
      "Iteration 756, loss = 0.10861772\n",
      "Iteration 757, loss = 0.10841722\n",
      "Iteration 758, loss = 0.10821824\n",
      "Iteration 759, loss = 0.10801995\n",
      "Iteration 760, loss = 0.10782382\n",
      "Iteration 761, loss = 0.10762725\n",
      "Iteration 762, loss = 0.10743259\n",
      "Iteration 763, loss = 0.10723856\n",
      "Iteration 764, loss = 0.10704555\n",
      "Iteration 765, loss = 0.10685359\n",
      "Iteration 766, loss = 0.10666243\n",
      "Iteration 767, loss = 0.10647226\n",
      "Iteration 768, loss = 0.10628348\n",
      "Iteration 769, loss = 0.10609527\n",
      "Iteration 770, loss = 0.10590835\n",
      "Iteration 771, loss = 0.10572213\n",
      "Iteration 772, loss = 0.10553635\n",
      "Iteration 773, loss = 0.10535151\n",
      "Iteration 774, loss = 0.10516748\n",
      "Iteration 775, loss = 0.10498513\n",
      "Iteration 776, loss = 0.10480275\n",
      "Iteration 777, loss = 0.10462187\n",
      "Iteration 778, loss = 0.10444188\n",
      "Iteration 779, loss = 0.10426267\n",
      "Iteration 780, loss = 0.10408397\n",
      "Iteration 781, loss = 0.10390595\n",
      "Iteration 782, loss = 0.10372886\n",
      "Iteration 783, loss = 0.10355223\n",
      "Iteration 784, loss = 0.10337759\n",
      "Iteration 785, loss = 0.10320329\n",
      "Iteration 786, loss = 0.10302850\n",
      "Iteration 787, loss = 0.10285636\n",
      "Iteration 788, loss = 0.10268500\n",
      "Iteration 789, loss = 0.10251446\n",
      "Iteration 790, loss = 0.10234414\n",
      "Iteration 791, loss = 0.10217409\n",
      "Iteration 792, loss = 0.10200466\n",
      "Iteration 793, loss = 0.10183626\n",
      "Iteration 794, loss = 0.10166827\n",
      "Iteration 795, loss = 0.10150373\n",
      "Iteration 796, loss = 0.10133841\n",
      "Iteration 797, loss = 0.10117298\n",
      "Iteration 798, loss = 0.10100750\n",
      "Iteration 799, loss = 0.10084394\n",
      "Iteration 800, loss = 0.10068192\n",
      "Iteration 801, loss = 0.10052045\n",
      "Iteration 802, loss = 0.10035949\n",
      "Iteration 803, loss = 0.10019882\n",
      "Iteration 804, loss = 0.10003853\n",
      "Iteration 805, loss = 0.09987888\n",
      "Iteration 806, loss = 0.09972137\n",
      "Iteration 807, loss = 0.09956477\n",
      "Iteration 808, loss = 0.09940796\n",
      "Iteration 809, loss = 0.09925100\n",
      "Iteration 810, loss = 0.09909629\n",
      "Iteration 811, loss = 0.09894278\n",
      "Iteration 812, loss = 0.09878943\n",
      "Iteration 813, loss = 0.09863630\n",
      "Iteration 814, loss = 0.09848360\n",
      "Iteration 815, loss = 0.09833126\n",
      "Iteration 816, loss = 0.09818142\n",
      "Iteration 817, loss = 0.09803199\n",
      "Iteration 818, loss = 0.09788242\n",
      "Iteration 819, loss = 0.09773285\n",
      "Iteration 820, loss = 0.09758391\n",
      "Iteration 821, loss = 0.09743699\n",
      "Iteration 822, loss = 0.09729031\n",
      "Iteration 823, loss = 0.09714382\n",
      "Iteration 824, loss = 0.09699811\n",
      "Iteration 825, loss = 0.09685352\n",
      "Iteration 826, loss = 0.09670904\n",
      "Iteration 827, loss = 0.09656578\n",
      "Iteration 828, loss = 0.09642282\n",
      "Iteration 829, loss = 0.09628073\n",
      "Iteration 830, loss = 0.09613913\n",
      "Iteration 831, loss = 0.09599790\n",
      "Iteration 832, loss = 0.09585772\n",
      "Iteration 833, loss = 0.09571824\n",
      "Iteration 834, loss = 0.09557899\n",
      "Iteration 835, loss = 0.09544046\n",
      "Iteration 836, loss = 0.09530262\n",
      "Iteration 837, loss = 0.09516540\n",
      "Iteration 838, loss = 0.09502864\n",
      "Iteration 839, loss = 0.09489245\n",
      "Iteration 840, loss = 0.09475704\n",
      "Iteration 841, loss = 0.09462233\n",
      "Iteration 842, loss = 0.09448790\n",
      "Iteration 843, loss = 0.09435399\n",
      "Iteration 844, loss = 0.09422112\n",
      "Iteration 845, loss = 0.09408835\n",
      "Iteration 846, loss = 0.09395686\n",
      "Iteration 847, loss = 0.09382559\n",
      "Iteration 848, loss = 0.09369462\n",
      "Iteration 849, loss = 0.09356457\n",
      "Iteration 850, loss = 0.09343518\n",
      "Iteration 851, loss = 0.09330592\n",
      "Iteration 852, loss = 0.09317771\n",
      "Iteration 853, loss = 0.09304984\n",
      "Iteration 854, loss = 0.09292222\n",
      "Iteration 855, loss = 0.09279546\n",
      "Iteration 856, loss = 0.09266918\n",
      "Iteration 857, loss = 0.09254357\n",
      "Iteration 858, loss = 0.09241845\n",
      "Iteration 859, loss = 0.09229383\n",
      "Iteration 860, loss = 0.09216979\n",
      "Iteration 861, loss = 0.09204637\n",
      "Iteration 862, loss = 0.09192352\n",
      "Iteration 863, loss = 0.09180101\n",
      "Iteration 864, loss = 0.09167921\n",
      "Iteration 865, loss = 0.09155795\n",
      "Iteration 866, loss = 0.09143705\n",
      "Iteration 867, loss = 0.09131684\n",
      "Iteration 868, loss = 0.09119694\n",
      "Iteration 869, loss = 0.09107788\n",
      "Iteration 870, loss = 0.09095910\n",
      "Iteration 871, loss = 0.09084076\n",
      "Iteration 872, loss = 0.09072323\n",
      "Iteration 873, loss = 0.09060590\n",
      "Iteration 874, loss = 0.09048929\n",
      "Iteration 875, loss = 0.09037293\n",
      "Iteration 876, loss = 0.09025694\n",
      "Iteration 877, loss = 0.09014138\n",
      "Iteration 878, loss = 0.09002620\n",
      "Iteration 879, loss = 0.08991165\n",
      "Iteration 880, loss = 0.08979727\n",
      "Iteration 881, loss = 0.08968336\n",
      "Iteration 882, loss = 0.08956997\n",
      "Iteration 883, loss = 0.08945712\n",
      "Iteration 884, loss = 0.08934449\n",
      "Iteration 885, loss = 0.08923278\n",
      "Iteration 886, loss = 0.08912146\n",
      "Iteration 887, loss = 0.08901029\n",
      "Iteration 888, loss = 0.08889964\n",
      "Iteration 889, loss = 0.08878958\n",
      "Iteration 890, loss = 0.08867990\n",
      "Iteration 891, loss = 0.08857083\n",
      "Iteration 892, loss = 0.08846173\n",
      "Iteration 893, loss = 0.08835284\n",
      "Iteration 894, loss = 0.08824448\n",
      "Iteration 895, loss = 0.08813660\n",
      "Iteration 896, loss = 0.08802911\n",
      "Iteration 897, loss = 0.08792207\n",
      "Iteration 898, loss = 0.08781539\n",
      "Iteration 899, loss = 0.08770917\n",
      "Iteration 900, loss = 0.08760334\n",
      "Iteration 901, loss = 0.08749790\n",
      "Iteration 902, loss = 0.08739296\n",
      "Iteration 903, loss = 0.08728838\n",
      "Iteration 904, loss = 0.08718418\n",
      "Iteration 905, loss = 0.08708043\n",
      "Iteration 906, loss = 0.08697709\n",
      "Iteration 907, loss = 0.08687425\n",
      "Iteration 908, loss = 0.08677178\n",
      "Iteration 909, loss = 0.08666992\n",
      "Iteration 910, loss = 0.08656820\n",
      "Iteration 911, loss = 0.08646697\n",
      "Iteration 912, loss = 0.08636623\n",
      "Iteration 913, loss = 0.08626610\n",
      "Iteration 914, loss = 0.08616657\n",
      "Iteration 915, loss = 0.08606733\n",
      "Iteration 916, loss = 0.08596865\n",
      "Iteration 917, loss = 0.08587033\n",
      "Iteration 918, loss = 0.08577237\n",
      "Iteration 919, loss = 0.08567482\n",
      "Iteration 920, loss = 0.08557772\n",
      "Iteration 921, loss = 0.08548105\n",
      "Iteration 922, loss = 0.08538472\n",
      "Iteration 923, loss = 0.08528892\n",
      "Iteration 924, loss = 0.08519355\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35165820\n",
      "Iteration 2, loss = 1.32460029\n",
      "Iteration 3, loss = 1.29803981\n",
      "Iteration 4, loss = 1.27197671\n",
      "Iteration 5, loss = 1.24647155\n",
      "Iteration 6, loss = 1.22154064\n",
      "Iteration 7, loss = 1.19721776\n",
      "Iteration 8, loss = 1.17350108\n",
      "Iteration 9, loss = 1.15031721\n",
      "Iteration 10, loss = 1.12771675\n",
      "Iteration 11, loss = 1.10577085\n",
      "Iteration 12, loss = 1.08449108\n",
      "Iteration 13, loss = 1.06392865\n",
      "Iteration 14, loss = 1.04405508\n",
      "Iteration 15, loss = 1.02489620\n",
      "Iteration 16, loss = 1.00644322\n",
      "Iteration 17, loss = 0.98871259\n",
      "Iteration 18, loss = 0.97169940\n",
      "Iteration 19, loss = 0.95540346\n",
      "Iteration 20, loss = 0.93980704\n",
      "Iteration 21, loss = 0.92490506\n",
      "Iteration 22, loss = 0.91068260\n",
      "Iteration 23, loss = 0.89713212\n",
      "Iteration 24, loss = 0.88428439\n",
      "Iteration 25, loss = 0.87211160\n",
      "Iteration 26, loss = 0.86061628\n",
      "Iteration 27, loss = 0.84973554\n",
      "Iteration 28, loss = 0.83948533\n",
      "Iteration 29, loss = 0.82984071\n",
      "Iteration 30, loss = 0.82076867\n",
      "Iteration 31, loss = 0.81225250\n",
      "Iteration 32, loss = 0.80423470\n",
      "Iteration 33, loss = 0.79671948\n",
      "Iteration 34, loss = 0.78971164\n",
      "Iteration 35, loss = 0.78319289\n",
      "Iteration 36, loss = 0.77714119\n",
      "Iteration 37, loss = 0.77149977\n",
      "Iteration 38, loss = 0.76623721\n",
      "Iteration 39, loss = 0.76133817\n",
      "Iteration 40, loss = 0.75679972\n",
      "Iteration 41, loss = 0.75263636\n",
      "Iteration 42, loss = 0.74875049\n",
      "Iteration 43, loss = 0.74511369\n",
      "Iteration 44, loss = 0.74170520\n",
      "Iteration 45, loss = 0.73864152\n",
      "Iteration 46, loss = 0.73570697\n",
      "Iteration 47, loss = 0.73290186\n",
      "Iteration 48, loss = 0.73019962\n",
      "Iteration 49, loss = 0.72756466\n",
      "Iteration 50, loss = 0.72499530\n",
      "Iteration 51, loss = 0.72249707\n",
      "Iteration 52, loss = 0.72005426\n",
      "Iteration 53, loss = 0.71763022\n",
      "Iteration 54, loss = 0.71523272\n",
      "Iteration 55, loss = 0.71284248\n",
      "Iteration 56, loss = 0.71045720\n",
      "Iteration 57, loss = 0.70806824\n",
      "Iteration 58, loss = 0.70567535\n",
      "Iteration 59, loss = 0.70328095\n",
      "Iteration 60, loss = 0.70084248\n",
      "Iteration 61, loss = 0.69837132\n",
      "Iteration 62, loss = 0.69589272\n",
      "Iteration 63, loss = 0.69340343\n",
      "Iteration 64, loss = 0.69088935\n",
      "Iteration 65, loss = 0.68835795\n",
      "Iteration 66, loss = 0.68581496\n",
      "Iteration 67, loss = 0.68324991\n",
      "Iteration 68, loss = 0.68066820\n",
      "Iteration 69, loss = 0.67805259\n",
      "Iteration 70, loss = 0.67538110\n",
      "Iteration 71, loss = 0.67260794\n",
      "Iteration 72, loss = 0.66974308\n",
      "Iteration 73, loss = 0.66683139\n",
      "Iteration 74, loss = 0.66393126\n",
      "Iteration 75, loss = 0.66101652\n",
      "Iteration 76, loss = 0.65809005\n",
      "Iteration 77, loss = 0.65518326\n",
      "Iteration 78, loss = 0.65226726\n",
      "Iteration 79, loss = 0.64942048\n",
      "Iteration 80, loss = 0.64660488\n",
      "Iteration 81, loss = 0.64382064\n",
      "Iteration 82, loss = 0.64106729\n",
      "Iteration 83, loss = 0.63833163\n",
      "Iteration 84, loss = 0.63564417\n",
      "Iteration 85, loss = 0.63298239\n",
      "Iteration 86, loss = 0.63037915\n",
      "Iteration 87, loss = 0.62778162\n",
      "Iteration 88, loss = 0.62519417\n",
      "Iteration 89, loss = 0.62261836\n",
      "Iteration 90, loss = 0.62005700\n",
      "Iteration 91, loss = 0.61751968\n",
      "Iteration 92, loss = 0.61499715\n",
      "Iteration 93, loss = 0.61247882\n",
      "Iteration 94, loss = 0.60997362\n",
      "Iteration 95, loss = 0.60748215\n",
      "Iteration 96, loss = 0.60499891\n",
      "Iteration 97, loss = 0.60250909\n",
      "Iteration 98, loss = 0.60002927\n",
      "Iteration 99, loss = 0.59756214\n",
      "Iteration 100, loss = 0.59510922\n",
      "Iteration 101, loss = 0.59267101\n",
      "Iteration 102, loss = 0.59024847\n",
      "Iteration 103, loss = 0.58784242\n",
      "Iteration 104, loss = 0.58545325\n",
      "Iteration 105, loss = 0.58308134\n",
      "Iteration 106, loss = 0.58072738\n",
      "Iteration 107, loss = 0.57839130\n",
      "Iteration 108, loss = 0.57607308\n",
      "Iteration 109, loss = 0.57377291\n",
      "Iteration 110, loss = 0.57149084\n",
      "Iteration 111, loss = 0.56922734\n",
      "Iteration 112, loss = 0.56698205\n",
      "Iteration 113, loss = 0.56475456\n",
      "Iteration 114, loss = 0.56254488\n",
      "Iteration 115, loss = 0.56035309\n",
      "Iteration 116, loss = 0.55819426\n",
      "Iteration 117, loss = 0.55615218\n",
      "Iteration 118, loss = 0.55416920\n",
      "Iteration 119, loss = 0.55224398\n",
      "Iteration 120, loss = 0.55044332\n",
      "Iteration 121, loss = 0.54884130\n",
      "Iteration 122, loss = 0.54730800\n",
      "Iteration 123, loss = 0.54578404\n",
      "Iteration 124, loss = 0.54425997\n",
      "Iteration 125, loss = 0.54271089\n",
      "Iteration 126, loss = 0.54111565\n",
      "Iteration 127, loss = 0.53948206\n",
      "Iteration 128, loss = 0.53782591\n",
      "Iteration 129, loss = 0.53617313\n",
      "Iteration 130, loss = 0.53453844\n",
      "Iteration 131, loss = 0.53292537\n",
      "Iteration 132, loss = 0.53134225\n",
      "Iteration 133, loss = 0.52977936\n",
      "Iteration 134, loss = 0.52826559\n",
      "Iteration 135, loss = 0.52677757\n",
      "Iteration 136, loss = 0.52533779\n",
      "Iteration 137, loss = 0.52392579\n",
      "Iteration 138, loss = 0.52252162\n",
      "Iteration 139, loss = 0.52113328\n",
      "Iteration 140, loss = 0.51975230\n",
      "Iteration 141, loss = 0.51837123\n",
      "Iteration 142, loss = 0.51699074\n",
      "Iteration 143, loss = 0.51561179\n",
      "Iteration 144, loss = 0.51423914\n",
      "Iteration 145, loss = 0.51287557\n",
      "Iteration 146, loss = 0.51152512\n",
      "Iteration 147, loss = 0.51018511\n",
      "Iteration 148, loss = 0.50885718\n",
      "Iteration 149, loss = 0.50754964\n",
      "Iteration 150, loss = 0.50625749\n",
      "Iteration 151, loss = 0.50497536\n",
      "Iteration 152, loss = 0.50371265\n",
      "Iteration 153, loss = 0.50246426\n",
      "Iteration 154, loss = 0.50121816\n",
      "Iteration 155, loss = 0.49997440\n",
      "Iteration 156, loss = 0.49873352\n",
      "Iteration 157, loss = 0.49749757\n",
      "Iteration 158, loss = 0.49627453\n",
      "Iteration 159, loss = 0.49506245\n",
      "Iteration 160, loss = 0.49386249\n",
      "Iteration 161, loss = 0.49267240\n",
      "Iteration 162, loss = 0.49149010\n",
      "Iteration 163, loss = 0.49031860\n",
      "Iteration 164, loss = 0.48915386\n",
      "Iteration 165, loss = 0.48799572\n",
      "Iteration 166, loss = 0.48684418\n",
      "Iteration 167, loss = 0.48569923\n",
      "Iteration 168, loss = 0.48456086\n",
      "Iteration 169, loss = 0.48342905\n",
      "Iteration 170, loss = 0.48230486\n",
      "Iteration 171, loss = 0.48118763\n",
      "Iteration 172, loss = 0.48007696\n",
      "Iteration 173, loss = 0.47897280\n",
      "Iteration 174, loss = 0.47787680\n",
      "Iteration 175, loss = 0.47679157\n",
      "Iteration 176, loss = 0.47571149\n",
      "Iteration 177, loss = 0.47463605\n",
      "Iteration 178, loss = 0.47356740\n",
      "Iteration 179, loss = 0.47250510\n",
      "Iteration 180, loss = 0.47144901\n",
      "Iteration 181, loss = 0.47039966\n",
      "Iteration 182, loss = 0.46935696\n",
      "Iteration 183, loss = 0.46831940\n",
      "Iteration 184, loss = 0.46728696\n",
      "Iteration 185, loss = 0.46625962\n",
      "Iteration 186, loss = 0.46523790\n",
      "Iteration 187, loss = 0.46422275\n",
      "Iteration 188, loss = 0.46321265\n",
      "Iteration 189, loss = 0.46220916\n",
      "Iteration 190, loss = 0.46121440\n",
      "Iteration 191, loss = 0.46022635\n",
      "Iteration 192, loss = 0.45924410\n",
      "Iteration 193, loss = 0.45826700\n",
      "Iteration 194, loss = 0.45729522\n",
      "Iteration 195, loss = 0.45633134\n",
      "Iteration 196, loss = 0.45537472\n",
      "Iteration 197, loss = 0.45442368\n",
      "Iteration 198, loss = 0.45347830\n",
      "Iteration 199, loss = 0.45253773\n",
      "Iteration 200, loss = 0.45160113\n",
      "Iteration 201, loss = 0.45066855\n",
      "Iteration 202, loss = 0.44974008\n",
      "Iteration 203, loss = 0.44881620\n",
      "Iteration 204, loss = 0.44789686\n",
      "Iteration 205, loss = 0.44698177\n",
      "Iteration 206, loss = 0.44607090\n",
      "Iteration 207, loss = 0.44516459\n",
      "Iteration 208, loss = 0.44426360\n",
      "Iteration 209, loss = 0.44336690\n",
      "Iteration 210, loss = 0.44247426\n",
      "Iteration 211, loss = 0.44158559\n",
      "Iteration 212, loss = 0.44070085\n",
      "Iteration 213, loss = 0.43981998\n",
      "Iteration 214, loss = 0.43894318\n",
      "Iteration 215, loss = 0.43807018\n",
      "Iteration 216, loss = 0.43720060\n",
      "Iteration 217, loss = 0.43633482\n",
      "Iteration 218, loss = 0.43547271\n",
      "Iteration 219, loss = 0.43461406\n",
      "Iteration 220, loss = 0.43375885\n",
      "Iteration 221, loss = 0.43290708\n",
      "Iteration 222, loss = 0.43205856\n",
      "Iteration 223, loss = 0.43121341\n",
      "Iteration 224, loss = 0.43037153\n",
      "Iteration 225, loss = 0.42953286\n",
      "Iteration 226, loss = 0.42869736\n",
      "Iteration 227, loss = 0.42786498\n",
      "Iteration 228, loss = 0.42703570\n",
      "Iteration 229, loss = 0.42620937\n",
      "Iteration 230, loss = 0.42538601\n",
      "Iteration 231, loss = 0.42456585\n",
      "Iteration 232, loss = 0.42374932\n",
      "Iteration 233, loss = 0.42293566\n",
      "Iteration 234, loss = 0.42212472\n",
      "Iteration 235, loss = 0.42131624\n",
      "Iteration 236, loss = 0.42051055\n",
      "Iteration 237, loss = 0.41970759\n",
      "Iteration 238, loss = 0.41890729\n",
      "Iteration 239, loss = 0.41810961\n",
      "Iteration 240, loss = 0.41731452\n",
      "Iteration 241, loss = 0.41652197\n",
      "Iteration 242, loss = 0.41573190\n",
      "Iteration 243, loss = 0.41494428\n",
      "Iteration 244, loss = 0.41415905\n",
      "Iteration 245, loss = 0.41337617\n",
      "Iteration 246, loss = 0.41259559\n",
      "Iteration 247, loss = 0.41181726\n",
      "Iteration 248, loss = 0.41104113\n",
      "Iteration 249, loss = 0.41026717\n",
      "Iteration 250, loss = 0.40949530\n",
      "Iteration 251, loss = 0.40872552\n",
      "Iteration 252, loss = 0.40795776\n",
      "Iteration 253, loss = 0.40719198\n",
      "Iteration 254, loss = 0.40642816\n",
      "Iteration 255, loss = 0.40566624\n",
      "Iteration 256, loss = 0.40490682\n",
      "Iteration 257, loss = 0.40415014\n",
      "Iteration 258, loss = 0.40339514\n",
      "Iteration 259, loss = 0.40264182\n",
      "Iteration 260, loss = 0.40189019\n",
      "Iteration 261, loss = 0.40114025\n",
      "Iteration 262, loss = 0.40039200\n",
      "Iteration 263, loss = 0.39964546\n",
      "Iteration 264, loss = 0.39890101\n",
      "Iteration 265, loss = 0.39815834\n",
      "Iteration 266, loss = 0.39741731\n",
      "Iteration 267, loss = 0.39667791\n",
      "Iteration 268, loss = 0.39594015\n",
      "Iteration 269, loss = 0.39520470\n",
      "Iteration 270, loss = 0.39447070\n",
      "Iteration 271, loss = 0.39373798\n",
      "Iteration 272, loss = 0.39300691\n",
      "Iteration 273, loss = 0.39227774\n",
      "Iteration 274, loss = 0.39155075\n",
      "Iteration 275, loss = 0.39082530\n",
      "Iteration 276, loss = 0.39010137\n",
      "Iteration 277, loss = 0.38937895\n",
      "Iteration 278, loss = 0.38865802\n",
      "Iteration 279, loss = 0.38793875\n",
      "Iteration 280, loss = 0.38722076\n",
      "Iteration 281, loss = 0.38650403\n",
      "Iteration 282, loss = 0.38578872\n",
      "Iteration 283, loss = 0.38507540\n",
      "Iteration 284, loss = 0.38436375\n",
      "Iteration 285, loss = 0.38365348\n",
      "Iteration 286, loss = 0.38294455\n",
      "Iteration 287, loss = 0.38223698\n",
      "Iteration 288, loss = 0.38153071\n",
      "Iteration 289, loss = 0.38082574\n",
      "Iteration 290, loss = 0.38012203\n",
      "Iteration 291, loss = 0.37941956\n",
      "Iteration 292, loss = 0.37871858\n",
      "Iteration 293, loss = 0.37801894\n",
      "Iteration 294, loss = 0.37732039\n",
      "Iteration 295, loss = 0.37662353\n",
      "Iteration 296, loss = 0.37592793\n",
      "Iteration 297, loss = 0.37523346\n",
      "Iteration 298, loss = 0.37454069\n",
      "Iteration 299, loss = 0.37384964\n",
      "Iteration 300, loss = 0.37316005\n",
      "Iteration 301, loss = 0.37247120\n",
      "Iteration 302, loss = 0.37178328\n",
      "Iteration 303, loss = 0.37109665\n",
      "Iteration 304, loss = 0.37041160\n",
      "Iteration 305, loss = 0.36972759\n",
      "Iteration 306, loss = 0.36904461\n",
      "Iteration 307, loss = 0.36836265\n",
      "Iteration 308, loss = 0.36768168\n",
      "Iteration 309, loss = 0.36700169\n",
      "Iteration 310, loss = 0.36632273\n",
      "Iteration 311, loss = 0.36564501\n",
      "Iteration 312, loss = 0.36496840\n",
      "Iteration 313, loss = 0.36429266\n",
      "Iteration 314, loss = 0.36361793\n",
      "Iteration 315, loss = 0.36294456\n",
      "Iteration 316, loss = 0.36227247\n",
      "Iteration 317, loss = 0.36160148\n",
      "Iteration 318, loss = 0.36093274\n",
      "Iteration 319, loss = 0.36026497\n",
      "Iteration 320, loss = 0.35959792\n",
      "Iteration 321, loss = 0.35893237\n",
      "Iteration 322, loss = 0.35826641\n",
      "Iteration 323, loss = 0.35760196\n",
      "Iteration 324, loss = 0.35693833\n",
      "Iteration 325, loss = 0.35627557\n",
      "Iteration 326, loss = 0.35561385\n",
      "Iteration 327, loss = 0.35495301\n",
      "Iteration 328, loss = 0.35429323\n",
      "Iteration 329, loss = 0.35363440\n",
      "Iteration 330, loss = 0.35297659\n",
      "Iteration 331, loss = 0.35231965\n",
      "Iteration 332, loss = 0.35166396\n",
      "Iteration 333, loss = 0.35100899\n",
      "Iteration 334, loss = 0.35035464\n",
      "Iteration 335, loss = 0.34970135\n",
      "Iteration 336, loss = 0.34904898\n",
      "Iteration 337, loss = 0.34839744\n",
      "Iteration 338, loss = 0.34774672\n",
      "Iteration 339, loss = 0.34709679\n",
      "Iteration 340, loss = 0.34644767\n",
      "Iteration 341, loss = 0.34579936\n",
      "Iteration 342, loss = 0.34515199\n",
      "Iteration 343, loss = 0.34450561\n",
      "Iteration 344, loss = 0.34386003\n",
      "Iteration 345, loss = 0.34321526\n",
      "Iteration 346, loss = 0.34257129\n",
      "Iteration 347, loss = 0.34192817\n",
      "Iteration 348, loss = 0.34128593\n",
      "Iteration 349, loss = 0.34064466\n",
      "Iteration 350, loss = 0.34000418\n",
      "Iteration 351, loss = 0.33936448\n",
      "Iteration 352, loss = 0.33872556\n",
      "Iteration 353, loss = 0.33808741\n",
      "Iteration 354, loss = 0.33745003\n",
      "Iteration 355, loss = 0.33681353\n",
      "Iteration 356, loss = 0.33617806\n",
      "Iteration 357, loss = 0.33554343\n",
      "Iteration 358, loss = 0.33490959\n",
      "Iteration 359, loss = 0.33427668\n",
      "Iteration 360, loss = 0.33364446\n",
      "Iteration 361, loss = 0.33301291\n",
      "Iteration 362, loss = 0.33238196\n",
      "Iteration 363, loss = 0.33175107\n",
      "Iteration 364, loss = 0.33111178\n",
      "Iteration 365, loss = 0.33043829\n",
      "Iteration 366, loss = 0.32971091\n",
      "Iteration 367, loss = 0.32901673\n",
      "Iteration 368, loss = 0.32838884\n",
      "Iteration 369, loss = 0.32773152\n",
      "Iteration 370, loss = 0.32704646\n",
      "Iteration 371, loss = 0.32633998\n",
      "Iteration 372, loss = 0.32562533\n",
      "Iteration 373, loss = 0.32493477\n",
      "Iteration 374, loss = 0.32425074\n",
      "Iteration 375, loss = 0.32352917\n",
      "Iteration 376, loss = 0.32282409\n",
      "Iteration 377, loss = 0.32211952\n",
      "Iteration 378, loss = 0.32140555\n",
      "Iteration 379, loss = 0.32067804\n",
      "Iteration 380, loss = 0.31994006\n",
      "Iteration 381, loss = 0.31919708\n",
      "Iteration 382, loss = 0.31845626\n",
      "Iteration 383, loss = 0.31771126\n",
      "Iteration 384, loss = 0.31694974\n",
      "Iteration 385, loss = 0.31618736\n",
      "Iteration 386, loss = 0.31542338\n",
      "Iteration 387, loss = 0.31465286\n",
      "Iteration 388, loss = 0.31387246\n",
      "Iteration 389, loss = 0.31308268\n",
      "Iteration 390, loss = 0.31228615\n",
      "Iteration 391, loss = 0.31148860\n",
      "Iteration 392, loss = 0.31068910\n",
      "Iteration 393, loss = 0.30987893\n",
      "Iteration 394, loss = 0.30906373\n",
      "Iteration 395, loss = 0.30824464\n",
      "Iteration 396, loss = 0.30742376\n",
      "Iteration 397, loss = 0.30659681\n",
      "Iteration 398, loss = 0.30576186\n",
      "Iteration 399, loss = 0.30492294\n",
      "Iteration 400, loss = 0.30407949\n",
      "Iteration 401, loss = 0.30323201\n",
      "Iteration 402, loss = 0.30238275\n",
      "Iteration 403, loss = 0.30152692\n",
      "Iteration 404, loss = 0.30066875\n",
      "Iteration 405, loss = 0.29980745\n",
      "Iteration 406, loss = 0.29894330\n",
      "Iteration 407, loss = 0.29807368\n",
      "Iteration 408, loss = 0.29720337\n",
      "Iteration 409, loss = 0.29632922\n",
      "Iteration 410, loss = 0.29545249\n",
      "Iteration 411, loss = 0.29457399\n",
      "Iteration 412, loss = 0.29369259\n",
      "Iteration 413, loss = 0.29280841\n",
      "Iteration 414, loss = 0.29191991\n",
      "Iteration 415, loss = 0.29102617\n",
      "Iteration 416, loss = 0.29014406\n",
      "Iteration 417, loss = 0.28924292\n",
      "Iteration 418, loss = 0.28835420\n",
      "Iteration 419, loss = 0.28746112\n",
      "Iteration 420, loss = 0.28656150\n",
      "Iteration 421, loss = 0.28565664\n",
      "Iteration 422, loss = 0.28475955\n",
      "Iteration 423, loss = 0.28385881\n",
      "Iteration 424, loss = 0.28295237\n",
      "Iteration 425, loss = 0.28204921\n",
      "Iteration 426, loss = 0.28114617\n",
      "Iteration 427, loss = 0.28023780\n",
      "Iteration 428, loss = 0.27933025\n",
      "Iteration 429, loss = 0.27842194\n",
      "Iteration 430, loss = 0.27751299\n",
      "Iteration 431, loss = 0.27660358\n",
      "Iteration 432, loss = 0.27569177\n",
      "Iteration 433, loss = 0.27477963\n",
      "Iteration 434, loss = 0.27386641\n",
      "Iteration 435, loss = 0.27295248\n",
      "Iteration 436, loss = 0.27203856\n",
      "Iteration 437, loss = 0.27112310\n",
      "Iteration 438, loss = 0.27020677\n",
      "Iteration 439, loss = 0.26928895\n",
      "Iteration 440, loss = 0.26837205\n",
      "Iteration 441, loss = 0.26745241\n",
      "Iteration 442, loss = 0.26653482\n",
      "Iteration 443, loss = 0.26561486\n",
      "Iteration 444, loss = 0.26469658\n",
      "Iteration 445, loss = 0.26377840\n",
      "Iteration 446, loss = 0.26286033\n",
      "Iteration 447, loss = 0.26194233\n",
      "Iteration 448, loss = 0.26102461\n",
      "Iteration 449, loss = 0.26010713\n",
      "Iteration 450, loss = 0.25918996\n",
      "Iteration 451, loss = 0.25827350\n",
      "Iteration 452, loss = 0.25735761\n",
      "Iteration 453, loss = 0.25644238\n",
      "Iteration 454, loss = 0.25552792\n",
      "Iteration 455, loss = 0.25461433\n",
      "Iteration 456, loss = 0.25370194\n",
      "Iteration 457, loss = 0.25279082\n",
      "Iteration 458, loss = 0.25188097\n",
      "Iteration 459, loss = 0.25097332\n",
      "Iteration 460, loss = 0.25006697\n",
      "Iteration 461, loss = 0.24916299\n",
      "Iteration 462, loss = 0.24826077\n",
      "Iteration 463, loss = 0.24736015\n",
      "Iteration 464, loss = 0.24646121\n",
      "Iteration 465, loss = 0.24556411\n",
      "Iteration 466, loss = 0.24467033\n",
      "Iteration 467, loss = 0.24378148\n",
      "Iteration 468, loss = 0.24289126\n",
      "Iteration 469, loss = 0.24200707\n",
      "Iteration 470, loss = 0.24112594\n",
      "Iteration 471, loss = 0.24024612\n",
      "Iteration 472, loss = 0.23936694\n",
      "Iteration 473, loss = 0.23848834\n",
      "Iteration 474, loss = 0.23761530\n",
      "Iteration 475, loss = 0.23675176\n",
      "Iteration 476, loss = 0.23588049\n",
      "Iteration 477, loss = 0.23501818\n",
      "Iteration 478, loss = 0.23415767\n",
      "Iteration 479, loss = 0.23329947\n",
      "Iteration 480, loss = 0.23244546\n",
      "Iteration 481, loss = 0.23159558\n",
      "Iteration 482, loss = 0.23074664\n",
      "Iteration 483, loss = 0.22990371\n",
      "Iteration 484, loss = 0.22906406\n",
      "Iteration 485, loss = 0.22822649\n",
      "Iteration 486, loss = 0.22739115\n",
      "Iteration 487, loss = 0.22656175\n",
      "Iteration 488, loss = 0.22573417\n",
      "Iteration 489, loss = 0.22491072\n",
      "Iteration 490, loss = 0.22409109\n",
      "Iteration 491, loss = 0.22327388\n",
      "Iteration 492, loss = 0.22246190\n",
      "Iteration 493, loss = 0.22165344\n",
      "Iteration 494, loss = 0.22084751\n",
      "Iteration 495, loss = 0.22004535\n",
      "Iteration 496, loss = 0.21924730\n",
      "Iteration 497, loss = 0.21845340\n",
      "Iteration 498, loss = 0.21766185\n",
      "Iteration 499, loss = 0.21687389\n",
      "Iteration 500, loss = 0.21609070\n",
      "Iteration 501, loss = 0.21531129\n",
      "Iteration 502, loss = 0.21453344\n",
      "Iteration 503, loss = 0.21376133\n",
      "Iteration 504, loss = 0.21299348\n",
      "Iteration 505, loss = 0.21222783\n",
      "Iteration 506, loss = 0.21146577\n",
      "Iteration 507, loss = 0.21070764\n",
      "Iteration 508, loss = 0.20995350\n",
      "Iteration 509, loss = 0.20920158\n",
      "Iteration 510, loss = 0.20845545\n",
      "Iteration 511, loss = 0.20771250\n",
      "Iteration 512, loss = 0.20697249\n",
      "Iteration 513, loss = 0.20623660\n",
      "Iteration 514, loss = 0.20550659\n",
      "Iteration 515, loss = 0.20478017\n",
      "Iteration 516, loss = 0.20405749\n",
      "Iteration 517, loss = 0.20333844\n",
      "Iteration 518, loss = 0.20262293\n",
      "Iteration 519, loss = 0.20191100\n",
      "Iteration 520, loss = 0.20120335\n",
      "Iteration 521, loss = 0.20049906\n",
      "Iteration 522, loss = 0.19979834\n",
      "Iteration 523, loss = 0.19910173\n",
      "Iteration 524, loss = 0.19840869\n",
      "Iteration 525, loss = 0.19771923\n",
      "Iteration 526, loss = 0.19703337\n",
      "Iteration 527, loss = 0.19635111\n",
      "Iteration 528, loss = 0.19567247\n",
      "Iteration 529, loss = 0.19499748\n",
      "Iteration 530, loss = 0.19432657\n",
      "Iteration 531, loss = 0.19365878\n",
      "Iteration 532, loss = 0.19299507\n",
      "Iteration 533, loss = 0.19233502\n",
      "Iteration 534, loss = 0.19167853\n",
      "Iteration 535, loss = 0.19102554\n",
      "Iteration 536, loss = 0.19037605\n",
      "Iteration 537, loss = 0.18973015\n",
      "Iteration 538, loss = 0.18908779\n",
      "Iteration 539, loss = 0.18844893\n",
      "Iteration 540, loss = 0.18781362\n",
      "Iteration 541, loss = 0.18718177\n",
      "Iteration 542, loss = 0.18655360\n",
      "Iteration 543, loss = 0.18592848\n",
      "Iteration 544, loss = 0.18530700\n",
      "Iteration 545, loss = 0.18468892\n",
      "Iteration 546, loss = 0.18407421\n",
      "Iteration 547, loss = 0.18346286\n",
      "Iteration 548, loss = 0.18285485\n",
      "Iteration 549, loss = 0.18225017\n",
      "Iteration 550, loss = 0.18164880\n",
      "Iteration 551, loss = 0.18105072\n",
      "Iteration 552, loss = 0.18045610\n",
      "Iteration 553, loss = 0.17986460\n",
      "Iteration 554, loss = 0.17927657\n",
      "Iteration 555, loss = 0.17869166\n",
      "Iteration 556, loss = 0.17810995\n",
      "Iteration 557, loss = 0.17753146\n",
      "Iteration 558, loss = 0.17695660\n",
      "Iteration 559, loss = 0.17638460\n",
      "Iteration 560, loss = 0.17581534\n",
      "Iteration 561, loss = 0.17525039\n",
      "Iteration 562, loss = 0.17468780\n",
      "Iteration 563, loss = 0.17412814\n",
      "Iteration 564, loss = 0.17357136\n",
      "Iteration 565, loss = 0.17301884\n",
      "Iteration 566, loss = 0.17246897\n",
      "Iteration 567, loss = 0.17192146\n",
      "Iteration 568, loss = 0.17137677\n",
      "Iteration 569, loss = 0.17083604\n",
      "Iteration 570, loss = 0.17029799\n",
      "Iteration 571, loss = 0.16976273\n",
      "Iteration 572, loss = 0.16923075\n",
      "Iteration 573, loss = 0.16870172\n",
      "Iteration 574, loss = 0.16817552\n",
      "Iteration 575, loss = 0.16765264\n",
      "Iteration 576, loss = 0.16713238\n",
      "Iteration 577, loss = 0.16661475\n",
      "Iteration 578, loss = 0.16610067\n",
      "Iteration 579, loss = 0.16558886\n",
      "Iteration 580, loss = 0.16508011\n",
      "Iteration 581, loss = 0.16457431\n",
      "Iteration 582, loss = 0.16407166\n",
      "Iteration 583, loss = 0.16357200\n",
      "Iteration 584, loss = 0.16307495\n",
      "Iteration 585, loss = 0.16258088\n",
      "Iteration 586, loss = 0.16208912\n",
      "Iteration 587, loss = 0.16160026\n",
      "Iteration 588, loss = 0.16111405\n",
      "Iteration 589, loss = 0.16063079\n",
      "Iteration 590, loss = 0.16014970\n",
      "Iteration 591, loss = 0.15967157\n",
      "Iteration 592, loss = 0.15919600\n",
      "Iteration 593, loss = 0.15872384\n",
      "Iteration 594, loss = 0.15825329\n",
      "Iteration 595, loss = 0.15778570\n",
      "Iteration 596, loss = 0.15732115\n",
      "Iteration 597, loss = 0.15685919\n",
      "Iteration 598, loss = 0.15639981\n",
      "Iteration 599, loss = 0.15594301\n",
      "Iteration 600, loss = 0.15548863\n",
      "Iteration 601, loss = 0.15503659\n",
      "Iteration 602, loss = 0.15458686\n",
      "Iteration 603, loss = 0.15413949\n",
      "Iteration 604, loss = 0.15369495\n",
      "Iteration 605, loss = 0.15325322\n",
      "Iteration 606, loss = 0.15281407\n",
      "Iteration 607, loss = 0.15237722\n",
      "Iteration 608, loss = 0.15194260\n",
      "Iteration 609, loss = 0.15151021\n",
      "Iteration 610, loss = 0.15108023\n",
      "Iteration 611, loss = 0.15065266\n",
      "Iteration 612, loss = 0.15022781\n",
      "Iteration 613, loss = 0.14980503\n",
      "Iteration 614, loss = 0.14938472\n",
      "Iteration 615, loss = 0.14896649\n",
      "Iteration 616, loss = 0.14855054\n",
      "Iteration 617, loss = 0.14813696\n",
      "Iteration 618, loss = 0.14772569\n",
      "Iteration 619, loss = 0.14731695\n",
      "Iteration 620, loss = 0.14690977\n",
      "Iteration 621, loss = 0.14650532\n",
      "Iteration 622, loss = 0.14610290\n",
      "Iteration 623, loss = 0.14570257\n",
      "Iteration 624, loss = 0.14530411\n",
      "Iteration 625, loss = 0.14490791\n",
      "Iteration 626, loss = 0.14451457\n",
      "Iteration 627, loss = 0.14412318\n",
      "Iteration 628, loss = 0.14373232\n",
      "Iteration 629, loss = 0.14334536\n",
      "Iteration 630, loss = 0.14296015\n",
      "Iteration 631, loss = 0.14257658\n",
      "Iteration 632, loss = 0.14219527\n",
      "Iteration 633, loss = 0.14181616\n",
      "Iteration 634, loss = 0.14143879\n",
      "Iteration 635, loss = 0.14106317\n",
      "Iteration 636, loss = 0.14069071\n",
      "Iteration 637, loss = 0.14031963\n",
      "Iteration 638, loss = 0.13995032\n",
      "Iteration 639, loss = 0.13958269\n",
      "Iteration 640, loss = 0.13921829\n",
      "Iteration 641, loss = 0.13885445\n",
      "Iteration 642, loss = 0.13849330\n",
      "Iteration 643, loss = 0.13813424\n",
      "Iteration 644, loss = 0.13777639\n",
      "Iteration 645, loss = 0.13742016\n",
      "Iteration 646, loss = 0.13706751\n",
      "Iteration 647, loss = 0.13671448\n",
      "Iteration 648, loss = 0.13636398\n",
      "Iteration 649, loss = 0.13601579\n",
      "Iteration 650, loss = 0.13566935\n",
      "Iteration 651, loss = 0.13532404\n",
      "Iteration 652, loss = 0.13498244\n",
      "Iteration 653, loss = 0.13464061\n",
      "Iteration 654, loss = 0.13430124\n",
      "Iteration 655, loss = 0.13396446\n",
      "Iteration 656, loss = 0.13362882\n",
      "Iteration 657, loss = 0.13329419\n",
      "Iteration 658, loss = 0.13296095\n",
      "Iteration 659, loss = 0.13262940\n",
      "Iteration 660, loss = 0.13230052\n",
      "Iteration 661, loss = 0.13197246\n",
      "Iteration 662, loss = 0.13164396\n",
      "Iteration 663, loss = 0.13131825\n",
      "Iteration 664, loss = 0.13099369\n",
      "Iteration 665, loss = 0.13067026\n",
      "Iteration 666, loss = 0.13034872\n",
      "Iteration 667, loss = 0.13002842\n",
      "Iteration 668, loss = 0.12970973\n",
      "Iteration 669, loss = 0.12939325\n",
      "Iteration 670, loss = 0.12907674\n",
      "Iteration 671, loss = 0.12876253\n",
      "Iteration 672, loss = 0.12844958\n",
      "Iteration 673, loss = 0.12813832\n",
      "Iteration 674, loss = 0.12782879\n",
      "Iteration 675, loss = 0.12752053\n",
      "Iteration 676, loss = 0.12721412\n",
      "Iteration 677, loss = 0.12690871\n",
      "Iteration 678, loss = 0.12660514\n",
      "Iteration 679, loss = 0.12630271\n",
      "Iteration 680, loss = 0.12600201\n",
      "Iteration 681, loss = 0.12570318\n",
      "Iteration 682, loss = 0.12540529\n",
      "Iteration 683, loss = 0.12511020\n",
      "Iteration 684, loss = 0.12481591\n",
      "Iteration 685, loss = 0.12452438\n",
      "Iteration 686, loss = 0.12423630\n",
      "Iteration 687, loss = 0.12395005\n",
      "Iteration 688, loss = 0.12366545\n",
      "Iteration 689, loss = 0.12338239\n",
      "Iteration 690, loss = 0.12310097\n",
      "Iteration 691, loss = 0.12282111\n",
      "Iteration 692, loss = 0.12254271\n",
      "Iteration 693, loss = 0.12226589\n",
      "Iteration 694, loss = 0.12199041\n",
      "Iteration 695, loss = 0.12171672\n",
      "Iteration 696, loss = 0.12144398\n",
      "Iteration 697, loss = 0.12117291\n",
      "Iteration 698, loss = 0.12090321\n",
      "Iteration 699, loss = 0.12063495\n",
      "Iteration 700, loss = 0.12036793\n",
      "Iteration 701, loss = 0.12010271\n",
      "Iteration 702, loss = 0.11983853\n",
      "Iteration 703, loss = 0.11957525\n",
      "Iteration 704, loss = 0.11931354\n",
      "Iteration 705, loss = 0.11905324\n",
      "Iteration 706, loss = 0.11879470\n",
      "Iteration 707, loss = 0.11853740\n",
      "Iteration 708, loss = 0.11828154\n",
      "Iteration 709, loss = 0.11802723\n",
      "Iteration 710, loss = 0.11777399\n",
      "Iteration 711, loss = 0.11752176\n",
      "Iteration 712, loss = 0.11727106\n",
      "Iteration 713, loss = 0.11702157\n",
      "Iteration 714, loss = 0.11677331\n",
      "Iteration 715, loss = 0.11652618\n",
      "Iteration 716, loss = 0.11628034\n",
      "Iteration 717, loss = 0.11603577\n",
      "Iteration 718, loss = 0.11579228\n",
      "Iteration 719, loss = 0.11554972\n",
      "Iteration 720, loss = 0.11530830\n",
      "Iteration 721, loss = 0.11506867\n",
      "Iteration 722, loss = 0.11482980\n",
      "Iteration 723, loss = 0.11459167\n",
      "Iteration 724, loss = 0.11435502\n",
      "Iteration 725, loss = 0.11411942\n",
      "Iteration 726, loss = 0.11388484\n",
      "Iteration 727, loss = 0.11365164\n",
      "Iteration 728, loss = 0.11341912\n",
      "Iteration 729, loss = 0.11318766\n",
      "Iteration 730, loss = 0.11295752\n",
      "Iteration 731, loss = 0.11272796\n",
      "Iteration 732, loss = 0.11249944\n",
      "Iteration 733, loss = 0.11227185\n",
      "Iteration 734, loss = 0.11204492\n",
      "Iteration 735, loss = 0.11181949\n",
      "Iteration 736, loss = 0.11159552\n",
      "Iteration 737, loss = 0.11137151\n",
      "Iteration 738, loss = 0.11114898\n",
      "Iteration 739, loss = 0.11092809\n",
      "Iteration 740, loss = 0.11070765\n",
      "Iteration 741, loss = 0.11048770\n",
      "Iteration 742, loss = 0.11026963\n",
      "Iteration 743, loss = 0.11005320\n",
      "Iteration 744, loss = 0.10983687\n",
      "Iteration 745, loss = 0.10962157\n",
      "Iteration 746, loss = 0.10940822\n",
      "Iteration 747, loss = 0.10919583\n",
      "Iteration 748, loss = 0.10898390\n",
      "Iteration 749, loss = 0.10877298\n",
      "Iteration 750, loss = 0.10856355\n",
      "Iteration 751, loss = 0.10835572\n",
      "Iteration 752, loss = 0.10814765\n",
      "Iteration 753, loss = 0.10794006\n",
      "Iteration 754, loss = 0.10773473\n",
      "Iteration 755, loss = 0.10753091\n",
      "Iteration 756, loss = 0.10732765\n",
      "Iteration 757, loss = 0.10712473\n",
      "Iteration 758, loss = 0.10692257\n",
      "Iteration 759, loss = 0.10672173\n",
      "Iteration 760, loss = 0.10652201\n",
      "Iteration 761, loss = 0.10632279\n",
      "Iteration 762, loss = 0.10612444\n",
      "Iteration 763, loss = 0.10592685\n",
      "Iteration 764, loss = 0.10573087\n",
      "Iteration 765, loss = 0.10553549\n",
      "Iteration 766, loss = 0.10534081\n",
      "Iteration 767, loss = 0.10514706\n",
      "Iteration 768, loss = 0.10495442\n",
      "Iteration 769, loss = 0.10476229\n",
      "Iteration 770, loss = 0.10457117\n",
      "Iteration 771, loss = 0.10438091\n",
      "Iteration 772, loss = 0.10419149\n",
      "Iteration 773, loss = 0.10400288\n",
      "Iteration 774, loss = 0.10381530\n",
      "Iteration 775, loss = 0.10362841\n",
      "Iteration 776, loss = 0.10344247\n",
      "Iteration 777, loss = 0.10325727\n",
      "Iteration 778, loss = 0.10307277\n",
      "Iteration 779, loss = 0.10288902\n",
      "Iteration 780, loss = 0.10270621\n",
      "Iteration 781, loss = 0.10252410\n",
      "Iteration 782, loss = 0.10234281\n",
      "Iteration 783, loss = 0.10216225\n",
      "Iteration 784, loss = 0.10198261\n",
      "Iteration 785, loss = 0.10180359\n",
      "Iteration 786, loss = 0.10162545\n",
      "Iteration 787, loss = 0.10144802\n",
      "Iteration 788, loss = 0.10127124\n",
      "Iteration 789, loss = 0.10109531\n",
      "Iteration 790, loss = 0.10092021\n",
      "Iteration 791, loss = 0.10074557\n",
      "Iteration 792, loss = 0.10057160\n",
      "Iteration 793, loss = 0.10039890\n",
      "Iteration 794, loss = 0.10022667\n",
      "Iteration 795, loss = 0.10005481\n",
      "Iteration 796, loss = 0.09988392\n",
      "Iteration 797, loss = 0.09971397\n",
      "Iteration 798, loss = 0.09954449\n",
      "Iteration 799, loss = 0.09937564\n",
      "Iteration 800, loss = 0.09920765\n",
      "Iteration 801, loss = 0.09904054\n",
      "Iteration 802, loss = 0.09887383\n",
      "Iteration 803, loss = 0.09870781\n",
      "Iteration 804, loss = 0.09854278\n",
      "Iteration 805, loss = 0.09837811\n",
      "Iteration 806, loss = 0.09821411\n",
      "Iteration 807, loss = 0.09805111\n",
      "Iteration 808, loss = 0.09788868\n",
      "Iteration 809, loss = 0.09772680\n",
      "Iteration 810, loss = 0.09756563\n",
      "Iteration 811, loss = 0.09740523\n",
      "Iteration 812, loss = 0.09724545\n",
      "Iteration 813, loss = 0.09708628\n",
      "Iteration 814, loss = 0.09692780\n",
      "Iteration 815, loss = 0.09677000\n",
      "Iteration 816, loss = 0.09661269\n",
      "Iteration 817, loss = 0.09645617\n",
      "Iteration 818, loss = 0.09630019\n",
      "Iteration 819, loss = 0.09614490\n",
      "Iteration 820, loss = 0.09599046\n",
      "Iteration 821, loss = 0.09583648\n",
      "Iteration 822, loss = 0.09568316\n",
      "Iteration 823, loss = 0.09553062\n",
      "Iteration 824, loss = 0.09537857\n",
      "Iteration 825, loss = 0.09522717\n",
      "Iteration 826, loss = 0.09507649\n",
      "Iteration 827, loss = 0.09492603\n",
      "Iteration 828, loss = 0.09477626\n",
      "Iteration 829, loss = 0.09462764\n",
      "Iteration 830, loss = 0.09447943\n",
      "Iteration 831, loss = 0.09433174\n",
      "Iteration 832, loss = 0.09418441\n",
      "Iteration 833, loss = 0.09403759\n",
      "Iteration 834, loss = 0.09389171\n",
      "Iteration 835, loss = 0.09374634\n",
      "Iteration 836, loss = 0.09360173\n",
      "Iteration 837, loss = 0.09345752\n",
      "Iteration 838, loss = 0.09331369\n",
      "Iteration 839, loss = 0.09317060\n",
      "Iteration 840, loss = 0.09302826\n",
      "Iteration 841, loss = 0.09288630\n",
      "Iteration 842, loss = 0.09274499\n",
      "Iteration 843, loss = 0.09260425\n",
      "Iteration 844, loss = 0.09246396\n",
      "Iteration 845, loss = 0.09232420\n",
      "Iteration 846, loss = 0.09218504\n",
      "Iteration 847, loss = 0.09204653\n",
      "Iteration 848, loss = 0.09190847\n",
      "Iteration 849, loss = 0.09177114\n",
      "Iteration 850, loss = 0.09163425\n",
      "Iteration 851, loss = 0.09149780\n",
      "Iteration 852, loss = 0.09136187\n",
      "Iteration 853, loss = 0.09122657\n",
      "Iteration 854, loss = 0.09109183\n",
      "Iteration 855, loss = 0.09095758\n",
      "Iteration 856, loss = 0.09082375\n",
      "Iteration 857, loss = 0.09069064\n",
      "Iteration 858, loss = 0.09055800\n",
      "Iteration 859, loss = 0.09042582\n",
      "Iteration 860, loss = 0.09029401\n",
      "Iteration 861, loss = 0.09016283\n",
      "Iteration 862, loss = 0.09003225\n",
      "Iteration 863, loss = 0.08990215\n",
      "Iteration 864, loss = 0.08977240\n",
      "Iteration 865, loss = 0.08964320\n",
      "Iteration 866, loss = 0.08951459\n",
      "Iteration 867, loss = 0.08938635\n",
      "Iteration 868, loss = 0.08925873\n",
      "Iteration 869, loss = 0.08913158\n",
      "Iteration 870, loss = 0.08900491\n",
      "Iteration 871, loss = 0.08887866\n",
      "Iteration 872, loss = 0.08875302\n",
      "Iteration 873, loss = 0.08862777\n",
      "Iteration 874, loss = 0.08850301\n",
      "Iteration 875, loss = 0.08837871\n",
      "Iteration 876, loss = 0.08825498\n",
      "Iteration 877, loss = 0.08813165\n",
      "Iteration 878, loss = 0.08800873\n",
      "Iteration 879, loss = 0.08788627\n",
      "Iteration 880, loss = 0.08776420\n",
      "Iteration 881, loss = 0.08764288\n",
      "Iteration 882, loss = 0.08752192\n",
      "Iteration 883, loss = 0.08740127\n",
      "Iteration 884, loss = 0.08728094\n",
      "Iteration 885, loss = 0.08716139\n",
      "Iteration 886, loss = 0.08704223\n",
      "Iteration 887, loss = 0.08692346\n",
      "Iteration 888, loss = 0.08680529\n",
      "Iteration 889, loss = 0.08668726\n",
      "Iteration 890, loss = 0.08656980\n",
      "Iteration 891, loss = 0.08645278\n",
      "Iteration 892, loss = 0.08633627\n",
      "Iteration 893, loss = 0.08622019\n",
      "Iteration 894, loss = 0.08610465\n",
      "Iteration 895, loss = 0.08598944\n",
      "Iteration 896, loss = 0.08587471\n",
      "Iteration 897, loss = 0.08576046\n",
      "Iteration 898, loss = 0.08564656\n",
      "Iteration 899, loss = 0.08553304\n",
      "Iteration 900, loss = 0.08541993\n",
      "Iteration 901, loss = 0.08530725\n",
      "Iteration 902, loss = 0.08519518\n",
      "Iteration 903, loss = 0.08508338\n",
      "Iteration 904, loss = 0.08497184\n",
      "Iteration 905, loss = 0.08486091\n",
      "Iteration 906, loss = 0.08475057\n",
      "Iteration 907, loss = 0.08464037\n",
      "Iteration 908, loss = 0.08453069\n",
      "Iteration 909, loss = 0.08442139\n",
      "Iteration 910, loss = 0.08431246\n",
      "Iteration 911, loss = 0.08420391\n",
      "Iteration 912, loss = 0.08409576\n",
      "Iteration 913, loss = 0.08398801\n",
      "Iteration 914, loss = 0.08388073\n",
      "Iteration 915, loss = 0.08377386\n",
      "Iteration 916, loss = 0.08366752\n",
      "Iteration 917, loss = 0.08356149\n",
      "Iteration 918, loss = 0.08345591\n",
      "Iteration 919, loss = 0.08335069\n",
      "Iteration 920, loss = 0.08324581\n",
      "Iteration 921, loss = 0.08314130\n",
      "Iteration 922, loss = 0.08303714\n",
      "Iteration 923, loss = 0.08293334\n",
      "Iteration 924, loss = 0.08283001\n",
      "Iteration 925, loss = 0.08272691\n",
      "Iteration 926, loss = 0.08262422\n",
      "Iteration 927, loss = 0.08252193\n",
      "Iteration 928, loss = 0.08241999\n",
      "Iteration 929, loss = 0.08231841\n",
      "Iteration 930, loss = 0.08221722\n",
      "Iteration 931, loss = 0.08211639\n",
      "Iteration 932, loss = 0.08201591\n",
      "Iteration 933, loss = 0.08191579\n",
      "Iteration 934, loss = 0.08181602\n",
      "Iteration 935, loss = 0.08171661\n",
      "Iteration 936, loss = 0.08161755\n",
      "Iteration 937, loss = 0.08151882\n",
      "Iteration 938, loss = 0.08142050\n",
      "Iteration 939, loss = 0.08132243\n",
      "Iteration 940, loss = 0.08122476\n",
      "Iteration 941, loss = 0.08112743\n",
      "Iteration 942, loss = 0.08103043\n",
      "Iteration 943, loss = 0.08093377\n",
      "Iteration 944, loss = 0.08083745\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35771515\n",
      "Iteration 2, loss = 1.33028224\n",
      "Iteration 3, loss = 1.30337479\n",
      "Iteration 4, loss = 1.27698898\n",
      "Iteration 5, loss = 1.25110636\n",
      "Iteration 6, loss = 1.22581117\n",
      "Iteration 7, loss = 1.20113194\n",
      "Iteration 8, loss = 1.17706852\n",
      "Iteration 9, loss = 1.15360906\n",
      "Iteration 10, loss = 1.13074113\n",
      "Iteration 11, loss = 1.10852997\n",
      "Iteration 12, loss = 1.08698852\n",
      "Iteration 13, loss = 1.06616725\n",
      "Iteration 14, loss = 1.04604292\n",
      "Iteration 15, loss = 1.02663495\n",
      "Iteration 16, loss = 1.00794286\n",
      "Iteration 17, loss = 0.98999628\n",
      "Iteration 18, loss = 0.97275542\n",
      "Iteration 19, loss = 0.95624511\n",
      "Iteration 20, loss = 0.94044768\n",
      "Iteration 21, loss = 0.92533910\n",
      "Iteration 22, loss = 0.91093493\n",
      "Iteration 23, loss = 0.89719725\n",
      "Iteration 24, loss = 0.88413857\n",
      "Iteration 25, loss = 0.87175274\n",
      "Iteration 26, loss = 0.86004347\n",
      "Iteration 27, loss = 0.84897663\n",
      "Iteration 28, loss = 0.83850959\n",
      "Iteration 29, loss = 0.82864611\n",
      "Iteration 30, loss = 0.81933114\n",
      "Iteration 31, loss = 0.81058818\n",
      "Iteration 32, loss = 0.80235101\n",
      "Iteration 33, loss = 0.79461758\n",
      "Iteration 34, loss = 0.78743027\n",
      "Iteration 35, loss = 0.78076523\n",
      "Iteration 36, loss = 0.77454702\n",
      "Iteration 37, loss = 0.76874542\n",
      "Iteration 38, loss = 0.76335694\n",
      "Iteration 39, loss = 0.75830375\n",
      "Iteration 40, loss = 0.75362497\n",
      "Iteration 41, loss = 0.74929341\n",
      "Iteration 42, loss = 0.74526189\n",
      "Iteration 43, loss = 0.74155073\n",
      "Iteration 44, loss = 0.73806066\n",
      "Iteration 45, loss = 0.73489587\n",
      "Iteration 46, loss = 0.73186256\n",
      "Iteration 47, loss = 0.72894940\n",
      "Iteration 48, loss = 0.72613795\n",
      "Iteration 49, loss = 0.72339763\n",
      "Iteration 50, loss = 0.72071402\n",
      "Iteration 51, loss = 0.71807477\n",
      "Iteration 52, loss = 0.71549009\n",
      "Iteration 53, loss = 0.71293489\n",
      "Iteration 54, loss = 0.71040321\n",
      "Iteration 55, loss = 0.70787802\n",
      "Iteration 56, loss = 0.70535458\n",
      "Iteration 57, loss = 0.70283787\n",
      "Iteration 58, loss = 0.70031197\n",
      "Iteration 59, loss = 0.69776891\n",
      "Iteration 60, loss = 0.69518902\n",
      "Iteration 61, loss = 0.69259964\n",
      "Iteration 62, loss = 0.68998139\n",
      "Iteration 63, loss = 0.68734105\n",
      "Iteration 64, loss = 0.68466150\n",
      "Iteration 65, loss = 0.68193829\n",
      "Iteration 66, loss = 0.67911636\n",
      "Iteration 67, loss = 0.67623728\n",
      "Iteration 68, loss = 0.67331647\n",
      "Iteration 69, loss = 0.67039899\n",
      "Iteration 70, loss = 0.66747044\n",
      "Iteration 71, loss = 0.66453946\n",
      "Iteration 72, loss = 0.66162562\n",
      "Iteration 73, loss = 0.65874041\n",
      "Iteration 74, loss = 0.65595070\n",
      "Iteration 75, loss = 0.65320184\n",
      "Iteration 76, loss = 0.65050474\n",
      "Iteration 77, loss = 0.64783743\n",
      "Iteration 78, loss = 0.64521785\n",
      "Iteration 79, loss = 0.64260691\n",
      "Iteration 80, loss = 0.64002817\n",
      "Iteration 81, loss = 0.63748053\n",
      "Iteration 82, loss = 0.63493915\n",
      "Iteration 83, loss = 0.63237701\n",
      "Iteration 84, loss = 0.62979845\n",
      "Iteration 85, loss = 0.62724421\n",
      "Iteration 86, loss = 0.62471739\n",
      "Iteration 87, loss = 0.62218179\n",
      "Iteration 88, loss = 0.61964843\n",
      "Iteration 89, loss = 0.61712904\n",
      "Iteration 90, loss = 0.61461193\n",
      "Iteration 91, loss = 0.61209971\n",
      "Iteration 92, loss = 0.60959660\n",
      "Iteration 93, loss = 0.60710410\n",
      "Iteration 94, loss = 0.60462157\n",
      "Iteration 95, loss = 0.60215114\n",
      "Iteration 96, loss = 0.59969513\n",
      "Iteration 97, loss = 0.59725413\n",
      "Iteration 98, loss = 0.59482935\n",
      "Iteration 99, loss = 0.59242196\n",
      "Iteration 100, loss = 0.59003205\n",
      "Iteration 101, loss = 0.58766025\n",
      "Iteration 102, loss = 0.58530820\n",
      "Iteration 103, loss = 0.58297676\n",
      "Iteration 104, loss = 0.58066396\n",
      "Iteration 105, loss = 0.57836951\n",
      "Iteration 106, loss = 0.57609345\n",
      "Iteration 107, loss = 0.57383646\n",
      "Iteration 108, loss = 0.57159824\n",
      "Iteration 109, loss = 0.56937818\n",
      "Iteration 110, loss = 0.56717618\n",
      "Iteration 111, loss = 0.56499229\n",
      "Iteration 112, loss = 0.56282645\n",
      "Iteration 113, loss = 0.56067838\n",
      "Iteration 114, loss = 0.55854800\n",
      "Iteration 115, loss = 0.55646269\n",
      "Iteration 116, loss = 0.55449193\n",
      "Iteration 117, loss = 0.55257894\n",
      "Iteration 118, loss = 0.55070517\n",
      "Iteration 119, loss = 0.54891757\n",
      "Iteration 120, loss = 0.54729320\n",
      "Iteration 121, loss = 0.54574691\n",
      "Iteration 122, loss = 0.54422999\n",
      "Iteration 123, loss = 0.54271404\n",
      "Iteration 124, loss = 0.54119064\n",
      "Iteration 125, loss = 0.53964715\n",
      "Iteration 126, loss = 0.53808703\n",
      "Iteration 127, loss = 0.53650791\n",
      "Iteration 128, loss = 0.53492054\n",
      "Iteration 129, loss = 0.53333860\n",
      "Iteration 130, loss = 0.53176759\n",
      "Iteration 131, loss = 0.53021363\n",
      "Iteration 132, loss = 0.52869051\n",
      "Iteration 133, loss = 0.52718971\n",
      "Iteration 134, loss = 0.52570396\n",
      "Iteration 135, loss = 0.52424314\n",
      "Iteration 136, loss = 0.52281415\n",
      "Iteration 137, loss = 0.52142704\n",
      "Iteration 138, loss = 0.52006283\n",
      "Iteration 139, loss = 0.51871013\n",
      "Iteration 140, loss = 0.51737431\n",
      "Iteration 141, loss = 0.51604632\n",
      "Iteration 142, loss = 0.51472140\n",
      "Iteration 143, loss = 0.51339978\n",
      "Iteration 144, loss = 0.51208424\n",
      "Iteration 145, loss = 0.51077411\n",
      "Iteration 146, loss = 0.50946863\n",
      "Iteration 147, loss = 0.50818213\n",
      "Iteration 148, loss = 0.50690731\n",
      "Iteration 149, loss = 0.50564853\n",
      "Iteration 150, loss = 0.50440715\n",
      "Iteration 151, loss = 0.50317867\n",
      "Iteration 152, loss = 0.50196157\n",
      "Iteration 153, loss = 0.50075601\n",
      "Iteration 154, loss = 0.49955898\n",
      "Iteration 155, loss = 0.49836992\n",
      "Iteration 156, loss = 0.49718705\n",
      "Iteration 157, loss = 0.49601056\n",
      "Iteration 158, loss = 0.49484295\n",
      "Iteration 159, loss = 0.49368356\n",
      "Iteration 160, loss = 0.49253229\n",
      "Iteration 161, loss = 0.49138904\n",
      "Iteration 162, loss = 0.49025432\n",
      "Iteration 163, loss = 0.48912785\n",
      "Iteration 164, loss = 0.48801221\n",
      "Iteration 165, loss = 0.48690831\n",
      "Iteration 166, loss = 0.48581382\n",
      "Iteration 167, loss = 0.48472784\n",
      "Iteration 168, loss = 0.48364870\n",
      "Iteration 169, loss = 0.48257566\n",
      "Iteration 170, loss = 0.48150878\n",
      "Iteration 171, loss = 0.48044855\n",
      "Iteration 172, loss = 0.47939545\n",
      "Iteration 173, loss = 0.47835006\n",
      "Iteration 174, loss = 0.47731370\n",
      "Iteration 175, loss = 0.47628489\n",
      "Iteration 176, loss = 0.47526398\n",
      "Iteration 177, loss = 0.47425155\n",
      "Iteration 178, loss = 0.47324601\n",
      "Iteration 179, loss = 0.47224712\n",
      "Iteration 180, loss = 0.47125526\n",
      "Iteration 181, loss = 0.47027054\n",
      "Iteration 182, loss = 0.46929160\n",
      "Iteration 183, loss = 0.46831839\n",
      "Iteration 184, loss = 0.46735085\n",
      "Iteration 185, loss = 0.46638900\n",
      "Iteration 186, loss = 0.46543446\n",
      "Iteration 187, loss = 0.46448571\n",
      "Iteration 188, loss = 0.46354290\n",
      "Iteration 189, loss = 0.46260645\n",
      "Iteration 190, loss = 0.46167595\n",
      "Iteration 191, loss = 0.46075090\n",
      "Iteration 192, loss = 0.45983122\n",
      "Iteration 193, loss = 0.45891679\n",
      "Iteration 194, loss = 0.45800754\n",
      "Iteration 195, loss = 0.45710338\n",
      "Iteration 196, loss = 0.45620421\n",
      "Iteration 197, loss = 0.45530993\n",
      "Iteration 198, loss = 0.45442047\n",
      "Iteration 199, loss = 0.45353574\n",
      "Iteration 200, loss = 0.45265639\n",
      "Iteration 201, loss = 0.45178131\n",
      "Iteration 202, loss = 0.45091040\n",
      "Iteration 203, loss = 0.45004365\n",
      "Iteration 204, loss = 0.44918164\n",
      "Iteration 205, loss = 0.44832446\n",
      "Iteration 206, loss = 0.44747196\n",
      "Iteration 207, loss = 0.44662359\n",
      "Iteration 208, loss = 0.44577928\n",
      "Iteration 209, loss = 0.44493895\n",
      "Iteration 210, loss = 0.44410273\n",
      "Iteration 211, loss = 0.44327111\n",
      "Iteration 212, loss = 0.44244367\n",
      "Iteration 213, loss = 0.44161994\n",
      "Iteration 214, loss = 0.44079993\n",
      "Iteration 215, loss = 0.43998359\n",
      "Iteration 216, loss = 0.43917104\n",
      "Iteration 217, loss = 0.43836283\n",
      "Iteration 218, loss = 0.43755821\n",
      "Iteration 219, loss = 0.43675732\n",
      "Iteration 220, loss = 0.43595991\n",
      "Iteration 221, loss = 0.43516591\n",
      "Iteration 222, loss = 0.43437522\n",
      "Iteration 223, loss = 0.43358790\n",
      "Iteration 224, loss = 0.43280391\n",
      "Iteration 225, loss = 0.43202344\n",
      "Iteration 226, loss = 0.43124606\n",
      "Iteration 227, loss = 0.43047190\n",
      "Iteration 228, loss = 0.42970102\n",
      "Iteration 229, loss = 0.42893321\n",
      "Iteration 230, loss = 0.42816880\n",
      "Iteration 231, loss = 0.42740742\n",
      "Iteration 232, loss = 0.42664887\n",
      "Iteration 233, loss = 0.42589342\n",
      "Iteration 234, loss = 0.42514036\n",
      "Iteration 235, loss = 0.42438980\n",
      "Iteration 236, loss = 0.42364208\n",
      "Iteration 237, loss = 0.42289738\n",
      "Iteration 238, loss = 0.42215527\n",
      "Iteration 239, loss = 0.42141572\n",
      "Iteration 240, loss = 0.42067869\n",
      "Iteration 241, loss = 0.41994415\n",
      "Iteration 242, loss = 0.41921208\n",
      "Iteration 243, loss = 0.41848238\n",
      "Iteration 244, loss = 0.41775508\n",
      "Iteration 245, loss = 0.41703012\n",
      "Iteration 246, loss = 0.41630745\n",
      "Iteration 247, loss = 0.41558704\n",
      "Iteration 248, loss = 0.41486885\n",
      "Iteration 249, loss = 0.41415285\n",
      "Iteration 250, loss = 0.41343905\n",
      "Iteration 251, loss = 0.41272734\n",
      "Iteration 252, loss = 0.41201772\n",
      "Iteration 253, loss = 0.41131014\n",
      "Iteration 254, loss = 0.41060455\n",
      "Iteration 255, loss = 0.40990093\n",
      "Iteration 256, loss = 0.40919922\n",
      "Iteration 257, loss = 0.40849942\n",
      "Iteration 258, loss = 0.40780145\n",
      "Iteration 259, loss = 0.40710530\n",
      "Iteration 260, loss = 0.40641094\n",
      "Iteration 261, loss = 0.40571832\n",
      "Iteration 262, loss = 0.40502741\n",
      "Iteration 263, loss = 0.40433914\n",
      "Iteration 264, loss = 0.40365347\n",
      "Iteration 265, loss = 0.40296923\n",
      "Iteration 266, loss = 0.40228728\n",
      "Iteration 267, loss = 0.40160721\n",
      "Iteration 268, loss = 0.40092869\n",
      "Iteration 269, loss = 0.40025232\n",
      "Iteration 270, loss = 0.39957826\n",
      "Iteration 271, loss = 0.39890613\n",
      "Iteration 272, loss = 0.39823549\n",
      "Iteration 273, loss = 0.39756633\n",
      "Iteration 274, loss = 0.39689863\n",
      "Iteration 275, loss = 0.39623234\n",
      "Iteration 276, loss = 0.39556794\n",
      "Iteration 277, loss = 0.39490535\n",
      "Iteration 278, loss = 0.39424418\n",
      "Iteration 279, loss = 0.39358440\n",
      "Iteration 280, loss = 0.39292610\n",
      "Iteration 281, loss = 0.39226973\n",
      "Iteration 282, loss = 0.39161478\n",
      "Iteration 283, loss = 0.39096145\n",
      "Iteration 284, loss = 0.39030996\n",
      "Iteration 285, loss = 0.38965924\n",
      "Iteration 286, loss = 0.38901008\n",
      "Iteration 287, loss = 0.38836244\n",
      "Iteration 288, loss = 0.38771605\n",
      "Iteration 289, loss = 0.38707090\n",
      "Iteration 290, loss = 0.38642699\n",
      "Iteration 291, loss = 0.38578430\n",
      "Iteration 292, loss = 0.38514283\n",
      "Iteration 293, loss = 0.38450259\n",
      "Iteration 294, loss = 0.38386354\n",
      "Iteration 295, loss = 0.38322594\n",
      "Iteration 296, loss = 0.38258926\n",
      "Iteration 297, loss = 0.38195354\n",
      "Iteration 298, loss = 0.38131918\n",
      "Iteration 299, loss = 0.38068591\n",
      "Iteration 300, loss = 0.38005372\n",
      "Iteration 301, loss = 0.37942276\n",
      "Iteration 302, loss = 0.37879286\n",
      "Iteration 303, loss = 0.37816442\n",
      "Iteration 304, loss = 0.37753727\n",
      "Iteration 305, loss = 0.37691105\n",
      "Iteration 306, loss = 0.37628579\n",
      "Iteration 307, loss = 0.37566149\n",
      "Iteration 308, loss = 0.37503814\n",
      "Iteration 309, loss = 0.37441577\n",
      "Iteration 310, loss = 0.37379480\n",
      "Iteration 311, loss = 0.37317499\n",
      "Iteration 312, loss = 0.37255618\n",
      "Iteration 313, loss = 0.37193835\n",
      "Iteration 314, loss = 0.37132150\n",
      "Iteration 315, loss = 0.37070578\n",
      "Iteration 316, loss = 0.37009101\n",
      "Iteration 317, loss = 0.36947697\n",
      "Iteration 318, loss = 0.36886362\n",
      "Iteration 319, loss = 0.36825131\n",
      "Iteration 320, loss = 0.36764070\n",
      "Iteration 321, loss = 0.36703150\n",
      "Iteration 322, loss = 0.36642110\n",
      "Iteration 323, loss = 0.36580982\n",
      "Iteration 324, loss = 0.36517948\n",
      "Iteration 325, loss = 0.36450625\n",
      "Iteration 326, loss = 0.36380082\n",
      "Iteration 327, loss = 0.36315580\n",
      "Iteration 328, loss = 0.36255454\n",
      "Iteration 329, loss = 0.36191635\n",
      "Iteration 330, loss = 0.36124553\n",
      "Iteration 331, loss = 0.36055527\n",
      "Iteration 332, loss = 0.35986622\n",
      "Iteration 333, loss = 0.35923251\n",
      "Iteration 334, loss = 0.35856047\n",
      "Iteration 335, loss = 0.35785644\n",
      "Iteration 336, loss = 0.35718762\n",
      "Iteration 337, loss = 0.35652394\n",
      "Iteration 338, loss = 0.35584509\n",
      "Iteration 339, loss = 0.35514984\n",
      "Iteration 340, loss = 0.35444182\n",
      "Iteration 341, loss = 0.35372680\n",
      "Iteration 342, loss = 0.35301536\n",
      "Iteration 343, loss = 0.35231629\n",
      "Iteration 344, loss = 0.35160432\n",
      "Iteration 345, loss = 0.35086944\n",
      "Iteration 346, loss = 0.35014116\n",
      "Iteration 347, loss = 0.34941771\n",
      "Iteration 348, loss = 0.34868567\n",
      "Iteration 349, loss = 0.34794234\n",
      "Iteration 350, loss = 0.34718959\n",
      "Iteration 351, loss = 0.34643209\n",
      "Iteration 352, loss = 0.34567379\n",
      "Iteration 353, loss = 0.34491530\n",
      "Iteration 354, loss = 0.34414947\n",
      "Iteration 355, loss = 0.34337613\n",
      "Iteration 356, loss = 0.34259879\n",
      "Iteration 357, loss = 0.34181714\n",
      "Iteration 358, loss = 0.34103111\n",
      "Iteration 359, loss = 0.34024122\n",
      "Iteration 360, loss = 0.33944395\n",
      "Iteration 361, loss = 0.33864077\n",
      "Iteration 362, loss = 0.33783492\n",
      "Iteration 363, loss = 0.33702579\n",
      "Iteration 364, loss = 0.33621328\n",
      "Iteration 365, loss = 0.33539608\n",
      "Iteration 366, loss = 0.33457407\n",
      "Iteration 367, loss = 0.33375006\n",
      "Iteration 368, loss = 0.33292377\n",
      "Iteration 369, loss = 0.33209298\n",
      "Iteration 370, loss = 0.33125892\n",
      "Iteration 371, loss = 0.33042265\n",
      "Iteration 372, loss = 0.32958395\n",
      "Iteration 373, loss = 0.32874243\n",
      "Iteration 374, loss = 0.32789807\n",
      "Iteration 375, loss = 0.32705093\n",
      "Iteration 376, loss = 0.32620166\n",
      "Iteration 377, loss = 0.32535059\n",
      "Iteration 378, loss = 0.32449757\n",
      "Iteration 379, loss = 0.32364252\n",
      "Iteration 380, loss = 0.32278553\n",
      "Iteration 381, loss = 0.32192672\n",
      "Iteration 382, loss = 0.32106620\n",
      "Iteration 383, loss = 0.32020449\n",
      "Iteration 384, loss = 0.31934136\n",
      "Iteration 385, loss = 0.31847688\n",
      "Iteration 386, loss = 0.31761118\n",
      "Iteration 387, loss = 0.31674437\n",
      "Iteration 388, loss = 0.31587658\n",
      "Iteration 389, loss = 0.31500793\n",
      "Iteration 390, loss = 0.31413881\n",
      "Iteration 391, loss = 0.31326935\n",
      "Iteration 392, loss = 0.31239955\n",
      "Iteration 393, loss = 0.31152913\n",
      "Iteration 394, loss = 0.31065819\n",
      "Iteration 395, loss = 0.30978728\n",
      "Iteration 396, loss = 0.30891627\n",
      "Iteration 397, loss = 0.30804527\n",
      "Iteration 398, loss = 0.30717492\n",
      "Iteration 399, loss = 0.30630575\n",
      "Iteration 400, loss = 0.30543547\n",
      "Iteration 401, loss = 0.30456640\n",
      "Iteration 402, loss = 0.30369783\n",
      "Iteration 403, loss = 0.30282986\n",
      "Iteration 404, loss = 0.30196255\n",
      "Iteration 405, loss = 0.30109598\n",
      "Iteration 406, loss = 0.30023018\n",
      "Iteration 407, loss = 0.29936522\n",
      "Iteration 408, loss = 0.29850119\n",
      "Iteration 409, loss = 0.29763853\n",
      "Iteration 410, loss = 0.29677650\n",
      "Iteration 411, loss = 0.29591577\n",
      "Iteration 412, loss = 0.29505613\n",
      "Iteration 413, loss = 0.29419762\n",
      "Iteration 414, loss = 0.29333948\n",
      "Iteration 415, loss = 0.29248157\n",
      "Iteration 416, loss = 0.29162729\n",
      "Iteration 417, loss = 0.29077386\n",
      "Iteration 418, loss = 0.28992357\n",
      "Iteration 419, loss = 0.28907077\n",
      "Iteration 420, loss = 0.28822714\n",
      "Iteration 421, loss = 0.28738137\n",
      "Iteration 422, loss = 0.28653052\n",
      "Iteration 423, loss = 0.28568837\n",
      "Iteration 424, loss = 0.28484617\n",
      "Iteration 425, loss = 0.28400270\n",
      "Iteration 426, loss = 0.28316613\n",
      "Iteration 427, loss = 0.28232722\n",
      "Iteration 428, loss = 0.28148746\n",
      "Iteration 429, loss = 0.28065264\n",
      "Iteration 430, loss = 0.27981773\n",
      "Iteration 431, loss = 0.27898536\n",
      "Iteration 432, loss = 0.27815330\n",
      "Iteration 433, loss = 0.27732079\n",
      "Iteration 434, loss = 0.27649036\n",
      "Iteration 435, loss = 0.27566101\n",
      "Iteration 436, loss = 0.27483125\n",
      "Iteration 437, loss = 0.27400460\n",
      "Iteration 438, loss = 0.27317623\n",
      "Iteration 439, loss = 0.27235149\n",
      "Iteration 440, loss = 0.27152615\n",
      "Iteration 441, loss = 0.27070192\n",
      "Iteration 442, loss = 0.26987888\n",
      "Iteration 443, loss = 0.26906016\n",
      "Iteration 444, loss = 0.26824170\n",
      "Iteration 445, loss = 0.26742552\n",
      "Iteration 446, loss = 0.26661012\n",
      "Iteration 447, loss = 0.26579772\n",
      "Iteration 448, loss = 0.26498490\n",
      "Iteration 449, loss = 0.26417476\n",
      "Iteration 450, loss = 0.26336832\n",
      "Iteration 451, loss = 0.26255975\n",
      "Iteration 452, loss = 0.26175341\n",
      "Iteration 453, loss = 0.26094957\n",
      "Iteration 454, loss = 0.26014810\n",
      "Iteration 455, loss = 0.25934763\n",
      "Iteration 456, loss = 0.25855008\n",
      "Iteration 457, loss = 0.25775471\n",
      "Iteration 458, loss = 0.25696178\n",
      "Iteration 459, loss = 0.25617019\n",
      "Iteration 460, loss = 0.25538120\n",
      "Iteration 461, loss = 0.25459436\n",
      "Iteration 462, loss = 0.25381005\n",
      "Iteration 463, loss = 0.25302748\n",
      "Iteration 464, loss = 0.25224750\n",
      "Iteration 465, loss = 0.25147025\n",
      "Iteration 466, loss = 0.25069456\n",
      "Iteration 467, loss = 0.24992172\n",
      "Iteration 468, loss = 0.24915129\n",
      "Iteration 469, loss = 0.24838329\n",
      "Iteration 470, loss = 0.24761776\n",
      "Iteration 471, loss = 0.24685478\n",
      "Iteration 472, loss = 0.24609485\n",
      "Iteration 473, loss = 0.24533745\n",
      "Iteration 474, loss = 0.24458267\n",
      "Iteration 475, loss = 0.24383079\n",
      "Iteration 476, loss = 0.24308187\n",
      "Iteration 477, loss = 0.24233778\n",
      "Iteration 478, loss = 0.24159671\n",
      "Iteration 479, loss = 0.24085847\n",
      "Iteration 480, loss = 0.24012308\n",
      "Iteration 481, loss = 0.23939060\n",
      "Iteration 482, loss = 0.23866109\n",
      "Iteration 483, loss = 0.23793462\n",
      "Iteration 484, loss = 0.23721137\n",
      "Iteration 485, loss = 0.23649124\n",
      "Iteration 486, loss = 0.23577428\n",
      "Iteration 487, loss = 0.23506051\n",
      "Iteration 488, loss = 0.23434998\n",
      "Iteration 489, loss = 0.23364269\n",
      "Iteration 490, loss = 0.23293876\n",
      "Iteration 491, loss = 0.23223828\n",
      "Iteration 492, loss = 0.23154136\n",
      "Iteration 493, loss = 0.23084775\n",
      "Iteration 494, loss = 0.23015756\n",
      "Iteration 495, loss = 0.22947058\n",
      "Iteration 496, loss = 0.22878702\n",
      "Iteration 497, loss = 0.22810678\n",
      "Iteration 498, loss = 0.22742989\n",
      "Iteration 499, loss = 0.22675628\n",
      "Iteration 500, loss = 0.22608601\n",
      "Iteration 501, loss = 0.22541904\n",
      "Iteration 502, loss = 0.22475538\n",
      "Iteration 503, loss = 0.22409585\n",
      "Iteration 504, loss = 0.22343848\n",
      "Iteration 505, loss = 0.22278522\n",
      "Iteration 506, loss = 0.22213545\n",
      "Iteration 507, loss = 0.22148891\n",
      "Iteration 508, loss = 0.22084563\n",
      "Iteration 509, loss = 0.22020554\n",
      "Iteration 510, loss = 0.21956904\n",
      "Iteration 511, loss = 0.21893510\n",
      "Iteration 512, loss = 0.21830488\n",
      "Iteration 513, loss = 0.21767773\n",
      "Iteration 514, loss = 0.21705466\n",
      "Iteration 515, loss = 0.21643453\n",
      "Iteration 516, loss = 0.21581761\n",
      "Iteration 517, loss = 0.21520425\n",
      "Iteration 518, loss = 0.21459331\n",
      "Iteration 519, loss = 0.21398673\n",
      "Iteration 520, loss = 0.21338316\n",
      "Iteration 521, loss = 0.21278201\n",
      "Iteration 522, loss = 0.21218381\n",
      "Iteration 523, loss = 0.21158994\n",
      "Iteration 524, loss = 0.21099916\n",
      "Iteration 525, loss = 0.21041051\n",
      "Iteration 526, loss = 0.20982524\n",
      "Iteration 527, loss = 0.20924417\n",
      "Iteration 528, loss = 0.20866527\n",
      "Iteration 529, loss = 0.20808866\n",
      "Iteration 530, loss = 0.20751671\n",
      "Iteration 531, loss = 0.20694746\n",
      "Iteration 532, loss = 0.20638050\n",
      "Iteration 533, loss = 0.20581663\n",
      "Iteration 534, loss = 0.20525656\n",
      "Iteration 535, loss = 0.20469926\n",
      "Iteration 536, loss = 0.20414493\n",
      "Iteration 537, loss = 0.20359342\n",
      "Iteration 538, loss = 0.20304531\n",
      "Iteration 539, loss = 0.20250048\n",
      "Iteration 540, loss = 0.20195751\n",
      "Iteration 541, loss = 0.20141777\n",
      "Iteration 542, loss = 0.20088146\n",
      "Iteration 543, loss = 0.20034944\n",
      "Iteration 544, loss = 0.19981980\n",
      "Iteration 545, loss = 0.19929304\n",
      "Iteration 546, loss = 0.19876895\n",
      "Iteration 547, loss = 0.19824810\n",
      "Iteration 548, loss = 0.19772971\n",
      "Iteration 549, loss = 0.19721425\n",
      "Iteration 550, loss = 0.19670205\n",
      "Iteration 551, loss = 0.19619204\n",
      "Iteration 552, loss = 0.19568509\n",
      "Iteration 553, loss = 0.19518061\n",
      "Iteration 554, loss = 0.19467951\n",
      "Iteration 555, loss = 0.19418082\n",
      "Iteration 556, loss = 0.19368435\n",
      "Iteration 557, loss = 0.19319179\n",
      "Iteration 558, loss = 0.19270147\n",
      "Iteration 559, loss = 0.19221346\n",
      "Iteration 560, loss = 0.19172777\n",
      "Iteration 561, loss = 0.19124533\n",
      "Iteration 562, loss = 0.19076590\n",
      "Iteration 563, loss = 0.19028775\n",
      "Iteration 564, loss = 0.18981349\n",
      "Iteration 565, loss = 0.18934177\n",
      "Iteration 566, loss = 0.18887182\n",
      "Iteration 567, loss = 0.18840508\n",
      "Iteration 568, loss = 0.18794111\n",
      "Iteration 569, loss = 0.18747966\n",
      "Iteration 570, loss = 0.18702091\n",
      "Iteration 571, loss = 0.18656458\n",
      "Iteration 572, loss = 0.18611092\n",
      "Iteration 573, loss = 0.18565969\n",
      "Iteration 574, loss = 0.18521125\n",
      "Iteration 575, loss = 0.18476501\n",
      "Iteration 576, loss = 0.18432100\n",
      "Iteration 577, loss = 0.18387923\n",
      "Iteration 578, loss = 0.18344001\n",
      "Iteration 579, loss = 0.18300366\n",
      "Iteration 580, loss = 0.18256933\n",
      "Iteration 581, loss = 0.18213786\n",
      "Iteration 582, loss = 0.18170854\n",
      "Iteration 583, loss = 0.18128137\n",
      "Iteration 584, loss = 0.18085637\n",
      "Iteration 585, loss = 0.18043357\n",
      "Iteration 586, loss = 0.18001302\n",
      "Iteration 587, loss = 0.17959472\n",
      "Iteration 588, loss = 0.17917924\n",
      "Iteration 589, loss = 0.17876607\n",
      "Iteration 590, loss = 0.17835463\n",
      "Iteration 591, loss = 0.17794532\n",
      "Iteration 592, loss = 0.17753866\n",
      "Iteration 593, loss = 0.17713408\n",
      "Iteration 594, loss = 0.17673158\n",
      "Iteration 595, loss = 0.17633116\n",
      "Iteration 596, loss = 0.17593281\n",
      "Iteration 597, loss = 0.17553655\n",
      "Iteration 598, loss = 0.17514282\n",
      "Iteration 599, loss = 0.17475084\n",
      "Iteration 600, loss = 0.17436075\n",
      "Iteration 601, loss = 0.17397304\n",
      "Iteration 602, loss = 0.17358732\n",
      "Iteration 603, loss = 0.17320371\n",
      "Iteration 604, loss = 0.17282226\n",
      "Iteration 605, loss = 0.17244288\n",
      "Iteration 606, loss = 0.17206533\n",
      "Iteration 607, loss = 0.17168987\n",
      "Iteration 608, loss = 0.17131644\n",
      "Iteration 609, loss = 0.17094499\n",
      "Iteration 610, loss = 0.17057547\n",
      "Iteration 611, loss = 0.17020787\n",
      "Iteration 612, loss = 0.16984219\n",
      "Iteration 613, loss = 0.16947871\n",
      "Iteration 614, loss = 0.16911679\n",
      "Iteration 615, loss = 0.16875686\n",
      "Iteration 616, loss = 0.16839893\n",
      "Iteration 617, loss = 0.16804269\n",
      "Iteration 618, loss = 0.16768833\n",
      "Iteration 619, loss = 0.16733578\n",
      "Iteration 620, loss = 0.16698497\n",
      "Iteration 621, loss = 0.16663603\n",
      "Iteration 622, loss = 0.16628896\n",
      "Iteration 623, loss = 0.16594360\n",
      "Iteration 624, loss = 0.16560007\n",
      "Iteration 625, loss = 0.16525831\n",
      "Iteration 626, loss = 0.16491846\n",
      "Iteration 627, loss = 0.16458017\n",
      "Iteration 628, loss = 0.16424397\n",
      "Iteration 629, loss = 0.16390929\n",
      "Iteration 630, loss = 0.16357611\n",
      "Iteration 631, loss = 0.16324518\n",
      "Iteration 632, loss = 0.16291586\n",
      "Iteration 633, loss = 0.16258788\n",
      "Iteration 634, loss = 0.16226152\n",
      "Iteration 635, loss = 0.16193758\n",
      "Iteration 636, loss = 0.16161501\n",
      "Iteration 637, loss = 0.16129376\n",
      "Iteration 638, loss = 0.16097408\n",
      "Iteration 639, loss = 0.16065658\n",
      "Iteration 640, loss = 0.16034046\n",
      "Iteration 641, loss = 0.16002571\n",
      "Iteration 642, loss = 0.15971310\n",
      "Iteration 643, loss = 0.15940172\n",
      "Iteration 644, loss = 0.15909171\n",
      "Iteration 645, loss = 0.15878301\n",
      "Iteration 646, loss = 0.15847621\n",
      "Iteration 647, loss = 0.15817091\n",
      "Iteration 648, loss = 0.15786667\n",
      "Iteration 649, loss = 0.15756421\n",
      "Iteration 650, loss = 0.15726325\n",
      "Iteration 651, loss = 0.15696373\n",
      "Iteration 652, loss = 0.15666559\n",
      "Iteration 653, loss = 0.15636884\n",
      "Iteration 654, loss = 0.15607356\n",
      "Iteration 655, loss = 0.15577968\n",
      "Iteration 656, loss = 0.15548725\n",
      "Iteration 657, loss = 0.15519636\n",
      "Iteration 658, loss = 0.15490698\n",
      "Iteration 659, loss = 0.15461900\n",
      "Iteration 660, loss = 0.15433243\n",
      "Iteration 661, loss = 0.15404728\n",
      "Iteration 662, loss = 0.15376291\n",
      "Iteration 663, loss = 0.15347902\n",
      "Iteration 664, loss = 0.15319642\n",
      "Iteration 665, loss = 0.15291501\n",
      "Iteration 666, loss = 0.15263487\n",
      "Iteration 667, loss = 0.15235568\n",
      "Iteration 668, loss = 0.15207756\n",
      "Iteration 669, loss = 0.15180069\n",
      "Iteration 670, loss = 0.15152507\n",
      "Iteration 671, loss = 0.15125071\n",
      "Iteration 672, loss = 0.15097760\n",
      "Iteration 673, loss = 0.15070581\n",
      "Iteration 674, loss = 0.15043559\n",
      "Iteration 675, loss = 0.15016671\n",
      "Iteration 676, loss = 0.14989954\n",
      "Iteration 677, loss = 0.14963402\n",
      "Iteration 678, loss = 0.14936984\n",
      "Iteration 679, loss = 0.14910700\n",
      "Iteration 680, loss = 0.14884550\n",
      "Iteration 681, loss = 0.14858522\n",
      "Iteration 682, loss = 0.14832617\n",
      "Iteration 683, loss = 0.14806903\n",
      "Iteration 684, loss = 0.14781311\n",
      "Iteration 685, loss = 0.14755849\n",
      "Iteration 686, loss = 0.14730673\n",
      "Iteration 687, loss = 0.14705636\n",
      "Iteration 688, loss = 0.14680738\n",
      "Iteration 689, loss = 0.14656005\n",
      "Iteration 690, loss = 0.14631412\n",
      "Iteration 691, loss = 0.14606952\n",
      "Iteration 692, loss = 0.14582621\n",
      "Iteration 693, loss = 0.14558416\n",
      "Iteration 694, loss = 0.14534335\n",
      "Iteration 695, loss = 0.14510376\n",
      "Iteration 696, loss = 0.14486542\n",
      "Iteration 697, loss = 0.14462812\n",
      "Iteration 698, loss = 0.14439193\n",
      "Iteration 699, loss = 0.14415692\n",
      "Iteration 700, loss = 0.14392305\n",
      "Iteration 701, loss = 0.14369034\n",
      "Iteration 702, loss = 0.14345863\n",
      "Iteration 703, loss = 0.14322720\n",
      "Iteration 704, loss = 0.14299680\n",
      "Iteration 705, loss = 0.14276745\n",
      "Iteration 706, loss = 0.14253824\n",
      "Iteration 707, loss = 0.14230982\n",
      "Iteration 708, loss = 0.14208230\n",
      "Iteration 709, loss = 0.14185573\n",
      "Iteration 710, loss = 0.14163005\n",
      "Iteration 711, loss = 0.14140544\n",
      "Iteration 712, loss = 0.14118189\n",
      "Iteration 713, loss = 0.14095956\n",
      "Iteration 714, loss = 0.14073812\n",
      "Iteration 715, loss = 0.14051752\n",
      "Iteration 716, loss = 0.14029761\n",
      "Iteration 717, loss = 0.14007859\n",
      "Iteration 718, loss = 0.13986053\n",
      "Iteration 719, loss = 0.13964535\n",
      "Iteration 720, loss = 0.13943148\n",
      "Iteration 721, loss = 0.13921859\n",
      "Iteration 722, loss = 0.13900674\n",
      "Iteration 723, loss = 0.13879592\n",
      "Iteration 724, loss = 0.13858611\n",
      "Iteration 725, loss = 0.13837739\n",
      "Iteration 726, loss = 0.13816997\n",
      "Iteration 727, loss = 0.13796366\n",
      "Iteration 728, loss = 0.13775835\n",
      "Iteration 729, loss = 0.13755403\n",
      "Iteration 730, loss = 0.13735100\n",
      "Iteration 731, loss = 0.13714960\n",
      "Iteration 732, loss = 0.13694901\n",
      "Iteration 733, loss = 0.13674930\n",
      "Iteration 734, loss = 0.13655054\n",
      "Iteration 735, loss = 0.13635246\n",
      "Iteration 736, loss = 0.13615485\n",
      "Iteration 737, loss = 0.13595792\n",
      "Iteration 738, loss = 0.13576191\n",
      "Iteration 739, loss = 0.13556675\n",
      "Iteration 740, loss = 0.13537257\n",
      "Iteration 741, loss = 0.13517929\n",
      "Iteration 742, loss = 0.13498682\n",
      "Iteration 743, loss = 0.13479517\n",
      "Iteration 744, loss = 0.13460469\n",
      "Iteration 745, loss = 0.13441538\n",
      "Iteration 746, loss = 0.13422677\n",
      "Iteration 747, loss = 0.13403908\n",
      "Iteration 748, loss = 0.13385230\n",
      "Iteration 749, loss = 0.13366661\n",
      "Iteration 750, loss = 0.13348217\n",
      "Iteration 751, loss = 0.13329862\n",
      "Iteration 752, loss = 0.13311613\n",
      "Iteration 753, loss = 0.13293458\n",
      "Iteration 754, loss = 0.13275385\n",
      "Iteration 755, loss = 0.13257396\n",
      "Iteration 756, loss = 0.13239495\n",
      "Iteration 757, loss = 0.13221683\n",
      "Iteration 758, loss = 0.13203964\n",
      "Iteration 759, loss = 0.13186323\n",
      "Iteration 760, loss = 0.13168760\n",
      "Iteration 761, loss = 0.13151273\n",
      "Iteration 762, loss = 0.13133857\n",
      "Iteration 763, loss = 0.13116514\n",
      "Iteration 764, loss = 0.13099246\n",
      "Iteration 765, loss = 0.13082059\n",
      "Iteration 766, loss = 0.13064946\n",
      "Iteration 767, loss = 0.13047905\n",
      "Iteration 768, loss = 0.13030939\n",
      "Iteration 769, loss = 0.13014063\n",
      "Iteration 770, loss = 0.12997270\n",
      "Iteration 771, loss = 0.12980536\n",
      "Iteration 772, loss = 0.12963835\n",
      "Iteration 773, loss = 0.12947217\n",
      "Iteration 774, loss = 0.12930658\n",
      "Iteration 775, loss = 0.12914176\n",
      "Iteration 776, loss = 0.12897750\n",
      "Iteration 777, loss = 0.12881406\n",
      "Iteration 778, loss = 0.12865130\n",
      "Iteration 779, loss = 0.12848908\n",
      "Iteration 780, loss = 0.12832767\n",
      "Iteration 781, loss = 0.12816687\n",
      "Iteration 782, loss = 0.12800669\n",
      "Iteration 783, loss = 0.12784736\n",
      "Iteration 784, loss = 0.12768860\n",
      "Iteration 785, loss = 0.12753045\n",
      "Iteration 786, loss = 0.12737321\n",
      "Iteration 787, loss = 0.12721635\n",
      "Iteration 788, loss = 0.12706037\n",
      "Iteration 789, loss = 0.12690509\n",
      "Iteration 790, loss = 0.12675028\n",
      "Iteration 791, loss = 0.12659598\n",
      "Iteration 792, loss = 0.12644234\n",
      "Iteration 793, loss = 0.12628987\n",
      "Iteration 794, loss = 0.12613756\n",
      "Iteration 795, loss = 0.12598564\n",
      "Iteration 796, loss = 0.12583492\n",
      "Iteration 797, loss = 0.12568498\n",
      "Iteration 798, loss = 0.12553571\n",
      "Iteration 799, loss = 0.12538722\n",
      "Iteration 800, loss = 0.12523936\n",
      "Iteration 801, loss = 0.12509198\n",
      "Iteration 802, loss = 0.12494550\n",
      "Iteration 803, loss = 0.12479960\n",
      "Iteration 804, loss = 0.12465408\n",
      "Iteration 805, loss = 0.12450960\n",
      "Iteration 806, loss = 0.12436561\n",
      "Iteration 807, loss = 0.12422202\n",
      "Iteration 808, loss = 0.12407893\n",
      "Iteration 809, loss = 0.12393676\n",
      "Iteration 810, loss = 0.12379509\n",
      "Iteration 811, loss = 0.12365378\n",
      "Iteration 812, loss = 0.12351311\n",
      "Iteration 813, loss = 0.12337293\n",
      "Iteration 814, loss = 0.12323326\n",
      "Iteration 815, loss = 0.12309413\n",
      "Iteration 816, loss = 0.12295564\n",
      "Iteration 817, loss = 0.12281764\n",
      "Iteration 818, loss = 0.12268025\n",
      "Iteration 819, loss = 0.12254343\n",
      "Iteration 820, loss = 0.12240715\n",
      "Iteration 821, loss = 0.12227141\n",
      "Iteration 822, loss = 0.12213631\n",
      "Iteration 823, loss = 0.12200133\n",
      "Iteration 824, loss = 0.12186695\n",
      "Iteration 825, loss = 0.12173303\n",
      "Iteration 826, loss = 0.12159959\n",
      "Iteration 827, loss = 0.12146664\n",
      "Iteration 828, loss = 0.12133438\n",
      "Iteration 829, loss = 0.12120235\n",
      "Iteration 830, loss = 0.12107097\n",
      "Iteration 831, loss = 0.12094008\n",
      "Iteration 832, loss = 0.12080968\n",
      "Iteration 833, loss = 0.12067975\n",
      "Iteration 834, loss = 0.12055033\n",
      "Iteration 835, loss = 0.12042144\n",
      "Iteration 836, loss = 0.12029302\n",
      "Iteration 837, loss = 0.12016510\n",
      "Iteration 838, loss = 0.12003771\n",
      "Iteration 839, loss = 0.11991038\n",
      "Iteration 840, loss = 0.11978332\n",
      "Iteration 841, loss = 0.11965667\n",
      "Iteration 842, loss = 0.11953047\n",
      "Iteration 843, loss = 0.11940472\n",
      "Iteration 844, loss = 0.11927941\n",
      "Iteration 845, loss = 0.11915466\n",
      "Iteration 846, loss = 0.11903038\n",
      "Iteration 847, loss = 0.11890659\n",
      "Iteration 848, loss = 0.11878329\n",
      "Iteration 849, loss = 0.11866045\n",
      "Iteration 850, loss = 0.11853806\n",
      "Iteration 851, loss = 0.11841612\n",
      "Iteration 852, loss = 0.11829461\n",
      "Iteration 853, loss = 0.11817345\n",
      "Iteration 854, loss = 0.11805276\n",
      "Iteration 855, loss = 0.11793251\n",
      "Iteration 856, loss = 0.11781262\n",
      "Iteration 857, loss = 0.11769314\n",
      "Iteration 858, loss = 0.11757410\n",
      "Iteration 859, loss = 0.11745547\n",
      "Iteration 860, loss = 0.11733728\n",
      "Iteration 861, loss = 0.11721963\n",
      "Iteration 862, loss = 0.11710278\n",
      "Iteration 863, loss = 0.11698640\n",
      "Iteration 864, loss = 0.11687048\n",
      "Iteration 865, loss = 0.11675502\n",
      "Iteration 866, loss = 0.11663999\n",
      "Iteration 867, loss = 0.11652542\n",
      "Iteration 868, loss = 0.11641128\n",
      "Iteration 869, loss = 0.11629757\n",
      "Iteration 870, loss = 0.11618429\n",
      "Iteration 871, loss = 0.11607143\n",
      "Iteration 872, loss = 0.11595899\n",
      "Iteration 873, loss = 0.11584699\n",
      "Iteration 874, loss = 0.11573540\n",
      "Iteration 875, loss = 0.11562473\n",
      "Iteration 876, loss = 0.11551484\n",
      "Iteration 877, loss = 0.11540541\n",
      "Iteration 878, loss = 0.11529682\n",
      "Iteration 879, loss = 0.11518876\n",
      "Iteration 880, loss = 0.11508111\n",
      "Iteration 881, loss = 0.11497387\n",
      "Iteration 882, loss = 0.11486703\n",
      "Iteration 883, loss = 0.11476060\n",
      "Iteration 884, loss = 0.11465456\n",
      "Iteration 885, loss = 0.11454892\n",
      "Iteration 886, loss = 0.11444369\n",
      "Iteration 887, loss = 0.11433884\n",
      "Iteration 888, loss = 0.11423438\n",
      "Iteration 889, loss = 0.11413031\n",
      "Iteration 890, loss = 0.11402662\n",
      "Iteration 891, loss = 0.11392332\n",
      "Iteration 892, loss = 0.11382039\n",
      "Iteration 893, loss = 0.11371782\n",
      "Iteration 894, loss = 0.11361564\n",
      "Iteration 895, loss = 0.11351382\n",
      "Iteration 896, loss = 0.11341238\n",
      "Iteration 897, loss = 0.11331130\n",
      "Iteration 898, loss = 0.11321059\n",
      "Iteration 899, loss = 0.11311023\n",
      "Iteration 900, loss = 0.11301025\n",
      "Iteration 901, loss = 0.11291061\n",
      "Iteration 902, loss = 0.11281134\n",
      "Iteration 903, loss = 0.11271241\n",
      "Iteration 904, loss = 0.11261385\n",
      "Iteration 905, loss = 0.11251562\n",
      "Iteration 906, loss = 0.11241775\n",
      "Iteration 907, loss = 0.11232023\n",
      "Iteration 908, loss = 0.11222305\n",
      "Iteration 909, loss = 0.11212622\n",
      "Iteration 910, loss = 0.11202972\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35721204\n",
      "Iteration 2, loss = 1.31986774\n",
      "Iteration 3, loss = 1.26960379\n",
      "Iteration 4, loss = 1.21066236\n",
      "Iteration 5, loss = 1.14741642\n",
      "Iteration 6, loss = 1.08366221\n",
      "Iteration 7, loss = 1.02259707\n",
      "Iteration 8, loss = 0.96694630\n",
      "Iteration 9, loss = 0.91839658\n",
      "Iteration 10, loss = 0.87769878\n",
      "Iteration 11, loss = 0.84479822\n",
      "Iteration 12, loss = 0.81886173\n",
      "Iteration 13, loss = 0.79877292\n",
      "Iteration 14, loss = 0.78331000\n",
      "Iteration 15, loss = 0.77135527\n",
      "Iteration 16, loss = 0.76197620\n",
      "Iteration 17, loss = 0.75416249\n",
      "Iteration 18, loss = 0.74766735\n",
      "Iteration 19, loss = 0.74236604\n",
      "Iteration 20, loss = 0.73795198\n",
      "Iteration 21, loss = 0.73444844\n",
      "Iteration 22, loss = 0.73124983\n",
      "Iteration 23, loss = 0.72775944\n",
      "Iteration 24, loss = 0.72374460\n",
      "Iteration 25, loss = 0.71924251\n",
      "Iteration 26, loss = 0.71419911\n",
      "Iteration 27, loss = 0.70879021\n",
      "Iteration 28, loss = 0.70320371\n",
      "Iteration 29, loss = 0.69756896\n",
      "Iteration 30, loss = 0.69196993\n",
      "Iteration 31, loss = 0.68655128\n",
      "Iteration 32, loss = 0.68141736\n",
      "Iteration 33, loss = 0.67664773\n",
      "Iteration 34, loss = 0.67226124\n",
      "Iteration 35, loss = 0.66822175\n",
      "Iteration 36, loss = 0.66453417\n",
      "Iteration 37, loss = 0.66114076\n",
      "Iteration 38, loss = 0.65801124\n",
      "Iteration 39, loss = 0.65510711\n",
      "Iteration 40, loss = 0.65237702\n",
      "Iteration 41, loss = 0.64977236\n",
      "Iteration 42, loss = 0.64726074\n",
      "Iteration 43, loss = 0.64481444\n",
      "Iteration 44, loss = 0.64241761\n",
      "Iteration 45, loss = 0.64004022\n",
      "Iteration 46, loss = 0.63767659\n",
      "Iteration 47, loss = 0.63531160\n",
      "Iteration 48, loss = 0.63294611\n",
      "Iteration 49, loss = 0.63057566\n",
      "Iteration 50, loss = 0.62820425\n",
      "Iteration 51, loss = 0.62583726\n",
      "Iteration 52, loss = 0.62348095\n",
      "Iteration 53, loss = 0.62114176\n",
      "Iteration 54, loss = 0.61882579\n",
      "Iteration 55, loss = 0.61653838\n",
      "Iteration 56, loss = 0.61428394\n",
      "Iteration 57, loss = 0.61206572\n",
      "Iteration 58, loss = 0.60988589\n",
      "Iteration 59, loss = 0.60774555\n",
      "Iteration 60, loss = 0.60564485\n",
      "Iteration 61, loss = 0.60358315\n",
      "Iteration 62, loss = 0.60155920\n",
      "Iteration 63, loss = 0.59957132\n",
      "Iteration 64, loss = 0.59761753\n",
      "Iteration 65, loss = 0.59569573\n",
      "Iteration 66, loss = 0.59380383\n",
      "Iteration 67, loss = 0.59193981\n",
      "Iteration 68, loss = 0.59010183\n",
      "Iteration 69, loss = 0.58828827\n",
      "Iteration 70, loss = 0.58649773\n",
      "Iteration 71, loss = 0.58472904\n",
      "Iteration 72, loss = 0.58298128\n",
      "Iteration 73, loss = 0.58125368\n",
      "Iteration 74, loss = 0.57954573\n",
      "Iteration 75, loss = 0.57785703\n",
      "Iteration 76, loss = 0.57618728\n",
      "Iteration 77, loss = 0.57453778\n",
      "Iteration 78, loss = 0.57290892\n",
      "Iteration 79, loss = 0.57129991\n",
      "Iteration 80, loss = 0.56971218\n",
      "Iteration 81, loss = 0.56814319\n",
      "Iteration 82, loss = 0.56659276\n",
      "Iteration 83, loss = 0.56506069\n",
      "Iteration 84, loss = 0.56355408\n",
      "Iteration 85, loss = 0.56207132\n",
      "Iteration 86, loss = 0.56060565\n",
      "Iteration 87, loss = 0.55915682\n",
      "Iteration 88, loss = 0.55772528\n",
      "Iteration 89, loss = 0.55631187\n",
      "Iteration 90, loss = 0.55491460\n",
      "Iteration 91, loss = 0.55353313\n",
      "Iteration 92, loss = 0.55216709\n",
      "Iteration 93, loss = 0.55081763\n",
      "Iteration 94, loss = 0.54948375\n",
      "Iteration 95, loss = 0.54816448\n",
      "Iteration 96, loss = 0.54685944\n",
      "Iteration 97, loss = 0.54556833\n",
      "Iteration 98, loss = 0.54429079\n",
      "Iteration 99, loss = 0.54302650\n",
      "Iteration 100, loss = 0.54177525\n",
      "Iteration 101, loss = 0.54053675\n",
      "Iteration 102, loss = 0.53931073\n",
      "Iteration 103, loss = 0.53809693\n",
      "Iteration 104, loss = 0.53689509\n",
      "Iteration 105, loss = 0.53570498\n",
      "Iteration 106, loss = 0.53452636\n",
      "Iteration 107, loss = 0.53335902\n",
      "Iteration 108, loss = 0.53220272\n",
      "Iteration 109, loss = 0.53105726\n",
      "Iteration 110, loss = 0.52992241\n",
      "Iteration 111, loss = 0.52879799\n",
      "Iteration 112, loss = 0.52768376\n",
      "Iteration 113, loss = 0.52657955\n",
      "Iteration 114, loss = 0.52548514\n",
      "Iteration 115, loss = 0.52440035\n",
      "Iteration 116, loss = 0.52332499\n",
      "Iteration 117, loss = 0.52225887\n",
      "Iteration 118, loss = 0.52120181\n",
      "Iteration 119, loss = 0.52015363\n",
      "Iteration 120, loss = 0.51911416\n",
      "Iteration 121, loss = 0.51808323\n",
      "Iteration 122, loss = 0.51706069\n",
      "Iteration 123, loss = 0.51604636\n",
      "Iteration 124, loss = 0.51504010\n",
      "Iteration 125, loss = 0.51404175\n",
      "Iteration 126, loss = 0.51305117\n",
      "Iteration 127, loss = 0.51206821\n",
      "Iteration 128, loss = 0.51109405\n",
      "Iteration 129, loss = 0.51012945\n",
      "Iteration 130, loss = 0.50917232\n",
      "Iteration 131, loss = 0.50822254\n",
      "Iteration 132, loss = 0.50728427\n",
      "Iteration 133, loss = 0.50635521\n",
      "Iteration 134, loss = 0.50543332\n",
      "Iteration 135, loss = 0.50452128\n",
      "Iteration 136, loss = 0.50361710\n",
      "Iteration 137, loss = 0.50271980\n",
      "Iteration 138, loss = 0.50183137\n",
      "Iteration 139, loss = 0.50095049\n",
      "Iteration 140, loss = 0.50007604\n",
      "Iteration 141, loss = 0.49920858\n",
      "Iteration 142, loss = 0.49834729\n",
      "Iteration 143, loss = 0.49749220\n",
      "Iteration 144, loss = 0.49664331\n",
      "Iteration 145, loss = 0.49580063\n",
      "Iteration 146, loss = 0.49496414\n",
      "Iteration 147, loss = 0.49413380\n",
      "Iteration 148, loss = 0.49330957\n",
      "Iteration 149, loss = 0.49249139\n",
      "Iteration 150, loss = 0.49167921\n",
      "Iteration 151, loss = 0.49087295\n",
      "Iteration 152, loss = 0.49007254\n",
      "Iteration 153, loss = 0.48927807\n",
      "Iteration 154, loss = 0.48848936\n",
      "Iteration 155, loss = 0.48770616\n",
      "Iteration 156, loss = 0.48692837\n",
      "Iteration 157, loss = 0.48615589\n",
      "Iteration 158, loss = 0.48538861\n",
      "Iteration 159, loss = 0.48462645\n",
      "Iteration 160, loss = 0.48386931\n",
      "Iteration 161, loss = 0.48311740\n",
      "Iteration 162, loss = 0.48237044\n",
      "Iteration 163, loss = 0.48162834\n",
      "Iteration 164, loss = 0.48089104\n",
      "Iteration 165, loss = 0.48015845\n",
      "Iteration 166, loss = 0.47943050\n",
      "Iteration 167, loss = 0.47870712\n",
      "Iteration 168, loss = 0.47798824\n",
      "Iteration 169, loss = 0.47727377\n",
      "Iteration 170, loss = 0.47656365\n",
      "Iteration 171, loss = 0.47585781\n",
      "Iteration 172, loss = 0.47515618\n",
      "Iteration 173, loss = 0.47445868\n",
      "Iteration 174, loss = 0.47376530\n",
      "Iteration 175, loss = 0.47307604\n",
      "Iteration 176, loss = 0.47239066\n",
      "Iteration 177, loss = 0.47170928\n",
      "Iteration 178, loss = 0.47103166\n",
      "Iteration 179, loss = 0.47035789\n",
      "Iteration 180, loss = 0.46968774\n",
      "Iteration 181, loss = 0.46902139\n",
      "Iteration 182, loss = 0.46835852\n",
      "Iteration 183, loss = 0.46769928\n",
      "Iteration 184, loss = 0.46704344\n",
      "Iteration 185, loss = 0.46639116\n",
      "Iteration 186, loss = 0.46574215\n",
      "Iteration 187, loss = 0.46509650\n",
      "Iteration 188, loss = 0.46445421\n",
      "Iteration 189, loss = 0.46381506\n",
      "Iteration 190, loss = 0.46317911\n",
      "Iteration 191, loss = 0.46254628\n",
      "Iteration 192, loss = 0.46191657\n",
      "Iteration 193, loss = 0.46128989\n",
      "Iteration 194, loss = 0.46066618\n",
      "Iteration 195, loss = 0.46004542\n",
      "Iteration 196, loss = 0.45942755\n",
      "Iteration 197, loss = 0.45881254\n",
      "Iteration 198, loss = 0.45820035\n",
      "Iteration 199, loss = 0.45759093\n",
      "Iteration 200, loss = 0.45698424\n",
      "Iteration 201, loss = 0.45638024\n",
      "Iteration 202, loss = 0.45577890\n",
      "Iteration 203, loss = 0.45518016\n",
      "Iteration 204, loss = 0.45458400\n",
      "Iteration 205, loss = 0.45399037\n",
      "Iteration 206, loss = 0.45339924\n",
      "Iteration 207, loss = 0.45281057\n",
      "Iteration 208, loss = 0.45222432\n",
      "Iteration 209, loss = 0.45164046\n",
      "Iteration 210, loss = 0.45105895\n",
      "Iteration 211, loss = 0.45047975\n",
      "Iteration 212, loss = 0.44990284\n",
      "Iteration 213, loss = 0.44932817\n",
      "Iteration 214, loss = 0.44875572\n",
      "Iteration 215, loss = 0.44818545\n",
      "Iteration 216, loss = 0.44761733\n",
      "Iteration 217, loss = 0.44705165\n",
      "Iteration 218, loss = 0.44648954\n",
      "Iteration 219, loss = 0.44592959\n",
      "Iteration 220, loss = 0.44537188\n",
      "Iteration 221, loss = 0.44481628\n",
      "Iteration 222, loss = 0.44426275\n",
      "Iteration 223, loss = 0.44371134\n",
      "Iteration 224, loss = 0.44316194\n",
      "Iteration 225, loss = 0.44261452\n",
      "Iteration 226, loss = 0.44206963\n",
      "Iteration 227, loss = 0.44152750\n",
      "Iteration 228, loss = 0.44098742\n",
      "Iteration 229, loss = 0.44044924\n",
      "Iteration 230, loss = 0.43991313\n",
      "Iteration 231, loss = 0.43937879\n",
      "Iteration 232, loss = 0.43884655\n",
      "Iteration 233, loss = 0.43831653\n",
      "Iteration 234, loss = 0.43778890\n",
      "Iteration 235, loss = 0.43726317\n",
      "Iteration 236, loss = 0.43673931\n",
      "Iteration 237, loss = 0.43621718\n",
      "Iteration 238, loss = 0.43569700\n",
      "Iteration 239, loss = 0.43517845\n",
      "Iteration 240, loss = 0.43466182\n",
      "Iteration 241, loss = 0.43414684\n",
      "Iteration 242, loss = 0.43363362\n",
      "Iteration 243, loss = 0.43312215\n",
      "Iteration 244, loss = 0.43261233\n",
      "Iteration 245, loss = 0.43210428\n",
      "Iteration 246, loss = 0.43159777\n",
      "Iteration 247, loss = 0.43109303\n",
      "Iteration 248, loss = 0.43058974\n",
      "Iteration 249, loss = 0.43008822\n",
      "Iteration 250, loss = 0.42958815\n",
      "Iteration 251, loss = 0.42908959\n",
      "Iteration 252, loss = 0.42859275\n",
      "Iteration 253, loss = 0.42809722\n",
      "Iteration 254, loss = 0.42760330\n",
      "Iteration 255, loss = 0.42711079\n",
      "Iteration 256, loss = 0.42661971\n",
      "Iteration 257, loss = 0.42613018\n",
      "Iteration 258, loss = 0.42564195\n",
      "Iteration 259, loss = 0.42515515\n",
      "Iteration 260, loss = 0.42467051\n",
      "Iteration 261, loss = 0.42418771\n",
      "Iteration 262, loss = 0.42370637\n",
      "Iteration 263, loss = 0.42322627\n",
      "Iteration 264, loss = 0.42274755\n",
      "Iteration 265, loss = 0.42227029\n",
      "Iteration 266, loss = 0.42179429\n",
      "Iteration 267, loss = 0.42131960\n",
      "Iteration 268, loss = 0.42084625\n",
      "Iteration 269, loss = 0.42037430\n",
      "Iteration 270, loss = 0.41990360\n",
      "Iteration 271, loss = 0.41943420\n",
      "Iteration 272, loss = 0.41896618\n",
      "Iteration 273, loss = 0.41849937\n",
      "Iteration 274, loss = 0.41803382\n",
      "Iteration 275, loss = 0.41756960\n",
      "Iteration 276, loss = 0.41710655\n",
      "Iteration 277, loss = 0.41664475\n",
      "Iteration 278, loss = 0.41618419\n",
      "Iteration 279, loss = 0.41572485\n",
      "Iteration 280, loss = 0.41526667\n",
      "Iteration 281, loss = 0.41480966\n",
      "Iteration 282, loss = 0.41435379\n",
      "Iteration 283, loss = 0.41389921\n",
      "Iteration 284, loss = 0.41344558\n",
      "Iteration 285, loss = 0.41299315\n",
      "Iteration 286, loss = 0.41254184\n",
      "Iteration 287, loss = 0.41209167\n",
      "Iteration 288, loss = 0.41164254\n",
      "Iteration 289, loss = 0.41119448\n",
      "Iteration 290, loss = 0.41074747\n",
      "Iteration 291, loss = 0.41030150\n",
      "Iteration 292, loss = 0.40985656\n",
      "Iteration 293, loss = 0.40941280\n",
      "Iteration 294, loss = 0.40896981\n",
      "Iteration 295, loss = 0.40852793\n",
      "Iteration 296, loss = 0.40808705\n",
      "Iteration 297, loss = 0.40764713\n",
      "Iteration 298, loss = 0.40720825\n",
      "Iteration 299, loss = 0.40677025\n",
      "Iteration 300, loss = 0.40633323\n",
      "Iteration 301, loss = 0.40589714\n",
      "Iteration 302, loss = 0.40546198\n",
      "Iteration 303, loss = 0.40502773\n",
      "Iteration 304, loss = 0.40459439\n",
      "Iteration 305, loss = 0.40416195\n",
      "Iteration 306, loss = 0.40373041\n",
      "Iteration 307, loss = 0.40329974\n",
      "Iteration 308, loss = 0.40286995\n",
      "Iteration 309, loss = 0.40244102\n",
      "Iteration 310, loss = 0.40201295\n",
      "Iteration 311, loss = 0.40158573\n",
      "Iteration 312, loss = 0.40115935\n",
      "Iteration 313, loss = 0.40073380\n",
      "Iteration 314, loss = 0.40030907\n",
      "Iteration 315, loss = 0.39988515\n",
      "Iteration 316, loss = 0.39946204\n",
      "Iteration 317, loss = 0.39903973\n",
      "Iteration 318, loss = 0.39861821\n",
      "Iteration 319, loss = 0.39819748\n",
      "Iteration 320, loss = 0.39777751\n",
      "Iteration 321, loss = 0.39735832\n",
      "Iteration 322, loss = 0.39693988\n",
      "Iteration 323, loss = 0.39652220\n",
      "Iteration 324, loss = 0.39610526\n",
      "Iteration 325, loss = 0.39568906\n",
      "Iteration 326, loss = 0.39527359\n",
      "Iteration 327, loss = 0.39485885\n",
      "Iteration 328, loss = 0.39444483\n",
      "Iteration 329, loss = 0.39403151\n",
      "Iteration 330, loss = 0.39361890\n",
      "Iteration 331, loss = 0.39320699\n",
      "Iteration 332, loss = 0.39279577\n",
      "Iteration 333, loss = 0.39238524\n",
      "Iteration 334, loss = 0.39197557\n",
      "Iteration 335, loss = 0.39156797\n",
      "Iteration 336, loss = 0.39116111\n",
      "Iteration 337, loss = 0.39075493\n",
      "Iteration 338, loss = 0.39034942\n",
      "Iteration 339, loss = 0.38994459\n",
      "Iteration 340, loss = 0.38954045\n",
      "Iteration 341, loss = 0.38913716\n",
      "Iteration 342, loss = 0.38873458\n",
      "Iteration 343, loss = 0.38833262\n",
      "Iteration 344, loss = 0.38793129\n",
      "Iteration 345, loss = 0.38753061\n",
      "Iteration 346, loss = 0.38713059\n",
      "Iteration 347, loss = 0.38673123\n",
      "Iteration 348, loss = 0.38633254\n",
      "Iteration 349, loss = 0.38593453\n",
      "Iteration 350, loss = 0.38553729\n",
      "Iteration 351, loss = 0.38514081\n",
      "Iteration 352, loss = 0.38474502\n",
      "Iteration 353, loss = 0.38434988\n",
      "Iteration 354, loss = 0.38395541\n",
      "Iteration 355, loss = 0.38356159\n",
      "Iteration 356, loss = 0.38316841\n",
      "Iteration 357, loss = 0.38277587\n",
      "Iteration 358, loss = 0.38238397\n",
      "Iteration 359, loss = 0.38199271\n",
      "Iteration 360, loss = 0.38160214\n",
      "Iteration 361, loss = 0.38121220\n",
      "Iteration 362, loss = 0.38082289\n",
      "Iteration 363, loss = 0.38043419\n",
      "Iteration 364, loss = 0.38004614\n",
      "Iteration 365, loss = 0.37965870\n",
      "Iteration 366, loss = 0.37927275\n",
      "Iteration 367, loss = 0.37888885\n",
      "Iteration 368, loss = 0.37850565\n",
      "Iteration 369, loss = 0.37812311\n",
      "Iteration 370, loss = 0.37774124\n",
      "Iteration 371, loss = 0.37736002\n",
      "Iteration 372, loss = 0.37697945\n",
      "Iteration 373, loss = 0.37659953\n",
      "Iteration 374, loss = 0.37622024\n",
      "Iteration 375, loss = 0.37584157\n",
      "Iteration 376, loss = 0.37546352\n",
      "Iteration 377, loss = 0.37508608\n",
      "Iteration 378, loss = 0.37470924\n",
      "Iteration 379, loss = 0.37433299\n",
      "Iteration 380, loss = 0.37395732\n",
      "Iteration 381, loss = 0.37358223\n",
      "Iteration 382, loss = 0.37320771\n",
      "Iteration 383, loss = 0.37283439\n",
      "Iteration 384, loss = 0.37246254\n",
      "Iteration 385, loss = 0.37209122\n",
      "Iteration 386, loss = 0.37172045\n",
      "Iteration 387, loss = 0.37135020\n",
      "Iteration 388, loss = 0.37098049\n",
      "Iteration 389, loss = 0.37061131\n",
      "Iteration 390, loss = 0.37024265\n",
      "Iteration 391, loss = 0.36987450\n",
      "Iteration 392, loss = 0.36950687\n",
      "Iteration 393, loss = 0.36913975\n",
      "Iteration 394, loss = 0.36877313\n",
      "Iteration 395, loss = 0.36840701\n",
      "Iteration 396, loss = 0.36804138\n",
      "Iteration 397, loss = 0.36767624\n",
      "Iteration 398, loss = 0.36731159\n",
      "Iteration 399, loss = 0.36694742\n",
      "Iteration 400, loss = 0.36658373\n",
      "Iteration 401, loss = 0.36622051\n",
      "Iteration 402, loss = 0.36585776\n",
      "Iteration 403, loss = 0.36549548\n",
      "Iteration 404, loss = 0.36513367\n",
      "Iteration 405, loss = 0.36477232\n",
      "Iteration 406, loss = 0.36441144\n",
      "Iteration 407, loss = 0.36405110\n",
      "Iteration 408, loss = 0.36369141\n",
      "Iteration 409, loss = 0.36333217\n",
      "Iteration 410, loss = 0.36297340\n",
      "Iteration 411, loss = 0.36261509\n",
      "Iteration 412, loss = 0.36225723\n",
      "Iteration 413, loss = 0.36189984\n",
      "Iteration 414, loss = 0.36154290\n",
      "Iteration 415, loss = 0.36118643\n",
      "Iteration 416, loss = 0.36083041\n",
      "Iteration 417, loss = 0.36047485\n",
      "Iteration 418, loss = 0.36011975\n",
      "Iteration 419, loss = 0.35976510\n",
      "Iteration 420, loss = 0.35941090\n",
      "Iteration 421, loss = 0.35905715\n",
      "Iteration 422, loss = 0.35870386\n",
      "Iteration 423, loss = 0.35835102\n",
      "Iteration 424, loss = 0.35799862\n",
      "Iteration 425, loss = 0.35764666\n",
      "Iteration 426, loss = 0.35729515\n",
      "Iteration 427, loss = 0.35694409\n",
      "Iteration 428, loss = 0.35659346\n",
      "Iteration 429, loss = 0.35624327\n",
      "Iteration 430, loss = 0.35589352\n",
      "Iteration 431, loss = 0.35554421\n",
      "Iteration 432, loss = 0.35519533\n",
      "Iteration 433, loss = 0.35484688\n",
      "Iteration 434, loss = 0.35449886\n",
      "Iteration 435, loss = 0.35415126\n",
      "Iteration 436, loss = 0.35380410\n",
      "Iteration 437, loss = 0.35345736\n",
      "Iteration 438, loss = 0.35311105\n",
      "Iteration 439, loss = 0.35276516\n",
      "Iteration 440, loss = 0.35241969\n",
      "Iteration 441, loss = 0.35207464\n",
      "Iteration 442, loss = 0.35173000\n",
      "Iteration 443, loss = 0.35138579\n",
      "Iteration 444, loss = 0.35104199\n",
      "Iteration 445, loss = 0.35069861\n",
      "Iteration 446, loss = 0.35035564\n",
      "Iteration 447, loss = 0.35001308\n",
      "Iteration 448, loss = 0.34967093\n",
      "Iteration 449, loss = 0.34932920\n",
      "Iteration 450, loss = 0.34898787\n",
      "Iteration 451, loss = 0.34864695\n",
      "Iteration 452, loss = 0.34830644\n",
      "Iteration 453, loss = 0.34796633\n",
      "Iteration 454, loss = 0.34762663\n",
      "Iteration 455, loss = 0.34728733\n",
      "Iteration 456, loss = 0.34694843\n",
      "Iteration 457, loss = 0.34660994\n",
      "Iteration 458, loss = 0.34627185\n",
      "Iteration 459, loss = 0.34593415\n",
      "Iteration 460, loss = 0.34559686\n",
      "Iteration 461, loss = 0.34525996\n",
      "Iteration 462, loss = 0.34492347\n",
      "Iteration 463, loss = 0.34458736\n",
      "Iteration 464, loss = 0.34425166\n",
      "Iteration 465, loss = 0.34391635\n",
      "Iteration 466, loss = 0.34358143\n",
      "Iteration 467, loss = 0.34324691\n",
      "Iteration 468, loss = 0.34291278\n",
      "Iteration 469, loss = 0.34257905\n",
      "Iteration 470, loss = 0.34224570\n",
      "Iteration 471, loss = 0.34191275\n",
      "Iteration 472, loss = 0.34158019\n",
      "Iteration 473, loss = 0.34124802\n",
      "Iteration 474, loss = 0.34091623\n",
      "Iteration 475, loss = 0.34058484\n",
      "Iteration 476, loss = 0.34025383\n",
      "Iteration 477, loss = 0.33992321\n",
      "Iteration 478, loss = 0.33959298\n",
      "Iteration 479, loss = 0.33926313\n",
      "Iteration 480, loss = 0.33893367\n",
      "Iteration 481, loss = 0.33860459\n",
      "Iteration 482, loss = 0.33827590\n",
      "Iteration 483, loss = 0.33794760\n",
      "Iteration 484, loss = 0.33761968\n",
      "Iteration 485, loss = 0.33729214\n",
      "Iteration 486, loss = 0.33696499\n",
      "Iteration 487, loss = 0.33663821\n",
      "Iteration 488, loss = 0.33631182\n",
      "Iteration 489, loss = 0.33598582\n",
      "Iteration 490, loss = 0.33566019\n",
      "Iteration 491, loss = 0.33533494\n",
      "Iteration 492, loss = 0.33501008\n",
      "Iteration 493, loss = 0.33468560\n",
      "Iteration 494, loss = 0.33436149\n",
      "Iteration 495, loss = 0.33403777\n",
      "Iteration 496, loss = 0.33371443\n",
      "Iteration 497, loss = 0.33339146\n",
      "Iteration 498, loss = 0.33306888\n",
      "Iteration 499, loss = 0.33274667\n",
      "Iteration 500, loss = 0.33242484\n",
      "Iteration 501, loss = 0.33210339\n",
      "Iteration 502, loss = 0.33178232\n",
      "Iteration 503, loss = 0.33146162\n",
      "Iteration 504, loss = 0.33114131\n",
      "Iteration 505, loss = 0.33082137\n",
      "Iteration 506, loss = 0.33050180\n",
      "Iteration 507, loss = 0.33018262\n",
      "Iteration 508, loss = 0.32986381\n",
      "Iteration 509, loss = 0.32954538\n",
      "Iteration 510, loss = 0.32922732\n",
      "Iteration 511, loss = 0.32890964\n",
      "Iteration 512, loss = 0.32859233\n",
      "Iteration 513, loss = 0.32827541\n",
      "Iteration 514, loss = 0.32795885\n",
      "Iteration 515, loss = 0.32764267\n",
      "Iteration 516, loss = 0.32732687\n",
      "Iteration 517, loss = 0.32701144\n",
      "Iteration 518, loss = 0.32669639\n",
      "Iteration 519, loss = 0.32638204\n",
      "Iteration 520, loss = 0.32606812\n",
      "Iteration 521, loss = 0.32575458\n",
      "Iteration 522, loss = 0.32544141\n",
      "Iteration 523, loss = 0.32512862\n",
      "Iteration 524, loss = 0.32481621\n",
      "Iteration 525, loss = 0.32450419\n",
      "Iteration 526, loss = 0.32419254\n",
      "Iteration 527, loss = 0.32388127\n",
      "Iteration 528, loss = 0.32357038\n",
      "Iteration 529, loss = 0.32325987\n",
      "Iteration 530, loss = 0.32294975\n",
      "Iteration 531, loss = 0.32264002\n",
      "Iteration 532, loss = 0.32233070\n",
      "Iteration 533, loss = 0.32202184\n",
      "Iteration 534, loss = 0.32171329\n",
      "Iteration 535, loss = 0.32140513\n",
      "Iteration 536, loss = 0.32109738\n",
      "Iteration 537, loss = 0.32079000\n",
      "Iteration 538, loss = 0.32048305\n",
      "Iteration 539, loss = 0.32017644\n",
      "Iteration 540, loss = 0.31987021\n",
      "Iteration 541, loss = 0.31956435\n",
      "Iteration 542, loss = 0.31925892\n",
      "Iteration 543, loss = 0.31895383\n",
      "Iteration 544, loss = 0.31864911\n",
      "Iteration 545, loss = 0.31834483\n",
      "Iteration 546, loss = 0.31804086\n",
      "Iteration 547, loss = 0.31773729\n",
      "Iteration 548, loss = 0.31743414\n",
      "Iteration 549, loss = 0.31713132\n",
      "Iteration 550, loss = 0.31682890\n",
      "Iteration 551, loss = 0.31652684\n",
      "Iteration 552, loss = 0.31622524\n",
      "Iteration 553, loss = 0.31592391\n",
      "Iteration 554, loss = 0.31562300\n",
      "Iteration 555, loss = 0.31532249\n",
      "Iteration 556, loss = 0.31502235\n",
      "Iteration 557, loss = 0.31472258\n",
      "Iteration 558, loss = 0.31442318\n",
      "Iteration 559, loss = 0.31412415\n",
      "Iteration 560, loss = 0.31382558\n",
      "Iteration 561, loss = 0.31352727\n",
      "Iteration 562, loss = 0.31322938\n",
      "Iteration 563, loss = 0.31293192\n",
      "Iteration 564, loss = 0.31263478\n",
      "Iteration 565, loss = 0.31233803\n",
      "Iteration 566, loss = 0.31204165\n",
      "Iteration 567, loss = 0.31174567\n",
      "Iteration 568, loss = 0.31145006\n",
      "Iteration 569, loss = 0.31115481\n",
      "Iteration 570, loss = 0.31085994\n",
      "Iteration 571, loss = 0.31056547\n",
      "Iteration 572, loss = 0.31027136\n",
      "Iteration 573, loss = 0.30997762\n",
      "Iteration 574, loss = 0.30968425\n",
      "Iteration 575, loss = 0.30939132\n",
      "Iteration 576, loss = 0.30909868\n",
      "Iteration 577, loss = 0.30880644\n",
      "Iteration 578, loss = 0.30851461\n",
      "Iteration 579, loss = 0.30822314\n",
      "Iteration 580, loss = 0.30793203\n",
      "Iteration 581, loss = 0.30764130\n",
      "Iteration 582, loss = 0.30735096\n",
      "Iteration 583, loss = 0.30706099\n",
      "Iteration 584, loss = 0.30677139\n",
      "Iteration 585, loss = 0.30648215\n",
      "Iteration 586, loss = 0.30619335\n",
      "Iteration 587, loss = 0.30590485\n",
      "Iteration 588, loss = 0.30561674\n",
      "Iteration 589, loss = 0.30532905\n",
      "Iteration 590, loss = 0.30504170\n",
      "Iteration 591, loss = 0.30475473\n",
      "Iteration 592, loss = 0.30446812\n",
      "Iteration 593, loss = 0.30418192\n",
      "Iteration 594, loss = 0.30389607\n",
      "Iteration 595, loss = 0.30361060\n",
      "Iteration 596, loss = 0.30332549\n",
      "Iteration 597, loss = 0.30304081\n",
      "Iteration 598, loss = 0.30275646\n",
      "Iteration 599, loss = 0.30247248\n",
      "Iteration 600, loss = 0.30218887\n",
      "Iteration 601, loss = 0.30190568\n",
      "Iteration 602, loss = 0.30162283\n",
      "Iteration 603, loss = 0.30134035\n",
      "Iteration 604, loss = 0.30105828\n",
      "Iteration 605, loss = 0.30077654\n",
      "Iteration 606, loss = 0.30049519\n",
      "Iteration 607, loss = 0.30021424\n",
      "Iteration 608, loss = 0.29993364\n",
      "Iteration 609, loss = 0.29965341\n",
      "Iteration 610, loss = 0.29937355\n",
      "Iteration 611, loss = 0.29909412\n",
      "Iteration 612, loss = 0.29881499\n",
      "Iteration 613, loss = 0.29853625\n",
      "Iteration 614, loss = 0.29825793\n",
      "Iteration 615, loss = 0.29797995\n",
      "Iteration 616, loss = 0.29770234\n",
      "Iteration 617, loss = 0.29742510\n",
      "Iteration 618, loss = 0.29714829\n",
      "Iteration 619, loss = 0.29687190\n",
      "Iteration 620, loss = 0.29659595\n",
      "Iteration 621, loss = 0.29632041\n",
      "Iteration 622, loss = 0.29604519\n",
      "Iteration 623, loss = 0.29577035\n",
      "Iteration 624, loss = 0.29549586\n",
      "Iteration 625, loss = 0.29522181\n",
      "Iteration 626, loss = 0.29494803\n",
      "Iteration 627, loss = 0.29467466\n",
      "Iteration 628, loss = 0.29440171\n",
      "Iteration 629, loss = 0.29412908\n",
      "Iteration 630, loss = 0.29385683\n",
      "Iteration 631, loss = 0.29358496\n",
      "Iteration 632, loss = 0.29331350\n",
      "Iteration 633, loss = 0.29304238\n",
      "Iteration 634, loss = 0.29277163\n",
      "Iteration 635, loss = 0.29250125\n",
      "Iteration 636, loss = 0.29223133\n",
      "Iteration 637, loss = 0.29196169\n",
      "Iteration 638, loss = 0.29169251\n",
      "Iteration 639, loss = 0.29142369\n",
      "Iteration 640, loss = 0.29115524\n",
      "Iteration 641, loss = 0.29088716\n",
      "Iteration 642, loss = 0.29061944\n",
      "Iteration 643, loss = 0.29035216\n",
      "Iteration 644, loss = 0.29008519\n",
      "Iteration 645, loss = 0.28981860\n",
      "Iteration 646, loss = 0.28955245\n",
      "Iteration 647, loss = 0.28928663\n",
      "Iteration 648, loss = 0.28902119\n",
      "Iteration 649, loss = 0.28875610\n",
      "Iteration 650, loss = 0.28849140\n",
      "Iteration 651, loss = 0.28822709\n",
      "Iteration 652, loss = 0.28796313\n",
      "Iteration 653, loss = 0.28769954\n",
      "Iteration 654, loss = 0.28743639\n",
      "Iteration 655, loss = 0.28717356\n",
      "Iteration 656, loss = 0.28691110\n",
      "Iteration 657, loss = 0.28664901\n",
      "Iteration 658, loss = 0.28638733\n",
      "Iteration 659, loss = 0.28612599\n",
      "Iteration 660, loss = 0.28586502\n",
      "Iteration 661, loss = 0.28560447\n",
      "Iteration 662, loss = 0.28534424\n",
      "Iteration 663, loss = 0.28508439\n",
      "Iteration 664, loss = 0.28482493\n",
      "Iteration 665, loss = 0.28456586\n",
      "Iteration 666, loss = 0.28430713\n",
      "Iteration 667, loss = 0.28404877\n",
      "Iteration 668, loss = 0.28379079\n",
      "Iteration 669, loss = 0.28353321\n",
      "Iteration 670, loss = 0.28327598\n",
      "Iteration 671, loss = 0.28301910\n",
      "Iteration 672, loss = 0.28276263\n",
      "Iteration 673, loss = 0.28250651\n",
      "Iteration 674, loss = 0.28225076\n",
      "Iteration 675, loss = 0.28199537\n",
      "Iteration 676, loss = 0.28174040\n",
      "Iteration 677, loss = 0.28148576\n",
      "Iteration 678, loss = 0.28123149\n",
      "Iteration 679, loss = 0.28097758\n",
      "Iteration 680, loss = 0.28072411\n",
      "Iteration 681, loss = 0.28047090\n",
      "Iteration 682, loss = 0.28021814\n",
      "Iteration 683, loss = 0.27996574\n",
      "Iteration 684, loss = 0.27971369\n",
      "Iteration 685, loss = 0.27946202\n",
      "Iteration 686, loss = 0.27921070\n",
      "Iteration 687, loss = 0.27895979\n",
      "Iteration 688, loss = 0.27870920\n",
      "Iteration 689, loss = 0.27845900\n",
      "Iteration 690, loss = 0.27820919\n",
      "Iteration 691, loss = 0.27795972\n",
      "Iteration 692, loss = 0.27771063\n",
      "Iteration 693, loss = 0.27746198\n",
      "Iteration 694, loss = 0.27721360\n",
      "Iteration 695, loss = 0.27696565\n",
      "Iteration 696, loss = 0.27671806\n",
      "Iteration 697, loss = 0.27647083\n",
      "Iteration 698, loss = 0.27622399\n",
      "Iteration 699, loss = 0.27597749\n",
      "Iteration 700, loss = 0.27573135\n",
      "Iteration 701, loss = 0.27548568\n",
      "Iteration 702, loss = 0.27524021\n",
      "Iteration 703, loss = 0.27499523\n",
      "Iteration 704, loss = 0.27475060\n",
      "Iteration 705, loss = 0.27450632\n",
      "Iteration 706, loss = 0.27426239\n",
      "Iteration 707, loss = 0.27401882\n",
      "Iteration 708, loss = 0.27377562\n",
      "Iteration 709, loss = 0.27353286\n",
      "Iteration 710, loss = 0.27329036\n",
      "Iteration 711, loss = 0.27304826\n",
      "Iteration 712, loss = 0.27280659\n",
      "Iteration 713, loss = 0.27256517\n",
      "Iteration 714, loss = 0.27232420\n",
      "Iteration 715, loss = 0.27208361\n",
      "Iteration 716, loss = 0.27184335\n",
      "Iteration 717, loss = 0.27160343\n",
      "Iteration 718, loss = 0.27136387\n",
      "Iteration 719, loss = 0.27112467\n",
      "Iteration 720, loss = 0.27088591\n",
      "Iteration 721, loss = 0.27064743\n",
      "Iteration 722, loss = 0.27040933\n",
      "Iteration 723, loss = 0.27017166\n",
      "Iteration 724, loss = 0.26993425\n",
      "Iteration 725, loss = 0.26969726\n",
      "Iteration 726, loss = 0.26946066\n",
      "Iteration 727, loss = 0.26922438\n",
      "Iteration 728, loss = 0.26898846\n",
      "Iteration 729, loss = 0.26875288\n",
      "Iteration 730, loss = 0.26851770\n",
      "Iteration 731, loss = 0.26828287\n",
      "Iteration 732, loss = 0.26804838\n",
      "Iteration 733, loss = 0.26781429\n",
      "Iteration 734, loss = 0.26758052\n",
      "Iteration 735, loss = 0.26734712\n",
      "Iteration 736, loss = 0.26711410\n",
      "Iteration 737, loss = 0.26688144\n",
      "Iteration 738, loss = 0.26664912\n",
      "Iteration 739, loss = 0.26641714\n",
      "Iteration 740, loss = 0.26618557\n",
      "Iteration 741, loss = 0.26595431\n",
      "Iteration 742, loss = 0.26572342\n",
      "Iteration 743, loss = 0.26549292\n",
      "Iteration 744, loss = 0.26526274\n",
      "Iteration 745, loss = 0.26503291\n",
      "Iteration 746, loss = 0.26480346\n",
      "Iteration 747, loss = 0.26457437\n",
      "Iteration 748, loss = 0.26434561\n",
      "Iteration 749, loss = 0.26411727\n",
      "Iteration 750, loss = 0.26388922\n",
      "Iteration 751, loss = 0.26366152\n",
      "Iteration 752, loss = 0.26343421\n",
      "Iteration 753, loss = 0.26320724\n",
      "Iteration 754, loss = 0.26298063\n",
      "Iteration 755, loss = 0.26275438\n",
      "Iteration 756, loss = 0.26252847\n",
      "Iteration 757, loss = 0.26230290\n",
      "Iteration 758, loss = 0.26207772\n",
      "Iteration 759, loss = 0.26185290\n",
      "Iteration 760, loss = 0.26162839\n",
      "Iteration 761, loss = 0.26140425\n",
      "Iteration 762, loss = 0.26118044\n",
      "Iteration 763, loss = 0.26095705\n",
      "Iteration 764, loss = 0.26073391\n",
      "Iteration 765, loss = 0.26051125\n",
      "Iteration 766, loss = 0.26028883\n",
      "Iteration 767, loss = 0.26006679\n",
      "Iteration 768, loss = 0.25984514\n",
      "Iteration 769, loss = 0.25962378\n",
      "Iteration 770, loss = 0.25940280\n",
      "Iteration 771, loss = 0.25918220\n",
      "Iteration 772, loss = 0.25896191\n",
      "Iteration 773, loss = 0.25874197\n",
      "Iteration 774, loss = 0.25852240\n",
      "Iteration 775, loss = 0.25830314\n",
      "Iteration 776, loss = 0.25808429\n",
      "Iteration 777, loss = 0.25786570\n",
      "Iteration 778, loss = 0.25764755\n",
      "Iteration 779, loss = 0.25742970\n",
      "Iteration 780, loss = 0.25721224\n",
      "Iteration 781, loss = 0.25699511\n",
      "Iteration 782, loss = 0.25677829\n",
      "Iteration 783, loss = 0.25656185\n",
      "Iteration 784, loss = 0.25634575\n",
      "Iteration 785, loss = 0.25613003\n",
      "Iteration 786, loss = 0.25591463\n",
      "Iteration 787, loss = 0.25569954\n",
      "Iteration 788, loss = 0.25548487\n",
      "Iteration 789, loss = 0.25527048\n",
      "Iteration 790, loss = 0.25505647\n",
      "Iteration 791, loss = 0.25484277\n",
      "Iteration 792, loss = 0.25462944\n",
      "Iteration 793, loss = 0.25441645\n",
      "Iteration 794, loss = 0.25420378\n",
      "Iteration 795, loss = 0.25399149\n",
      "Iteration 796, loss = 0.25377950\n",
      "Iteration 797, loss = 0.25356791\n",
      "Iteration 798, loss = 0.25335659\n",
      "Iteration 799, loss = 0.25314565\n",
      "Iteration 800, loss = 0.25293505\n",
      "Iteration 801, loss = 0.25272481\n",
      "Iteration 802, loss = 0.25251486\n",
      "Iteration 803, loss = 0.25230529\n",
      "Iteration 804, loss = 0.25209602\n",
      "Iteration 805, loss = 0.25188714\n",
      "Iteration 806, loss = 0.25167854\n",
      "Iteration 807, loss = 0.25147034\n",
      "Iteration 808, loss = 0.25126244\n",
      "Iteration 809, loss = 0.25105490\n",
      "Iteration 810, loss = 0.25084765\n",
      "Iteration 811, loss = 0.25064080\n",
      "Iteration 812, loss = 0.25043424\n",
      "Iteration 813, loss = 0.25022805\n",
      "Iteration 814, loss = 0.25002215\n",
      "Iteration 815, loss = 0.24981662\n",
      "Iteration 816, loss = 0.24961140\n",
      "Iteration 817, loss = 0.24940654\n",
      "Iteration 818, loss = 0.24920201\n",
      "Iteration 819, loss = 0.24899781\n",
      "Iteration 820, loss = 0.24879395\n",
      "Iteration 821, loss = 0.24859040\n",
      "Iteration 822, loss = 0.24838722\n",
      "Iteration 823, loss = 0.24818433\n",
      "Iteration 824, loss = 0.24798181\n",
      "Iteration 825, loss = 0.24777956\n",
      "Iteration 826, loss = 0.24757773\n",
      "Iteration 827, loss = 0.24737614\n",
      "Iteration 828, loss = 0.24717496\n",
      "Iteration 829, loss = 0.24697407\n",
      "Iteration 830, loss = 0.24677349\n",
      "Iteration 831, loss = 0.24657330\n",
      "Iteration 832, loss = 0.24637337\n",
      "Iteration 833, loss = 0.24617383\n",
      "Iteration 834, loss = 0.24597459\n",
      "Iteration 835, loss = 0.24577566\n",
      "Iteration 836, loss = 0.24557712\n",
      "Iteration 837, loss = 0.24537885\n",
      "Iteration 838, loss = 0.24518090\n",
      "Iteration 839, loss = 0.24498336\n",
      "Iteration 840, loss = 0.24478607\n",
      "Iteration 841, loss = 0.24458912\n",
      "Iteration 842, loss = 0.24439249\n",
      "Iteration 843, loss = 0.24419617\n",
      "Iteration 844, loss = 0.24400024\n",
      "Iteration 845, loss = 0.24380455\n",
      "Iteration 846, loss = 0.24360926\n",
      "Iteration 847, loss = 0.24341425\n",
      "Iteration 848, loss = 0.24321955\n",
      "Iteration 849, loss = 0.24302519\n",
      "Iteration 850, loss = 0.24283115\n",
      "Iteration 851, loss = 0.24263742\n",
      "Iteration 852, loss = 0.24244405\n",
      "Iteration 853, loss = 0.24225097\n",
      "Iteration 854, loss = 0.24205821\n",
      "Iteration 855, loss = 0.24186578\n",
      "Iteration 856, loss = 0.24167367\n",
      "Iteration 857, loss = 0.24148185\n",
      "Iteration 858, loss = 0.24129041\n",
      "Iteration 859, loss = 0.24109921\n",
      "Iteration 860, loss = 0.24090837\n",
      "Iteration 861, loss = 0.24071786\n",
      "Iteration 862, loss = 0.24052763\n",
      "Iteration 863, loss = 0.24033774\n",
      "Iteration 864, loss = 0.24014817\n",
      "Iteration 865, loss = 0.23995890\n",
      "Iteration 866, loss = 0.23976994\n",
      "Iteration 867, loss = 0.23958133\n",
      "Iteration 868, loss = 0.23939300\n",
      "Iteration 869, loss = 0.23920500\n",
      "Iteration 870, loss = 0.23901730\n",
      "Iteration 871, loss = 0.23882993\n",
      "Iteration 872, loss = 0.23864286\n",
      "Iteration 873, loss = 0.23845610\n",
      "Iteration 874, loss = 0.23826965\n",
      "Iteration 875, loss = 0.23808354\n",
      "Iteration 876, loss = 0.23789770\n",
      "Iteration 877, loss = 0.23771219\n",
      "Iteration 878, loss = 0.23752701\n",
      "Iteration 879, loss = 0.23734210\n",
      "Iteration 880, loss = 0.23715751\n",
      "Iteration 881, loss = 0.23697325\n",
      "Iteration 882, loss = 0.23678928\n",
      "Iteration 883, loss = 0.23660562\n",
      "Iteration 884, loss = 0.23642227\n",
      "Iteration 885, loss = 0.23623921\n",
      "Iteration 886, loss = 0.23605652\n",
      "Iteration 887, loss = 0.23587405\n",
      "Iteration 888, loss = 0.23569193\n",
      "Iteration 889, loss = 0.23551010\n",
      "Iteration 890, loss = 0.23532863\n",
      "Iteration 891, loss = 0.23514739\n",
      "Iteration 892, loss = 0.23496648\n",
      "Iteration 893, loss = 0.23478588\n",
      "Iteration 894, loss = 0.23460557\n",
      "Iteration 895, loss = 0.23442560\n",
      "Iteration 896, loss = 0.23424589\n",
      "Iteration 897, loss = 0.23406650\n",
      "Iteration 898, loss = 0.23388740\n",
      "Iteration 899, loss = 0.23370862\n",
      "Iteration 900, loss = 0.23353013\n",
      "Iteration 901, loss = 0.23335195\n",
      "Iteration 902, loss = 0.23317404\n",
      "Iteration 903, loss = 0.23299645\n",
      "Iteration 904, loss = 0.23281915\n",
      "Iteration 905, loss = 0.23264218\n",
      "Iteration 906, loss = 0.23246546\n",
      "Iteration 907, loss = 0.23228907\n",
      "Iteration 908, loss = 0.23211296\n",
      "Iteration 909, loss = 0.23193716\n",
      "Iteration 910, loss = 0.23176164\n",
      "Iteration 911, loss = 0.23158643\n",
      "Iteration 912, loss = 0.23141152\n",
      "Iteration 913, loss = 0.23123689\n",
      "Iteration 914, loss = 0.23106257\n",
      "Iteration 915, loss = 0.23088852\n",
      "Iteration 916, loss = 0.23071479\n",
      "Iteration 917, loss = 0.23054133\n",
      "Iteration 918, loss = 0.23036818\n",
      "Iteration 919, loss = 0.23019532\n",
      "Iteration 920, loss = 0.23002274\n",
      "Iteration 921, loss = 0.22985046\n",
      "Iteration 922, loss = 0.22967847\n",
      "Iteration 923, loss = 0.22950677\n",
      "Iteration 924, loss = 0.22933537\n",
      "Iteration 925, loss = 0.22916424\n",
      "Iteration 926, loss = 0.22899341\n",
      "Iteration 927, loss = 0.22882286\n",
      "Iteration 928, loss = 0.22865261\n",
      "Iteration 929, loss = 0.22848264\n",
      "Iteration 930, loss = 0.22831297\n",
      "Iteration 931, loss = 0.22814358\n",
      "Iteration 932, loss = 0.22797447\n",
      "Iteration 933, loss = 0.22780565\n",
      "Iteration 934, loss = 0.22763712\n",
      "Iteration 935, loss = 0.22746886\n",
      "Iteration 936, loss = 0.22730090\n",
      "Iteration 937, loss = 0.22713322\n",
      "Iteration 938, loss = 0.22696582\n",
      "Iteration 939, loss = 0.22679870\n",
      "Iteration 940, loss = 0.22663188\n",
      "Iteration 941, loss = 0.22646533\n",
      "Iteration 942, loss = 0.22629906\n",
      "Iteration 943, loss = 0.22613307\n",
      "Iteration 944, loss = 0.22596738\n",
      "Iteration 945, loss = 0.22580194\n",
      "Iteration 946, loss = 0.22563682\n",
      "Iteration 947, loss = 0.22547194\n",
      "Iteration 948, loss = 0.22530736\n",
      "Iteration 949, loss = 0.22514305\n",
      "Iteration 950, loss = 0.22497903\n",
      "Iteration 951, loss = 0.22481528\n",
      "Iteration 952, loss = 0.22465181\n",
      "Iteration 953, loss = 0.22448861\n",
      "Iteration 954, loss = 0.22432571\n",
      "Iteration 955, loss = 0.22416306\n",
      "Iteration 956, loss = 0.22400068\n",
      "Iteration 957, loss = 0.22383860\n",
      "Iteration 958, loss = 0.22367678\n",
      "Iteration 959, loss = 0.22351524\n",
      "Iteration 960, loss = 0.22335397\n",
      "Iteration 961, loss = 0.22319298\n",
      "Iteration 962, loss = 0.22303225\n",
      "Iteration 963, loss = 0.22287181\n",
      "Iteration 964, loss = 0.22271162\n",
      "Iteration 965, loss = 0.22255173\n",
      "Iteration 966, loss = 0.22239209\n",
      "Iteration 967, loss = 0.22223272\n",
      "Iteration 968, loss = 0.22207362\n",
      "Iteration 969, loss = 0.22191480\n",
      "Iteration 970, loss = 0.22175624\n",
      "Iteration 971, loss = 0.22159797\n",
      "Iteration 972, loss = 0.22143994\n",
      "Iteration 973, loss = 0.22128220\n",
      "Iteration 974, loss = 0.22112471\n",
      "Iteration 975, loss = 0.22096750\n",
      "Iteration 976, loss = 0.22081054\n",
      "Iteration 977, loss = 0.22065387\n",
      "Iteration 978, loss = 0.22049745\n",
      "Iteration 979, loss = 0.22034129\n",
      "Iteration 980, loss = 0.22018541\n",
      "Iteration 981, loss = 0.22002979\n",
      "Iteration 982, loss = 0.21987442\n",
      "Iteration 983, loss = 0.21971933\n",
      "Iteration 984, loss = 0.21956450\n",
      "Iteration 985, loss = 0.21940993\n",
      "Iteration 986, loss = 0.21925562\n",
      "Iteration 987, loss = 0.21910157\n",
      "Iteration 988, loss = 0.21894779\n",
      "Iteration 989, loss = 0.21879426\n",
      "Iteration 990, loss = 0.21864100\n",
      "Iteration 991, loss = 0.21848799\n",
      "Iteration 992, loss = 0.21833525\n",
      "Iteration 993, loss = 0.21818276\n",
      "Iteration 994, loss = 0.21803053\n",
      "Iteration 995, loss = 0.21787855\n",
      "Iteration 996, loss = 0.21772685\n",
      "Iteration 997, loss = 0.21757538\n",
      "Iteration 998, loss = 0.21742419\n",
      "Iteration 999, loss = 0.21727324\n",
      "Iteration 1000, loss = 0.21712255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.36148097\n",
      "Iteration 2, loss = 1.32450113\n",
      "Iteration 3, loss = 1.27472347\n",
      "Iteration 4, loss = 1.21631650\n",
      "Iteration 5, loss = 1.15351700\n",
      "Iteration 6, loss = 1.09020961\n",
      "Iteration 7, loss = 1.02958053\n",
      "Iteration 8, loss = 0.97428959\n",
      "Iteration 9, loss = 0.92596325\n",
      "Iteration 10, loss = 0.88538384\n",
      "Iteration 11, loss = 0.85255106\n",
      "Iteration 12, loss = 0.82656239\n",
      "Iteration 13, loss = 0.80638828\n",
      "Iteration 14, loss = 0.79075824\n",
      "Iteration 15, loss = 0.77849601\n",
      "Iteration 16, loss = 0.76869616\n",
      "Iteration 17, loss = 0.76063668\n",
      "Iteration 18, loss = 0.75384468\n",
      "Iteration 19, loss = 0.74822788\n",
      "Iteration 20, loss = 0.74346250\n",
      "Iteration 21, loss = 0.73952440\n",
      "Iteration 22, loss = 0.73595039\n",
      "Iteration 23, loss = 0.73212255\n",
      "Iteration 24, loss = 0.72798934\n",
      "Iteration 25, loss = 0.72340162\n",
      "Iteration 26, loss = 0.71826116\n",
      "Iteration 27, loss = 0.71284127\n",
      "Iteration 28, loss = 0.70723956\n",
      "Iteration 29, loss = 0.70152970\n",
      "Iteration 30, loss = 0.69589369\n",
      "Iteration 31, loss = 0.69043535\n",
      "Iteration 32, loss = 0.68525653\n",
      "Iteration 33, loss = 0.68044306\n",
      "Iteration 34, loss = 0.67598206\n",
      "Iteration 35, loss = 0.67189548\n",
      "Iteration 36, loss = 0.66815188\n",
      "Iteration 37, loss = 0.66470241\n",
      "Iteration 38, loss = 0.66151842\n",
      "Iteration 39, loss = 0.65856630\n",
      "Iteration 40, loss = 0.65578194\n",
      "Iteration 41, loss = 0.65312172\n",
      "Iteration 42, loss = 0.65055373\n",
      "Iteration 43, loss = 0.64805211\n",
      "Iteration 44, loss = 0.64558872\n",
      "Iteration 45, loss = 0.64314527\n",
      "Iteration 46, loss = 0.64070663\n",
      "Iteration 47, loss = 0.63826215\n",
      "Iteration 48, loss = 0.63581004\n",
      "Iteration 49, loss = 0.63335199\n",
      "Iteration 50, loss = 0.63089220\n",
      "Iteration 51, loss = 0.62843649\n",
      "Iteration 52, loss = 0.62599146\n",
      "Iteration 53, loss = 0.62356382\n",
      "Iteration 54, loss = 0.62115982\n",
      "Iteration 55, loss = 0.61878519\n",
      "Iteration 56, loss = 0.61644431\n",
      "Iteration 57, loss = 0.61413978\n",
      "Iteration 58, loss = 0.61187428\n",
      "Iteration 59, loss = 0.60964937\n",
      "Iteration 60, loss = 0.60746552\n",
      "Iteration 61, loss = 0.60532773\n",
      "Iteration 62, loss = 0.60322990\n",
      "Iteration 63, loss = 0.60116931\n",
      "Iteration 64, loss = 0.59914376\n",
      "Iteration 65, loss = 0.59715147\n",
      "Iteration 66, loss = 0.59519033\n",
      "Iteration 67, loss = 0.59325806\n",
      "Iteration 68, loss = 0.59135273\n",
      "Iteration 69, loss = 0.58947265\n",
      "Iteration 70, loss = 0.58761996\n",
      "Iteration 71, loss = 0.58579297\n",
      "Iteration 72, loss = 0.58398890\n",
      "Iteration 73, loss = 0.58220753\n",
      "Iteration 74, loss = 0.58044706\n",
      "Iteration 75, loss = 0.57870703\n",
      "Iteration 76, loss = 0.57698709\n",
      "Iteration 77, loss = 0.57528695\n",
      "Iteration 78, loss = 0.57360639\n",
      "Iteration 79, loss = 0.57194520\n",
      "Iteration 80, loss = 0.57030319\n",
      "Iteration 81, loss = 0.56868016\n",
      "Iteration 82, loss = 0.56707588\n",
      "Iteration 83, loss = 0.56549110\n",
      "Iteration 84, loss = 0.56392971\n",
      "Iteration 85, loss = 0.56238741\n",
      "Iteration 86, loss = 0.56086309\n",
      "Iteration 87, loss = 0.55935915\n",
      "Iteration 88, loss = 0.55787241\n",
      "Iteration 89, loss = 0.55640297\n",
      "Iteration 90, loss = 0.55494931\n",
      "Iteration 91, loss = 0.55351158\n",
      "Iteration 92, loss = 0.55208926\n",
      "Iteration 93, loss = 0.55068187\n",
      "Iteration 94, loss = 0.54928894\n",
      "Iteration 95, loss = 0.54791016\n",
      "Iteration 96, loss = 0.54654519\n",
      "Iteration 97, loss = 0.54519372\n",
      "Iteration 98, loss = 0.54385578\n",
      "Iteration 99, loss = 0.54253154\n",
      "Iteration 100, loss = 0.54122005\n",
      "Iteration 101, loss = 0.53992105\n",
      "Iteration 102, loss = 0.53863427\n",
      "Iteration 103, loss = 0.53736079\n",
      "Iteration 104, loss = 0.53610146\n",
      "Iteration 105, loss = 0.53485403\n",
      "Iteration 106, loss = 0.53361826\n",
      "Iteration 107, loss = 0.53239395\n",
      "Iteration 108, loss = 0.53118091\n",
      "Iteration 109, loss = 0.52997892\n",
      "Iteration 110, loss = 0.52878777\n",
      "Iteration 111, loss = 0.52760738\n",
      "Iteration 112, loss = 0.52643766\n",
      "Iteration 113, loss = 0.52527817\n",
      "Iteration 114, loss = 0.52412874\n",
      "Iteration 115, loss = 0.52298920\n",
      "Iteration 116, loss = 0.52186256\n",
      "Iteration 117, loss = 0.52074581\n",
      "Iteration 118, loss = 0.51963885\n",
      "Iteration 119, loss = 0.51854310\n",
      "Iteration 120, loss = 0.51746315\n",
      "Iteration 121, loss = 0.51639265\n",
      "Iteration 122, loss = 0.51533186\n",
      "Iteration 123, loss = 0.51428099\n",
      "Iteration 124, loss = 0.51323882\n",
      "Iteration 125, loss = 0.51220527\n",
      "Iteration 126, loss = 0.51118029\n",
      "Iteration 127, loss = 0.51016455\n",
      "Iteration 128, loss = 0.50915701\n",
      "Iteration 129, loss = 0.50815770\n",
      "Iteration 130, loss = 0.50716657\n",
      "Iteration 131, loss = 0.50618357\n",
      "Iteration 132, loss = 0.50520863\n",
      "Iteration 133, loss = 0.50424168\n",
      "Iteration 134, loss = 0.50328262\n",
      "Iteration 135, loss = 0.50233136\n",
      "Iteration 136, loss = 0.50138778\n",
      "Iteration 137, loss = 0.50045176\n",
      "Iteration 138, loss = 0.49952319\n",
      "Iteration 139, loss = 0.49860195\n",
      "Iteration 140, loss = 0.49768792\n",
      "Iteration 141, loss = 0.49678097\n",
      "Iteration 142, loss = 0.49588098\n",
      "Iteration 143, loss = 0.49498784\n",
      "Iteration 144, loss = 0.49410143\n",
      "Iteration 145, loss = 0.49322163\n",
      "Iteration 146, loss = 0.49234831\n",
      "Iteration 147, loss = 0.49148135\n",
      "Iteration 148, loss = 0.49062063\n",
      "Iteration 149, loss = 0.48976606\n",
      "Iteration 150, loss = 0.48891772\n",
      "Iteration 151, loss = 0.48807561\n",
      "Iteration 152, loss = 0.48723942\n",
      "Iteration 153, loss = 0.48640906\n",
      "Iteration 154, loss = 0.48558443\n",
      "Iteration 155, loss = 0.48476545\n",
      "Iteration 156, loss = 0.48395203\n",
      "Iteration 157, loss = 0.48314407\n",
      "Iteration 158, loss = 0.48234148\n",
      "Iteration 159, loss = 0.48154417\n",
      "Iteration 160, loss = 0.48075206\n",
      "Iteration 161, loss = 0.47996505\n",
      "Iteration 162, loss = 0.47918307\n",
      "Iteration 163, loss = 0.47840601\n",
      "Iteration 164, loss = 0.47763380\n",
      "Iteration 165, loss = 0.47686635\n",
      "Iteration 166, loss = 0.47610358\n",
      "Iteration 167, loss = 0.47534540\n",
      "Iteration 168, loss = 0.47459237\n",
      "Iteration 169, loss = 0.47384388\n",
      "Iteration 170, loss = 0.47309983\n",
      "Iteration 171, loss = 0.47236016\n",
      "Iteration 172, loss = 0.47162481\n",
      "Iteration 173, loss = 0.47089370\n",
      "Iteration 174, loss = 0.47016679\n",
      "Iteration 175, loss = 0.46944399\n",
      "Iteration 176, loss = 0.46872525\n",
      "Iteration 177, loss = 0.46801050\n",
      "Iteration 178, loss = 0.46729968\n",
      "Iteration 179, loss = 0.46659272\n",
      "Iteration 180, loss = 0.46588956\n",
      "Iteration 181, loss = 0.46519014\n",
      "Iteration 182, loss = 0.46449439\n",
      "Iteration 183, loss = 0.46380226\n",
      "Iteration 184, loss = 0.46311369\n",
      "Iteration 185, loss = 0.46242861\n",
      "Iteration 186, loss = 0.46174698\n",
      "Iteration 187, loss = 0.46106872\n",
      "Iteration 188, loss = 0.46039380\n",
      "Iteration 189, loss = 0.45972214\n",
      "Iteration 190, loss = 0.45905370\n",
      "Iteration 191, loss = 0.45838843\n",
      "Iteration 192, loss = 0.45772627\n",
      "Iteration 193, loss = 0.45706717\n",
      "Iteration 194, loss = 0.45641109\n",
      "Iteration 195, loss = 0.45575797\n",
      "Iteration 196, loss = 0.45510777\n",
      "Iteration 197, loss = 0.45446044\n",
      "Iteration 198, loss = 0.45381593\n",
      "Iteration 199, loss = 0.45317420\n",
      "Iteration 200, loss = 0.45253521\n",
      "Iteration 201, loss = 0.45189891\n",
      "Iteration 202, loss = 0.45126526\n",
      "Iteration 203, loss = 0.45063422\n",
      "Iteration 204, loss = 0.45000575\n",
      "Iteration 205, loss = 0.44937981\n",
      "Iteration 206, loss = 0.44875635\n",
      "Iteration 207, loss = 0.44813535\n",
      "Iteration 208, loss = 0.44751677\n",
      "Iteration 209, loss = 0.44690093\n",
      "Iteration 210, loss = 0.44628823\n",
      "Iteration 211, loss = 0.44567787\n",
      "Iteration 212, loss = 0.44506982\n",
      "Iteration 213, loss = 0.44446437\n",
      "Iteration 214, loss = 0.44386266\n",
      "Iteration 215, loss = 0.44326331\n",
      "Iteration 216, loss = 0.44266629\n",
      "Iteration 217, loss = 0.44207158\n",
      "Iteration 218, loss = 0.44147913\n",
      "Iteration 219, loss = 0.44088893\n",
      "Iteration 220, loss = 0.44030093\n",
      "Iteration 221, loss = 0.43971510\n",
      "Iteration 222, loss = 0.43913141\n",
      "Iteration 223, loss = 0.43855072\n",
      "Iteration 224, loss = 0.43797182\n",
      "Iteration 225, loss = 0.43739487\n",
      "Iteration 226, loss = 0.43681988\n",
      "Iteration 227, loss = 0.43624784\n",
      "Iteration 228, loss = 0.43567835\n",
      "Iteration 229, loss = 0.43511087\n",
      "Iteration 230, loss = 0.43454541\n",
      "Iteration 231, loss = 0.43398217\n",
      "Iteration 232, loss = 0.43342090\n",
      "Iteration 233, loss = 0.43286145\n",
      "Iteration 234, loss = 0.43230403\n",
      "Iteration 235, loss = 0.43174847\n",
      "Iteration 236, loss = 0.43119476\n",
      "Iteration 237, loss = 0.43064307\n",
      "Iteration 238, loss = 0.43009297\n",
      "Iteration 239, loss = 0.42954481\n",
      "Iteration 240, loss = 0.42899848\n",
      "Iteration 241, loss = 0.42845384\n",
      "Iteration 242, loss = 0.42791093\n",
      "Iteration 243, loss = 0.42736983\n",
      "Iteration 244, loss = 0.42683105\n",
      "Iteration 245, loss = 0.42629460\n",
      "Iteration 246, loss = 0.42575983\n",
      "Iteration 247, loss = 0.42522673\n",
      "Iteration 248, loss = 0.42469529\n",
      "Iteration 249, loss = 0.42416550\n",
      "Iteration 250, loss = 0.42363747\n",
      "Iteration 251, loss = 0.42311099\n",
      "Iteration 252, loss = 0.42258615\n",
      "Iteration 253, loss = 0.42206287\n",
      "Iteration 254, loss = 0.42154126\n",
      "Iteration 255, loss = 0.42102105\n",
      "Iteration 256, loss = 0.42050250\n",
      "Iteration 257, loss = 0.41998538\n",
      "Iteration 258, loss = 0.41946975\n",
      "Iteration 259, loss = 0.41895569\n",
      "Iteration 260, loss = 0.41844298\n",
      "Iteration 261, loss = 0.41793167\n",
      "Iteration 262, loss = 0.41742189\n",
      "Iteration 263, loss = 0.41691340\n",
      "Iteration 264, loss = 0.41640640\n",
      "Iteration 265, loss = 0.41590065\n",
      "Iteration 266, loss = 0.41539630\n",
      "Iteration 267, loss = 0.41489322\n",
      "Iteration 268, loss = 0.41439155\n",
      "Iteration 269, loss = 0.41389106\n",
      "Iteration 270, loss = 0.41339193\n",
      "Iteration 271, loss = 0.41289397\n",
      "Iteration 272, loss = 0.41239738\n",
      "Iteration 273, loss = 0.41190191\n",
      "Iteration 274, loss = 0.41140765\n",
      "Iteration 275, loss = 0.41091467\n",
      "Iteration 276, loss = 0.41042279\n",
      "Iteration 277, loss = 0.40993219\n",
      "Iteration 278, loss = 0.40944261\n",
      "Iteration 279, loss = 0.40895432\n",
      "Iteration 280, loss = 0.40846708\n",
      "Iteration 281, loss = 0.40798094\n",
      "Iteration 282, loss = 0.40749589\n",
      "Iteration 283, loss = 0.40701208\n",
      "Iteration 284, loss = 0.40653082\n",
      "Iteration 285, loss = 0.40605232\n",
      "Iteration 286, loss = 0.40557542\n",
      "Iteration 287, loss = 0.40509936\n",
      "Iteration 288, loss = 0.40462411\n",
      "Iteration 289, loss = 0.40414972\n",
      "Iteration 290, loss = 0.40367632\n",
      "Iteration 291, loss = 0.40320382\n",
      "Iteration 292, loss = 0.40273230\n",
      "Iteration 293, loss = 0.40226187\n",
      "Iteration 294, loss = 0.40179241\n",
      "Iteration 295, loss = 0.40132404\n",
      "Iteration 296, loss = 0.40085714\n",
      "Iteration 297, loss = 0.40039185\n",
      "Iteration 298, loss = 0.39992764\n",
      "Iteration 299, loss = 0.39946447\n",
      "Iteration 300, loss = 0.39900241\n",
      "Iteration 301, loss = 0.39854129\n",
      "Iteration 302, loss = 0.39808120\n",
      "Iteration 303, loss = 0.39762214\n",
      "Iteration 304, loss = 0.39716404\n",
      "Iteration 305, loss = 0.39670690\n",
      "Iteration 306, loss = 0.39625074\n",
      "Iteration 307, loss = 0.39579585\n",
      "Iteration 308, loss = 0.39534189\n",
      "Iteration 309, loss = 0.39488891\n",
      "Iteration 310, loss = 0.39443691\n",
      "Iteration 311, loss = 0.39398592\n",
      "Iteration 312, loss = 0.39353584\n",
      "Iteration 313, loss = 0.39308675\n",
      "Iteration 314, loss = 0.39263853\n",
      "Iteration 315, loss = 0.39219131\n",
      "Iteration 316, loss = 0.39174497\n",
      "Iteration 317, loss = 0.39129956\n",
      "Iteration 318, loss = 0.39085507\n",
      "Iteration 319, loss = 0.39041156\n",
      "Iteration 320, loss = 0.38996883\n",
      "Iteration 321, loss = 0.38952705\n",
      "Iteration 322, loss = 0.38908613\n",
      "Iteration 323, loss = 0.38864610\n",
      "Iteration 324, loss = 0.38820699\n",
      "Iteration 325, loss = 0.38776867\n",
      "Iteration 326, loss = 0.38733121\n",
      "Iteration 327, loss = 0.38689465\n",
      "Iteration 328, loss = 0.38645888\n",
      "Iteration 329, loss = 0.38602398\n",
      "Iteration 330, loss = 0.38558997\n",
      "Iteration 331, loss = 0.38515668\n",
      "Iteration 332, loss = 0.38472424\n",
      "Iteration 333, loss = 0.38429263\n",
      "Iteration 334, loss = 0.38386183\n",
      "Iteration 335, loss = 0.38343182\n",
      "Iteration 336, loss = 0.38300261\n",
      "Iteration 337, loss = 0.38257418\n",
      "Iteration 338, loss = 0.38214657\n",
      "Iteration 339, loss = 0.38171972\n",
      "Iteration 340, loss = 0.38129366\n",
      "Iteration 341, loss = 0.38086835\n",
      "Iteration 342, loss = 0.38044380\n",
      "Iteration 343, loss = 0.38002002\n",
      "Iteration 344, loss = 0.37959698\n",
      "Iteration 345, loss = 0.37917470\n",
      "Iteration 346, loss = 0.37875315\n",
      "Iteration 347, loss = 0.37833234\n",
      "Iteration 348, loss = 0.37791226\n",
      "Iteration 349, loss = 0.37749291\n",
      "Iteration 350, loss = 0.37707428\n",
      "Iteration 351, loss = 0.37665637\n",
      "Iteration 352, loss = 0.37623916\n",
      "Iteration 353, loss = 0.37582266\n",
      "Iteration 354, loss = 0.37540702\n",
      "Iteration 355, loss = 0.37499201\n",
      "Iteration 356, loss = 0.37457769\n",
      "Iteration 357, loss = 0.37416404\n",
      "Iteration 358, loss = 0.37375107\n",
      "Iteration 359, loss = 0.37333878\n",
      "Iteration 360, loss = 0.37292720\n",
      "Iteration 361, loss = 0.37251632\n",
      "Iteration 362, loss = 0.37210612\n",
      "Iteration 363, loss = 0.37169660\n",
      "Iteration 364, loss = 0.37128775\n",
      "Iteration 365, loss = 0.37087956\n",
      "Iteration 366, loss = 0.37047205\n",
      "Iteration 367, loss = 0.37006518\n",
      "Iteration 368, loss = 0.36965897\n",
      "Iteration 369, loss = 0.36925343\n",
      "Iteration 370, loss = 0.36885061\n",
      "Iteration 371, loss = 0.36844873\n",
      "Iteration 372, loss = 0.36804756\n",
      "Iteration 373, loss = 0.36764708\n",
      "Iteration 374, loss = 0.36724730\n",
      "Iteration 375, loss = 0.36684821\n",
      "Iteration 376, loss = 0.36644985\n",
      "Iteration 377, loss = 0.36605209\n",
      "Iteration 378, loss = 0.36565503\n",
      "Iteration 379, loss = 0.36525863\n",
      "Iteration 380, loss = 0.36486288\n",
      "Iteration 381, loss = 0.36446776\n",
      "Iteration 382, loss = 0.36407328\n",
      "Iteration 383, loss = 0.36367942\n",
      "Iteration 384, loss = 0.36328617\n",
      "Iteration 385, loss = 0.36289354\n",
      "Iteration 386, loss = 0.36250151\n",
      "Iteration 387, loss = 0.36211008\n",
      "Iteration 388, loss = 0.36171925\n",
      "Iteration 389, loss = 0.36132900\n",
      "Iteration 390, loss = 0.36093934\n",
      "Iteration 391, loss = 0.36055026\n",
      "Iteration 392, loss = 0.36016176\n",
      "Iteration 393, loss = 0.35977384\n",
      "Iteration 394, loss = 0.35938648\n",
      "Iteration 395, loss = 0.35899969\n",
      "Iteration 396, loss = 0.35861346\n",
      "Iteration 397, loss = 0.35822779\n",
      "Iteration 398, loss = 0.35784268\n",
      "Iteration 399, loss = 0.35745812\n",
      "Iteration 400, loss = 0.35707411\n",
      "Iteration 401, loss = 0.35669064\n",
      "Iteration 402, loss = 0.35630771\n",
      "Iteration 403, loss = 0.35592532\n",
      "Iteration 404, loss = 0.35554348\n",
      "Iteration 405, loss = 0.35516230\n",
      "Iteration 406, loss = 0.35478172\n",
      "Iteration 407, loss = 0.35440194\n",
      "Iteration 408, loss = 0.35402270\n",
      "Iteration 409, loss = 0.35364400\n",
      "Iteration 410, loss = 0.35326584\n",
      "Iteration 411, loss = 0.35288821\n",
      "Iteration 412, loss = 0.35251116\n",
      "Iteration 413, loss = 0.35213488\n",
      "Iteration 414, loss = 0.35175908\n",
      "Iteration 415, loss = 0.35138380\n",
      "Iteration 416, loss = 0.35100904\n",
      "Iteration 417, loss = 0.35063479\n",
      "Iteration 418, loss = 0.35026106\n",
      "Iteration 419, loss = 0.34988785\n",
      "Iteration 420, loss = 0.34951516\n",
      "Iteration 421, loss = 0.34914298\n",
      "Iteration 422, loss = 0.34877132\n",
      "Iteration 423, loss = 0.34840031\n",
      "Iteration 424, loss = 0.34802961\n",
      "Iteration 425, loss = 0.34765952\n",
      "Iteration 426, loss = 0.34728995\n",
      "Iteration 427, loss = 0.34692088\n",
      "Iteration 428, loss = 0.34655232\n",
      "Iteration 429, loss = 0.34618428\n",
      "Iteration 430, loss = 0.34581675\n",
      "Iteration 431, loss = 0.34544971\n",
      "Iteration 432, loss = 0.34508318\n",
      "Iteration 433, loss = 0.34471727\n",
      "Iteration 434, loss = 0.34435168\n",
      "Iteration 435, loss = 0.34398668\n",
      "Iteration 436, loss = 0.34362217\n",
      "Iteration 437, loss = 0.34325815\n",
      "Iteration 438, loss = 0.34289462\n",
      "Iteration 439, loss = 0.34253158\n",
      "Iteration 440, loss = 0.34216903\n",
      "Iteration 441, loss = 0.34180697\n",
      "Iteration 442, loss = 0.34144540\n",
      "Iteration 443, loss = 0.34108442\n",
      "Iteration 444, loss = 0.34072377\n",
      "Iteration 445, loss = 0.34036368\n",
      "Iteration 446, loss = 0.34000407\n",
      "Iteration 447, loss = 0.33964495\n",
      "Iteration 448, loss = 0.33928630\n",
      "Iteration 449, loss = 0.33892812\n",
      "Iteration 450, loss = 0.33857043\n",
      "Iteration 451, loss = 0.33821321\n",
      "Iteration 452, loss = 0.33785647\n",
      "Iteration 453, loss = 0.33750021\n",
      "Iteration 454, loss = 0.33714446\n",
      "Iteration 455, loss = 0.33678914\n",
      "Iteration 456, loss = 0.33643432\n",
      "Iteration 457, loss = 0.33607996\n",
      "Iteration 458, loss = 0.33572606\n",
      "Iteration 459, loss = 0.33537264\n",
      "Iteration 460, loss = 0.33502007\n",
      "Iteration 461, loss = 0.33466827\n",
      "Iteration 462, loss = 0.33431692\n",
      "Iteration 463, loss = 0.33396605\n",
      "Iteration 464, loss = 0.33361564\n",
      "Iteration 465, loss = 0.33326569\n",
      "Iteration 466, loss = 0.33291620\n",
      "Iteration 467, loss = 0.33256718\n",
      "Iteration 468, loss = 0.33221868\n",
      "Iteration 469, loss = 0.33187058\n",
      "Iteration 470, loss = 0.33152297\n",
      "Iteration 471, loss = 0.33117581\n",
      "Iteration 472, loss = 0.33082910\n",
      "Iteration 473, loss = 0.33048284\n",
      "Iteration 474, loss = 0.33013703\n",
      "Iteration 475, loss = 0.32979178\n",
      "Iteration 476, loss = 0.32944684\n",
      "Iteration 477, loss = 0.32910240\n",
      "Iteration 478, loss = 0.32875840\n",
      "Iteration 479, loss = 0.32841484\n",
      "Iteration 480, loss = 0.32807181\n",
      "Iteration 481, loss = 0.32772914\n",
      "Iteration 482, loss = 0.32738693\n",
      "Iteration 483, loss = 0.32704516\n",
      "Iteration 484, loss = 0.32670382\n",
      "Iteration 485, loss = 0.32636294\n",
      "Iteration 486, loss = 0.32602255\n",
      "Iteration 487, loss = 0.32568255\n",
      "Iteration 488, loss = 0.32534299\n",
      "Iteration 489, loss = 0.32500386\n",
      "Iteration 490, loss = 0.32466516\n",
      "Iteration 491, loss = 0.32432699\n",
      "Iteration 492, loss = 0.32398916\n",
      "Iteration 493, loss = 0.32365180\n",
      "Iteration 494, loss = 0.32331487\n",
      "Iteration 495, loss = 0.32297837\n",
      "Iteration 496, loss = 0.32264231\n",
      "Iteration 497, loss = 0.32230675\n",
      "Iteration 498, loss = 0.32197158\n",
      "Iteration 499, loss = 0.32163683\n",
      "Iteration 500, loss = 0.32130251\n",
      "Iteration 501, loss = 0.32096862\n",
      "Iteration 502, loss = 0.32063516\n",
      "Iteration 503, loss = 0.32030218\n",
      "Iteration 504, loss = 0.31996961\n",
      "Iteration 505, loss = 0.31963747\n",
      "Iteration 506, loss = 0.31930576\n",
      "Iteration 507, loss = 0.31897447\n",
      "Iteration 508, loss = 0.31864361\n",
      "Iteration 509, loss = 0.31831321\n",
      "Iteration 510, loss = 0.31798326\n",
      "Iteration 511, loss = 0.31765372\n",
      "Iteration 512, loss = 0.31732460\n",
      "Iteration 513, loss = 0.31699590\n",
      "Iteration 514, loss = 0.31666763\n",
      "Iteration 515, loss = 0.31633978\n",
      "Iteration 516, loss = 0.31601245\n",
      "Iteration 517, loss = 0.31568546\n",
      "Iteration 518, loss = 0.31535892\n",
      "Iteration 519, loss = 0.31503282\n",
      "Iteration 520, loss = 0.31470713\n",
      "Iteration 521, loss = 0.31438187\n",
      "Iteration 522, loss = 0.31405708\n",
      "Iteration 523, loss = 0.31373270\n",
      "Iteration 524, loss = 0.31340875\n",
      "Iteration 525, loss = 0.31308522\n",
      "Iteration 526, loss = 0.31276211\n",
      "Iteration 527, loss = 0.31243942\n",
      "Iteration 528, loss = 0.31211716\n",
      "Iteration 529, loss = 0.31179538\n",
      "Iteration 530, loss = 0.31147399\n",
      "Iteration 531, loss = 0.31115304\n",
      "Iteration 532, loss = 0.31083250\n",
      "Iteration 533, loss = 0.31051239\n",
      "Iteration 534, loss = 0.31019270\n",
      "Iteration 535, loss = 0.30987344\n",
      "Iteration 536, loss = 0.30955470\n",
      "Iteration 537, loss = 0.30923625\n",
      "Iteration 538, loss = 0.30891829\n",
      "Iteration 539, loss = 0.30860075\n",
      "Iteration 540, loss = 0.30828364\n",
      "Iteration 541, loss = 0.30796694\n",
      "Iteration 542, loss = 0.30765067\n",
      "Iteration 543, loss = 0.30733491\n",
      "Iteration 544, loss = 0.30701947\n",
      "Iteration 545, loss = 0.30670450\n",
      "Iteration 546, loss = 0.30638994\n",
      "Iteration 547, loss = 0.30607582\n",
      "Iteration 548, loss = 0.30576211\n",
      "Iteration 549, loss = 0.30544882\n",
      "Iteration 550, loss = 0.30513624\n",
      "Iteration 551, loss = 0.30482421\n",
      "Iteration 552, loss = 0.30451258\n",
      "Iteration 553, loss = 0.30420136\n",
      "Iteration 554, loss = 0.30389057\n",
      "Iteration 555, loss = 0.30358020\n",
      "Iteration 556, loss = 0.30327025\n",
      "Iteration 557, loss = 0.30296072\n",
      "Iteration 558, loss = 0.30265171\n",
      "Iteration 559, loss = 0.30234302\n",
      "Iteration 560, loss = 0.30203481\n",
      "Iteration 561, loss = 0.30172702\n",
      "Iteration 562, loss = 0.30141965\n",
      "Iteration 563, loss = 0.30111271\n",
      "Iteration 564, loss = 0.30080620\n",
      "Iteration 565, loss = 0.30050022\n",
      "Iteration 566, loss = 0.30019454\n",
      "Iteration 567, loss = 0.29988934\n",
      "Iteration 568, loss = 0.29958457\n",
      "Iteration 569, loss = 0.29928022\n",
      "Iteration 570, loss = 0.29897629\n",
      "Iteration 571, loss = 0.29867281\n",
      "Iteration 572, loss = 0.29836981\n",
      "Iteration 573, loss = 0.29806720\n",
      "Iteration 574, loss = 0.29776501\n",
      "Iteration 575, loss = 0.29746324\n",
      "Iteration 576, loss = 0.29716190\n",
      "Iteration 577, loss = 0.29686098\n",
      "Iteration 578, loss = 0.29656060\n",
      "Iteration 579, loss = 0.29626050\n",
      "Iteration 580, loss = 0.29596089\n",
      "Iteration 581, loss = 0.29566170\n",
      "Iteration 582, loss = 0.29536293\n",
      "Iteration 583, loss = 0.29506467\n",
      "Iteration 584, loss = 0.29476676\n",
      "Iteration 585, loss = 0.29446929\n",
      "Iteration 586, loss = 0.29417224\n",
      "Iteration 587, loss = 0.29387562\n",
      "Iteration 588, loss = 0.29357941\n",
      "Iteration 589, loss = 0.29328377\n",
      "Iteration 590, loss = 0.29298839\n",
      "Iteration 591, loss = 0.29269348\n",
      "Iteration 592, loss = 0.29239901\n",
      "Iteration 593, loss = 0.29210496\n",
      "Iteration 594, loss = 0.29181144\n",
      "Iteration 595, loss = 0.29151821\n",
      "Iteration 596, loss = 0.29122545\n",
      "Iteration 597, loss = 0.29093314\n",
      "Iteration 598, loss = 0.29064120\n",
      "Iteration 599, loss = 0.29034978\n",
      "Iteration 600, loss = 0.29005874\n",
      "Iteration 601, loss = 0.28976813\n",
      "Iteration 602, loss = 0.28947793\n",
      "Iteration 603, loss = 0.28918816\n",
      "Iteration 604, loss = 0.28889885\n",
      "Iteration 605, loss = 0.28860996\n",
      "Iteration 606, loss = 0.28832149\n",
      "Iteration 607, loss = 0.28803342\n",
      "Iteration 608, loss = 0.28774578\n",
      "Iteration 609, loss = 0.28745860\n",
      "Iteration 610, loss = 0.28717182\n",
      "Iteration 611, loss = 0.28688548\n",
      "Iteration 612, loss = 0.28659952\n",
      "Iteration 613, loss = 0.28631401\n",
      "Iteration 614, loss = 0.28602900\n",
      "Iteration 615, loss = 0.28574430\n",
      "Iteration 616, loss = 0.28546008\n",
      "Iteration 617, loss = 0.28517625\n",
      "Iteration 618, loss = 0.28489285\n",
      "Iteration 619, loss = 0.28460994\n",
      "Iteration 620, loss = 0.28432742\n",
      "Iteration 621, loss = 0.28404528\n",
      "Iteration 622, loss = 0.28376358\n",
      "Iteration 623, loss = 0.28348227\n",
      "Iteration 624, loss = 0.28320155\n",
      "Iteration 625, loss = 0.28292101\n",
      "Iteration 626, loss = 0.28264100\n",
      "Iteration 627, loss = 0.28236143\n",
      "Iteration 628, loss = 0.28208232\n",
      "Iteration 629, loss = 0.28180358\n",
      "Iteration 630, loss = 0.28152526\n",
      "Iteration 631, loss = 0.28124734\n",
      "Iteration 632, loss = 0.28096983\n",
      "Iteration 633, loss = 0.28069287\n",
      "Iteration 634, loss = 0.28041617\n",
      "Iteration 635, loss = 0.28013993\n",
      "Iteration 636, loss = 0.27986415\n",
      "Iteration 637, loss = 0.27958882\n",
      "Iteration 638, loss = 0.27931386\n",
      "Iteration 639, loss = 0.27903932\n",
      "Iteration 640, loss = 0.27876517\n",
      "Iteration 641, loss = 0.27849146\n",
      "Iteration 642, loss = 0.27821829\n",
      "Iteration 643, loss = 0.27794533\n",
      "Iteration 644, loss = 0.27767287\n",
      "Iteration 645, loss = 0.27740089\n",
      "Iteration 646, loss = 0.27712930\n",
      "Iteration 647, loss = 0.27685810\n",
      "Iteration 648, loss = 0.27658732\n",
      "Iteration 649, loss = 0.27631694\n",
      "Iteration 650, loss = 0.27604703\n",
      "Iteration 651, loss = 0.27577752\n",
      "Iteration 652, loss = 0.27550839\n",
      "Iteration 653, loss = 0.27523970\n",
      "Iteration 654, loss = 0.27497145\n",
      "Iteration 655, loss = 0.27470360\n",
      "Iteration 656, loss = 0.27443615\n",
      "Iteration 657, loss = 0.27416911\n",
      "Iteration 658, loss = 0.27390248\n",
      "Iteration 659, loss = 0.27363635\n",
      "Iteration 660, loss = 0.27337056\n",
      "Iteration 661, loss = 0.27310517\n",
      "Iteration 662, loss = 0.27284020\n",
      "Iteration 663, loss = 0.27257575\n",
      "Iteration 664, loss = 0.27231155\n",
      "Iteration 665, loss = 0.27204783\n",
      "Iteration 666, loss = 0.27178455\n",
      "Iteration 667, loss = 0.27152170\n",
      "Iteration 668, loss = 0.27125923\n",
      "Iteration 669, loss = 0.27099716\n",
      "Iteration 670, loss = 0.27073548\n",
      "Iteration 671, loss = 0.27047430\n",
      "Iteration 672, loss = 0.27021345\n",
      "Iteration 673, loss = 0.26995302\n",
      "Iteration 674, loss = 0.26969300\n",
      "Iteration 675, loss = 0.26943348\n",
      "Iteration 676, loss = 0.26917431\n",
      "Iteration 677, loss = 0.26891552\n",
      "Iteration 678, loss = 0.26865713\n",
      "Iteration 679, loss = 0.26839923\n",
      "Iteration 680, loss = 0.26814166\n",
      "Iteration 681, loss = 0.26788452\n",
      "Iteration 682, loss = 0.26762779\n",
      "Iteration 683, loss = 0.26737154\n",
      "Iteration 684, loss = 0.26711563\n",
      "Iteration 685, loss = 0.26686012\n",
      "Iteration 686, loss = 0.26660499\n",
      "Iteration 687, loss = 0.26635041\n",
      "Iteration 688, loss = 0.26609606\n",
      "Iteration 689, loss = 0.26584218\n",
      "Iteration 690, loss = 0.26558879\n",
      "Iteration 691, loss = 0.26533571\n",
      "Iteration 692, loss = 0.26508306\n",
      "Iteration 693, loss = 0.26483080\n",
      "Iteration 694, loss = 0.26457900\n",
      "Iteration 695, loss = 0.26432757\n",
      "Iteration 696, loss = 0.26407654\n",
      "Iteration 697, loss = 0.26382590\n",
      "Iteration 698, loss = 0.26357575\n",
      "Iteration 699, loss = 0.26332590\n",
      "Iteration 700, loss = 0.26307647\n",
      "Iteration 701, loss = 0.26282747\n",
      "Iteration 702, loss = 0.26257892\n",
      "Iteration 703, loss = 0.26233072\n",
      "Iteration 704, loss = 0.26208290\n",
      "Iteration 705, loss = 0.26183548\n",
      "Iteration 706, loss = 0.26158854\n",
      "Iteration 707, loss = 0.26134195\n",
      "Iteration 708, loss = 0.26109573\n",
      "Iteration 709, loss = 0.26084999\n",
      "Iteration 710, loss = 0.26060457\n",
      "Iteration 711, loss = 0.26035957\n",
      "Iteration 712, loss = 0.26011500\n",
      "Iteration 713, loss = 0.25987084\n",
      "Iteration 714, loss = 0.25962704\n",
      "Iteration 715, loss = 0.25938362\n",
      "Iteration 716, loss = 0.25914067\n",
      "Iteration 717, loss = 0.25889807\n",
      "Iteration 718, loss = 0.25865585\n",
      "Iteration 719, loss = 0.25841406\n",
      "Iteration 720, loss = 0.25817270\n",
      "Iteration 721, loss = 0.25793169\n",
      "Iteration 722, loss = 0.25769105\n",
      "Iteration 723, loss = 0.25745087\n",
      "Iteration 724, loss = 0.25721105\n",
      "Iteration 725, loss = 0.25697161\n",
      "Iteration 726, loss = 0.25673260\n",
      "Iteration 727, loss = 0.25649399\n",
      "Iteration 728, loss = 0.25625575\n",
      "Iteration 729, loss = 0.25601788\n",
      "Iteration 730, loss = 0.25578050\n",
      "Iteration 731, loss = 0.25554339\n",
      "Iteration 732, loss = 0.25530671\n",
      "Iteration 733, loss = 0.25507052\n",
      "Iteration 734, loss = 0.25483460\n",
      "Iteration 735, loss = 0.25459910\n",
      "Iteration 736, loss = 0.25436402\n",
      "Iteration 737, loss = 0.25412934\n",
      "Iteration 738, loss = 0.25389502\n",
      "Iteration 739, loss = 0.25366107\n",
      "Iteration 740, loss = 0.25342761\n",
      "Iteration 741, loss = 0.25319441\n",
      "Iteration 742, loss = 0.25296164\n",
      "Iteration 743, loss = 0.25272935\n",
      "Iteration 744, loss = 0.25249737\n",
      "Iteration 745, loss = 0.25226576\n",
      "Iteration 746, loss = 0.25203451\n",
      "Iteration 747, loss = 0.25180383\n",
      "Iteration 748, loss = 0.25157330\n",
      "Iteration 749, loss = 0.25134332\n",
      "Iteration 750, loss = 0.25111367\n",
      "Iteration 751, loss = 0.25088438\n",
      "Iteration 752, loss = 0.25065546\n",
      "Iteration 753, loss = 0.25042702\n",
      "Iteration 754, loss = 0.25019885\n",
      "Iteration 755, loss = 0.24997113\n",
      "Iteration 756, loss = 0.24974382\n",
      "Iteration 757, loss = 0.24951684\n",
      "Iteration 758, loss = 0.24929023\n",
      "Iteration 759, loss = 0.24906410\n",
      "Iteration 760, loss = 0.24883823\n",
      "Iteration 761, loss = 0.24861279\n",
      "Iteration 762, loss = 0.24838780\n",
      "Iteration 763, loss = 0.24816311\n",
      "Iteration 764, loss = 0.24793879\n",
      "Iteration 765, loss = 0.24771491\n",
      "Iteration 766, loss = 0.24749136\n",
      "Iteration 767, loss = 0.24726817\n",
      "Iteration 768, loss = 0.24704549\n",
      "Iteration 769, loss = 0.24682302\n",
      "Iteration 770, loss = 0.24660101\n",
      "Iteration 771, loss = 0.24637942\n",
      "Iteration 772, loss = 0.24615814\n",
      "Iteration 773, loss = 0.24593722\n",
      "Iteration 774, loss = 0.24571676\n",
      "Iteration 775, loss = 0.24549660\n",
      "Iteration 776, loss = 0.24527684\n",
      "Iteration 777, loss = 0.24505753\n",
      "Iteration 778, loss = 0.24483851\n",
      "Iteration 779, loss = 0.24461985\n",
      "Iteration 780, loss = 0.24440167\n",
      "Iteration 781, loss = 0.24418372\n",
      "Iteration 782, loss = 0.24396625\n",
      "Iteration 783, loss = 0.24374912\n",
      "Iteration 784, loss = 0.24353234\n",
      "Iteration 785, loss = 0.24331595\n",
      "Iteration 786, loss = 0.24309997\n",
      "Iteration 787, loss = 0.24288430\n",
      "Iteration 788, loss = 0.24266902\n",
      "Iteration 789, loss = 0.24245413\n",
      "Iteration 790, loss = 0.24223957\n",
      "Iteration 791, loss = 0.24202543\n",
      "Iteration 792, loss = 0.24181161\n",
      "Iteration 793, loss = 0.24159815\n",
      "Iteration 794, loss = 0.24138517\n",
      "Iteration 795, loss = 0.24117240\n",
      "Iteration 796, loss = 0.24096009\n",
      "Iteration 797, loss = 0.24074814\n",
      "Iteration 798, loss = 0.24053652\n",
      "Iteration 799, loss = 0.24032526\n",
      "Iteration 800, loss = 0.24011444\n",
      "Iteration 801, loss = 0.23990391\n",
      "Iteration 802, loss = 0.23969376\n",
      "Iteration 803, loss = 0.23948400\n",
      "Iteration 804, loss = 0.23927455\n",
      "Iteration 805, loss = 0.23906555\n",
      "Iteration 806, loss = 0.23885680\n",
      "Iteration 807, loss = 0.23864849\n",
      "Iteration 808, loss = 0.23844053\n",
      "Iteration 809, loss = 0.23823290\n",
      "Iteration 810, loss = 0.23802565\n",
      "Iteration 811, loss = 0.23781877\n",
      "Iteration 812, loss = 0.23761220\n",
      "Iteration 813, loss = 0.23740609\n",
      "Iteration 814, loss = 0.23720021\n",
      "Iteration 815, loss = 0.23699478\n",
      "Iteration 816, loss = 0.23678968\n",
      "Iteration 817, loss = 0.23658491\n",
      "Iteration 818, loss = 0.23638055\n",
      "Iteration 819, loss = 0.23617650\n",
      "Iteration 820, loss = 0.23597279\n",
      "Iteration 821, loss = 0.23576953\n",
      "Iteration 822, loss = 0.23556654\n",
      "Iteration 823, loss = 0.23536388\n",
      "Iteration 824, loss = 0.23516167\n",
      "Iteration 825, loss = 0.23495972\n",
      "Iteration 826, loss = 0.23475820\n",
      "Iteration 827, loss = 0.23455695\n",
      "Iteration 828, loss = 0.23435612\n",
      "Iteration 829, loss = 0.23415561\n",
      "Iteration 830, loss = 0.23395542\n",
      "Iteration 831, loss = 0.23375568\n",
      "Iteration 832, loss = 0.23355616\n",
      "Iteration 833, loss = 0.23335710\n",
      "Iteration 834, loss = 0.23315832\n",
      "Iteration 835, loss = 0.23295987\n",
      "Iteration 836, loss = 0.23276188\n",
      "Iteration 837, loss = 0.23256409\n",
      "Iteration 838, loss = 0.23236677\n",
      "Iteration 839, loss = 0.23216972\n",
      "Iteration 840, loss = 0.23197301\n",
      "Iteration 841, loss = 0.23177675\n",
      "Iteration 842, loss = 0.23158073\n",
      "Iteration 843, loss = 0.23138504\n",
      "Iteration 844, loss = 0.23118979\n",
      "Iteration 845, loss = 0.23099478\n",
      "Iteration 846, loss = 0.23080022\n",
      "Iteration 847, loss = 0.23060588\n",
      "Iteration 848, loss = 0.23041201\n",
      "Iteration 849, loss = 0.23021839\n",
      "Iteration 850, loss = 0.23002509\n",
      "Iteration 851, loss = 0.22983226\n",
      "Iteration 852, loss = 0.22963963\n",
      "Iteration 853, loss = 0.22944741\n",
      "Iteration 854, loss = 0.22925548\n",
      "Iteration 855, loss = 0.22906393\n",
      "Iteration 856, loss = 0.22887270\n",
      "Iteration 857, loss = 0.22868180\n",
      "Iteration 858, loss = 0.22849129\n",
      "Iteration 859, loss = 0.22830104\n",
      "Iteration 860, loss = 0.22811120\n",
      "Iteration 861, loss = 0.22792160\n",
      "Iteration 862, loss = 0.22773244\n",
      "Iteration 863, loss = 0.22754352\n",
      "Iteration 864, loss = 0.22735500\n",
      "Iteration 865, loss = 0.22716679\n",
      "Iteration 866, loss = 0.22697888\n",
      "Iteration 867, loss = 0.22679141\n",
      "Iteration 868, loss = 0.22660413\n",
      "Iteration 869, loss = 0.22641730\n",
      "Iteration 870, loss = 0.22623072\n",
      "Iteration 871, loss = 0.22604446\n",
      "Iteration 872, loss = 0.22585861\n",
      "Iteration 873, loss = 0.22567299\n",
      "Iteration 874, loss = 0.22548784\n",
      "Iteration 875, loss = 0.22530288\n",
      "Iteration 876, loss = 0.22511830\n",
      "Iteration 877, loss = 0.22493401\n",
      "Iteration 878, loss = 0.22475013\n",
      "Iteration 879, loss = 0.22456646\n",
      "Iteration 880, loss = 0.22438324\n",
      "Iteration 881, loss = 0.22420023\n",
      "Iteration 882, loss = 0.22401764\n",
      "Iteration 883, loss = 0.22383531\n",
      "Iteration 884, loss = 0.22365332\n",
      "Iteration 885, loss = 0.22347168\n",
      "Iteration 886, loss = 0.22329030\n",
      "Iteration 887, loss = 0.22310935\n",
      "Iteration 888, loss = 0.22292861\n",
      "Iteration 889, loss = 0.22274826\n",
      "Iteration 890, loss = 0.22256817\n",
      "Iteration 891, loss = 0.22238846\n",
      "Iteration 892, loss = 0.22220900\n",
      "Iteration 893, loss = 0.22202995\n",
      "Iteration 894, loss = 0.22185111\n",
      "Iteration 895, loss = 0.22167270\n",
      "Iteration 896, loss = 0.22149450\n",
      "Iteration 897, loss = 0.22131671\n",
      "Iteration 898, loss = 0.22113916\n",
      "Iteration 899, loss = 0.22096198\n",
      "Iteration 900, loss = 0.22078509\n",
      "Iteration 901, loss = 0.22060850\n",
      "Iteration 902, loss = 0.22043228\n",
      "Iteration 903, loss = 0.22025629\n",
      "Iteration 904, loss = 0.22008071\n",
      "Iteration 905, loss = 0.21990535\n",
      "Iteration 906, loss = 0.21973039\n",
      "Iteration 907, loss = 0.21955566\n",
      "Iteration 908, loss = 0.21938128\n",
      "Iteration 909, loss = 0.21920717\n",
      "Iteration 910, loss = 0.21903346\n",
      "Iteration 911, loss = 0.21885996\n",
      "Iteration 912, loss = 0.21868686\n",
      "Iteration 913, loss = 0.21851398\n",
      "Iteration 914, loss = 0.21834147\n",
      "Iteration 915, loss = 0.21816921\n",
      "Iteration 916, loss = 0.21799735\n",
      "Iteration 917, loss = 0.21782570\n",
      "Iteration 918, loss = 0.21765440\n",
      "Iteration 919, loss = 0.21748336\n",
      "Iteration 920, loss = 0.21731273\n",
      "Iteration 921, loss = 0.21714229\n",
      "Iteration 922, loss = 0.21697224\n",
      "Iteration 923, loss = 0.21680242\n",
      "Iteration 924, loss = 0.21663294\n",
      "Iteration 925, loss = 0.21646373\n",
      "Iteration 926, loss = 0.21629490\n",
      "Iteration 927, loss = 0.21612627\n",
      "Iteration 928, loss = 0.21595806\n",
      "Iteration 929, loss = 0.21579005\n",
      "Iteration 930, loss = 0.21562233\n",
      "Iteration 931, loss = 0.21545497\n",
      "Iteration 932, loss = 0.21528787\n",
      "Iteration 933, loss = 0.21512110\n",
      "Iteration 934, loss = 0.21495460\n",
      "Iteration 935, loss = 0.21478842\n",
      "Iteration 936, loss = 0.21462250\n",
      "Iteration 937, loss = 0.21445694\n",
      "Iteration 938, loss = 0.21429159\n",
      "Iteration 939, loss = 0.21412665\n",
      "Iteration 940, loss = 0.21396189\n",
      "Iteration 941, loss = 0.21379748\n",
      "Iteration 942, loss = 0.21363332\n",
      "Iteration 943, loss = 0.21346952\n",
      "Iteration 944, loss = 0.21330593\n",
      "Iteration 945, loss = 0.21314274\n",
      "Iteration 946, loss = 0.21297976\n",
      "Iteration 947, loss = 0.21281705\n",
      "Iteration 948, loss = 0.21265470\n",
      "Iteration 949, loss = 0.21249257\n",
      "Iteration 950, loss = 0.21233082\n",
      "Iteration 951, loss = 0.21216927\n",
      "Iteration 952, loss = 0.21200809\n",
      "Iteration 953, loss = 0.21184711\n",
      "Iteration 954, loss = 0.21168649\n",
      "Iteration 955, loss = 0.21152610\n",
      "Iteration 956, loss = 0.21136602\n",
      "Iteration 957, loss = 0.21120622\n",
      "Iteration 958, loss = 0.21104671\n",
      "Iteration 959, loss = 0.21088750\n",
      "Iteration 960, loss = 0.21072853\n",
      "Iteration 961, loss = 0.21056992\n",
      "Iteration 962, loss = 0.21041152\n",
      "Iteration 963, loss = 0.21025344\n",
      "Iteration 964, loss = 0.21009561\n",
      "Iteration 965, loss = 0.20993810\n",
      "Iteration 966, loss = 0.20978084\n",
      "Iteration 967, loss = 0.20962387\n",
      "Iteration 968, loss = 0.20946721\n",
      "Iteration 969, loss = 0.20931077\n",
      "Iteration 970, loss = 0.20915468\n",
      "Iteration 971, loss = 0.20899879\n",
      "Iteration 972, loss = 0.20884325\n",
      "Iteration 973, loss = 0.20868800\n",
      "Iteration 974, loss = 0.20853301\n",
      "Iteration 975, loss = 0.20837829\n",
      "Iteration 976, loss = 0.20822380\n",
      "Iteration 977, loss = 0.20806962\n",
      "Iteration 978, loss = 0.20791573\n",
      "Iteration 979, loss = 0.20776210\n",
      "Iteration 980, loss = 0.20760878\n",
      "Iteration 981, loss = 0.20745568\n",
      "Iteration 982, loss = 0.20730288\n",
      "Iteration 983, loss = 0.20715038\n",
      "Iteration 984, loss = 0.20699811\n",
      "Iteration 985, loss = 0.20684611\n",
      "Iteration 986, loss = 0.20669443\n",
      "Iteration 987, loss = 0.20654296\n",
      "Iteration 988, loss = 0.20639179\n",
      "Iteration 989, loss = 0.20624089\n",
      "Iteration 990, loss = 0.20609028\n",
      "Iteration 991, loss = 0.20593990\n",
      "Iteration 992, loss = 0.20578978\n",
      "Iteration 993, loss = 0.20564000\n",
      "Iteration 994, loss = 0.20549040\n",
      "Iteration 995, loss = 0.20534109\n",
      "Iteration 996, loss = 0.20519212\n",
      "Iteration 997, loss = 0.20504333\n",
      "Iteration 998, loss = 0.20489483\n",
      "Iteration 999, loss = 0.20474657\n",
      "Iteration 1000, loss = 0.20459866\n",
      "Iteration 1, loss = 1.35303276\n",
      "Iteration 2, loss = 1.31599158\n",
      "Iteration 3, loss = 1.26612351\n",
      "Iteration 4, loss = 1.20761350\n",
      "Iteration 5, loss = 1.14474263\n",
      "Iteration 6, loss = 1.08131907\n",
      "Iteration 7, loss = 1.02058590\n",
      "Iteration 8, loss = 0.96516032\n",
      "Iteration 9, loss = 0.91667007\n",
      "Iteration 10, loss = 0.87589729\n",
      "Iteration 11, loss = 0.84277230\n",
      "Iteration 12, loss = 0.81652024\n",
      "Iteration 13, loss = 0.79613708\n",
      "Iteration 14, loss = 0.78035390\n",
      "Iteration 15, loss = 0.76810262\n",
      "Iteration 16, loss = 0.75848384\n",
      "Iteration 17, loss = 0.75070554\n",
      "Iteration 18, loss = 0.74431720\n",
      "Iteration 19, loss = 0.73898611\n",
      "Iteration 20, loss = 0.73448886\n",
      "Iteration 21, loss = 0.73072165\n",
      "Iteration 22, loss = 0.72714395\n",
      "Iteration 23, loss = 0.72322114\n",
      "Iteration 24, loss = 0.71900412\n",
      "Iteration 25, loss = 0.71442113\n",
      "Iteration 26, loss = 0.70934791\n",
      "Iteration 27, loss = 0.70393448\n",
      "Iteration 28, loss = 0.69835204\n",
      "Iteration 29, loss = 0.69264527\n",
      "Iteration 30, loss = 0.68700039\n",
      "Iteration 31, loss = 0.68153306\n",
      "Iteration 32, loss = 0.67635869\n",
      "Iteration 33, loss = 0.67151066\n",
      "Iteration 34, loss = 0.66702492\n",
      "Iteration 35, loss = 0.66287852\n",
      "Iteration 36, loss = 0.65907178\n",
      "Iteration 37, loss = 0.65556385\n",
      "Iteration 38, loss = 0.65233588\n",
      "Iteration 39, loss = 0.64934261\n",
      "Iteration 40, loss = 0.64653006\n",
      "Iteration 41, loss = 0.64385280\n",
      "Iteration 42, loss = 0.64127591\n",
      "Iteration 43, loss = 0.63878289\n",
      "Iteration 44, loss = 0.63633267\n",
      "Iteration 45, loss = 0.63390494\n",
      "Iteration 46, loss = 0.63149261\n",
      "Iteration 47, loss = 0.62907871\n",
      "Iteration 48, loss = 0.62665940\n",
      "Iteration 49, loss = 0.62423486\n",
      "Iteration 50, loss = 0.62180906\n",
      "Iteration 51, loss = 0.61938721\n",
      "Iteration 52, loss = 0.61697315\n",
      "Iteration 53, loss = 0.61457351\n",
      "Iteration 54, loss = 0.61219467\n",
      "Iteration 55, loss = 0.60984237\n",
      "Iteration 56, loss = 0.60752143\n",
      "Iteration 57, loss = 0.60523560\n",
      "Iteration 58, loss = 0.60298746\n",
      "Iteration 59, loss = 0.60077848\n",
      "Iteration 60, loss = 0.59860915\n",
      "Iteration 61, loss = 0.59647999\n",
      "Iteration 62, loss = 0.59439563\n",
      "Iteration 63, loss = 0.59234853\n",
      "Iteration 64, loss = 0.59033670\n",
      "Iteration 65, loss = 0.58835799\n",
      "Iteration 66, loss = 0.58641021\n",
      "Iteration 67, loss = 0.58449124\n",
      "Iteration 68, loss = 0.58259910\n",
      "Iteration 69, loss = 0.58073202\n",
      "Iteration 70, loss = 0.57888849\n",
      "Iteration 71, loss = 0.57707084\n",
      "Iteration 72, loss = 0.57527733\n",
      "Iteration 73, loss = 0.57350441\n",
      "Iteration 74, loss = 0.57175499\n",
      "Iteration 75, loss = 0.57002883\n",
      "Iteration 76, loss = 0.56832404\n",
      "Iteration 77, loss = 0.56664029\n",
      "Iteration 78, loss = 0.56497615\n",
      "Iteration 79, loss = 0.56333132\n",
      "Iteration 80, loss = 0.56170557\n",
      "Iteration 81, loss = 0.56009866\n",
      "Iteration 82, loss = 0.55851033\n",
      "Iteration 83, loss = 0.55694030\n",
      "Iteration 84, loss = 0.55538848\n",
      "Iteration 85, loss = 0.55385701\n",
      "Iteration 86, loss = 0.55234427\n",
      "Iteration 87, loss = 0.55084862\n",
      "Iteration 88, loss = 0.54937128\n",
      "Iteration 89, loss = 0.54791146\n",
      "Iteration 90, loss = 0.54646893\n",
      "Iteration 91, loss = 0.54504114\n",
      "Iteration 92, loss = 0.54362750\n",
      "Iteration 93, loss = 0.54222820\n",
      "Iteration 94, loss = 0.54084606\n",
      "Iteration 95, loss = 0.53947868\n",
      "Iteration 96, loss = 0.53812509\n",
      "Iteration 97, loss = 0.53678523\n",
      "Iteration 98, loss = 0.53545927\n",
      "Iteration 99, loss = 0.53414624\n",
      "Iteration 100, loss = 0.53284582\n",
      "Iteration 101, loss = 0.53155773\n",
      "Iteration 102, loss = 0.53028172\n",
      "Iteration 103, loss = 0.52901753\n",
      "Iteration 104, loss = 0.52776496\n",
      "Iteration 105, loss = 0.52652376\n",
      "Iteration 106, loss = 0.52529386\n",
      "Iteration 107, loss = 0.52407481\n",
      "Iteration 108, loss = 0.52286850\n",
      "Iteration 109, loss = 0.52167435\n",
      "Iteration 110, loss = 0.52049090\n",
      "Iteration 111, loss = 0.51931952\n",
      "Iteration 112, loss = 0.51816259\n",
      "Iteration 113, loss = 0.51701597\n",
      "Iteration 114, loss = 0.51587959\n",
      "Iteration 115, loss = 0.51475309\n",
      "Iteration 116, loss = 0.51363658\n",
      "Iteration 117, loss = 0.51253073\n",
      "Iteration 118, loss = 0.51143500\n",
      "Iteration 119, loss = 0.51035078\n",
      "Iteration 120, loss = 0.50927611\n",
      "Iteration 121, loss = 0.50821087\n",
      "Iteration 122, loss = 0.50715401\n",
      "Iteration 123, loss = 0.50610569\n",
      "Iteration 124, loss = 0.50506596\n",
      "Iteration 125, loss = 0.50403466\n",
      "Iteration 126, loss = 0.50301179\n",
      "Iteration 127, loss = 0.50199775\n",
      "Iteration 128, loss = 0.50099233\n",
      "Iteration 129, loss = 0.49999518\n",
      "Iteration 130, loss = 0.49900623\n",
      "Iteration 131, loss = 0.49802527\n",
      "Iteration 132, loss = 0.49705216\n",
      "Iteration 133, loss = 0.49608680\n",
      "Iteration 134, loss = 0.49512907\n",
      "Iteration 135, loss = 0.49417874\n",
      "Iteration 136, loss = 0.49323570\n",
      "Iteration 137, loss = 0.49229990\n",
      "Iteration 138, loss = 0.49137111\n",
      "Iteration 139, loss = 0.49044923\n",
      "Iteration 140, loss = 0.48953413\n",
      "Iteration 141, loss = 0.48862577\n",
      "Iteration 142, loss = 0.48772386\n",
      "Iteration 143, loss = 0.48682840\n",
      "Iteration 144, loss = 0.48593924\n",
      "Iteration 145, loss = 0.48505626\n",
      "Iteration 146, loss = 0.48417937\n",
      "Iteration 147, loss = 0.48330903\n",
      "Iteration 148, loss = 0.48244463\n",
      "Iteration 149, loss = 0.48158617\n",
      "Iteration 150, loss = 0.48073347\n",
      "Iteration 151, loss = 0.47988645\n",
      "Iteration 152, loss = 0.47904501\n",
      "Iteration 153, loss = 0.47820907\n",
      "Iteration 154, loss = 0.47737853\n",
      "Iteration 155, loss = 0.47655330\n",
      "Iteration 156, loss = 0.47573329\n",
      "Iteration 157, loss = 0.47491842\n",
      "Iteration 158, loss = 0.47410859\n",
      "Iteration 159, loss = 0.47330372\n",
      "Iteration 160, loss = 0.47250372\n",
      "Iteration 161, loss = 0.47170850\n",
      "Iteration 162, loss = 0.47091799\n",
      "Iteration 163, loss = 0.47013209\n",
      "Iteration 164, loss = 0.46935073\n",
      "Iteration 165, loss = 0.46857383\n",
      "Iteration 166, loss = 0.46780131\n",
      "Iteration 167, loss = 0.46703309\n",
      "Iteration 168, loss = 0.46626910\n",
      "Iteration 169, loss = 0.46550926\n",
      "Iteration 170, loss = 0.46475350\n",
      "Iteration 171, loss = 0.46400175\n",
      "Iteration 172, loss = 0.46325394\n",
      "Iteration 173, loss = 0.46251001\n",
      "Iteration 174, loss = 0.46176988\n",
      "Iteration 175, loss = 0.46103348\n",
      "Iteration 176, loss = 0.46030077\n",
      "Iteration 177, loss = 0.45957166\n",
      "Iteration 178, loss = 0.45884611\n",
      "Iteration 179, loss = 0.45812405\n",
      "Iteration 180, loss = 0.45740541\n",
      "Iteration 181, loss = 0.45669016\n",
      "Iteration 182, loss = 0.45597822\n",
      "Iteration 183, loss = 0.45526954\n",
      "Iteration 184, loss = 0.45456408\n",
      "Iteration 185, loss = 0.45386176\n",
      "Iteration 186, loss = 0.45316255\n",
      "Iteration 187, loss = 0.45246640\n",
      "Iteration 188, loss = 0.45177324\n",
      "Iteration 189, loss = 0.45108304\n",
      "Iteration 190, loss = 0.45039689\n",
      "Iteration 191, loss = 0.44971439\n",
      "Iteration 192, loss = 0.44903485\n",
      "Iteration 193, loss = 0.44835822\n",
      "Iteration 194, loss = 0.44768446\n",
      "Iteration 195, loss = 0.44701351\n",
      "Iteration 196, loss = 0.44634534\n",
      "Iteration 197, loss = 0.44568007\n",
      "Iteration 198, loss = 0.44501893\n",
      "Iteration 199, loss = 0.44436052\n",
      "Iteration 200, loss = 0.44370479\n",
      "Iteration 201, loss = 0.44305172\n",
      "Iteration 202, loss = 0.44240127\n",
      "Iteration 203, loss = 0.44175340\n",
      "Iteration 204, loss = 0.44110914\n",
      "Iteration 205, loss = 0.44046766\n",
      "Iteration 206, loss = 0.43982869\n",
      "Iteration 207, loss = 0.43919222\n",
      "Iteration 208, loss = 0.43855897\n",
      "Iteration 209, loss = 0.43792851\n",
      "Iteration 210, loss = 0.43730044\n",
      "Iteration 211, loss = 0.43667474\n",
      "Iteration 212, loss = 0.43605141\n",
      "Iteration 213, loss = 0.43543040\n",
      "Iteration 214, loss = 0.43481171\n",
      "Iteration 215, loss = 0.43419530\n",
      "Iteration 216, loss = 0.43358114\n",
      "Iteration 217, loss = 0.43296921\n",
      "Iteration 218, loss = 0.43235996\n",
      "Iteration 219, loss = 0.43175271\n",
      "Iteration 220, loss = 0.43114744\n",
      "Iteration 221, loss = 0.43054416\n",
      "Iteration 222, loss = 0.42994293\n",
      "Iteration 223, loss = 0.42934483\n",
      "Iteration 224, loss = 0.42874900\n",
      "Iteration 225, loss = 0.42815571\n",
      "Iteration 226, loss = 0.42756433\n",
      "Iteration 227, loss = 0.42697489\n",
      "Iteration 228, loss = 0.42638750\n",
      "Iteration 229, loss = 0.42580207\n",
      "Iteration 230, loss = 0.42521867\n",
      "Iteration 231, loss = 0.42463717\n",
      "Iteration 232, loss = 0.42405764\n",
      "Iteration 233, loss = 0.42348055\n",
      "Iteration 234, loss = 0.42290503\n",
      "Iteration 235, loss = 0.42233164\n",
      "Iteration 236, loss = 0.42176015\n",
      "Iteration 237, loss = 0.42119055\n",
      "Iteration 238, loss = 0.42062280\n",
      "Iteration 239, loss = 0.42005689\n",
      "Iteration 240, loss = 0.41949280\n",
      "Iteration 241, loss = 0.41893050\n",
      "Iteration 242, loss = 0.41836996\n",
      "Iteration 243, loss = 0.41781117\n",
      "Iteration 244, loss = 0.41725410\n",
      "Iteration 245, loss = 0.41669872\n",
      "Iteration 246, loss = 0.41614501\n",
      "Iteration 247, loss = 0.41559294\n",
      "Iteration 248, loss = 0.41504250\n",
      "Iteration 249, loss = 0.41449366\n",
      "Iteration 250, loss = 0.41394640\n",
      "Iteration 251, loss = 0.41340069\n",
      "Iteration 252, loss = 0.41285652\n",
      "Iteration 253, loss = 0.41231386\n",
      "Iteration 254, loss = 0.41177270\n",
      "Iteration 255, loss = 0.41123300\n",
      "Iteration 256, loss = 0.41069476\n",
      "Iteration 257, loss = 0.41015795\n",
      "Iteration 258, loss = 0.40962256\n",
      "Iteration 259, loss = 0.40908864\n",
      "Iteration 260, loss = 0.40855598\n",
      "Iteration 261, loss = 0.40802474\n",
      "Iteration 262, loss = 0.40749486\n",
      "Iteration 263, loss = 0.40696634\n",
      "Iteration 264, loss = 0.40643910\n",
      "Iteration 265, loss = 0.40591316\n",
      "Iteration 266, loss = 0.40538848\n",
      "Iteration 267, loss = 0.40486507\n",
      "Iteration 268, loss = 0.40434296\n",
      "Iteration 269, loss = 0.40382204\n",
      "Iteration 270, loss = 0.40330236\n",
      "Iteration 271, loss = 0.40278391\n",
      "Iteration 272, loss = 0.40226660\n",
      "Iteration 273, loss = 0.40175054\n",
      "Iteration 274, loss = 0.40123557\n",
      "Iteration 275, loss = 0.40072181\n",
      "Iteration 276, loss = 0.40020918\n",
      "Iteration 277, loss = 0.39969763\n",
      "Iteration 278, loss = 0.39918731\n",
      "Iteration 279, loss = 0.39867796\n",
      "Iteration 280, loss = 0.39816979\n",
      "Iteration 281, loss = 0.39766263\n",
      "Iteration 282, loss = 0.39715660\n",
      "Iteration 283, loss = 0.39665156\n",
      "Iteration 284, loss = 0.39614766\n",
      "Iteration 285, loss = 0.39564469\n",
      "Iteration 286, loss = 0.39514282\n",
      "Iteration 287, loss = 0.39464191\n",
      "Iteration 288, loss = 0.39414200\n",
      "Iteration 289, loss = 0.39364313\n",
      "Iteration 290, loss = 0.39314538\n",
      "Iteration 291, loss = 0.39265071\n",
      "Iteration 292, loss = 0.39215698\n",
      "Iteration 293, loss = 0.39166511\n",
      "Iteration 294, loss = 0.39117405\n",
      "Iteration 295, loss = 0.39068365\n",
      "Iteration 296, loss = 0.39019399\n",
      "Iteration 297, loss = 0.38970511\n",
      "Iteration 298, loss = 0.38921701\n",
      "Iteration 299, loss = 0.38872977\n",
      "Iteration 300, loss = 0.38824385\n",
      "Iteration 301, loss = 0.38775906\n",
      "Iteration 302, loss = 0.38727554\n",
      "Iteration 303, loss = 0.38679304\n",
      "Iteration 304, loss = 0.38631149\n",
      "Iteration 305, loss = 0.38583096\n",
      "Iteration 306, loss = 0.38535136\n",
      "Iteration 307, loss = 0.38487269\n",
      "Iteration 308, loss = 0.38439503\n",
      "Iteration 309, loss = 0.38391821\n",
      "Iteration 310, loss = 0.38344232\n",
      "Iteration 311, loss = 0.38296741\n",
      "Iteration 312, loss = 0.38249333\n",
      "Iteration 313, loss = 0.38202012\n",
      "Iteration 314, loss = 0.38154788\n",
      "Iteration 315, loss = 0.38107664\n",
      "Iteration 316, loss = 0.38060625\n",
      "Iteration 317, loss = 0.38013681\n",
      "Iteration 318, loss = 0.37966815\n",
      "Iteration 319, loss = 0.37920038\n",
      "Iteration 320, loss = 0.37873355\n",
      "Iteration 321, loss = 0.37826751\n",
      "Iteration 322, loss = 0.37780231\n",
      "Iteration 323, loss = 0.37733794\n",
      "Iteration 324, loss = 0.37687452\n",
      "Iteration 325, loss = 0.37641177\n",
      "Iteration 326, loss = 0.37594998\n",
      "Iteration 327, loss = 0.37548896\n",
      "Iteration 328, loss = 0.37502874\n",
      "Iteration 329, loss = 0.37456932\n",
      "Iteration 330, loss = 0.37411069\n",
      "Iteration 331, loss = 0.37365296\n",
      "Iteration 332, loss = 0.37319586\n",
      "Iteration 333, loss = 0.37273965\n",
      "Iteration 334, loss = 0.37228418\n",
      "Iteration 335, loss = 0.37182949\n",
      "Iteration 336, loss = 0.37137554\n",
      "Iteration 337, loss = 0.37092235\n",
      "Iteration 338, loss = 0.37046994\n",
      "Iteration 339, loss = 0.37001826\n",
      "Iteration 340, loss = 0.36956731\n",
      "Iteration 341, loss = 0.36911710\n",
      "Iteration 342, loss = 0.36866760\n",
      "Iteration 343, loss = 0.36821891\n",
      "Iteration 344, loss = 0.36777082\n",
      "Iteration 345, loss = 0.36732349\n",
      "Iteration 346, loss = 0.36687687\n",
      "Iteration 347, loss = 0.36643094\n",
      "Iteration 348, loss = 0.36598576\n",
      "Iteration 349, loss = 0.36554139\n",
      "Iteration 350, loss = 0.36509772\n",
      "Iteration 351, loss = 0.36465473\n",
      "Iteration 352, loss = 0.36421242\n",
      "Iteration 353, loss = 0.36377079\n",
      "Iteration 354, loss = 0.36332985\n",
      "Iteration 355, loss = 0.36289136\n",
      "Iteration 356, loss = 0.36245427\n",
      "Iteration 357, loss = 0.36201793\n",
      "Iteration 358, loss = 0.36158233\n",
      "Iteration 359, loss = 0.36114748\n",
      "Iteration 360, loss = 0.36071341\n",
      "Iteration 361, loss = 0.36028003\n",
      "Iteration 362, loss = 0.35984738\n",
      "Iteration 363, loss = 0.35941546\n",
      "Iteration 364, loss = 0.35898426\n",
      "Iteration 365, loss = 0.35855373\n",
      "Iteration 366, loss = 0.35812389\n",
      "Iteration 367, loss = 0.35769476\n",
      "Iteration 368, loss = 0.35726628\n",
      "Iteration 369, loss = 0.35683845\n",
      "Iteration 370, loss = 0.35641239\n",
      "Iteration 371, loss = 0.35598754\n",
      "Iteration 372, loss = 0.35556330\n",
      "Iteration 373, loss = 0.35513970\n",
      "Iteration 374, loss = 0.35471677\n",
      "Iteration 375, loss = 0.35429442\n",
      "Iteration 376, loss = 0.35387280\n",
      "Iteration 377, loss = 0.35345170\n",
      "Iteration 378, loss = 0.35303129\n",
      "Iteration 379, loss = 0.35261142\n",
      "Iteration 380, loss = 0.35219225\n",
      "Iteration 381, loss = 0.35177378\n",
      "Iteration 382, loss = 0.35135613\n",
      "Iteration 383, loss = 0.35093897\n",
      "Iteration 384, loss = 0.35052241\n",
      "Iteration 385, loss = 0.35010656\n",
      "Iteration 386, loss = 0.34969119\n",
      "Iteration 387, loss = 0.34927650\n",
      "Iteration 388, loss = 0.34886237\n",
      "Iteration 389, loss = 0.34844881\n",
      "Iteration 390, loss = 0.34803586\n",
      "Iteration 391, loss = 0.34762344\n",
      "Iteration 392, loss = 0.34721169\n",
      "Iteration 393, loss = 0.34680034\n",
      "Iteration 394, loss = 0.34638983\n",
      "Iteration 395, loss = 0.34597964\n",
      "Iteration 396, loss = 0.34557000\n",
      "Iteration 397, loss = 0.34516106\n",
      "Iteration 398, loss = 0.34475260\n",
      "Iteration 399, loss = 0.34434468\n",
      "Iteration 400, loss = 0.34393740\n",
      "Iteration 401, loss = 0.34353055\n",
      "Iteration 402, loss = 0.34312438\n",
      "Iteration 403, loss = 0.34271859\n",
      "Iteration 404, loss = 0.34231354\n",
      "Iteration 405, loss = 0.34190881\n",
      "Iteration 406, loss = 0.34150484\n",
      "Iteration 407, loss = 0.34110119\n",
      "Iteration 408, loss = 0.34069829\n",
      "Iteration 409, loss = 0.34029571\n",
      "Iteration 410, loss = 0.33989388\n",
      "Iteration 411, loss = 0.33949236\n",
      "Iteration 412, loss = 0.33909159\n",
      "Iteration 413, loss = 0.33869113\n",
      "Iteration 414, loss = 0.33829141\n",
      "Iteration 415, loss = 0.33789200\n",
      "Iteration 416, loss = 0.33749339\n",
      "Iteration 417, loss = 0.33709506\n",
      "Iteration 418, loss = 0.33669724\n",
      "Iteration 419, loss = 0.33630006\n",
      "Iteration 420, loss = 0.33590334\n",
      "Iteration 421, loss = 0.33550712\n",
      "Iteration 422, loss = 0.33511151\n",
      "Iteration 423, loss = 0.33471624\n",
      "Iteration 424, loss = 0.33432171\n",
      "Iteration 425, loss = 0.33392746\n",
      "Iteration 426, loss = 0.33353395\n",
      "Iteration 427, loss = 0.33314076\n",
      "Iteration 428, loss = 0.33274810\n",
      "Iteration 429, loss = 0.33235597\n",
      "Iteration 430, loss = 0.33196440\n",
      "Iteration 431, loss = 0.33157322\n",
      "Iteration 432, loss = 0.33118270\n",
      "Iteration 433, loss = 0.33079249\n",
      "Iteration 434, loss = 0.33040299\n",
      "Iteration 435, loss = 0.33001377\n",
      "Iteration 436, loss = 0.32962526\n",
      "Iteration 437, loss = 0.32923705\n",
      "Iteration 438, loss = 0.32884950\n",
      "Iteration 439, loss = 0.32846233\n",
      "Iteration 440, loss = 0.32807582\n",
      "Iteration 441, loss = 0.32768985\n",
      "Iteration 442, loss = 0.32730428\n",
      "Iteration 443, loss = 0.32691936\n",
      "Iteration 444, loss = 0.32653473\n",
      "Iteration 445, loss = 0.32615077\n",
      "Iteration 446, loss = 0.32576711\n",
      "Iteration 447, loss = 0.32538412\n",
      "Iteration 448, loss = 0.32500147\n",
      "Iteration 449, loss = 0.32461941\n",
      "Iteration 450, loss = 0.32423785\n",
      "Iteration 451, loss = 0.32385688\n",
      "Iteration 452, loss = 0.32347623\n",
      "Iteration 453, loss = 0.32309620\n",
      "Iteration 454, loss = 0.32271658\n",
      "Iteration 455, loss = 0.32233753\n",
      "Iteration 456, loss = 0.32195895\n",
      "Iteration 457, loss = 0.32158080\n",
      "Iteration 458, loss = 0.32120330\n",
      "Iteration 459, loss = 0.32082613\n",
      "Iteration 460, loss = 0.32044959\n",
      "Iteration 461, loss = 0.32007337\n",
      "Iteration 462, loss = 0.31969788\n",
      "Iteration 463, loss = 0.31932266\n",
      "Iteration 464, loss = 0.31894799\n",
      "Iteration 465, loss = 0.31857385\n",
      "Iteration 466, loss = 0.31820019\n",
      "Iteration 467, loss = 0.31782701\n",
      "Iteration 468, loss = 0.31745431\n",
      "Iteration 469, loss = 0.31708214\n",
      "Iteration 470, loss = 0.31671036\n",
      "Iteration 471, loss = 0.31633925\n",
      "Iteration 472, loss = 0.31596844\n",
      "Iteration 473, loss = 0.31559827\n",
      "Iteration 474, loss = 0.31522841\n",
      "Iteration 475, loss = 0.31485926\n",
      "Iteration 476, loss = 0.31449041\n",
      "Iteration 477, loss = 0.31412209\n",
      "Iteration 478, loss = 0.31375428\n",
      "Iteration 479, loss = 0.31338695\n",
      "Iteration 480, loss = 0.31302013\n",
      "Iteration 481, loss = 0.31265373\n",
      "Iteration 482, loss = 0.31228793\n",
      "Iteration 483, loss = 0.31192247\n",
      "Iteration 484, loss = 0.31155770\n",
      "Iteration 485, loss = 0.31119318\n",
      "Iteration 486, loss = 0.31082931\n",
      "Iteration 487, loss = 0.31046581\n",
      "Iteration 488, loss = 0.31010287\n",
      "Iteration 489, loss = 0.30974035\n",
      "Iteration 490, loss = 0.30937839\n",
      "Iteration 491, loss = 0.30901686\n",
      "Iteration 492, loss = 0.30865581\n",
      "Iteration 493, loss = 0.30829532\n",
      "Iteration 494, loss = 0.30793517\n",
      "Iteration 495, loss = 0.30757572\n",
      "Iteration 496, loss = 0.30721652\n",
      "Iteration 497, loss = 0.30685797\n",
      "Iteration 498, loss = 0.30649978\n",
      "Iteration 499, loss = 0.30614212\n",
      "Iteration 500, loss = 0.30578495\n",
      "Iteration 501, loss = 0.30542825\n",
      "Iteration 502, loss = 0.30507206\n",
      "Iteration 503, loss = 0.30471626\n",
      "Iteration 504, loss = 0.30436112\n",
      "Iteration 505, loss = 0.30400628\n",
      "Iteration 506, loss = 0.30365205\n",
      "Iteration 507, loss = 0.30329813\n",
      "Iteration 508, loss = 0.30294493\n",
      "Iteration 509, loss = 0.30259195\n",
      "Iteration 510, loss = 0.30223974\n",
      "Iteration 511, loss = 0.30188781\n",
      "Iteration 512, loss = 0.30153631\n",
      "Iteration 513, loss = 0.30118547\n",
      "Iteration 514, loss = 0.30083493\n",
      "Iteration 515, loss = 0.30048501\n",
      "Iteration 516, loss = 0.30013545\n",
      "Iteration 517, loss = 0.29978648\n",
      "Iteration 518, loss = 0.29943788\n",
      "Iteration 519, loss = 0.29908987\n",
      "Iteration 520, loss = 0.29874225\n",
      "Iteration 521, loss = 0.29839515\n",
      "Iteration 522, loss = 0.29804856\n",
      "Iteration 523, loss = 0.29770234\n",
      "Iteration 524, loss = 0.29735678\n",
      "Iteration 525, loss = 0.29701148\n",
      "Iteration 526, loss = 0.29666688\n",
      "Iteration 527, loss = 0.29632260\n",
      "Iteration 528, loss = 0.29597875\n",
      "Iteration 529, loss = 0.29563556\n",
      "Iteration 530, loss = 0.29529268\n",
      "Iteration 531, loss = 0.29495047\n",
      "Iteration 532, loss = 0.29460852\n",
      "Iteration 533, loss = 0.29426721\n",
      "Iteration 534, loss = 0.29392626\n",
      "Iteration 535, loss = 0.29358585\n",
      "Iteration 536, loss = 0.29324591\n",
      "Iteration 537, loss = 0.29290644\n",
      "Iteration 538, loss = 0.29256750\n",
      "Iteration 539, loss = 0.29222893\n",
      "Iteration 540, loss = 0.29189102\n",
      "Iteration 541, loss = 0.29155337\n",
      "Iteration 542, loss = 0.29121639\n",
      "Iteration 543, loss = 0.29087975\n",
      "Iteration 544, loss = 0.29054357\n",
      "Iteration 545, loss = 0.29020800\n",
      "Iteration 546, loss = 0.28987276\n",
      "Iteration 547, loss = 0.28953820\n",
      "Iteration 548, loss = 0.28920389\n",
      "Iteration 549, loss = 0.28887022\n",
      "Iteration 550, loss = 0.28853692\n",
      "Iteration 551, loss = 0.28820412\n",
      "Iteration 552, loss = 0.28787184\n",
      "Iteration 553, loss = 0.28753996\n",
      "Iteration 554, loss = 0.28720870\n",
      "Iteration 555, loss = 0.28687776\n",
      "Iteration 556, loss = 0.28654743\n",
      "Iteration 557, loss = 0.28621742\n",
      "Iteration 558, loss = 0.28588807\n",
      "Iteration 559, loss = 0.28555902\n",
      "Iteration 560, loss = 0.28523058\n",
      "Iteration 561, loss = 0.28490255\n",
      "Iteration 562, loss = 0.28457496\n",
      "Iteration 563, loss = 0.28424802\n",
      "Iteration 564, loss = 0.28392138\n",
      "Iteration 565, loss = 0.28359524\n",
      "Iteration 566, loss = 0.28326962\n",
      "Iteration 567, loss = 0.28294445\n",
      "Iteration 568, loss = 0.28261981\n",
      "Iteration 569, loss = 0.28229554\n",
      "Iteration 570, loss = 0.28197194\n",
      "Iteration 571, loss = 0.28164859\n",
      "Iteration 572, loss = 0.28132589\n",
      "Iteration 573, loss = 0.28100355\n",
      "Iteration 574, loss = 0.28068184\n",
      "Iteration 575, loss = 0.28036060\n",
      "Iteration 576, loss = 0.28003965\n",
      "Iteration 577, loss = 0.27971940\n",
      "Iteration 578, loss = 0.27939943\n",
      "Iteration 579, loss = 0.27908012\n",
      "Iteration 580, loss = 0.27876118\n",
      "Iteration 581, loss = 0.27844273\n",
      "Iteration 582, loss = 0.27812482\n",
      "Iteration 583, loss = 0.27780726\n",
      "Iteration 584, loss = 0.27749030\n",
      "Iteration 585, loss = 0.27717380\n",
      "Iteration 586, loss = 0.27685771\n",
      "Iteration 587, loss = 0.27654215\n",
      "Iteration 588, loss = 0.27622705\n",
      "Iteration 589, loss = 0.27591241\n",
      "Iteration 590, loss = 0.27559832\n",
      "Iteration 591, loss = 0.27528458\n",
      "Iteration 592, loss = 0.27497140\n",
      "Iteration 593, loss = 0.27465873\n",
      "Iteration 594, loss = 0.27434645\n",
      "Iteration 595, loss = 0.27403466\n",
      "Iteration 596, loss = 0.27372346\n",
      "Iteration 597, loss = 0.27341252\n",
      "Iteration 598, loss = 0.27310223\n",
      "Iteration 599, loss = 0.27279234\n",
      "Iteration 600, loss = 0.27248289\n",
      "Iteration 601, loss = 0.27217405\n",
      "Iteration 602, loss = 0.27186556\n",
      "Iteration 603, loss = 0.27155757\n",
      "Iteration 604, loss = 0.27125013\n",
      "Iteration 605, loss = 0.27094306\n",
      "Iteration 606, loss = 0.27063650\n",
      "Iteration 607, loss = 0.27033047\n",
      "Iteration 608, loss = 0.27002481\n",
      "Iteration 609, loss = 0.26971971\n",
      "Iteration 610, loss = 0.26941505\n",
      "Iteration 611, loss = 0.26911081\n",
      "Iteration 612, loss = 0.26880719\n",
      "Iteration 613, loss = 0.26850394\n",
      "Iteration 614, loss = 0.26820110\n",
      "Iteration 615, loss = 0.26789889\n",
      "Iteration 616, loss = 0.26759701\n",
      "Iteration 617, loss = 0.26729574\n",
      "Iteration 618, loss = 0.26699480\n",
      "Iteration 619, loss = 0.26669437\n",
      "Iteration 620, loss = 0.26639455\n",
      "Iteration 621, loss = 0.26609505\n",
      "Iteration 622, loss = 0.26579603\n",
      "Iteration 623, loss = 0.26549758\n",
      "Iteration 624, loss = 0.26519951\n",
      "Iteration 625, loss = 0.26490199\n",
      "Iteration 626, loss = 0.26460486\n",
      "Iteration 627, loss = 0.26430825\n",
      "Iteration 628, loss = 0.26401217\n",
      "Iteration 629, loss = 0.26371646\n",
      "Iteration 630, loss = 0.26342121\n",
      "Iteration 631, loss = 0.26312658\n",
      "Iteration 632, loss = 0.26283227\n",
      "Iteration 633, loss = 0.26253849\n",
      "Iteration 634, loss = 0.26224516\n",
      "Iteration 635, loss = 0.26195229\n",
      "Iteration 636, loss = 0.26165998\n",
      "Iteration 637, loss = 0.26136804\n",
      "Iteration 638, loss = 0.26107657\n",
      "Iteration 639, loss = 0.26078566\n",
      "Iteration 640, loss = 0.26049510\n",
      "Iteration 641, loss = 0.26020512\n",
      "Iteration 642, loss = 0.25991552\n",
      "Iteration 643, loss = 0.25962639\n",
      "Iteration 644, loss = 0.25933786\n",
      "Iteration 645, loss = 0.25904965\n",
      "Iteration 646, loss = 0.25876193\n",
      "Iteration 647, loss = 0.25847474\n",
      "Iteration 648, loss = 0.25818796\n",
      "Iteration 649, loss = 0.25790173\n",
      "Iteration 650, loss = 0.25761582\n",
      "Iteration 651, loss = 0.25733051\n",
      "Iteration 652, loss = 0.25704562\n",
      "Iteration 653, loss = 0.25676115\n",
      "Iteration 654, loss = 0.25647724\n",
      "Iteration 655, loss = 0.25619372\n",
      "Iteration 656, loss = 0.25591067\n",
      "Iteration 657, loss = 0.25562816\n",
      "Iteration 658, loss = 0.25534606\n",
      "Iteration 659, loss = 0.25506437\n",
      "Iteration 660, loss = 0.25478323\n",
      "Iteration 661, loss = 0.25450250\n",
      "Iteration 662, loss = 0.25422233\n",
      "Iteration 663, loss = 0.25394244\n",
      "Iteration 664, loss = 0.25366321\n",
      "Iteration 665, loss = 0.25338432\n",
      "Iteration 666, loss = 0.25310590\n",
      "Iteration 667, loss = 0.25282805\n",
      "Iteration 668, loss = 0.25255056\n",
      "Iteration 669, loss = 0.25227349\n",
      "Iteration 670, loss = 0.25199708\n",
      "Iteration 671, loss = 0.25172089\n",
      "Iteration 672, loss = 0.25144532\n",
      "Iteration 673, loss = 0.25117013\n",
      "Iteration 674, loss = 0.25089538\n",
      "Iteration 675, loss = 0.25062121\n",
      "Iteration 676, loss = 0.25034737\n",
      "Iteration 677, loss = 0.25007410\n",
      "Iteration 678, loss = 0.24980118\n",
      "Iteration 679, loss = 0.24952884\n",
      "Iteration 680, loss = 0.24925686\n",
      "Iteration 681, loss = 0.24898532\n",
      "Iteration 682, loss = 0.24871442\n",
      "Iteration 683, loss = 0.24844378\n",
      "Iteration 684, loss = 0.24817362\n",
      "Iteration 685, loss = 0.24790403\n",
      "Iteration 686, loss = 0.24763477\n",
      "Iteration 687, loss = 0.24736613\n",
      "Iteration 688, loss = 0.24709774\n",
      "Iteration 689, loss = 0.24683001\n",
      "Iteration 690, loss = 0.24656264\n",
      "Iteration 691, loss = 0.24629577\n",
      "Iteration 692, loss = 0.24602952\n",
      "Iteration 693, loss = 0.24576355\n",
      "Iteration 694, loss = 0.24549814\n",
      "Iteration 695, loss = 0.24523304\n",
      "Iteration 696, loss = 0.24496860\n",
      "Iteration 697, loss = 0.24470439\n",
      "Iteration 698, loss = 0.24444075\n",
      "Iteration 699, loss = 0.24417757\n",
      "Iteration 700, loss = 0.24391475\n",
      "Iteration 701, loss = 0.24365254\n",
      "Iteration 702, loss = 0.24339057\n",
      "Iteration 703, loss = 0.24312929\n",
      "Iteration 704, loss = 0.24286828\n",
      "Iteration 705, loss = 0.24260770\n",
      "Iteration 706, loss = 0.24234775\n",
      "Iteration 707, loss = 0.24208811\n",
      "Iteration 708, loss = 0.24182893\n",
      "Iteration 709, loss = 0.24157017\n",
      "Iteration 710, loss = 0.24131206\n",
      "Iteration 711, loss = 0.24105436\n",
      "Iteration 712, loss = 0.24079723\n",
      "Iteration 713, loss = 0.24054042\n",
      "Iteration 714, loss = 0.24028410\n",
      "Iteration 715, loss = 0.24002823\n",
      "Iteration 716, loss = 0.23977286\n",
      "Iteration 717, loss = 0.23951789\n",
      "Iteration 718, loss = 0.23926341\n",
      "Iteration 719, loss = 0.23900937\n",
      "Iteration 720, loss = 0.23875577\n",
      "Iteration 721, loss = 0.23850261\n",
      "Iteration 722, loss = 0.23824991\n",
      "Iteration 723, loss = 0.23799765\n",
      "Iteration 724, loss = 0.23774580\n",
      "Iteration 725, loss = 0.23749451\n",
      "Iteration 726, loss = 0.23724349\n",
      "Iteration 727, loss = 0.23699306\n",
      "Iteration 728, loss = 0.23674298\n",
      "Iteration 729, loss = 0.23649343\n",
      "Iteration 730, loss = 0.23624418\n",
      "Iteration 731, loss = 0.23599553\n",
      "Iteration 732, loss = 0.23574718\n",
      "Iteration 733, loss = 0.23549937\n",
      "Iteration 734, loss = 0.23525195\n",
      "Iteration 735, loss = 0.23500497\n",
      "Iteration 736, loss = 0.23475842\n",
      "Iteration 737, loss = 0.23451235\n",
      "Iteration 738, loss = 0.23426668\n",
      "Iteration 739, loss = 0.23402141\n",
      "Iteration 740, loss = 0.23377669\n",
      "Iteration 741, loss = 0.23353223\n",
      "Iteration 742, loss = 0.23328843\n",
      "Iteration 743, loss = 0.23304488\n",
      "Iteration 744, loss = 0.23280179\n",
      "Iteration 745, loss = 0.23255912\n",
      "Iteration 746, loss = 0.23231702\n",
      "Iteration 747, loss = 0.23207520\n",
      "Iteration 748, loss = 0.23183386\n",
      "Iteration 749, loss = 0.23159293\n",
      "Iteration 750, loss = 0.23135248\n",
      "Iteration 751, loss = 0.23111242\n",
      "Iteration 752, loss = 0.23087280\n",
      "Iteration 753, loss = 0.23063359\n",
      "Iteration 754, loss = 0.23039484\n",
      "Iteration 755, loss = 0.23015650\n",
      "Iteration 756, loss = 0.22991855\n",
      "Iteration 757, loss = 0.22968119\n",
      "Iteration 758, loss = 0.22944405\n",
      "Iteration 759, loss = 0.22920750\n",
      "Iteration 760, loss = 0.22897124\n",
      "Iteration 761, loss = 0.22873548\n",
      "Iteration 762, loss = 0.22850013\n",
      "Iteration 763, loss = 0.22826518\n",
      "Iteration 764, loss = 0.22803072\n",
      "Iteration 765, loss = 0.22779658\n",
      "Iteration 766, loss = 0.22756299\n",
      "Iteration 767, loss = 0.22732972\n",
      "Iteration 768, loss = 0.22709694\n",
      "Iteration 769, loss = 0.22686449\n",
      "Iteration 770, loss = 0.22663252\n",
      "Iteration 771, loss = 0.22640098\n",
      "Iteration 772, loss = 0.22616979\n",
      "Iteration 773, loss = 0.22593915\n",
      "Iteration 774, loss = 0.22570876\n",
      "Iteration 775, loss = 0.22547892\n",
      "Iteration 776, loss = 0.22524940\n",
      "Iteration 777, loss = 0.22502033\n",
      "Iteration 778, loss = 0.22479170\n",
      "Iteration 779, loss = 0.22456342\n",
      "Iteration 780, loss = 0.22433572\n",
      "Iteration 781, loss = 0.22410822\n",
      "Iteration 782, loss = 0.22388129\n",
      "Iteration 783, loss = 0.22365466\n",
      "Iteration 784, loss = 0.22342847\n",
      "Iteration 785, loss = 0.22320276\n",
      "Iteration 786, loss = 0.22297736\n",
      "Iteration 787, loss = 0.22275250\n",
      "Iteration 788, loss = 0.22252787\n",
      "Iteration 789, loss = 0.22230384\n",
      "Iteration 790, loss = 0.22208006\n",
      "Iteration 791, loss = 0.22185676\n",
      "Iteration 792, loss = 0.22163392\n",
      "Iteration 793, loss = 0.22141138\n",
      "Iteration 794, loss = 0.22118930\n",
      "Iteration 795, loss = 0.22096763\n",
      "Iteration 796, loss = 0.22074630\n",
      "Iteration 797, loss = 0.22052553\n",
      "Iteration 798, loss = 0.22030501\n",
      "Iteration 799, loss = 0.22008493\n",
      "Iteration 800, loss = 0.21986529\n",
      "Iteration 801, loss = 0.21964600\n",
      "Iteration 802, loss = 0.21942721\n",
      "Iteration 803, loss = 0.21920867\n",
      "Iteration 804, loss = 0.21899071\n",
      "Iteration 805, loss = 0.21877302\n",
      "Iteration 806, loss = 0.21855575\n",
      "Iteration 807, loss = 0.21833886\n",
      "Iteration 808, loss = 0.21812242\n",
      "Iteration 809, loss = 0.21790633\n",
      "Iteration 810, loss = 0.21769070\n",
      "Iteration 811, loss = 0.21747540\n",
      "Iteration 812, loss = 0.21726052\n",
      "Iteration 813, loss = 0.21704607\n",
      "Iteration 814, loss = 0.21683193\n",
      "Iteration 815, loss = 0.21661832\n",
      "Iteration 816, loss = 0.21640499\n",
      "Iteration 817, loss = 0.21619203\n",
      "Iteration 818, loss = 0.21597962\n",
      "Iteration 819, loss = 0.21576739\n",
      "Iteration 820, loss = 0.21555574\n",
      "Iteration 821, loss = 0.21534437\n",
      "Iteration 822, loss = 0.21513341\n",
      "Iteration 823, loss = 0.21492283\n",
      "Iteration 824, loss = 0.21471268\n",
      "Iteration 825, loss = 0.21450285\n",
      "Iteration 826, loss = 0.21429351\n",
      "Iteration 827, loss = 0.21408448\n",
      "Iteration 828, loss = 0.21387582\n",
      "Iteration 829, loss = 0.21366766\n",
      "Iteration 830, loss = 0.21345971\n",
      "Iteration 831, loss = 0.21325233\n",
      "Iteration 832, loss = 0.21304522\n",
      "Iteration 833, loss = 0.21283853\n",
      "Iteration 834, loss = 0.21263214\n",
      "Iteration 835, loss = 0.21242632\n",
      "Iteration 836, loss = 0.21222067\n",
      "Iteration 837, loss = 0.21201554\n",
      "Iteration 838, loss = 0.21181071\n",
      "Iteration 839, loss = 0.21160630\n",
      "Iteration 840, loss = 0.21140227\n",
      "Iteration 841, loss = 0.21119859\n",
      "Iteration 842, loss = 0.21099529\n",
      "Iteration 843, loss = 0.21079244\n",
      "Iteration 844, loss = 0.21058987\n",
      "Iteration 845, loss = 0.21038771\n",
      "Iteration 846, loss = 0.21018599\n",
      "Iteration 847, loss = 0.20998458\n",
      "Iteration 848, loss = 0.20978354\n",
      "Iteration 849, loss = 0.20958286\n",
      "Iteration 850, loss = 0.20938262\n",
      "Iteration 851, loss = 0.20918267\n",
      "Iteration 852, loss = 0.20898320\n",
      "Iteration 853, loss = 0.20878401\n",
      "Iteration 854, loss = 0.20858518\n",
      "Iteration 855, loss = 0.20838680\n",
      "Iteration 856, loss = 0.20818874\n",
      "Iteration 857, loss = 0.20799102\n",
      "Iteration 858, loss = 0.20779376\n",
      "Iteration 859, loss = 0.20759680\n",
      "Iteration 860, loss = 0.20740020\n",
      "Iteration 861, loss = 0.20720396\n",
      "Iteration 862, loss = 0.20700818\n",
      "Iteration 863, loss = 0.20681266\n",
      "Iteration 864, loss = 0.20661754\n",
      "Iteration 865, loss = 0.20642280\n",
      "Iteration 866, loss = 0.20622839\n",
      "Iteration 867, loss = 0.20603432\n",
      "Iteration 868, loss = 0.20584075\n",
      "Iteration 869, loss = 0.20564739\n",
      "Iteration 870, loss = 0.20545443\n",
      "Iteration 871, loss = 0.20526186\n",
      "Iteration 872, loss = 0.20506966\n",
      "Iteration 873, loss = 0.20487776\n",
      "Iteration 874, loss = 0.20468626\n",
      "Iteration 875, loss = 0.20449519\n",
      "Iteration 876, loss = 0.20430463\n",
      "Iteration 877, loss = 0.20411439\n",
      "Iteration 878, loss = 0.20392451\n",
      "Iteration 879, loss = 0.20373503\n",
      "Iteration 880, loss = 0.20354587\n",
      "Iteration 881, loss = 0.20335712\n",
      "Iteration 882, loss = 0.20316869\n",
      "Iteration 883, loss = 0.20298066\n",
      "Iteration 884, loss = 0.20279294\n",
      "Iteration 885, loss = 0.20260561\n",
      "Iteration 886, loss = 0.20241860\n",
      "Iteration 887, loss = 0.20223199\n",
      "Iteration 888, loss = 0.20204569\n",
      "Iteration 889, loss = 0.20185980\n",
      "Iteration 890, loss = 0.20167421\n",
      "Iteration 891, loss = 0.20148898\n",
      "Iteration 892, loss = 0.20130411\n",
      "Iteration 893, loss = 0.20111958\n",
      "Iteration 894, loss = 0.20093540\n",
      "Iteration 895, loss = 0.20075155\n",
      "Iteration 896, loss = 0.20056809\n",
      "Iteration 897, loss = 0.20038492\n",
      "Iteration 898, loss = 0.20020212\n",
      "Iteration 899, loss = 0.20001965\n",
      "Iteration 900, loss = 0.19983753\n",
      "Iteration 901, loss = 0.19965575\n",
      "Iteration 902, loss = 0.19947431\n",
      "Iteration 903, loss = 0.19929321\n",
      "Iteration 904, loss = 0.19911246\n",
      "Iteration 905, loss = 0.19893209\n",
      "Iteration 906, loss = 0.19875197\n",
      "Iteration 907, loss = 0.19857224\n",
      "Iteration 908, loss = 0.19839283\n",
      "Iteration 909, loss = 0.19821379\n",
      "Iteration 910, loss = 0.19803503\n",
      "Iteration 911, loss = 0.19785666\n",
      "Iteration 912, loss = 0.19767858\n",
      "Iteration 913, loss = 0.19750084\n",
      "Iteration 914, loss = 0.19732345\n",
      "Iteration 915, loss = 0.19714637\n",
      "Iteration 916, loss = 0.19696965\n",
      "Iteration 917, loss = 0.19679327\n",
      "Iteration 918, loss = 0.19661720\n",
      "Iteration 919, loss = 0.19644144\n",
      "Iteration 920, loss = 0.19626606\n",
      "Iteration 921, loss = 0.19609097\n",
      "Iteration 922, loss = 0.19591619\n",
      "Iteration 923, loss = 0.19574180\n",
      "Iteration 924, loss = 0.19556767\n",
      "Iteration 925, loss = 0.19539389\n",
      "Iteration 926, loss = 0.19522045\n",
      "Iteration 927, loss = 0.19504734\n",
      "Iteration 928, loss = 0.19487453\n",
      "Iteration 929, loss = 0.19470206\n",
      "Iteration 930, loss = 0.19452991\n",
      "Iteration 931, loss = 0.19435806\n",
      "Iteration 932, loss = 0.19418657\n",
      "Iteration 933, loss = 0.19401535\n",
      "Iteration 934, loss = 0.19384453\n",
      "Iteration 935, loss = 0.19367396\n",
      "Iteration 936, loss = 0.19350373\n",
      "Iteration 937, loss = 0.19333384\n",
      "Iteration 938, loss = 0.19316424\n",
      "Iteration 939, loss = 0.19299496\n",
      "Iteration 940, loss = 0.19282601\n",
      "Iteration 941, loss = 0.19265736\n",
      "Iteration 942, loss = 0.19248908\n",
      "Iteration 943, loss = 0.19232105\n",
      "Iteration 944, loss = 0.19215337\n",
      "Iteration 945, loss = 0.19198598\n",
      "Iteration 946, loss = 0.19181890\n",
      "Iteration 947, loss = 0.19165216\n",
      "Iteration 948, loss = 0.19148571\n",
      "Iteration 949, loss = 0.19131960\n",
      "Iteration 950, loss = 0.19115378\n",
      "Iteration 951, loss = 0.19098826\n",
      "Iteration 952, loss = 0.19082307\n",
      "Iteration 953, loss = 0.19065815\n",
      "Iteration 954, loss = 0.19049360\n",
      "Iteration 955, loss = 0.19032932\n",
      "Iteration 956, loss = 0.19016537\n",
      "Iteration 957, loss = 0.19000171\n",
      "Iteration 958, loss = 0.18983835\n",
      "Iteration 959, loss = 0.18967529\n",
      "Iteration 960, loss = 0.18951257\n",
      "Iteration 961, loss = 0.18935013\n",
      "Iteration 962, loss = 0.18918801\n",
      "Iteration 963, loss = 0.18902616\n",
      "Iteration 964, loss = 0.18886464\n",
      "Iteration 965, loss = 0.18870341\n",
      "Iteration 966, loss = 0.18854252\n",
      "Iteration 967, loss = 0.18838188\n",
      "Iteration 968, loss = 0.18822155\n",
      "Iteration 969, loss = 0.18806151\n",
      "Iteration 970, loss = 0.18790180\n",
      "Iteration 971, loss = 0.18774242\n",
      "Iteration 972, loss = 0.18758325\n",
      "Iteration 973, loss = 0.18742445\n",
      "Iteration 974, loss = 0.18726589\n",
      "Iteration 975, loss = 0.18710766\n",
      "Iteration 976, loss = 0.18694976\n",
      "Iteration 977, loss = 0.18679207\n",
      "Iteration 978, loss = 0.18663474\n",
      "Iteration 979, loss = 0.18647766\n",
      "Iteration 980, loss = 0.18632089\n",
      "Iteration 981, loss = 0.18616448\n",
      "Iteration 982, loss = 0.18600826\n",
      "Iteration 983, loss = 0.18585236\n",
      "Iteration 984, loss = 0.18569677\n",
      "Iteration 985, loss = 0.18554148\n",
      "Iteration 986, loss = 0.18538647\n",
      "Iteration 987, loss = 0.18523174\n",
      "Iteration 988, loss = 0.18507729\n",
      "Iteration 989, loss = 0.18492314\n",
      "Iteration 990, loss = 0.18476930\n",
      "Iteration 991, loss = 0.18461570\n",
      "Iteration 992, loss = 0.18446242\n",
      "Iteration 993, loss = 0.18430941\n",
      "Iteration 994, loss = 0.18415672\n",
      "Iteration 995, loss = 0.18400428\n",
      "Iteration 996, loss = 0.18385211\n",
      "Iteration 997, loss = 0.18370024\n",
      "Iteration 998, loss = 0.18354869\n",
      "Iteration 999, loss = 0.18339738\n",
      "Iteration 1000, loss = 0.18324634\n",
      "Iteration 1, loss = 1.35165820\n",
      "Iteration 2, loss = 1.31479672\n",
      "Iteration 3, loss = 1.26518330\n",
      "Iteration 4, loss = 1.20695536\n",
      "Iteration 5, loss = 1.14435940\n",
      "Iteration 6, loss = 1.08116064\n",
      "Iteration 7, loss = 1.02062790\n",
      "Iteration 8, loss = 0.96544874\n",
      "Iteration 9, loss = 0.91730971\n",
      "Iteration 10, loss = 0.87688140\n",
      "Iteration 11, loss = 0.84409054\n",
      "Iteration 12, loss = 0.81809185\n",
      "Iteration 13, loss = 0.79793326\n",
      "Iteration 14, loss = 0.78237749\n",
      "Iteration 15, loss = 0.77031758\n",
      "Iteration 16, loss = 0.76090277\n",
      "Iteration 17, loss = 0.75313911\n",
      "Iteration 18, loss = 0.74672448\n",
      "Iteration 19, loss = 0.74136719\n",
      "Iteration 20, loss = 0.73677897\n",
      "Iteration 21, loss = 0.73289239\n",
      "Iteration 22, loss = 0.72940609\n",
      "Iteration 23, loss = 0.72562339\n",
      "Iteration 24, loss = 0.72150246\n",
      "Iteration 25, loss = 0.71689034\n",
      "Iteration 26, loss = 0.71177166\n",
      "Iteration 27, loss = 0.70637501\n",
      "Iteration 28, loss = 0.70077283\n",
      "Iteration 29, loss = 0.69506261\n",
      "Iteration 30, loss = 0.68942855\n",
      "Iteration 31, loss = 0.68397271\n",
      "Iteration 32, loss = 0.67881663\n",
      "Iteration 33, loss = 0.67401590\n",
      "Iteration 34, loss = 0.66957716\n",
      "Iteration 35, loss = 0.66547880\n",
      "Iteration 36, loss = 0.66172206\n",
      "Iteration 37, loss = 0.65826401\n",
      "Iteration 38, loss = 0.65507485\n",
      "Iteration 39, loss = 0.65210329\n",
      "Iteration 40, loss = 0.64930088\n",
      "Iteration 41, loss = 0.64662609\n",
      "Iteration 42, loss = 0.64404742\n",
      "Iteration 43, loss = 0.64153827\n",
      "Iteration 44, loss = 0.63907490\n",
      "Iteration 45, loss = 0.63663705\n",
      "Iteration 46, loss = 0.63420178\n",
      "Iteration 47, loss = 0.63176098\n",
      "Iteration 48, loss = 0.62931335\n",
      "Iteration 49, loss = 0.62686443\n",
      "Iteration 50, loss = 0.62441103\n",
      "Iteration 51, loss = 0.62195874\n",
      "Iteration 52, loss = 0.61951604\n",
      "Iteration 53, loss = 0.61709314\n",
      "Iteration 54, loss = 0.61469369\n",
      "Iteration 55, loss = 0.61232366\n",
      "Iteration 56, loss = 0.60998785\n",
      "Iteration 57, loss = 0.60768983\n",
      "Iteration 58, loss = 0.60543189\n",
      "Iteration 59, loss = 0.60321513\n",
      "Iteration 60, loss = 0.60103962\n",
      "Iteration 61, loss = 0.59890456\n",
      "Iteration 62, loss = 0.59680850\n",
      "Iteration 63, loss = 0.59474953\n",
      "Iteration 64, loss = 0.59272545\n",
      "Iteration 65, loss = 0.59073396\n",
      "Iteration 66, loss = 0.58877278\n",
      "Iteration 67, loss = 0.58684498\n",
      "Iteration 68, loss = 0.58494449\n",
      "Iteration 69, loss = 0.58306880\n",
      "Iteration 70, loss = 0.58121681\n",
      "Iteration 71, loss = 0.57938875\n",
      "Iteration 72, loss = 0.57758231\n",
      "Iteration 73, loss = 0.57579671\n",
      "Iteration 74, loss = 0.57403135\n",
      "Iteration 75, loss = 0.57228580\n",
      "Iteration 76, loss = 0.57055970\n",
      "Iteration 77, loss = 0.56885283\n",
      "Iteration 78, loss = 0.56716498\n",
      "Iteration 79, loss = 0.56550060\n",
      "Iteration 80, loss = 0.56385495\n",
      "Iteration 81, loss = 0.56222784\n",
      "Iteration 82, loss = 0.56062208\n",
      "Iteration 83, loss = 0.55903723\n",
      "Iteration 84, loss = 0.55747066\n",
      "Iteration 85, loss = 0.55592394\n",
      "Iteration 86, loss = 0.55439485\n",
      "Iteration 87, loss = 0.55288306\n",
      "Iteration 88, loss = 0.55138822\n",
      "Iteration 89, loss = 0.54990998\n",
      "Iteration 90, loss = 0.54844798\n",
      "Iteration 91, loss = 0.54700185\n",
      "Iteration 92, loss = 0.54557125\n",
      "Iteration 93, loss = 0.54415577\n",
      "Iteration 94, loss = 0.54275514\n",
      "Iteration 95, loss = 0.54136912\n",
      "Iteration 96, loss = 0.53999731\n",
      "Iteration 97, loss = 0.53863996\n",
      "Iteration 98, loss = 0.53729636\n",
      "Iteration 99, loss = 0.53596615\n",
      "Iteration 100, loss = 0.53464904\n",
      "Iteration 101, loss = 0.53334481\n",
      "Iteration 102, loss = 0.53205319\n",
      "Iteration 103, loss = 0.53077452\n",
      "Iteration 104, loss = 0.52950885\n",
      "Iteration 105, loss = 0.52825496\n",
      "Iteration 106, loss = 0.52701261\n",
      "Iteration 107, loss = 0.52578156\n",
      "Iteration 108, loss = 0.52456160\n",
      "Iteration 109, loss = 0.52335251\n",
      "Iteration 110, loss = 0.52215405\n",
      "Iteration 111, loss = 0.52096602\n",
      "Iteration 112, loss = 0.51978972\n",
      "Iteration 113, loss = 0.51862458\n",
      "Iteration 114, loss = 0.51746952\n",
      "Iteration 115, loss = 0.51632435\n",
      "Iteration 116, loss = 0.51518887\n",
      "Iteration 117, loss = 0.51406290\n",
      "Iteration 118, loss = 0.51294625\n",
      "Iteration 119, loss = 0.51183874\n",
      "Iteration 120, loss = 0.51074018\n",
      "Iteration 121, loss = 0.50965041\n",
      "Iteration 122, loss = 0.50856924\n",
      "Iteration 123, loss = 0.50749650\n",
      "Iteration 124, loss = 0.50643204\n",
      "Iteration 125, loss = 0.50537602\n",
      "Iteration 126, loss = 0.50432831\n",
      "Iteration 127, loss = 0.50328840\n",
      "Iteration 128, loss = 0.50225618\n",
      "Iteration 129, loss = 0.50123149\n",
      "Iteration 130, loss = 0.50021423\n",
      "Iteration 131, loss = 0.49920426\n",
      "Iteration 132, loss = 0.49820345\n",
      "Iteration 133, loss = 0.49721312\n",
      "Iteration 134, loss = 0.49623080\n",
      "Iteration 135, loss = 0.49525725\n",
      "Iteration 136, loss = 0.49429147\n",
      "Iteration 137, loss = 0.49333262\n",
      "Iteration 138, loss = 0.49238190\n",
      "Iteration 139, loss = 0.49143941\n",
      "Iteration 140, loss = 0.49050369\n",
      "Iteration 141, loss = 0.48957501\n",
      "Iteration 142, loss = 0.48865250\n",
      "Iteration 143, loss = 0.48773618\n",
      "Iteration 144, loss = 0.48682606\n",
      "Iteration 145, loss = 0.48592215\n",
      "Iteration 146, loss = 0.48502444\n",
      "Iteration 147, loss = 0.48413288\n",
      "Iteration 148, loss = 0.48324745\n",
      "Iteration 149, loss = 0.48236809\n",
      "Iteration 150, loss = 0.48149473\n",
      "Iteration 151, loss = 0.48062731\n",
      "Iteration 152, loss = 0.47976576\n",
      "Iteration 153, loss = 0.47890999\n",
      "Iteration 154, loss = 0.47805991\n",
      "Iteration 155, loss = 0.47721557\n",
      "Iteration 156, loss = 0.47637676\n",
      "Iteration 157, loss = 0.47554327\n",
      "Iteration 158, loss = 0.47471497\n",
      "Iteration 159, loss = 0.47389179\n",
      "Iteration 160, loss = 0.47307373\n",
      "Iteration 161, loss = 0.47226080\n",
      "Iteration 162, loss = 0.47145282\n",
      "Iteration 163, loss = 0.47064970\n",
      "Iteration 164, loss = 0.46985137\n",
      "Iteration 165, loss = 0.46905774\n",
      "Iteration 166, loss = 0.46826874\n",
      "Iteration 167, loss = 0.46748429\n",
      "Iteration 168, loss = 0.46670432\n",
      "Iteration 169, loss = 0.46592874\n",
      "Iteration 170, loss = 0.46515749\n",
      "Iteration 171, loss = 0.46439049\n",
      "Iteration 172, loss = 0.46362767\n",
      "Iteration 173, loss = 0.46286896\n",
      "Iteration 174, loss = 0.46211429\n",
      "Iteration 175, loss = 0.46136358\n",
      "Iteration 176, loss = 0.46061678\n",
      "Iteration 177, loss = 0.45987381\n",
      "Iteration 178, loss = 0.45913491\n",
      "Iteration 179, loss = 0.45840011\n",
      "Iteration 180, loss = 0.45766902\n",
      "Iteration 181, loss = 0.45694161\n",
      "Iteration 182, loss = 0.45621780\n",
      "Iteration 183, loss = 0.45549887\n",
      "Iteration 184, loss = 0.45478344\n",
      "Iteration 185, loss = 0.45407146\n",
      "Iteration 186, loss = 0.45336290\n",
      "Iteration 187, loss = 0.45265773\n",
      "Iteration 188, loss = 0.45195591\n",
      "Iteration 189, loss = 0.45125742\n",
      "Iteration 190, loss = 0.45056222\n",
      "Iteration 191, loss = 0.44987026\n",
      "Iteration 192, loss = 0.44918151\n",
      "Iteration 193, loss = 0.44849592\n",
      "Iteration 194, loss = 0.44781345\n",
      "Iteration 195, loss = 0.44713405\n",
      "Iteration 196, loss = 0.44645766\n",
      "Iteration 197, loss = 0.44578426\n",
      "Iteration 198, loss = 0.44511378\n",
      "Iteration 199, loss = 0.44444619\n",
      "Iteration 200, loss = 0.44378143\n",
      "Iteration 201, loss = 0.44311947\n",
      "Iteration 202, loss = 0.44246026\n",
      "Iteration 203, loss = 0.44180376\n",
      "Iteration 204, loss = 0.44114992\n",
      "Iteration 205, loss = 0.44049871\n",
      "Iteration 206, loss = 0.43985009\n",
      "Iteration 207, loss = 0.43920402\n",
      "Iteration 208, loss = 0.43856047\n",
      "Iteration 209, loss = 0.43791938\n",
      "Iteration 210, loss = 0.43728074\n",
      "Iteration 211, loss = 0.43664449\n",
      "Iteration 212, loss = 0.43601061\n",
      "Iteration 213, loss = 0.43537907\n",
      "Iteration 214, loss = 0.43474982\n",
      "Iteration 215, loss = 0.43412283\n",
      "Iteration 216, loss = 0.43349807\n",
      "Iteration 217, loss = 0.43287551\n",
      "Iteration 218, loss = 0.43225512\n",
      "Iteration 219, loss = 0.43163685\n",
      "Iteration 220, loss = 0.43102070\n",
      "Iteration 221, loss = 0.43040661\n",
      "Iteration 222, loss = 0.42979456\n",
      "Iteration 223, loss = 0.42918453\n",
      "Iteration 224, loss = 0.42857649\n",
      "Iteration 225, loss = 0.42797040\n",
      "Iteration 226, loss = 0.42736624\n",
      "Iteration 227, loss = 0.42676399\n",
      "Iteration 228, loss = 0.42616361\n",
      "Iteration 229, loss = 0.42556508\n",
      "Iteration 230, loss = 0.42496838\n",
      "Iteration 231, loss = 0.42437348\n",
      "Iteration 232, loss = 0.42378036\n",
      "Iteration 233, loss = 0.42318899\n",
      "Iteration 234, loss = 0.42259934\n",
      "Iteration 235, loss = 0.42201141\n",
      "Iteration 236, loss = 0.42142516\n",
      "Iteration 237, loss = 0.42084061\n",
      "Iteration 238, loss = 0.42025931\n",
      "Iteration 239, loss = 0.41967975\n",
      "Iteration 240, loss = 0.41910189\n",
      "Iteration 241, loss = 0.41852571\n",
      "Iteration 242, loss = 0.41795118\n",
      "Iteration 243, loss = 0.41737829\n",
      "Iteration 244, loss = 0.41680701\n",
      "Iteration 245, loss = 0.41623732\n",
      "Iteration 246, loss = 0.41566920\n",
      "Iteration 247, loss = 0.41510262\n",
      "Iteration 248, loss = 0.41453758\n",
      "Iteration 249, loss = 0.41397404\n",
      "Iteration 250, loss = 0.41341198\n",
      "Iteration 251, loss = 0.41285140\n",
      "Iteration 252, loss = 0.41229226\n",
      "Iteration 253, loss = 0.41173455\n",
      "Iteration 254, loss = 0.41117825\n",
      "Iteration 255, loss = 0.41062335\n",
      "Iteration 256, loss = 0.41007119\n",
      "Iteration 257, loss = 0.40952166\n",
      "Iteration 258, loss = 0.40897354\n",
      "Iteration 259, loss = 0.40842683\n",
      "Iteration 260, loss = 0.40788153\n",
      "Iteration 261, loss = 0.40733763\n",
      "Iteration 262, loss = 0.40679511\n",
      "Iteration 263, loss = 0.40625532\n",
      "Iteration 264, loss = 0.40571702\n",
      "Iteration 265, loss = 0.40518001\n",
      "Iteration 266, loss = 0.40464430\n",
      "Iteration 267, loss = 0.40410991\n",
      "Iteration 268, loss = 0.40357682\n",
      "Iteration 269, loss = 0.40304505\n",
      "Iteration 270, loss = 0.40251458\n",
      "Iteration 271, loss = 0.40198542\n",
      "Iteration 272, loss = 0.40145756\n",
      "Iteration 273, loss = 0.40093098\n",
      "Iteration 274, loss = 0.40040567\n",
      "Iteration 275, loss = 0.39988163\n",
      "Iteration 276, loss = 0.39935882\n",
      "Iteration 277, loss = 0.39883725\n",
      "Iteration 278, loss = 0.39831690\n",
      "Iteration 279, loss = 0.39779775\n",
      "Iteration 280, loss = 0.39727979\n",
      "Iteration 281, loss = 0.39676302\n",
      "Iteration 282, loss = 0.39624741\n",
      "Iteration 283, loss = 0.39573295\n",
      "Iteration 284, loss = 0.39521965\n",
      "Iteration 285, loss = 0.39470747\n",
      "Iteration 286, loss = 0.39419643\n",
      "Iteration 287, loss = 0.39368650\n",
      "Iteration 288, loss = 0.39317767\n",
      "Iteration 289, loss = 0.39266994\n",
      "Iteration 290, loss = 0.39216329\n",
      "Iteration 291, loss = 0.39165772\n",
      "Iteration 292, loss = 0.39115321\n",
      "Iteration 293, loss = 0.39064975\n",
      "Iteration 294, loss = 0.39014734\n",
      "Iteration 295, loss = 0.38964595\n",
      "Iteration 296, loss = 0.38914559\n",
      "Iteration 297, loss = 0.38864623\n",
      "Iteration 298, loss = 0.38814787\n",
      "Iteration 299, loss = 0.38765051\n",
      "Iteration 300, loss = 0.38715412\n",
      "Iteration 301, loss = 0.38665870\n",
      "Iteration 302, loss = 0.38616457\n",
      "Iteration 303, loss = 0.38567297\n",
      "Iteration 304, loss = 0.38518231\n",
      "Iteration 305, loss = 0.38469280\n",
      "Iteration 306, loss = 0.38420434\n",
      "Iteration 307, loss = 0.38371673\n",
      "Iteration 308, loss = 0.38322999\n",
      "Iteration 309, loss = 0.38274414\n",
      "Iteration 310, loss = 0.38225920\n",
      "Iteration 311, loss = 0.38177520\n",
      "Iteration 312, loss = 0.38129212\n",
      "Iteration 313, loss = 0.38081000\n",
      "Iteration 314, loss = 0.38032884\n",
      "Iteration 315, loss = 0.37984877\n",
      "Iteration 316, loss = 0.37936980\n",
      "Iteration 317, loss = 0.37889180\n",
      "Iteration 318, loss = 0.37841474\n",
      "Iteration 319, loss = 0.37793862\n",
      "Iteration 320, loss = 0.37746343\n",
      "Iteration 321, loss = 0.37698915\n",
      "Iteration 322, loss = 0.37651578\n",
      "Iteration 323, loss = 0.37604331\n",
      "Iteration 324, loss = 0.37557173\n",
      "Iteration 325, loss = 0.37510102\n",
      "Iteration 326, loss = 0.37463119\n",
      "Iteration 327, loss = 0.37416221\n",
      "Iteration 328, loss = 0.37369409\n",
      "Iteration 329, loss = 0.37322681\n",
      "Iteration 330, loss = 0.37276048\n",
      "Iteration 331, loss = 0.37229504\n",
      "Iteration 332, loss = 0.37183044\n",
      "Iteration 333, loss = 0.37136667\n",
      "Iteration 334, loss = 0.37090373\n",
      "Iteration 335, loss = 0.37044162\n",
      "Iteration 336, loss = 0.36998033\n",
      "Iteration 337, loss = 0.36951986\n",
      "Iteration 338, loss = 0.36906021\n",
      "Iteration 339, loss = 0.36860137\n",
      "Iteration 340, loss = 0.36814334\n",
      "Iteration 341, loss = 0.36768611\n",
      "Iteration 342, loss = 0.36722967\n",
      "Iteration 343, loss = 0.36677403\n",
      "Iteration 344, loss = 0.36631918\n",
      "Iteration 345, loss = 0.36586510\n",
      "Iteration 346, loss = 0.36541181\n",
      "Iteration 347, loss = 0.36495929\n",
      "Iteration 348, loss = 0.36450753\n",
      "Iteration 349, loss = 0.36405653\n",
      "Iteration 350, loss = 0.36360629\n",
      "Iteration 351, loss = 0.36315680\n",
      "Iteration 352, loss = 0.36270806\n",
      "Iteration 353, loss = 0.36226005\n",
      "Iteration 354, loss = 0.36181278\n",
      "Iteration 355, loss = 0.36136625\n",
      "Iteration 356, loss = 0.36092044\n",
      "Iteration 357, loss = 0.36047535\n",
      "Iteration 358, loss = 0.36003097\n",
      "Iteration 359, loss = 0.35958731\n",
      "Iteration 360, loss = 0.35914436\n",
      "Iteration 361, loss = 0.35870211\n",
      "Iteration 362, loss = 0.35826055\n",
      "Iteration 363, loss = 0.35781970\n",
      "Iteration 364, loss = 0.35737953\n",
      "Iteration 365, loss = 0.35694005\n",
      "Iteration 366, loss = 0.35650125\n",
      "Iteration 367, loss = 0.35606313\n",
      "Iteration 368, loss = 0.35562568\n",
      "Iteration 369, loss = 0.35518891\n",
      "Iteration 370, loss = 0.35475280\n",
      "Iteration 371, loss = 0.35431735\n",
      "Iteration 372, loss = 0.35388276\n",
      "Iteration 373, loss = 0.35344886\n",
      "Iteration 374, loss = 0.35301561\n",
      "Iteration 375, loss = 0.35258302\n",
      "Iteration 376, loss = 0.35215107\n",
      "Iteration 377, loss = 0.35171978\n",
      "Iteration 378, loss = 0.35129045\n",
      "Iteration 379, loss = 0.35086315\n",
      "Iteration 380, loss = 0.35043657\n",
      "Iteration 381, loss = 0.35001072\n",
      "Iteration 382, loss = 0.34958560\n",
      "Iteration 383, loss = 0.34916119\n",
      "Iteration 384, loss = 0.34873748\n",
      "Iteration 385, loss = 0.34831448\n",
      "Iteration 386, loss = 0.34789217\n",
      "Iteration 387, loss = 0.34747055\n",
      "Iteration 388, loss = 0.34704960\n",
      "Iteration 389, loss = 0.34662933\n",
      "Iteration 390, loss = 0.34620971\n",
      "Iteration 391, loss = 0.34579074\n",
      "Iteration 392, loss = 0.34537242\n",
      "Iteration 393, loss = 0.34495474\n",
      "Iteration 394, loss = 0.34453769\n",
      "Iteration 395, loss = 0.34412127\n",
      "Iteration 396, loss = 0.34370546\n",
      "Iteration 397, loss = 0.34329028\n",
      "Iteration 398, loss = 0.34287571\n",
      "Iteration 399, loss = 0.34246174\n",
      "Iteration 400, loss = 0.34204838\n",
      "Iteration 401, loss = 0.34163563\n",
      "Iteration 402, loss = 0.34122346\n",
      "Iteration 403, loss = 0.34081190\n",
      "Iteration 404, loss = 0.34040092\n",
      "Iteration 405, loss = 0.33999053\n",
      "Iteration 406, loss = 0.33958073\n",
      "Iteration 407, loss = 0.33917151\n",
      "Iteration 408, loss = 0.33876286\n",
      "Iteration 409, loss = 0.33835479\n",
      "Iteration 410, loss = 0.33794730\n",
      "Iteration 411, loss = 0.33754037\n",
      "Iteration 412, loss = 0.33713401\n",
      "Iteration 413, loss = 0.33672822\n",
      "Iteration 414, loss = 0.33632299\n",
      "Iteration 415, loss = 0.33591832\n",
      "Iteration 416, loss = 0.33551421\n",
      "Iteration 417, loss = 0.33511066\n",
      "Iteration 418, loss = 0.33470766\n",
      "Iteration 419, loss = 0.33430522\n",
      "Iteration 420, loss = 0.33390333\n",
      "Iteration 421, loss = 0.33350199\n",
      "Iteration 422, loss = 0.33310120\n",
      "Iteration 423, loss = 0.33270095\n",
      "Iteration 424, loss = 0.33230126\n",
      "Iteration 425, loss = 0.33190213\n",
      "Iteration 426, loss = 0.33150372\n",
      "Iteration 427, loss = 0.33110585\n",
      "Iteration 428, loss = 0.33070851\n",
      "Iteration 429, loss = 0.33031171\n",
      "Iteration 430, loss = 0.32991545\n",
      "Iteration 431, loss = 0.32951973\n",
      "Iteration 432, loss = 0.32912455\n",
      "Iteration 433, loss = 0.32872992\n",
      "Iteration 434, loss = 0.32833582\n",
      "Iteration 435, loss = 0.32794227\n",
      "Iteration 436, loss = 0.32754927\n",
      "Iteration 437, loss = 0.32715739\n",
      "Iteration 438, loss = 0.32676628\n",
      "Iteration 439, loss = 0.32637573\n",
      "Iteration 440, loss = 0.32598573\n",
      "Iteration 441, loss = 0.32559631\n",
      "Iteration 442, loss = 0.32520744\n",
      "Iteration 443, loss = 0.32481911\n",
      "Iteration 444, loss = 0.32443134\n",
      "Iteration 445, loss = 0.32404412\n",
      "Iteration 446, loss = 0.32365744\n",
      "Iteration 447, loss = 0.32327130\n",
      "Iteration 448, loss = 0.32288571\n",
      "Iteration 449, loss = 0.32250065\n",
      "Iteration 450, loss = 0.32211613\n",
      "Iteration 451, loss = 0.32173215\n",
      "Iteration 452, loss = 0.32134869\n",
      "Iteration 453, loss = 0.32096577\n",
      "Iteration 454, loss = 0.32058337\n",
      "Iteration 455, loss = 0.32020150\n",
      "Iteration 456, loss = 0.31982016\n",
      "Iteration 457, loss = 0.31943935\n",
      "Iteration 458, loss = 0.31905906\n",
      "Iteration 459, loss = 0.31867930\n",
      "Iteration 460, loss = 0.31830005\n",
      "Iteration 461, loss = 0.31792133\n",
      "Iteration 462, loss = 0.31754314\n",
      "Iteration 463, loss = 0.31716546\n",
      "Iteration 464, loss = 0.31678830\n",
      "Iteration 465, loss = 0.31641166\n",
      "Iteration 466, loss = 0.31603554\n",
      "Iteration 467, loss = 0.31565993\n",
      "Iteration 468, loss = 0.31528485\n",
      "Iteration 469, loss = 0.31491027\n",
      "Iteration 470, loss = 0.31453621\n",
      "Iteration 471, loss = 0.31416267\n",
      "Iteration 472, loss = 0.31378963\n",
      "Iteration 473, loss = 0.31341711\n",
      "Iteration 474, loss = 0.31304510\n",
      "Iteration 475, loss = 0.31267360\n",
      "Iteration 476, loss = 0.31230261\n",
      "Iteration 477, loss = 0.31193214\n",
      "Iteration 478, loss = 0.31156217\n",
      "Iteration 479, loss = 0.31119271\n",
      "Iteration 480, loss = 0.31082376\n",
      "Iteration 481, loss = 0.31045531\n",
      "Iteration 482, loss = 0.31008738\n",
      "Iteration 483, loss = 0.30971995\n",
      "Iteration 484, loss = 0.30935312\n",
      "Iteration 485, loss = 0.30898677\n",
      "Iteration 486, loss = 0.30862088\n",
      "Iteration 487, loss = 0.30825549\n",
      "Iteration 488, loss = 0.30789065\n",
      "Iteration 489, loss = 0.30752633\n",
      "Iteration 490, loss = 0.30716252\n",
      "Iteration 491, loss = 0.30679919\n",
      "Iteration 492, loss = 0.30643635\n",
      "Iteration 493, loss = 0.30607408\n",
      "Iteration 494, loss = 0.30571222\n",
      "Iteration 495, loss = 0.30535095\n",
      "Iteration 496, loss = 0.30499014\n",
      "Iteration 497, loss = 0.30462981\n",
      "Iteration 498, loss = 0.30426997\n",
      "Iteration 499, loss = 0.30391072\n",
      "Iteration 500, loss = 0.30355187\n",
      "Iteration 501, loss = 0.30319356\n",
      "Iteration 502, loss = 0.30283575\n",
      "Iteration 503, loss = 0.30247842\n",
      "Iteration 504, loss = 0.30212165\n",
      "Iteration 505, loss = 0.30176530\n",
      "Iteration 506, loss = 0.30140951\n",
      "Iteration 507, loss = 0.30105420\n",
      "Iteration 508, loss = 0.30069938\n",
      "Iteration 509, loss = 0.30034505\n",
      "Iteration 510, loss = 0.29999127\n",
      "Iteration 511, loss = 0.29963792\n",
      "Iteration 512, loss = 0.29928510\n",
      "Iteration 513, loss = 0.29893281\n",
      "Iteration 514, loss = 0.29858098\n",
      "Iteration 515, loss = 0.29822965\n",
      "Iteration 516, loss = 0.29787881\n",
      "Iteration 517, loss = 0.29752850\n",
      "Iteration 518, loss = 0.29717865\n",
      "Iteration 519, loss = 0.29682931\n",
      "Iteration 520, loss = 0.29648049\n",
      "Iteration 521, loss = 0.29613214\n",
      "Iteration 522, loss = 0.29578429\n",
      "Iteration 523, loss = 0.29543697\n",
      "Iteration 524, loss = 0.29509009\n",
      "Iteration 525, loss = 0.29474374\n",
      "Iteration 526, loss = 0.29439790\n",
      "Iteration 527, loss = 0.29405253\n",
      "Iteration 528, loss = 0.29370766\n",
      "Iteration 529, loss = 0.29336330\n",
      "Iteration 530, loss = 0.29301942\n",
      "Iteration 531, loss = 0.29267603\n",
      "Iteration 532, loss = 0.29233317\n",
      "Iteration 533, loss = 0.29199077\n",
      "Iteration 534, loss = 0.29164888\n",
      "Iteration 535, loss = 0.29130750\n",
      "Iteration 536, loss = 0.29096659\n",
      "Iteration 537, loss = 0.29062618\n",
      "Iteration 538, loss = 0.29028629\n",
      "Iteration 539, loss = 0.28994687\n",
      "Iteration 540, loss = 0.28960795\n",
      "Iteration 541, loss = 0.28926953\n",
      "Iteration 542, loss = 0.28893161\n",
      "Iteration 543, loss = 0.28859417\n",
      "Iteration 544, loss = 0.28825722\n",
      "Iteration 545, loss = 0.28792080\n",
      "Iteration 546, loss = 0.28758484\n",
      "Iteration 547, loss = 0.28724937\n",
      "Iteration 548, loss = 0.28691445\n",
      "Iteration 549, loss = 0.28657994\n",
      "Iteration 550, loss = 0.28624601\n",
      "Iteration 551, loss = 0.28591253\n",
      "Iteration 552, loss = 0.28557954\n",
      "Iteration 553, loss = 0.28524703\n",
      "Iteration 554, loss = 0.28491505\n",
      "Iteration 555, loss = 0.28458353\n",
      "Iteration 556, loss = 0.28425252\n",
      "Iteration 557, loss = 0.28392202\n",
      "Iteration 558, loss = 0.28359199\n",
      "Iteration 559, loss = 0.28326245\n",
      "Iteration 560, loss = 0.28293344\n",
      "Iteration 561, loss = 0.28260487\n",
      "Iteration 562, loss = 0.28227683\n",
      "Iteration 563, loss = 0.28194928\n",
      "Iteration 564, loss = 0.28162221\n",
      "Iteration 565, loss = 0.28129562\n",
      "Iteration 566, loss = 0.28096958\n",
      "Iteration 567, loss = 0.28064396\n",
      "Iteration 568, loss = 0.28031888\n",
      "Iteration 569, loss = 0.27999428\n",
      "Iteration 570, loss = 0.27967016\n",
      "Iteration 571, loss = 0.27934653\n",
      "Iteration 572, loss = 0.27902375\n",
      "Iteration 573, loss = 0.27870143\n",
      "Iteration 574, loss = 0.27837958\n",
      "Iteration 575, loss = 0.27805821\n",
      "Iteration 576, loss = 0.27773732\n",
      "Iteration 577, loss = 0.27741697\n",
      "Iteration 578, loss = 0.27709716\n",
      "Iteration 579, loss = 0.27677781\n",
      "Iteration 580, loss = 0.27645905\n",
      "Iteration 581, loss = 0.27614070\n",
      "Iteration 582, loss = 0.27582283\n",
      "Iteration 583, loss = 0.27550555\n",
      "Iteration 584, loss = 0.27518865\n",
      "Iteration 585, loss = 0.27487226\n",
      "Iteration 586, loss = 0.27455649\n",
      "Iteration 587, loss = 0.27424112\n",
      "Iteration 588, loss = 0.27392641\n",
      "Iteration 589, loss = 0.27361224\n",
      "Iteration 590, loss = 0.27329851\n",
      "Iteration 591, loss = 0.27298525\n",
      "Iteration 592, loss = 0.27267245\n",
      "Iteration 593, loss = 0.27236024\n",
      "Iteration 594, loss = 0.27204842\n",
      "Iteration 595, loss = 0.27173706\n",
      "Iteration 596, loss = 0.27142630\n",
      "Iteration 597, loss = 0.27111587\n",
      "Iteration 598, loss = 0.27080604\n",
      "Iteration 599, loss = 0.27049666\n",
      "Iteration 600, loss = 0.27018775\n",
      "Iteration 601, loss = 0.26987934\n",
      "Iteration 602, loss = 0.26957144\n",
      "Iteration 603, loss = 0.26926396\n",
      "Iteration 604, loss = 0.26895705\n",
      "Iteration 605, loss = 0.26865055\n",
      "Iteration 606, loss = 0.26834471\n",
      "Iteration 607, loss = 0.26803932\n",
      "Iteration 608, loss = 0.26773427\n",
      "Iteration 609, loss = 0.26742971\n",
      "Iteration 610, loss = 0.26712581\n",
      "Iteration 611, loss = 0.26682225\n",
      "Iteration 612, loss = 0.26651931\n",
      "Iteration 613, loss = 0.26621674\n",
      "Iteration 614, loss = 0.26591475\n",
      "Iteration 615, loss = 0.26561315\n",
      "Iteration 616, loss = 0.26531208\n",
      "Iteration 617, loss = 0.26501151\n",
      "Iteration 618, loss = 0.26471140\n",
      "Iteration 619, loss = 0.26441177\n",
      "Iteration 620, loss = 0.26411262\n",
      "Iteration 621, loss = 0.26381400\n",
      "Iteration 622, loss = 0.26351575\n",
      "Iteration 623, loss = 0.26321814\n",
      "Iteration 624, loss = 0.26292087\n",
      "Iteration 625, loss = 0.26262418\n",
      "Iteration 626, loss = 0.26232787\n",
      "Iteration 627, loss = 0.26203215\n",
      "Iteration 628, loss = 0.26173681\n",
      "Iteration 629, loss = 0.26144202\n",
      "Iteration 630, loss = 0.26114764\n",
      "Iteration 631, loss = 0.26085385\n",
      "Iteration 632, loss = 0.26056042\n",
      "Iteration 633, loss = 0.26026751\n",
      "Iteration 634, loss = 0.25997509\n",
      "Iteration 635, loss = 0.25968313\n",
      "Iteration 636, loss = 0.25939163\n",
      "Iteration 637, loss = 0.25910067\n",
      "Iteration 638, loss = 0.25881012\n",
      "Iteration 639, loss = 0.25852008\n",
      "Iteration 640, loss = 0.25823053\n",
      "Iteration 641, loss = 0.25794142\n",
      "Iteration 642, loss = 0.25765279\n",
      "Iteration 643, loss = 0.25736467\n",
      "Iteration 644, loss = 0.25707699\n",
      "Iteration 645, loss = 0.25678977\n",
      "Iteration 646, loss = 0.25650311\n",
      "Iteration 647, loss = 0.25621677\n",
      "Iteration 648, loss = 0.25593105\n",
      "Iteration 649, loss = 0.25564569\n",
      "Iteration 650, loss = 0.25536093\n",
      "Iteration 651, loss = 0.25507655\n",
      "Iteration 652, loss = 0.25479259\n",
      "Iteration 653, loss = 0.25450919\n",
      "Iteration 654, loss = 0.25422622\n",
      "Iteration 655, loss = 0.25394376\n",
      "Iteration 656, loss = 0.25366172\n",
      "Iteration 657, loss = 0.25338021\n",
      "Iteration 658, loss = 0.25309910\n",
      "Iteration 659, loss = 0.25281850\n",
      "Iteration 660, loss = 0.25253836\n",
      "Iteration 661, loss = 0.25225871\n",
      "Iteration 662, loss = 0.25197947\n",
      "Iteration 663, loss = 0.25170081\n",
      "Iteration 664, loss = 0.25142248\n",
      "Iteration 665, loss = 0.25114469\n",
      "Iteration 666, loss = 0.25086732\n",
      "Iteration 667, loss = 0.25059048\n",
      "Iteration 668, loss = 0.25031404\n",
      "Iteration 669, loss = 0.25003816\n",
      "Iteration 670, loss = 0.24976266\n",
      "Iteration 671, loss = 0.24948758\n",
      "Iteration 672, loss = 0.24921313\n",
      "Iteration 673, loss = 0.24893901\n",
      "Iteration 674, loss = 0.24866532\n",
      "Iteration 675, loss = 0.24839215\n",
      "Iteration 676, loss = 0.24811951\n",
      "Iteration 677, loss = 0.24784722\n",
      "Iteration 678, loss = 0.24757545\n",
      "Iteration 679, loss = 0.24730411\n",
      "Iteration 680, loss = 0.24703326\n",
      "Iteration 681, loss = 0.24676288\n",
      "Iteration 682, loss = 0.24649290\n",
      "Iteration 683, loss = 0.24622346\n",
      "Iteration 684, loss = 0.24595440\n",
      "Iteration 685, loss = 0.24568585\n",
      "Iteration 686, loss = 0.24541772\n",
      "Iteration 687, loss = 0.24515010\n",
      "Iteration 688, loss = 0.24488287\n",
      "Iteration 689, loss = 0.24461614\n",
      "Iteration 690, loss = 0.24434987\n",
      "Iteration 691, loss = 0.24408401\n",
      "Iteration 692, loss = 0.24381867\n",
      "Iteration 693, loss = 0.24355373\n",
      "Iteration 694, loss = 0.24328928\n",
      "Iteration 695, loss = 0.24302526\n",
      "Iteration 696, loss = 0.24276168\n",
      "Iteration 697, loss = 0.24249860\n",
      "Iteration 698, loss = 0.24223596\n",
      "Iteration 699, loss = 0.24197373\n",
      "Iteration 700, loss = 0.24171198\n",
      "Iteration 701, loss = 0.24145064\n",
      "Iteration 702, loss = 0.24118986\n",
      "Iteration 703, loss = 0.24092940\n",
      "Iteration 704, loss = 0.24066947\n",
      "Iteration 705, loss = 0.24040995\n",
      "Iteration 706, loss = 0.24015088\n",
      "Iteration 707, loss = 0.23989232\n",
      "Iteration 708, loss = 0.23963412\n",
      "Iteration 709, loss = 0.23937644\n",
      "Iteration 710, loss = 0.23911914\n",
      "Iteration 711, loss = 0.23886232\n",
      "Iteration 712, loss = 0.23860598\n",
      "Iteration 713, loss = 0.23835002\n",
      "Iteration 714, loss = 0.23809452\n",
      "Iteration 715, loss = 0.23783950\n",
      "Iteration 716, loss = 0.23758488\n",
      "Iteration 717, loss = 0.23733076\n",
      "Iteration 718, loss = 0.23707701\n",
      "Iteration 719, loss = 0.23682374\n",
      "Iteration 720, loss = 0.23657094\n",
      "Iteration 721, loss = 0.23631851\n",
      "Iteration 722, loss = 0.23606659\n",
      "Iteration 723, loss = 0.23581507\n",
      "Iteration 724, loss = 0.23556399\n",
      "Iteration 725, loss = 0.23531339\n",
      "Iteration 726, loss = 0.23506318\n",
      "Iteration 727, loss = 0.23481341\n",
      "Iteration 728, loss = 0.23456413\n",
      "Iteration 729, loss = 0.23431522\n",
      "Iteration 730, loss = 0.23406676\n",
      "Iteration 731, loss = 0.23381882\n",
      "Iteration 732, loss = 0.23357120\n",
      "Iteration 733, loss = 0.23332409\n",
      "Iteration 734, loss = 0.23307739\n",
      "Iteration 735, loss = 0.23283108\n",
      "Iteration 736, loss = 0.23258523\n",
      "Iteration 737, loss = 0.23233985\n",
      "Iteration 738, loss = 0.23209488\n",
      "Iteration 739, loss = 0.23185033\n",
      "Iteration 740, loss = 0.23160623\n",
      "Iteration 741, loss = 0.23136251\n",
      "Iteration 742, loss = 0.23111933\n",
      "Iteration 743, loss = 0.23087646\n",
      "Iteration 744, loss = 0.23063406\n",
      "Iteration 745, loss = 0.23039215\n",
      "Iteration 746, loss = 0.23015058\n",
      "Iteration 747, loss = 0.22990944\n",
      "Iteration 748, loss = 0.22966884\n",
      "Iteration 749, loss = 0.22942856\n",
      "Iteration 750, loss = 0.22918874\n",
      "Iteration 751, loss = 0.22894929\n",
      "Iteration 752, loss = 0.22871038\n",
      "Iteration 753, loss = 0.22847179\n",
      "Iteration 754, loss = 0.22823368\n",
      "Iteration 755, loss = 0.22799596\n",
      "Iteration 756, loss = 0.22775866\n",
      "Iteration 757, loss = 0.22752178\n",
      "Iteration 758, loss = 0.22728535\n",
      "Iteration 759, loss = 0.22704932\n",
      "Iteration 760, loss = 0.22681368\n",
      "Iteration 761, loss = 0.22657857\n",
      "Iteration 762, loss = 0.22634379\n",
      "Iteration 763, loss = 0.22610941\n",
      "Iteration 764, loss = 0.22587549\n",
      "Iteration 765, loss = 0.22564205\n",
      "Iteration 766, loss = 0.22540886\n",
      "Iteration 767, loss = 0.22517621\n",
      "Iteration 768, loss = 0.22494397\n",
      "Iteration 769, loss = 0.22471210\n",
      "Iteration 770, loss = 0.22448068\n",
      "Iteration 771, loss = 0.22424965\n",
      "Iteration 772, loss = 0.22401905\n",
      "Iteration 773, loss = 0.22378886\n",
      "Iteration 774, loss = 0.22355904\n",
      "Iteration 775, loss = 0.22332973\n",
      "Iteration 776, loss = 0.22310073\n",
      "Iteration 777, loss = 0.22287219\n",
      "Iteration 778, loss = 0.22264403\n",
      "Iteration 779, loss = 0.22241633\n",
      "Iteration 780, loss = 0.22218903\n",
      "Iteration 781, loss = 0.22196210\n",
      "Iteration 782, loss = 0.22173556\n",
      "Iteration 783, loss = 0.22150955\n",
      "Iteration 784, loss = 0.22128379\n",
      "Iteration 785, loss = 0.22105849\n",
      "Iteration 786, loss = 0.22083364\n",
      "Iteration 787, loss = 0.22060915\n",
      "Iteration 788, loss = 0.22038508\n",
      "Iteration 789, loss = 0.22016140\n",
      "Iteration 790, loss = 0.21993812\n",
      "Iteration 791, loss = 0.21971531\n",
      "Iteration 792, loss = 0.21949279\n",
      "Iteration 793, loss = 0.21927074\n",
      "Iteration 794, loss = 0.21904907\n",
      "Iteration 795, loss = 0.21882782\n",
      "Iteration 796, loss = 0.21860698\n",
      "Iteration 797, loss = 0.21838650\n",
      "Iteration 798, loss = 0.21816642\n",
      "Iteration 799, loss = 0.21794673\n",
      "Iteration 800, loss = 0.21772751\n",
      "Iteration 801, loss = 0.21750860\n",
      "Iteration 802, loss = 0.21729011\n",
      "Iteration 803, loss = 0.21707202\n",
      "Iteration 804, loss = 0.21685436\n",
      "Iteration 805, loss = 0.21663705\n",
      "Iteration 806, loss = 0.21642015\n",
      "Iteration 807, loss = 0.21620362\n",
      "Iteration 808, loss = 0.21598748\n",
      "Iteration 809, loss = 0.21577177\n",
      "Iteration 810, loss = 0.21555643\n",
      "Iteration 811, loss = 0.21534148\n",
      "Iteration 812, loss = 0.21512691\n",
      "Iteration 813, loss = 0.21491274\n",
      "Iteration 814, loss = 0.21469896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 815, loss = 0.21448556\n",
      "Iteration 816, loss = 0.21427256\n",
      "Iteration 817, loss = 0.21405993\n",
      "Iteration 818, loss = 0.21384768\n",
      "Iteration 819, loss = 0.21363583\n",
      "Iteration 820, loss = 0.21342441\n",
      "Iteration 821, loss = 0.21321328\n",
      "Iteration 822, loss = 0.21300260\n",
      "Iteration 823, loss = 0.21279227\n",
      "Iteration 824, loss = 0.21258232\n",
      "Iteration 825, loss = 0.21237278\n",
      "Iteration 826, loss = 0.21216363\n",
      "Iteration 827, loss = 0.21195483\n",
      "Iteration 828, loss = 0.21174641\n",
      "Iteration 829, loss = 0.21153839\n",
      "Iteration 830, loss = 0.21133070\n",
      "Iteration 831, loss = 0.21112343\n",
      "Iteration 832, loss = 0.21091652\n",
      "Iteration 833, loss = 0.21071004\n",
      "Iteration 834, loss = 0.21050385\n",
      "Iteration 835, loss = 0.21029808\n",
      "Iteration 836, loss = 0.21009268\n",
      "Iteration 837, loss = 0.20988765\n",
      "Iteration 838, loss = 0.20968299\n",
      "Iteration 839, loss = 0.20947872\n",
      "Iteration 840, loss = 0.20927482\n",
      "Iteration 841, loss = 0.20907129\n",
      "Iteration 842, loss = 0.20886812\n",
      "Iteration 843, loss = 0.20866534\n",
      "Iteration 844, loss = 0.20846290\n",
      "Iteration 845, loss = 0.20826083\n",
      "Iteration 846, loss = 0.20805915\n",
      "Iteration 847, loss = 0.20785782\n",
      "Iteration 848, loss = 0.20765686\n",
      "Iteration 849, loss = 0.20745631\n",
      "Iteration 850, loss = 0.20725606\n",
      "Iteration 851, loss = 0.20705622\n",
      "Iteration 852, loss = 0.20685672\n",
      "Iteration 853, loss = 0.20665759\n",
      "Iteration 854, loss = 0.20645882\n",
      "Iteration 855, loss = 0.20626043\n",
      "Iteration 856, loss = 0.20606238\n",
      "Iteration 857, loss = 0.20586470\n",
      "Iteration 858, loss = 0.20566738\n",
      "Iteration 859, loss = 0.20547043\n",
      "Iteration 860, loss = 0.20527384\n",
      "Iteration 861, loss = 0.20507761\n",
      "Iteration 862, loss = 0.20488173\n",
      "Iteration 863, loss = 0.20468620\n",
      "Iteration 864, loss = 0.20449103\n",
      "Iteration 865, loss = 0.20429624\n",
      "Iteration 866, loss = 0.20410177\n",
      "Iteration 867, loss = 0.20390767\n",
      "Iteration 868, loss = 0.20371392\n",
      "Iteration 869, loss = 0.20352054\n",
      "Iteration 870, loss = 0.20332751\n",
      "Iteration 871, loss = 0.20313482\n",
      "Iteration 872, loss = 0.20294249\n",
      "Iteration 873, loss = 0.20275051\n",
      "Iteration 874, loss = 0.20255888\n",
      "Iteration 875, loss = 0.20236760\n",
      "Iteration 876, loss = 0.20217668\n",
      "Iteration 877, loss = 0.20198610\n",
      "Iteration 878, loss = 0.20179587\n",
      "Iteration 879, loss = 0.20160598\n",
      "Iteration 880, loss = 0.20141644\n",
      "Iteration 881, loss = 0.20122726\n",
      "Iteration 882, loss = 0.20103841\n",
      "Iteration 883, loss = 0.20084991\n",
      "Iteration 884, loss = 0.20066175\n",
      "Iteration 885, loss = 0.20047394\n",
      "Iteration 886, loss = 0.20028647\n",
      "Iteration 887, loss = 0.20009935\n",
      "Iteration 888, loss = 0.19991256\n",
      "Iteration 889, loss = 0.19972612\n",
      "Iteration 890, loss = 0.19954002\n",
      "Iteration 891, loss = 0.19935426\n",
      "Iteration 892, loss = 0.19916883\n",
      "Iteration 893, loss = 0.19898375\n",
      "Iteration 894, loss = 0.19879902\n",
      "Iteration 895, loss = 0.19861460\n",
      "Iteration 896, loss = 0.19843053\n",
      "Iteration 897, loss = 0.19824679\n",
      "Iteration 898, loss = 0.19806340\n",
      "Iteration 899, loss = 0.19788033\n",
      "Iteration 900, loss = 0.19769760\n",
      "Iteration 901, loss = 0.19751521\n",
      "Iteration 902, loss = 0.19733315\n",
      "Iteration 903, loss = 0.19715142\n",
      "Iteration 904, loss = 0.19697002\n",
      "Iteration 905, loss = 0.19678896\n",
      "Iteration 906, loss = 0.19660822\n",
      "Iteration 907, loss = 0.19642782\n",
      "Iteration 908, loss = 0.19624774\n",
      "Iteration 909, loss = 0.19606799\n",
      "Iteration 910, loss = 0.19588857\n",
      "Iteration 911, loss = 0.19570948\n",
      "Iteration 912, loss = 0.19553072\n",
      "Iteration 913, loss = 0.19535229\n",
      "Iteration 914, loss = 0.19517417\n",
      "Iteration 915, loss = 0.19499638\n",
      "Iteration 916, loss = 0.19481892\n",
      "Iteration 917, loss = 0.19464178\n",
      "Iteration 918, loss = 0.19446497\n",
      "Iteration 919, loss = 0.19428848\n",
      "Iteration 920, loss = 0.19411230\n",
      "Iteration 921, loss = 0.19393645\n",
      "Iteration 922, loss = 0.19376092\n",
      "Iteration 923, loss = 0.19358572\n",
      "Iteration 924, loss = 0.19341083\n",
      "Iteration 925, loss = 0.19323626\n",
      "Iteration 926, loss = 0.19306201\n",
      "Iteration 927, loss = 0.19288807\n",
      "Iteration 928, loss = 0.19271445\n",
      "Iteration 929, loss = 0.19254115\n",
      "Iteration 930, loss = 0.19236817\n",
      "Iteration 931, loss = 0.19219550\n",
      "Iteration 932, loss = 0.19202314\n",
      "Iteration 933, loss = 0.19185110\n",
      "Iteration 934, loss = 0.19167937\n",
      "Iteration 935, loss = 0.19150795\n",
      "Iteration 936, loss = 0.19133685\n",
      "Iteration 937, loss = 0.19116605\n",
      "Iteration 938, loss = 0.19099557\n",
      "Iteration 939, loss = 0.19082540\n",
      "Iteration 940, loss = 0.19065554\n",
      "Iteration 941, loss = 0.19048599\n",
      "Iteration 942, loss = 0.19031675\n",
      "Iteration 943, loss = 0.19014781\n",
      "Iteration 944, loss = 0.18997918\n",
      "Iteration 945, loss = 0.18981086\n",
      "Iteration 946, loss = 0.18964284\n",
      "Iteration 947, loss = 0.18947513\n",
      "Iteration 948, loss = 0.18930772\n",
      "Iteration 949, loss = 0.18914062\n",
      "Iteration 950, loss = 0.18897382\n",
      "Iteration 951, loss = 0.18880732\n",
      "Iteration 952, loss = 0.18864113\n",
      "Iteration 953, loss = 0.18847524\n",
      "Iteration 954, loss = 0.18830964\n",
      "Iteration 955, loss = 0.18814437\n",
      "Iteration 956, loss = 0.18797939\n",
      "Iteration 957, loss = 0.18781471\n",
      "Iteration 958, loss = 0.18765034\n",
      "Iteration 959, loss = 0.18748626\n",
      "Iteration 960, loss = 0.18732249\n",
      "Iteration 961, loss = 0.18715900\n",
      "Iteration 962, loss = 0.18699582\n",
      "Iteration 963, loss = 0.18683293\n",
      "Iteration 964, loss = 0.18667034\n",
      "Iteration 965, loss = 0.18650804\n",
      "Iteration 966, loss = 0.18634604\n",
      "Iteration 967, loss = 0.18618433\n",
      "Iteration 968, loss = 0.18602291\n",
      "Iteration 969, loss = 0.18586179\n",
      "Iteration 970, loss = 0.18570096\n",
      "Iteration 971, loss = 0.18554042\n",
      "Iteration 972, loss = 0.18538017\n",
      "Iteration 973, loss = 0.18522021\n",
      "Iteration 974, loss = 0.18506054\n",
      "Iteration 975, loss = 0.18490115\n",
      "Iteration 976, loss = 0.18474206\n",
      "Iteration 977, loss = 0.18458325\n",
      "Iteration 978, loss = 0.18442474\n",
      "Iteration 979, loss = 0.18426651\n",
      "Iteration 980, loss = 0.18410856\n",
      "Iteration 981, loss = 0.18395091\n",
      "Iteration 982, loss = 0.18379353\n",
      "Iteration 983, loss = 0.18363644\n",
      "Iteration 984, loss = 0.18347964\n",
      "Iteration 985, loss = 0.18332312\n",
      "Iteration 986, loss = 0.18316688\n",
      "Iteration 987, loss = 0.18301092\n",
      "Iteration 988, loss = 0.18285525\n",
      "Iteration 989, loss = 0.18269985\n",
      "Iteration 990, loss = 0.18254473\n",
      "Iteration 991, loss = 0.18238990\n",
      "Iteration 992, loss = 0.18223534\n",
      "Iteration 993, loss = 0.18208106\n",
      "Iteration 994, loss = 0.18192706\n",
      "Iteration 995, loss = 0.18177334\n",
      "Iteration 996, loss = 0.18161989\n",
      "Iteration 997, loss = 0.18146673\n",
      "Iteration 998, loss = 0.18131384\n",
      "Iteration 999, loss = 0.18116123\n",
      "Iteration 1000, loss = 0.18100890\n",
      "Iteration 1, loss = 1.35771515\n",
      "Iteration 2, loss = 1.31975062\n",
      "Iteration 3, loss = 1.26863347\n",
      "Iteration 4, loss = 1.20866780\n",
      "Iteration 5, loss = 1.14427403\n",
      "Iteration 6, loss = 1.07938645\n",
      "Iteration 7, loss = 1.01738115\n",
      "Iteration 8, loss = 0.96093608\n",
      "Iteration 9, loss = 0.91178455\n",
      "Iteration 10, loss = 0.87063331\n",
      "Iteration 11, loss = 0.83738992\n",
      "Iteration 12, loss = 0.81125295\n",
      "Iteration 13, loss = 0.79125128\n",
      "Iteration 14, loss = 0.77600337\n",
      "Iteration 15, loss = 0.76451479\n",
      "Iteration 16, loss = 0.75573784\n",
      "Iteration 17, loss = 0.74854392\n",
      "Iteration 18, loss = 0.74269769\n",
      "Iteration 19, loss = 0.73779653\n",
      "Iteration 20, loss = 0.73361557\n",
      "Iteration 21, loss = 0.72992070\n",
      "Iteration 22, loss = 0.72649481\n",
      "Iteration 23, loss = 0.72260664\n",
      "Iteration 24, loss = 0.71815908\n",
      "Iteration 25, loss = 0.71321542\n",
      "Iteration 26, loss = 0.70777862\n",
      "Iteration 27, loss = 0.70208874\n",
      "Iteration 28, loss = 0.69628204\n",
      "Iteration 29, loss = 0.69037184\n",
      "Iteration 30, loss = 0.68455621\n",
      "Iteration 31, loss = 0.67894645\n",
      "Iteration 32, loss = 0.67366091\n",
      "Iteration 33, loss = 0.66877354\n",
      "Iteration 34, loss = 0.66426474\n",
      "Iteration 35, loss = 0.66013927\n",
      "Iteration 36, loss = 0.65640425\n",
      "Iteration 37, loss = 0.65298539\n",
      "Iteration 38, loss = 0.64986078\n",
      "Iteration 39, loss = 0.64697358\n",
      "Iteration 40, loss = 0.64426236\n",
      "Iteration 41, loss = 0.64167719\n",
      "Iteration 42, loss = 0.63918213\n",
      "Iteration 43, loss = 0.63674773\n",
      "Iteration 44, loss = 0.63434665\n",
      "Iteration 45, loss = 0.63196471\n",
      "Iteration 46, loss = 0.62958669\n",
      "Iteration 47, loss = 0.62720331\n",
      "Iteration 48, loss = 0.62480863\n",
      "Iteration 49, loss = 0.62240445\n",
      "Iteration 50, loss = 0.61999554\n",
      "Iteration 51, loss = 0.61758839\n",
      "Iteration 52, loss = 0.61519099\n",
      "Iteration 53, loss = 0.61281442\n",
      "Iteration 54, loss = 0.61046211\n",
      "Iteration 55, loss = 0.60813910\n",
      "Iteration 56, loss = 0.60585064\n",
      "Iteration 57, loss = 0.60360063\n",
      "Iteration 58, loss = 0.60139160\n",
      "Iteration 59, loss = 0.59922488\n",
      "Iteration 60, loss = 0.59710132\n",
      "Iteration 61, loss = 0.59501880\n",
      "Iteration 62, loss = 0.59297523\n",
      "Iteration 63, loss = 0.59096902\n",
      "Iteration 64, loss = 0.58899763\n",
      "Iteration 65, loss = 0.58705863\n",
      "Iteration 66, loss = 0.58515001\n",
      "Iteration 67, loss = 0.58327289\n",
      "Iteration 68, loss = 0.58142543\n",
      "Iteration 69, loss = 0.57960294\n",
      "Iteration 70, loss = 0.57780391\n",
      "Iteration 71, loss = 0.57602709\n",
      "Iteration 72, loss = 0.57427163\n",
      "Iteration 73, loss = 0.57253932\n",
      "Iteration 74, loss = 0.57082733\n",
      "Iteration 75, loss = 0.56913522\n",
      "Iteration 76, loss = 0.56746264\n",
      "Iteration 77, loss = 0.56580933\n",
      "Iteration 78, loss = 0.56417506\n",
      "Iteration 79, loss = 0.56255965\n",
      "Iteration 80, loss = 0.56096374\n",
      "Iteration 81, loss = 0.55938967\n",
      "Iteration 82, loss = 0.55783392\n",
      "Iteration 83, loss = 0.55629937\n",
      "Iteration 84, loss = 0.55478520\n",
      "Iteration 85, loss = 0.55328885\n",
      "Iteration 86, loss = 0.55181067\n",
      "Iteration 87, loss = 0.55035072\n",
      "Iteration 88, loss = 0.54890753\n",
      "Iteration 89, loss = 0.54748076\n",
      "Iteration 90, loss = 0.54607003\n",
      "Iteration 91, loss = 0.54467502\n",
      "Iteration 92, loss = 0.54329536\n",
      "Iteration 93, loss = 0.54193068\n",
      "Iteration 94, loss = 0.54058068\n",
      "Iteration 95, loss = 0.53924503\n",
      "Iteration 96, loss = 0.53792346\n",
      "Iteration 97, loss = 0.53661567\n",
      "Iteration 98, loss = 0.53532138\n",
      "Iteration 99, loss = 0.53404033\n",
      "Iteration 100, loss = 0.53277226\n",
      "Iteration 101, loss = 0.53151692\n",
      "Iteration 102, loss = 0.53027407\n",
      "Iteration 103, loss = 0.52904346\n",
      "Iteration 104, loss = 0.52782486\n",
      "Iteration 105, loss = 0.52661804\n",
      "Iteration 106, loss = 0.52542278\n",
      "Iteration 107, loss = 0.52423884\n",
      "Iteration 108, loss = 0.52306600\n",
      "Iteration 109, loss = 0.52190407\n",
      "Iteration 110, loss = 0.52075366\n",
      "Iteration 111, loss = 0.51961351\n",
      "Iteration 112, loss = 0.51848341\n",
      "Iteration 113, loss = 0.51736316\n",
      "Iteration 114, loss = 0.51625411\n",
      "Iteration 115, loss = 0.51515562\n",
      "Iteration 116, loss = 0.51406668\n",
      "Iteration 117, loss = 0.51298710\n",
      "Iteration 118, loss = 0.51191670\n",
      "Iteration 119, loss = 0.51085531\n",
      "Iteration 120, loss = 0.50980321\n",
      "Iteration 121, loss = 0.50876000\n",
      "Iteration 122, loss = 0.50772534\n",
      "Iteration 123, loss = 0.50669908\n",
      "Iteration 124, loss = 0.50568105\n",
      "Iteration 125, loss = 0.50467110\n",
      "Iteration 126, loss = 0.50366909\n",
      "Iteration 127, loss = 0.50267486\n",
      "Iteration 128, loss = 0.50168828\n",
      "Iteration 129, loss = 0.50070943\n",
      "Iteration 130, loss = 0.49974052\n",
      "Iteration 131, loss = 0.49877906\n",
      "Iteration 132, loss = 0.49782491\n",
      "Iteration 133, loss = 0.49687795\n",
      "Iteration 134, loss = 0.49594001\n",
      "Iteration 135, loss = 0.49501047\n",
      "Iteration 136, loss = 0.49408943\n",
      "Iteration 137, loss = 0.49317659\n",
      "Iteration 138, loss = 0.49227040\n",
      "Iteration 139, loss = 0.49137144\n",
      "Iteration 140, loss = 0.49047932\n",
      "Iteration 141, loss = 0.48959355\n",
      "Iteration 142, loss = 0.48871411\n",
      "Iteration 143, loss = 0.48784096\n",
      "Iteration 144, loss = 0.48697408\n",
      "Iteration 145, loss = 0.48611339\n",
      "Iteration 146, loss = 0.48525886\n",
      "Iteration 147, loss = 0.48441039\n",
      "Iteration 148, loss = 0.48356793\n",
      "Iteration 149, loss = 0.48273139\n",
      "Iteration 150, loss = 0.48190068\n",
      "Iteration 151, loss = 0.48107573\n",
      "Iteration 152, loss = 0.48025642\n",
      "Iteration 153, loss = 0.47944269\n",
      "Iteration 154, loss = 0.47863442\n",
      "Iteration 155, loss = 0.47783154\n",
      "Iteration 156, loss = 0.47703406\n",
      "Iteration 157, loss = 0.47624192\n",
      "Iteration 158, loss = 0.47545493\n",
      "Iteration 159, loss = 0.47467303\n",
      "Iteration 160, loss = 0.47389615\n",
      "Iteration 161, loss = 0.47312420\n",
      "Iteration 162, loss = 0.47235712\n",
      "Iteration 163, loss = 0.47159482\n",
      "Iteration 164, loss = 0.47083723\n",
      "Iteration 165, loss = 0.47008428\n",
      "Iteration 166, loss = 0.46933589\n",
      "Iteration 167, loss = 0.46859198\n",
      "Iteration 168, loss = 0.46785247\n",
      "Iteration 169, loss = 0.46711730\n",
      "Iteration 170, loss = 0.46638639\n",
      "Iteration 171, loss = 0.46565966\n",
      "Iteration 172, loss = 0.46493706\n",
      "Iteration 173, loss = 0.46421850\n",
      "Iteration 174, loss = 0.46350392\n",
      "Iteration 175, loss = 0.46279324\n",
      "Iteration 176, loss = 0.46208642\n",
      "Iteration 177, loss = 0.46138337\n",
      "Iteration 178, loss = 0.46068403\n",
      "Iteration 179, loss = 0.45998835\n",
      "Iteration 180, loss = 0.45929626\n",
      "Iteration 181, loss = 0.45860874\n",
      "Iteration 182, loss = 0.45792501\n",
      "Iteration 183, loss = 0.45724470\n",
      "Iteration 184, loss = 0.45656777\n",
      "Iteration 185, loss = 0.45589420\n",
      "Iteration 186, loss = 0.45522395\n",
      "Iteration 187, loss = 0.45455698\n",
      "Iteration 188, loss = 0.45389327\n",
      "Iteration 189, loss = 0.45323277\n",
      "Iteration 190, loss = 0.45257543\n",
      "Iteration 191, loss = 0.45192121\n",
      "Iteration 192, loss = 0.45127065\n",
      "Iteration 193, loss = 0.45062357\n",
      "Iteration 194, loss = 0.44997954\n",
      "Iteration 195, loss = 0.44933852\n",
      "Iteration 196, loss = 0.44870046\n",
      "Iteration 197, loss = 0.44806532\n",
      "Iteration 198, loss = 0.44743305\n",
      "Iteration 199, loss = 0.44680361\n",
      "Iteration 200, loss = 0.44617696\n",
      "Iteration 201, loss = 0.44555306\n",
      "Iteration 202, loss = 0.44493186\n",
      "Iteration 203, loss = 0.44431334\n",
      "Iteration 204, loss = 0.44369744\n",
      "Iteration 205, loss = 0.44308414\n",
      "Iteration 206, loss = 0.44247339\n",
      "Iteration 207, loss = 0.44186517\n",
      "Iteration 208, loss = 0.44125942\n",
      "Iteration 209, loss = 0.44065612\n",
      "Iteration 210, loss = 0.44005523\n",
      "Iteration 211, loss = 0.43945672\n",
      "Iteration 212, loss = 0.43886055\n",
      "Iteration 213, loss = 0.43826669\n",
      "Iteration 214, loss = 0.43767510\n",
      "Iteration 215, loss = 0.43708575\n",
      "Iteration 216, loss = 0.43649861\n",
      "Iteration 217, loss = 0.43591365\n",
      "Iteration 218, loss = 0.43533082\n",
      "Iteration 219, loss = 0.43475011\n",
      "Iteration 220, loss = 0.43417149\n",
      "Iteration 221, loss = 0.43359491\n",
      "Iteration 222, loss = 0.43302036\n",
      "Iteration 223, loss = 0.43244780\n",
      "Iteration 224, loss = 0.43187721\n",
      "Iteration 225, loss = 0.43130856\n",
      "Iteration 226, loss = 0.43074182\n",
      "Iteration 227, loss = 0.43017696\n",
      "Iteration 228, loss = 0.42961396\n",
      "Iteration 229, loss = 0.42905279\n",
      "Iteration 230, loss = 0.42849343\n",
      "Iteration 231, loss = 0.42793585\n",
      "Iteration 232, loss = 0.42738003\n",
      "Iteration 233, loss = 0.42682594\n",
      "Iteration 234, loss = 0.42627357\n",
      "Iteration 235, loss = 0.42572288\n",
      "Iteration 236, loss = 0.42517386\n",
      "Iteration 237, loss = 0.42462659\n",
      "Iteration 238, loss = 0.42408234\n",
      "Iteration 239, loss = 0.42353978\n",
      "Iteration 240, loss = 0.42299890\n",
      "Iteration 241, loss = 0.42245968\n",
      "Iteration 242, loss = 0.42192209\n",
      "Iteration 243, loss = 0.42138611\n",
      "Iteration 244, loss = 0.42085172\n",
      "Iteration 245, loss = 0.42031890\n",
      "Iteration 246, loss = 0.41978762\n",
      "Iteration 247, loss = 0.41925788\n",
      "Iteration 248, loss = 0.41873024\n",
      "Iteration 249, loss = 0.41820465\n",
      "Iteration 250, loss = 0.41768059\n",
      "Iteration 251, loss = 0.41715803\n",
      "Iteration 252, loss = 0.41663697\n",
      "Iteration 253, loss = 0.41611739\n",
      "Iteration 254, loss = 0.41559927\n",
      "Iteration 255, loss = 0.41508345\n",
      "Iteration 256, loss = 0.41456905\n",
      "Iteration 257, loss = 0.41405606\n",
      "Iteration 258, loss = 0.41354460\n",
      "Iteration 259, loss = 0.41303587\n",
      "Iteration 260, loss = 0.41252847\n",
      "Iteration 261, loss = 0.41202243\n",
      "Iteration 262, loss = 0.41151775\n",
      "Iteration 263, loss = 0.41101443\n",
      "Iteration 264, loss = 0.41051248\n",
      "Iteration 265, loss = 0.41001189\n",
      "Iteration 266, loss = 0.40951266\n",
      "Iteration 267, loss = 0.40901478\n",
      "Iteration 268, loss = 0.40851824\n",
      "Iteration 269, loss = 0.40802302\n",
      "Iteration 270, loss = 0.40752912\n",
      "Iteration 271, loss = 0.40703652\n",
      "Iteration 272, loss = 0.40654520\n",
      "Iteration 273, loss = 0.40605516\n",
      "Iteration 274, loss = 0.40556637\n",
      "Iteration 275, loss = 0.40507882\n",
      "Iteration 276, loss = 0.40459251\n",
      "Iteration 277, loss = 0.40410741\n",
      "Iteration 278, loss = 0.40362352\n",
      "Iteration 279, loss = 0.40314083\n",
      "Iteration 280, loss = 0.40265932\n",
      "Iteration 281, loss = 0.40217898\n",
      "Iteration 282, loss = 0.40169981\n",
      "Iteration 283, loss = 0.40122179\n",
      "Iteration 284, loss = 0.40074491\n",
      "Iteration 285, loss = 0.40026915\n",
      "Iteration 286, loss = 0.39979452\n",
      "Iteration 287, loss = 0.39932099\n",
      "Iteration 288, loss = 0.39884855\n",
      "Iteration 289, loss = 0.39837720\n",
      "Iteration 290, loss = 0.39790691\n",
      "Iteration 291, loss = 0.39743769\n",
      "Iteration 292, loss = 0.39696952\n",
      "Iteration 293, loss = 0.39650238\n",
      "Iteration 294, loss = 0.39603627\n",
      "Iteration 295, loss = 0.39557118\n",
      "Iteration 296, loss = 0.39510709\n",
      "Iteration 297, loss = 0.39464399\n",
      "Iteration 298, loss = 0.39418192\n",
      "Iteration 299, loss = 0.39372140\n",
      "Iteration 300, loss = 0.39326187\n",
      "Iteration 301, loss = 0.39280334\n",
      "Iteration 302, loss = 0.39234578\n",
      "Iteration 303, loss = 0.39188920\n",
      "Iteration 304, loss = 0.39143359\n",
      "Iteration 305, loss = 0.39097893\n",
      "Iteration 306, loss = 0.39052524\n",
      "Iteration 307, loss = 0.39007248\n",
      "Iteration 308, loss = 0.38962067\n",
      "Iteration 309, loss = 0.38916978\n",
      "Iteration 310, loss = 0.38871982\n",
      "Iteration 311, loss = 0.38827078\n",
      "Iteration 312, loss = 0.38782264\n",
      "Iteration 313, loss = 0.38737540\n",
      "Iteration 314, loss = 0.38692905\n",
      "Iteration 315, loss = 0.38648358\n",
      "Iteration 316, loss = 0.38603899\n",
      "Iteration 317, loss = 0.38559527\n",
      "Iteration 318, loss = 0.38515241\n",
      "Iteration 319, loss = 0.38471040\n",
      "Iteration 320, loss = 0.38426923\n",
      "Iteration 321, loss = 0.38382891\n",
      "Iteration 322, loss = 0.38338941\n",
      "Iteration 323, loss = 0.38295074\n",
      "Iteration 324, loss = 0.38251288\n",
      "Iteration 325, loss = 0.38207583\n",
      "Iteration 326, loss = 0.38163958\n",
      "Iteration 327, loss = 0.38120413\n",
      "Iteration 328, loss = 0.38076947\n",
      "Iteration 329, loss = 0.38033559\n",
      "Iteration 330, loss = 0.37990249\n",
      "Iteration 331, loss = 0.37947015\n",
      "Iteration 332, loss = 0.37903858\n",
      "Iteration 333, loss = 0.37860777\n",
      "Iteration 334, loss = 0.37817771\n",
      "Iteration 335, loss = 0.37774839\n",
      "Iteration 336, loss = 0.37731982\n",
      "Iteration 337, loss = 0.37689198\n",
      "Iteration 338, loss = 0.37646487\n",
      "Iteration 339, loss = 0.37603848\n",
      "Iteration 340, loss = 0.37561281\n",
      "Iteration 341, loss = 0.37518786\n",
      "Iteration 342, loss = 0.37476361\n",
      "Iteration 343, loss = 0.37434007\n",
      "Iteration 344, loss = 0.37391723\n",
      "Iteration 345, loss = 0.37349508\n",
      "Iteration 346, loss = 0.37307362\n",
      "Iteration 347, loss = 0.37265285\n",
      "Iteration 348, loss = 0.37223276\n",
      "Iteration 349, loss = 0.37181334\n",
      "Iteration 350, loss = 0.37139525\n",
      "Iteration 351, loss = 0.37097877\n",
      "Iteration 352, loss = 0.37056298\n",
      "Iteration 353, loss = 0.37014784\n",
      "Iteration 354, loss = 0.36973334\n",
      "Iteration 355, loss = 0.36931951\n",
      "Iteration 356, loss = 0.36890633\n",
      "Iteration 357, loss = 0.36849383\n",
      "Iteration 358, loss = 0.36808200\n",
      "Iteration 359, loss = 0.36767085\n",
      "Iteration 360, loss = 0.36726038\n",
      "Iteration 361, loss = 0.36685060\n",
      "Iteration 362, loss = 0.36644149\n",
      "Iteration 363, loss = 0.36603307\n",
      "Iteration 364, loss = 0.36562532\n",
      "Iteration 365, loss = 0.36521826\n",
      "Iteration 366, loss = 0.36481188\n",
      "Iteration 367, loss = 0.36440618\n",
      "Iteration 368, loss = 0.36400115\n",
      "Iteration 369, loss = 0.36359679\n",
      "Iteration 370, loss = 0.36319310\n",
      "Iteration 371, loss = 0.36279007\n",
      "Iteration 372, loss = 0.36238771\n",
      "Iteration 373, loss = 0.36198601\n",
      "Iteration 374, loss = 0.36158496\n",
      "Iteration 375, loss = 0.36118456\n",
      "Iteration 376, loss = 0.36078480\n",
      "Iteration 377, loss = 0.36038569\n",
      "Iteration 378, loss = 0.35998722\n",
      "Iteration 379, loss = 0.35958939\n",
      "Iteration 380, loss = 0.35919218\n",
      "Iteration 381, loss = 0.35879561\n",
      "Iteration 382, loss = 0.35839966\n",
      "Iteration 383, loss = 0.35800433\n",
      "Iteration 384, loss = 0.35760962\n",
      "Iteration 385, loss = 0.35721553\n",
      "Iteration 386, loss = 0.35682205\n",
      "Iteration 387, loss = 0.35642917\n",
      "Iteration 388, loss = 0.35603690\n",
      "Iteration 389, loss = 0.35564540\n",
      "Iteration 390, loss = 0.35525646\n",
      "Iteration 391, loss = 0.35486817\n",
      "Iteration 392, loss = 0.35448052\n",
      "Iteration 393, loss = 0.35409351\n",
      "Iteration 394, loss = 0.35370714\n",
      "Iteration 395, loss = 0.35332139\n",
      "Iteration 396, loss = 0.35293627\n",
      "Iteration 397, loss = 0.35255176\n",
      "Iteration 398, loss = 0.35216786\n",
      "Iteration 399, loss = 0.35178456\n",
      "Iteration 400, loss = 0.35140186\n",
      "Iteration 401, loss = 0.35101974\n",
      "Iteration 402, loss = 0.35063820\n",
      "Iteration 403, loss = 0.35025724\n",
      "Iteration 404, loss = 0.34987684\n",
      "Iteration 405, loss = 0.34949701\n",
      "Iteration 406, loss = 0.34911774\n",
      "Iteration 407, loss = 0.34873903\n",
      "Iteration 408, loss = 0.34836169\n",
      "Iteration 409, loss = 0.34798524\n",
      "Iteration 410, loss = 0.34760930\n",
      "Iteration 411, loss = 0.34723388\n",
      "Iteration 412, loss = 0.34685898\n",
      "Iteration 413, loss = 0.34648485\n",
      "Iteration 414, loss = 0.34611125\n",
      "Iteration 415, loss = 0.34573816\n",
      "Iteration 416, loss = 0.34536561\n",
      "Iteration 417, loss = 0.34499358\n",
      "Iteration 418, loss = 0.34462209\n",
      "Iteration 419, loss = 0.34425112\n",
      "Iteration 420, loss = 0.34388068\n",
      "Iteration 421, loss = 0.34351078\n",
      "Iteration 422, loss = 0.34314140\n",
      "Iteration 423, loss = 0.34277255\n",
      "Iteration 424, loss = 0.34240422\n",
      "Iteration 425, loss = 0.34203643\n",
      "Iteration 426, loss = 0.34166915\n",
      "Iteration 427, loss = 0.34130241\n",
      "Iteration 428, loss = 0.34093618\n",
      "Iteration 429, loss = 0.34057048\n",
      "Iteration 430, loss = 0.34020530\n",
      "Iteration 431, loss = 0.33984064\n",
      "Iteration 432, loss = 0.33947649\n",
      "Iteration 433, loss = 0.33911287\n",
      "Iteration 434, loss = 0.33874976\n",
      "Iteration 435, loss = 0.33838716\n",
      "Iteration 436, loss = 0.33802508\n",
      "Iteration 437, loss = 0.33766350\n",
      "Iteration 438, loss = 0.33730244\n",
      "Iteration 439, loss = 0.33694188\n",
      "Iteration 440, loss = 0.33658183\n",
      "Iteration 441, loss = 0.33622228\n",
      "Iteration 442, loss = 0.33586324\n",
      "Iteration 443, loss = 0.33550470\n",
      "Iteration 444, loss = 0.33514665\n",
      "Iteration 445, loss = 0.33478911\n",
      "Iteration 446, loss = 0.33443207\n",
      "Iteration 447, loss = 0.33407552\n",
      "Iteration 448, loss = 0.33371947\n",
      "Iteration 449, loss = 0.33336392\n",
      "Iteration 450, loss = 0.33300886\n",
      "Iteration 451, loss = 0.33265429\n",
      "Iteration 452, loss = 0.33230022\n",
      "Iteration 453, loss = 0.33194663\n",
      "Iteration 454, loss = 0.33159354\n",
      "Iteration 455, loss = 0.33124094\n",
      "Iteration 456, loss = 0.33088882\n",
      "Iteration 457, loss = 0.33053720\n",
      "Iteration 458, loss = 0.33018606\n",
      "Iteration 459, loss = 0.32983540\n",
      "Iteration 460, loss = 0.32948524\n",
      "Iteration 461, loss = 0.32913555\n",
      "Iteration 462, loss = 0.32878636\n",
      "Iteration 463, loss = 0.32843764\n",
      "Iteration 464, loss = 0.32808941\n",
      "Iteration 465, loss = 0.32774166\n",
      "Iteration 466, loss = 0.32739440\n",
      "Iteration 467, loss = 0.32704761\n",
      "Iteration 468, loss = 0.32670131\n",
      "Iteration 469, loss = 0.32635548\n",
      "Iteration 470, loss = 0.32601014\n",
      "Iteration 471, loss = 0.32566527\n",
      "Iteration 472, loss = 0.32532088\n",
      "Iteration 473, loss = 0.32497697\n",
      "Iteration 474, loss = 0.32463354\n",
      "Iteration 475, loss = 0.32429059\n",
      "Iteration 476, loss = 0.32394811\n",
      "Iteration 477, loss = 0.32360611\n",
      "Iteration 478, loss = 0.32326458\n",
      "Iteration 479, loss = 0.32292353\n",
      "Iteration 480, loss = 0.32258295\n",
      "Iteration 481, loss = 0.32224285\n",
      "Iteration 482, loss = 0.32190322\n",
      "Iteration 483, loss = 0.32156406\n",
      "Iteration 484, loss = 0.32122538\n",
      "Iteration 485, loss = 0.32088717\n",
      "Iteration 486, loss = 0.32054943\n",
      "Iteration 487, loss = 0.32021217\n",
      "Iteration 488, loss = 0.31987554\n",
      "Iteration 489, loss = 0.31953945\n",
      "Iteration 490, loss = 0.31920376\n",
      "Iteration 491, loss = 0.31886848\n",
      "Iteration 492, loss = 0.31853364\n",
      "Iteration 493, loss = 0.31819923\n",
      "Iteration 494, loss = 0.31786551\n",
      "Iteration 495, loss = 0.31753219\n",
      "Iteration 496, loss = 0.31719931\n",
      "Iteration 497, loss = 0.31686687\n",
      "Iteration 498, loss = 0.31653488\n",
      "Iteration 499, loss = 0.31620337\n",
      "Iteration 500, loss = 0.31587236\n",
      "Iteration 501, loss = 0.31554186\n",
      "Iteration 502, loss = 0.31521174\n",
      "Iteration 503, loss = 0.31488218\n",
      "Iteration 504, loss = 0.31455301\n",
      "Iteration 505, loss = 0.31422435\n",
      "Iteration 506, loss = 0.31389616\n",
      "Iteration 507, loss = 0.31356842\n",
      "Iteration 508, loss = 0.31324116\n",
      "Iteration 509, loss = 0.31291438\n",
      "Iteration 510, loss = 0.31258803\n",
      "Iteration 511, loss = 0.31226223\n",
      "Iteration 512, loss = 0.31193677\n",
      "Iteration 513, loss = 0.31161191\n",
      "Iteration 514, loss = 0.31128744\n",
      "Iteration 515, loss = 0.31096342\n",
      "Iteration 516, loss = 0.31063986\n",
      "Iteration 517, loss = 0.31031682\n",
      "Iteration 518, loss = 0.30999419\n",
      "Iteration 519, loss = 0.30967211\n",
      "Iteration 520, loss = 0.30935037\n",
      "Iteration 521, loss = 0.30902921\n",
      "Iteration 522, loss = 0.30870846\n",
      "Iteration 523, loss = 0.30838814\n",
      "Iteration 524, loss = 0.30806832\n",
      "Iteration 525, loss = 0.30774895\n",
      "Iteration 526, loss = 0.30743006\n",
      "Iteration 527, loss = 0.30711163\n",
      "Iteration 528, loss = 0.30679364\n",
      "Iteration 529, loss = 0.30647616\n",
      "Iteration 530, loss = 0.30615908\n",
      "Iteration 531, loss = 0.30584251\n",
      "Iteration 532, loss = 0.30552638\n",
      "Iteration 533, loss = 0.30521069\n",
      "Iteration 534, loss = 0.30489553\n",
      "Iteration 535, loss = 0.30458075\n",
      "Iteration 536, loss = 0.30426649\n",
      "Iteration 537, loss = 0.30395266\n",
      "Iteration 538, loss = 0.30363928\n",
      "Iteration 539, loss = 0.30332642\n",
      "Iteration 540, loss = 0.30301394\n",
      "Iteration 541, loss = 0.30270199\n",
      "Iteration 542, loss = 0.30239045\n",
      "Iteration 543, loss = 0.30207970\n",
      "Iteration 544, loss = 0.30176947\n",
      "Iteration 545, loss = 0.30145958\n",
      "Iteration 546, loss = 0.30115027\n",
      "Iteration 547, loss = 0.30084134\n",
      "Iteration 548, loss = 0.30053285\n",
      "Iteration 549, loss = 0.30022484\n",
      "Iteration 550, loss = 0.29991729\n",
      "Iteration 551, loss = 0.29961021\n",
      "Iteration 552, loss = 0.29930359\n",
      "Iteration 553, loss = 0.29899740\n",
      "Iteration 554, loss = 0.29869175\n",
      "Iteration 555, loss = 0.29838645\n",
      "Iteration 556, loss = 0.29808172\n",
      "Iteration 557, loss = 0.29777737\n",
      "Iteration 558, loss = 0.29747350\n",
      "Iteration 559, loss = 0.29717016\n",
      "Iteration 560, loss = 0.29686721\n",
      "Iteration 561, loss = 0.29656471\n",
      "Iteration 562, loss = 0.29626277\n",
      "Iteration 563, loss = 0.29596117\n",
      "Iteration 564, loss = 0.29566012\n",
      "Iteration 565, loss = 0.29535948\n",
      "Iteration 566, loss = 0.29505930\n",
      "Iteration 567, loss = 0.29475965\n",
      "Iteration 568, loss = 0.29446036\n",
      "Iteration 569, loss = 0.29416163\n",
      "Iteration 570, loss = 0.29386330\n",
      "Iteration 571, loss = 0.29356542\n",
      "Iteration 572, loss = 0.29326802\n",
      "Iteration 573, loss = 0.29297107\n",
      "Iteration 574, loss = 0.29267459\n",
      "Iteration 575, loss = 0.29237858\n",
      "Iteration 576, loss = 0.29208299\n",
      "Iteration 577, loss = 0.29178790\n",
      "Iteration 578, loss = 0.29149324\n",
      "Iteration 579, loss = 0.29119903\n",
      "Iteration 580, loss = 0.29090533\n",
      "Iteration 581, loss = 0.29061204\n",
      "Iteration 582, loss = 0.29031918\n",
      "Iteration 583, loss = 0.29002688\n",
      "Iteration 584, loss = 0.28973491\n",
      "Iteration 585, loss = 0.28944348\n",
      "Iteration 586, loss = 0.28915247\n",
      "Iteration 587, loss = 0.28886189\n",
      "Iteration 588, loss = 0.28857183\n",
      "Iteration 589, loss = 0.28828215\n",
      "Iteration 590, loss = 0.28799298\n",
      "Iteration 591, loss = 0.28770425\n",
      "Iteration 592, loss = 0.28741595\n",
      "Iteration 593, loss = 0.28712814\n",
      "Iteration 594, loss = 0.28684075\n",
      "Iteration 595, loss = 0.28655381\n",
      "Iteration 596, loss = 0.28626738\n",
      "Iteration 597, loss = 0.28598135\n",
      "Iteration 598, loss = 0.28569575\n",
      "Iteration 599, loss = 0.28541068\n",
      "Iteration 600, loss = 0.28512597\n",
      "Iteration 601, loss = 0.28484179\n",
      "Iteration 602, loss = 0.28455801\n",
      "Iteration 603, loss = 0.28427467\n",
      "Iteration 604, loss = 0.28399182\n",
      "Iteration 605, loss = 0.28370939\n",
      "Iteration 606, loss = 0.28342742\n",
      "Iteration 607, loss = 0.28314595\n",
      "Iteration 608, loss = 0.28286489\n",
      "Iteration 609, loss = 0.28258427\n",
      "Iteration 610, loss = 0.28230411\n",
      "Iteration 611, loss = 0.28202438\n",
      "Iteration 612, loss = 0.28174516\n",
      "Iteration 613, loss = 0.28146631\n",
      "Iteration 614, loss = 0.28118798\n",
      "Iteration 615, loss = 0.28091006\n",
      "Iteration 616, loss = 0.28063258\n",
      "Iteration 617, loss = 0.28035556\n",
      "Iteration 618, loss = 0.28007900\n",
      "Iteration 619, loss = 0.27980285\n",
      "Iteration 620, loss = 0.27952721\n",
      "Iteration 621, loss = 0.27925195\n",
      "Iteration 622, loss = 0.27897716\n",
      "Iteration 623, loss = 0.27870283\n",
      "Iteration 624, loss = 0.27842892\n",
      "Iteration 625, loss = 0.27815547\n",
      "Iteration 626, loss = 0.27788247\n",
      "Iteration 627, loss = 0.27760989\n",
      "Iteration 628, loss = 0.27733779\n",
      "Iteration 629, loss = 0.27706609\n",
      "Iteration 630, loss = 0.27679486\n",
      "Iteration 631, loss = 0.27652409\n",
      "Iteration 632, loss = 0.27625373\n",
      "Iteration 633, loss = 0.27598381\n",
      "Iteration 634, loss = 0.27571438\n",
      "Iteration 635, loss = 0.27544533\n",
      "Iteration 636, loss = 0.27517676\n",
      "Iteration 637, loss = 0.27490862\n",
      "Iteration 638, loss = 0.27464091\n",
      "Iteration 639, loss = 0.27437369\n",
      "Iteration 640, loss = 0.27410683\n",
      "Iteration 641, loss = 0.27384048\n",
      "Iteration 642, loss = 0.27357453\n",
      "Iteration 643, loss = 0.27330902\n",
      "Iteration 644, loss = 0.27304397\n",
      "Iteration 645, loss = 0.27277935\n",
      "Iteration 646, loss = 0.27251516\n",
      "Iteration 647, loss = 0.27225143\n",
      "Iteration 648, loss = 0.27198811\n",
      "Iteration 649, loss = 0.27172523\n",
      "Iteration 650, loss = 0.27146282\n",
      "Iteration 651, loss = 0.27120082\n",
      "Iteration 652, loss = 0.27093922\n",
      "Iteration 653, loss = 0.27067816\n",
      "Iteration 654, loss = 0.27041742\n",
      "Iteration 655, loss = 0.27015719\n",
      "Iteration 656, loss = 0.26989737\n",
      "Iteration 657, loss = 0.26963796\n",
      "Iteration 658, loss = 0.26937913\n",
      "Iteration 659, loss = 0.26912067\n",
      "Iteration 660, loss = 0.26886264\n",
      "Iteration 661, loss = 0.26860504\n",
      "Iteration 662, loss = 0.26834784\n",
      "Iteration 663, loss = 0.26809110\n",
      "Iteration 664, loss = 0.26783484\n",
      "Iteration 665, loss = 0.26757897\n",
      "Iteration 666, loss = 0.26732356\n",
      "Iteration 667, loss = 0.26706856\n",
      "Iteration 668, loss = 0.26681407\n",
      "Iteration 669, loss = 0.26655991\n",
      "Iteration 670, loss = 0.26630623\n",
      "Iteration 671, loss = 0.26605297\n",
      "Iteration 672, loss = 0.26580014\n",
      "Iteration 673, loss = 0.26554776\n",
      "Iteration 674, loss = 0.26529577\n",
      "Iteration 675, loss = 0.26504425\n",
      "Iteration 676, loss = 0.26479310\n",
      "Iteration 677, loss = 0.26454248\n",
      "Iteration 678, loss = 0.26429219\n",
      "Iteration 679, loss = 0.26404236\n",
      "Iteration 680, loss = 0.26379297\n",
      "Iteration 681, loss = 0.26354400\n",
      "Iteration 682, loss = 0.26329546\n",
      "Iteration 683, loss = 0.26304733\n",
      "Iteration 684, loss = 0.26279964\n",
      "Iteration 685, loss = 0.26255236\n",
      "Iteration 686, loss = 0.26230556\n",
      "Iteration 687, loss = 0.26205912\n",
      "Iteration 688, loss = 0.26181311\n",
      "Iteration 689, loss = 0.26156756\n",
      "Iteration 690, loss = 0.26132242\n",
      "Iteration 691, loss = 0.26107769\n",
      "Iteration 692, loss = 0.26083341\n",
      "Iteration 693, loss = 0.26058951\n",
      "Iteration 694, loss = 0.26034604\n",
      "Iteration 695, loss = 0.26010304\n",
      "Iteration 696, loss = 0.25986041\n",
      "Iteration 697, loss = 0.25961824\n",
      "Iteration 698, loss = 0.25937645\n",
      "Iteration 699, loss = 0.25913514\n",
      "Iteration 700, loss = 0.25889422\n",
      "Iteration 701, loss = 0.25865370\n",
      "Iteration 702, loss = 0.25841360\n",
      "Iteration 703, loss = 0.25817397\n",
      "Iteration 704, loss = 0.25793469\n",
      "Iteration 705, loss = 0.25769586\n",
      "Iteration 706, loss = 0.25745744\n",
      "Iteration 707, loss = 0.25721944\n",
      "Iteration 708, loss = 0.25698186\n",
      "Iteration 709, loss = 0.25674470\n",
      "Iteration 710, loss = 0.25650793\n",
      "Iteration 711, loss = 0.25627158\n",
      "Iteration 712, loss = 0.25603567\n",
      "Iteration 713, loss = 0.25580015\n",
      "Iteration 714, loss = 0.25556505\n",
      "Iteration 715, loss = 0.25533040\n",
      "Iteration 716, loss = 0.25509617\n",
      "Iteration 717, loss = 0.25486240\n",
      "Iteration 718, loss = 0.25462903\n",
      "Iteration 719, loss = 0.25439611\n",
      "Iteration 720, loss = 0.25416354\n",
      "Iteration 721, loss = 0.25393144\n",
      "Iteration 722, loss = 0.25369976\n",
      "Iteration 723, loss = 0.25346843\n",
      "Iteration 724, loss = 0.25323757\n",
      "Iteration 725, loss = 0.25300707\n",
      "Iteration 726, loss = 0.25277704\n",
      "Iteration 727, loss = 0.25254732\n",
      "Iteration 728, loss = 0.25231813\n",
      "Iteration 729, loss = 0.25208921\n",
      "Iteration 730, loss = 0.25186077\n",
      "Iteration 731, loss = 0.25163275\n",
      "Iteration 732, loss = 0.25140506\n",
      "Iteration 733, loss = 0.25117786\n",
      "Iteration 734, loss = 0.25095102\n",
      "Iteration 735, loss = 0.25072457\n",
      "Iteration 736, loss = 0.25049851\n",
      "Iteration 737, loss = 0.25027289\n",
      "Iteration 738, loss = 0.25004764\n",
      "Iteration 739, loss = 0.24982278\n",
      "Iteration 740, loss = 0.24959838\n",
      "Iteration 741, loss = 0.24937430\n",
      "Iteration 742, loss = 0.24915068\n",
      "Iteration 743, loss = 0.24892740\n",
      "Iteration 744, loss = 0.24870458\n",
      "Iteration 745, loss = 0.24848209\n",
      "Iteration 746, loss = 0.24826001\n",
      "Iteration 747, loss = 0.24803843\n",
      "Iteration 748, loss = 0.24781710\n",
      "Iteration 749, loss = 0.24759619\n",
      "Iteration 750, loss = 0.24737577\n",
      "Iteration 751, loss = 0.24715566\n",
      "Iteration 752, loss = 0.24693597\n",
      "Iteration 753, loss = 0.24671662\n",
      "Iteration 754, loss = 0.24649771\n",
      "Iteration 755, loss = 0.24627921\n",
      "Iteration 756, loss = 0.24606104\n",
      "Iteration 757, loss = 0.24584331\n",
      "Iteration 758, loss = 0.24562596\n",
      "Iteration 759, loss = 0.24540896\n",
      "Iteration 760, loss = 0.24519242\n",
      "Iteration 761, loss = 0.24497619\n",
      "Iteration 762, loss = 0.24476040\n",
      "Iteration 763, loss = 0.24454497\n",
      "Iteration 764, loss = 0.24432991\n",
      "Iteration 765, loss = 0.24411531\n",
      "Iteration 766, loss = 0.24390102\n",
      "Iteration 767, loss = 0.24368712\n",
      "Iteration 768, loss = 0.24347366\n",
      "Iteration 769, loss = 0.24326054\n",
      "Iteration 770, loss = 0.24304778\n",
      "Iteration 771, loss = 0.24283546\n",
      "Iteration 772, loss = 0.24262348\n",
      "Iteration 773, loss = 0.24241192\n",
      "Iteration 774, loss = 0.24220070\n",
      "Iteration 775, loss = 0.24198987\n",
      "Iteration 776, loss = 0.24177942\n",
      "Iteration 777, loss = 0.24156935\n",
      "Iteration 778, loss = 0.24135962\n",
      "Iteration 779, loss = 0.24115040\n",
      "Iteration 780, loss = 0.24094138\n",
      "Iteration 781, loss = 0.24073285\n",
      "Iteration 782, loss = 0.24052469\n",
      "Iteration 783, loss = 0.24031686\n",
      "Iteration 784, loss = 0.24010943\n",
      "Iteration 785, loss = 0.23990238\n",
      "Iteration 786, loss = 0.23969572\n",
      "Iteration 787, loss = 0.23948938\n",
      "Iteration 788, loss = 0.23928346\n",
      "Iteration 789, loss = 0.23907790\n",
      "Iteration 790, loss = 0.23887271\n",
      "Iteration 791, loss = 0.23866791\n",
      "Iteration 792, loss = 0.23846344\n",
      "Iteration 793, loss = 0.23825939\n",
      "Iteration 794, loss = 0.23805567\n",
      "Iteration 795, loss = 0.23785232\n",
      "Iteration 796, loss = 0.23764936\n",
      "Iteration 797, loss = 0.23744679\n",
      "Iteration 798, loss = 0.23724453\n",
      "Iteration 799, loss = 0.23704266\n",
      "Iteration 800, loss = 0.23684118\n",
      "Iteration 801, loss = 0.23664005\n",
      "Iteration 802, loss = 0.23643927\n",
      "Iteration 803, loss = 0.23623889\n",
      "Iteration 804, loss = 0.23603884\n",
      "Iteration 805, loss = 0.23583917\n",
      "Iteration 806, loss = 0.23563989\n",
      "Iteration 807, loss = 0.23544091\n",
      "Iteration 808, loss = 0.23524232\n",
      "Iteration 809, loss = 0.23504414\n",
      "Iteration 810, loss = 0.23484627\n",
      "Iteration 811, loss = 0.23464875\n",
      "Iteration 812, loss = 0.23445160\n",
      "Iteration 813, loss = 0.23425485\n",
      "Iteration 814, loss = 0.23405841\n",
      "Iteration 815, loss = 0.23386232\n",
      "Iteration 816, loss = 0.23366669\n",
      "Iteration 817, loss = 0.23347129\n",
      "Iteration 818, loss = 0.23327629\n",
      "Iteration 819, loss = 0.23308168\n",
      "Iteration 820, loss = 0.23288741\n",
      "Iteration 821, loss = 0.23269348\n",
      "Iteration 822, loss = 0.23249989\n",
      "Iteration 823, loss = 0.23230672\n",
      "Iteration 824, loss = 0.23211382\n",
      "Iteration 825, loss = 0.23192131\n",
      "Iteration 826, loss = 0.23172917\n",
      "Iteration 827, loss = 0.23153736\n",
      "Iteration 828, loss = 0.23134592\n",
      "Iteration 829, loss = 0.23115481\n",
      "Iteration 830, loss = 0.23096404\n",
      "Iteration 831, loss = 0.23077368\n",
      "Iteration 832, loss = 0.23058360\n",
      "Iteration 833, loss = 0.23039388\n",
      "Iteration 834, loss = 0.23020455\n",
      "Iteration 835, loss = 0.23001553\n",
      "Iteration 836, loss = 0.22982687\n",
      "Iteration 837, loss = 0.22963855\n",
      "Iteration 838, loss = 0.22945057\n",
      "Iteration 839, loss = 0.22926296\n",
      "Iteration 840, loss = 0.22907567\n",
      "Iteration 841, loss = 0.22888876\n",
      "Iteration 842, loss = 0.22870213\n",
      "Iteration 843, loss = 0.22851589\n",
      "Iteration 844, loss = 0.22832999\n",
      "Iteration 845, loss = 0.22814442\n",
      "Iteration 846, loss = 0.22795919\n",
      "Iteration 847, loss = 0.22777431\n",
      "Iteration 848, loss = 0.22758976\n",
      "Iteration 849, loss = 0.22740555\n",
      "Iteration 850, loss = 0.22722169\n",
      "Iteration 851, loss = 0.22703815\n",
      "Iteration 852, loss = 0.22685501\n",
      "Iteration 853, loss = 0.22667222\n",
      "Iteration 854, loss = 0.22648978\n",
      "Iteration 855, loss = 0.22630768\n",
      "Iteration 856, loss = 0.22612596\n",
      "Iteration 857, loss = 0.22594455\n",
      "Iteration 858, loss = 0.22576348\n",
      "Iteration 859, loss = 0.22558273\n",
      "Iteration 860, loss = 0.22540232\n",
      "Iteration 861, loss = 0.22522227\n",
      "Iteration 862, loss = 0.22504253\n",
      "Iteration 863, loss = 0.22486313\n",
      "Iteration 864, loss = 0.22468409\n",
      "Iteration 865, loss = 0.22450534\n",
      "Iteration 866, loss = 0.22432692\n",
      "Iteration 867, loss = 0.22414886\n",
      "Iteration 868, loss = 0.22397112\n",
      "Iteration 869, loss = 0.22379370\n",
      "Iteration 870, loss = 0.22361661\n",
      "Iteration 871, loss = 0.22343985\n",
      "Iteration 872, loss = 0.22326342\n",
      "Iteration 873, loss = 0.22308730\n",
      "Iteration 874, loss = 0.22291152\n",
      "Iteration 875, loss = 0.22273607\n",
      "Iteration 876, loss = 0.22256093\n",
      "Iteration 877, loss = 0.22238613\n",
      "Iteration 878, loss = 0.22221164\n",
      "Iteration 879, loss = 0.22203746\n",
      "Iteration 880, loss = 0.22186362\n",
      "Iteration 881, loss = 0.22169010\n",
      "Iteration 882, loss = 0.22151692\n",
      "Iteration 883, loss = 0.22134402\n",
      "Iteration 884, loss = 0.22117146\n",
      "Iteration 885, loss = 0.22099921\n",
      "Iteration 886, loss = 0.22082728\n",
      "Iteration 887, loss = 0.22065568\n",
      "Iteration 888, loss = 0.22048437\n",
      "Iteration 889, loss = 0.22031345\n",
      "Iteration 890, loss = 0.22014274\n",
      "Iteration 891, loss = 0.21997242\n",
      "Iteration 892, loss = 0.21980238\n",
      "Iteration 893, loss = 0.21963266\n",
      "Iteration 894, loss = 0.21946327\n",
      "Iteration 895, loss = 0.21929417\n",
      "Iteration 896, loss = 0.21912539\n",
      "Iteration 897, loss = 0.21895693\n",
      "Iteration 898, loss = 0.21878878\n",
      "Iteration 899, loss = 0.21862094\n",
      "Iteration 900, loss = 0.21845340\n",
      "Iteration 901, loss = 0.21828620\n",
      "Iteration 902, loss = 0.21811927\n",
      "Iteration 903, loss = 0.21795266\n",
      "Iteration 904, loss = 0.21778637\n",
      "Iteration 905, loss = 0.21762037\n",
      "Iteration 906, loss = 0.21745469\n",
      "Iteration 907, loss = 0.21728930\n",
      "Iteration 908, loss = 0.21712425\n",
      "Iteration 909, loss = 0.21695948\n",
      "Iteration 910, loss = 0.21679502\n",
      "Iteration 911, loss = 0.21663085\n",
      "Iteration 912, loss = 0.21646700\n",
      "Iteration 913, loss = 0.21630344\n",
      "Iteration 914, loss = 0.21614020\n",
      "Iteration 915, loss = 0.21597725\n",
      "Iteration 916, loss = 0.21581459\n",
      "Iteration 917, loss = 0.21565227\n",
      "Iteration 918, loss = 0.21549020\n",
      "Iteration 919, loss = 0.21532846\n",
      "Iteration 920, loss = 0.21516700\n",
      "Iteration 921, loss = 0.21500587\n",
      "Iteration 922, loss = 0.21484499\n",
      "Iteration 923, loss = 0.21468447\n",
      "Iteration 924, loss = 0.21452420\n",
      "Iteration 925, loss = 0.21436422\n",
      "Iteration 926, loss = 0.21420459\n",
      "Iteration 927, loss = 0.21404521\n",
      "Iteration 928, loss = 0.21388610\n",
      "Iteration 929, loss = 0.21372734\n",
      "Iteration 930, loss = 0.21356885\n",
      "Iteration 931, loss = 0.21341064\n",
      "Iteration 932, loss = 0.21325276\n",
      "Iteration 933, loss = 0.21309513\n",
      "Iteration 934, loss = 0.21293782\n",
      "Iteration 935, loss = 0.21278078\n",
      "Iteration 936, loss = 0.21262405\n",
      "Iteration 937, loss = 0.21246758\n",
      "Iteration 938, loss = 0.21231144\n",
      "Iteration 939, loss = 0.21215555\n",
      "Iteration 940, loss = 0.21199997\n",
      "Iteration 941, loss = 0.21184467\n",
      "Iteration 942, loss = 0.21168966\n",
      "Iteration 943, loss = 0.21153494\n",
      "Iteration 944, loss = 0.21138048\n",
      "Iteration 945, loss = 0.21122636\n",
      "Iteration 946, loss = 0.21107247\n",
      "Iteration 947, loss = 0.21091888\n",
      "Iteration 948, loss = 0.21076557\n",
      "Iteration 949, loss = 0.21061257\n",
      "Iteration 950, loss = 0.21045982\n",
      "Iteration 951, loss = 0.21030738\n",
      "Iteration 952, loss = 0.21015519\n",
      "Iteration 953, loss = 0.21000331\n",
      "Iteration 954, loss = 0.20985168\n",
      "Iteration 955, loss = 0.20970038\n",
      "Iteration 956, loss = 0.20954930\n",
      "Iteration 957, loss = 0.20939853\n",
      "Iteration 958, loss = 0.20924802\n",
      "Iteration 959, loss = 0.20909782\n",
      "Iteration 960, loss = 0.20894787\n",
      "Iteration 961, loss = 0.20879821\n",
      "Iteration 962, loss = 0.20864881\n",
      "Iteration 963, loss = 0.20849971\n",
      "Iteration 964, loss = 0.20835085\n",
      "Iteration 965, loss = 0.20820231\n",
      "Iteration 966, loss = 0.20805400\n",
      "Iteration 967, loss = 0.20790599\n",
      "Iteration 968, loss = 0.20775823\n",
      "Iteration 969, loss = 0.20761078\n",
      "Iteration 970, loss = 0.20746356\n",
      "Iteration 971, loss = 0.20731663\n",
      "Iteration 972, loss = 0.20716996\n",
      "Iteration 973, loss = 0.20702359\n",
      "Iteration 974, loss = 0.20687745\n",
      "Iteration 975, loss = 0.20673161\n",
      "Iteration 976, loss = 0.20658601\n",
      "Iteration 977, loss = 0.20644071\n",
      "Iteration 978, loss = 0.20629565\n",
      "Iteration 979, loss = 0.20615087\n",
      "Iteration 980, loss = 0.20600635\n",
      "Iteration 981, loss = 0.20586211\n",
      "Iteration 982, loss = 0.20571811\n",
      "Iteration 983, loss = 0.20557440\n",
      "Iteration 984, loss = 0.20543093\n",
      "Iteration 985, loss = 0.20528776\n",
      "Iteration 986, loss = 0.20514482\n",
      "Iteration 987, loss = 0.20500215\n",
      "Iteration 988, loss = 0.20485973\n",
      "Iteration 989, loss = 0.20471761\n",
      "Iteration 990, loss = 0.20457571\n",
      "Iteration 991, loss = 0.20443411\n",
      "Iteration 992, loss = 0.20429273\n",
      "Iteration 993, loss = 0.20415163\n",
      "Iteration 994, loss = 0.20401078\n",
      "Iteration 995, loss = 0.20387020\n",
      "Iteration 996, loss = 0.20372986\n",
      "Iteration 997, loss = 0.20358981\n",
      "Iteration 998, loss = 0.20344997\n",
      "Iteration 999, loss = 0.20331044\n",
      "Iteration 1000, loss = 0.20317113\n",
      "Iteration 1, loss = 1.91716398\n",
      "Iteration 2, loss = 3.39215154\n",
      "Iteration 3, loss = 1.37237638\n",
      "Iteration 4, loss = 0.80971506\n",
      "Iteration 5, loss = 1.11163849\n",
      "Iteration 6, loss = 0.57488434\n",
      "Iteration 7, loss = 1.05671239\n",
      "Iteration 8, loss = 0.54395369\n",
      "Iteration 9, loss = 0.48320272\n",
      "Iteration 10, loss = 0.57156945\n",
      "Iteration 11, loss = 0.48509864\n",
      "Iteration 12, loss = 0.38156776\n",
      "Iteration 13, loss = 0.40736377\n",
      "Iteration 14, loss = 0.39741170\n",
      "Iteration 15, loss = 0.31752559\n",
      "Iteration 16, loss = 0.30074655\n",
      "Iteration 17, loss = 0.35040065\n",
      "Iteration 18, loss = 0.32585837\n",
      "Iteration 19, loss = 0.27017888\n",
      "Iteration 20, loss = 0.29877149\n",
      "Iteration 21, loss = 0.28827944\n",
      "Iteration 22, loss = 0.23413981\n",
      "Iteration 23, loss = 0.25227144\n",
      "Iteration 24, loss = 0.24927558\n",
      "Iteration 25, loss = 0.20648486\n",
      "Iteration 26, loss = 0.21837762\n",
      "Iteration 27, loss = 0.21506476\n",
      "Iteration 28, loss = 0.18102812\n",
      "Iteration 29, loss = 0.19352201\n",
      "Iteration 30, loss = 0.18547734\n",
      "Iteration 31, loss = 0.16171823\n",
      "Iteration 32, loss = 0.17417181\n",
      "Iteration 33, loss = 0.16051747\n",
      "Iteration 34, loss = 0.14910349\n",
      "Iteration 35, loss = 0.15793909\n",
      "Iteration 36, loss = 0.14267958\n",
      "Iteration 37, loss = 0.14026109\n",
      "Iteration 38, loss = 0.14352291\n",
      "Iteration 39, loss = 0.13005165\n",
      "Iteration 40, loss = 0.13481996\n",
      "Iteration 41, loss = 0.13045064\n",
      "Iteration 42, loss = 0.12372569\n",
      "Iteration 43, loss = 0.12843853\n",
      "Iteration 44, loss = 0.12020360\n",
      "Iteration 45, loss = 0.12129326\n",
      "Iteration 46, loss = 0.12049450\n",
      "Iteration 47, loss = 0.11516498\n",
      "Iteration 48, loss = 0.11822924\n",
      "Iteration 49, loss = 0.11328362\n",
      "Iteration 50, loss = 0.11367251\n",
      "Iteration 51, loss = 0.11306239\n",
      "Iteration 52, loss = 0.10982830\n",
      "Iteration 53, loss = 0.11158692\n",
      "Iteration 54, loss = 0.10815023\n",
      "Iteration 55, loss = 0.10888817\n",
      "Iteration 56, loss = 0.10757443\n",
      "Iteration 57, loss = 0.10623232\n",
      "Iteration 58, loss = 0.10669944\n",
      "Iteration 59, loss = 0.10451892\n",
      "Iteration 60, loss = 0.10525136\n",
      "Iteration 61, loss = 0.10351370\n",
      "Iteration 62, loss = 0.10357214\n",
      "Iteration 63, loss = 0.10273791\n",
      "Iteration 64, loss = 0.10204908\n",
      "Iteration 65, loss = 0.10194639\n",
      "Iteration 66, loss = 0.10079542\n",
      "Iteration 67, loss = 0.10100101\n",
      "Iteration 68, loss = 0.09984853\n",
      "Iteration 69, loss = 0.09995490\n",
      "Iteration 70, loss = 0.09906990\n",
      "Iteration 71, loss = 0.09893652\n",
      "Iteration 72, loss = 0.09833716\n",
      "Iteration 73, loss = 0.09799328\n",
      "Iteration 74, loss = 0.09763248\n",
      "Iteration 75, loss = 0.09713049\n",
      "Iteration 76, loss = 0.09691480\n",
      "Iteration 77, loss = 0.09636024\n",
      "Iteration 78, loss = 0.09619844\n",
      "Iteration 79, loss = 0.09564275\n",
      "Iteration 80, loss = 0.09549697\n",
      "Iteration 81, loss = 0.09497389\n",
      "Iteration 82, loss = 0.09481112\n",
      "Iteration 83, loss = 0.09433550\n",
      "Iteration 84, loss = 0.09415419\n",
      "Iteration 85, loss = 0.09372039\n",
      "Iteration 86, loss = 0.09352026\n",
      "Iteration 87, loss = 0.09312692\n",
      "Iteration 88, loss = 0.09291415\n",
      "Iteration 89, loss = 0.09255015\n",
      "Iteration 90, loss = 0.09233021\n",
      "Iteration 91, loss = 0.09199189\n",
      "Iteration 92, loss = 0.09176905\n",
      "Iteration 93, loss = 0.09144907\n",
      "Iteration 94, loss = 0.09122645\n",
      "Iteration 95, loss = 0.09092308\n",
      "Iteration 96, loss = 0.09070275\n",
      "Iteration 97, loss = 0.09041252\n",
      "Iteration 98, loss = 0.09019469\n",
      "Iteration 99, loss = 0.08991734\n",
      "Iteration 100, loss = 0.08970262\n",
      "Iteration 101, loss = 0.08943722\n",
      "Iteration 102, loss = 0.08922431\n",
      "Iteration 103, loss = 0.08897098\n",
      "Iteration 104, loss = 0.08875947\n",
      "Iteration 105, loss = 0.08851822\n",
      "Iteration 106, loss = 0.08830707\n",
      "Iteration 107, loss = 0.08807779\n",
      "Iteration 108, loss = 0.08786696\n",
      "Iteration 109, loss = 0.08764921\n",
      "Iteration 110, loss = 0.08743887\n",
      "Iteration 111, loss = 0.08723149\n",
      "Iteration 112, loss = 0.08702263\n",
      "Iteration 113, loss = 0.08682392\n",
      "Iteration 114, loss = 0.08661807\n",
      "Iteration 115, loss = 0.08642585\n",
      "Iteration 116, loss = 0.08622468\n",
      "Iteration 117, loss = 0.08603685\n",
      "Iteration 118, loss = 0.08584206\n",
      "Iteration 119, loss = 0.08565703\n",
      "Iteration 120, loss = 0.08546937\n",
      "Iteration 121, loss = 0.08528648\n",
      "Iteration 122, loss = 0.08510584\n",
      "Iteration 123, loss = 0.08492541\n",
      "Iteration 124, loss = 0.08475061\n",
      "Iteration 125, loss = 0.08457373\n",
      "Iteration 126, loss = 0.08440326\n",
      "Iteration 127, loss = 0.08423109\n",
      "Iteration 128, loss = 0.08406370\n",
      "Iteration 129, loss = 0.08389683\n",
      "Iteration 130, loss = 0.08373210\n",
      "Iteration 131, loss = 0.08357026\n",
      "Iteration 132, loss = 0.08340860\n",
      "Iteration 133, loss = 0.08325082\n",
      "Iteration 134, loss = 0.08309299\n",
      "Iteration 135, loss = 0.08293835\n",
      "Iteration 136, loss = 0.08278481\n",
      "Iteration 137, loss = 0.08263292\n",
      "Iteration 138, loss = 0.08248341\n",
      "Iteration 139, loss = 0.08233458\n",
      "Iteration 140, loss = 0.08218842\n",
      "Iteration 141, loss = 0.08204309\n",
      "Iteration 142, loss = 0.08189972\n",
      "Iteration 143, loss = 0.08175800\n",
      "Iteration 144, loss = 0.08161739\n",
      "Iteration 145, loss = 0.08147889\n",
      "Iteration 146, loss = 0.08134135\n",
      "Iteration 147, loss = 0.08120561\n",
      "Iteration 148, loss = 0.08107130\n",
      "Iteration 149, loss = 0.08093826\n",
      "Iteration 150, loss = 0.08080695\n",
      "Iteration 151, loss = 0.08067672\n",
      "Iteration 152, loss = 0.08054806\n",
      "Iteration 153, loss = 0.08042075\n",
      "Iteration 154, loss = 0.08029461\n",
      "Iteration 155, loss = 0.08017001\n",
      "Iteration 156, loss = 0.08004651\n",
      "Iteration 157, loss = 0.07992434\n",
      "Iteration 158, loss = 0.07980348\n",
      "Iteration 159, loss = 0.07968377\n",
      "Iteration 160, loss = 0.07956541\n",
      "Iteration 161, loss = 0.07944816\n",
      "Iteration 162, loss = 0.07933207\n",
      "Iteration 163, loss = 0.07921721\n",
      "Iteration 164, loss = 0.07910341\n",
      "Iteration 165, loss = 0.07899077\n",
      "Iteration 166, loss = 0.07887923\n",
      "Iteration 167, loss = 0.07876873\n",
      "Iteration 168, loss = 0.07865935\n",
      "Iteration 169, loss = 0.07855100\n",
      "Iteration 170, loss = 0.07844367\n",
      "Iteration 171, loss = 0.07833740\n",
      "Iteration 172, loss = 0.07823214\n",
      "Iteration 173, loss = 0.07812787\n",
      "Iteration 174, loss = 0.07802461\n",
      "Iteration 175, loss = 0.07792228\n",
      "Iteration 176, loss = 0.07782095\n",
      "Iteration 177, loss = 0.07772055\n",
      "Iteration 178, loss = 0.07762110\n",
      "Iteration 179, loss = 0.07752260\n",
      "Iteration 180, loss = 0.07742501\n",
      "Iteration 181, loss = 0.07732831\n",
      "Iteration 182, loss = 0.07723250\n",
      "Iteration 183, loss = 0.07713755\n",
      "Iteration 184, loss = 0.07704345\n",
      "Iteration 185, loss = 0.07695020\n",
      "Iteration 186, loss = 0.07685778\n",
      "Iteration 187, loss = 0.07676618\n",
      "Iteration 188, loss = 0.07667540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92564666\n",
      "Iteration 2, loss = 3.39945134\n",
      "Iteration 3, loss = 1.36052596\n",
      "Iteration 4, loss = 0.81661730\n",
      "Iteration 5, loss = 1.07290492\n",
      "Iteration 6, loss = 0.52744380\n",
      "Iteration 7, loss = 1.28750612\n",
      "Iteration 8, loss = 0.55091271\n",
      "Iteration 9, loss = 0.53286401\n",
      "Iteration 10, loss = 0.67541372\n",
      "Iteration 11, loss = 0.55715154\n",
      "Iteration 12, loss = 0.43408793\n",
      "Iteration 13, loss = 0.42581179\n",
      "Iteration 14, loss = 0.37872221\n",
      "Iteration 15, loss = 0.30151316\n",
      "Iteration 16, loss = 0.31350157\n",
      "Iteration 17, loss = 0.36937715\n",
      "Iteration 18, loss = 0.31200681\n",
      "Iteration 19, loss = 0.26581165\n",
      "Iteration 20, loss = 0.29488372\n",
      "Iteration 21, loss = 0.25463304\n",
      "Iteration 22, loss = 0.22994012\n",
      "Iteration 23, loss = 0.25717399\n",
      "Iteration 24, loss = 0.22526134\n",
      "Iteration 25, loss = 0.19999361\n",
      "Iteration 26, loss = 0.21736197\n",
      "Iteration 27, loss = 0.18906662\n",
      "Iteration 28, loss = 0.17406340\n",
      "Iteration 29, loss = 0.18368034\n",
      "Iteration 30, loss = 0.16192629\n",
      "Iteration 31, loss = 0.15650364\n",
      "Iteration 32, loss = 0.16214818\n",
      "Iteration 33, loss = 0.14249252\n",
      "Iteration 34, loss = 0.14479728\n",
      "Iteration 35, loss = 0.14307341\n",
      "Iteration 36, loss = 0.12962335\n",
      "Iteration 37, loss = 0.13486506\n",
      "Iteration 38, loss = 0.12800484\n",
      "Iteration 39, loss = 0.12200719\n",
      "Iteration 40, loss = 0.12578025\n",
      "Iteration 41, loss = 0.11747226\n",
      "Iteration 42, loss = 0.11716234\n",
      "Iteration 43, loss = 0.11719227\n",
      "Iteration 44, loss = 0.11084815\n",
      "Iteration 45, loss = 0.11338119\n",
      "Iteration 46, loss = 0.10970263\n",
      "Iteration 47, loss = 0.10757308\n",
      "Iteration 48, loss = 0.10883794\n",
      "Iteration 49, loss = 0.10453111\n",
      "Iteration 50, loss = 0.10566914\n",
      "Iteration 51, loss = 0.10395522\n",
      "Iteration 52, loss = 0.10211692\n",
      "Iteration 53, loss = 0.10302197\n",
      "Iteration 54, loss = 0.10028855\n",
      "Iteration 55, loss = 0.10089276\n",
      "Iteration 56, loss = 0.09970169\n",
      "Iteration 57, loss = 0.09862207\n",
      "Iteration 58, loss = 0.09896271\n",
      "Iteration 59, loss = 0.09721032\n",
      "Iteration 60, loss = 0.09762286\n",
      "Iteration 61, loss = 0.09649889\n",
      "Iteration 62, loss = 0.09608596\n",
      "Iteration 63, loss = 0.09587422\n",
      "Iteration 64, loss = 0.09486106\n",
      "Iteration 65, loss = 0.09502282\n",
      "Iteration 66, loss = 0.09401727\n",
      "Iteration 67, loss = 0.09398948\n",
      "Iteration 68, loss = 0.09337907\n",
      "Iteration 69, loss = 0.09297704\n",
      "Iteration 70, loss = 0.09276121\n",
      "Iteration 71, loss = 0.09211162\n",
      "Iteration 72, loss = 0.09206617\n",
      "Iteration 73, loss = 0.09141949\n",
      "Iteration 74, loss = 0.09131832\n",
      "Iteration 75, loss = 0.09083105\n",
      "Iteration 76, loss = 0.09058595\n",
      "Iteration 77, loss = 0.09027189\n",
      "Iteration 78, loss = 0.08991297\n",
      "Iteration 79, loss = 0.08971445\n",
      "Iteration 80, loss = 0.08930335\n",
      "Iteration 81, loss = 0.08915014\n",
      "Iteration 82, loss = 0.08875183\n",
      "Iteration 83, loss = 0.08858685\n",
      "Iteration 84, loss = 0.08823820\n",
      "Iteration 85, loss = 0.08803983\n",
      "Iteration 86, loss = 0.08774532\n",
      "Iteration 87, loss = 0.08751512\n",
      "Iteration 88, loss = 0.08726806\n",
      "Iteration 89, loss = 0.08701780\n",
      "Iteration 90, loss = 0.08680498\n",
      "Iteration 91, loss = 0.08654727\n",
      "Iteration 92, loss = 0.08635251\n",
      "Iteration 93, loss = 0.08609681\n",
      "Iteration 94, loss = 0.08591163\n",
      "Iteration 95, loss = 0.08566478\n",
      "Iteration 96, loss = 0.08548392\n",
      "Iteration 97, loss = 0.08524951\n",
      "Iteration 98, loss = 0.08507018\n",
      "Iteration 99, loss = 0.08484761\n",
      "Iteration 100, loss = 0.08466898\n",
      "Iteration 101, loss = 0.08445786\n",
      "Iteration 102, loss = 0.08428046\n",
      "Iteration 103, loss = 0.08407922\n",
      "Iteration 104, loss = 0.08390375\n",
      "Iteration 105, loss = 0.08371133\n",
      "Iteration 106, loss = 0.08353861\n",
      "Iteration 107, loss = 0.08335363\n",
      "Iteration 108, loss = 0.08318413\n",
      "Iteration 109, loss = 0.08300626\n",
      "Iteration 110, loss = 0.08284091\n",
      "Iteration 111, loss = 0.08266873\n",
      "Iteration 112, loss = 0.08250761\n",
      "Iteration 113, loss = 0.08234109\n",
      "Iteration 114, loss = 0.08218381\n",
      "Iteration 115, loss = 0.08202197\n",
      "Iteration 116, loss = 0.08186842\n",
      "Iteration 117, loss = 0.08171096\n",
      "Iteration 118, loss = 0.08156109\n",
      "Iteration 119, loss = 0.08140773\n",
      "Iteration 120, loss = 0.08126126\n",
      "Iteration 121, loss = 0.08111183\n",
      "Iteration 122, loss = 0.08096860\n",
      "Iteration 123, loss = 0.08082292\n",
      "Iteration 124, loss = 0.08068273\n",
      "Iteration 125, loss = 0.08054085\n",
      "Iteration 126, loss = 0.08040373\n",
      "Iteration 127, loss = 0.08026551\n",
      "Iteration 128, loss = 0.08013106\n",
      "Iteration 129, loss = 0.07999629\n",
      "Iteration 130, loss = 0.07986440\n",
      "Iteration 131, loss = 0.07973303\n",
      "Iteration 132, loss = 0.07960381\n",
      "Iteration 133, loss = 0.07947575\n",
      "Iteration 134, loss = 0.07934899\n",
      "Iteration 135, loss = 0.07922393\n",
      "Iteration 136, loss = 0.07909972\n",
      "Iteration 137, loss = 0.07897753\n",
      "Iteration 138, loss = 0.07885589\n",
      "Iteration 139, loss = 0.07873631\n",
      "Iteration 140, loss = 0.07861720\n",
      "Iteration 141, loss = 0.07850001\n",
      "Iteration 142, loss = 0.07838345\n",
      "Iteration 143, loss = 0.07826847\n",
      "Iteration 144, loss = 0.07815438\n",
      "Iteration 145, loss = 0.07804156\n",
      "Iteration 146, loss = 0.07792996\n",
      "Iteration 147, loss = 0.07781932\n",
      "Iteration 148, loss = 0.07770997\n",
      "Iteration 149, loss = 0.07760150\n",
      "Iteration 150, loss = 0.07749432\n",
      "Iteration 151, loss = 0.07738796\n",
      "Iteration 152, loss = 0.07728279\n",
      "Iteration 153, loss = 0.07717853\n",
      "Iteration 154, loss = 0.07707531\n",
      "Iteration 155, loss = 0.07697306\n",
      "Iteration 156, loss = 0.07687166\n",
      "Iteration 157, loss = 0.07677130\n",
      "Iteration 158, loss = 0.07667184\n",
      "Iteration 159, loss = 0.07657345\n",
      "Iteration 160, loss = 0.07647589\n",
      "Iteration 161, loss = 0.07637926\n",
      "Iteration 162, loss = 0.07628348\n",
      "Iteration 163, loss = 0.07618853\n",
      "Iteration 164, loss = 0.07609446\n",
      "Iteration 165, loss = 0.07600115\n",
      "Iteration 166, loss = 0.07590870\n",
      "Iteration 167, loss = 0.07581701\n",
      "Iteration 168, loss = 0.07572612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90632821\n",
      "Iteration 2, loss = 3.39276275\n",
      "Iteration 3, loss = 1.34522771\n",
      "Iteration 4, loss = 0.81488990\n",
      "Iteration 5, loss = 1.06036669\n",
      "Iteration 6, loss = 0.53607746\n",
      "Iteration 7, loss = 1.19393819\n",
      "Iteration 8, loss = 0.52687136\n",
      "Iteration 9, loss = 0.50956823\n",
      "Iteration 10, loss = 0.63456444\n",
      "Iteration 11, loss = 0.52209688\n",
      "Iteration 12, loss = 0.41025866\n",
      "Iteration 13, loss = 0.41054959\n",
      "Iteration 14, loss = 0.35796388\n",
      "Iteration 15, loss = 0.28413752\n",
      "Iteration 16, loss = 0.29126634\n",
      "Iteration 17, loss = 0.33984079\n",
      "Iteration 18, loss = 0.28770589\n",
      "Iteration 19, loss = 0.24858502\n",
      "Iteration 20, loss = 0.27374706\n",
      "Iteration 21, loss = 0.23093901\n",
      "Iteration 22, loss = 0.21387634\n",
      "Iteration 23, loss = 0.23081759\n",
      "Iteration 24, loss = 0.19194479\n",
      "Iteration 25, loss = 0.17621483\n",
      "Iteration 26, loss = 0.18709930\n",
      "Iteration 27, loss = 0.15950465\n",
      "Iteration 28, loss = 0.15298195\n",
      "Iteration 29, loss = 0.15776612\n",
      "Iteration 30, loss = 0.13676201\n",
      "Iteration 31, loss = 0.13318051\n",
      "Iteration 32, loss = 0.13556111\n",
      "Iteration 33, loss = 0.11873393\n",
      "Iteration 34, loss = 0.11900326\n",
      "Iteration 35, loss = 0.11857372\n",
      "Iteration 36, loss = 0.10666203\n",
      "Iteration 37, loss = 0.10782019\n",
      "Iteration 38, loss = 0.10635201\n",
      "Iteration 39, loss = 0.09791792\n",
      "Iteration 40, loss = 0.09983614\n",
      "Iteration 41, loss = 0.09759171\n",
      "Iteration 42, loss = 0.09187956\n",
      "Iteration 43, loss = 0.09363788\n",
      "Iteration 44, loss = 0.09123527\n",
      "Iteration 45, loss = 0.08756631\n",
      "Iteration 46, loss = 0.08897809\n",
      "Iteration 47, loss = 0.08674779\n",
      "Iteration 48, loss = 0.08428836\n",
      "Iteration 49, loss = 0.08535955\n",
      "Iteration 50, loss = 0.08327494\n",
      "Iteration 51, loss = 0.08184266\n",
      "Iteration 52, loss = 0.08247075\n",
      "Iteration 53, loss = 0.08071662\n",
      "Iteration 54, loss = 0.07978760\n",
      "Iteration 55, loss = 0.08015016\n",
      "Iteration 56, loss = 0.07861048\n",
      "Iteration 57, loss = 0.07815391\n",
      "Iteration 58, loss = 0.07819609\n",
      "Iteration 59, loss = 0.07695660\n",
      "Iteration 60, loss = 0.07669652\n",
      "Iteration 61, loss = 0.07655159\n",
      "Iteration 62, loss = 0.07554842\n",
      "Iteration 63, loss = 0.07545075\n",
      "Iteration 64, loss = 0.07513290\n",
      "Iteration 65, loss = 0.07437016\n",
      "Iteration 66, loss = 0.07431026\n",
      "Iteration 67, loss = 0.07389276\n",
      "Iteration 68, loss = 0.07334099\n",
      "Iteration 69, loss = 0.07326757\n",
      "Iteration 70, loss = 0.07280886\n",
      "Iteration 71, loss = 0.07241738\n",
      "Iteration 72, loss = 0.07229733\n",
      "Iteration 73, loss = 0.07183956\n",
      "Iteration 74, loss = 0.07157672\n",
      "Iteration 75, loss = 0.07138605\n",
      "Iteration 76, loss = 0.07097659\n",
      "Iteration 77, loss = 0.07078060\n",
      "Iteration 78, loss = 0.07053366\n",
      "Iteration 79, loss = 0.07019122\n",
      "Iteration 80, loss = 0.07002245\n",
      "Iteration 81, loss = 0.06974059\n",
      "Iteration 82, loss = 0.06946801\n",
      "Iteration 83, loss = 0.06929174\n",
      "Iteration 84, loss = 0.06900486\n",
      "Iteration 85, loss = 0.06878920\n",
      "Iteration 86, loss = 0.06858893\n",
      "Iteration 87, loss = 0.06832446\n",
      "Iteration 88, loss = 0.06813881\n",
      "Iteration 89, loss = 0.06791885\n",
      "Iteration 90, loss = 0.06768907\n",
      "Iteration 91, loss = 0.06751079\n",
      "Iteration 92, loss = 0.06728782\n",
      "Iteration 93, loss = 0.06709228\n",
      "Iteration 94, loss = 0.06690932\n",
      "Iteration 95, loss = 0.06669750\n",
      "Iteration 96, loss = 0.06652235\n",
      "Iteration 97, loss = 0.06633211\n",
      "Iteration 98, loss = 0.06614058\n",
      "Iteration 99, loss = 0.06597158\n",
      "Iteration 100, loss = 0.06578223\n",
      "Iteration 101, loss = 0.06560961\n",
      "Iteration 102, loss = 0.06543911\n",
      "Iteration 103, loss = 0.06525981\n",
      "Iteration 104, loss = 0.06509802\n",
      "Iteration 105, loss = 0.06492707\n",
      "Iteration 106, loss = 0.06476139\n",
      "Iteration 107, loss = 0.06460294\n",
      "Iteration 108, loss = 0.06443714\n",
      "Iteration 109, loss = 0.06428214\n",
      "Iteration 110, loss = 0.06412491\n",
      "Iteration 111, loss = 0.06396921\n",
      "Iteration 112, loss = 0.06382058\n",
      "Iteration 113, loss = 0.06366783\n",
      "Iteration 114, loss = 0.06352185\n",
      "Iteration 115, loss = 0.06337640\n",
      "Iteration 116, loss = 0.06323093\n",
      "Iteration 117, loss = 0.06309083\n",
      "Iteration 118, loss = 0.06294878\n",
      "Iteration 119, loss = 0.06281068\n",
      "Iteration 120, loss = 0.06267397\n",
      "Iteration 121, loss = 0.06253741\n",
      "Iteration 122, loss = 0.06240474\n",
      "Iteration 123, loss = 0.06227151\n",
      "Iteration 124, loss = 0.06214108\n",
      "Iteration 125, loss = 0.06201209\n",
      "Iteration 126, loss = 0.06188365\n",
      "Iteration 127, loss = 0.06175818\n",
      "Iteration 128, loss = 0.06163282\n",
      "Iteration 129, loss = 0.06150962\n",
      "Iteration 130, loss = 0.06138780\n",
      "Iteration 131, loss = 0.06126686\n",
      "Iteration 132, loss = 0.06114817\n",
      "Iteration 133, loss = 0.06102999\n",
      "Iteration 134, loss = 0.06091371\n",
      "Iteration 135, loss = 0.06079856\n",
      "Iteration 136, loss = 0.06068443\n",
      "Iteration 137, loss = 0.06057199\n",
      "Iteration 138, loss = 0.06046031\n",
      "Iteration 139, loss = 0.06035018\n",
      "Iteration 140, loss = 0.06024097\n",
      "Iteration 141, loss = 0.06013286\n",
      "Iteration 142, loss = 0.06002600\n",
      "Iteration 143, loss = 0.05991994\n",
      "Iteration 144, loss = 0.05981523\n",
      "Iteration 145, loss = 0.05971137\n",
      "Iteration 146, loss = 0.05960860\n",
      "Iteration 147, loss = 0.05950697\n",
      "Iteration 148, loss = 0.05940639\n",
      "Iteration 149, loss = 0.05930707\n",
      "Iteration 150, loss = 0.05920863\n",
      "Iteration 151, loss = 0.05911122\n",
      "Iteration 152, loss = 0.05901467\n",
      "Iteration 153, loss = 0.05891900\n",
      "Iteration 154, loss = 0.05882430\n",
      "Iteration 155, loss = 0.05873041\n",
      "Iteration 156, loss = 0.05863744\n",
      "Iteration 157, loss = 0.05854527\n",
      "Iteration 158, loss = 0.05845395\n",
      "Iteration 159, loss = 0.05836342\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92980133\n",
      "Iteration 2, loss = 3.39060795\n",
      "Iteration 3, loss = 1.36145023\n",
      "Iteration 4, loss = 0.80647857\n",
      "Iteration 5, loss = 1.03637442\n",
      "Iteration 6, loss = 0.51452343\n",
      "Iteration 7, loss = 1.28941989\n",
      "Iteration 8, loss = 0.51563688\n",
      "Iteration 9, loss = 0.55275498\n",
      "Iteration 10, loss = 0.68952431\n",
      "Iteration 11, loss = 0.54466334\n",
      "Iteration 12, loss = 0.42181185\n",
      "Iteration 13, loss = 0.42402007\n",
      "Iteration 14, loss = 0.36968324\n",
      "Iteration 15, loss = 0.28392398\n",
      "Iteration 16, loss = 0.30431124\n",
      "Iteration 17, loss = 0.36108143\n",
      "Iteration 18, loss = 0.29644516\n",
      "Iteration 19, loss = 0.24733976\n",
      "Iteration 20, loss = 0.27960419\n",
      "Iteration 21, loss = 0.24411606\n",
      "Iteration 22, loss = 0.21188827\n",
      "Iteration 23, loss = 0.24112535\n",
      "Iteration 24, loss = 0.21441579\n",
      "Iteration 25, loss = 0.17840493\n",
      "Iteration 26, loss = 0.19405089\n",
      "Iteration 27, loss = 0.18003532\n",
      "Iteration 28, loss = 0.15352856\n",
      "Iteration 29, loss = 0.16393203\n",
      "Iteration 30, loss = 0.15596332\n",
      "Iteration 31, loss = 0.13407503\n",
      "Iteration 32, loss = 0.14065696\n",
      "Iteration 33, loss = 0.13551273\n",
      "Iteration 34, loss = 0.11874432\n",
      "Iteration 35, loss = 0.12359749\n",
      "Iteration 36, loss = 0.11984631\n",
      "Iteration 37, loss = 0.10719776\n",
      "Iteration 38, loss = 0.11059247\n",
      "Iteration 39, loss = 0.10766185\n",
      "Iteration 40, loss = 0.09842528\n",
      "Iteration 41, loss = 0.10125306\n",
      "Iteration 42, loss = 0.09856327\n",
      "Iteration 43, loss = 0.09184830\n",
      "Iteration 44, loss = 0.09409114\n",
      "Iteration 45, loss = 0.09146480\n",
      "Iteration 46, loss = 0.08688854\n",
      "Iteration 47, loss = 0.08868495\n",
      "Iteration 48, loss = 0.08607401\n",
      "Iteration 49, loss = 0.08355985\n",
      "Iteration 50, loss = 0.08298731\n",
      "Iteration 51, loss = 0.08064711\n",
      "Iteration 52, loss = 0.08051212\n",
      "Iteration 53, loss = 0.07964240\n",
      "Iteration 54, loss = 0.07798966\n",
      "Iteration 55, loss = 0.07836219\n",
      "Iteration 56, loss = 0.07614078\n",
      "Iteration 57, loss = 0.07518265\n",
      "Iteration 58, loss = 0.07429040\n",
      "Iteration 59, loss = 0.07352006\n",
      "Iteration 60, loss = 0.07284965\n",
      "Iteration 61, loss = 0.07213833\n",
      "Iteration 62, loss = 0.07152835\n",
      "Iteration 63, loss = 0.07082279\n",
      "Iteration 64, loss = 0.07023083\n",
      "Iteration 65, loss = 0.06956113\n",
      "Iteration 66, loss = 0.06886349\n",
      "Iteration 67, loss = 0.06822906\n",
      "Iteration 68, loss = 0.06754471\n",
      "Iteration 69, loss = 0.06684525\n",
      "Iteration 70, loss = 0.06622498\n",
      "Iteration 71, loss = 0.06640605\n",
      "Iteration 72, loss = 0.06944316\n",
      "Iteration 73, loss = 0.06947463\n",
      "Iteration 74, loss = 0.06384110\n",
      "Iteration 75, loss = 0.06565838\n",
      "Iteration 76, loss = 0.06535195\n",
      "Iteration 77, loss = 0.06212836\n",
      "Iteration 78, loss = 0.06428805\n",
      "Iteration 79, loss = 0.06190426\n",
      "Iteration 80, loss = 0.06154547\n",
      "Iteration 81, loss = 0.06247290\n",
      "Iteration 82, loss = 0.05995910\n",
      "Iteration 83, loss = 0.06065987\n",
      "Iteration 84, loss = 0.06088395\n",
      "Iteration 85, loss = 0.05860954\n",
      "Iteration 86, loss = 0.05902756\n",
      "Iteration 87, loss = 0.05980616\n",
      "Iteration 88, loss = 0.05791857\n",
      "Iteration 89, loss = 0.05680813\n",
      "Iteration 90, loss = 0.05759054\n",
      "Iteration 91, loss = 0.05819839\n",
      "Iteration 92, loss = 0.05780802\n",
      "Iteration 93, loss = 0.05592978\n",
      "Iteration 94, loss = 0.05477316\n",
      "Iteration 95, loss = 0.05481406\n",
      "Iteration 96, loss = 0.05576018\n",
      "Iteration 97, loss = 0.05784093\n",
      "Iteration 98, loss = 0.05860706\n",
      "Iteration 99, loss = 0.05867904\n",
      "Iteration 100, loss = 0.05414309\n",
      "Iteration 101, loss = 0.05271652\n",
      "Iteration 102, loss = 0.05497102\n",
      "Iteration 103, loss = 0.05574286\n",
      "Iteration 104, loss = 0.05429505\n",
      "Iteration 105, loss = 0.05170376\n",
      "Iteration 106, loss = 0.05234730\n",
      "Iteration 107, loss = 0.05432927\n",
      "Iteration 108, loss = 0.05303318\n",
      "Iteration 109, loss = 0.05108209\n",
      "Iteration 110, loss = 0.05075582\n",
      "Iteration 111, loss = 0.05187337\n",
      "Iteration 112, loss = 0.05247795\n",
      "Iteration 113, loss = 0.05090351\n",
      "Iteration 114, loss = 0.04977033\n",
      "Iteration 115, loss = 0.05000804\n",
      "Iteration 116, loss = 0.05070004\n",
      "Iteration 117, loss = 0.05094203\n",
      "Iteration 118, loss = 0.04988647\n",
      "Iteration 119, loss = 0.04898574\n",
      "Iteration 120, loss = 0.04876700\n",
      "Iteration 121, loss = 0.04912862\n",
      "Iteration 122, loss = 0.04963961\n",
      "Iteration 123, loss = 0.04951184\n",
      "Iteration 124, loss = 0.04919915\n",
      "Iteration 125, loss = 0.04844705\n",
      "Iteration 126, loss = 0.04789851\n",
      "Iteration 127, loss = 0.04761909\n",
      "Iteration 128, loss = 0.04761011\n",
      "Iteration 129, loss = 0.04779898\n",
      "Iteration 130, loss = 0.04804710\n",
      "Iteration 131, loss = 0.04855633\n",
      "Iteration 132, loss = 0.04883595\n",
      "Iteration 133, loss = 0.04962921\n",
      "Iteration 134, loss = 0.04934323\n",
      "Iteration 135, loss = 0.04935506\n",
      "Iteration 136, loss = 0.04797330\n",
      "Iteration 137, loss = 0.04692966\n",
      "Iteration 138, loss = 0.04625334\n",
      "Iteration 139, loss = 0.04627654\n",
      "Iteration 140, loss = 0.04676216\n",
      "Iteration 141, loss = 0.04710489\n",
      "Iteration 142, loss = 0.04735031\n",
      "Iteration 143, loss = 0.04676065\n",
      "Iteration 144, loss = 0.04616015\n",
      "Iteration 145, loss = 0.04558254\n",
      "Iteration 146, loss = 0.04538556\n",
      "Iteration 147, loss = 0.04551626\n",
      "Iteration 148, loss = 0.04572995\n",
      "Iteration 149, loss = 0.04592192\n",
      "Iteration 150, loss = 0.04574598\n",
      "Iteration 151, loss = 0.04550539\n",
      "Iteration 152, loss = 0.04508819\n",
      "Iteration 153, loss = 0.04477402\n",
      "Iteration 154, loss = 0.04459705\n",
      "Iteration 155, loss = 0.04456148\n",
      "Iteration 156, loss = 0.04461545\n",
      "Iteration 157, loss = 0.04467113\n",
      "Iteration 158, loss = 0.04474433\n",
      "Iteration 159, loss = 0.04469596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 160, loss = 0.04466862\n",
      "Iteration 161, loss = 0.04449834\n",
      "Iteration 162, loss = 0.04436699\n",
      "Iteration 163, loss = 0.04415426\n",
      "Iteration 164, loss = 0.04398593\n",
      "Iteration 165, loss = 0.04380399\n",
      "Iteration 166, loss = 0.04365863\n",
      "Iteration 167, loss = 0.04352629\n",
      "Iteration 168, loss = 0.04341628\n",
      "Iteration 169, loss = 0.04331807\n",
      "Iteration 170, loss = 0.04322935\n",
      "Iteration 171, loss = 0.04314672\n",
      "Iteration 172, loss = 0.04306829\n",
      "Iteration 173, loss = 0.04299317\n",
      "Iteration 174, loss = 0.04292156\n",
      "Iteration 175, loss = 0.04285565\n",
      "Iteration 176, loss = 0.04279906\n",
      "Iteration 177, loss = 0.04276247\n",
      "Iteration 178, loss = 0.04276014\n",
      "Iteration 179, loss = 0.04284740\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.94163030\n",
      "Iteration 2, loss = 3.40464561\n",
      "Iteration 3, loss = 1.36994469\n",
      "Iteration 4, loss = 0.81471570\n",
      "Iteration 5, loss = 1.09537369\n",
      "Iteration 6, loss = 0.52868664\n",
      "Iteration 7, loss = 1.26647530\n",
      "Iteration 8, loss = 0.56935444\n",
      "Iteration 9, loss = 0.51282026\n",
      "Iteration 10, loss = 0.66449759\n",
      "Iteration 11, loss = 0.56280234\n",
      "Iteration 12, loss = 0.43245892\n",
      "Iteration 13, loss = 0.43952956\n",
      "Iteration 14, loss = 0.39926065\n",
      "Iteration 15, loss = 0.32118186\n",
      "Iteration 16, loss = 0.31283931\n",
      "Iteration 17, loss = 0.36276022\n",
      "Iteration 18, loss = 0.35291155\n",
      "Iteration 19, loss = 0.29023516\n",
      "Iteration 20, loss = 0.29369215\n",
      "Iteration 21, loss = 0.29038631\n",
      "Iteration 22, loss = 0.23783109\n",
      "Iteration 23, loss = 0.25459140\n",
      "Iteration 24, loss = 0.25312474\n",
      "Iteration 25, loss = 0.20999864\n",
      "Iteration 26, loss = 0.22393928\n",
      "Iteration 27, loss = 0.21245456\n",
      "Iteration 28, loss = 0.18379435\n",
      "Iteration 29, loss = 0.19837799\n",
      "Iteration 30, loss = 0.18044927\n",
      "Iteration 31, loss = 0.16635194\n",
      "Iteration 32, loss = 0.17740928\n",
      "Iteration 33, loss = 0.15546173\n",
      "Iteration 34, loss = 0.15632704\n",
      "Iteration 35, loss = 0.15633208\n",
      "Iteration 36, loss = 0.14034041\n",
      "Iteration 37, loss = 0.14753941\n",
      "Iteration 38, loss = 0.13788813\n",
      "Iteration 39, loss = 0.13345396\n",
      "Iteration 40, loss = 0.13666002\n",
      "Iteration 41, loss = 0.12593442\n",
      "Iteration 42, loss = 0.12937812\n",
      "Iteration 43, loss = 0.12471311\n",
      "Iteration 44, loss = 0.12101962\n",
      "Iteration 45, loss = 0.12324702\n",
      "Iteration 46, loss = 0.11648391\n",
      "Iteration 47, loss = 0.11877201\n",
      "Iteration 48, loss = 0.11520879\n",
      "Iteration 49, loss = 0.11366634\n",
      "Iteration 50, loss = 0.11424456\n",
      "Iteration 51, loss = 0.11018743\n",
      "Iteration 52, loss = 0.11193680\n",
      "Iteration 53, loss = 0.10862836\n",
      "Iteration 54, loss = 0.10888656\n",
      "Iteration 55, loss = 0.10783040\n",
      "Iteration 56, loss = 0.10617897\n",
      "Iteration 57, loss = 0.10669907\n",
      "Iteration 58, loss = 0.10440105\n",
      "Iteration 59, loss = 0.10511223\n",
      "Iteration 60, loss = 0.10326385\n",
      "Iteration 61, loss = 0.10338542\n",
      "Iteration 62, loss = 0.10236190\n",
      "Iteration 63, loss = 0.10179930\n",
      "Iteration 64, loss = 0.10150040\n",
      "Iteration 65, loss = 0.10045064\n",
      "Iteration 66, loss = 0.10053567\n",
      "Iteration 67, loss = 0.09938698\n",
      "Iteration 68, loss = 0.09949830\n",
      "Iteration 69, loss = 0.09848898\n",
      "Iteration 70, loss = 0.09847843\n",
      "Iteration 71, loss = 0.09767930\n",
      "Iteration 72, loss = 0.09750044\n",
      "Iteration 73, loss = 0.09692682\n",
      "Iteration 74, loss = 0.09659385\n",
      "Iteration 75, loss = 0.09618266\n",
      "Iteration 76, loss = 0.09576612\n",
      "Iteration 77, loss = 0.09545963\n",
      "Iteration 78, loss = 0.09499520\n",
      "Iteration 79, loss = 0.09475047\n",
      "Iteration 80, loss = 0.09428247\n",
      "Iteration 81, loss = 0.09406259\n",
      "Iteration 82, loss = 0.09360949\n",
      "Iteration 83, loss = 0.09340176\n",
      "Iteration 84, loss = 0.09297332\n",
      "Iteration 85, loss = 0.09276353\n",
      "Iteration 86, loss = 0.09236278\n",
      "Iteration 87, loss = 0.09215147\n",
      "Iteration 88, loss = 0.09177720\n",
      "Iteration 89, loss = 0.09156260\n",
      "Iteration 90, loss = 0.09121567\n",
      "Iteration 91, loss = 0.09099951\n",
      "Iteration 92, loss = 0.09067475\n",
      "Iteration 93, loss = 0.09045566\n",
      "Iteration 94, loss = 0.09015149\n",
      "Iteration 95, loss = 0.08993139\n",
      "Iteration 96, loss = 0.08964580\n",
      "Iteration 97, loss = 0.08942420\n",
      "Iteration 98, loss = 0.08915598\n",
      "Iteration 99, loss = 0.08893392\n",
      "Iteration 100, loss = 0.08868225\n",
      "Iteration 101, loss = 0.08846001\n",
      "Iteration 102, loss = 0.08822333\n",
      "Iteration 103, loss = 0.08800142\n",
      "Iteration 104, loss = 0.08777877\n",
      "Iteration 105, loss = 0.08755856\n",
      "Iteration 106, loss = 0.08734786\n",
      "Iteration 107, loss = 0.08713012\n",
      "Iteration 108, loss = 0.08692930\n",
      "Iteration 109, loss = 0.08671602\n",
      "Iteration 110, loss = 0.08652258\n",
      "Iteration 111, loss = 0.08631518\n",
      "Iteration 112, loss = 0.08612673\n",
      "Iteration 113, loss = 0.08592684\n",
      "Iteration 114, loss = 0.08574178\n",
      "Iteration 115, loss = 0.08555055\n",
      "Iteration 116, loss = 0.08536807\n",
      "Iteration 117, loss = 0.08518542\n",
      "Iteration 118, loss = 0.08500518\n",
      "Iteration 119, loss = 0.08482990\n",
      "Iteration 120, loss = 0.08465269\n",
      "Iteration 121, loss = 0.08448314\n",
      "Iteration 122, loss = 0.08431017\n",
      "Iteration 123, loss = 0.08414462\n",
      "Iteration 124, loss = 0.08397700\n",
      "Iteration 125, loss = 0.08381439\n",
      "Iteration 126, loss = 0.08365260\n",
      "Iteration 127, loss = 0.08349263\n",
      "Iteration 128, loss = 0.08333612\n",
      "Iteration 129, loss = 0.08317948\n",
      "Iteration 130, loss = 0.08302730\n",
      "Iteration 131, loss = 0.08287480\n",
      "Iteration 132, loss = 0.08272587\n",
      "Iteration 133, loss = 0.08257778\n",
      "Iteration 134, loss = 0.08243149\n",
      "Iteration 135, loss = 0.08228755\n",
      "Iteration 136, loss = 0.08214417\n",
      "Iteration 137, loss = 0.08200371\n",
      "Iteration 138, loss = 0.08186380\n",
      "Iteration 139, loss = 0.08172609\n",
      "Iteration 140, loss = 0.08158974\n",
      "Iteration 141, loss = 0.08145460\n",
      "Iteration 142, loss = 0.08132152\n",
      "Iteration 143, loss = 0.08118922\n",
      "Iteration 144, loss = 0.08105888\n",
      "Iteration 145, loss = 0.08092966\n",
      "Iteration 146, loss = 0.08080178\n",
      "Iteration 147, loss = 0.08067549\n",
      "Iteration 148, loss = 0.08055016\n",
      "Iteration 149, loss = 0.08042647\n",
      "Iteration 150, loss = 0.08030388\n",
      "Iteration 151, loss = 0.08018256\n",
      "Iteration 152, loss = 0.08006263\n",
      "Iteration 153, loss = 0.07994369\n",
      "Iteration 154, loss = 0.07982615\n",
      "Iteration 155, loss = 0.07970968\n",
      "Iteration 156, loss = 0.07959436\n",
      "Iteration 157, loss = 0.07948029\n",
      "Iteration 158, loss = 0.07936723\n",
      "Iteration 159, loss = 0.07925539\n",
      "Iteration 160, loss = 0.07914462\n",
      "Iteration 161, loss = 0.07903487\n",
      "Iteration 162, loss = 0.07892627\n",
      "Iteration 163, loss = 0.07881871\n",
      "Iteration 164, loss = 0.07871234\n",
      "Iteration 165, loss = 0.07860702\n",
      "Iteration 166, loss = 0.07850265\n",
      "Iteration 167, loss = 0.07839933\n",
      "Iteration 168, loss = 0.07829698\n",
      "Iteration 169, loss = 0.07819558\n",
      "Iteration 170, loss = 0.07809516\n",
      "Iteration 171, loss = 0.07799564\n",
      "Iteration 172, loss = 0.07789706\n",
      "Iteration 173, loss = 0.07779940\n",
      "Iteration 174, loss = 0.07770262\n",
      "Iteration 175, loss = 0.07760685\n",
      "Iteration 176, loss = 0.07751201\n",
      "Iteration 177, loss = 0.07741803\n",
      "Iteration 178, loss = 0.07732491\n",
      "Iteration 179, loss = 0.07723265\n",
      "Iteration 180, loss = 0.07714120\n",
      "Iteration 181, loss = 0.07705060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91716398\n",
      "Iteration 2, loss = 3.92262608\n",
      "Iteration 3, loss = 1.63140018\n",
      "Iteration 4, loss = 1.12082820\n",
      "Iteration 5, loss = 0.89317501\n",
      "Iteration 6, loss = 0.81107945\n",
      "Iteration 7, loss = 0.70362097\n",
      "Iteration 8, loss = 0.58092223\n",
      "Iteration 9, loss = 0.50059998\n",
      "Iteration 10, loss = 0.45201741\n",
      "Iteration 11, loss = 0.41441154\n",
      "Iteration 12, loss = 0.38046211\n",
      "Iteration 13, loss = 0.34847094\n",
      "Iteration 14, loss = 0.31948239\n",
      "Iteration 15, loss = 0.29877254\n",
      "Iteration 16, loss = 0.40416144\n",
      "Iteration 17, loss = 1.78539580\n",
      "Iteration 18, loss = 1.50960723\n",
      "Iteration 19, loss = 1.14093166\n",
      "Iteration 20, loss = 0.47512657\n",
      "Iteration 21, loss = 0.40394162\n",
      "Iteration 22, loss = 0.41025548\n",
      "Iteration 23, loss = 0.40828609\n",
      "Iteration 24, loss = 0.40264580\n",
      "Iteration 25, loss = 0.39673264\n",
      "Iteration 26, loss = 0.38923500\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92564666\n",
      "Iteration 2, loss = 3.98811238\n",
      "Iteration 3, loss = 1.57678317\n",
      "Iteration 4, loss = 1.13301148\n",
      "Iteration 5, loss = 0.90165079\n",
      "Iteration 6, loss = 0.81668391\n",
      "Iteration 7, loss = 0.70095571\n",
      "Iteration 8, loss = 0.58036275\n",
      "Iteration 9, loss = 0.52531272\n",
      "Iteration 10, loss = 0.47608085\n",
      "Iteration 11, loss = 0.43419746\n",
      "Iteration 12, loss = 0.39618291\n",
      "Iteration 13, loss = 0.36001834\n",
      "Iteration 14, loss = 0.32697861\n",
      "Iteration 15, loss = 0.31473550\n",
      "Iteration 16, loss = 0.66104554\n",
      "Iteration 17, loss = 2.77017829\n",
      "Iteration 18, loss = 0.40186633\n",
      "Iteration 19, loss = 0.40979744\n",
      "Iteration 20, loss = 0.64637644\n",
      "Iteration 21, loss = 0.46102109\n",
      "Iteration 22, loss = 0.40501950\n",
      "Iteration 23, loss = 0.39851353\n",
      "Iteration 24, loss = 0.35890127\n",
      "Iteration 25, loss = 0.34311364\n",
      "Iteration 26, loss = 0.32762961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90632821\n",
      "Iteration 2, loss = 3.88334905\n",
      "Iteration 3, loss = 1.55055618\n",
      "Iteration 4, loss = 1.20847257\n",
      "Iteration 5, loss = 0.86140393\n",
      "Iteration 6, loss = 0.77620518\n",
      "Iteration 7, loss = 0.65289900\n",
      "Iteration 8, loss = 0.52595094\n",
      "Iteration 9, loss = 0.46332285\n",
      "Iteration 10, loss = 0.42144554\n",
      "Iteration 11, loss = 0.38699866\n",
      "Iteration 12, loss = 0.35474222\n",
      "Iteration 13, loss = 0.32444759\n",
      "Iteration 14, loss = 0.33700231\n",
      "Iteration 15, loss = 0.94670810\n",
      "Iteration 16, loss = 2.44463851\n",
      "Iteration 17, loss = 0.84304569\n",
      "Iteration 18, loss = 0.57260691\n",
      "Iteration 19, loss = 0.52266630\n",
      "Iteration 20, loss = 0.44793684\n",
      "Iteration 21, loss = 0.43493594\n",
      "Iteration 22, loss = 0.42107903\n",
      "Iteration 23, loss = 0.39962187\n",
      "Iteration 24, loss = 0.36801417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92980133\n",
      "Iteration 2, loss = 3.95536278\n",
      "Iteration 3, loss = 1.59658496\n",
      "Iteration 4, loss = 1.14667995\n",
      "Iteration 5, loss = 0.89258341\n",
      "Iteration 6, loss = 0.82073742\n",
      "Iteration 7, loss = 0.69262824\n",
      "Iteration 8, loss = 0.58531147\n",
      "Iteration 9, loss = 0.53678470\n",
      "Iteration 10, loss = 0.47828444\n",
      "Iteration 11, loss = 0.43534095\n",
      "Iteration 12, loss = 0.39626689\n",
      "Iteration 13, loss = 0.37104392\n",
      "Iteration 14, loss = 0.49280152\n",
      "Iteration 15, loss = 1.63043071\n",
      "Iteration 16, loss = 1.57705873\n",
      "Iteration 17, loss = 0.57289714\n",
      "Iteration 18, loss = 0.65495342\n",
      "Iteration 19, loss = 0.57581588\n",
      "Iteration 20, loss = 0.48865364\n",
      "Iteration 21, loss = 0.46354407\n",
      "Iteration 22, loss = 0.44845427\n",
      "Iteration 23, loss = 0.42719853\n",
      "Iteration 24, loss = 0.40797328\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.94163030\n",
      "Iteration 2, loss = 4.02333274\n",
      "Iteration 3, loss = 1.61379911\n",
      "Iteration 4, loss = 1.10258556\n",
      "Iteration 5, loss = 0.88934632\n",
      "Iteration 6, loss = 0.80640042\n",
      "Iteration 7, loss = 0.69353957\n",
      "Iteration 8, loss = 0.57847618\n",
      "Iteration 9, loss = 0.52126322\n",
      "Iteration 10, loss = 0.47024348\n",
      "Iteration 11, loss = 0.42932641\n",
      "Iteration 12, loss = 0.39290556\n",
      "Iteration 13, loss = 0.35832700\n",
      "Iteration 14, loss = 0.34107606\n",
      "Iteration 15, loss = 0.53362812\n",
      "Iteration 16, loss = 1.95606564\n",
      "Iteration 17, loss = 1.41272778\n",
      "Iteration 18, loss = 0.61147971\n",
      "Iteration 19, loss = 0.70125365\n",
      "Iteration 20, loss = 0.59965875\n",
      "Iteration 21, loss = 0.51181351\n",
      "Iteration 22, loss = 0.48223858\n",
      "Iteration 23, loss = 0.46586854\n",
      "Iteration 24, loss = 0.44204123\n",
      "Iteration 25, loss = 0.42010713\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91716398\n",
      "Iteration 2, loss = 1.53072926\n",
      "Iteration 3, loss = 1.24135840\n",
      "Iteration 4, loss = 1.06807408\n",
      "Iteration 5, loss = 1.03265039\n",
      "Iteration 6, loss = 1.04575491\n",
      "Iteration 7, loss = 1.00772329\n",
      "Iteration 8, loss = 0.92434942\n",
      "Iteration 9, loss = 0.83035834\n",
      "Iteration 10, loss = 0.75790882\n",
      "Iteration 11, loss = 0.72253314\n",
      "Iteration 12, loss = 0.70485697\n",
      "Iteration 13, loss = 0.68606728\n",
      "Iteration 14, loss = 0.66092068\n",
      "Iteration 15, loss = 0.63296253\n",
      "Iteration 16, loss = 0.60668663\n",
      "Iteration 17, loss = 0.58274988\n",
      "Iteration 18, loss = 0.55862055\n",
      "Iteration 19, loss = 0.53302607\n",
      "Iteration 20, loss = 0.50902140\n",
      "Iteration 21, loss = 0.48814000\n",
      "Iteration 22, loss = 0.47058633\n",
      "Iteration 23, loss = 0.45760176\n",
      "Iteration 24, loss = 0.44884260\n",
      "Iteration 25, loss = 0.44177655\n",
      "Iteration 26, loss = 0.43309863\n",
      "Iteration 27, loss = 0.42097580\n",
      "Iteration 28, loss = 0.40649051\n",
      "Iteration 29, loss = 0.39219834\n",
      "Iteration 30, loss = 0.38026424\n",
      "Iteration 31, loss = 0.37091523\n",
      "Iteration 32, loss = 0.36276409\n",
      "Iteration 33, loss = 0.35452869\n",
      "Iteration 34, loss = 0.34626963\n",
      "Iteration 35, loss = 0.33912949\n",
      "Iteration 36, loss = 0.33319711\n",
      "Iteration 37, loss = 0.32681341\n",
      "Iteration 38, loss = 0.31916672\n",
      "Iteration 39, loss = 0.31234174\n",
      "Iteration 40, loss = 0.30463349\n",
      "Iteration 41, loss = 0.29720380\n",
      "Iteration 42, loss = 0.29082011\n",
      "Iteration 43, loss = 0.28467694\n",
      "Iteration 44, loss = 0.27869548\n",
      "Iteration 45, loss = 0.27296260\n",
      "Iteration 46, loss = 0.26719452\n",
      "Iteration 47, loss = 0.26138230\n",
      "Iteration 48, loss = 0.25583131\n",
      "Iteration 49, loss = 0.25004521\n",
      "Iteration 50, loss = 0.24445718\n",
      "Iteration 51, loss = 0.23884846\n",
      "Iteration 52, loss = 0.23322700\n",
      "Iteration 53, loss = 0.22756136\n",
      "Iteration 54, loss = 0.22185237\n",
      "Iteration 55, loss = 0.21616014\n",
      "Iteration 56, loss = 0.21052647\n",
      "Iteration 57, loss = 0.20493552\n",
      "Iteration 58, loss = 0.19944235\n",
      "Iteration 59, loss = 0.19402622\n",
      "Iteration 60, loss = 0.18859398\n",
      "Iteration 61, loss = 0.18323798\n",
      "Iteration 62, loss = 0.17818791\n",
      "Iteration 63, loss = 0.17323665\n",
      "Iteration 64, loss = 0.16846181\n",
      "Iteration 65, loss = 0.16372529\n",
      "Iteration 66, loss = 0.15927939\n",
      "Iteration 67, loss = 0.15491079\n",
      "Iteration 68, loss = 0.15075371\n",
      "Iteration 69, loss = 0.14674908\n",
      "Iteration 70, loss = 0.14292811\n",
      "Iteration 71, loss = 0.13928147\n",
      "Iteration 72, loss = 0.13582740\n",
      "Iteration 73, loss = 0.13252822\n",
      "Iteration 74, loss = 0.12945945\n",
      "Iteration 75, loss = 0.12649472\n",
      "Iteration 76, loss = 0.12380730\n",
      "Iteration 77, loss = 0.12117246\n",
      "Iteration 78, loss = 0.11875649\n",
      "Iteration 79, loss = 0.11646518\n",
      "Iteration 80, loss = 0.11427358\n",
      "Iteration 81, loss = 0.11229305\n",
      "Iteration 82, loss = 0.11037740\n",
      "Iteration 83, loss = 0.10858283\n",
      "Iteration 84, loss = 0.10694864\n",
      "Iteration 85, loss = 0.10536588\n",
      "Iteration 86, loss = 0.10388463\n",
      "Iteration 87, loss = 0.10253971\n",
      "Iteration 88, loss = 0.10124511\n",
      "Iteration 89, loss = 0.10002004\n",
      "Iteration 90, loss = 0.09890709\n",
      "Iteration 91, loss = 0.09784793\n",
      "Iteration 92, loss = 0.09683261\n",
      "Iteration 93, loss = 0.09590136\n",
      "Iteration 94, loss = 0.09503198\n",
      "Iteration 95, loss = 0.09419315\n",
      "Iteration 96, loss = 0.09340465\n",
      "Iteration 97, loss = 0.09267894\n",
      "Iteration 98, loss = 0.09199275\n",
      "Iteration 99, loss = 0.09133253\n",
      "Iteration 100, loss = 0.09070839\n",
      "Iteration 101, loss = 0.09012809\n",
      "Iteration 102, loss = 0.08958265\n",
      "Iteration 103, loss = 0.08906102\n",
      "Iteration 104, loss = 0.08856067\n",
      "Iteration 105, loss = 0.08808681\n",
      "Iteration 106, loss = 0.08764138\n",
      "Iteration 107, loss = 0.08722052\n",
      "Iteration 108, loss = 0.08681950\n",
      "Iteration 109, loss = 0.08643523\n",
      "Iteration 110, loss = 0.08606584\n",
      "Iteration 111, loss = 0.08571213\n",
      "Iteration 112, loss = 0.08537372\n",
      "Iteration 113, loss = 0.08505049\n",
      "Iteration 114, loss = 0.08474161\n",
      "Iteration 115, loss = 0.08444618\n",
      "Iteration 116, loss = 0.08416362\n",
      "Iteration 117, loss = 0.08389323\n",
      "Iteration 118, loss = 0.08363456\n",
      "Iteration 119, loss = 0.08338863\n",
      "Iteration 120, loss = 0.08315649\n",
      "Iteration 121, loss = 0.08294207\n",
      "Iteration 122, loss = 0.08274619\n",
      "Iteration 123, loss = 0.08256165\n",
      "Iteration 124, loss = 0.08236720\n",
      "Iteration 125, loss = 0.08213159\n",
      "Iteration 126, loss = 0.08187791\n",
      "Iteration 127, loss = 0.08165889\n",
      "Iteration 128, loss = 0.08149651\n",
      "Iteration 129, loss = 0.08135709\n",
      "Iteration 130, loss = 0.08119247\n",
      "Iteration 131, loss = 0.08099499\n",
      "Iteration 132, loss = 0.08079970\n",
      "Iteration 133, loss = 0.08064246\n",
      "Iteration 134, loss = 0.08051225\n",
      "Iteration 135, loss = 0.08037354\n",
      "Iteration 136, loss = 0.08021235\n",
      "Iteration 137, loss = 0.08004594\n",
      "Iteration 138, loss = 0.07990021\n",
      "Iteration 139, loss = 0.07977576\n",
      "Iteration 140, loss = 0.07965332\n",
      "Iteration 141, loss = 0.07951916\n",
      "Iteration 142, loss = 0.07937654\n",
      "Iteration 143, loss = 0.07924046\n",
      "Iteration 144, loss = 0.07911825\n",
      "Iteration 145, loss = 0.07900531\n",
      "Iteration 146, loss = 0.07889185\n",
      "Iteration 147, loss = 0.07877269\n",
      "Iteration 148, loss = 0.07865037\n",
      "Iteration 149, loss = 0.07853126\n",
      "Iteration 150, loss = 0.07841950\n",
      "Iteration 151, loss = 0.07831415\n",
      "Iteration 152, loss = 0.07821142\n",
      "Iteration 153, loss = 0.07810801\n",
      "Iteration 154, loss = 0.07800277\n",
      "Iteration 155, loss = 0.07789703\n",
      "Iteration 156, loss = 0.07779281\n",
      "Iteration 157, loss = 0.07769167\n",
      "Iteration 158, loss = 0.07759401\n",
      "Iteration 159, loss = 0.07749929\n",
      "Iteration 160, loss = 0.07740661\n",
      "Iteration 161, loss = 0.07731526\n",
      "Iteration 162, loss = 0.07722476\n",
      "Iteration 163, loss = 0.07713509\n",
      "Iteration 164, loss = 0.07704622\n",
      "Iteration 165, loss = 0.07695827\n",
      "Iteration 166, loss = 0.07687157\n",
      "Iteration 167, loss = 0.07678603\n",
      "Iteration 168, loss = 0.07670190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92564666\n",
      "Iteration 2, loss = 1.53593022\n",
      "Iteration 3, loss = 1.24185233\n",
      "Iteration 4, loss = 1.06571155\n",
      "Iteration 5, loss = 1.03232224\n",
      "Iteration 6, loss = 1.04888826\n",
      "Iteration 7, loss = 1.01284221\n",
      "Iteration 8, loss = 0.92981193\n",
      "Iteration 9, loss = 0.83610920\n",
      "Iteration 10, loss = 0.76363602\n",
      "Iteration 11, loss = 0.72712302\n",
      "Iteration 12, loss = 0.70908490\n",
      "Iteration 13, loss = 0.68953059\n",
      "Iteration 14, loss = 0.66365528\n",
      "Iteration 15, loss = 0.63574884\n",
      "Iteration 16, loss = 0.61011563\n",
      "Iteration 17, loss = 0.58646735\n",
      "Iteration 18, loss = 0.56198541\n",
      "Iteration 19, loss = 0.53623665\n",
      "Iteration 20, loss = 0.51276559\n",
      "Iteration 21, loss = 0.49125206\n",
      "Iteration 22, loss = 0.47318603\n",
      "Iteration 23, loss = 0.45996093\n",
      "Iteration 24, loss = 0.45101912\n",
      "Iteration 25, loss = 0.44342171\n",
      "Iteration 26, loss = 0.43401055\n",
      "Iteration 27, loss = 0.42117723\n",
      "Iteration 28, loss = 0.40623987\n",
      "Iteration 29, loss = 0.39181825\n",
      "Iteration 30, loss = 0.37984878\n",
      "Iteration 31, loss = 0.37033364\n",
      "Iteration 32, loss = 0.36197327\n",
      "Iteration 33, loss = 0.35356962\n",
      "Iteration 34, loss = 0.34551168\n",
      "Iteration 35, loss = 0.33860594\n",
      "Iteration 36, loss = 0.33244359\n",
      "Iteration 37, loss = 0.32543909\n",
      "Iteration 38, loss = 0.31738837\n",
      "Iteration 39, loss = 0.30930068\n",
      "Iteration 40, loss = 0.30153611\n",
      "Iteration 41, loss = 0.29448418\n",
      "Iteration 42, loss = 0.28780068\n",
      "Iteration 43, loss = 0.28125638\n",
      "Iteration 44, loss = 0.27482794\n",
      "Iteration 45, loss = 0.26861943\n",
      "Iteration 46, loss = 0.26251583\n",
      "Iteration 47, loss = 0.25637177\n",
      "Iteration 48, loss = 0.25013778\n",
      "Iteration 49, loss = 0.24397125\n",
      "Iteration 50, loss = 0.23806035\n",
      "Iteration 51, loss = 0.23187856\n",
      "Iteration 52, loss = 0.22610430\n",
      "Iteration 53, loss = 0.22021504\n",
      "Iteration 54, loss = 0.21402521\n",
      "Iteration 55, loss = 0.20820510\n",
      "Iteration 56, loss = 0.20238322\n",
      "Iteration 57, loss = 0.19658982\n",
      "Iteration 58, loss = 0.19107256\n",
      "Iteration 59, loss = 0.18551572\n",
      "Iteration 60, loss = 0.18003389\n",
      "Iteration 61, loss = 0.17477745\n",
      "Iteration 62, loss = 0.16953099\n",
      "Iteration 63, loss = 0.16453101\n",
      "Iteration 64, loss = 0.15964408\n",
      "Iteration 65, loss = 0.15489374\n",
      "Iteration 66, loss = 0.15029683\n",
      "Iteration 67, loss = 0.14581337\n",
      "Iteration 68, loss = 0.14155706\n",
      "Iteration 69, loss = 0.13746813\n",
      "Iteration 70, loss = 0.13358499\n",
      "Iteration 71, loss = 0.12985687\n",
      "Iteration 72, loss = 0.12633706\n",
      "Iteration 73, loss = 0.12300377\n",
      "Iteration 74, loss = 0.11986809\n",
      "Iteration 75, loss = 0.11690781\n",
      "Iteration 76, loss = 0.11412311\n",
      "Iteration 77, loss = 0.11151066\n",
      "Iteration 78, loss = 0.10905153\n",
      "Iteration 79, loss = 0.10676363\n",
      "Iteration 80, loss = 0.10461552\n",
      "Iteration 81, loss = 0.10261375\n",
      "Iteration 82, loss = 0.10075300\n",
      "Iteration 83, loss = 0.09901077\n",
      "Iteration 84, loss = 0.09739191\n",
      "Iteration 85, loss = 0.09589102\n",
      "Iteration 86, loss = 0.09449063\n",
      "Iteration 87, loss = 0.09319073\n",
      "Iteration 88, loss = 0.09198851\n",
      "Iteration 89, loss = 0.09086981\n",
      "Iteration 90, loss = 0.08982692\n",
      "Iteration 91, loss = 0.08886035\n",
      "Iteration 92, loss = 0.08796480\n",
      "Iteration 93, loss = 0.08713036\n",
      "Iteration 94, loss = 0.08635313\n",
      "Iteration 95, loss = 0.08563052\n",
      "Iteration 96, loss = 0.08496021\n",
      "Iteration 97, loss = 0.08433872\n",
      "Iteration 98, loss = 0.08375999\n",
      "Iteration 99, loss = 0.08322049\n",
      "Iteration 100, loss = 0.08271687\n",
      "Iteration 101, loss = 0.08224659\n",
      "Iteration 102, loss = 0.08180730\n",
      "Iteration 103, loss = 0.08139688\n",
      "Iteration 104, loss = 0.08101251\n",
      "Iteration 105, loss = 0.08065226\n",
      "Iteration 106, loss = 0.08031459\n",
      "Iteration 107, loss = 0.07999760\n",
      "Iteration 108, loss = 0.07970038\n",
      "Iteration 109, loss = 0.07942257\n",
      "Iteration 110, loss = 0.07916609\n",
      "Iteration 111, loss = 0.07893358\n",
      "Iteration 112, loss = 0.07872788\n",
      "Iteration 113, loss = 0.07853620\n",
      "Iteration 114, loss = 0.07831688\n",
      "Iteration 115, loss = 0.07806323\n",
      "Iteration 116, loss = 0.07783027\n",
      "Iteration 117, loss = 0.07766437\n",
      "Iteration 118, loss = 0.07752601\n",
      "Iteration 119, loss = 0.07735403\n",
      "Iteration 120, loss = 0.07715701\n",
      "Iteration 121, loss = 0.07698955\n",
      "Iteration 122, loss = 0.07686303\n",
      "Iteration 123, loss = 0.07673315\n",
      "Iteration 124, loss = 0.07657663\n",
      "Iteration 125, loss = 0.07642361\n",
      "Iteration 126, loss = 0.07629982\n",
      "Iteration 127, loss = 0.07618802\n",
      "Iteration 128, loss = 0.07606260\n",
      "Iteration 129, loss = 0.07592791\n",
      "Iteration 130, loss = 0.07580715\n",
      "Iteration 131, loss = 0.07570247\n",
      "Iteration 132, loss = 0.07559793\n",
      "Iteration 133, loss = 0.07548447\n",
      "Iteration 134, loss = 0.07537098\n",
      "Iteration 135, loss = 0.07526835\n",
      "Iteration 136, loss = 0.07517344\n",
      "Iteration 137, loss = 0.07507659\n",
      "Iteration 138, loss = 0.07497563\n",
      "Iteration 139, loss = 0.07487529\n",
      "Iteration 140, loss = 0.07478135\n",
      "Iteration 141, loss = 0.07469341\n",
      "Iteration 142, loss = 0.07460595\n",
      "Iteration 143, loss = 0.07451656\n",
      "Iteration 144, loss = 0.07442632\n",
      "Iteration 145, loss = 0.07433854\n",
      "Iteration 146, loss = 0.07425435\n",
      "Iteration 147, loss = 0.07417288\n",
      "Iteration 148, loss = 0.07409221\n",
      "Iteration 149, loss = 0.07401166\n",
      "Iteration 150, loss = 0.07393106\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90632821\n",
      "Iteration 2, loss = 1.52143197\n",
      "Iteration 3, loss = 1.23316583\n",
      "Iteration 4, loss = 1.06138341\n",
      "Iteration 5, loss = 1.02630559\n",
      "Iteration 6, loss = 1.03711330\n",
      "Iteration 7, loss = 0.99680069\n",
      "Iteration 8, loss = 0.91191110\n",
      "Iteration 9, loss = 0.81925558\n",
      "Iteration 10, loss = 0.74820288\n",
      "Iteration 11, loss = 0.71100643\n",
      "Iteration 12, loss = 0.69322792\n",
      "Iteration 13, loss = 0.67359237\n",
      "Iteration 14, loss = 0.64773417\n",
      "Iteration 15, loss = 0.61934004\n",
      "Iteration 16, loss = 0.59252018\n",
      "Iteration 17, loss = 0.56749074\n",
      "Iteration 18, loss = 0.54264357\n",
      "Iteration 19, loss = 0.51906265\n",
      "Iteration 20, loss = 0.49672686\n",
      "Iteration 21, loss = 0.47540530\n",
      "Iteration 22, loss = 0.45694991\n",
      "Iteration 23, loss = 0.44306282\n",
      "Iteration 24, loss = 0.43343214\n",
      "Iteration 25, loss = 0.42529680\n",
      "Iteration 26, loss = 0.41554897\n",
      "Iteration 27, loss = 0.40266446\n",
      "Iteration 28, loss = 0.38775647\n",
      "Iteration 29, loss = 0.37332241\n",
      "Iteration 30, loss = 0.36127291\n",
      "Iteration 31, loss = 0.35167533\n",
      "Iteration 32, loss = 0.34312154\n",
      "Iteration 33, loss = 0.33451584\n",
      "Iteration 34, loss = 0.32625617\n",
      "Iteration 35, loss = 0.31902676\n",
      "Iteration 36, loss = 0.31230941\n",
      "Iteration 37, loss = 0.30475358\n",
      "Iteration 38, loss = 0.29614110\n",
      "Iteration 39, loss = 0.28839504\n",
      "Iteration 40, loss = 0.28044754\n",
      "Iteration 41, loss = 0.27314661\n",
      "Iteration 42, loss = 0.26636964\n",
      "Iteration 43, loss = 0.25981527\n",
      "Iteration 44, loss = 0.25340746\n",
      "Iteration 45, loss = 0.24717951\n",
      "Iteration 46, loss = 0.24095703\n",
      "Iteration 47, loss = 0.23471212\n",
      "Iteration 48, loss = 0.22857671\n",
      "Iteration 49, loss = 0.22237470\n",
      "Iteration 50, loss = 0.21625662\n",
      "Iteration 51, loss = 0.21014036\n",
      "Iteration 52, loss = 0.20403361\n",
      "Iteration 53, loss = 0.19790485\n",
      "Iteration 54, loss = 0.19177012\n",
      "Iteration 55, loss = 0.18569287\n",
      "Iteration 56, loss = 0.17977817\n",
      "Iteration 57, loss = 0.17392964\n",
      "Iteration 58, loss = 0.16818748\n",
      "Iteration 59, loss = 0.16263396\n",
      "Iteration 60, loss = 0.15711779\n",
      "Iteration 61, loss = 0.15164146\n",
      "Iteration 62, loss = 0.14644605\n",
      "Iteration 63, loss = 0.14144498\n",
      "Iteration 64, loss = 0.13659237\n",
      "Iteration 65, loss = 0.13188689\n",
      "Iteration 66, loss = 0.12733609\n",
      "Iteration 67, loss = 0.12293246\n",
      "Iteration 68, loss = 0.11871193\n",
      "Iteration 69, loss = 0.11467461\n",
      "Iteration 70, loss = 0.11084750\n",
      "Iteration 71, loss = 0.10723629\n",
      "Iteration 72, loss = 0.10383940\n",
      "Iteration 73, loss = 0.10061981\n",
      "Iteration 74, loss = 0.09759294\n",
      "Iteration 75, loss = 0.09477748\n",
      "Iteration 76, loss = 0.09216202\n",
      "Iteration 77, loss = 0.08971554\n",
      "Iteration 78, loss = 0.08741908\n",
      "Iteration 79, loss = 0.08526597\n",
      "Iteration 80, loss = 0.08325806\n",
      "Iteration 81, loss = 0.08140102\n",
      "Iteration 82, loss = 0.07968322\n",
      "Iteration 83, loss = 0.07809212\n",
      "Iteration 84, loss = 0.07661496\n",
      "Iteration 85, loss = 0.07524729\n",
      "Iteration 86, loss = 0.07398078\n",
      "Iteration 87, loss = 0.07280887\n",
      "Iteration 88, loss = 0.07172479\n",
      "Iteration 89, loss = 0.07072121\n",
      "Iteration 90, loss = 0.06979410\n",
      "Iteration 91, loss = 0.06893524\n",
      "Iteration 92, loss = 0.06813835\n",
      "Iteration 93, loss = 0.06739974\n",
      "Iteration 94, loss = 0.06671443\n",
      "Iteration 95, loss = 0.06607746\n",
      "Iteration 96, loss = 0.06548582\n",
      "Iteration 97, loss = 0.06493508\n",
      "Iteration 98, loss = 0.06442172\n",
      "Iteration 99, loss = 0.06394380\n",
      "Iteration 100, loss = 0.06349804\n",
      "Iteration 101, loss = 0.06308087\n",
      "Iteration 102, loss = 0.06269079\n",
      "Iteration 103, loss = 0.06232567\n",
      "Iteration 104, loss = 0.06198298\n",
      "Iteration 105, loss = 0.06166102\n",
      "Iteration 106, loss = 0.06135865\n",
      "Iteration 107, loss = 0.06107396\n",
      "Iteration 108, loss = 0.06080548\n",
      "Iteration 109, loss = 0.06055211\n",
      "Iteration 110, loss = 0.06031278\n",
      "Iteration 111, loss = 0.06008616\n",
      "Iteration 112, loss = 0.05987124\n",
      "Iteration 113, loss = 0.05966726\n",
      "Iteration 114, loss = 0.05947338\n",
      "Iteration 115, loss = 0.05928873\n",
      "Iteration 116, loss = 0.05911272\n",
      "Iteration 117, loss = 0.05894473\n",
      "Iteration 118, loss = 0.05878416\n",
      "Iteration 119, loss = 0.05863051\n",
      "Iteration 120, loss = 0.05848334\n",
      "Iteration 121, loss = 0.05834216\n",
      "Iteration 122, loss = 0.05820656\n",
      "Iteration 123, loss = 0.05807613\n",
      "Iteration 124, loss = 0.05795054\n",
      "Iteration 125, loss = 0.05782946\n",
      "Iteration 126, loss = 0.05771258\n",
      "Iteration 127, loss = 0.05759960\n",
      "Iteration 128, loss = 0.05749028\n",
      "Iteration 129, loss = 0.05738439\n",
      "Iteration 130, loss = 0.05728169\n",
      "Iteration 131, loss = 0.05718199\n",
      "Iteration 132, loss = 0.05708509\n",
      "Iteration 133, loss = 0.05699081\n",
      "Iteration 134, loss = 0.05689898\n",
      "Iteration 135, loss = 0.05680947\n",
      "Iteration 136, loss = 0.05672209\n",
      "Iteration 137, loss = 0.05663673\n",
      "Iteration 138, loss = 0.05655326\n",
      "Iteration 139, loss = 0.05647158\n",
      "Iteration 140, loss = 0.05639159\n",
      "Iteration 141, loss = 0.05631318\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92980133\n",
      "Iteration 2, loss = 1.53939357\n",
      "Iteration 3, loss = 1.24584935\n",
      "Iteration 4, loss = 1.07053841\n",
      "Iteration 5, loss = 1.03578649\n",
      "Iteration 6, loss = 1.04999023\n",
      "Iteration 7, loss = 1.01255598\n",
      "Iteration 8, loss = 0.92877748\n",
      "Iteration 9, loss = 0.83484490\n",
      "Iteration 10, loss = 0.76128978\n",
      "Iteration 11, loss = 0.71973032\n",
      "Iteration 12, loss = 0.70163299\n",
      "Iteration 13, loss = 0.68322067\n",
      "Iteration 14, loss = 0.65858110\n",
      "Iteration 15, loss = 0.63110817\n",
      "Iteration 16, loss = 0.60472320\n",
      "Iteration 17, loss = 0.57924346\n",
      "Iteration 18, loss = 0.55321576\n",
      "Iteration 19, loss = 0.52749294\n",
      "Iteration 20, loss = 0.50417866\n",
      "Iteration 21, loss = 0.48250338\n",
      "Iteration 22, loss = 0.46395711\n",
      "Iteration 23, loss = 0.44987838\n",
      "Iteration 24, loss = 0.44002261\n",
      "Iteration 25, loss = 0.43171033\n",
      "Iteration 26, loss = 0.42193557\n",
      "Iteration 27, loss = 0.40895628\n",
      "Iteration 28, loss = 0.39368580\n",
      "Iteration 29, loss = 0.37856219\n",
      "Iteration 30, loss = 0.36569372\n",
      "Iteration 31, loss = 0.35553402\n",
      "Iteration 32, loss = 0.34672715\n",
      "Iteration 33, loss = 0.33810719\n",
      "Iteration 34, loss = 0.32981165\n",
      "Iteration 35, loss = 0.32225981\n",
      "Iteration 36, loss = 0.31530762\n",
      "Iteration 37, loss = 0.30774424\n",
      "Iteration 38, loss = 0.29994901\n",
      "Iteration 39, loss = 0.29088333\n",
      "Iteration 40, loss = 0.28283228\n",
      "Iteration 41, loss = 0.27549589\n",
      "Iteration 42, loss = 0.26855393\n",
      "Iteration 43, loss = 0.26181973\n",
      "Iteration 44, loss = 0.25514807\n",
      "Iteration 45, loss = 0.24862362\n",
      "Iteration 46, loss = 0.24228789\n",
      "Iteration 47, loss = 0.23600693\n",
      "Iteration 48, loss = 0.22969668\n",
      "Iteration 49, loss = 0.22333882\n",
      "Iteration 50, loss = 0.21700868\n",
      "Iteration 51, loss = 0.21071286\n",
      "Iteration 52, loss = 0.20451286\n",
      "Iteration 53, loss = 0.19841253\n",
      "Iteration 54, loss = 0.19230868\n",
      "Iteration 55, loss = 0.18626086\n",
      "Iteration 56, loss = 0.18032794\n",
      "Iteration 57, loss = 0.17434735\n",
      "Iteration 58, loss = 0.16859166\n",
      "Iteration 59, loss = 0.16300167\n",
      "Iteration 60, loss = 0.15752405\n",
      "Iteration 61, loss = 0.15214525\n",
      "Iteration 62, loss = 0.14688236\n",
      "Iteration 63, loss = 0.14173764\n",
      "Iteration 64, loss = 0.13674667\n",
      "Iteration 65, loss = 0.13191462\n",
      "Iteration 66, loss = 0.12723128\n",
      "Iteration 67, loss = 0.12272532\n",
      "Iteration 68, loss = 0.11841179\n",
      "Iteration 69, loss = 0.11426581\n",
      "Iteration 70, loss = 0.11031194\n",
      "Iteration 71, loss = 0.10657123\n",
      "Iteration 72, loss = 0.10301456\n",
      "Iteration 73, loss = 0.09964299\n",
      "Iteration 74, loss = 0.09644853\n",
      "Iteration 75, loss = 0.09343464\n",
      "Iteration 76, loss = 0.09060009\n",
      "Iteration 77, loss = 0.08793207\n",
      "Iteration 78, loss = 0.08541662\n",
      "Iteration 79, loss = 0.08304698\n",
      "Iteration 80, loss = 0.08081965\n",
      "Iteration 81, loss = 0.07872713\n",
      "Iteration 82, loss = 0.07676398\n",
      "Iteration 83, loss = 0.07492218\n",
      "Iteration 84, loss = 0.07319331\n",
      "Iteration 85, loss = 0.07157180\n",
      "Iteration 86, loss = 0.07005366\n",
      "Iteration 87, loss = 0.06862976\n",
      "Iteration 88, loss = 0.06729608\n",
      "Iteration 89, loss = 0.06604470\n",
      "Iteration 90, loss = 0.06486949\n",
      "Iteration 91, loss = 0.06376823\n",
      "Iteration 92, loss = 0.06273453\n",
      "Iteration 93, loss = 0.06176242\n",
      "Iteration 94, loss = 0.06084919\n",
      "Iteration 95, loss = 0.05999111\n",
      "Iteration 96, loss = 0.05918390\n",
      "Iteration 97, loss = 0.05842327\n",
      "Iteration 98, loss = 0.05770778\n",
      "Iteration 99, loss = 0.05703289\n",
      "Iteration 100, loss = 0.05639491\n",
      "Iteration 101, loss = 0.05579177\n",
      "Iteration 102, loss = 0.05522172\n",
      "Iteration 103, loss = 0.05468261\n",
      "Iteration 104, loss = 0.05417172\n",
      "Iteration 105, loss = 0.05368716\n",
      "Iteration 106, loss = 0.05322735\n",
      "Iteration 107, loss = 0.05279074\n",
      "Iteration 108, loss = 0.05237584\n",
      "Iteration 109, loss = 0.05198101\n",
      "Iteration 110, loss = 0.05160476\n",
      "Iteration 111, loss = 0.05124574\n",
      "Iteration 112, loss = 0.05090287\n",
      "Iteration 113, loss = 0.05057518\n",
      "Iteration 114, loss = 0.05026177\n",
      "Iteration 115, loss = 0.04996177\n",
      "Iteration 116, loss = 0.04967435\n",
      "Iteration 117, loss = 0.04939880\n",
      "Iteration 118, loss = 0.04913434\n",
      "Iteration 119, loss = 0.04888059\n",
      "Iteration 120, loss = 0.04863678\n",
      "Iteration 121, loss = 0.04840253\n",
      "Iteration 122, loss = 0.04817784\n",
      "Iteration 123, loss = 0.04796346\n",
      "Iteration 124, loss = 0.04776228\n",
      "Iteration 125, loss = 0.04758081\n",
      "Iteration 126, loss = 0.04742864\n",
      "Iteration 127, loss = 0.04729763\n",
      "Iteration 128, loss = 0.04711465\n",
      "Iteration 129, loss = 0.04686849\n",
      "Iteration 130, loss = 0.04666541\n",
      "Iteration 131, loss = 0.04655970\n",
      "Iteration 132, loss = 0.04643738\n",
      "Iteration 133, loss = 0.04623489\n",
      "Iteration 134, loss = 0.04607041\n",
      "Iteration 135, loss = 0.04597417\n",
      "Iteration 136, loss = 0.04584009\n",
      "Iteration 137, loss = 0.04566938\n",
      "Iteration 138, loss = 0.04554668\n",
      "Iteration 139, loss = 0.04544898\n",
      "Iteration 140, loss = 0.04531250\n",
      "Iteration 141, loss = 0.04517264\n",
      "Iteration 142, loss = 0.04507330\n",
      "Iteration 143, loss = 0.04497063\n",
      "Iteration 144, loss = 0.04484070\n",
      "Iteration 145, loss = 0.04472728\n",
      "Iteration 146, loss = 0.04463796\n",
      "Iteration 147, loss = 0.04453427\n",
      "Iteration 148, loss = 0.04441759\n",
      "Iteration 149, loss = 0.04431930\n",
      "Iteration 150, loss = 0.04423290\n",
      "Iteration 151, loss = 0.04413415\n",
      "Iteration 152, loss = 0.04403011\n",
      "Iteration 153, loss = 0.04394072\n",
      "Iteration 154, loss = 0.04385814\n",
      "Iteration 155, loss = 0.04376597\n",
      "Iteration 156, loss = 0.04367160\n",
      "Iteration 157, loss = 0.04358718\n",
      "Iteration 158, loss = 0.04350807\n",
      "Iteration 159, loss = 0.04342407\n",
      "Iteration 160, loss = 0.04333760\n",
      "Iteration 161, loss = 0.04325713\n",
      "Iteration 162, loss = 0.04318179\n",
      "Iteration 163, loss = 0.04310491\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.94163030\n",
      "Iteration 2, loss = 1.54802883\n",
      "Iteration 3, loss = 1.24985438\n",
      "Iteration 4, loss = 1.06867523\n",
      "Iteration 5, loss = 1.03134124\n",
      "Iteration 6, loss = 1.04843548\n",
      "Iteration 7, loss = 1.01330630\n",
      "Iteration 8, loss = 0.92975774\n",
      "Iteration 9, loss = 0.83412512\n",
      "Iteration 10, loss = 0.75916778\n",
      "Iteration 11, loss = 0.71939948\n",
      "Iteration 12, loss = 0.70163541\n",
      "Iteration 13, loss = 0.68325086\n",
      "Iteration 14, loss = 0.65797465\n",
      "Iteration 15, loss = 0.62958630\n",
      "Iteration 16, loss = 0.60325424\n",
      "Iteration 17, loss = 0.57951096\n",
      "Iteration 18, loss = 0.55580589\n",
      "Iteration 19, loss = 0.53139525\n",
      "Iteration 20, loss = 0.50872980\n",
      "Iteration 21, loss = 0.48750263\n",
      "Iteration 22, loss = 0.46923634\n",
      "Iteration 23, loss = 0.45560874\n",
      "Iteration 24, loss = 0.44656260\n",
      "Iteration 25, loss = 0.43967349\n",
      "Iteration 26, loss = 0.43156418\n",
      "Iteration 27, loss = 0.42017702\n",
      "Iteration 28, loss = 0.40631075\n",
      "Iteration 29, loss = 0.39232512\n",
      "Iteration 30, loss = 0.38038372\n",
      "Iteration 31, loss = 0.37098583\n",
      "Iteration 32, loss = 0.36285311\n",
      "Iteration 33, loss = 0.35463784\n",
      "Iteration 34, loss = 0.34649886\n",
      "Iteration 35, loss = 0.33948397\n",
      "Iteration 36, loss = 0.33353170\n",
      "Iteration 37, loss = 0.32724702\n",
      "Iteration 38, loss = 0.31980087\n",
      "Iteration 39, loss = 0.31296197\n",
      "Iteration 40, loss = 0.30497777\n",
      "Iteration 41, loss = 0.29796236\n",
      "Iteration 42, loss = 0.29163421\n",
      "Iteration 43, loss = 0.28551319\n",
      "Iteration 44, loss = 0.27953269\n",
      "Iteration 45, loss = 0.27372498\n",
      "Iteration 46, loss = 0.26790787\n",
      "Iteration 47, loss = 0.26211554\n",
      "Iteration 48, loss = 0.25636863\n",
      "Iteration 49, loss = 0.25068271\n",
      "Iteration 50, loss = 0.24496538\n",
      "Iteration 51, loss = 0.23919312\n",
      "Iteration 52, loss = 0.23336649\n",
      "Iteration 53, loss = 0.22750570\n",
      "Iteration 54, loss = 0.22162189\n",
      "Iteration 55, loss = 0.21576849\n",
      "Iteration 56, loss = 0.21000541\n",
      "Iteration 57, loss = 0.20439188\n",
      "Iteration 58, loss = 0.19889414\n",
      "Iteration 59, loss = 0.19326584\n",
      "Iteration 60, loss = 0.18768679\n",
      "Iteration 61, loss = 0.18228005\n",
      "Iteration 62, loss = 0.17705299\n",
      "Iteration 63, loss = 0.17198673\n",
      "Iteration 64, loss = 0.16707448\n",
      "Iteration 65, loss = 0.16229677\n",
      "Iteration 66, loss = 0.15764511\n",
      "Iteration 67, loss = 0.15313807\n",
      "Iteration 68, loss = 0.14876809\n",
      "Iteration 69, loss = 0.14459139\n",
      "Iteration 70, loss = 0.14058923\n",
      "Iteration 71, loss = 0.13681082\n",
      "Iteration 72, loss = 0.13322554\n",
      "Iteration 73, loss = 0.12982313\n",
      "Iteration 74, loss = 0.12663834\n",
      "Iteration 75, loss = 0.12362916\n",
      "Iteration 76, loss = 0.12080256\n",
      "Iteration 77, loss = 0.11814106\n",
      "Iteration 78, loss = 0.11564433\n",
      "Iteration 79, loss = 0.11331367\n",
      "Iteration 80, loss = 0.11112960\n",
      "Iteration 81, loss = 0.10908880\n",
      "Iteration 82, loss = 0.10718490\n",
      "Iteration 83, loss = 0.10540648\n",
      "Iteration 84, loss = 0.10374581\n",
      "Iteration 85, loss = 0.10219592\n",
      "Iteration 86, loss = 0.10075219\n",
      "Iteration 87, loss = 0.09940896\n",
      "Iteration 88, loss = 0.09815813\n",
      "Iteration 89, loss = 0.09699341\n",
      "Iteration 90, loss = 0.09590818\n",
      "Iteration 91, loss = 0.09489592\n",
      "Iteration 92, loss = 0.09395271\n",
      "Iteration 93, loss = 0.09307425\n",
      "Iteration 94, loss = 0.09225414\n",
      "Iteration 95, loss = 0.09148813\n",
      "Iteration 96, loss = 0.09077236\n",
      "Iteration 97, loss = 0.09010447\n",
      "Iteration 98, loss = 0.08948401\n",
      "Iteration 99, loss = 0.08891902\n",
      "Iteration 100, loss = 0.08842812\n",
      "Iteration 101, loss = 0.08803091\n",
      "Iteration 102, loss = 0.08764188\n",
      "Iteration 103, loss = 0.08705531\n",
      "Iteration 104, loss = 0.08652613\n",
      "Iteration 105, loss = 0.08625530\n",
      "Iteration 106, loss = 0.08592493\n",
      "Iteration 107, loss = 0.08545456\n",
      "Iteration 108, loss = 0.08514614\n",
      "Iteration 109, loss = 0.08491256\n",
      "Iteration 110, loss = 0.08454193\n",
      "Iteration 111, loss = 0.08423241\n",
      "Iteration 112, loss = 0.08403235\n",
      "Iteration 113, loss = 0.08373906\n",
      "Iteration 114, loss = 0.08344901\n",
      "Iteration 115, loss = 0.08326329\n",
      "Iteration 116, loss = 0.08302563\n",
      "Iteration 117, loss = 0.08276122\n",
      "Iteration 118, loss = 0.08258124\n",
      "Iteration 119, loss = 0.08238435\n",
      "Iteration 120, loss = 0.08214828\n",
      "Iteration 121, loss = 0.08197090\n",
      "Iteration 122, loss = 0.08180283\n",
      "Iteration 123, loss = 0.08159562\n",
      "Iteration 124, loss = 0.08141978\n",
      "Iteration 125, loss = 0.08126990\n",
      "Iteration 126, loss = 0.08109151\n",
      "Iteration 127, loss = 0.08091937\n",
      "Iteration 128, loss = 0.08077762\n",
      "Iteration 129, loss = 0.08062547\n",
      "Iteration 130, loss = 0.08046281\n",
      "Iteration 131, loss = 0.08032187\n",
      "Iteration 132, loss = 0.08018867\n",
      "Iteration 133, loss = 0.08004232\n",
      "Iteration 134, loss = 0.07990135\n",
      "Iteration 135, loss = 0.07977584\n",
      "Iteration 136, loss = 0.07964775\n",
      "Iteration 137, loss = 0.07951365\n",
      "Iteration 138, loss = 0.07938826\n",
      "Iteration 139, loss = 0.07927120\n",
      "Iteration 140, loss = 0.07915060\n",
      "Iteration 141, loss = 0.07902816\n",
      "Iteration 142, loss = 0.07891292\n",
      "Iteration 143, loss = 0.07880275\n",
      "Iteration 144, loss = 0.07869045\n",
      "Iteration 145, loss = 0.07857750\n",
      "Iteration 146, loss = 0.07846940\n",
      "Iteration 147, loss = 0.07836539\n",
      "Iteration 148, loss = 0.07826100\n",
      "Iteration 149, loss = 0.07815590\n",
      "Iteration 150, loss = 0.07805349\n",
      "Iteration 151, loss = 0.07795469\n",
      "Iteration 152, loss = 0.07785711\n",
      "Iteration 153, loss = 0.07775920\n",
      "Iteration 154, loss = 0.07766209\n",
      "Iteration 155, loss = 0.07756739\n",
      "Iteration 156, loss = 0.07747496\n",
      "Iteration 157, loss = 0.07738339\n",
      "Iteration 158, loss = 0.07729206\n",
      "Iteration 159, loss = 0.07720152\n",
      "Iteration 160, loss = 0.07711261\n",
      "Iteration 161, loss = 0.07702542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91716398\n",
      "Iteration 2, loss = 1.54449948\n",
      "Iteration 3, loss = 1.23237610\n",
      "Iteration 4, loss = 1.09098040\n",
      "Iteration 5, loss = 1.10732441\n",
      "Iteration 6, loss = 1.06279520\n",
      "Iteration 7, loss = 0.96961789\n",
      "Iteration 8, loss = 0.89028003\n",
      "Iteration 9, loss = 0.82194205\n",
      "Iteration 10, loss = 0.76330471\n",
      "Iteration 11, loss = 0.71742607\n",
      "Iteration 12, loss = 0.68683111\n",
      "Iteration 13, loss = 0.65886270\n",
      "Iteration 14, loss = 0.62981888\n",
      "Iteration 15, loss = 0.60038999\n",
      "Iteration 16, loss = 0.57265714\n",
      "Iteration 17, loss = 0.54972856\n",
      "Iteration 18, loss = 0.53014365\n",
      "Iteration 19, loss = 0.51241853\n",
      "Iteration 20, loss = 0.49599805\n",
      "Iteration 21, loss = 0.48065016\n",
      "Iteration 22, loss = 0.46647363\n",
      "Iteration 23, loss = 0.45346021\n",
      "Iteration 24, loss = 0.44160491\n",
      "Iteration 25, loss = 0.43082153\n",
      "Iteration 26, loss = 0.42097086\n",
      "Iteration 27, loss = 0.41175679\n",
      "Iteration 28, loss = 0.40296185\n",
      "Iteration 29, loss = 0.39474035\n",
      "Iteration 30, loss = 0.38703280\n",
      "Iteration 31, loss = 0.37980684\n",
      "Iteration 32, loss = 0.37295980\n",
      "Iteration 33, loss = 0.36646925\n",
      "Iteration 34, loss = 0.36028634\n",
      "Iteration 35, loss = 0.35437524\n",
      "Iteration 36, loss = 0.34871121\n",
      "Iteration 37, loss = 0.34327186\n",
      "Iteration 38, loss = 0.33803266\n",
      "Iteration 39, loss = 0.33297781\n",
      "Iteration 40, loss = 0.32809311\n",
      "Iteration 41, loss = 0.32336723\n",
      "Iteration 42, loss = 0.31879115\n",
      "Iteration 43, loss = 0.31435465\n",
      "Iteration 44, loss = 0.31004765\n",
      "Iteration 45, loss = 0.30586394\n",
      "Iteration 46, loss = 0.30179779\n",
      "Iteration 47, loss = 0.29784104\n",
      "Iteration 48, loss = 0.29398849\n",
      "Iteration 49, loss = 0.29023575\n",
      "Iteration 50, loss = 0.28657935\n",
      "Iteration 51, loss = 0.28301721\n",
      "Iteration 52, loss = 0.27954134\n",
      "Iteration 53, loss = 0.27615186\n",
      "Iteration 54, loss = 0.27284501\n",
      "Iteration 55, loss = 0.26961871\n",
      "Iteration 56, loss = 0.26647185\n",
      "Iteration 57, loss = 0.26340080\n",
      "Iteration 58, loss = 0.26040357\n",
      "Iteration 59, loss = 0.25747761\n",
      "Iteration 60, loss = 0.25462197\n",
      "Iteration 61, loss = 0.25183332\n",
      "Iteration 62, loss = 0.24911128\n",
      "Iteration 63, loss = 0.24645367\n",
      "Iteration 64, loss = 0.24385781\n",
      "Iteration 65, loss = 0.24132242\n",
      "Iteration 66, loss = 0.23884603\n",
      "Iteration 67, loss = 0.23642736\n",
      "Iteration 68, loss = 0.23406619\n",
      "Iteration 69, loss = 0.23175858\n",
      "Iteration 70, loss = 0.22950524\n",
      "Iteration 71, loss = 0.22730318\n",
      "Iteration 72, loss = 0.22515150\n",
      "Iteration 73, loss = 0.22304951\n",
      "Iteration 74, loss = 0.22099521\n",
      "Iteration 75, loss = 0.21898762\n",
      "Iteration 76, loss = 0.21702630\n",
      "Iteration 77, loss = 0.21510946\n",
      "Iteration 78, loss = 0.21323600\n",
      "Iteration 79, loss = 0.21140389\n",
      "Iteration 80, loss = 0.20961349\n",
      "Iteration 81, loss = 0.20786204\n",
      "Iteration 82, loss = 0.20614933\n",
      "Iteration 83, loss = 0.20447491\n",
      "Iteration 84, loss = 0.20283704\n",
      "Iteration 85, loss = 0.20123518\n",
      "Iteration 86, loss = 0.19966780\n",
      "Iteration 87, loss = 0.19813529\n",
      "Iteration 88, loss = 0.19663544\n",
      "Iteration 89, loss = 0.19516857\n",
      "Iteration 90, loss = 0.19373312\n",
      "Iteration 91, loss = 0.19232770\n",
      "Iteration 92, loss = 0.19095167\n",
      "Iteration 93, loss = 0.18960495\n",
      "Iteration 94, loss = 0.18828652\n",
      "Iteration 95, loss = 0.18699575\n",
      "Iteration 96, loss = 0.18573212\n",
      "Iteration 97, loss = 0.18449458\n",
      "Iteration 98, loss = 0.18328229\n",
      "Iteration 99, loss = 0.18209490\n",
      "Iteration 100, loss = 0.18093182\n",
      "Iteration 101, loss = 0.17979322\n",
      "Iteration 102, loss = 0.17867760\n",
      "Iteration 103, loss = 0.17758470\n",
      "Iteration 104, loss = 0.17651347\n",
      "Iteration 105, loss = 0.17546325\n",
      "Iteration 106, loss = 0.17443351\n",
      "Iteration 107, loss = 0.17342389\n",
      "Iteration 108, loss = 0.17243396\n",
      "Iteration 109, loss = 0.17146315\n",
      "Iteration 110, loss = 0.17051131\n",
      "Iteration 111, loss = 0.16957751\n",
      "Iteration 112, loss = 0.16866130\n",
      "Iteration 113, loss = 0.16776233\n",
      "Iteration 114, loss = 0.16688036\n",
      "Iteration 115, loss = 0.16601493\n",
      "Iteration 116, loss = 0.16516545\n",
      "Iteration 117, loss = 0.16433152\n",
      "Iteration 118, loss = 0.16351296\n",
      "Iteration 119, loss = 0.16270896\n",
      "Iteration 120, loss = 0.16191950\n",
      "Iteration 121, loss = 0.16114418\n",
      "Iteration 122, loss = 0.16038256\n",
      "Iteration 123, loss = 0.15963429\n",
      "Iteration 124, loss = 0.15889903\n",
      "Iteration 125, loss = 0.15817651\n",
      "Iteration 126, loss = 0.15746639\n",
      "Iteration 127, loss = 0.15676842\n",
      "Iteration 128, loss = 0.15608230\n",
      "Iteration 129, loss = 0.15540774\n",
      "Iteration 130, loss = 0.15474463\n",
      "Iteration 131, loss = 0.15409263\n",
      "Iteration 132, loss = 0.15345142\n",
      "Iteration 133, loss = 0.15282080\n",
      "Iteration 134, loss = 0.15220054\n",
      "Iteration 135, loss = 0.15159036\n",
      "Iteration 136, loss = 0.15099006\n",
      "Iteration 137, loss = 0.15039941\n",
      "Iteration 138, loss = 0.14981818\n",
      "Iteration 139, loss = 0.14924615\n",
      "Iteration 140, loss = 0.14868312\n",
      "Iteration 141, loss = 0.14812888\n",
      "Iteration 142, loss = 0.14758326\n",
      "Iteration 143, loss = 0.14704606\n",
      "Iteration 144, loss = 0.14651713\n",
      "Iteration 145, loss = 0.14599627\n",
      "Iteration 146, loss = 0.14548330\n",
      "Iteration 147, loss = 0.14497806\n",
      "Iteration 148, loss = 0.14448043\n",
      "Iteration 149, loss = 0.14399022\n",
      "Iteration 150, loss = 0.14350726\n",
      "Iteration 151, loss = 0.14303140\n",
      "Iteration 152, loss = 0.14256249\n",
      "Iteration 153, loss = 0.14210043\n",
      "Iteration 154, loss = 0.14164506\n",
      "Iteration 155, loss = 0.14119622\n",
      "Iteration 156, loss = 0.14075379\n",
      "Iteration 157, loss = 0.14031770\n",
      "Iteration 158, loss = 0.13988778\n",
      "Iteration 159, loss = 0.13946390\n",
      "Iteration 160, loss = 0.13904598\n",
      "Iteration 161, loss = 0.13863382\n",
      "Iteration 162, loss = 0.13822738\n",
      "Iteration 163, loss = 0.13782652\n",
      "Iteration 164, loss = 0.13743115\n",
      "Iteration 165, loss = 0.13704113\n",
      "Iteration 166, loss = 0.13665637\n",
      "Iteration 167, loss = 0.13627680\n",
      "Iteration 168, loss = 0.13590226\n",
      "Iteration 169, loss = 0.13553271\n",
      "Iteration 170, loss = 0.13516802\n",
      "Iteration 171, loss = 0.13480812\n",
      "Iteration 172, loss = 0.13445290\n",
      "Iteration 173, loss = 0.13410231\n",
      "Iteration 174, loss = 0.13375622\n",
      "Iteration 175, loss = 0.13341461\n",
      "Iteration 176, loss = 0.13307738\n",
      "Iteration 177, loss = 0.13274442\n",
      "Iteration 178, loss = 0.13241566\n",
      "Iteration 179, loss = 0.13209111\n",
      "Iteration 180, loss = 0.13177050\n",
      "Iteration 181, loss = 0.13145392\n",
      "Iteration 182, loss = 0.13114124\n",
      "Iteration 183, loss = 0.13083240\n",
      "Iteration 184, loss = 0.13052738\n",
      "Iteration 185, loss = 0.13022610\n",
      "Iteration 186, loss = 0.12992848\n",
      "Iteration 187, loss = 0.12963444\n",
      "Iteration 188, loss = 0.12934392\n",
      "Iteration 189, loss = 0.12905685\n",
      "Iteration 190, loss = 0.12877317\n",
      "Iteration 191, loss = 0.12849284\n",
      "Iteration 192, loss = 0.12821581\n",
      "Iteration 193, loss = 0.12794200\n",
      "Iteration 194, loss = 0.12767137\n",
      "Iteration 195, loss = 0.12740386\n",
      "Iteration 196, loss = 0.12713941\n",
      "Iteration 197, loss = 0.12687799\n",
      "Iteration 198, loss = 0.12661952\n",
      "Iteration 199, loss = 0.12636399\n",
      "Iteration 200, loss = 0.12611132\n",
      "Iteration 201, loss = 0.12586147\n",
      "Iteration 202, loss = 0.12561441\n",
      "Iteration 203, loss = 0.12537008\n",
      "Iteration 204, loss = 0.12512844\n",
      "Iteration 205, loss = 0.12488945\n",
      "Iteration 206, loss = 0.12465306\n",
      "Iteration 207, loss = 0.12441924\n",
      "Iteration 208, loss = 0.12418795\n",
      "Iteration 209, loss = 0.12395913\n",
      "Iteration 210, loss = 0.12373278\n",
      "Iteration 211, loss = 0.12350885\n",
      "Iteration 212, loss = 0.12328730\n",
      "Iteration 213, loss = 0.12306808\n",
      "Iteration 214, loss = 0.12285118\n",
      "Iteration 215, loss = 0.12263653\n",
      "Iteration 216, loss = 0.12242411\n",
      "Iteration 217, loss = 0.12221392\n",
      "Iteration 218, loss = 0.12200585\n",
      "Iteration 219, loss = 0.12179993\n",
      "Iteration 220, loss = 0.12159610\n",
      "Iteration 221, loss = 0.12139434\n",
      "Iteration 222, loss = 0.12119464\n",
      "Iteration 223, loss = 0.12099693\n",
      "Iteration 224, loss = 0.12080119\n",
      "Iteration 225, loss = 0.12060740\n",
      "Iteration 226, loss = 0.12041553\n",
      "Iteration 227, loss = 0.12022555\n",
      "Iteration 228, loss = 0.12003743\n",
      "Iteration 229, loss = 0.11985109\n",
      "Iteration 230, loss = 0.11966658\n",
      "Iteration 231, loss = 0.11948386\n",
      "Iteration 232, loss = 0.11930292\n",
      "Iteration 233, loss = 0.11912369\n",
      "Iteration 234, loss = 0.11894616\n",
      "Iteration 235, loss = 0.11877031\n",
      "Iteration 236, loss = 0.11859612\n",
      "Iteration 237, loss = 0.11842357\n",
      "Iteration 238, loss = 0.11825263\n",
      "Iteration 239, loss = 0.11808327\n",
      "Iteration 240, loss = 0.11791547\n",
      "Iteration 241, loss = 0.11774921\n",
      "Iteration 242, loss = 0.11758449\n",
      "Iteration 243, loss = 0.11742130\n",
      "Iteration 244, loss = 0.11725960\n",
      "Iteration 245, loss = 0.11709938\n",
      "Iteration 246, loss = 0.11694060\n",
      "Iteration 247, loss = 0.11678326\n",
      "Iteration 248, loss = 0.11662733\n",
      "Iteration 249, loss = 0.11647280\n",
      "Iteration 250, loss = 0.11631964\n",
      "Iteration 251, loss = 0.11616785\n",
      "Iteration 252, loss = 0.11601740\n",
      "Iteration 253, loss = 0.11586827\n",
      "Iteration 254, loss = 0.11572046\n",
      "Iteration 255, loss = 0.11557390\n",
      "Iteration 256, loss = 0.11542866\n",
      "Iteration 257, loss = 0.11528467\n",
      "Iteration 258, loss = 0.11514191\n",
      "Iteration 259, loss = 0.11500040\n",
      "Iteration 260, loss = 0.11486010\n",
      "Iteration 261, loss = 0.11472094\n",
      "Iteration 262, loss = 0.11458293\n",
      "Iteration 263, loss = 0.11444605\n",
      "Iteration 264, loss = 0.11431029\n",
      "Iteration 265, loss = 0.11417566\n",
      "Iteration 266, loss = 0.11404218\n",
      "Iteration 267, loss = 0.11390979\n",
      "Iteration 268, loss = 0.11377846\n",
      "Iteration 269, loss = 0.11364821\n",
      "Iteration 270, loss = 0.11351902\n",
      "Iteration 271, loss = 0.11339087\n",
      "Iteration 272, loss = 0.11326375\n",
      "Iteration 273, loss = 0.11313766\n",
      "Iteration 274, loss = 0.11301257\n",
      "Iteration 275, loss = 0.11288849\n",
      "Iteration 276, loss = 0.11276540\n",
      "Iteration 277, loss = 0.11264327\n",
      "Iteration 278, loss = 0.11252210\n",
      "Iteration 279, loss = 0.11240188\n",
      "Iteration 280, loss = 0.11228257\n",
      "Iteration 281, loss = 0.11216420\n",
      "Iteration 282, loss = 0.11204675\n",
      "Iteration 283, loss = 0.11193022\n",
      "Iteration 284, loss = 0.11181457\n",
      "Iteration 285, loss = 0.11169982\n",
      "Iteration 286, loss = 0.11158593\n",
      "Iteration 287, loss = 0.11147292\n",
      "Iteration 288, loss = 0.11136077\n",
      "Iteration 289, loss = 0.11124946\n",
      "Iteration 290, loss = 0.11113893\n",
      "Iteration 291, loss = 0.11102918\n",
      "Iteration 292, loss = 0.11092025\n",
      "Iteration 293, loss = 0.11081213\n",
      "Iteration 294, loss = 0.11070480\n",
      "Iteration 295, loss = 0.11059826\n",
      "Iteration 296, loss = 0.11049249\n",
      "Iteration 297, loss = 0.11038750\n",
      "Iteration 298, loss = 0.11028327\n",
      "Iteration 299, loss = 0.11017981\n",
      "Iteration 300, loss = 0.11007709\n",
      "Iteration 301, loss = 0.10997512\n",
      "Iteration 302, loss = 0.10987389\n",
      "Iteration 303, loss = 0.10977339\n",
      "Iteration 304, loss = 0.10967361\n",
      "Iteration 305, loss = 0.10957455\n",
      "Iteration 306, loss = 0.10947620\n",
      "Iteration 307, loss = 0.10937863\n",
      "Iteration 308, loss = 0.10928174\n",
      "Iteration 309, loss = 0.10918549\n",
      "Iteration 310, loss = 0.10908971\n",
      "Iteration 311, loss = 0.10899503\n",
      "Iteration 312, loss = 0.10890102\n",
      "Iteration 313, loss = 0.10880766\n",
      "Iteration 314, loss = 0.10871497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92564666\n",
      "Iteration 2, loss = 1.54689016\n",
      "Iteration 3, loss = 1.22799555\n",
      "Iteration 4, loss = 1.08946023\n",
      "Iteration 5, loss = 1.10903032\n",
      "Iteration 6, loss = 1.06231227\n",
      "Iteration 7, loss = 0.96958677\n",
      "Iteration 8, loss = 0.89313197\n",
      "Iteration 9, loss = 0.82435853\n",
      "Iteration 10, loss = 0.76481728\n",
      "Iteration 11, loss = 0.71836439\n",
      "Iteration 12, loss = 0.68831001\n",
      "Iteration 13, loss = 0.66109213\n",
      "Iteration 14, loss = 0.63250285\n",
      "Iteration 15, loss = 0.60320478\n",
      "Iteration 16, loss = 0.57525406\n",
      "Iteration 17, loss = 0.55189417\n",
      "Iteration 18, loss = 0.53170241\n",
      "Iteration 19, loss = 0.51349891\n",
      "Iteration 20, loss = 0.49682465\n",
      "Iteration 21, loss = 0.48139152\n",
      "Iteration 22, loss = 0.46711996\n",
      "Iteration 23, loss = 0.45384379\n",
      "Iteration 24, loss = 0.44150929\n",
      "Iteration 25, loss = 0.43012629\n",
      "Iteration 26, loss = 0.41970458\n",
      "Iteration 27, loss = 0.41003811\n",
      "Iteration 28, loss = 0.40097262\n",
      "Iteration 29, loss = 0.39248954\n",
      "Iteration 30, loss = 0.38452532\n",
      "Iteration 31, loss = 0.37700428\n",
      "Iteration 32, loss = 0.36989180\n",
      "Iteration 33, loss = 0.36313268\n",
      "Iteration 34, loss = 0.35667289\n",
      "Iteration 35, loss = 0.35049627\n",
      "Iteration 36, loss = 0.34457387\n",
      "Iteration 37, loss = 0.33888525\n",
      "Iteration 38, loss = 0.33341324\n",
      "Iteration 39, loss = 0.32813641\n",
      "Iteration 40, loss = 0.32304657\n",
      "Iteration 41, loss = 0.31812376\n",
      "Iteration 42, loss = 0.31335527\n",
      "Iteration 43, loss = 0.30872996\n",
      "Iteration 44, loss = 0.30423888\n",
      "Iteration 45, loss = 0.29987391\n",
      "Iteration 46, loss = 0.29563021\n",
      "Iteration 47, loss = 0.29150329\n",
      "Iteration 48, loss = 0.28748656\n",
      "Iteration 49, loss = 0.28357318\n",
      "Iteration 50, loss = 0.27976075\n",
      "Iteration 51, loss = 0.27604526\n",
      "Iteration 52, loss = 0.27242499\n",
      "Iteration 53, loss = 0.26889694\n",
      "Iteration 54, loss = 0.26545612\n",
      "Iteration 55, loss = 0.26210054\n",
      "Iteration 56, loss = 0.25882799\n",
      "Iteration 57, loss = 0.25563630\n",
      "Iteration 58, loss = 0.25252219\n",
      "Iteration 59, loss = 0.24948381\n",
      "Iteration 60, loss = 0.24651910\n",
      "Iteration 61, loss = 0.24362572\n",
      "Iteration 62, loss = 0.24080366\n",
      "Iteration 63, loss = 0.23804836\n",
      "Iteration 64, loss = 0.23536039\n",
      "Iteration 65, loss = 0.23273619\n",
      "Iteration 66, loss = 0.23017513\n",
      "Iteration 67, loss = 0.22767609\n",
      "Iteration 68, loss = 0.22523627\n",
      "Iteration 69, loss = 0.22285616\n",
      "Iteration 70, loss = 0.22053249\n",
      "Iteration 71, loss = 0.21826429\n",
      "Iteration 72, loss = 0.21604946\n",
      "Iteration 73, loss = 0.21388779\n",
      "Iteration 74, loss = 0.21177626\n",
      "Iteration 75, loss = 0.20971506\n",
      "Iteration 76, loss = 0.20770142\n",
      "Iteration 77, loss = 0.20573510\n",
      "Iteration 78, loss = 0.20381464\n",
      "Iteration 79, loss = 0.20193871\n",
      "Iteration 80, loss = 0.20010609\n",
      "Iteration 81, loss = 0.19831639\n",
      "Iteration 82, loss = 0.19656740\n",
      "Iteration 83, loss = 0.19485888\n",
      "Iteration 84, loss = 0.19318953\n",
      "Iteration 85, loss = 0.19155781\n",
      "Iteration 86, loss = 0.18996296\n",
      "Iteration 87, loss = 0.18840409\n",
      "Iteration 88, loss = 0.18688018\n",
      "Iteration 89, loss = 0.18539023\n",
      "Iteration 90, loss = 0.18393347\n",
      "Iteration 91, loss = 0.18250913\n",
      "Iteration 92, loss = 0.18111624\n",
      "Iteration 93, loss = 0.17975401\n",
      "Iteration 94, loss = 0.17842129\n",
      "Iteration 95, loss = 0.17711795\n",
      "Iteration 96, loss = 0.17584487\n",
      "Iteration 97, loss = 0.17459979\n",
      "Iteration 98, loss = 0.17338143\n",
      "Iteration 99, loss = 0.17218902\n",
      "Iteration 100, loss = 0.17102237\n",
      "Iteration 101, loss = 0.16988012\n",
      "Iteration 102, loss = 0.16876201\n",
      "Iteration 103, loss = 0.16766724\n",
      "Iteration 104, loss = 0.16659518\n",
      "Iteration 105, loss = 0.16554507\n",
      "Iteration 106, loss = 0.16451639\n",
      "Iteration 107, loss = 0.16350869\n",
      "Iteration 108, loss = 0.16252135\n",
      "Iteration 109, loss = 0.16155413\n",
      "Iteration 110, loss = 0.16060608\n",
      "Iteration 111, loss = 0.15967687\n",
      "Iteration 112, loss = 0.15876588\n",
      "Iteration 113, loss = 0.15787270\n",
      "Iteration 114, loss = 0.15699683\n",
      "Iteration 115, loss = 0.15613779\n",
      "Iteration 116, loss = 0.15529518\n",
      "Iteration 117, loss = 0.15446850\n",
      "Iteration 118, loss = 0.15365741\n",
      "Iteration 119, loss = 0.15286146\n",
      "Iteration 120, loss = 0.15208025\n",
      "Iteration 121, loss = 0.15131353\n",
      "Iteration 122, loss = 0.15056083\n",
      "Iteration 123, loss = 0.14982189\n",
      "Iteration 124, loss = 0.14909635\n",
      "Iteration 125, loss = 0.14838391\n",
      "Iteration 126, loss = 0.14768417\n",
      "Iteration 127, loss = 0.14699681\n",
      "Iteration 128, loss = 0.14632177\n",
      "Iteration 129, loss = 0.14565848\n",
      "Iteration 130, loss = 0.14500669\n",
      "Iteration 131, loss = 0.14436612\n",
      "Iteration 132, loss = 0.14373652\n",
      "Iteration 133, loss = 0.14311768\n",
      "Iteration 134, loss = 0.14250936\n",
      "Iteration 135, loss = 0.14191141\n",
      "Iteration 136, loss = 0.14132342\n",
      "Iteration 137, loss = 0.14074519\n",
      "Iteration 138, loss = 0.14017650\n",
      "Iteration 139, loss = 0.13961714\n",
      "Iteration 140, loss = 0.13906692\n",
      "Iteration 141, loss = 0.13852562\n",
      "Iteration 142, loss = 0.13799305\n",
      "Iteration 143, loss = 0.13746901\n",
      "Iteration 144, loss = 0.13695335\n",
      "Iteration 145, loss = 0.13644582\n",
      "Iteration 146, loss = 0.13594629\n",
      "Iteration 147, loss = 0.13545453\n",
      "Iteration 148, loss = 0.13497040\n",
      "Iteration 149, loss = 0.13449376\n",
      "Iteration 150, loss = 0.13402441\n",
      "Iteration 151, loss = 0.13356221\n",
      "Iteration 152, loss = 0.13310701\n",
      "Iteration 153, loss = 0.13265867\n",
      "Iteration 154, loss = 0.13221703\n",
      "Iteration 155, loss = 0.13178195\n",
      "Iteration 156, loss = 0.13135334\n",
      "Iteration 157, loss = 0.13093100\n",
      "Iteration 158, loss = 0.13051485\n",
      "Iteration 159, loss = 0.13010476\n",
      "Iteration 160, loss = 0.12970059\n",
      "Iteration 161, loss = 0.12930221\n",
      "Iteration 162, loss = 0.12890954\n",
      "Iteration 163, loss = 0.12852243\n",
      "Iteration 164, loss = 0.12814079\n",
      "Iteration 165, loss = 0.12776449\n",
      "Iteration 166, loss = 0.12739344\n",
      "Iteration 167, loss = 0.12702754\n",
      "Iteration 168, loss = 0.12666667\n",
      "Iteration 169, loss = 0.12631081\n",
      "Iteration 170, loss = 0.12595972\n",
      "Iteration 171, loss = 0.12561343\n",
      "Iteration 172, loss = 0.12527182\n",
      "Iteration 173, loss = 0.12493481\n",
      "Iteration 174, loss = 0.12460222\n",
      "Iteration 175, loss = 0.12427405\n",
      "Iteration 176, loss = 0.12395020\n",
      "Iteration 177, loss = 0.12363064\n",
      "Iteration 178, loss = 0.12331518\n",
      "Iteration 179, loss = 0.12300382\n",
      "Iteration 180, loss = 0.12269648\n",
      "Iteration 181, loss = 0.12239308\n",
      "Iteration 182, loss = 0.12209354\n",
      "Iteration 183, loss = 0.12179778\n",
      "Iteration 184, loss = 0.12150578\n",
      "Iteration 185, loss = 0.12121748\n",
      "Iteration 186, loss = 0.12093281\n",
      "Iteration 187, loss = 0.12065163\n",
      "Iteration 188, loss = 0.12037393\n",
      "Iteration 189, loss = 0.12009951\n",
      "Iteration 190, loss = 0.11982843\n",
      "Iteration 191, loss = 0.11956060\n",
      "Iteration 192, loss = 0.11929601\n",
      "Iteration 193, loss = 0.11903458\n",
      "Iteration 194, loss = 0.11877626\n",
      "Iteration 195, loss = 0.11852101\n",
      "Iteration 196, loss = 0.11826874\n",
      "Iteration 197, loss = 0.11801944\n",
      "Iteration 198, loss = 0.11777305\n",
      "Iteration 199, loss = 0.11752952\n",
      "Iteration 200, loss = 0.11728883\n",
      "Iteration 201, loss = 0.11705089\n",
      "Iteration 202, loss = 0.11681568\n",
      "Iteration 203, loss = 0.11658315\n",
      "Iteration 204, loss = 0.11635325\n",
      "Iteration 205, loss = 0.11612594\n",
      "Iteration 206, loss = 0.11590119\n",
      "Iteration 207, loss = 0.11567894\n",
      "Iteration 208, loss = 0.11545916\n",
      "Iteration 209, loss = 0.11524179\n",
      "Iteration 210, loss = 0.11502681\n",
      "Iteration 211, loss = 0.11481416\n",
      "Iteration 212, loss = 0.11460382\n",
      "Iteration 213, loss = 0.11439572\n",
      "Iteration 214, loss = 0.11418980\n",
      "Iteration 215, loss = 0.11398604\n",
      "Iteration 216, loss = 0.11378446\n",
      "Iteration 217, loss = 0.11358499\n",
      "Iteration 218, loss = 0.11338762\n",
      "Iteration 219, loss = 0.11319230\n",
      "Iteration 220, loss = 0.11299899\n",
      "Iteration 221, loss = 0.11280770\n",
      "Iteration 222, loss = 0.11261836\n",
      "Iteration 223, loss = 0.11243094\n",
      "Iteration 224, loss = 0.11224541\n",
      "Iteration 225, loss = 0.11206177\n",
      "Iteration 226, loss = 0.11187999\n",
      "Iteration 227, loss = 0.11170005\n",
      "Iteration 228, loss = 0.11152191\n",
      "Iteration 229, loss = 0.11134551\n",
      "Iteration 230, loss = 0.11117088\n",
      "Iteration 231, loss = 0.11099798\n",
      "Iteration 232, loss = 0.11082681\n",
      "Iteration 233, loss = 0.11065727\n",
      "Iteration 234, loss = 0.11048927\n",
      "Iteration 235, loss = 0.11032293\n",
      "Iteration 236, loss = 0.11015820\n",
      "Iteration 237, loss = 0.10999506\n",
      "Iteration 238, loss = 0.10983344\n",
      "Iteration 239, loss = 0.10967331\n",
      "Iteration 240, loss = 0.10951471\n",
      "Iteration 241, loss = 0.10935761\n",
      "Iteration 242, loss = 0.10920202\n",
      "Iteration 243, loss = 0.10904775\n",
      "Iteration 244, loss = 0.10889462\n",
      "Iteration 245, loss = 0.10874291\n",
      "Iteration 246, loss = 0.10859316\n",
      "Iteration 247, loss = 0.10844507\n",
      "Iteration 248, loss = 0.10829833\n",
      "Iteration 249, loss = 0.10815294\n",
      "Iteration 250, loss = 0.10800889\n",
      "Iteration 251, loss = 0.10786594\n",
      "Iteration 252, loss = 0.10771781\n",
      "Iteration 253, loss = 0.10756961\n",
      "Iteration 254, loss = 0.10742143\n",
      "Iteration 255, loss = 0.10727349\n",
      "Iteration 256, loss = 0.10712612\n",
      "Iteration 257, loss = 0.10697855\n",
      "Iteration 258, loss = 0.10683288\n",
      "Iteration 259, loss = 0.10669120\n",
      "Iteration 260, loss = 0.10654126\n",
      "Iteration 261, loss = 0.10637584\n",
      "Iteration 262, loss = 0.10620433\n",
      "Iteration 263, loss = 0.10603823\n",
      "Iteration 264, loss = 0.10589728\n",
      "Iteration 265, loss = 0.10576188\n",
      "Iteration 266, loss = 0.10565054\n",
      "Iteration 267, loss = 0.10554004\n",
      "Iteration 268, loss = 0.10542394\n",
      "Iteration 269, loss = 0.10530534\n",
      "Iteration 270, loss = 0.10518096\n",
      "Iteration 271, loss = 0.10505586\n",
      "Iteration 272, loss = 0.10492661\n",
      "Iteration 273, loss = 0.10479476\n",
      "Iteration 274, loss = 0.10467963\n",
      "Iteration 275, loss = 0.10456705\n",
      "Iteration 276, loss = 0.10445526\n",
      "Iteration 277, loss = 0.10434330\n",
      "Iteration 278, loss = 0.10423089\n",
      "Iteration 279, loss = 0.10411818\n",
      "Iteration 280, loss = 0.10400541\n",
      "Iteration 281, loss = 0.10389407\n",
      "Iteration 282, loss = 0.10378327\n",
      "Iteration 283, loss = 0.10367355\n",
      "Iteration 284, loss = 0.10356462\n",
      "Iteration 285, loss = 0.10345731\n",
      "Iteration 286, loss = 0.10335165\n",
      "Iteration 287, loss = 0.10324683\n",
      "Iteration 288, loss = 0.10314369\n",
      "Iteration 289, loss = 0.10304045\n",
      "Iteration 290, loss = 0.10293775\n",
      "Iteration 291, loss = 0.10283557\n",
      "Iteration 292, loss = 0.10273560\n",
      "Iteration 293, loss = 0.10263557\n",
      "Iteration 294, loss = 0.10253622\n",
      "Iteration 295, loss = 0.10243728\n",
      "Iteration 296, loss = 0.10233888\n",
      "Iteration 297, loss = 0.10224454\n",
      "Iteration 298, loss = 0.10214628\n",
      "Iteration 299, loss = 0.10205057\n",
      "Iteration 300, loss = 0.10195529\n",
      "Iteration 301, loss = 0.10186255\n",
      "Iteration 302, loss = 0.10176887\n",
      "Iteration 303, loss = 0.10167607\n",
      "Iteration 304, loss = 0.10158365\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90632821\n",
      "Iteration 2, loss = 1.53884643\n",
      "Iteration 3, loss = 1.22814779\n",
      "Iteration 4, loss = 1.08434876\n",
      "Iteration 5, loss = 1.09690740\n",
      "Iteration 6, loss = 1.05141210\n",
      "Iteration 7, loss = 0.95725883\n",
      "Iteration 8, loss = 0.87783884\n",
      "Iteration 9, loss = 0.80809506\n",
      "Iteration 10, loss = 0.74807259\n",
      "Iteration 11, loss = 0.70200312\n",
      "Iteration 12, loss = 0.67110800\n",
      "Iteration 13, loss = 0.64365590\n",
      "Iteration 14, loss = 0.61529623\n",
      "Iteration 15, loss = 0.58628224\n",
      "Iteration 16, loss = 0.55930867\n",
      "Iteration 17, loss = 0.53667188\n",
      "Iteration 18, loss = 0.51634736\n",
      "Iteration 19, loss = 0.49787482\n",
      "Iteration 20, loss = 0.48093673\n",
      "Iteration 21, loss = 0.46542150\n",
      "Iteration 22, loss = 0.45117980\n",
      "Iteration 23, loss = 0.43805681\n",
      "Iteration 24, loss = 0.42586504\n",
      "Iteration 25, loss = 0.41456030\n",
      "Iteration 26, loss = 0.40406880\n",
      "Iteration 27, loss = 0.39425079\n",
      "Iteration 28, loss = 0.38506528\n",
      "Iteration 29, loss = 0.37646919\n",
      "Iteration 30, loss = 0.36837077\n",
      "Iteration 31, loss = 0.36071878\n",
      "Iteration 32, loss = 0.35346836\n",
      "Iteration 33, loss = 0.34656253\n",
      "Iteration 34, loss = 0.33995385\n",
      "Iteration 35, loss = 0.33361217\n",
      "Iteration 36, loss = 0.32751644\n",
      "Iteration 37, loss = 0.32165206\n",
      "Iteration 38, loss = 0.31599548\n",
      "Iteration 39, loss = 0.31053433\n",
      "Iteration 40, loss = 0.30525109\n",
      "Iteration 41, loss = 0.30013975\n",
      "Iteration 42, loss = 0.29518355\n",
      "Iteration 43, loss = 0.29037045\n",
      "Iteration 44, loss = 0.28569566\n",
      "Iteration 45, loss = 0.28115149\n",
      "Iteration 46, loss = 0.27673553\n",
      "Iteration 47, loss = 0.27244075\n",
      "Iteration 48, loss = 0.26826210\n",
      "Iteration 49, loss = 0.26419255\n",
      "Iteration 50, loss = 0.26023023\n",
      "Iteration 51, loss = 0.25636910\n",
      "Iteration 52, loss = 0.25260781\n",
      "Iteration 53, loss = 0.24894333\n",
      "Iteration 54, loss = 0.24537175\n",
      "Iteration 55, loss = 0.24189180\n",
      "Iteration 56, loss = 0.23849855\n",
      "Iteration 57, loss = 0.23519245\n",
      "Iteration 58, loss = 0.23197003\n",
      "Iteration 59, loss = 0.22882864\n",
      "Iteration 60, loss = 0.22576710\n",
      "Iteration 61, loss = 0.22278149\n",
      "Iteration 62, loss = 0.21987147\n",
      "Iteration 63, loss = 0.21703384\n",
      "Iteration 64, loss = 0.21426843\n",
      "Iteration 65, loss = 0.21157138\n",
      "Iteration 66, loss = 0.20894239\n",
      "Iteration 67, loss = 0.20637836\n",
      "Iteration 68, loss = 0.20387877\n",
      "Iteration 69, loss = 0.20144091\n",
      "Iteration 70, loss = 0.19906394\n",
      "Iteration 71, loss = 0.19674532\n",
      "Iteration 72, loss = 0.19448484\n",
      "Iteration 73, loss = 0.19227997\n",
      "Iteration 74, loss = 0.19012924\n",
      "Iteration 75, loss = 0.18803107\n",
      "Iteration 76, loss = 0.18598455\n",
      "Iteration 77, loss = 0.18398763\n",
      "Iteration 78, loss = 0.18203906\n",
      "Iteration 79, loss = 0.18013801\n",
      "Iteration 80, loss = 0.17828271\n",
      "Iteration 81, loss = 0.17647189\n",
      "Iteration 82, loss = 0.17470427\n",
      "Iteration 83, loss = 0.17297906\n",
      "Iteration 84, loss = 0.17129481\n",
      "Iteration 85, loss = 0.16965021\n",
      "Iteration 86, loss = 0.16804403\n",
      "Iteration 87, loss = 0.16647518\n",
      "Iteration 88, loss = 0.16494261\n",
      "Iteration 89, loss = 0.16344576\n",
      "Iteration 90, loss = 0.16198316\n",
      "Iteration 91, loss = 0.16055413\n",
      "Iteration 92, loss = 0.15915784\n",
      "Iteration 93, loss = 0.15779273\n",
      "Iteration 94, loss = 0.15645860\n",
      "Iteration 95, loss = 0.15515482\n",
      "Iteration 96, loss = 0.15388026\n",
      "Iteration 97, loss = 0.15263543\n",
      "Iteration 98, loss = 0.15141899\n",
      "Iteration 99, loss = 0.15022936\n",
      "Iteration 100, loss = 0.14906551\n",
      "Iteration 101, loss = 0.14792692\n",
      "Iteration 102, loss = 0.14681280\n",
      "Iteration 103, loss = 0.14572252\n",
      "Iteration 104, loss = 0.14465570\n",
      "Iteration 105, loss = 0.14361148\n",
      "Iteration 106, loss = 0.14258918\n",
      "Iteration 107, loss = 0.14158807\n",
      "Iteration 108, loss = 0.14060759\n",
      "Iteration 109, loss = 0.13964723\n",
      "Iteration 110, loss = 0.13870654\n",
      "Iteration 111, loss = 0.13778475\n",
      "Iteration 112, loss = 0.13688151\n",
      "Iteration 113, loss = 0.13599633\n",
      "Iteration 114, loss = 0.13512861\n",
      "Iteration 115, loss = 0.13427811\n",
      "Iteration 116, loss = 0.13344430\n",
      "Iteration 117, loss = 0.13262667\n",
      "Iteration 118, loss = 0.13182480\n",
      "Iteration 119, loss = 0.13103825\n",
      "Iteration 120, loss = 0.13026667\n",
      "Iteration 121, loss = 0.12950964\n",
      "Iteration 122, loss = 0.12876678\n",
      "Iteration 123, loss = 0.12803775\n",
      "Iteration 124, loss = 0.12732216\n",
      "Iteration 125, loss = 0.12661973\n",
      "Iteration 126, loss = 0.12593012\n",
      "Iteration 127, loss = 0.12525296\n",
      "Iteration 128, loss = 0.12458798\n",
      "Iteration 129, loss = 0.12393492\n",
      "Iteration 130, loss = 0.12329343\n",
      "Iteration 131, loss = 0.12266328\n",
      "Iteration 132, loss = 0.12204423\n",
      "Iteration 133, loss = 0.12143593\n",
      "Iteration 134, loss = 0.12083812\n",
      "Iteration 135, loss = 0.12025057\n",
      "Iteration 136, loss = 0.11967303\n",
      "Iteration 137, loss = 0.11910526\n",
      "Iteration 138, loss = 0.11854705\n",
      "Iteration 139, loss = 0.11799817\n",
      "Iteration 140, loss = 0.11745844\n",
      "Iteration 141, loss = 0.11692763\n",
      "Iteration 142, loss = 0.11640554\n",
      "Iteration 143, loss = 0.11589195\n",
      "Iteration 144, loss = 0.11538670\n",
      "Iteration 145, loss = 0.11488957\n",
      "Iteration 146, loss = 0.11440039\n",
      "Iteration 147, loss = 0.11391899\n",
      "Iteration 148, loss = 0.11344520\n",
      "Iteration 149, loss = 0.11297885\n",
      "Iteration 150, loss = 0.11251977\n",
      "Iteration 151, loss = 0.11206782\n",
      "Iteration 152, loss = 0.11162284\n",
      "Iteration 153, loss = 0.11118468\n",
      "Iteration 154, loss = 0.11075319\n",
      "Iteration 155, loss = 0.11032823\n",
      "Iteration 156, loss = 0.10990967\n",
      "Iteration 157, loss = 0.10949737\n",
      "Iteration 158, loss = 0.10909122\n",
      "Iteration 159, loss = 0.10869107\n",
      "Iteration 160, loss = 0.10829680\n",
      "Iteration 161, loss = 0.10790829\n",
      "Iteration 162, loss = 0.10752542\n",
      "Iteration 163, loss = 0.10714807\n",
      "Iteration 164, loss = 0.10677613\n",
      "Iteration 165, loss = 0.10640950\n",
      "Iteration 166, loss = 0.10604807\n",
      "Iteration 167, loss = 0.10569173\n",
      "Iteration 168, loss = 0.10534038\n",
      "Iteration 169, loss = 0.10499394\n",
      "Iteration 170, loss = 0.10465229\n",
      "Iteration 171, loss = 0.10431534\n",
      "Iteration 172, loss = 0.10398301\n",
      "Iteration 173, loss = 0.10365524\n",
      "Iteration 174, loss = 0.10333194\n",
      "Iteration 175, loss = 0.10301304\n",
      "Iteration 176, loss = 0.10269840\n",
      "Iteration 177, loss = 0.10238795\n",
      "Iteration 178, loss = 0.10208161\n",
      "Iteration 179, loss = 0.10177930\n",
      "Iteration 180, loss = 0.10148095\n",
      "Iteration 181, loss = 0.10118648\n",
      "Iteration 182, loss = 0.10089583\n",
      "Iteration 183, loss = 0.10060896\n",
      "Iteration 184, loss = 0.10032574\n",
      "Iteration 185, loss = 0.10004614\n",
      "Iteration 186, loss = 0.09977007\n",
      "Iteration 187, loss = 0.09949748\n",
      "Iteration 188, loss = 0.09922831\n",
      "Iteration 189, loss = 0.09896250\n",
      "Iteration 190, loss = 0.09869999\n",
      "Iteration 191, loss = 0.09844073\n",
      "Iteration 192, loss = 0.09818461\n",
      "Iteration 193, loss = 0.09793157\n",
      "Iteration 194, loss = 0.09768163\n",
      "Iteration 195, loss = 0.09743469\n",
      "Iteration 196, loss = 0.09719073\n",
      "Iteration 197, loss = 0.09694968\n",
      "Iteration 198, loss = 0.09671149\n",
      "Iteration 199, loss = 0.09647612\n",
      "Iteration 200, loss = 0.09624352\n",
      "Iteration 201, loss = 0.09601365\n",
      "Iteration 202, loss = 0.09578645\n",
      "Iteration 203, loss = 0.09556189\n",
      "Iteration 204, loss = 0.09533989\n",
      "Iteration 205, loss = 0.09512045\n",
      "Iteration 206, loss = 0.09490350\n",
      "Iteration 207, loss = 0.09468902\n",
      "Iteration 208, loss = 0.09447697\n",
      "Iteration 209, loss = 0.09426730\n",
      "Iteration 210, loss = 0.09405998\n",
      "Iteration 211, loss = 0.09385499\n",
      "Iteration 212, loss = 0.09365228\n",
      "Iteration 213, loss = 0.09345182\n",
      "Iteration 214, loss = 0.09325355\n",
      "Iteration 215, loss = 0.09305744\n",
      "Iteration 216, loss = 0.09286346\n",
      "Iteration 217, loss = 0.09267158\n",
      "Iteration 218, loss = 0.09248176\n",
      "Iteration 219, loss = 0.09229396\n",
      "Iteration 220, loss = 0.09210818\n",
      "Iteration 221, loss = 0.09192439\n",
      "Iteration 222, loss = 0.09174253\n",
      "Iteration 223, loss = 0.09156254\n",
      "Iteration 224, loss = 0.09138440\n",
      "Iteration 225, loss = 0.09120810\n",
      "Iteration 226, loss = 0.09103361\n",
      "Iteration 227, loss = 0.09086090\n",
      "Iteration 228, loss = 0.09068996\n",
      "Iteration 229, loss = 0.09052076\n",
      "Iteration 230, loss = 0.09035327\n",
      "Iteration 231, loss = 0.09018745\n",
      "Iteration 232, loss = 0.09002331\n",
      "Iteration 233, loss = 0.08986079\n",
      "Iteration 234, loss = 0.08969988\n",
      "Iteration 235, loss = 0.08954056\n",
      "Iteration 236, loss = 0.08938280\n",
      "Iteration 237, loss = 0.08922656\n",
      "Iteration 238, loss = 0.08907185\n",
      "Iteration 239, loss = 0.08891865\n",
      "Iteration 240, loss = 0.08876693\n",
      "Iteration 241, loss = 0.08861667\n",
      "Iteration 242, loss = 0.08846784\n",
      "Iteration 243, loss = 0.08832041\n",
      "Iteration 244, loss = 0.08817442\n",
      "Iteration 245, loss = 0.08802982\n",
      "Iteration 246, loss = 0.08788659\n",
      "Iteration 247, loss = 0.08774468\n",
      "Iteration 248, loss = 0.08760406\n",
      "Iteration 249, loss = 0.08746472\n",
      "Iteration 250, loss = 0.08732668\n",
      "Iteration 251, loss = 0.08718989\n",
      "Iteration 252, loss = 0.08705434\n",
      "Iteration 253, loss = 0.08691999\n",
      "Iteration 254, loss = 0.08678682\n",
      "Iteration 255, loss = 0.08665484\n",
      "Iteration 256, loss = 0.08652404\n",
      "Iteration 257, loss = 0.08639438\n",
      "Iteration 258, loss = 0.08626587\n",
      "Iteration 259, loss = 0.08613852\n",
      "Iteration 260, loss = 0.08601234\n",
      "Iteration 261, loss = 0.08588729\n",
      "Iteration 262, loss = 0.08576334\n",
      "Iteration 263, loss = 0.08564055\n",
      "Iteration 264, loss = 0.08551886\n",
      "Iteration 265, loss = 0.08539822\n",
      "Iteration 266, loss = 0.08527869\n",
      "Iteration 267, loss = 0.08516022\n",
      "Iteration 268, loss = 0.08504274\n",
      "Iteration 269, loss = 0.08492586\n",
      "Iteration 270, loss = 0.08480998\n",
      "Iteration 271, loss = 0.08469505\n",
      "Iteration 272, loss = 0.08458107\n",
      "Iteration 273, loss = 0.08446804\n",
      "Iteration 274, loss = 0.08435595\n",
      "Iteration 275, loss = 0.08424477\n",
      "Iteration 276, loss = 0.08413451\n",
      "Iteration 277, loss = 0.08402516\n",
      "Iteration 278, loss = 0.08391670\n",
      "Iteration 279, loss = 0.08380903\n",
      "Iteration 280, loss = 0.08370215\n",
      "Iteration 281, loss = 0.08359619\n",
      "Iteration 282, loss = 0.08349117\n",
      "Iteration 283, loss = 0.08338700\n",
      "Iteration 284, loss = 0.08328367\n",
      "Iteration 285, loss = 0.08318120\n",
      "Iteration 286, loss = 0.08307935\n",
      "Iteration 287, loss = 0.08297830\n",
      "Iteration 288, loss = 0.08287804\n",
      "Iteration 289, loss = 0.08277857\n",
      "Iteration 290, loss = 0.08267987\n",
      "Iteration 291, loss = 0.08258305\n",
      "Iteration 292, loss = 0.08248764\n",
      "Iteration 293, loss = 0.08239305\n",
      "Iteration 294, loss = 0.08229926\n",
      "Iteration 295, loss = 0.08220629\n",
      "Iteration 296, loss = 0.08211396\n",
      "Iteration 297, loss = 0.08202237\n",
      "Iteration 298, loss = 0.08193133\n",
      "Iteration 299, loss = 0.08184085\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92980133\n",
      "Iteration 2, loss = 1.55133863\n",
      "Iteration 3, loss = 1.23387661\n",
      "Iteration 4, loss = 1.09300117\n",
      "Iteration 5, loss = 1.10904151\n",
      "Iteration 6, loss = 1.06091533\n",
      "Iteration 7, loss = 0.96497161\n",
      "Iteration 8, loss = 0.88549126\n",
      "Iteration 9, loss = 0.81567207\n",
      "Iteration 10, loss = 0.75577350\n",
      "Iteration 11, loss = 0.70934962\n",
      "Iteration 12, loss = 0.67892114\n",
      "Iteration 13, loss = 0.65144880\n",
      "Iteration 14, loss = 0.62301505\n",
      "Iteration 15, loss = 0.59368115\n",
      "Iteration 16, loss = 0.56660536\n",
      "Iteration 17, loss = 0.54386884\n",
      "Iteration 18, loss = 0.52345294\n",
      "Iteration 19, loss = 0.50481608\n",
      "Iteration 20, loss = 0.48763599\n",
      "Iteration 21, loss = 0.47166200\n",
      "Iteration 22, loss = 0.45677610\n",
      "Iteration 23, loss = 0.44294881\n",
      "Iteration 24, loss = 0.43020078\n",
      "Iteration 25, loss = 0.41847086\n",
      "Iteration 26, loss = 0.40749865\n",
      "Iteration 27, loss = 0.39721639\n",
      "Iteration 28, loss = 0.38759025\n",
      "Iteration 29, loss = 0.37862091\n",
      "Iteration 30, loss = 0.37030501\n",
      "Iteration 31, loss = 0.36248945\n",
      "Iteration 32, loss = 0.35508752\n",
      "Iteration 33, loss = 0.34797543\n",
      "Iteration 34, loss = 0.34112594\n",
      "Iteration 35, loss = 0.33456084\n",
      "Iteration 36, loss = 0.32827760\n",
      "Iteration 37, loss = 0.32224274\n",
      "Iteration 38, loss = 0.31644145\n",
      "Iteration 39, loss = 0.31085125\n",
      "Iteration 40, loss = 0.30546115\n",
      "Iteration 41, loss = 0.30024126\n",
      "Iteration 42, loss = 0.29517948\n",
      "Iteration 43, loss = 0.29026792\n",
      "Iteration 44, loss = 0.28549901\n",
      "Iteration 45, loss = 0.28086939\n",
      "Iteration 46, loss = 0.27636801\n",
      "Iteration 47, loss = 0.27199943\n",
      "Iteration 48, loss = 0.26774976\n",
      "Iteration 49, loss = 0.26361479\n",
      "Iteration 50, loss = 0.25959049\n",
      "Iteration 51, loss = 0.25567238\n",
      "Iteration 52, loss = 0.25185871\n",
      "Iteration 53, loss = 0.24814475\n",
      "Iteration 54, loss = 0.24452889\n",
      "Iteration 55, loss = 0.24100878\n",
      "Iteration 56, loss = 0.23757785\n",
      "Iteration 57, loss = 0.23423623\n",
      "Iteration 58, loss = 0.23098044\n",
      "Iteration 59, loss = 0.22780816\n",
      "Iteration 60, loss = 0.22471674\n",
      "Iteration 61, loss = 0.22170469\n",
      "Iteration 62, loss = 0.21876956\n",
      "Iteration 63, loss = 0.21590857\n",
      "Iteration 64, loss = 0.21312045\n",
      "Iteration 65, loss = 0.21040283\n",
      "Iteration 66, loss = 0.20775398\n",
      "Iteration 67, loss = 0.20517227\n",
      "Iteration 68, loss = 0.20265548\n",
      "Iteration 69, loss = 0.20020135\n",
      "Iteration 70, loss = 0.19780799\n",
      "Iteration 71, loss = 0.19547385\n",
      "Iteration 72, loss = 0.19319741\n",
      "Iteration 73, loss = 0.19097683\n",
      "Iteration 74, loss = 0.18881043\n",
      "Iteration 75, loss = 0.18669655\n",
      "Iteration 76, loss = 0.18463377\n",
      "Iteration 77, loss = 0.18262050\n",
      "Iteration 78, loss = 0.18065531\n",
      "Iteration 79, loss = 0.17873738\n",
      "Iteration 80, loss = 0.17686488\n",
      "Iteration 81, loss = 0.17503651\n",
      "Iteration 82, loss = 0.17325084\n",
      "Iteration 83, loss = 0.17150697\n",
      "Iteration 84, loss = 0.16980348\n",
      "Iteration 85, loss = 0.16813960\n",
      "Iteration 86, loss = 0.16651387\n",
      "Iteration 87, loss = 0.16492582\n",
      "Iteration 88, loss = 0.16337380\n",
      "Iteration 89, loss = 0.16185691\n",
      "Iteration 90, loss = 0.16037430\n",
      "Iteration 91, loss = 0.15892517\n",
      "Iteration 92, loss = 0.15750957\n",
      "Iteration 93, loss = 0.15612570\n",
      "Iteration 94, loss = 0.15477193\n",
      "Iteration 95, loss = 0.15344758\n",
      "Iteration 96, loss = 0.15215200\n",
      "Iteration 97, loss = 0.15088427\n",
      "Iteration 98, loss = 0.14964383\n",
      "Iteration 99, loss = 0.14842979\n",
      "Iteration 100, loss = 0.14724082\n",
      "Iteration 101, loss = 0.14607675\n",
      "Iteration 102, loss = 0.14493686\n",
      "Iteration 103, loss = 0.14382056\n",
      "Iteration 104, loss = 0.14272717\n",
      "Iteration 105, loss = 0.14165598\n",
      "Iteration 106, loss = 0.14060617\n",
      "Iteration 107, loss = 0.13957725\n",
      "Iteration 108, loss = 0.13856865\n",
      "Iteration 109, loss = 0.13757978\n",
      "Iteration 110, loss = 0.13661019\n",
      "Iteration 111, loss = 0.13565921\n",
      "Iteration 112, loss = 0.13472660\n",
      "Iteration 113, loss = 0.13381156\n",
      "Iteration 114, loss = 0.13291398\n",
      "Iteration 115, loss = 0.13203306\n",
      "Iteration 116, loss = 0.13116860\n",
      "Iteration 117, loss = 0.13031992\n",
      "Iteration 118, loss = 0.12948669\n",
      "Iteration 119, loss = 0.12866861\n",
      "Iteration 120, loss = 0.12786514\n",
      "Iteration 121, loss = 0.12707601\n",
      "Iteration 122, loss = 0.12630102\n",
      "Iteration 123, loss = 0.12553966\n",
      "Iteration 124, loss = 0.12479157\n",
      "Iteration 125, loss = 0.12405657\n",
      "Iteration 126, loss = 0.12333402\n",
      "Iteration 127, loss = 0.12262391\n",
      "Iteration 128, loss = 0.12192590\n",
      "Iteration 129, loss = 0.12123961\n",
      "Iteration 130, loss = 0.12056483\n",
      "Iteration 131, loss = 0.11990127\n",
      "Iteration 132, loss = 0.11924862\n",
      "Iteration 133, loss = 0.11860665\n",
      "Iteration 134, loss = 0.11797515\n",
      "Iteration 135, loss = 0.11735392\n",
      "Iteration 136, loss = 0.11674263\n",
      "Iteration 137, loss = 0.11614107\n",
      "Iteration 138, loss = 0.11554902\n",
      "Iteration 139, loss = 0.11496629\n",
      "Iteration 140, loss = 0.11439265\n",
      "Iteration 141, loss = 0.11382793\n",
      "Iteration 142, loss = 0.11327189\n",
      "Iteration 143, loss = 0.11272436\n",
      "Iteration 144, loss = 0.11218514\n",
      "Iteration 145, loss = 0.11165406\n",
      "Iteration 146, loss = 0.11113094\n",
      "Iteration 147, loss = 0.11061561\n",
      "Iteration 148, loss = 0.11010790\n",
      "Iteration 149, loss = 0.10960766\n",
      "Iteration 150, loss = 0.10911471\n",
      "Iteration 151, loss = 0.10862893\n",
      "Iteration 152, loss = 0.10815013\n",
      "Iteration 153, loss = 0.10767825\n",
      "Iteration 154, loss = 0.10721301\n",
      "Iteration 155, loss = 0.10675439\n",
      "Iteration 156, loss = 0.10630222\n",
      "Iteration 157, loss = 0.10585635\n",
      "Iteration 158, loss = 0.10541666\n",
      "Iteration 159, loss = 0.10498305\n",
      "Iteration 160, loss = 0.10455536\n",
      "Iteration 161, loss = 0.10413349\n",
      "Iteration 162, loss = 0.10371737\n",
      "Iteration 163, loss = 0.10330686\n",
      "Iteration 164, loss = 0.10290186\n",
      "Iteration 165, loss = 0.10250227\n",
      "Iteration 166, loss = 0.10210797\n",
      "Iteration 167, loss = 0.10171877\n",
      "Iteration 168, loss = 0.10133468\n",
      "Iteration 169, loss = 0.10095555\n",
      "Iteration 170, loss = 0.10058129\n",
      "Iteration 171, loss = 0.10021180\n",
      "Iteration 172, loss = 0.09984702\n",
      "Iteration 173, loss = 0.09948689\n",
      "Iteration 174, loss = 0.09913131\n",
      "Iteration 175, loss = 0.09878013\n",
      "Iteration 176, loss = 0.09843329\n",
      "Iteration 177, loss = 0.09809073\n",
      "Iteration 178, loss = 0.09775239\n",
      "Iteration 179, loss = 0.09741818\n",
      "Iteration 180, loss = 0.09708805\n",
      "Iteration 181, loss = 0.09676191\n",
      "Iteration 182, loss = 0.09643969\n",
      "Iteration 183, loss = 0.09612127\n",
      "Iteration 184, loss = 0.09580657\n",
      "Iteration 185, loss = 0.09549553\n",
      "Iteration 186, loss = 0.09518811\n",
      "Iteration 187, loss = 0.09488425\n",
      "Iteration 188, loss = 0.09458389\n",
      "Iteration 189, loss = 0.09428695\n",
      "Iteration 190, loss = 0.09399328\n",
      "Iteration 191, loss = 0.09370292\n",
      "Iteration 192, loss = 0.09341581\n",
      "Iteration 193, loss = 0.09313192\n",
      "Iteration 194, loss = 0.09285117\n",
      "Iteration 195, loss = 0.09257348\n",
      "Iteration 196, loss = 0.09229886\n",
      "Iteration 197, loss = 0.09202715\n",
      "Iteration 198, loss = 0.09175839\n",
      "Iteration 199, loss = 0.09149255\n",
      "Iteration 200, loss = 0.09122959\n",
      "Iteration 201, loss = 0.09096941\n",
      "Iteration 202, loss = 0.09071197\n",
      "Iteration 203, loss = 0.09045727\n",
      "Iteration 204, loss = 0.09020524\n",
      "Iteration 205, loss = 0.08995580\n",
      "Iteration 206, loss = 0.08970891\n",
      "Iteration 207, loss = 0.08946454\n",
      "Iteration 208, loss = 0.08922266\n",
      "Iteration 209, loss = 0.08898324\n",
      "Iteration 210, loss = 0.08874619\n",
      "Iteration 211, loss = 0.08851156\n",
      "Iteration 212, loss = 0.08827947\n",
      "Iteration 213, loss = 0.08804988\n",
      "Iteration 214, loss = 0.08782266\n",
      "Iteration 215, loss = 0.08759770\n",
      "Iteration 216, loss = 0.08737547\n",
      "Iteration 217, loss = 0.08715579\n",
      "Iteration 218, loss = 0.08693831\n",
      "Iteration 219, loss = 0.08672298\n",
      "Iteration 220, loss = 0.08650977\n",
      "Iteration 221, loss = 0.08629660\n",
      "Iteration 222, loss = 0.08607916\n",
      "Iteration 223, loss = 0.08586180\n",
      "Iteration 224, loss = 0.08564471\n",
      "Iteration 225, loss = 0.08542822\n",
      "Iteration 226, loss = 0.08521173\n",
      "Iteration 227, loss = 0.08499624\n",
      "Iteration 228, loss = 0.08478157\n",
      "Iteration 229, loss = 0.08457121\n",
      "Iteration 230, loss = 0.08436120\n",
      "Iteration 231, loss = 0.08415521\n",
      "Iteration 232, loss = 0.08396138\n",
      "Iteration 233, loss = 0.08377373\n",
      "Iteration 234, loss = 0.08359498\n",
      "Iteration 235, loss = 0.08341537\n",
      "Iteration 236, loss = 0.08323488\n",
      "Iteration 237, loss = 0.08305371\n",
      "Iteration 238, loss = 0.08287213\n",
      "Iteration 239, loss = 0.08269043\n",
      "Iteration 240, loss = 0.08250921\n",
      "Iteration 241, loss = 0.08233109\n",
      "Iteration 242, loss = 0.08215884\n",
      "Iteration 243, loss = 0.08198805\n",
      "Iteration 244, loss = 0.08181869\n",
      "Iteration 245, loss = 0.08165073\n",
      "Iteration 246, loss = 0.08148416\n",
      "Iteration 247, loss = 0.08131894\n",
      "Iteration 248, loss = 0.08115506\n",
      "Iteration 249, loss = 0.08099251\n",
      "Iteration 250, loss = 0.08083127\n",
      "Iteration 251, loss = 0.08067131\n",
      "Iteration 252, loss = 0.08051262\n",
      "Iteration 253, loss = 0.08035520\n",
      "Iteration 254, loss = 0.08019901\n",
      "Iteration 255, loss = 0.08004402\n",
      "Iteration 256, loss = 0.07989023\n",
      "Iteration 257, loss = 0.07974043\n",
      "Iteration 258, loss = 0.07958903\n",
      "Iteration 259, loss = 0.07943881\n",
      "Iteration 260, loss = 0.07929065\n",
      "Iteration 261, loss = 0.07914361\n",
      "Iteration 262, loss = 0.07899882\n",
      "Iteration 263, loss = 0.07885440\n",
      "Iteration 264, loss = 0.07871101\n",
      "Iteration 265, loss = 0.07856860\n",
      "Iteration 266, loss = 0.07842767\n",
      "Iteration 267, loss = 0.07828813\n",
      "Iteration 268, loss = 0.07814928\n",
      "Iteration 269, loss = 0.07801156\n",
      "Iteration 270, loss = 0.07787580\n",
      "Iteration 271, loss = 0.07774037\n",
      "Iteration 272, loss = 0.07760581\n",
      "Iteration 273, loss = 0.07747318\n",
      "Iteration 274, loss = 0.07734082\n",
      "Iteration 275, loss = 0.07720952\n",
      "Iteration 276, loss = 0.07708022\n",
      "Iteration 277, loss = 0.07695087\n",
      "Iteration 278, loss = 0.07682263\n",
      "Iteration 279, loss = 0.07669609\n",
      "Iteration 280, loss = 0.07657002\n",
      "Iteration 281, loss = 0.07644475\n",
      "Iteration 282, loss = 0.07632075\n",
      "Iteration 283, loss = 0.07619794\n",
      "Iteration 284, loss = 0.07607547\n",
      "Iteration 285, loss = 0.07595402\n",
      "Iteration 286, loss = 0.07583427\n",
      "Iteration 287, loss = 0.07571451\n",
      "Iteration 288, loss = 0.07559541\n",
      "Iteration 289, loss = 0.07547867\n",
      "Iteration 290, loss = 0.07536153\n",
      "Iteration 291, loss = 0.07524500\n",
      "Iteration 292, loss = 0.07513136\n",
      "Iteration 293, loss = 0.07501562\n",
      "Iteration 294, loss = 0.07490346\n",
      "Iteration 295, loss = 0.07479076\n",
      "Iteration 296, loss = 0.07467853\n",
      "Iteration 297, loss = 0.07456695\n",
      "Iteration 298, loss = 0.07445795\n",
      "Iteration 299, loss = 0.07434723\n",
      "Iteration 300, loss = 0.07423973\n",
      "Iteration 301, loss = 0.07413181\n",
      "Iteration 302, loss = 0.07402431\n",
      "Iteration 303, loss = 0.07391729\n",
      "Iteration 304, loss = 0.07381336\n",
      "Iteration 305, loss = 0.07370736\n",
      "Iteration 306, loss = 0.07360362\n",
      "Iteration 307, loss = 0.07350007\n",
      "Iteration 308, loss = 0.07339689\n",
      "Iteration 309, loss = 0.07329583\n",
      "Iteration 310, loss = 0.07319366\n",
      "Iteration 311, loss = 0.07309365\n",
      "Iteration 312, loss = 0.07299370\n",
      "Iteration 313, loss = 0.07289412\n",
      "Iteration 314, loss = 0.07279622\n",
      "Iteration 315, loss = 0.07269793\n",
      "Iteration 316, loss = 0.07260056\n",
      "Iteration 317, loss = 0.07250483\n",
      "Iteration 318, loss = 0.07240860\n",
      "Iteration 319, loss = 0.07231301\n",
      "Iteration 320, loss = 0.07221906\n",
      "Iteration 321, loss = 0.07212458\n",
      "Iteration 322, loss = 0.07203217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.94163030\n",
      "Iteration 2, loss = 1.55656928\n",
      "Iteration 3, loss = 1.23077489\n",
      "Iteration 4, loss = 1.08707879\n",
      "Iteration 5, loss = 1.10665413\n",
      "Iteration 6, loss = 1.05743140\n",
      "Iteration 7, loss = 0.96236176\n",
      "Iteration 8, loss = 0.88383765\n",
      "Iteration 9, loss = 0.81304633\n",
      "Iteration 10, loss = 0.75228554\n",
      "Iteration 11, loss = 0.70640234\n",
      "Iteration 12, loss = 0.67603132\n",
      "Iteration 13, loss = 0.64802874\n",
      "Iteration 14, loss = 0.61885596\n",
      "Iteration 15, loss = 0.58917954\n",
      "Iteration 16, loss = 0.56180670\n",
      "Iteration 17, loss = 0.53892058\n",
      "Iteration 18, loss = 0.51929404\n",
      "Iteration 19, loss = 0.50150706\n",
      "Iteration 20, loss = 0.48527051\n",
      "Iteration 21, loss = 0.47039213\n",
      "Iteration 22, loss = 0.45669076\n",
      "Iteration 23, loss = 0.44406306\n",
      "Iteration 24, loss = 0.43237979\n",
      "Iteration 25, loss = 0.42166356\n",
      "Iteration 26, loss = 0.41177216\n",
      "Iteration 27, loss = 0.40251411\n",
      "Iteration 28, loss = 0.39386869\n",
      "Iteration 29, loss = 0.38579749\n",
      "Iteration 30, loss = 0.37825263\n",
      "Iteration 31, loss = 0.37112773\n",
      "Iteration 32, loss = 0.36437761\n",
      "Iteration 33, loss = 0.35795075\n",
      "Iteration 34, loss = 0.35181058\n",
      "Iteration 35, loss = 0.34592827\n",
      "Iteration 36, loss = 0.34028113\n",
      "Iteration 37, loss = 0.33485416\n",
      "Iteration 38, loss = 0.32962975\n",
      "Iteration 39, loss = 0.32459343\n",
      "Iteration 40, loss = 0.31972840\n",
      "Iteration 41, loss = 0.31501980\n",
      "Iteration 42, loss = 0.31045618\n",
      "Iteration 43, loss = 0.30602816\n",
      "Iteration 44, loss = 0.30172763\n",
      "Iteration 45, loss = 0.29754762\n",
      "Iteration 46, loss = 0.29348518\n",
      "Iteration 47, loss = 0.28953375\n",
      "Iteration 48, loss = 0.28568687\n",
      "Iteration 49, loss = 0.28194009\n",
      "Iteration 50, loss = 0.27828972\n",
      "Iteration 51, loss = 0.27473371\n",
      "Iteration 52, loss = 0.27126923\n",
      "Iteration 53, loss = 0.26789258\n",
      "Iteration 54, loss = 0.26460027\n",
      "Iteration 55, loss = 0.26138979\n",
      "Iteration 56, loss = 0.25825915\n",
      "Iteration 57, loss = 0.25520552\n",
      "Iteration 58, loss = 0.25222709\n",
      "Iteration 59, loss = 0.24932198\n",
      "Iteration 60, loss = 0.24648814\n",
      "Iteration 61, loss = 0.24372340\n",
      "Iteration 62, loss = 0.24102606\n",
      "Iteration 63, loss = 0.23839416\n",
      "Iteration 64, loss = 0.23582663\n",
      "Iteration 65, loss = 0.23332118\n",
      "Iteration 66, loss = 0.23087656\n",
      "Iteration 67, loss = 0.22849149\n",
      "Iteration 68, loss = 0.22616449\n",
      "Iteration 69, loss = 0.22389292\n",
      "Iteration 70, loss = 0.22167714\n",
      "Iteration 71, loss = 0.21951349\n",
      "Iteration 72, loss = 0.21740191\n",
      "Iteration 73, loss = 0.21534056\n",
      "Iteration 74, loss = 0.21332806\n",
      "Iteration 75, loss = 0.21136343\n",
      "Iteration 76, loss = 0.20944520\n",
      "Iteration 77, loss = 0.20757199\n",
      "Iteration 78, loss = 0.20574263\n",
      "Iteration 79, loss = 0.20395629\n",
      "Iteration 80, loss = 0.20221138\n",
      "Iteration 81, loss = 0.20050693\n",
      "Iteration 82, loss = 0.19884171\n",
      "Iteration 83, loss = 0.19721542\n",
      "Iteration 84, loss = 0.19562641\n",
      "Iteration 85, loss = 0.19407370\n",
      "Iteration 86, loss = 0.19255586\n",
      "Iteration 87, loss = 0.19107228\n",
      "Iteration 88, loss = 0.18962169\n",
      "Iteration 89, loss = 0.18820360\n",
      "Iteration 90, loss = 0.18681701\n",
      "Iteration 91, loss = 0.18546138\n",
      "Iteration 92, loss = 0.18413577\n",
      "Iteration 93, loss = 0.18283921\n",
      "Iteration 94, loss = 0.18157081\n",
      "Iteration 95, loss = 0.18033006\n",
      "Iteration 96, loss = 0.17911606\n",
      "Iteration 97, loss = 0.17792890\n",
      "Iteration 98, loss = 0.17676732\n",
      "Iteration 99, loss = 0.17563028\n",
      "Iteration 100, loss = 0.17451708\n",
      "Iteration 101, loss = 0.17342742\n",
      "Iteration 102, loss = 0.17236052\n",
      "Iteration 103, loss = 0.17131559\n",
      "Iteration 104, loss = 0.17029217\n",
      "Iteration 105, loss = 0.16928960\n",
      "Iteration 106, loss = 0.16830740\n",
      "Iteration 107, loss = 0.16734480\n",
      "Iteration 108, loss = 0.16640174\n",
      "Iteration 109, loss = 0.16547781\n",
      "Iteration 110, loss = 0.16457207\n",
      "Iteration 111, loss = 0.16368409\n",
      "Iteration 112, loss = 0.16281331\n",
      "Iteration 113, loss = 0.16195939\n",
      "Iteration 114, loss = 0.16112209\n",
      "Iteration 115, loss = 0.16030083\n",
      "Iteration 116, loss = 0.15949508\n",
      "Iteration 117, loss = 0.15870472\n",
      "Iteration 118, loss = 0.15792908\n",
      "Iteration 119, loss = 0.15716776\n",
      "Iteration 120, loss = 0.15642040\n",
      "Iteration 121, loss = 0.15568674\n",
      "Iteration 122, loss = 0.15496636\n",
      "Iteration 123, loss = 0.15425894\n",
      "Iteration 124, loss = 0.15356417\n",
      "Iteration 125, loss = 0.15288178\n",
      "Iteration 126, loss = 0.15221142\n",
      "Iteration 127, loss = 0.15155282\n",
      "Iteration 128, loss = 0.15090570\n",
      "Iteration 129, loss = 0.15026977\n",
      "Iteration 130, loss = 0.14964478\n",
      "Iteration 131, loss = 0.14903046\n",
      "Iteration 132, loss = 0.14842657\n",
      "Iteration 133, loss = 0.14783299\n",
      "Iteration 134, loss = 0.14724939\n",
      "Iteration 135, loss = 0.14667552\n",
      "Iteration 136, loss = 0.14611114\n",
      "Iteration 137, loss = 0.14555605\n",
      "Iteration 138, loss = 0.14501006\n",
      "Iteration 139, loss = 0.14447294\n",
      "Iteration 140, loss = 0.14394448\n",
      "Iteration 141, loss = 0.14342451\n",
      "Iteration 142, loss = 0.14291282\n",
      "Iteration 143, loss = 0.14240924\n",
      "Iteration 144, loss = 0.14191359\n",
      "Iteration 145, loss = 0.14142572\n",
      "Iteration 146, loss = 0.14094549\n",
      "Iteration 147, loss = 0.14047270\n",
      "Iteration 148, loss = 0.14000716\n",
      "Iteration 149, loss = 0.13954873\n",
      "Iteration 150, loss = 0.13909736\n",
      "Iteration 151, loss = 0.13865271\n",
      "Iteration 152, loss = 0.13821481\n",
      "Iteration 153, loss = 0.13778344\n",
      "Iteration 154, loss = 0.13735844\n",
      "Iteration 155, loss = 0.13693970\n",
      "Iteration 156, loss = 0.13652709\n",
      "Iteration 157, loss = 0.13612050\n",
      "Iteration 158, loss = 0.13571977\n",
      "Iteration 159, loss = 0.13532479\n",
      "Iteration 160, loss = 0.13493551\n",
      "Iteration 161, loss = 0.13455169\n",
      "Iteration 162, loss = 0.13417339\n",
      "Iteration 163, loss = 0.13380039\n",
      "Iteration 164, loss = 0.13343261\n",
      "Iteration 165, loss = 0.13306990\n",
      "Iteration 166, loss = 0.13271219\n",
      "Iteration 167, loss = 0.13235939\n",
      "Iteration 168, loss = 0.13201141\n",
      "Iteration 169, loss = 0.13166815\n",
      "Iteration 170, loss = 0.13132953\n",
      "Iteration 171, loss = 0.13099549\n",
      "Iteration 172, loss = 0.13066586\n",
      "Iteration 173, loss = 0.13034062\n",
      "Iteration 174, loss = 0.13001967\n",
      "Iteration 175, loss = 0.12970293\n",
      "Iteration 176, loss = 0.12939031\n",
      "Iteration 177, loss = 0.12908176\n",
      "Iteration 178, loss = 0.12877720\n",
      "Iteration 179, loss = 0.12847658\n",
      "Iteration 180, loss = 0.12817980\n",
      "Iteration 181, loss = 0.12788678\n",
      "Iteration 182, loss = 0.12759746\n",
      "Iteration 183, loss = 0.12731178\n",
      "Iteration 184, loss = 0.12702967\n",
      "Iteration 185, loss = 0.12675106\n",
      "Iteration 186, loss = 0.12647589\n",
      "Iteration 187, loss = 0.12620410\n",
      "Iteration 188, loss = 0.12593569\n",
      "Iteration 189, loss = 0.12567056\n",
      "Iteration 190, loss = 0.12540866\n",
      "Iteration 191, loss = 0.12514989\n",
      "Iteration 192, loss = 0.12489423\n",
      "Iteration 193, loss = 0.12464161\n",
      "Iteration 194, loss = 0.12439199\n",
      "Iteration 195, loss = 0.12414530\n",
      "Iteration 196, loss = 0.12390151\n",
      "Iteration 197, loss = 0.12366056\n",
      "Iteration 198, loss = 0.12342241\n",
      "Iteration 199, loss = 0.12318701\n",
      "Iteration 200, loss = 0.12295430\n",
      "Iteration 201, loss = 0.12272427\n",
      "Iteration 202, loss = 0.12249682\n",
      "Iteration 203, loss = 0.12227196\n",
      "Iteration 204, loss = 0.12204961\n",
      "Iteration 205, loss = 0.12182976\n",
      "Iteration 206, loss = 0.12161235\n",
      "Iteration 207, loss = 0.12139736\n",
      "Iteration 208, loss = 0.12118473\n",
      "Iteration 209, loss = 0.12097444\n",
      "Iteration 210, loss = 0.12076638\n",
      "Iteration 211, loss = 0.12056058\n",
      "Iteration 212, loss = 0.12035700\n",
      "Iteration 213, loss = 0.12015562\n",
      "Iteration 214, loss = 0.11995637\n",
      "Iteration 215, loss = 0.11975921\n",
      "Iteration 216, loss = 0.11956412\n",
      "Iteration 217, loss = 0.11937109\n",
      "Iteration 218, loss = 0.11918010\n",
      "Iteration 219, loss = 0.11899109\n",
      "Iteration 220, loss = 0.11880404\n",
      "Iteration 221, loss = 0.11861893\n",
      "Iteration 222, loss = 0.11843572\n",
      "Iteration 223, loss = 0.11825438\n",
      "Iteration 224, loss = 0.11807488\n",
      "Iteration 225, loss = 0.11789720\n",
      "Iteration 226, loss = 0.11772132\n",
      "Iteration 227, loss = 0.11754719\n",
      "Iteration 228, loss = 0.11737481\n",
      "Iteration 229, loss = 0.11720414\n",
      "Iteration 230, loss = 0.11703517\n",
      "Iteration 231, loss = 0.11686786\n",
      "Iteration 232, loss = 0.11670218\n",
      "Iteration 233, loss = 0.11653812\n",
      "Iteration 234, loss = 0.11637566\n",
      "Iteration 235, loss = 0.11621475\n",
      "Iteration 236, loss = 0.11605538\n",
      "Iteration 237, loss = 0.11589752\n",
      "Iteration 238, loss = 0.11574118\n",
      "Iteration 239, loss = 0.11558631\n",
      "Iteration 240, loss = 0.11543286\n",
      "Iteration 241, loss = 0.11528081\n",
      "Iteration 242, loss = 0.11513018\n",
      "Iteration 243, loss = 0.11498091\n",
      "Iteration 244, loss = 0.11483302\n",
      "Iteration 245, loss = 0.11468648\n",
      "Iteration 246, loss = 0.11454128\n",
      "Iteration 247, loss = 0.11439739\n",
      "Iteration 248, loss = 0.11425481\n",
      "Iteration 249, loss = 0.11411351\n",
      "Iteration 250, loss = 0.11397348\n",
      "Iteration 251, loss = 0.11383471\n",
      "Iteration 252, loss = 0.11369718\n",
      "Iteration 253, loss = 0.11356085\n",
      "Iteration 254, loss = 0.11342573\n",
      "Iteration 255, loss = 0.11329180\n",
      "Iteration 256, loss = 0.11315905\n",
      "Iteration 257, loss = 0.11302746\n",
      "Iteration 258, loss = 0.11289702\n",
      "Iteration 259, loss = 0.11276770\n",
      "Iteration 260, loss = 0.11263949\n",
      "Iteration 261, loss = 0.11251239\n",
      "Iteration 262, loss = 0.11238636\n",
      "Iteration 263, loss = 0.11226140\n",
      "Iteration 264, loss = 0.11213747\n",
      "Iteration 265, loss = 0.11201456\n",
      "Iteration 266, loss = 0.11189270\n",
      "Iteration 267, loss = 0.11177185\n",
      "Iteration 268, loss = 0.11165204\n",
      "Iteration 269, loss = 0.11153329\n",
      "Iteration 270, loss = 0.11141549\n",
      "Iteration 271, loss = 0.11129866\n",
      "Iteration 272, loss = 0.11118282\n",
      "Iteration 273, loss = 0.11106797\n",
      "Iteration 274, loss = 0.11095406\n",
      "Iteration 275, loss = 0.11084116\n",
      "Iteration 276, loss = 0.11072917\n",
      "Iteration 277, loss = 0.11061791\n",
      "Iteration 278, loss = 0.11050789\n",
      "Iteration 279, loss = 0.11039895\n",
      "Iteration 280, loss = 0.11029088\n",
      "Iteration 281, loss = 0.11018368\n",
      "Iteration 282, loss = 0.11007732\n",
      "Iteration 283, loss = 0.10997180\n",
      "Iteration 284, loss = 0.10986611\n",
      "Iteration 285, loss = 0.10975862\n",
      "Iteration 286, loss = 0.10965105\n",
      "Iteration 287, loss = 0.10954348\n",
      "Iteration 288, loss = 0.10943603\n",
      "Iteration 289, loss = 0.10932886\n",
      "Iteration 290, loss = 0.10922200\n",
      "Iteration 291, loss = 0.10911535\n",
      "Iteration 292, loss = 0.10900957\n",
      "Iteration 293, loss = 0.10890431\n",
      "Iteration 294, loss = 0.10879947\n",
      "Iteration 295, loss = 0.10868541\n",
      "Iteration 296, loss = 0.10856089\n",
      "Iteration 297, loss = 0.10842784\n",
      "Iteration 298, loss = 0.10830549\n",
      "Iteration 299, loss = 0.10819423\n",
      "Iteration 300, loss = 0.10808425\n",
      "Iteration 301, loss = 0.10799125\n",
      "Iteration 302, loss = 0.10791003\n",
      "Iteration 303, loss = 0.10782948\n",
      "Iteration 304, loss = 0.10774620\n",
      "Iteration 305, loss = 0.10765830\n",
      "Iteration 306, loss = 0.10756553\n",
      "Iteration 307, loss = 0.10746952\n",
      "Iteration 308, loss = 0.10737593\n",
      "Iteration 309, loss = 0.10728336\n",
      "Iteration 310, loss = 0.10719124\n",
      "Iteration 311, loss = 0.10710347\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91716398\n",
      "Iteration 2, loss = 1.87334245\n",
      "Iteration 3, loss = 1.83059730\n",
      "Iteration 4, loss = 1.78895149\n",
      "Iteration 5, loss = 1.74844199\n",
      "Iteration 6, loss = 1.70887170\n",
      "Iteration 7, loss = 1.67011197\n",
      "Iteration 8, loss = 1.63220233\n",
      "Iteration 9, loss = 1.59505713\n",
      "Iteration 10, loss = 1.55863695\n",
      "Iteration 11, loss = 1.52301999\n",
      "Iteration 12, loss = 1.48820494\n",
      "Iteration 13, loss = 1.45421145\n",
      "Iteration 14, loss = 1.42107357\n",
      "Iteration 15, loss = 1.38885147\n",
      "Iteration 16, loss = 1.35753189\n",
      "Iteration 17, loss = 1.32715513\n",
      "Iteration 18, loss = 1.29780456\n",
      "Iteration 19, loss = 1.26955973\n",
      "Iteration 20, loss = 1.24247067\n",
      "Iteration 21, loss = 1.21659262\n",
      "Iteration 22, loss = 1.19197727\n",
      "Iteration 23, loss = 1.16867053\n",
      "Iteration 24, loss = 1.14675410\n",
      "Iteration 25, loss = 1.12620913\n",
      "Iteration 26, loss = 1.10707051\n",
      "Iteration 27, loss = 1.08932924\n",
      "Iteration 28, loss = 1.07296118\n",
      "Iteration 29, loss = 1.05791723\n",
      "Iteration 30, loss = 1.04413942\n",
      "Iteration 31, loss = 1.03152723\n",
      "Iteration 32, loss = 1.01999108\n",
      "Iteration 33, loss = 1.00942610\n",
      "Iteration 34, loss = 0.99967829\n",
      "Iteration 35, loss = 0.99062044\n",
      "Iteration 36, loss = 0.98212755\n",
      "Iteration 37, loss = 0.97407238\n",
      "Iteration 38, loss = 0.96635081\n",
      "Iteration 39, loss = 0.95880866\n",
      "Iteration 40, loss = 0.95135886\n",
      "Iteration 41, loss = 0.94390437\n",
      "Iteration 42, loss = 0.93638573\n",
      "Iteration 43, loss = 0.92879247\n",
      "Iteration 44, loss = 0.92113263\n",
      "Iteration 45, loss = 0.91351996\n",
      "Iteration 46, loss = 0.90592757\n",
      "Iteration 47, loss = 0.89837393\n",
      "Iteration 48, loss = 0.89089124\n",
      "Iteration 49, loss = 0.88343903\n",
      "Iteration 50, loss = 0.87613975\n",
      "Iteration 51, loss = 0.86891115\n",
      "Iteration 52, loss = 0.86170857\n",
      "Iteration 53, loss = 0.85455230\n",
      "Iteration 54, loss = 0.84743492\n",
      "Iteration 55, loss = 0.84036780\n",
      "Iteration 56, loss = 0.83339222\n",
      "Iteration 57, loss = 0.82658845\n",
      "Iteration 58, loss = 0.81997477\n",
      "Iteration 59, loss = 0.81361831\n",
      "Iteration 60, loss = 0.80752907\n",
      "Iteration 61, loss = 0.80164713\n",
      "Iteration 62, loss = 0.79597935\n",
      "Iteration 63, loss = 0.79059641\n",
      "Iteration 64, loss = 0.78538992\n",
      "Iteration 65, loss = 0.78033241\n",
      "Iteration 66, loss = 0.77539218\n",
      "Iteration 67, loss = 0.77060406\n",
      "Iteration 68, loss = 0.76590987\n",
      "Iteration 69, loss = 0.76127647\n",
      "Iteration 70, loss = 0.75672134\n",
      "Iteration 71, loss = 0.75222895\n",
      "Iteration 72, loss = 0.74778206\n",
      "Iteration 73, loss = 0.74335415\n",
      "Iteration 74, loss = 0.73894833\n",
      "Iteration 75, loss = 0.73456061\n",
      "Iteration 76, loss = 0.73020053\n",
      "Iteration 77, loss = 0.72587129\n",
      "Iteration 78, loss = 0.72157773\n",
      "Iteration 79, loss = 0.71732352\n",
      "Iteration 80, loss = 0.71310656\n",
      "Iteration 81, loss = 0.70892657\n",
      "Iteration 82, loss = 0.70478273\n",
      "Iteration 83, loss = 0.70067705\n",
      "Iteration 84, loss = 0.69661586\n",
      "Iteration 85, loss = 0.69259175\n",
      "Iteration 86, loss = 0.68861284\n",
      "Iteration 87, loss = 0.68467598\n",
      "Iteration 88, loss = 0.68077386\n",
      "Iteration 89, loss = 0.67691175\n",
      "Iteration 90, loss = 0.67309302\n",
      "Iteration 91, loss = 0.66932770\n",
      "Iteration 92, loss = 0.66559388\n",
      "Iteration 93, loss = 0.66189158\n",
      "Iteration 94, loss = 0.65822565\n",
      "Iteration 95, loss = 0.65459593\n",
      "Iteration 96, loss = 0.65100427\n",
      "Iteration 97, loss = 0.64744595\n",
      "Iteration 98, loss = 0.64392319\n",
      "Iteration 99, loss = 0.64043430\n",
      "Iteration 100, loss = 0.63697424\n",
      "Iteration 101, loss = 0.63354768\n",
      "Iteration 102, loss = 0.63015503\n",
      "Iteration 103, loss = 0.62679706\n",
      "Iteration 104, loss = 0.62346683\n",
      "Iteration 105, loss = 0.62015244\n",
      "Iteration 106, loss = 0.61686336\n",
      "Iteration 107, loss = 0.61359929\n",
      "Iteration 108, loss = 0.61036564\n",
      "Iteration 109, loss = 0.60717014\n",
      "Iteration 110, loss = 0.60401279\n",
      "Iteration 111, loss = 0.60087755\n",
      "Iteration 112, loss = 0.59777103\n",
      "Iteration 113, loss = 0.59468789\n",
      "Iteration 114, loss = 0.59163945\n",
      "Iteration 115, loss = 0.58864700\n",
      "Iteration 116, loss = 0.58568883\n",
      "Iteration 117, loss = 0.58277211\n",
      "Iteration 118, loss = 0.57987733\n",
      "Iteration 119, loss = 0.57701081\n",
      "Iteration 120, loss = 0.57420560\n",
      "Iteration 121, loss = 0.57144271\n",
      "Iteration 122, loss = 0.56872770\n",
      "Iteration 123, loss = 0.56606409\n",
      "Iteration 124, loss = 0.56342948\n",
      "Iteration 125, loss = 0.56082662\n",
      "Iteration 126, loss = 0.55825474\n",
      "Iteration 127, loss = 0.55570244\n",
      "Iteration 128, loss = 0.55317569\n",
      "Iteration 129, loss = 0.55067489\n",
      "Iteration 130, loss = 0.54819988\n",
      "Iteration 131, loss = 0.54576076\n",
      "Iteration 132, loss = 0.54334362\n",
      "Iteration 133, loss = 0.54095492\n",
      "Iteration 134, loss = 0.53859166\n",
      "Iteration 135, loss = 0.53627186\n",
      "Iteration 136, loss = 0.53398109\n",
      "Iteration 137, loss = 0.53170900\n",
      "Iteration 138, loss = 0.52946657\n",
      "Iteration 139, loss = 0.52725158\n",
      "Iteration 140, loss = 0.52507096\n",
      "Iteration 141, loss = 0.52291270\n",
      "Iteration 142, loss = 0.52077563\n",
      "Iteration 143, loss = 0.51866607\n",
      "Iteration 144, loss = 0.51657778\n",
      "Iteration 145, loss = 0.51451110\n",
      "Iteration 146, loss = 0.51246629\n",
      "Iteration 147, loss = 0.51044650\n",
      "Iteration 148, loss = 0.50844622\n",
      "Iteration 149, loss = 0.50646447\n",
      "Iteration 150, loss = 0.50450166\n",
      "Iteration 151, loss = 0.50255722\n",
      "Iteration 152, loss = 0.50063118\n",
      "Iteration 153, loss = 0.49872605\n",
      "Iteration 154, loss = 0.49683773\n",
      "Iteration 155, loss = 0.49496593\n",
      "Iteration 156, loss = 0.49311062\n",
      "Iteration 157, loss = 0.49127141\n",
      "Iteration 158, loss = 0.48944810\n",
      "Iteration 159, loss = 0.48764048\n",
      "Iteration 160, loss = 0.48584913\n",
      "Iteration 161, loss = 0.48407350\n",
      "Iteration 162, loss = 0.48231272\n",
      "Iteration 163, loss = 0.48056646\n",
      "Iteration 164, loss = 0.47883451\n",
      "Iteration 165, loss = 0.47711652\n",
      "Iteration 166, loss = 0.47541216\n",
      "Iteration 167, loss = 0.47372115\n",
      "Iteration 168, loss = 0.47204339\n",
      "Iteration 169, loss = 0.47038040\n",
      "Iteration 170, loss = 0.46873125\n",
      "Iteration 171, loss = 0.46709524\n",
      "Iteration 172, loss = 0.46547166\n",
      "Iteration 173, loss = 0.46386032\n",
      "Iteration 174, loss = 0.46226100\n",
      "Iteration 175, loss = 0.46067351\n",
      "Iteration 176, loss = 0.45909765\n",
      "Iteration 177, loss = 0.45753310\n",
      "Iteration 178, loss = 0.45597965\n",
      "Iteration 179, loss = 0.45443751\n",
      "Iteration 180, loss = 0.45290668\n",
      "Iteration 181, loss = 0.45138694\n",
      "Iteration 182, loss = 0.44987807\n",
      "Iteration 183, loss = 0.44837950\n",
      "Iteration 184, loss = 0.44689145\n",
      "Iteration 185, loss = 0.44541361\n",
      "Iteration 186, loss = 0.44394546\n",
      "Iteration 187, loss = 0.44248692\n",
      "Iteration 188, loss = 0.44103789\n",
      "Iteration 189, loss = 0.43959834\n",
      "Iteration 190, loss = 0.43816788\n",
      "Iteration 191, loss = 0.43674665\n",
      "Iteration 192, loss = 0.43533445\n",
      "Iteration 193, loss = 0.43393113\n",
      "Iteration 194, loss = 0.43253657\n",
      "Iteration 195, loss = 0.43115098\n",
      "Iteration 196, loss = 0.42977375\n",
      "Iteration 197, loss = 0.42840457\n",
      "Iteration 198, loss = 0.42704344\n",
      "Iteration 199, loss = 0.42569068\n",
      "Iteration 200, loss = 0.42434575\n",
      "Iteration 201, loss = 0.42300848\n",
      "Iteration 202, loss = 0.42167900\n",
      "Iteration 203, loss = 0.42035706\n",
      "Iteration 204, loss = 0.41904252\n",
      "Iteration 205, loss = 0.41773527\n",
      "Iteration 206, loss = 0.41643531\n",
      "Iteration 207, loss = 0.41514288\n",
      "Iteration 208, loss = 0.41385740\n",
      "Iteration 209, loss = 0.41257875\n",
      "Iteration 210, loss = 0.41130684\n",
      "Iteration 211, loss = 0.41004156\n",
      "Iteration 212, loss = 0.40878281\n",
      "Iteration 213, loss = 0.40753047\n",
      "Iteration 214, loss = 0.40628446\n",
      "Iteration 215, loss = 0.40504468\n",
      "Iteration 216, loss = 0.40381101\n",
      "Iteration 217, loss = 0.40258336\n",
      "Iteration 218, loss = 0.40136144\n",
      "Iteration 219, loss = 0.40014530\n",
      "Iteration 220, loss = 0.39893489\n",
      "Iteration 221, loss = 0.39773109\n",
      "Iteration 222, loss = 0.39653301\n",
      "Iteration 223, loss = 0.39534065\n",
      "Iteration 224, loss = 0.39415383\n",
      "Iteration 225, loss = 0.39297248\n",
      "Iteration 226, loss = 0.39179652\n",
      "Iteration 227, loss = 0.39062588\n",
      "Iteration 228, loss = 0.38946050\n",
      "Iteration 229, loss = 0.38830030\n",
      "Iteration 230, loss = 0.38714539\n",
      "Iteration 231, loss = 0.38599615\n",
      "Iteration 232, loss = 0.38485179\n",
      "Iteration 233, loss = 0.38371246\n",
      "Iteration 234, loss = 0.38257811\n",
      "Iteration 235, loss = 0.38144877\n",
      "Iteration 236, loss = 0.38032480\n",
      "Iteration 237, loss = 0.37920592\n",
      "Iteration 238, loss = 0.37809338\n",
      "Iteration 239, loss = 0.37698559\n",
      "Iteration 240, loss = 0.37588240\n",
      "Iteration 241, loss = 0.37478372\n",
      "Iteration 242, loss = 0.37368952\n",
      "Iteration 243, loss = 0.37260001\n",
      "Iteration 244, loss = 0.37151508\n",
      "Iteration 245, loss = 0.37043467\n",
      "Iteration 246, loss = 0.36935873\n",
      "Iteration 247, loss = 0.36828724\n",
      "Iteration 248, loss = 0.36722016\n",
      "Iteration 249, loss = 0.36615746\n",
      "Iteration 250, loss = 0.36509911\n",
      "Iteration 251, loss = 0.36404507\n",
      "Iteration 252, loss = 0.36299533\n",
      "Iteration 253, loss = 0.36194984\n",
      "Iteration 254, loss = 0.36090856\n",
      "Iteration 255, loss = 0.35987147\n",
      "Iteration 256, loss = 0.35883853\n",
      "Iteration 257, loss = 0.35780969\n",
      "Iteration 258, loss = 0.35678493\n",
      "Iteration 259, loss = 0.35576420\n",
      "Iteration 260, loss = 0.35474750\n",
      "Iteration 261, loss = 0.35373500\n",
      "Iteration 262, loss = 0.35272639\n",
      "Iteration 263, loss = 0.35172165\n",
      "Iteration 264, loss = 0.35072086\n",
      "Iteration 265, loss = 0.34972367\n",
      "Iteration 266, loss = 0.34873024\n",
      "Iteration 267, loss = 0.34774056\n",
      "Iteration 268, loss = 0.34675462\n",
      "Iteration 269, loss = 0.34577236\n",
      "Iteration 270, loss = 0.34479394\n",
      "Iteration 271, loss = 0.34381919\n",
      "Iteration 272, loss = 0.34284803\n",
      "Iteration 273, loss = 0.34188045\n",
      "Iteration 274, loss = 0.34091644\n",
      "Iteration 275, loss = 0.33995616\n",
      "Iteration 276, loss = 0.33899950\n",
      "Iteration 277, loss = 0.33804634\n",
      "Iteration 278, loss = 0.33709676\n",
      "Iteration 279, loss = 0.33615068\n",
      "Iteration 280, loss = 0.33520803\n",
      "Iteration 281, loss = 0.33426880\n",
      "Iteration 282, loss = 0.33333305\n",
      "Iteration 283, loss = 0.33240075\n",
      "Iteration 284, loss = 0.33147186\n",
      "Iteration 285, loss = 0.33054645\n",
      "Iteration 286, loss = 0.32962442\n",
      "Iteration 287, loss = 0.32870579\n",
      "Iteration 288, loss = 0.32779051\n",
      "Iteration 289, loss = 0.32687860\n",
      "Iteration 290, loss = 0.32597007\n",
      "Iteration 291, loss = 0.32506490\n",
      "Iteration 292, loss = 0.32416292\n",
      "Iteration 293, loss = 0.32326425\n",
      "Iteration 294, loss = 0.32236881\n",
      "Iteration 295, loss = 0.32147658\n",
      "Iteration 296, loss = 0.32058760\n",
      "Iteration 297, loss = 0.31970183\n",
      "Iteration 298, loss = 0.31881921\n",
      "Iteration 299, loss = 0.31793973\n",
      "Iteration 300, loss = 0.31706354\n",
      "Iteration 301, loss = 0.31619048\n",
      "Iteration 302, loss = 0.31532052\n",
      "Iteration 303, loss = 0.31445367\n",
      "Iteration 304, loss = 0.31358991\n",
      "Iteration 305, loss = 0.31272923\n",
      "Iteration 306, loss = 0.31187172\n",
      "Iteration 307, loss = 0.31101727\n",
      "Iteration 308, loss = 0.31016681\n",
      "Iteration 309, loss = 0.30931966\n",
      "Iteration 310, loss = 0.30847531\n",
      "Iteration 311, loss = 0.30763359\n",
      "Iteration 312, loss = 0.30679476\n",
      "Iteration 313, loss = 0.30595879\n",
      "Iteration 314, loss = 0.30512572\n",
      "Iteration 315, loss = 0.30429554\n",
      "Iteration 316, loss = 0.30346906\n",
      "Iteration 317, loss = 0.30264571\n",
      "Iteration 318, loss = 0.30182532\n",
      "Iteration 319, loss = 0.30100775\n",
      "Iteration 320, loss = 0.30019299\n",
      "Iteration 321, loss = 0.29938104\n",
      "Iteration 322, loss = 0.29857190\n",
      "Iteration 323, loss = 0.29776558\n",
      "Iteration 324, loss = 0.29696208\n",
      "Iteration 325, loss = 0.29616225\n",
      "Iteration 326, loss = 0.29536526\n",
      "Iteration 327, loss = 0.29457098\n",
      "Iteration 328, loss = 0.29377944\n",
      "Iteration 329, loss = 0.29299068\n",
      "Iteration 330, loss = 0.29220444\n",
      "Iteration 331, loss = 0.29142086\n",
      "Iteration 332, loss = 0.29064010\n",
      "Iteration 333, loss = 0.28986261\n",
      "Iteration 334, loss = 0.28908777\n",
      "Iteration 335, loss = 0.28831561\n",
      "Iteration 336, loss = 0.28754610\n",
      "Iteration 337, loss = 0.28677979\n",
      "Iteration 338, loss = 0.28601616\n",
      "Iteration 339, loss = 0.28525519\n",
      "Iteration 340, loss = 0.28449693\n",
      "Iteration 341, loss = 0.28374132\n",
      "Iteration 342, loss = 0.28298837\n",
      "Iteration 343, loss = 0.28223871\n",
      "Iteration 344, loss = 0.28149167\n",
      "Iteration 345, loss = 0.28074751\n",
      "Iteration 346, loss = 0.28000605\n",
      "Iteration 347, loss = 0.27926710\n",
      "Iteration 348, loss = 0.27853115\n",
      "Iteration 349, loss = 0.27779797\n",
      "Iteration 350, loss = 0.27706737\n",
      "Iteration 351, loss = 0.27633940\n",
      "Iteration 352, loss = 0.27561408\n",
      "Iteration 353, loss = 0.27489133\n",
      "Iteration 354, loss = 0.27417117\n",
      "Iteration 355, loss = 0.27345432\n",
      "Iteration 356, loss = 0.27273998\n",
      "Iteration 357, loss = 0.27202813\n",
      "Iteration 358, loss = 0.27131984\n",
      "Iteration 359, loss = 0.27061288\n",
      "Iteration 360, loss = 0.26990883\n",
      "Iteration 361, loss = 0.26920798\n",
      "Iteration 362, loss = 0.26850961\n",
      "Iteration 363, loss = 0.26781374\n",
      "Iteration 364, loss = 0.26712039\n",
      "Iteration 365, loss = 0.26642963\n",
      "Iteration 366, loss = 0.26574145\n",
      "Iteration 367, loss = 0.26505621\n",
      "Iteration 368, loss = 0.26437368\n",
      "Iteration 369, loss = 0.26369364\n",
      "Iteration 370, loss = 0.26301599\n",
      "Iteration 371, loss = 0.26234108\n",
      "Iteration 372, loss = 0.26166884\n",
      "Iteration 373, loss = 0.26099907\n",
      "Iteration 374, loss = 0.26033181\n",
      "Iteration 375, loss = 0.25966703\n",
      "Iteration 376, loss = 0.25900474\n",
      "Iteration 377, loss = 0.25834534\n",
      "Iteration 378, loss = 0.25768836\n",
      "Iteration 379, loss = 0.25703370\n",
      "Iteration 380, loss = 0.25638142\n",
      "Iteration 381, loss = 0.25573193\n",
      "Iteration 382, loss = 0.25508468\n",
      "Iteration 383, loss = 0.25444035\n",
      "Iteration 384, loss = 0.25379875\n",
      "Iteration 385, loss = 0.25315905\n",
      "Iteration 386, loss = 0.25252136\n",
      "Iteration 387, loss = 0.25188667\n",
      "Iteration 388, loss = 0.25125496\n",
      "Iteration 389, loss = 0.25062537\n",
      "Iteration 390, loss = 0.24999798\n",
      "Iteration 391, loss = 0.24937260\n",
      "Iteration 392, loss = 0.24875002\n",
      "Iteration 393, loss = 0.24813015\n",
      "Iteration 394, loss = 0.24751189\n",
      "Iteration 395, loss = 0.24689455\n",
      "Iteration 396, loss = 0.24627881\n",
      "Iteration 397, loss = 0.24566363\n",
      "Iteration 398, loss = 0.24504410\n",
      "Iteration 399, loss = 0.24442715\n",
      "Iteration 400, loss = 0.24380938\n",
      "Iteration 401, loss = 0.24318885\n",
      "Iteration 402, loss = 0.24256694\n",
      "Iteration 403, loss = 0.24194934\n",
      "Iteration 404, loss = 0.24132846\n",
      "Iteration 405, loss = 0.24070718\n",
      "Iteration 406, loss = 0.24008685\n",
      "Iteration 407, loss = 0.23946485\n",
      "Iteration 408, loss = 0.23884124\n",
      "Iteration 409, loss = 0.23821639\n",
      "Iteration 410, loss = 0.23759036\n",
      "Iteration 411, loss = 0.23696421\n",
      "Iteration 412, loss = 0.23633655\n",
      "Iteration 413, loss = 0.23570833\n",
      "Iteration 414, loss = 0.23508040\n",
      "Iteration 415, loss = 0.23445077\n",
      "Iteration 416, loss = 0.23381958\n",
      "Iteration 417, loss = 0.23318977\n",
      "Iteration 418, loss = 0.23255874\n",
      "Iteration 419, loss = 0.23192741\n",
      "Iteration 420, loss = 0.23129722\n",
      "Iteration 421, loss = 0.23066581\n",
      "Iteration 422, loss = 0.23003418\n",
      "Iteration 423, loss = 0.22940306\n",
      "Iteration 424, loss = 0.22877228\n",
      "Iteration 425, loss = 0.22814133\n",
      "Iteration 426, loss = 0.22751116\n",
      "Iteration 427, loss = 0.22688144\n",
      "Iteration 428, loss = 0.22625211\n",
      "Iteration 429, loss = 0.22562340\n",
      "Iteration 430, loss = 0.22499562\n",
      "Iteration 431, loss = 0.22436868\n",
      "Iteration 432, loss = 0.22374251\n",
      "Iteration 433, loss = 0.22311757\n",
      "Iteration 434, loss = 0.22249372\n",
      "Iteration 435, loss = 0.22187092\n",
      "Iteration 436, loss = 0.22124946\n",
      "Iteration 437, loss = 0.22062934\n",
      "Iteration 438, loss = 0.22001074\n",
      "Iteration 439, loss = 0.21939365\n",
      "Iteration 440, loss = 0.21877786\n",
      "Iteration 441, loss = 0.21816401\n",
      "Iteration 442, loss = 0.21755196\n",
      "Iteration 443, loss = 0.21694173\n",
      "Iteration 444, loss = 0.21633338\n",
      "Iteration 445, loss = 0.21572698\n",
      "Iteration 446, loss = 0.21512307\n",
      "Iteration 447, loss = 0.21452091\n",
      "Iteration 448, loss = 0.21392073\n",
      "Iteration 449, loss = 0.21332312\n",
      "Iteration 450, loss = 0.21272774\n",
      "Iteration 451, loss = 0.21213460\n",
      "Iteration 452, loss = 0.21154384\n",
      "Iteration 453, loss = 0.21095559\n",
      "Iteration 454, loss = 0.21036975\n",
      "Iteration 455, loss = 0.20978641\n",
      "Iteration 456, loss = 0.20920573\n",
      "Iteration 457, loss = 0.20862749\n",
      "Iteration 458, loss = 0.20805179\n",
      "Iteration 459, loss = 0.20747866\n",
      "Iteration 460, loss = 0.20690817\n",
      "Iteration 461, loss = 0.20634038\n",
      "Iteration 462, loss = 0.20577520\n",
      "Iteration 463, loss = 0.20521260\n",
      "Iteration 464, loss = 0.20465286\n",
      "Iteration 465, loss = 0.20409580\n",
      "Iteration 466, loss = 0.20354151\n",
      "Iteration 467, loss = 0.20299030\n",
      "Iteration 468, loss = 0.20244242\n",
      "Iteration 469, loss = 0.20189726\n",
      "Iteration 470, loss = 0.20135496\n",
      "Iteration 471, loss = 0.20081551\n",
      "Iteration 472, loss = 0.20027872\n",
      "Iteration 473, loss = 0.19974490\n",
      "Iteration 474, loss = 0.19921387\n",
      "Iteration 475, loss = 0.19868580\n",
      "Iteration 476, loss = 0.19816084\n",
      "Iteration 477, loss = 0.19763849\n",
      "Iteration 478, loss = 0.19711897\n",
      "Iteration 479, loss = 0.19660249\n",
      "Iteration 480, loss = 0.19608852\n",
      "Iteration 481, loss = 0.19557718\n",
      "Iteration 482, loss = 0.19506905\n",
      "Iteration 483, loss = 0.19456373\n",
      "Iteration 484, loss = 0.19406071\n",
      "Iteration 485, loss = 0.19356081\n",
      "Iteration 486, loss = 0.19306393\n",
      "Iteration 487, loss = 0.19256974\n",
      "Iteration 488, loss = 0.19207796\n",
      "Iteration 489, loss = 0.19158977\n",
      "Iteration 490, loss = 0.19110391\n",
      "Iteration 491, loss = 0.19062030\n",
      "Iteration 492, loss = 0.19013957\n",
      "Iteration 493, loss = 0.18966231\n",
      "Iteration 494, loss = 0.18918696\n",
      "Iteration 495, loss = 0.18871475\n",
      "Iteration 496, loss = 0.18824552\n",
      "Iteration 497, loss = 0.18777858\n",
      "Iteration 498, loss = 0.18731405\n",
      "Iteration 499, loss = 0.18685244\n",
      "Iteration 500, loss = 0.18639388\n",
      "Iteration 501, loss = 0.18593802\n",
      "Iteration 502, loss = 0.18548460\n",
      "Iteration 503, loss = 0.18503396\n",
      "Iteration 504, loss = 0.18458591\n",
      "Iteration 505, loss = 0.18414007\n",
      "Iteration 506, loss = 0.18369685\n",
      "Iteration 507, loss = 0.18325601\n",
      "Iteration 508, loss = 0.18281774\n",
      "Iteration 509, loss = 0.18238172\n",
      "Iteration 510, loss = 0.18194792\n",
      "Iteration 511, loss = 0.18151651\n",
      "Iteration 512, loss = 0.18108735\n",
      "Iteration 513, loss = 0.18066054\n",
      "Iteration 514, loss = 0.18023605\n",
      "Iteration 515, loss = 0.17981363\n",
      "Iteration 516, loss = 0.17939371\n",
      "Iteration 517, loss = 0.17897611\n",
      "Iteration 518, loss = 0.17856088\n",
      "Iteration 519, loss = 0.17814755\n",
      "Iteration 520, loss = 0.17773665\n",
      "Iteration 521, loss = 0.17732803\n",
      "Iteration 522, loss = 0.17692112\n",
      "Iteration 523, loss = 0.17651638\n",
      "Iteration 524, loss = 0.17611377\n",
      "Iteration 525, loss = 0.17571347\n",
      "Iteration 526, loss = 0.17531545\n",
      "Iteration 527, loss = 0.17491962\n",
      "Iteration 528, loss = 0.17452541\n",
      "Iteration 529, loss = 0.17413301\n",
      "Iteration 530, loss = 0.17374269\n",
      "Iteration 531, loss = 0.17335453\n",
      "Iteration 532, loss = 0.17296802\n",
      "Iteration 533, loss = 0.17258304\n",
      "Iteration 534, loss = 0.17219974\n",
      "Iteration 535, loss = 0.17181830\n",
      "Iteration 536, loss = 0.17143886\n",
      "Iteration 537, loss = 0.17106259\n",
      "Iteration 538, loss = 0.17068830\n",
      "Iteration 539, loss = 0.17031635\n",
      "Iteration 540, loss = 0.16994668\n",
      "Iteration 541, loss = 0.16957906\n",
      "Iteration 542, loss = 0.16921345\n",
      "Iteration 543, loss = 0.16885007\n",
      "Iteration 544, loss = 0.16848875\n",
      "Iteration 545, loss = 0.16812945\n",
      "Iteration 546, loss = 0.16777218\n",
      "Iteration 547, loss = 0.16741692\n",
      "Iteration 548, loss = 0.16706364\n",
      "Iteration 549, loss = 0.16671261\n",
      "Iteration 550, loss = 0.16636351\n",
      "Iteration 551, loss = 0.16601667\n",
      "Iteration 552, loss = 0.16567178\n",
      "Iteration 553, loss = 0.16532837\n",
      "Iteration 554, loss = 0.16498703\n",
      "Iteration 555, loss = 0.16464755\n",
      "Iteration 556, loss = 0.16431022\n",
      "Iteration 557, loss = 0.16397505\n",
      "Iteration 558, loss = 0.16364226\n",
      "Iteration 559, loss = 0.16331209\n",
      "Iteration 560, loss = 0.16298391\n",
      "Iteration 561, loss = 0.16265747\n",
      "Iteration 562, loss = 0.16233292\n",
      "Iteration 563, loss = 0.16201034\n",
      "Iteration 564, loss = 0.16168926\n",
      "Iteration 565, loss = 0.16136981\n",
      "Iteration 566, loss = 0.16105195\n",
      "Iteration 567, loss = 0.16073603\n",
      "Iteration 568, loss = 0.16042190\n",
      "Iteration 569, loss = 0.16010948\n",
      "Iteration 570, loss = 0.15979875\n",
      "Iteration 571, loss = 0.15948971\n",
      "Iteration 572, loss = 0.15918249\n",
      "Iteration 573, loss = 0.15887698\n",
      "Iteration 574, loss = 0.15857449\n",
      "Iteration 575, loss = 0.15827378\n",
      "Iteration 576, loss = 0.15797477\n",
      "Iteration 577, loss = 0.15767763\n",
      "Iteration 578, loss = 0.15738216\n",
      "Iteration 579, loss = 0.15708833\n",
      "Iteration 580, loss = 0.15679611\n",
      "Iteration 581, loss = 0.15650533\n",
      "Iteration 582, loss = 0.15621596\n",
      "Iteration 583, loss = 0.15592816\n",
      "Iteration 584, loss = 0.15564202\n",
      "Iteration 585, loss = 0.15535760\n",
      "Iteration 586, loss = 0.15507481\n",
      "Iteration 587, loss = 0.15479360\n",
      "Iteration 588, loss = 0.15451388\n",
      "Iteration 589, loss = 0.15423563\n",
      "Iteration 590, loss = 0.15395893\n",
      "Iteration 591, loss = 0.15368375\n",
      "Iteration 592, loss = 0.15341005\n",
      "Iteration 593, loss = 0.15313821\n",
      "Iteration 594, loss = 0.15286779\n",
      "Iteration 595, loss = 0.15259864\n",
      "Iteration 596, loss = 0.15233091\n",
      "Iteration 597, loss = 0.15206461\n",
      "Iteration 598, loss = 0.15179968\n",
      "Iteration 599, loss = 0.15153621\n",
      "Iteration 600, loss = 0.15127426\n",
      "Iteration 601, loss = 0.15101365\n",
      "Iteration 602, loss = 0.15075432\n",
      "Iteration 603, loss = 0.15049615\n",
      "Iteration 604, loss = 0.15023939\n",
      "Iteration 605, loss = 0.14998394\n",
      "Iteration 606, loss = 0.14972985\n",
      "Iteration 607, loss = 0.14947705\n",
      "Iteration 608, loss = 0.14922563\n",
      "Iteration 609, loss = 0.14897547\n",
      "Iteration 610, loss = 0.14872658\n",
      "Iteration 611, loss = 0.14847895\n",
      "Iteration 612, loss = 0.14823256\n",
      "Iteration 613, loss = 0.14798740\n",
      "Iteration 614, loss = 0.14774346\n",
      "Iteration 615, loss = 0.14750073\n",
      "Iteration 616, loss = 0.14725911\n",
      "Iteration 617, loss = 0.14701848\n",
      "Iteration 618, loss = 0.14677901\n",
      "Iteration 619, loss = 0.14654068\n",
      "Iteration 620, loss = 0.14630366\n",
      "Iteration 621, loss = 0.14606767\n",
      "Iteration 622, loss = 0.14583270\n",
      "Iteration 623, loss = 0.14559894\n",
      "Iteration 624, loss = 0.14536638\n",
      "Iteration 625, loss = 0.14513493\n",
      "Iteration 626, loss = 0.14490456\n",
      "Iteration 627, loss = 0.14467528\n",
      "Iteration 628, loss = 0.14444700\n",
      "Iteration 629, loss = 0.14421988\n",
      "Iteration 630, loss = 0.14399365\n",
      "Iteration 631, loss = 0.14376863\n",
      "Iteration 632, loss = 0.14354505\n",
      "Iteration 633, loss = 0.14332257\n",
      "Iteration 634, loss = 0.14310126\n",
      "Iteration 635, loss = 0.14288096\n",
      "Iteration 636, loss = 0.14266183\n",
      "Iteration 637, loss = 0.14244373\n",
      "Iteration 638, loss = 0.14222671\n",
      "Iteration 639, loss = 0.14201074\n",
      "Iteration 640, loss = 0.14179588\n",
      "Iteration 641, loss = 0.14158206\n",
      "Iteration 642, loss = 0.14136929\n",
      "Iteration 643, loss = 0.14115754\n",
      "Iteration 644, loss = 0.14094684\n",
      "Iteration 645, loss = 0.14073711\n",
      "Iteration 646, loss = 0.14052840\n",
      "Iteration 647, loss = 0.14032069\n",
      "Iteration 648, loss = 0.14011395\n",
      "Iteration 649, loss = 0.13990820\n",
      "Iteration 650, loss = 0.13970341\n",
      "Iteration 651, loss = 0.13949959\n",
      "Iteration 652, loss = 0.13929674\n",
      "Iteration 653, loss = 0.13909483\n",
      "Iteration 654, loss = 0.13889388\n",
      "Iteration 655, loss = 0.13869386\n",
      "Iteration 656, loss = 0.13849481\n",
      "Iteration 657, loss = 0.13829666\n",
      "Iteration 658, loss = 0.13809941\n",
      "Iteration 659, loss = 0.13790302\n",
      "Iteration 660, loss = 0.13770752\n",
      "Iteration 661, loss = 0.13751285\n",
      "Iteration 662, loss = 0.13731911\n",
      "Iteration 663, loss = 0.13712612\n",
      "Iteration 664, loss = 0.13693440\n",
      "Iteration 665, loss = 0.13674337\n",
      "Iteration 666, loss = 0.13655273\n",
      "Iteration 667, loss = 0.13636241\n",
      "Iteration 668, loss = 0.13617308\n",
      "Iteration 669, loss = 0.13598705\n",
      "Iteration 670, loss = 0.13579793\n",
      "Iteration 671, loss = 0.13561379\n",
      "Iteration 672, loss = 0.13542958\n",
      "Iteration 673, loss = 0.13524501\n",
      "Iteration 674, loss = 0.13505987\n",
      "Iteration 675, loss = 0.13487405\n",
      "Iteration 676, loss = 0.13468795\n",
      "Iteration 677, loss = 0.13450517\n",
      "Iteration 678, loss = 0.13432503\n",
      "Iteration 679, loss = 0.13414130\n",
      "Iteration 680, loss = 0.13396314\n",
      "Iteration 681, loss = 0.13378509\n",
      "Iteration 682, loss = 0.13360561\n",
      "Iteration 683, loss = 0.13342476\n",
      "Iteration 684, loss = 0.13324364\n",
      "Iteration 685, loss = 0.13306429\n",
      "Iteration 686, loss = 0.13289244\n",
      "Iteration 687, loss = 0.13271050\n",
      "Iteration 688, loss = 0.13253443\n",
      "Iteration 689, loss = 0.13236066\n",
      "Iteration 690, loss = 0.13218665\n",
      "Iteration 691, loss = 0.13201171\n",
      "Iteration 692, loss = 0.13183615\n",
      "Iteration 693, loss = 0.13166124\n",
      "Iteration 694, loss = 0.13148747\n",
      "Iteration 695, loss = 0.13131669\n",
      "Iteration 696, loss = 0.13114388\n",
      "Iteration 697, loss = 0.13097096\n",
      "Iteration 698, loss = 0.13080068\n",
      "Iteration 699, loss = 0.13063043\n",
      "Iteration 700, loss = 0.13046003\n",
      "Iteration 701, loss = 0.13028963\n",
      "Iteration 702, loss = 0.13011987\n",
      "Iteration 703, loss = 0.12995133\n",
      "Iteration 704, loss = 0.12978258\n",
      "Iteration 705, loss = 0.12961489\n",
      "Iteration 706, loss = 0.12944770\n",
      "Iteration 707, loss = 0.12928070\n",
      "Iteration 708, loss = 0.12911390\n",
      "Iteration 709, loss = 0.12894735\n",
      "Iteration 710, loss = 0.12878270\n",
      "Iteration 711, loss = 0.12861768\n",
      "Iteration 712, loss = 0.12845212\n",
      "Iteration 713, loss = 0.12828786\n",
      "Iteration 714, loss = 0.12812461\n",
      "Iteration 715, loss = 0.12796155\n",
      "Iteration 716, loss = 0.12779836\n",
      "Iteration 717, loss = 0.12763554\n",
      "Iteration 718, loss = 0.12747459\n",
      "Iteration 719, loss = 0.12731369\n",
      "Iteration 720, loss = 0.12715218\n",
      "Iteration 721, loss = 0.12699199\n",
      "Iteration 722, loss = 0.12683299\n",
      "Iteration 723, loss = 0.12667399\n",
      "Iteration 724, loss = 0.12651495\n",
      "Iteration 725, loss = 0.12635639\n",
      "Iteration 726, loss = 0.12619948\n",
      "Iteration 727, loss = 0.12604267\n",
      "Iteration 728, loss = 0.12588553\n",
      "Iteration 729, loss = 0.12573004\n",
      "Iteration 730, loss = 0.12557479\n",
      "Iteration 731, loss = 0.12541968\n",
      "Iteration 732, loss = 0.12526579\n",
      "Iteration 733, loss = 0.12511144\n",
      "Iteration 734, loss = 0.12495763\n",
      "Iteration 735, loss = 0.12480364\n",
      "Iteration 736, loss = 0.12464908\n",
      "Iteration 737, loss = 0.12449382\n",
      "Iteration 738, loss = 0.12433780\n",
      "Iteration 739, loss = 0.12418145\n",
      "Iteration 740, loss = 0.12402359\n",
      "Iteration 741, loss = 0.12386511\n",
      "Iteration 742, loss = 0.12370538\n",
      "Iteration 743, loss = 0.12354389\n",
      "Iteration 744, loss = 0.12338047\n",
      "Iteration 745, loss = 0.12321512\n",
      "Iteration 746, loss = 0.12304794\n",
      "Iteration 747, loss = 0.12287953\n",
      "Iteration 748, loss = 0.12270870\n",
      "Iteration 749, loss = 0.12253596\n",
      "Iteration 750, loss = 0.12236121\n",
      "Iteration 751, loss = 0.12218450\n",
      "Iteration 752, loss = 0.12200581\n",
      "Iteration 753, loss = 0.12182516\n",
      "Iteration 754, loss = 0.12164289\n",
      "Iteration 755, loss = 0.12145869\n",
      "Iteration 756, loss = 0.12127262\n",
      "Iteration 757, loss = 0.12108490\n",
      "Iteration 758, loss = 0.12089554\n",
      "Iteration 759, loss = 0.12070466\n",
      "Iteration 760, loss = 0.12051227\n",
      "Iteration 761, loss = 0.12031844\n",
      "Iteration 762, loss = 0.12012326\n",
      "Iteration 763, loss = 0.11992710\n",
      "Iteration 764, loss = 0.11972955\n",
      "Iteration 765, loss = 0.11953097\n",
      "Iteration 766, loss = 0.11933146\n",
      "Iteration 767, loss = 0.11913098\n",
      "Iteration 768, loss = 0.11892964\n",
      "Iteration 769, loss = 0.11872752\n",
      "Iteration 770, loss = 0.11852471\n",
      "Iteration 771, loss = 0.11832136\n",
      "Iteration 772, loss = 0.11811740\n",
      "Iteration 773, loss = 0.11791316\n",
      "Iteration 774, loss = 0.11770849\n",
      "Iteration 775, loss = 0.11750354\n",
      "Iteration 776, loss = 0.11729833\n",
      "Iteration 777, loss = 0.11709292\n",
      "Iteration 778, loss = 0.11688739\n",
      "Iteration 779, loss = 0.11668179\n",
      "Iteration 780, loss = 0.11647619\n",
      "Iteration 781, loss = 0.11627065\n",
      "Iteration 782, loss = 0.11606522\n",
      "Iteration 783, loss = 0.11585997\n",
      "Iteration 784, loss = 0.11565498\n",
      "Iteration 785, loss = 0.11545025\n",
      "Iteration 786, loss = 0.11524587\n",
      "Iteration 787, loss = 0.11504185\n",
      "Iteration 788, loss = 0.11483829\n",
      "Iteration 789, loss = 0.11463518\n",
      "Iteration 790, loss = 0.11443257\n",
      "Iteration 791, loss = 0.11423054\n",
      "Iteration 792, loss = 0.11402909\n",
      "Iteration 793, loss = 0.11382827\n",
      "Iteration 794, loss = 0.11362812\n",
      "Iteration 795, loss = 0.11342865\n",
      "Iteration 796, loss = 0.11322998\n",
      "Iteration 797, loss = 0.11303210\n",
      "Iteration 798, loss = 0.11283499\n",
      "Iteration 799, loss = 0.11263872\n",
      "Iteration 800, loss = 0.11244343\n",
      "Iteration 801, loss = 0.11224889\n",
      "Iteration 802, loss = 0.11205526\n",
      "Iteration 803, loss = 0.11186248\n",
      "Iteration 804, loss = 0.11167063\n",
      "Iteration 805, loss = 0.11147976\n",
      "Iteration 806, loss = 0.11128988\n",
      "Iteration 807, loss = 0.11110106\n",
      "Iteration 808, loss = 0.11091319\n",
      "Iteration 809, loss = 0.11072628\n",
      "Iteration 810, loss = 0.11054039\n",
      "Iteration 811, loss = 0.11035556\n",
      "Iteration 812, loss = 0.11017169\n",
      "Iteration 813, loss = 0.10998897\n",
      "Iteration 814, loss = 0.10980731\n",
      "Iteration 815, loss = 0.10962669\n",
      "Iteration 816, loss = 0.10944712\n",
      "Iteration 817, loss = 0.10926863\n",
      "Iteration 818, loss = 0.10909123\n",
      "Iteration 819, loss = 0.10891492\n",
      "Iteration 820, loss = 0.10873979\n",
      "Iteration 821, loss = 0.10856562\n",
      "Iteration 822, loss = 0.10839264\n",
      "Iteration 823, loss = 0.10822076\n",
      "Iteration 824, loss = 0.10805000\n",
      "Iteration 825, loss = 0.10788035\n",
      "Iteration 826, loss = 0.10771180\n",
      "Iteration 827, loss = 0.10754436\n",
      "Iteration 828, loss = 0.10737805\n",
      "Iteration 829, loss = 0.10721284\n",
      "Iteration 830, loss = 0.10704874\n",
      "Iteration 831, loss = 0.10688579\n",
      "Iteration 832, loss = 0.10672390\n",
      "Iteration 833, loss = 0.10656313\n",
      "Iteration 834, loss = 0.10640348\n",
      "Iteration 835, loss = 0.10624497\n",
      "Iteration 836, loss = 0.10608761\n",
      "Iteration 837, loss = 0.10593135\n",
      "Iteration 838, loss = 0.10577618\n",
      "Iteration 839, loss = 0.10562210\n",
      "Iteration 840, loss = 0.10546911\n",
      "Iteration 841, loss = 0.10531719\n",
      "Iteration 842, loss = 0.10516634\n",
      "Iteration 843, loss = 0.10501656\n",
      "Iteration 844, loss = 0.10486792\n",
      "Iteration 845, loss = 0.10472023\n",
      "Iteration 846, loss = 0.10457366\n",
      "Iteration 847, loss = 0.10442814\n",
      "Iteration 848, loss = 0.10428365\n",
      "Iteration 849, loss = 0.10414020\n",
      "Iteration 850, loss = 0.10399780\n",
      "Iteration 851, loss = 0.10385641\n",
      "Iteration 852, loss = 0.10371602\n",
      "Iteration 853, loss = 0.10357665\n",
      "Iteration 854, loss = 0.10343830\n",
      "Iteration 855, loss = 0.10330098\n",
      "Iteration 856, loss = 0.10316468\n",
      "Iteration 857, loss = 0.10302931\n",
      "Iteration 858, loss = 0.10289500\n",
      "Iteration 859, loss = 0.10276168\n",
      "Iteration 860, loss = 0.10262932\n",
      "Iteration 861, loss = 0.10249792\n",
      "Iteration 862, loss = 0.10236746\n",
      "Iteration 863, loss = 0.10223795\n",
      "Iteration 864, loss = 0.10210939\n",
      "Iteration 865, loss = 0.10198176\n",
      "Iteration 866, loss = 0.10185505\n",
      "Iteration 867, loss = 0.10172927\n",
      "Iteration 868, loss = 0.10160440\n",
      "Iteration 869, loss = 0.10148044\n",
      "Iteration 870, loss = 0.10135747\n",
      "Iteration 871, loss = 0.10123539\n",
      "Iteration 872, loss = 0.10111439\n",
      "Iteration 873, loss = 0.10099408\n",
      "Iteration 874, loss = 0.10087477\n",
      "Iteration 875, loss = 0.10075631\n",
      "Iteration 876, loss = 0.10063864\n",
      "Iteration 877, loss = 0.10052188\n",
      "Iteration 878, loss = 0.10040596\n",
      "Iteration 879, loss = 0.10029082\n",
      "Iteration 880, loss = 0.10017659\n",
      "Iteration 881, loss = 0.10006315\n",
      "Iteration 882, loss = 0.09995054\n",
      "Iteration 883, loss = 0.09983881\n",
      "Iteration 884, loss = 0.09972783\n",
      "Iteration 885, loss = 0.09961767\n",
      "Iteration 886, loss = 0.09950836\n",
      "Iteration 887, loss = 0.09939979\n",
      "Iteration 888, loss = 0.09929210\n",
      "Iteration 889, loss = 0.09918519\n",
      "Iteration 890, loss = 0.09907905\n",
      "Iteration 891, loss = 0.09897368\n",
      "Iteration 892, loss = 0.09886907\n",
      "Iteration 893, loss = 0.09876521\n",
      "Iteration 894, loss = 0.09866213\n",
      "Iteration 895, loss = 0.09855976\n",
      "Iteration 896, loss = 0.09845816\n",
      "Iteration 897, loss = 0.09835729\n",
      "Iteration 898, loss = 0.09825717\n",
      "Iteration 899, loss = 0.09815774\n",
      "Iteration 900, loss = 0.09805903\n",
      "Iteration 901, loss = 0.09796103\n",
      "Iteration 902, loss = 0.09786380\n",
      "Iteration 903, loss = 0.09776728\n",
      "Iteration 904, loss = 0.09767144\n",
      "Iteration 905, loss = 0.09757628\n",
      "Iteration 906, loss = 0.09748180\n",
      "Iteration 907, loss = 0.09738798\n",
      "Iteration 908, loss = 0.09729483\n",
      "Iteration 909, loss = 0.09720234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92564666\n",
      "Iteration 2, loss = 1.88148122\n",
      "Iteration 3, loss = 1.83837827\n",
      "Iteration 4, loss = 1.79628362\n",
      "Iteration 5, loss = 1.75523019\n",
      "Iteration 6, loss = 1.71506751\n",
      "Iteration 7, loss = 1.67572137\n",
      "Iteration 8, loss = 1.63722320\n",
      "Iteration 9, loss = 1.59949272\n",
      "Iteration 10, loss = 1.56254813\n",
      "Iteration 11, loss = 1.52641572\n",
      "Iteration 12, loss = 1.49113087\n",
      "Iteration 13, loss = 1.45670356\n",
      "Iteration 14, loss = 1.42313988\n",
      "Iteration 15, loss = 1.39052110\n",
      "Iteration 16, loss = 1.35883948\n",
      "Iteration 17, loss = 1.32810859\n",
      "Iteration 18, loss = 1.29840192\n",
      "Iteration 19, loss = 1.26980437\n",
      "Iteration 20, loss = 1.24235491\n",
      "Iteration 21, loss = 1.21613247\n",
      "Iteration 22, loss = 1.19121166\n",
      "Iteration 23, loss = 1.16764370\n",
      "Iteration 24, loss = 1.14549120\n",
      "Iteration 25, loss = 1.12474202\n",
      "Iteration 26, loss = 1.10543115\n",
      "Iteration 27, loss = 1.08754993\n",
      "Iteration 28, loss = 1.07107021\n",
      "Iteration 29, loss = 1.05594721\n",
      "Iteration 30, loss = 1.04212449\n",
      "Iteration 31, loss = 1.02950265\n",
      "Iteration 32, loss = 1.01799437\n",
      "Iteration 33, loss = 1.00749752\n",
      "Iteration 34, loss = 0.99785490\n",
      "Iteration 35, loss = 0.98891799\n",
      "Iteration 36, loss = 0.98055688\n",
      "Iteration 37, loss = 0.97263658\n",
      "Iteration 38, loss = 0.96504376\n",
      "Iteration 39, loss = 0.95766065\n",
      "Iteration 40, loss = 0.95037629\n",
      "Iteration 41, loss = 0.94309053\n",
      "Iteration 42, loss = 0.93578678\n",
      "Iteration 43, loss = 0.92843973\n",
      "Iteration 44, loss = 0.92107514\n",
      "Iteration 45, loss = 0.91371982\n",
      "Iteration 46, loss = 0.90640126\n",
      "Iteration 47, loss = 0.89905454\n",
      "Iteration 48, loss = 0.89177467\n",
      "Iteration 49, loss = 0.88460110\n",
      "Iteration 50, loss = 0.87742646\n",
      "Iteration 51, loss = 0.87027775\n",
      "Iteration 52, loss = 0.86314673\n",
      "Iteration 53, loss = 0.85604554\n",
      "Iteration 54, loss = 0.84894680\n",
      "Iteration 55, loss = 0.84190128\n",
      "Iteration 56, loss = 0.83495320\n",
      "Iteration 57, loss = 0.82817101\n",
      "Iteration 58, loss = 0.82155907\n",
      "Iteration 59, loss = 0.81519320\n",
      "Iteration 60, loss = 0.80905525\n",
      "Iteration 61, loss = 0.80312367\n",
      "Iteration 62, loss = 0.79736906\n",
      "Iteration 63, loss = 0.79189391\n",
      "Iteration 64, loss = 0.78658862\n",
      "Iteration 65, loss = 0.78149276\n",
      "Iteration 66, loss = 0.77656010\n",
      "Iteration 67, loss = 0.77183246\n",
      "Iteration 68, loss = 0.76719757\n",
      "Iteration 69, loss = 0.76264320\n",
      "Iteration 70, loss = 0.75818564\n",
      "Iteration 71, loss = 0.75376062\n",
      "Iteration 72, loss = 0.74934265\n",
      "Iteration 73, loss = 0.74494199\n",
      "Iteration 74, loss = 0.74055398\n",
      "Iteration 75, loss = 0.73618085\n",
      "Iteration 76, loss = 0.73182939\n",
      "Iteration 77, loss = 0.72750733\n",
      "Iteration 78, loss = 0.72321408\n",
      "Iteration 79, loss = 0.71895623\n",
      "Iteration 80, loss = 0.71474014\n",
      "Iteration 81, loss = 0.71056105\n",
      "Iteration 82, loss = 0.70641861\n",
      "Iteration 83, loss = 0.70232072\n",
      "Iteration 84, loss = 0.69825897\n",
      "Iteration 85, loss = 0.69423912\n",
      "Iteration 86, loss = 0.69025753\n",
      "Iteration 87, loss = 0.68630520\n",
      "Iteration 88, loss = 0.68238574\n",
      "Iteration 89, loss = 0.67849809\n",
      "Iteration 90, loss = 0.67464817\n",
      "Iteration 91, loss = 0.67082535\n",
      "Iteration 92, loss = 0.66703504\n",
      "Iteration 93, loss = 0.66327987\n",
      "Iteration 94, loss = 0.65956802\n",
      "Iteration 95, loss = 0.65589995\n",
      "Iteration 96, loss = 0.65227227\n",
      "Iteration 97, loss = 0.64868225\n",
      "Iteration 98, loss = 0.64512196\n",
      "Iteration 99, loss = 0.64159770\n",
      "Iteration 100, loss = 0.63810739\n",
      "Iteration 101, loss = 0.63464989\n",
      "Iteration 102, loss = 0.63122620\n",
      "Iteration 103, loss = 0.62782446\n",
      "Iteration 104, loss = 0.62444884\n",
      "Iteration 105, loss = 0.62110836\n",
      "Iteration 106, loss = 0.61779306\n",
      "Iteration 107, loss = 0.61450772\n",
      "Iteration 108, loss = 0.61125937\n",
      "Iteration 109, loss = 0.60804847\n",
      "Iteration 110, loss = 0.60486992\n",
      "Iteration 111, loss = 0.60170726\n",
      "Iteration 112, loss = 0.59856655\n",
      "Iteration 113, loss = 0.59548146\n",
      "Iteration 114, loss = 0.59243123\n",
      "Iteration 115, loss = 0.58942336\n",
      "Iteration 116, loss = 0.58644368\n",
      "Iteration 117, loss = 0.58349746\n",
      "Iteration 118, loss = 0.58062541\n",
      "Iteration 119, loss = 0.57779583\n",
      "Iteration 120, loss = 0.57501312\n",
      "Iteration 121, loss = 0.57227809\n",
      "Iteration 122, loss = 0.56957306\n",
      "Iteration 123, loss = 0.56688825\n",
      "Iteration 124, loss = 0.56424762\n",
      "Iteration 125, loss = 0.56162790\n",
      "Iteration 126, loss = 0.55903648\n",
      "Iteration 127, loss = 0.55647286\n",
      "Iteration 128, loss = 0.55393497\n",
      "Iteration 129, loss = 0.55142748\n",
      "Iteration 130, loss = 0.54895642\n",
      "Iteration 131, loss = 0.54650899\n",
      "Iteration 132, loss = 0.54408313\n",
      "Iteration 133, loss = 0.54168103\n",
      "Iteration 134, loss = 0.53931752\n",
      "Iteration 135, loss = 0.53699628\n",
      "Iteration 136, loss = 0.53469455\n",
      "Iteration 137, loss = 0.53241388\n",
      "Iteration 138, loss = 0.53016390\n",
      "Iteration 139, loss = 0.52794054\n",
      "Iteration 140, loss = 0.52574429\n",
      "Iteration 141, loss = 0.52357169\n",
      "Iteration 142, loss = 0.52142052\n",
      "Iteration 143, loss = 0.51929080\n",
      "Iteration 144, loss = 0.51718171\n",
      "Iteration 145, loss = 0.51509427\n",
      "Iteration 146, loss = 0.51302824\n",
      "Iteration 147, loss = 0.51098148\n",
      "Iteration 148, loss = 0.50895366\n",
      "Iteration 149, loss = 0.50694444\n",
      "Iteration 150, loss = 0.50495371\n",
      "Iteration 151, loss = 0.50298112\n",
      "Iteration 152, loss = 0.50102610\n",
      "Iteration 153, loss = 0.49908849\n",
      "Iteration 154, loss = 0.49716960\n",
      "Iteration 155, loss = 0.49526803\n",
      "Iteration 156, loss = 0.49338303\n",
      "Iteration 157, loss = 0.49151439\n",
      "Iteration 158, loss = 0.48966190\n",
      "Iteration 159, loss = 0.48782534\n",
      "Iteration 160, loss = 0.48600451\n",
      "Iteration 161, loss = 0.48419926\n",
      "Iteration 162, loss = 0.48240916\n",
      "Iteration 163, loss = 0.48063385\n",
      "Iteration 164, loss = 0.47887310\n",
      "Iteration 165, loss = 0.47712671\n",
      "Iteration 166, loss = 0.47539429\n",
      "Iteration 167, loss = 0.47367585\n",
      "Iteration 168, loss = 0.47197106\n",
      "Iteration 169, loss = 0.47027968\n",
      "Iteration 170, loss = 0.46860143\n",
      "Iteration 171, loss = 0.46693618\n",
      "Iteration 172, loss = 0.46528360\n",
      "Iteration 173, loss = 0.46364344\n",
      "Iteration 174, loss = 0.46201546\n",
      "Iteration 175, loss = 0.46039942\n",
      "Iteration 176, loss = 0.45879509\n",
      "Iteration 177, loss = 0.45720222\n",
      "Iteration 178, loss = 0.45562121\n",
      "Iteration 179, loss = 0.45405197\n",
      "Iteration 180, loss = 0.45249357\n",
      "Iteration 181, loss = 0.45094587\n",
      "Iteration 182, loss = 0.44940883\n",
      "Iteration 183, loss = 0.44788300\n",
      "Iteration 184, loss = 0.44636710\n",
      "Iteration 185, loss = 0.44486098\n",
      "Iteration 186, loss = 0.44336478\n",
      "Iteration 187, loss = 0.44187832\n",
      "Iteration 188, loss = 0.44040135\n",
      "Iteration 189, loss = 0.43893365\n",
      "Iteration 190, loss = 0.43747663\n",
      "Iteration 191, loss = 0.43602880\n",
      "Iteration 192, loss = 0.43458851\n",
      "Iteration 193, loss = 0.43315742\n",
      "Iteration 194, loss = 0.43173505\n",
      "Iteration 195, loss = 0.43032099\n",
      "Iteration 196, loss = 0.42891506\n",
      "Iteration 197, loss = 0.42751709\n",
      "Iteration 198, loss = 0.42612709\n",
      "Iteration 199, loss = 0.42474596\n",
      "Iteration 200, loss = 0.42337384\n",
      "Iteration 201, loss = 0.42200990\n",
      "Iteration 202, loss = 0.42065377\n",
      "Iteration 203, loss = 0.41930450\n",
      "Iteration 204, loss = 0.41796210\n",
      "Iteration 205, loss = 0.41662867\n",
      "Iteration 206, loss = 0.41530296\n",
      "Iteration 207, loss = 0.41398404\n",
      "Iteration 208, loss = 0.41267184\n",
      "Iteration 209, loss = 0.41136627\n",
      "Iteration 210, loss = 0.41006798\n",
      "Iteration 211, loss = 0.40877685\n",
      "Iteration 212, loss = 0.40749228\n",
      "Iteration 213, loss = 0.40621422\n",
      "Iteration 214, loss = 0.40494261\n",
      "Iteration 215, loss = 0.40367748\n",
      "Iteration 216, loss = 0.40241843\n",
      "Iteration 217, loss = 0.40116580\n",
      "Iteration 218, loss = 0.39991934\n",
      "Iteration 219, loss = 0.39867893\n",
      "Iteration 220, loss = 0.39744448\n",
      "Iteration 221, loss = 0.39621604\n",
      "Iteration 222, loss = 0.39499342\n",
      "Iteration 223, loss = 0.39377654\n",
      "Iteration 224, loss = 0.39256556\n",
      "Iteration 225, loss = 0.39136015\n",
      "Iteration 226, loss = 0.39016029\n",
      "Iteration 227, loss = 0.38896602\n",
      "Iteration 228, loss = 0.38777709\n",
      "Iteration 229, loss = 0.38659357\n",
      "Iteration 230, loss = 0.38541541\n",
      "Iteration 231, loss = 0.38424249\n",
      "Iteration 232, loss = 0.38307484\n",
      "Iteration 233, loss = 0.38191230\n",
      "Iteration 234, loss = 0.38075487\n",
      "Iteration 235, loss = 0.37960247\n",
      "Iteration 236, loss = 0.37845488\n",
      "Iteration 237, loss = 0.37731208\n",
      "Iteration 238, loss = 0.37617411\n",
      "Iteration 239, loss = 0.37504085\n",
      "Iteration 240, loss = 0.37391242\n",
      "Iteration 241, loss = 0.37278859\n",
      "Iteration 242, loss = 0.37166937\n",
      "Iteration 243, loss = 0.37055486\n",
      "Iteration 244, loss = 0.36944485\n",
      "Iteration 245, loss = 0.36833935\n",
      "Iteration 246, loss = 0.36723823\n",
      "Iteration 247, loss = 0.36614158\n",
      "Iteration 248, loss = 0.36504940\n",
      "Iteration 249, loss = 0.36396178\n",
      "Iteration 250, loss = 0.36287879\n",
      "Iteration 251, loss = 0.36179998\n",
      "Iteration 252, loss = 0.36072538\n",
      "Iteration 253, loss = 0.35965500\n",
      "Iteration 254, loss = 0.35858879\n",
      "Iteration 255, loss = 0.35752670\n",
      "Iteration 256, loss = 0.35646877\n",
      "Iteration 257, loss = 0.35541493\n",
      "Iteration 258, loss = 0.35436505\n",
      "Iteration 259, loss = 0.35331920\n",
      "Iteration 260, loss = 0.35227732\n",
      "Iteration 261, loss = 0.35123949\n",
      "Iteration 262, loss = 0.35020540\n",
      "Iteration 263, loss = 0.34917539\n",
      "Iteration 264, loss = 0.34814923\n",
      "Iteration 265, loss = 0.34712699\n",
      "Iteration 266, loss = 0.34610854\n",
      "Iteration 267, loss = 0.34509391\n",
      "Iteration 268, loss = 0.34408303\n",
      "Iteration 269, loss = 0.34307589\n",
      "Iteration 270, loss = 0.34207255\n",
      "Iteration 271, loss = 0.34107292\n",
      "Iteration 272, loss = 0.34007695\n",
      "Iteration 273, loss = 0.33908475\n",
      "Iteration 274, loss = 0.33809635\n",
      "Iteration 275, loss = 0.33711155\n",
      "Iteration 276, loss = 0.33613026\n",
      "Iteration 277, loss = 0.33515250\n",
      "Iteration 278, loss = 0.33417835\n",
      "Iteration 279, loss = 0.33320772\n",
      "Iteration 280, loss = 0.33224061\n",
      "Iteration 281, loss = 0.33127695\n",
      "Iteration 282, loss = 0.33031672\n",
      "Iteration 283, loss = 0.32935994\n",
      "Iteration 284, loss = 0.32840658\n",
      "Iteration 285, loss = 0.32745661\n",
      "Iteration 286, loss = 0.32651000\n",
      "Iteration 287, loss = 0.32556674\n",
      "Iteration 288, loss = 0.32462689\n",
      "Iteration 289, loss = 0.32369037\n",
      "Iteration 290, loss = 0.32275713\n",
      "Iteration 291, loss = 0.32182719\n",
      "Iteration 292, loss = 0.32090053\n",
      "Iteration 293, loss = 0.31997711\n",
      "Iteration 294, loss = 0.31905693\n",
      "Iteration 295, loss = 0.31814002\n",
      "Iteration 296, loss = 0.31722629\n",
      "Iteration 297, loss = 0.31631573\n",
      "Iteration 298, loss = 0.31540838\n",
      "Iteration 299, loss = 0.31450420\n",
      "Iteration 300, loss = 0.31360370\n",
      "Iteration 301, loss = 0.31270640\n",
      "Iteration 302, loss = 0.31181206\n",
      "Iteration 303, loss = 0.31092069\n",
      "Iteration 304, loss = 0.31003229\n",
      "Iteration 305, loss = 0.30914696\n",
      "Iteration 306, loss = 0.30826459\n",
      "Iteration 307, loss = 0.30738593\n",
      "Iteration 308, loss = 0.30651033\n",
      "Iteration 309, loss = 0.30563773\n",
      "Iteration 310, loss = 0.30476812\n",
      "Iteration 311, loss = 0.30390148\n",
      "Iteration 312, loss = 0.30303863\n",
      "Iteration 313, loss = 0.30217877\n",
      "Iteration 314, loss = 0.30132181\n",
      "Iteration 315, loss = 0.30046776\n",
      "Iteration 316, loss = 0.29961672\n",
      "Iteration 317, loss = 0.29876876\n",
      "Iteration 318, loss = 0.29792364\n",
      "Iteration 319, loss = 0.29708138\n",
      "Iteration 320, loss = 0.29624198\n",
      "Iteration 321, loss = 0.29540545\n",
      "Iteration 322, loss = 0.29457239\n",
      "Iteration 323, loss = 0.29374204\n",
      "Iteration 324, loss = 0.29291437\n",
      "Iteration 325, loss = 0.29208972\n",
      "Iteration 326, loss = 0.29126821\n",
      "Iteration 327, loss = 0.29044960\n",
      "Iteration 328, loss = 0.28963378\n",
      "Iteration 329, loss = 0.28882079\n",
      "Iteration 330, loss = 0.28801097\n",
      "Iteration 331, loss = 0.28720406\n",
      "Iteration 332, loss = 0.28639994\n",
      "Iteration 333, loss = 0.28559858\n",
      "Iteration 334, loss = 0.28480031\n",
      "Iteration 335, loss = 0.28400501\n",
      "Iteration 336, loss = 0.28321243\n",
      "Iteration 337, loss = 0.28242264\n",
      "Iteration 338, loss = 0.28163563\n",
      "Iteration 339, loss = 0.28085161\n",
      "Iteration 340, loss = 0.28007036\n",
      "Iteration 341, loss = 0.27929210\n",
      "Iteration 342, loss = 0.27851670\n",
      "Iteration 343, loss = 0.27774408\n",
      "Iteration 344, loss = 0.27697406\n",
      "Iteration 345, loss = 0.27620699\n",
      "Iteration 346, loss = 0.27544296\n",
      "Iteration 347, loss = 0.27468160\n",
      "Iteration 348, loss = 0.27392295\n",
      "Iteration 349, loss = 0.27316700\n",
      "Iteration 350, loss = 0.27241389\n",
      "Iteration 351, loss = 0.27166348\n",
      "Iteration 352, loss = 0.27091580\n",
      "Iteration 353, loss = 0.27017097\n",
      "Iteration 354, loss = 0.26942889\n",
      "Iteration 355, loss = 0.26868970\n",
      "Iteration 356, loss = 0.26795328\n",
      "Iteration 357, loss = 0.26721940\n",
      "Iteration 358, loss = 0.26648759\n",
      "Iteration 359, loss = 0.26575834\n",
      "Iteration 360, loss = 0.26503287\n",
      "Iteration 361, loss = 0.26430956\n",
      "Iteration 362, loss = 0.26358906\n",
      "Iteration 363, loss = 0.26287166\n",
      "Iteration 364, loss = 0.26215640\n",
      "Iteration 365, loss = 0.26144397\n",
      "Iteration 366, loss = 0.26073434\n",
      "Iteration 367, loss = 0.26002705\n",
      "Iteration 368, loss = 0.25932248\n",
      "Iteration 369, loss = 0.25862069\n",
      "Iteration 370, loss = 0.25792169\n",
      "Iteration 371, loss = 0.25722516\n",
      "Iteration 372, loss = 0.25653114\n",
      "Iteration 373, loss = 0.25583978\n",
      "Iteration 374, loss = 0.25515041\n",
      "Iteration 375, loss = 0.25446438\n",
      "Iteration 376, loss = 0.25377911\n",
      "Iteration 377, loss = 0.25309695\n",
      "Iteration 378, loss = 0.25241370\n",
      "Iteration 379, loss = 0.25172857\n",
      "Iteration 380, loss = 0.25104473\n",
      "Iteration 381, loss = 0.25036072\n",
      "Iteration 382, loss = 0.24967279\n",
      "Iteration 383, loss = 0.24898318\n",
      "Iteration 384, loss = 0.24829609\n",
      "Iteration 385, loss = 0.24760847\n",
      "Iteration 386, loss = 0.24691743\n",
      "Iteration 387, loss = 0.24622646\n",
      "Iteration 388, loss = 0.24553520\n",
      "Iteration 389, loss = 0.24484217\n",
      "Iteration 390, loss = 0.24414815\n",
      "Iteration 391, loss = 0.24345277\n",
      "Iteration 392, loss = 0.24275615\n",
      "Iteration 393, loss = 0.24205901\n",
      "Iteration 394, loss = 0.24136087\n",
      "Iteration 395, loss = 0.24066122\n",
      "Iteration 396, loss = 0.23996179\n",
      "Iteration 397, loss = 0.23926167\n",
      "Iteration 398, loss = 0.23856120\n",
      "Iteration 399, loss = 0.23786066\n",
      "Iteration 400, loss = 0.23715976\n",
      "Iteration 401, loss = 0.23645885\n",
      "Iteration 402, loss = 0.23575783\n",
      "Iteration 403, loss = 0.23505689\n",
      "Iteration 404, loss = 0.23435610\n",
      "Iteration 405, loss = 0.23365533\n",
      "Iteration 406, loss = 0.23295511\n",
      "Iteration 407, loss = 0.23225538\n",
      "Iteration 408, loss = 0.23155688\n",
      "Iteration 409, loss = 0.23085885\n",
      "Iteration 410, loss = 0.23016118\n",
      "Iteration 411, loss = 0.22946496\n",
      "Iteration 412, loss = 0.22876973\n",
      "Iteration 413, loss = 0.22807560\n",
      "Iteration 414, loss = 0.22738269\n",
      "Iteration 415, loss = 0.22669111\n",
      "Iteration 416, loss = 0.22600099\n",
      "Iteration 417, loss = 0.22531266\n",
      "Iteration 418, loss = 0.22462710\n",
      "Iteration 419, loss = 0.22394317\n",
      "Iteration 420, loss = 0.22326101\n",
      "Iteration 421, loss = 0.22258065\n",
      "Iteration 422, loss = 0.22190224\n",
      "Iteration 423, loss = 0.22122585\n",
      "Iteration 424, loss = 0.22055192\n",
      "Iteration 425, loss = 0.21987958\n",
      "Iteration 426, loss = 0.21920987\n",
      "Iteration 427, loss = 0.21854267\n",
      "Iteration 428, loss = 0.21787737\n",
      "Iteration 429, loss = 0.21721431\n",
      "Iteration 430, loss = 0.21655489\n",
      "Iteration 431, loss = 0.21589534\n",
      "Iteration 432, loss = 0.21523993\n",
      "Iteration 433, loss = 0.21458752\n",
      "Iteration 434, loss = 0.21393739\n",
      "Iteration 435, loss = 0.21329000\n",
      "Iteration 436, loss = 0.21264571\n",
      "Iteration 437, loss = 0.21200453\n",
      "Iteration 438, loss = 0.21136575\n",
      "Iteration 439, loss = 0.21072960\n",
      "Iteration 440, loss = 0.21009679\n",
      "Iteration 441, loss = 0.20946690\n",
      "Iteration 442, loss = 0.20884002\n",
      "Iteration 443, loss = 0.20821606\n",
      "Iteration 444, loss = 0.20759515\n",
      "Iteration 445, loss = 0.20697728\n",
      "Iteration 446, loss = 0.20636249\n",
      "Iteration 447, loss = 0.20575080\n",
      "Iteration 448, loss = 0.20514223\n",
      "Iteration 449, loss = 0.20453645\n",
      "Iteration 450, loss = 0.20393374\n",
      "Iteration 451, loss = 0.20333413\n",
      "Iteration 452, loss = 0.20273763\n",
      "Iteration 453, loss = 0.20214424\n",
      "Iteration 454, loss = 0.20155395\n",
      "Iteration 455, loss = 0.20096680\n",
      "Iteration 456, loss = 0.20038280\n",
      "Iteration 457, loss = 0.19980193\n",
      "Iteration 458, loss = 0.19922419\n",
      "Iteration 459, loss = 0.19864969\n",
      "Iteration 460, loss = 0.19807824\n",
      "Iteration 461, loss = 0.19750990\n",
      "Iteration 462, loss = 0.19694460\n",
      "Iteration 463, loss = 0.19638240\n",
      "Iteration 464, loss = 0.19582327\n",
      "Iteration 465, loss = 0.19526723\n",
      "Iteration 466, loss = 0.19471426\n",
      "Iteration 467, loss = 0.19416430\n",
      "Iteration 468, loss = 0.19361728\n",
      "Iteration 469, loss = 0.19307338\n",
      "Iteration 470, loss = 0.19253253\n",
      "Iteration 471, loss = 0.19199470\n",
      "Iteration 472, loss = 0.19145994\n",
      "Iteration 473, loss = 0.19092818\n",
      "Iteration 474, loss = 0.19039942\n",
      "Iteration 475, loss = 0.18987365\n",
      "Iteration 476, loss = 0.18935087\n",
      "Iteration 477, loss = 0.18883138\n",
      "Iteration 478, loss = 0.18831512\n",
      "Iteration 479, loss = 0.18780182\n",
      "Iteration 480, loss = 0.18729145\n",
      "Iteration 481, loss = 0.18678405\n",
      "Iteration 482, loss = 0.18627953\n",
      "Iteration 483, loss = 0.18577761\n",
      "Iteration 484, loss = 0.18527820\n",
      "Iteration 485, loss = 0.18478155\n",
      "Iteration 486, loss = 0.18428773\n",
      "Iteration 487, loss = 0.18379672\n",
      "Iteration 488, loss = 0.18330841\n",
      "Iteration 489, loss = 0.18282284\n",
      "Iteration 490, loss = 0.18233997\n",
      "Iteration 491, loss = 0.18185991\n",
      "Iteration 492, loss = 0.18138258\n",
      "Iteration 493, loss = 0.18090795\n",
      "Iteration 494, loss = 0.18043590\n",
      "Iteration 495, loss = 0.17996647\n",
      "Iteration 496, loss = 0.17950007\n",
      "Iteration 497, loss = 0.17903626\n",
      "Iteration 498, loss = 0.17857509\n",
      "Iteration 499, loss = 0.17811620\n",
      "Iteration 500, loss = 0.17765927\n",
      "Iteration 501, loss = 0.17720474\n",
      "Iteration 502, loss = 0.17675269\n",
      "Iteration 503, loss = 0.17630281\n",
      "Iteration 504, loss = 0.17585521\n",
      "Iteration 505, loss = 0.17540949\n",
      "Iteration 506, loss = 0.17496607\n",
      "Iteration 507, loss = 0.17452401\n",
      "Iteration 508, loss = 0.17408394\n",
      "Iteration 509, loss = 0.17364614\n",
      "Iteration 510, loss = 0.17321161\n",
      "Iteration 511, loss = 0.17277950\n",
      "Iteration 512, loss = 0.17234965\n",
      "Iteration 513, loss = 0.17192227\n",
      "Iteration 514, loss = 0.17149742\n",
      "Iteration 515, loss = 0.17107494\n",
      "Iteration 516, loss = 0.17065497\n",
      "Iteration 517, loss = 0.17023728\n",
      "Iteration 518, loss = 0.16982220\n",
      "Iteration 519, loss = 0.16940962\n",
      "Iteration 520, loss = 0.16899929\n",
      "Iteration 521, loss = 0.16859184\n",
      "Iteration 522, loss = 0.16818693\n",
      "Iteration 523, loss = 0.16778432\n",
      "Iteration 524, loss = 0.16738414\n",
      "Iteration 525, loss = 0.16698653\n",
      "Iteration 526, loss = 0.16659185\n",
      "Iteration 527, loss = 0.16619974\n",
      "Iteration 528, loss = 0.16581115\n",
      "Iteration 529, loss = 0.16542480\n",
      "Iteration 530, loss = 0.16504074\n",
      "Iteration 531, loss = 0.16465899\n",
      "Iteration 532, loss = 0.16427974\n",
      "Iteration 533, loss = 0.16390204\n",
      "Iteration 534, loss = 0.16352629\n",
      "Iteration 535, loss = 0.16315291\n",
      "Iteration 536, loss = 0.16278168\n",
      "Iteration 537, loss = 0.16241269\n",
      "Iteration 538, loss = 0.16204612\n",
      "Iteration 539, loss = 0.16168170\n",
      "Iteration 540, loss = 0.16131934\n",
      "Iteration 541, loss = 0.16095873\n",
      "Iteration 542, loss = 0.16060010\n",
      "Iteration 543, loss = 0.16024405\n",
      "Iteration 544, loss = 0.15989126\n",
      "Iteration 545, loss = 0.15954054\n",
      "Iteration 546, loss = 0.15919192\n",
      "Iteration 547, loss = 0.15884564\n",
      "Iteration 548, loss = 0.15850201\n",
      "Iteration 549, loss = 0.15816057\n",
      "Iteration 550, loss = 0.15782109\n",
      "Iteration 551, loss = 0.15748357\n",
      "Iteration 552, loss = 0.15714786\n",
      "Iteration 553, loss = 0.15681395\n",
      "Iteration 554, loss = 0.15648208\n",
      "Iteration 555, loss = 0.15615224\n",
      "Iteration 556, loss = 0.15582455\n",
      "Iteration 557, loss = 0.15549867\n",
      "Iteration 558, loss = 0.15517464\n",
      "Iteration 559, loss = 0.15485253\n",
      "Iteration 560, loss = 0.15453216\n",
      "Iteration 561, loss = 0.15421358\n",
      "Iteration 562, loss = 0.15389675\n",
      "Iteration 563, loss = 0.15358173\n",
      "Iteration 564, loss = 0.15326848\n",
      "Iteration 565, loss = 0.15295700\n",
      "Iteration 566, loss = 0.15264726\n",
      "Iteration 567, loss = 0.15233921\n",
      "Iteration 568, loss = 0.15203285\n",
      "Iteration 569, loss = 0.15172816\n",
      "Iteration 570, loss = 0.15142508\n",
      "Iteration 571, loss = 0.15112363\n",
      "Iteration 572, loss = 0.15082377\n",
      "Iteration 573, loss = 0.15052550\n",
      "Iteration 574, loss = 0.15022880\n",
      "Iteration 575, loss = 0.14993366\n",
      "Iteration 576, loss = 0.14964008\n",
      "Iteration 577, loss = 0.14934804\n",
      "Iteration 578, loss = 0.14905753\n",
      "Iteration 579, loss = 0.14876854\n",
      "Iteration 580, loss = 0.14848107\n",
      "Iteration 581, loss = 0.14819509\n",
      "Iteration 582, loss = 0.14791060\n",
      "Iteration 583, loss = 0.14762758\n",
      "Iteration 584, loss = 0.14734605\n",
      "Iteration 585, loss = 0.14706599\n",
      "Iteration 586, loss = 0.14678747\n",
      "Iteration 587, loss = 0.14651039\n",
      "Iteration 588, loss = 0.14623470\n",
      "Iteration 589, loss = 0.14596041\n",
      "Iteration 590, loss = 0.14568755\n",
      "Iteration 591, loss = 0.14541627\n",
      "Iteration 592, loss = 0.14514634\n",
      "Iteration 593, loss = 0.14487772\n",
      "Iteration 594, loss = 0.14461049\n",
      "Iteration 595, loss = 0.14434470\n",
      "Iteration 596, loss = 0.14408028\n",
      "Iteration 597, loss = 0.14381721\n",
      "Iteration 598, loss = 0.14355548\n",
      "Iteration 599, loss = 0.14329511\n",
      "Iteration 600, loss = 0.14303607\n",
      "Iteration 601, loss = 0.14277835\n",
      "Iteration 602, loss = 0.14252194\n",
      "Iteration 603, loss = 0.14226684\n",
      "Iteration 604, loss = 0.14201308\n",
      "Iteration 605, loss = 0.14176063\n",
      "Iteration 606, loss = 0.14150940\n",
      "Iteration 607, loss = 0.14125954\n",
      "Iteration 608, loss = 0.14101095\n",
      "Iteration 609, loss = 0.14076358\n",
      "Iteration 610, loss = 0.14051740\n",
      "Iteration 611, loss = 0.14027252\n",
      "Iteration 612, loss = 0.14002893\n",
      "Iteration 613, loss = 0.13978653\n",
      "Iteration 614, loss = 0.13954535\n",
      "Iteration 615, loss = 0.13930537\n",
      "Iteration 616, loss = 0.13906660\n",
      "Iteration 617, loss = 0.13882904\n",
      "Iteration 618, loss = 0.13859267\n",
      "Iteration 619, loss = 0.13835749\n",
      "Iteration 620, loss = 0.13812350\n",
      "Iteration 621, loss = 0.13789069\n",
      "Iteration 622, loss = 0.13765903\n",
      "Iteration 623, loss = 0.13742850\n",
      "Iteration 624, loss = 0.13719911\n",
      "Iteration 625, loss = 0.13697083\n",
      "Iteration 626, loss = 0.13674367\n",
      "Iteration 627, loss = 0.13651761\n",
      "Iteration 628, loss = 0.13629265\n",
      "Iteration 629, loss = 0.13606879\n",
      "Iteration 630, loss = 0.13584617\n",
      "Iteration 631, loss = 0.13562459\n",
      "Iteration 632, loss = 0.13540396\n",
      "Iteration 633, loss = 0.13518433\n",
      "Iteration 634, loss = 0.13496597\n",
      "Iteration 635, loss = 0.13474867\n",
      "Iteration 636, loss = 0.13453237\n",
      "Iteration 637, loss = 0.13431703\n",
      "Iteration 638, loss = 0.13410263\n",
      "Iteration 639, loss = 0.13388922\n",
      "Iteration 640, loss = 0.13367699\n",
      "Iteration 641, loss = 0.13346571\n",
      "Iteration 642, loss = 0.13325537\n",
      "Iteration 643, loss = 0.13304597\n",
      "Iteration 644, loss = 0.13283751\n",
      "Iteration 645, loss = 0.13263000\n",
      "Iteration 646, loss = 0.13242343\n",
      "Iteration 647, loss = 0.13221772\n",
      "Iteration 648, loss = 0.13201295\n",
      "Iteration 649, loss = 0.13180906\n",
      "Iteration 650, loss = 0.13160601\n",
      "Iteration 651, loss = 0.13140376\n",
      "Iteration 652, loss = 0.13120231\n",
      "Iteration 653, loss = 0.13100161\n",
      "Iteration 654, loss = 0.13080165\n",
      "Iteration 655, loss = 0.13060240\n",
      "Iteration 656, loss = 0.13040381\n",
      "Iteration 657, loss = 0.13020586\n",
      "Iteration 658, loss = 0.13000853\n",
      "Iteration 659, loss = 0.12981173\n",
      "Iteration 660, loss = 0.12961544\n",
      "Iteration 661, loss = 0.12941963\n",
      "Iteration 662, loss = 0.12922425\n",
      "Iteration 663, loss = 0.12902930\n",
      "Iteration 664, loss = 0.12883469\n",
      "Iteration 665, loss = 0.12864046\n",
      "Iteration 666, loss = 0.12844648\n",
      "Iteration 667, loss = 0.12825271\n",
      "Iteration 668, loss = 0.12805913\n",
      "Iteration 669, loss = 0.12786558\n",
      "Iteration 670, loss = 0.12767245\n",
      "Iteration 671, loss = 0.12747915\n",
      "Iteration 672, loss = 0.12728615\n",
      "Iteration 673, loss = 0.12709261\n",
      "Iteration 674, loss = 0.12689975\n",
      "Iteration 675, loss = 0.12670654\n",
      "Iteration 676, loss = 0.12651291\n",
      "Iteration 677, loss = 0.12632016\n",
      "Iteration 678, loss = 0.12612642\n",
      "Iteration 679, loss = 0.12593356\n",
      "Iteration 680, loss = 0.12574034\n",
      "Iteration 681, loss = 0.12554695\n",
      "Iteration 682, loss = 0.12535505\n",
      "Iteration 683, loss = 0.12516122\n",
      "Iteration 684, loss = 0.12496885\n",
      "Iteration 685, loss = 0.12477625\n",
      "Iteration 686, loss = 0.12458358\n",
      "Iteration 687, loss = 0.12439144\n",
      "Iteration 688, loss = 0.12419967\n",
      "Iteration 689, loss = 0.12400785\n",
      "Iteration 690, loss = 0.12381663\n",
      "Iteration 691, loss = 0.12362553\n",
      "Iteration 692, loss = 0.12343535\n",
      "Iteration 693, loss = 0.12324481\n",
      "Iteration 694, loss = 0.12305505\n",
      "Iteration 695, loss = 0.12286543\n",
      "Iteration 696, loss = 0.12267560\n",
      "Iteration 697, loss = 0.12248705\n",
      "Iteration 698, loss = 0.12229868\n",
      "Iteration 699, loss = 0.12211201\n",
      "Iteration 700, loss = 0.12192575\n",
      "Iteration 701, loss = 0.12173991\n",
      "Iteration 702, loss = 0.12155417\n",
      "Iteration 703, loss = 0.12137057\n",
      "Iteration 704, loss = 0.12118649\n",
      "Iteration 705, loss = 0.12100343\n",
      "Iteration 706, loss = 0.12082181\n",
      "Iteration 707, loss = 0.12064039\n",
      "Iteration 708, loss = 0.12045995\n",
      "Iteration 709, loss = 0.12028050\n",
      "Iteration 710, loss = 0.12010230\n",
      "Iteration 711, loss = 0.11992440\n",
      "Iteration 712, loss = 0.11974777\n",
      "Iteration 713, loss = 0.11957226\n",
      "Iteration 714, loss = 0.11939751\n",
      "Iteration 715, loss = 0.11922351\n",
      "Iteration 716, loss = 0.11905033\n",
      "Iteration 717, loss = 0.11887796\n",
      "Iteration 718, loss = 0.11870673\n",
      "Iteration 719, loss = 0.11853653\n",
      "Iteration 720, loss = 0.11836751\n",
      "Iteration 721, loss = 0.11820027\n",
      "Iteration 722, loss = 0.11803289\n",
      "Iteration 723, loss = 0.11786710\n",
      "Iteration 724, loss = 0.11770233\n",
      "Iteration 725, loss = 0.11753854\n",
      "Iteration 726, loss = 0.11737564\n",
      "Iteration 727, loss = 0.11721364\n",
      "Iteration 728, loss = 0.11705254\n",
      "Iteration 729, loss = 0.11689236\n",
      "Iteration 730, loss = 0.11673311\n",
      "Iteration 731, loss = 0.11657475\n",
      "Iteration 732, loss = 0.11641762\n",
      "Iteration 733, loss = 0.11626147\n",
      "Iteration 734, loss = 0.11610617\n",
      "Iteration 735, loss = 0.11595153\n",
      "Iteration 736, loss = 0.11579766\n",
      "Iteration 737, loss = 0.11564468\n",
      "Iteration 738, loss = 0.11549325\n",
      "Iteration 739, loss = 0.11534276\n",
      "Iteration 740, loss = 0.11519271\n",
      "Iteration 741, loss = 0.11504303\n",
      "Iteration 742, loss = 0.11489357\n",
      "Iteration 743, loss = 0.11474419\n",
      "Iteration 744, loss = 0.11459482\n",
      "Iteration 745, loss = 0.11444507\n",
      "Iteration 746, loss = 0.11429476\n",
      "Iteration 747, loss = 0.11414370\n",
      "Iteration 748, loss = 0.11399188\n",
      "Iteration 749, loss = 0.11383910\n",
      "Iteration 750, loss = 0.11368536\n",
      "Iteration 751, loss = 0.11353099\n",
      "Iteration 752, loss = 0.11337493\n",
      "Iteration 753, loss = 0.11321703\n",
      "Iteration 754, loss = 0.11305735\n",
      "Iteration 755, loss = 0.11289580\n",
      "Iteration 756, loss = 0.11273307\n",
      "Iteration 757, loss = 0.11256867\n",
      "Iteration 758, loss = 0.11240213\n",
      "Iteration 759, loss = 0.11223403\n",
      "Iteration 760, loss = 0.11206421\n",
      "Iteration 761, loss = 0.11189253\n",
      "Iteration 762, loss = 0.11171908\n",
      "Iteration 763, loss = 0.11154391\n",
      "Iteration 764, loss = 0.11136725\n",
      "Iteration 765, loss = 0.11118966\n",
      "Iteration 766, loss = 0.11101052\n",
      "Iteration 767, loss = 0.11083017\n",
      "Iteration 768, loss = 0.11064836\n",
      "Iteration 769, loss = 0.11046519\n",
      "Iteration 770, loss = 0.11028078\n",
      "Iteration 771, loss = 0.11009566\n",
      "Iteration 772, loss = 0.10990950\n",
      "Iteration 773, loss = 0.10972274\n",
      "Iteration 774, loss = 0.10953508\n",
      "Iteration 775, loss = 0.10934646\n",
      "Iteration 776, loss = 0.10915697\n",
      "Iteration 777, loss = 0.10896704\n",
      "Iteration 778, loss = 0.10877715\n",
      "Iteration 779, loss = 0.10858672\n",
      "Iteration 780, loss = 0.10839576\n",
      "Iteration 781, loss = 0.10820436\n",
      "Iteration 782, loss = 0.10801259\n",
      "Iteration 783, loss = 0.10782080\n",
      "Iteration 784, loss = 0.10762943\n",
      "Iteration 785, loss = 0.10743790\n",
      "Iteration 786, loss = 0.10724633\n",
      "Iteration 787, loss = 0.10705475\n",
      "Iteration 788, loss = 0.10686345\n",
      "Iteration 789, loss = 0.10667223\n",
      "Iteration 790, loss = 0.10648130\n",
      "Iteration 791, loss = 0.10629068\n",
      "Iteration 792, loss = 0.10610035\n",
      "Iteration 793, loss = 0.10591047\n",
      "Iteration 794, loss = 0.10572123\n",
      "Iteration 795, loss = 0.10553249\n",
      "Iteration 796, loss = 0.10534411\n",
      "Iteration 797, loss = 0.10515615\n",
      "Iteration 798, loss = 0.10496883\n",
      "Iteration 799, loss = 0.10478239\n",
      "Iteration 800, loss = 0.10459644\n",
      "Iteration 801, loss = 0.10441136\n",
      "Iteration 802, loss = 0.10422685\n",
      "Iteration 803, loss = 0.10404289\n",
      "Iteration 804, loss = 0.10385987\n",
      "Iteration 805, loss = 0.10367778\n",
      "Iteration 806, loss = 0.10349649\n",
      "Iteration 807, loss = 0.10331597\n",
      "Iteration 808, loss = 0.10313636\n",
      "Iteration 809, loss = 0.10295764\n",
      "Iteration 810, loss = 0.10277984\n",
      "Iteration 811, loss = 0.10260300\n",
      "Iteration 812, loss = 0.10242707\n",
      "Iteration 813, loss = 0.10225227\n",
      "Iteration 814, loss = 0.10207852\n",
      "Iteration 815, loss = 0.10190569\n",
      "Iteration 816, loss = 0.10173383\n",
      "Iteration 817, loss = 0.10156308\n",
      "Iteration 818, loss = 0.10139332\n",
      "Iteration 819, loss = 0.10122454\n",
      "Iteration 820, loss = 0.10105701\n",
      "Iteration 821, loss = 0.10089045\n",
      "Iteration 822, loss = 0.10072486\n",
      "Iteration 823, loss = 0.10056034\n",
      "Iteration 824, loss = 0.10039707\n",
      "Iteration 825, loss = 0.10023487\n",
      "Iteration 826, loss = 0.10007363\n",
      "Iteration 827, loss = 0.09991354\n",
      "Iteration 828, loss = 0.09975459\n",
      "Iteration 829, loss = 0.09959659\n",
      "Iteration 830, loss = 0.09943953\n",
      "Iteration 831, loss = 0.09928355\n",
      "Iteration 832, loss = 0.09912861\n",
      "Iteration 833, loss = 0.09897472\n",
      "Iteration 834, loss = 0.09882187\n",
      "Iteration 835, loss = 0.09866991\n",
      "Iteration 836, loss = 0.09851888\n",
      "Iteration 837, loss = 0.09836878\n",
      "Iteration 838, loss = 0.09821951\n",
      "Iteration 839, loss = 0.09807086\n",
      "Iteration 840, loss = 0.09792296\n",
      "Iteration 841, loss = 0.09777625\n",
      "Iteration 842, loss = 0.09763040\n",
      "Iteration 843, loss = 0.09748507\n",
      "Iteration 844, loss = 0.09733998\n",
      "Iteration 845, loss = 0.09719498\n",
      "Iteration 846, loss = 0.09705025\n",
      "Iteration 847, loss = 0.09690563\n",
      "Iteration 848, loss = 0.09676111\n",
      "Iteration 849, loss = 0.09661693\n",
      "Iteration 850, loss = 0.09647335\n",
      "Iteration 851, loss = 0.09632936\n",
      "Iteration 852, loss = 0.09618583\n",
      "Iteration 853, loss = 0.09604196\n",
      "Iteration 854, loss = 0.09589828\n",
      "Iteration 855, loss = 0.09575452\n",
      "Iteration 856, loss = 0.09561038\n",
      "Iteration 857, loss = 0.09546654\n",
      "Iteration 858, loss = 0.09532260\n",
      "Iteration 859, loss = 0.09517837\n",
      "Iteration 860, loss = 0.09503428\n",
      "Iteration 861, loss = 0.09489009\n",
      "Iteration 862, loss = 0.09474623\n",
      "Iteration 863, loss = 0.09460228\n",
      "Iteration 864, loss = 0.09445822\n",
      "Iteration 865, loss = 0.09431415\n",
      "Iteration 866, loss = 0.09417006\n",
      "Iteration 867, loss = 0.09402610\n",
      "Iteration 868, loss = 0.09388246\n",
      "Iteration 869, loss = 0.09373899\n",
      "Iteration 870, loss = 0.09359553\n",
      "Iteration 871, loss = 0.09345237\n",
      "Iteration 872, loss = 0.09330953\n",
      "Iteration 873, loss = 0.09316696\n",
      "Iteration 874, loss = 0.09302469\n",
      "Iteration 875, loss = 0.09288290\n",
      "Iteration 876, loss = 0.09274154\n",
      "Iteration 877, loss = 0.09260059\n",
      "Iteration 878, loss = 0.09246014\n",
      "Iteration 879, loss = 0.09232020\n",
      "Iteration 880, loss = 0.09218079\n",
      "Iteration 881, loss = 0.09204212\n",
      "Iteration 882, loss = 0.09190392\n",
      "Iteration 883, loss = 0.09176633\n",
      "Iteration 884, loss = 0.09162951\n",
      "Iteration 885, loss = 0.09149340\n",
      "Iteration 886, loss = 0.09135800\n",
      "Iteration 887, loss = 0.09122334\n",
      "Iteration 888, loss = 0.09108945\n",
      "Iteration 889, loss = 0.09095633\n",
      "Iteration 890, loss = 0.09082404\n",
      "Iteration 891, loss = 0.09069258\n",
      "Iteration 892, loss = 0.09056196\n",
      "Iteration 893, loss = 0.09043222\n",
      "Iteration 894, loss = 0.09030340\n",
      "Iteration 895, loss = 0.09017560\n",
      "Iteration 896, loss = 0.09004854\n",
      "Iteration 897, loss = 0.08992246\n",
      "Iteration 898, loss = 0.08979739\n",
      "Iteration 899, loss = 0.08967331\n",
      "Iteration 900, loss = 0.08955026\n",
      "Iteration 901, loss = 0.08942814\n",
      "Iteration 902, loss = 0.08930692\n",
      "Iteration 903, loss = 0.08918684\n",
      "Iteration 904, loss = 0.08906757\n",
      "Iteration 905, loss = 0.08894947\n",
      "Iteration 906, loss = 0.08883225\n",
      "Iteration 907, loss = 0.08871618\n",
      "Iteration 908, loss = 0.08860115\n",
      "Iteration 909, loss = 0.08848713\n",
      "Iteration 910, loss = 0.08837420\n",
      "Iteration 911, loss = 0.08826243\n",
      "Iteration 912, loss = 0.08815161\n",
      "Iteration 913, loss = 0.08804239\n",
      "Iteration 914, loss = 0.08793381\n",
      "Iteration 915, loss = 0.08782675\n",
      "Iteration 916, loss = 0.08772056\n",
      "Iteration 917, loss = 0.08761531\n",
      "Iteration 918, loss = 0.08751149\n",
      "Iteration 919, loss = 0.08740818\n",
      "Iteration 920, loss = 0.08730611\n",
      "Iteration 921, loss = 0.08720453\n",
      "Iteration 922, loss = 0.08710506\n",
      "Iteration 923, loss = 0.08700541\n",
      "Iteration 924, loss = 0.08690790\n",
      "Iteration 925, loss = 0.08681024\n",
      "Iteration 926, loss = 0.08671480\n",
      "Iteration 927, loss = 0.08661912\n",
      "Iteration 928, loss = 0.08652548\n",
      "Iteration 929, loss = 0.08643196\n",
      "Iteration 930, loss = 0.08634012\n",
      "Iteration 931, loss = 0.08624866\n",
      "Iteration 932, loss = 0.08615848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90632821\n",
      "Iteration 2, loss = 1.86268893\n",
      "Iteration 3, loss = 1.82009983\n",
      "Iteration 4, loss = 1.77857748\n",
      "Iteration 5, loss = 1.73817155\n",
      "Iteration 6, loss = 1.69870850\n",
      "Iteration 7, loss = 1.66007635\n",
      "Iteration 8, loss = 1.62229402\n",
      "Iteration 9, loss = 1.58525955\n",
      "Iteration 10, loss = 1.54896749\n",
      "Iteration 11, loss = 1.51348519\n",
      "Iteration 12, loss = 1.47881304\n",
      "Iteration 13, loss = 1.44495592\n",
      "Iteration 14, loss = 1.41194128\n",
      "Iteration 15, loss = 1.37986711\n",
      "Iteration 16, loss = 1.34868867\n",
      "Iteration 17, loss = 1.31845986\n",
      "Iteration 18, loss = 1.28927478\n",
      "Iteration 19, loss = 1.26118443\n",
      "Iteration 20, loss = 1.23422756\n",
      "Iteration 21, loss = 1.20850335\n",
      "Iteration 22, loss = 1.18403321\n",
      "Iteration 23, loss = 1.16087069\n",
      "Iteration 24, loss = 1.13909160\n",
      "Iteration 25, loss = 1.11870063\n",
      "Iteration 26, loss = 1.09970191\n",
      "Iteration 27, loss = 1.08208445\n",
      "Iteration 28, loss = 1.06582252\n",
      "Iteration 29, loss = 1.05086589\n",
      "Iteration 30, loss = 1.03714558\n",
      "Iteration 31, loss = 1.02458091\n",
      "Iteration 32, loss = 1.01308393\n",
      "Iteration 33, loss = 1.00253346\n",
      "Iteration 34, loss = 0.99277924\n",
      "Iteration 35, loss = 0.98369793\n",
      "Iteration 36, loss = 0.97516377\n",
      "Iteration 37, loss = 0.96704699\n",
      "Iteration 38, loss = 0.95925234\n",
      "Iteration 39, loss = 0.95163579\n",
      "Iteration 40, loss = 0.94410727\n",
      "Iteration 41, loss = 0.93657847\n",
      "Iteration 42, loss = 0.92900779\n",
      "Iteration 43, loss = 0.92140475\n",
      "Iteration 44, loss = 0.91380368\n",
      "Iteration 45, loss = 0.90623561\n",
      "Iteration 46, loss = 0.89868240\n",
      "Iteration 47, loss = 0.89119948\n",
      "Iteration 48, loss = 0.88380127\n",
      "Iteration 49, loss = 0.87644825\n",
      "Iteration 50, loss = 0.86914982\n",
      "Iteration 51, loss = 0.86184892\n",
      "Iteration 52, loss = 0.85456740\n",
      "Iteration 53, loss = 0.84731311\n",
      "Iteration 54, loss = 0.84009087\n",
      "Iteration 55, loss = 0.83295373\n",
      "Iteration 56, loss = 0.82591031\n",
      "Iteration 57, loss = 0.81900035\n",
      "Iteration 58, loss = 0.81226083\n",
      "Iteration 59, loss = 0.80571827\n",
      "Iteration 60, loss = 0.79936963\n",
      "Iteration 61, loss = 0.79324942\n",
      "Iteration 62, loss = 0.78734730\n",
      "Iteration 63, loss = 0.78173048\n",
      "Iteration 64, loss = 0.77630439\n",
      "Iteration 65, loss = 0.77107826\n",
      "Iteration 66, loss = 0.76603554\n",
      "Iteration 67, loss = 0.76117148\n",
      "Iteration 68, loss = 0.75641359\n",
      "Iteration 69, loss = 0.75174223\n",
      "Iteration 70, loss = 0.74715493\n",
      "Iteration 71, loss = 0.74261886\n",
      "Iteration 72, loss = 0.73810508\n",
      "Iteration 73, loss = 0.73361604\n",
      "Iteration 74, loss = 0.72914101\n",
      "Iteration 75, loss = 0.72468199\n",
      "Iteration 76, loss = 0.72025085\n",
      "Iteration 77, loss = 0.71584872\n",
      "Iteration 78, loss = 0.71147773\n",
      "Iteration 79, loss = 0.70714781\n",
      "Iteration 80, loss = 0.70285139\n",
      "Iteration 81, loss = 0.69858921\n",
      "Iteration 82, loss = 0.69434827\n",
      "Iteration 83, loss = 0.69013792\n",
      "Iteration 84, loss = 0.68596919\n",
      "Iteration 85, loss = 0.68184292\n",
      "Iteration 86, loss = 0.67773857\n",
      "Iteration 87, loss = 0.67367134\n",
      "Iteration 88, loss = 0.66964201\n",
      "Iteration 89, loss = 0.66565939\n",
      "Iteration 90, loss = 0.66173772\n",
      "Iteration 91, loss = 0.65785896\n",
      "Iteration 92, loss = 0.65401512\n",
      "Iteration 93, loss = 0.65021336\n",
      "Iteration 94, loss = 0.64645095\n",
      "Iteration 95, loss = 0.64272742\n",
      "Iteration 96, loss = 0.63903762\n",
      "Iteration 97, loss = 0.63537844\n",
      "Iteration 98, loss = 0.63175768\n",
      "Iteration 99, loss = 0.62817976\n",
      "Iteration 100, loss = 0.62464050\n",
      "Iteration 101, loss = 0.62113823\n",
      "Iteration 102, loss = 0.61767797\n",
      "Iteration 103, loss = 0.61424061\n",
      "Iteration 104, loss = 0.61083433\n",
      "Iteration 105, loss = 0.60745891\n",
      "Iteration 106, loss = 0.60412089\n",
      "Iteration 107, loss = 0.60082894\n",
      "Iteration 108, loss = 0.59757780\n",
      "Iteration 109, loss = 0.59436442\n",
      "Iteration 110, loss = 0.59118411\n",
      "Iteration 111, loss = 0.58805482\n",
      "Iteration 112, loss = 0.58499033\n",
      "Iteration 113, loss = 0.58196451\n",
      "Iteration 114, loss = 0.57898087\n",
      "Iteration 115, loss = 0.57604834\n",
      "Iteration 116, loss = 0.57315389\n",
      "Iteration 117, loss = 0.57028890\n",
      "Iteration 118, loss = 0.56746791\n",
      "Iteration 119, loss = 0.56467192\n",
      "Iteration 120, loss = 0.56190272\n",
      "Iteration 121, loss = 0.55916082\n",
      "Iteration 122, loss = 0.55644891\n",
      "Iteration 123, loss = 0.55376483\n",
      "Iteration 124, loss = 0.55111867\n",
      "Iteration 125, loss = 0.54850243\n",
      "Iteration 126, loss = 0.54591118\n",
      "Iteration 127, loss = 0.54334604\n",
      "Iteration 128, loss = 0.54080866\n",
      "Iteration 129, loss = 0.53831432\n",
      "Iteration 130, loss = 0.53585035\n",
      "Iteration 131, loss = 0.53340687\n",
      "Iteration 132, loss = 0.53098515\n",
      "Iteration 133, loss = 0.52858655\n",
      "Iteration 134, loss = 0.52621749\n",
      "Iteration 135, loss = 0.52387493\n",
      "Iteration 136, loss = 0.52156171\n",
      "Iteration 137, loss = 0.51927118\n",
      "Iteration 138, loss = 0.51700298\n",
      "Iteration 139, loss = 0.51475631\n",
      "Iteration 140, loss = 0.51253124\n",
      "Iteration 141, loss = 0.51032717\n",
      "Iteration 142, loss = 0.50814406\n",
      "Iteration 143, loss = 0.50598156\n",
      "Iteration 144, loss = 0.50383958\n",
      "Iteration 145, loss = 0.50171802\n",
      "Iteration 146, loss = 0.49961634\n",
      "Iteration 147, loss = 0.49753481\n",
      "Iteration 148, loss = 0.49547415\n",
      "Iteration 149, loss = 0.49343308\n",
      "Iteration 150, loss = 0.49141014\n",
      "Iteration 151, loss = 0.48940519\n",
      "Iteration 152, loss = 0.48741786\n",
      "Iteration 153, loss = 0.48544782\n",
      "Iteration 154, loss = 0.48349498\n",
      "Iteration 155, loss = 0.48155860\n",
      "Iteration 156, loss = 0.47963858\n",
      "Iteration 157, loss = 0.47773602\n",
      "Iteration 158, loss = 0.47585124\n",
      "Iteration 159, loss = 0.47398221\n",
      "Iteration 160, loss = 0.47212810\n",
      "Iteration 161, loss = 0.47028870\n",
      "Iteration 162, loss = 0.46846385\n",
      "Iteration 163, loss = 0.46665330\n",
      "Iteration 164, loss = 0.46485678\n",
      "Iteration 165, loss = 0.46307371\n",
      "Iteration 166, loss = 0.46130418\n",
      "Iteration 167, loss = 0.45954796\n",
      "Iteration 168, loss = 0.45780459\n",
      "Iteration 169, loss = 0.45607379\n",
      "Iteration 170, loss = 0.45435633\n",
      "Iteration 171, loss = 0.45265153\n",
      "Iteration 172, loss = 0.45095901\n",
      "Iteration 173, loss = 0.44927851\n",
      "Iteration 174, loss = 0.44760982\n",
      "Iteration 175, loss = 0.44595270\n",
      "Iteration 176, loss = 0.44430841\n",
      "Iteration 177, loss = 0.44267591\n",
      "Iteration 178, loss = 0.44105445\n",
      "Iteration 179, loss = 0.43944504\n",
      "Iteration 180, loss = 0.43784581\n",
      "Iteration 181, loss = 0.43625675\n",
      "Iteration 182, loss = 0.43467786\n",
      "Iteration 183, loss = 0.43310877\n",
      "Iteration 184, loss = 0.43154939\n",
      "Iteration 185, loss = 0.42999994\n",
      "Iteration 186, loss = 0.42846089\n",
      "Iteration 187, loss = 0.42693158\n",
      "Iteration 188, loss = 0.42541164\n",
      "Iteration 189, loss = 0.42390085\n",
      "Iteration 190, loss = 0.42239857\n",
      "Iteration 191, loss = 0.42090511\n",
      "Iteration 192, loss = 0.41942019\n",
      "Iteration 193, loss = 0.41794365\n",
      "Iteration 194, loss = 0.41647535\n",
      "Iteration 195, loss = 0.41501526\n",
      "Iteration 196, loss = 0.41356500\n",
      "Iteration 197, loss = 0.41212284\n",
      "Iteration 198, loss = 0.41068874\n",
      "Iteration 199, loss = 0.40926307\n",
      "Iteration 200, loss = 0.40784496\n",
      "Iteration 201, loss = 0.40643423\n",
      "Iteration 202, loss = 0.40503147\n",
      "Iteration 203, loss = 0.40363618\n",
      "Iteration 204, loss = 0.40224827\n",
      "Iteration 205, loss = 0.40086776\n",
      "Iteration 206, loss = 0.39949442\n",
      "Iteration 207, loss = 0.39812806\n",
      "Iteration 208, loss = 0.39676863\n",
      "Iteration 209, loss = 0.39541610\n",
      "Iteration 210, loss = 0.39407034\n",
      "Iteration 211, loss = 0.39273127\n",
      "Iteration 212, loss = 0.39139881\n",
      "Iteration 213, loss = 0.39007291\n",
      "Iteration 214, loss = 0.38875347\n",
      "Iteration 215, loss = 0.38744047\n",
      "Iteration 216, loss = 0.38613386\n",
      "Iteration 217, loss = 0.38483343\n",
      "Iteration 218, loss = 0.38353915\n",
      "Iteration 219, loss = 0.38225109\n",
      "Iteration 220, loss = 0.38096901\n",
      "Iteration 221, loss = 0.37969286\n",
      "Iteration 222, loss = 0.37842261\n",
      "Iteration 223, loss = 0.37715807\n",
      "Iteration 224, loss = 0.37589924\n",
      "Iteration 225, loss = 0.37464614\n",
      "Iteration 226, loss = 0.37339864\n",
      "Iteration 227, loss = 0.37215663\n",
      "Iteration 228, loss = 0.37091973\n",
      "Iteration 229, loss = 0.36968811\n",
      "Iteration 230, loss = 0.36846184\n",
      "Iteration 231, loss = 0.36724082\n",
      "Iteration 232, loss = 0.36602497\n",
      "Iteration 233, loss = 0.36481421\n",
      "Iteration 234, loss = 0.36360848\n",
      "Iteration 235, loss = 0.36240774\n",
      "Iteration 236, loss = 0.36121193\n",
      "Iteration 237, loss = 0.36002099\n",
      "Iteration 238, loss = 0.35883489\n",
      "Iteration 239, loss = 0.35765363\n",
      "Iteration 240, loss = 0.35647707\n",
      "Iteration 241, loss = 0.35530516\n",
      "Iteration 242, loss = 0.35413790\n",
      "Iteration 243, loss = 0.35297543\n",
      "Iteration 244, loss = 0.35181751\n",
      "Iteration 245, loss = 0.35066411\n",
      "Iteration 246, loss = 0.34951518\n",
      "Iteration 247, loss = 0.34837067\n",
      "Iteration 248, loss = 0.34723056\n",
      "Iteration 249, loss = 0.34609483\n",
      "Iteration 250, loss = 0.34496359\n",
      "Iteration 251, loss = 0.34383665\n",
      "Iteration 252, loss = 0.34271403\n",
      "Iteration 253, loss = 0.34159571\n",
      "Iteration 254, loss = 0.34048157\n",
      "Iteration 255, loss = 0.33937159\n",
      "Iteration 256, loss = 0.33826574\n",
      "Iteration 257, loss = 0.33716472\n",
      "Iteration 258, loss = 0.33606869\n",
      "Iteration 259, loss = 0.33497660\n",
      "Iteration 260, loss = 0.33388848\n",
      "Iteration 261, loss = 0.33280430\n",
      "Iteration 262, loss = 0.33172405\n",
      "Iteration 263, loss = 0.33064798\n",
      "Iteration 264, loss = 0.32957721\n",
      "Iteration 265, loss = 0.32851101\n",
      "Iteration 266, loss = 0.32744906\n",
      "Iteration 267, loss = 0.32639075\n",
      "Iteration 268, loss = 0.32533599\n",
      "Iteration 269, loss = 0.32428484\n",
      "Iteration 270, loss = 0.32323742\n",
      "Iteration 271, loss = 0.32219362\n",
      "Iteration 272, loss = 0.32115394\n",
      "Iteration 273, loss = 0.32011831\n",
      "Iteration 274, loss = 0.31908653\n",
      "Iteration 275, loss = 0.31805855\n",
      "Iteration 276, loss = 0.31703436\n",
      "Iteration 277, loss = 0.31601443\n",
      "Iteration 278, loss = 0.31499823\n",
      "Iteration 279, loss = 0.31398585\n",
      "Iteration 280, loss = 0.31297693\n",
      "Iteration 281, loss = 0.31197165\n",
      "Iteration 282, loss = 0.31097024\n",
      "Iteration 283, loss = 0.30997263\n",
      "Iteration 284, loss = 0.30897842\n",
      "Iteration 285, loss = 0.30798802\n",
      "Iteration 286, loss = 0.30700119\n",
      "Iteration 287, loss = 0.30601806\n",
      "Iteration 288, loss = 0.30503853\n",
      "Iteration 289, loss = 0.30406249\n",
      "Iteration 290, loss = 0.30308996\n",
      "Iteration 291, loss = 0.30212091\n",
      "Iteration 292, loss = 0.30115540\n",
      "Iteration 293, loss = 0.30019342\n",
      "Iteration 294, loss = 0.29923506\n",
      "Iteration 295, loss = 0.29828023\n",
      "Iteration 296, loss = 0.29732883\n",
      "Iteration 297, loss = 0.29638085\n",
      "Iteration 298, loss = 0.29543626\n",
      "Iteration 299, loss = 0.29449531\n",
      "Iteration 300, loss = 0.29355782\n",
      "Iteration 301, loss = 0.29262361\n",
      "Iteration 302, loss = 0.29169264\n",
      "Iteration 303, loss = 0.29076526\n",
      "Iteration 304, loss = 0.28984127\n",
      "Iteration 305, loss = 0.28892063\n",
      "Iteration 306, loss = 0.28800332\n",
      "Iteration 307, loss = 0.28708942\n",
      "Iteration 308, loss = 0.28617881\n",
      "Iteration 309, loss = 0.28527179\n",
      "Iteration 310, loss = 0.28436814\n",
      "Iteration 311, loss = 0.28346772\n",
      "Iteration 312, loss = 0.28257054\n",
      "Iteration 313, loss = 0.28167674\n",
      "Iteration 314, loss = 0.28078625\n",
      "Iteration 315, loss = 0.27989895\n",
      "Iteration 316, loss = 0.27901495\n",
      "Iteration 317, loss = 0.27813421\n",
      "Iteration 318, loss = 0.27725668\n",
      "Iteration 319, loss = 0.27638236\n",
      "Iteration 320, loss = 0.27551129\n",
      "Iteration 321, loss = 0.27464346\n",
      "Iteration 322, loss = 0.27377891\n",
      "Iteration 323, loss = 0.27291751\n",
      "Iteration 324, loss = 0.27205926\n",
      "Iteration 325, loss = 0.27120426\n",
      "Iteration 326, loss = 0.27035239\n",
      "Iteration 327, loss = 0.26950372\n",
      "Iteration 328, loss = 0.26865832\n",
      "Iteration 329, loss = 0.26781606\n",
      "Iteration 330, loss = 0.26697694\n",
      "Iteration 331, loss = 0.26614093\n",
      "Iteration 332, loss = 0.26530808\n",
      "Iteration 333, loss = 0.26447834\n",
      "Iteration 334, loss = 0.26365169\n",
      "Iteration 335, loss = 0.26282817\n",
      "Iteration 336, loss = 0.26200773\n",
      "Iteration 337, loss = 0.26119037\n",
      "Iteration 338, loss = 0.26037610\n",
      "Iteration 339, loss = 0.25956491\n",
      "Iteration 340, loss = 0.25875679\n",
      "Iteration 341, loss = 0.25795174\n",
      "Iteration 342, loss = 0.25714971\n",
      "Iteration 343, loss = 0.25635072\n",
      "Iteration 344, loss = 0.25555477\n",
      "Iteration 345, loss = 0.25476186\n",
      "Iteration 346, loss = 0.25397199\n",
      "Iteration 347, loss = 0.25318508\n",
      "Iteration 348, loss = 0.25240128\n",
      "Iteration 349, loss = 0.25162047\n",
      "Iteration 350, loss = 0.25084266\n",
      "Iteration 351, loss = 0.25006783\n",
      "Iteration 352, loss = 0.24929598\n",
      "Iteration 353, loss = 0.24852711\n",
      "Iteration 354, loss = 0.24776119\n",
      "Iteration 355, loss = 0.24699825\n",
      "Iteration 356, loss = 0.24623825\n",
      "Iteration 357, loss = 0.24548118\n",
      "Iteration 358, loss = 0.24472781\n",
      "Iteration 359, loss = 0.24397638\n",
      "Iteration 360, loss = 0.24322738\n",
      "Iteration 361, loss = 0.24248202\n",
      "Iteration 362, loss = 0.24173918\n",
      "Iteration 363, loss = 0.24100010\n",
      "Iteration 364, loss = 0.24026334\n",
      "Iteration 365, loss = 0.23952894\n",
      "Iteration 366, loss = 0.23879811\n",
      "Iteration 367, loss = 0.23807022\n",
      "Iteration 368, loss = 0.23734488\n",
      "Iteration 369, loss = 0.23662216\n",
      "Iteration 370, loss = 0.23590205\n",
      "Iteration 371, loss = 0.23518596\n",
      "Iteration 372, loss = 0.23447225\n",
      "Iteration 373, loss = 0.23376113\n",
      "Iteration 374, loss = 0.23305276\n",
      "Iteration 375, loss = 0.23234579\n",
      "Iteration 376, loss = 0.23163933\n",
      "Iteration 377, loss = 0.23093418\n",
      "Iteration 378, loss = 0.23022797\n",
      "Iteration 379, loss = 0.22951930\n",
      "Iteration 380, loss = 0.22881003\n",
      "Iteration 381, loss = 0.22809889\n",
      "Iteration 382, loss = 0.22738834\n",
      "Iteration 383, loss = 0.22667751\n",
      "Iteration 384, loss = 0.22596442\n",
      "Iteration 385, loss = 0.22525222\n",
      "Iteration 386, loss = 0.22453966\n",
      "Iteration 387, loss = 0.22382601\n",
      "Iteration 388, loss = 0.22311075\n",
      "Iteration 389, loss = 0.22239382\n",
      "Iteration 390, loss = 0.22167606\n",
      "Iteration 391, loss = 0.22095843\n",
      "Iteration 392, loss = 0.22023833\n",
      "Iteration 393, loss = 0.21951945\n",
      "Iteration 394, loss = 0.21880072\n",
      "Iteration 395, loss = 0.21807904\n",
      "Iteration 396, loss = 0.21735934\n",
      "Iteration 397, loss = 0.21663921\n",
      "Iteration 398, loss = 0.21591871\n",
      "Iteration 399, loss = 0.21519891\n",
      "Iteration 400, loss = 0.21447931\n",
      "Iteration 401, loss = 0.21375924\n",
      "Iteration 402, loss = 0.21303975\n",
      "Iteration 403, loss = 0.21232069\n",
      "Iteration 404, loss = 0.21160281\n",
      "Iteration 405, loss = 0.21088488\n",
      "Iteration 406, loss = 0.21016772\n",
      "Iteration 407, loss = 0.20945202\n",
      "Iteration 408, loss = 0.20873692\n",
      "Iteration 409, loss = 0.20802247\n",
      "Iteration 410, loss = 0.20730931\n",
      "Iteration 411, loss = 0.20659766\n",
      "Iteration 412, loss = 0.20588741\n",
      "Iteration 413, loss = 0.20517870\n",
      "Iteration 414, loss = 0.20447179\n",
      "Iteration 415, loss = 0.20376636\n",
      "Iteration 416, loss = 0.20306294\n",
      "Iteration 417, loss = 0.20236147\n",
      "Iteration 418, loss = 0.20166204\n",
      "Iteration 419, loss = 0.20096473\n",
      "Iteration 420, loss = 0.20026964\n",
      "Iteration 421, loss = 0.19957682\n",
      "Iteration 422, loss = 0.19888634\n",
      "Iteration 423, loss = 0.19819828\n",
      "Iteration 424, loss = 0.19751270\n",
      "Iteration 425, loss = 0.19682966\n",
      "Iteration 426, loss = 0.19614921\n",
      "Iteration 427, loss = 0.19547179\n",
      "Iteration 428, loss = 0.19479671\n",
      "Iteration 429, loss = 0.19412459\n",
      "Iteration 430, loss = 0.19345519\n",
      "Iteration 431, loss = 0.19278859\n",
      "Iteration 432, loss = 0.19212503\n",
      "Iteration 433, loss = 0.19146441\n",
      "Iteration 434, loss = 0.19080654\n",
      "Iteration 435, loss = 0.19015163\n",
      "Iteration 436, loss = 0.18949983\n",
      "Iteration 437, loss = 0.18885111\n",
      "Iteration 438, loss = 0.18820553\n",
      "Iteration 439, loss = 0.18756308\n",
      "Iteration 440, loss = 0.18692382\n",
      "Iteration 441, loss = 0.18628773\n",
      "Iteration 442, loss = 0.18565488\n",
      "Iteration 443, loss = 0.18502533\n",
      "Iteration 444, loss = 0.18439885\n",
      "Iteration 445, loss = 0.18377569\n",
      "Iteration 446, loss = 0.18315580\n",
      "Iteration 447, loss = 0.18253924\n",
      "Iteration 448, loss = 0.18192580\n",
      "Iteration 449, loss = 0.18131560\n",
      "Iteration 450, loss = 0.18070868\n",
      "Iteration 451, loss = 0.18010504\n",
      "Iteration 452, loss = 0.17950467\n",
      "Iteration 453, loss = 0.17890759\n",
      "Iteration 454, loss = 0.17831376\n",
      "Iteration 455, loss = 0.17772325\n",
      "Iteration 456, loss = 0.17713586\n",
      "Iteration 457, loss = 0.17655183\n",
      "Iteration 458, loss = 0.17597108\n",
      "Iteration 459, loss = 0.17539359\n",
      "Iteration 460, loss = 0.17481936\n",
      "Iteration 461, loss = 0.17424838\n",
      "Iteration 462, loss = 0.17368070\n",
      "Iteration 463, loss = 0.17311626\n",
      "Iteration 464, loss = 0.17255502\n",
      "Iteration 465, loss = 0.17199702\n",
      "Iteration 466, loss = 0.17144236\n",
      "Iteration 467, loss = 0.17089122\n",
      "Iteration 468, loss = 0.17034328\n",
      "Iteration 469, loss = 0.16979858\n",
      "Iteration 470, loss = 0.16925709\n",
      "Iteration 471, loss = 0.16871873\n",
      "Iteration 472, loss = 0.16818408\n",
      "Iteration 473, loss = 0.16765215\n",
      "Iteration 474, loss = 0.16712333\n",
      "Iteration 475, loss = 0.16659764\n",
      "Iteration 476, loss = 0.16607513\n",
      "Iteration 477, loss = 0.16555572\n",
      "Iteration 478, loss = 0.16503928\n",
      "Iteration 479, loss = 0.16452572\n",
      "Iteration 480, loss = 0.16401502\n",
      "Iteration 481, loss = 0.16350731\n",
      "Iteration 482, loss = 0.16300280\n",
      "Iteration 483, loss = 0.16250101\n",
      "Iteration 484, loss = 0.16200195\n",
      "Iteration 485, loss = 0.16150555\n",
      "Iteration 486, loss = 0.16101190\n",
      "Iteration 487, loss = 0.16052082\n",
      "Iteration 488, loss = 0.16003229\n",
      "Iteration 489, loss = 0.15954714\n",
      "Iteration 490, loss = 0.15906503\n",
      "Iteration 491, loss = 0.15858576\n",
      "Iteration 492, loss = 0.15810944\n",
      "Iteration 493, loss = 0.15763589\n",
      "Iteration 494, loss = 0.15716512\n",
      "Iteration 495, loss = 0.15669716\n",
      "Iteration 496, loss = 0.15623172\n",
      "Iteration 497, loss = 0.15576898\n",
      "Iteration 498, loss = 0.15530877\n",
      "Iteration 499, loss = 0.15485056\n",
      "Iteration 500, loss = 0.15439459\n",
      "Iteration 501, loss = 0.15394099\n",
      "Iteration 502, loss = 0.15348986\n",
      "Iteration 503, loss = 0.15304123\n",
      "Iteration 504, loss = 0.15259505\n",
      "Iteration 505, loss = 0.15215134\n",
      "Iteration 506, loss = 0.15171042\n",
      "Iteration 507, loss = 0.15127207\n",
      "Iteration 508, loss = 0.15083634\n",
      "Iteration 509, loss = 0.15040315\n",
      "Iteration 510, loss = 0.14997253\n",
      "Iteration 511, loss = 0.14954445\n",
      "Iteration 512, loss = 0.14911868\n",
      "Iteration 513, loss = 0.14869520\n",
      "Iteration 514, loss = 0.14827396\n",
      "Iteration 515, loss = 0.14785518\n",
      "Iteration 516, loss = 0.14743889\n",
      "Iteration 517, loss = 0.14702563\n",
      "Iteration 518, loss = 0.14661510\n",
      "Iteration 519, loss = 0.14620681\n",
      "Iteration 520, loss = 0.14580052\n",
      "Iteration 521, loss = 0.14539660\n",
      "Iteration 522, loss = 0.14499505\n",
      "Iteration 523, loss = 0.14459608\n",
      "Iteration 524, loss = 0.14419982\n",
      "Iteration 525, loss = 0.14380597\n",
      "Iteration 526, loss = 0.14341444\n",
      "Iteration 527, loss = 0.14302519\n",
      "Iteration 528, loss = 0.14263808\n",
      "Iteration 529, loss = 0.14225332\n",
      "Iteration 530, loss = 0.14187087\n",
      "Iteration 531, loss = 0.14149015\n",
      "Iteration 532, loss = 0.14111163\n",
      "Iteration 533, loss = 0.14073552\n",
      "Iteration 534, loss = 0.14036167\n",
      "Iteration 535, loss = 0.13999010\n",
      "Iteration 536, loss = 0.13962062\n",
      "Iteration 537, loss = 0.13925336\n",
      "Iteration 538, loss = 0.13888822\n",
      "Iteration 539, loss = 0.13852475\n",
      "Iteration 540, loss = 0.13816403\n",
      "Iteration 541, loss = 0.13780656\n",
      "Iteration 542, loss = 0.13745153\n",
      "Iteration 543, loss = 0.13709873\n",
      "Iteration 544, loss = 0.13674793\n",
      "Iteration 545, loss = 0.13639929\n",
      "Iteration 546, loss = 0.13605257\n",
      "Iteration 547, loss = 0.13570821\n",
      "Iteration 548, loss = 0.13536585\n",
      "Iteration 549, loss = 0.13502537\n",
      "Iteration 550, loss = 0.13468687\n",
      "Iteration 551, loss = 0.13435031\n",
      "Iteration 552, loss = 0.13401584\n",
      "Iteration 553, loss = 0.13368353\n",
      "Iteration 554, loss = 0.13335319\n",
      "Iteration 555, loss = 0.13302452\n",
      "Iteration 556, loss = 0.13269773\n",
      "Iteration 557, loss = 0.13237293\n",
      "Iteration 558, loss = 0.13204999\n",
      "Iteration 559, loss = 0.13172943\n",
      "Iteration 560, loss = 0.13141081\n",
      "Iteration 561, loss = 0.13109354\n",
      "Iteration 562, loss = 0.13077808\n",
      "Iteration 563, loss = 0.13046458\n",
      "Iteration 564, loss = 0.13015277\n",
      "Iteration 565, loss = 0.12984262\n",
      "Iteration 566, loss = 0.12953420\n",
      "Iteration 567, loss = 0.12922757\n",
      "Iteration 568, loss = 0.12892260\n",
      "Iteration 569, loss = 0.12861929\n",
      "Iteration 570, loss = 0.12831767\n",
      "Iteration 571, loss = 0.12801790\n",
      "Iteration 572, loss = 0.12771976\n",
      "Iteration 573, loss = 0.12742343\n",
      "Iteration 574, loss = 0.12712856\n",
      "Iteration 575, loss = 0.12683522\n",
      "Iteration 576, loss = 0.12654356\n",
      "Iteration 577, loss = 0.12625327\n",
      "Iteration 578, loss = 0.12596466\n",
      "Iteration 579, loss = 0.12567769\n",
      "Iteration 580, loss = 0.12539243\n",
      "Iteration 581, loss = 0.12510862\n",
      "Iteration 582, loss = 0.12482638\n",
      "Iteration 583, loss = 0.12454566\n",
      "Iteration 584, loss = 0.12426663\n",
      "Iteration 585, loss = 0.12398906\n",
      "Iteration 586, loss = 0.12371292\n",
      "Iteration 587, loss = 0.12343824\n",
      "Iteration 588, loss = 0.12316517\n",
      "Iteration 589, loss = 0.12289362\n",
      "Iteration 590, loss = 0.12262346\n",
      "Iteration 591, loss = 0.12235464\n",
      "Iteration 592, loss = 0.12208731\n",
      "Iteration 593, loss = 0.12182149\n",
      "Iteration 594, loss = 0.12155705\n",
      "Iteration 595, loss = 0.12129400\n",
      "Iteration 596, loss = 0.12103235\n",
      "Iteration 597, loss = 0.12077208\n",
      "Iteration 598, loss = 0.12051320\n",
      "Iteration 599, loss = 0.12025573\n",
      "Iteration 600, loss = 0.11999956\n",
      "Iteration 601, loss = 0.11974477\n",
      "Iteration 602, loss = 0.11949132\n",
      "Iteration 603, loss = 0.11923925\n",
      "Iteration 604, loss = 0.11898853\n",
      "Iteration 605, loss = 0.11873911\n",
      "Iteration 606, loss = 0.11849100\n",
      "Iteration 607, loss = 0.11824418\n",
      "Iteration 608, loss = 0.11799864\n",
      "Iteration 609, loss = 0.11775437\n",
      "Iteration 610, loss = 0.11751137\n",
      "Iteration 611, loss = 0.11726966\n",
      "Iteration 612, loss = 0.11702930\n",
      "Iteration 613, loss = 0.11679019\n",
      "Iteration 614, loss = 0.11655230\n",
      "Iteration 615, loss = 0.11631582\n",
      "Iteration 616, loss = 0.11608029\n",
      "Iteration 617, loss = 0.11584608\n",
      "Iteration 618, loss = 0.11561316\n",
      "Iteration 619, loss = 0.11538142\n",
      "Iteration 620, loss = 0.11515084\n",
      "Iteration 621, loss = 0.11492141\n",
      "Iteration 622, loss = 0.11469310\n",
      "Iteration 623, loss = 0.11446593\n",
      "Iteration 624, loss = 0.11423997\n",
      "Iteration 625, loss = 0.11401527\n",
      "Iteration 626, loss = 0.11379156\n",
      "Iteration 627, loss = 0.11356910\n",
      "Iteration 628, loss = 0.11334779\n",
      "Iteration 629, loss = 0.11312754\n",
      "Iteration 630, loss = 0.11290833\n",
      "Iteration 631, loss = 0.11269034\n",
      "Iteration 632, loss = 0.11247347\n",
      "Iteration 633, loss = 0.11225760\n",
      "Iteration 634, loss = 0.11204276\n",
      "Iteration 635, loss = 0.11182925\n",
      "Iteration 636, loss = 0.11161673\n",
      "Iteration 637, loss = 0.11140513\n",
      "Iteration 638, loss = 0.11119450\n",
      "Iteration 639, loss = 0.11098514\n",
      "Iteration 640, loss = 0.11077703\n",
      "Iteration 641, loss = 0.11056997\n",
      "Iteration 642, loss = 0.11036397\n",
      "Iteration 643, loss = 0.11015907\n",
      "Iteration 644, loss = 0.10995518\n",
      "Iteration 645, loss = 0.10975234\n",
      "Iteration 646, loss = 0.10955049\n",
      "Iteration 647, loss = 0.10934966\n",
      "Iteration 648, loss = 0.10914981\n",
      "Iteration 649, loss = 0.10895095\n",
      "Iteration 650, loss = 0.10875307\n",
      "Iteration 651, loss = 0.10855623\n",
      "Iteration 652, loss = 0.10836033\n",
      "Iteration 653, loss = 0.10816541\n",
      "Iteration 654, loss = 0.10797145\n",
      "Iteration 655, loss = 0.10777843\n",
      "Iteration 656, loss = 0.10758635\n",
      "Iteration 657, loss = 0.10739511\n",
      "Iteration 658, loss = 0.10720475\n",
      "Iteration 659, loss = 0.10701527\n",
      "Iteration 660, loss = 0.10682665\n",
      "Iteration 661, loss = 0.10663890\n",
      "Iteration 662, loss = 0.10645204\n",
      "Iteration 663, loss = 0.10626601\n",
      "Iteration 664, loss = 0.10608075\n",
      "Iteration 665, loss = 0.10589620\n",
      "Iteration 666, loss = 0.10571244\n",
      "Iteration 667, loss = 0.10552922\n",
      "Iteration 668, loss = 0.10534685\n",
      "Iteration 669, loss = 0.10516485\n",
      "Iteration 670, loss = 0.10498359\n",
      "Iteration 671, loss = 0.10480266\n",
      "Iteration 672, loss = 0.10462246\n",
      "Iteration 673, loss = 0.10444227\n",
      "Iteration 674, loss = 0.10426268\n",
      "Iteration 675, loss = 0.10408326\n",
      "Iteration 676, loss = 0.10390403\n",
      "Iteration 677, loss = 0.10372549\n",
      "Iteration 678, loss = 0.10354681\n",
      "Iteration 679, loss = 0.10336859\n",
      "Iteration 680, loss = 0.10319106\n",
      "Iteration 681, loss = 0.10301362\n",
      "Iteration 682, loss = 0.10283674\n",
      "Iteration 683, loss = 0.10265964\n",
      "Iteration 684, loss = 0.10248416\n",
      "Iteration 685, loss = 0.10230788\n",
      "Iteration 686, loss = 0.10213296\n",
      "Iteration 687, loss = 0.10195750\n",
      "Iteration 688, loss = 0.10178243\n",
      "Iteration 689, loss = 0.10160830\n",
      "Iteration 690, loss = 0.10143443\n",
      "Iteration 691, loss = 0.10126053\n",
      "Iteration 692, loss = 0.10108705\n",
      "Iteration 693, loss = 0.10091438\n",
      "Iteration 694, loss = 0.10074155\n",
      "Iteration 695, loss = 0.10056947\n",
      "Iteration 696, loss = 0.10039774\n",
      "Iteration 697, loss = 0.10022628\n",
      "Iteration 698, loss = 0.10005550\n",
      "Iteration 699, loss = 0.09988481\n",
      "Iteration 700, loss = 0.09971474\n",
      "Iteration 701, loss = 0.09954541\n",
      "Iteration 702, loss = 0.09937645\n",
      "Iteration 703, loss = 0.09920822\n",
      "Iteration 704, loss = 0.09904014\n",
      "Iteration 705, loss = 0.09887284\n",
      "Iteration 706, loss = 0.09870604\n",
      "Iteration 707, loss = 0.09853976\n",
      "Iteration 708, loss = 0.09837397\n",
      "Iteration 709, loss = 0.09820933\n",
      "Iteration 710, loss = 0.09804439\n",
      "Iteration 711, loss = 0.09788064\n",
      "Iteration 712, loss = 0.09771748\n",
      "Iteration 713, loss = 0.09755447\n",
      "Iteration 714, loss = 0.09739224\n",
      "Iteration 715, loss = 0.09723076\n",
      "Iteration 716, loss = 0.09707016\n",
      "Iteration 717, loss = 0.09691044\n",
      "Iteration 718, loss = 0.09675125\n",
      "Iteration 719, loss = 0.09659261\n",
      "Iteration 720, loss = 0.09643456\n",
      "Iteration 721, loss = 0.09627739\n",
      "Iteration 722, loss = 0.09612095\n",
      "Iteration 723, loss = 0.09596526\n",
      "Iteration 724, loss = 0.09581029\n",
      "Iteration 725, loss = 0.09565621\n",
      "Iteration 726, loss = 0.09550288\n",
      "Iteration 727, loss = 0.09535036\n",
      "Iteration 728, loss = 0.09519856\n",
      "Iteration 729, loss = 0.09504752\n",
      "Iteration 730, loss = 0.09489715\n",
      "Iteration 731, loss = 0.09474736\n",
      "Iteration 732, loss = 0.09459818\n",
      "Iteration 733, loss = 0.09444938\n",
      "Iteration 734, loss = 0.09430088\n",
      "Iteration 735, loss = 0.09415249\n",
      "Iteration 736, loss = 0.09400403\n",
      "Iteration 737, loss = 0.09385582\n",
      "Iteration 738, loss = 0.09370710\n",
      "Iteration 739, loss = 0.09355824\n",
      "Iteration 740, loss = 0.09340867\n",
      "Iteration 741, loss = 0.09325853\n",
      "Iteration 742, loss = 0.09310769\n",
      "Iteration 743, loss = 0.09295561\n",
      "Iteration 744, loss = 0.09280215\n",
      "Iteration 745, loss = 0.09264720\n",
      "Iteration 746, loss = 0.09249064\n",
      "Iteration 747, loss = 0.09233270\n",
      "Iteration 748, loss = 0.09217309\n",
      "Iteration 749, loss = 0.09201198\n",
      "Iteration 750, loss = 0.09184952\n",
      "Iteration 751, loss = 0.09168549\n",
      "Iteration 752, loss = 0.09151980\n",
      "Iteration 753, loss = 0.09135259\n",
      "Iteration 754, loss = 0.09118349\n",
      "Iteration 755, loss = 0.09101293\n",
      "Iteration 756, loss = 0.09084078\n",
      "Iteration 757, loss = 0.09066707\n",
      "Iteration 758, loss = 0.09049198\n",
      "Iteration 759, loss = 0.09031558\n",
      "Iteration 760, loss = 0.09013748\n",
      "Iteration 761, loss = 0.08995839\n",
      "Iteration 762, loss = 0.08977806\n",
      "Iteration 763, loss = 0.08959672\n",
      "Iteration 764, loss = 0.08941427\n",
      "Iteration 765, loss = 0.08923097\n",
      "Iteration 766, loss = 0.08904673\n",
      "Iteration 767, loss = 0.08886158\n",
      "Iteration 768, loss = 0.08867593\n",
      "Iteration 769, loss = 0.08848949\n",
      "Iteration 770, loss = 0.08830241\n",
      "Iteration 771, loss = 0.08811498\n",
      "Iteration 772, loss = 0.08792697\n",
      "Iteration 773, loss = 0.08773864\n",
      "Iteration 774, loss = 0.08755015\n",
      "Iteration 775, loss = 0.08736137\n",
      "Iteration 776, loss = 0.08717253\n",
      "Iteration 777, loss = 0.08698375\n",
      "Iteration 778, loss = 0.08679471\n",
      "Iteration 779, loss = 0.08660585\n",
      "Iteration 780, loss = 0.08641716\n",
      "Iteration 781, loss = 0.08622859\n",
      "Iteration 782, loss = 0.08604012\n",
      "Iteration 783, loss = 0.08585200\n",
      "Iteration 784, loss = 0.08566422\n",
      "Iteration 785, loss = 0.08547669\n",
      "Iteration 786, loss = 0.08528965\n",
      "Iteration 787, loss = 0.08510321\n",
      "Iteration 788, loss = 0.08491718\n",
      "Iteration 789, loss = 0.08473160\n",
      "Iteration 790, loss = 0.08454658\n",
      "Iteration 791, loss = 0.08436229\n",
      "Iteration 792, loss = 0.08417868\n",
      "Iteration 793, loss = 0.08399573\n",
      "Iteration 794, loss = 0.08381352\n",
      "Iteration 795, loss = 0.08363219\n",
      "Iteration 796, loss = 0.08345155\n",
      "Iteration 797, loss = 0.08327162\n",
      "Iteration 798, loss = 0.08309263\n",
      "Iteration 799, loss = 0.08291452\n",
      "Iteration 800, loss = 0.08273730\n",
      "Iteration 801, loss = 0.08256098\n",
      "Iteration 802, loss = 0.08238558\n",
      "Iteration 803, loss = 0.08221113\n",
      "Iteration 804, loss = 0.08203765\n",
      "Iteration 805, loss = 0.08186515\n",
      "Iteration 806, loss = 0.08169365\n",
      "Iteration 807, loss = 0.08152317\n",
      "Iteration 808, loss = 0.08135371\n",
      "Iteration 809, loss = 0.08118530\n",
      "Iteration 810, loss = 0.08101794\n",
      "Iteration 811, loss = 0.08085165\n",
      "Iteration 812, loss = 0.08068642\n",
      "Iteration 813, loss = 0.08052229\n",
      "Iteration 814, loss = 0.08035924\n",
      "Iteration 815, loss = 0.08019729\n",
      "Iteration 816, loss = 0.08003646\n",
      "Iteration 817, loss = 0.07987670\n",
      "Iteration 818, loss = 0.07971808\n",
      "Iteration 819, loss = 0.07956060\n",
      "Iteration 820, loss = 0.07940424\n",
      "Iteration 821, loss = 0.07924901\n",
      "Iteration 822, loss = 0.07909489\n",
      "Iteration 823, loss = 0.07894190\n",
      "Iteration 824, loss = 0.07879005\n",
      "Iteration 825, loss = 0.07863932\n",
      "Iteration 826, loss = 0.07848972\n",
      "Iteration 827, loss = 0.07834126\n",
      "Iteration 828, loss = 0.07819395\n",
      "Iteration 829, loss = 0.07804771\n",
      "Iteration 830, loss = 0.07790264\n",
      "Iteration 831, loss = 0.07775869\n",
      "Iteration 832, loss = 0.07761585\n",
      "Iteration 833, loss = 0.07747413\n",
      "Iteration 834, loss = 0.07733353\n",
      "Iteration 835, loss = 0.07719406\n",
      "Iteration 836, loss = 0.07705568\n",
      "Iteration 837, loss = 0.07691838\n",
      "Iteration 838, loss = 0.07678218\n",
      "Iteration 839, loss = 0.07664710\n",
      "Iteration 840, loss = 0.07651312\n",
      "Iteration 841, loss = 0.07638022\n",
      "Iteration 842, loss = 0.07624839\n",
      "Iteration 843, loss = 0.07611763\n",
      "Iteration 844, loss = 0.07598792\n",
      "Iteration 845, loss = 0.07585929\n",
      "Iteration 846, loss = 0.07573173\n",
      "Iteration 847, loss = 0.07560521\n",
      "Iteration 848, loss = 0.07547973\n",
      "Iteration 849, loss = 0.07535530\n",
      "Iteration 850, loss = 0.07523191\n",
      "Iteration 851, loss = 0.07510955\n",
      "Iteration 852, loss = 0.07498820\n",
      "Iteration 853, loss = 0.07486785\n",
      "Iteration 854, loss = 0.07474855\n",
      "Iteration 855, loss = 0.07463018\n",
      "Iteration 856, loss = 0.07451293\n",
      "Iteration 857, loss = 0.07439682\n",
      "Iteration 858, loss = 0.07428164\n",
      "Iteration 859, loss = 0.07416747\n",
      "Iteration 860, loss = 0.07405423\n",
      "Iteration 861, loss = 0.07394190\n",
      "Iteration 862, loss = 0.07383048\n",
      "Iteration 863, loss = 0.07371997\n",
      "Iteration 864, loss = 0.07361037\n",
      "Iteration 865, loss = 0.07350167\n",
      "Iteration 866, loss = 0.07339387\n",
      "Iteration 867, loss = 0.07328697\n",
      "Iteration 868, loss = 0.07318094\n",
      "Iteration 869, loss = 0.07307579\n",
      "Iteration 870, loss = 0.07297151\n",
      "Iteration 871, loss = 0.07286817\n",
      "Iteration 872, loss = 0.07276568\n",
      "Iteration 873, loss = 0.07266406\n",
      "Iteration 874, loss = 0.07256341\n",
      "Iteration 875, loss = 0.07246371\n",
      "Iteration 876, loss = 0.07236478\n",
      "Iteration 877, loss = 0.07226664\n",
      "Iteration 878, loss = 0.07216923\n",
      "Iteration 879, loss = 0.07207259\n",
      "Iteration 880, loss = 0.07197674\n",
      "Iteration 881, loss = 0.07188177\n",
      "Iteration 882, loss = 0.07178764\n",
      "Iteration 883, loss = 0.07169411\n",
      "Iteration 884, loss = 0.07160167\n",
      "Iteration 885, loss = 0.07150977\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92980133\n",
      "Iteration 2, loss = 1.88558228\n",
      "Iteration 3, loss = 1.84243492\n",
      "Iteration 4, loss = 1.80036064\n",
      "Iteration 5, loss = 1.75937657\n",
      "Iteration 6, loss = 1.71927825\n",
      "Iteration 7, loss = 1.67999812\n",
      "Iteration 8, loss = 1.64159663\n",
      "Iteration 9, loss = 1.60399129\n",
      "Iteration 10, loss = 1.56711953\n",
      "Iteration 11, loss = 1.53104079\n",
      "Iteration 12, loss = 1.49577670\n",
      "Iteration 13, loss = 1.46134378\n",
      "Iteration 14, loss = 1.42777597\n",
      "Iteration 15, loss = 1.39513979\n",
      "Iteration 16, loss = 1.36342709\n",
      "Iteration 17, loss = 1.33269263\n",
      "Iteration 18, loss = 1.30298592\n",
      "Iteration 19, loss = 1.27440243\n",
      "Iteration 20, loss = 1.24697323\n",
      "Iteration 21, loss = 1.22077702\n",
      "Iteration 22, loss = 1.19587045\n",
      "Iteration 23, loss = 1.17231085\n",
      "Iteration 24, loss = 1.15015159\n",
      "Iteration 25, loss = 1.12937200\n",
      "Iteration 26, loss = 1.11000807\n",
      "Iteration 27, loss = 1.09204854\n",
      "Iteration 28, loss = 1.07547064\n",
      "Iteration 29, loss = 1.06023333\n",
      "Iteration 30, loss = 1.04626947\n",
      "Iteration 31, loss = 1.03348608\n",
      "Iteration 32, loss = 1.02178729\n",
      "Iteration 33, loss = 1.01108710\n",
      "Iteration 34, loss = 1.00120880\n",
      "Iteration 35, loss = 0.99203146\n",
      "Iteration 36, loss = 0.98344871\n",
      "Iteration 37, loss = 0.97531705\n",
      "Iteration 38, loss = 0.96753686\n",
      "Iteration 39, loss = 0.95995661\n",
      "Iteration 40, loss = 0.95248741\n",
      "Iteration 41, loss = 0.94503977\n",
      "Iteration 42, loss = 0.93757150\n",
      "Iteration 43, loss = 0.93006224\n",
      "Iteration 44, loss = 0.92252825\n",
      "Iteration 45, loss = 0.91495705\n",
      "Iteration 46, loss = 0.90740887\n",
      "Iteration 47, loss = 0.89994998\n",
      "Iteration 48, loss = 0.89253682\n",
      "Iteration 49, loss = 0.88525838\n",
      "Iteration 50, loss = 0.87800314\n",
      "Iteration 51, loss = 0.87076845\n",
      "Iteration 52, loss = 0.86357488\n",
      "Iteration 53, loss = 0.85638887\n",
      "Iteration 54, loss = 0.84921868\n",
      "Iteration 55, loss = 0.84212133\n",
      "Iteration 56, loss = 0.83510150\n",
      "Iteration 57, loss = 0.82824573\n",
      "Iteration 58, loss = 0.82154462\n",
      "Iteration 59, loss = 0.81508587\n",
      "Iteration 60, loss = 0.80886041\n",
      "Iteration 61, loss = 0.80284215\n",
      "Iteration 62, loss = 0.79701352\n",
      "Iteration 63, loss = 0.79139906\n",
      "Iteration 64, loss = 0.78599020\n",
      "Iteration 65, loss = 0.78079400\n",
      "Iteration 66, loss = 0.77576237\n",
      "Iteration 67, loss = 0.77092498\n",
      "Iteration 68, loss = 0.76617552\n",
      "Iteration 69, loss = 0.76152702\n",
      "Iteration 70, loss = 0.75695581\n",
      "Iteration 71, loss = 0.75241663\n",
      "Iteration 72, loss = 0.74790643\n",
      "Iteration 73, loss = 0.74341628\n",
      "Iteration 74, loss = 0.73893733\n",
      "Iteration 75, loss = 0.73447656\n",
      "Iteration 76, loss = 0.73004378\n",
      "Iteration 77, loss = 0.72563819\n",
      "Iteration 78, loss = 0.72126595\n",
      "Iteration 79, loss = 0.71692350\n",
      "Iteration 80, loss = 0.71261492\n",
      "Iteration 81, loss = 0.70835113\n",
      "Iteration 82, loss = 0.70412435\n",
      "Iteration 83, loss = 0.69993419\n",
      "Iteration 84, loss = 0.69577319\n",
      "Iteration 85, loss = 0.69163422\n",
      "Iteration 86, loss = 0.68752221\n",
      "Iteration 87, loss = 0.68344390\n",
      "Iteration 88, loss = 0.67939103\n",
      "Iteration 89, loss = 0.67537055\n",
      "Iteration 90, loss = 0.67138630\n",
      "Iteration 91, loss = 0.66743758\n",
      "Iteration 92, loss = 0.66352484\n",
      "Iteration 93, loss = 0.65965016\n",
      "Iteration 94, loss = 0.65581380\n",
      "Iteration 95, loss = 0.65201528\n",
      "Iteration 96, loss = 0.64826534\n",
      "Iteration 97, loss = 0.64455598\n",
      "Iteration 98, loss = 0.64088131\n",
      "Iteration 99, loss = 0.63723307\n",
      "Iteration 100, loss = 0.63361028\n",
      "Iteration 101, loss = 0.63002585\n",
      "Iteration 102, loss = 0.62647136\n",
      "Iteration 103, loss = 0.62295528\n",
      "Iteration 104, loss = 0.61947292\n",
      "Iteration 105, loss = 0.61601063\n",
      "Iteration 106, loss = 0.61257533\n",
      "Iteration 107, loss = 0.60915931\n",
      "Iteration 108, loss = 0.60577489\n",
      "Iteration 109, loss = 0.60242989\n",
      "Iteration 110, loss = 0.59912347\n",
      "Iteration 111, loss = 0.59585147\n",
      "Iteration 112, loss = 0.59262655\n",
      "Iteration 113, loss = 0.58948494\n",
      "Iteration 114, loss = 0.58639298\n",
      "Iteration 115, loss = 0.58335529\n",
      "Iteration 116, loss = 0.58036383\n",
      "Iteration 117, loss = 0.57740843\n",
      "Iteration 118, loss = 0.57448158\n",
      "Iteration 119, loss = 0.57159493\n",
      "Iteration 120, loss = 0.56873721\n",
      "Iteration 121, loss = 0.56590681\n",
      "Iteration 122, loss = 0.56310781\n",
      "Iteration 123, loss = 0.56034098\n",
      "Iteration 124, loss = 0.55760281\n",
      "Iteration 125, loss = 0.55488900\n",
      "Iteration 126, loss = 0.55220270\n",
      "Iteration 127, loss = 0.54955939\n",
      "Iteration 128, loss = 0.54696059\n",
      "Iteration 129, loss = 0.54438419\n",
      "Iteration 130, loss = 0.54183592\n",
      "Iteration 131, loss = 0.53932629\n",
      "Iteration 132, loss = 0.53685337\n",
      "Iteration 133, loss = 0.53440557\n",
      "Iteration 134, loss = 0.53198251\n",
      "Iteration 135, loss = 0.52958745\n",
      "Iteration 136, loss = 0.52721750\n",
      "Iteration 137, loss = 0.52487380\n",
      "Iteration 138, loss = 0.52255153\n",
      "Iteration 139, loss = 0.52025386\n",
      "Iteration 140, loss = 0.51797831\n",
      "Iteration 141, loss = 0.51572443\n",
      "Iteration 142, loss = 0.51349150\n",
      "Iteration 143, loss = 0.51127986\n",
      "Iteration 144, loss = 0.50909021\n",
      "Iteration 145, loss = 0.50692063\n",
      "Iteration 146, loss = 0.50477167\n",
      "Iteration 147, loss = 0.50264295\n",
      "Iteration 148, loss = 0.50053286\n",
      "Iteration 149, loss = 0.49844165\n",
      "Iteration 150, loss = 0.49636867\n",
      "Iteration 151, loss = 0.49431363\n",
      "Iteration 152, loss = 0.49227623\n",
      "Iteration 153, loss = 0.49025619\n",
      "Iteration 154, loss = 0.48825323\n",
      "Iteration 155, loss = 0.48626704\n",
      "Iteration 156, loss = 0.48429725\n",
      "Iteration 157, loss = 0.48234350\n",
      "Iteration 158, loss = 0.48040560\n",
      "Iteration 159, loss = 0.47848328\n",
      "Iteration 160, loss = 0.47657613\n",
      "Iteration 161, loss = 0.47468374\n",
      "Iteration 162, loss = 0.47280605\n",
      "Iteration 163, loss = 0.47094278\n",
      "Iteration 164, loss = 0.46909372\n",
      "Iteration 165, loss = 0.46725858\n",
      "Iteration 166, loss = 0.46543710\n",
      "Iteration 167, loss = 0.46363130\n",
      "Iteration 168, loss = 0.46184061\n",
      "Iteration 169, loss = 0.46006247\n",
      "Iteration 170, loss = 0.45829678\n",
      "Iteration 171, loss = 0.45654347\n",
      "Iteration 172, loss = 0.45480241\n",
      "Iteration 173, loss = 0.45307348\n",
      "Iteration 174, loss = 0.45135656\n",
      "Iteration 175, loss = 0.44965142\n",
      "Iteration 176, loss = 0.44795785\n",
      "Iteration 177, loss = 0.44627681\n",
      "Iteration 178, loss = 0.44460718\n",
      "Iteration 179, loss = 0.44294872\n",
      "Iteration 180, loss = 0.44130110\n",
      "Iteration 181, loss = 0.43966354\n",
      "Iteration 182, loss = 0.43803650\n",
      "Iteration 183, loss = 0.43641970\n",
      "Iteration 184, loss = 0.43481257\n",
      "Iteration 185, loss = 0.43321534\n",
      "Iteration 186, loss = 0.43162964\n",
      "Iteration 187, loss = 0.43005467\n",
      "Iteration 188, loss = 0.42849018\n",
      "Iteration 189, loss = 0.42693549\n",
      "Iteration 190, loss = 0.42538997\n",
      "Iteration 191, loss = 0.42385383\n",
      "Iteration 192, loss = 0.42232720\n",
      "Iteration 193, loss = 0.42080961\n",
      "Iteration 194, loss = 0.41930089\n",
      "Iteration 195, loss = 0.41780095\n",
      "Iteration 196, loss = 0.41630966\n",
      "Iteration 197, loss = 0.41482693\n",
      "Iteration 198, loss = 0.41335264\n",
      "Iteration 199, loss = 0.41188667\n",
      "Iteration 200, loss = 0.41042891\n",
      "Iteration 201, loss = 0.40897923\n",
      "Iteration 202, loss = 0.40753759\n",
      "Iteration 203, loss = 0.40610381\n",
      "Iteration 204, loss = 0.40467741\n",
      "Iteration 205, loss = 0.40325858\n",
      "Iteration 206, loss = 0.40184751\n",
      "Iteration 207, loss = 0.40044381\n",
      "Iteration 208, loss = 0.39904786\n",
      "Iteration 209, loss = 0.39765898\n",
      "Iteration 210, loss = 0.39627707\n",
      "Iteration 211, loss = 0.39490220\n",
      "Iteration 212, loss = 0.39353472\n",
      "Iteration 213, loss = 0.39217444\n",
      "Iteration 214, loss = 0.39082090\n",
      "Iteration 215, loss = 0.38947409\n",
      "Iteration 216, loss = 0.38813393\n",
      "Iteration 217, loss = 0.38680049\n",
      "Iteration 218, loss = 0.38547366\n",
      "Iteration 219, loss = 0.38415304\n",
      "Iteration 220, loss = 0.38283881\n",
      "Iteration 221, loss = 0.38153111\n",
      "Iteration 222, loss = 0.38022965\n",
      "Iteration 223, loss = 0.37893426\n",
      "Iteration 224, loss = 0.37764509\n",
      "Iteration 225, loss = 0.37636192\n",
      "Iteration 226, loss = 0.37508491\n",
      "Iteration 227, loss = 0.37381394\n",
      "Iteration 228, loss = 0.37254883\n",
      "Iteration 229, loss = 0.37128964\n",
      "Iteration 230, loss = 0.37003617\n",
      "Iteration 231, loss = 0.36878824\n",
      "Iteration 232, loss = 0.36754634\n",
      "Iteration 233, loss = 0.36630998\n",
      "Iteration 234, loss = 0.36507914\n",
      "Iteration 235, loss = 0.36385374\n",
      "Iteration 236, loss = 0.36263363\n",
      "Iteration 237, loss = 0.36141890\n",
      "Iteration 238, loss = 0.36020947\n",
      "Iteration 239, loss = 0.35900553\n",
      "Iteration 240, loss = 0.35780647\n",
      "Iteration 241, loss = 0.35661303\n",
      "Iteration 242, loss = 0.35542473\n",
      "Iteration 243, loss = 0.35424124\n",
      "Iteration 244, loss = 0.35306310\n",
      "Iteration 245, loss = 0.35189002\n",
      "Iteration 246, loss = 0.35072167\n",
      "Iteration 247, loss = 0.34955814\n",
      "Iteration 248, loss = 0.34839944\n",
      "Iteration 249, loss = 0.34724552\n",
      "Iteration 250, loss = 0.34609635\n",
      "Iteration 251, loss = 0.34495187\n",
      "Iteration 252, loss = 0.34381207\n",
      "Iteration 253, loss = 0.34267689\n",
      "Iteration 254, loss = 0.34154630\n",
      "Iteration 255, loss = 0.34042026\n",
      "Iteration 256, loss = 0.33929875\n",
      "Iteration 257, loss = 0.33818171\n",
      "Iteration 258, loss = 0.33706913\n",
      "Iteration 259, loss = 0.33596097\n",
      "Iteration 260, loss = 0.33485719\n",
      "Iteration 261, loss = 0.33375777\n",
      "Iteration 262, loss = 0.33266266\n",
      "Iteration 263, loss = 0.33157185\n",
      "Iteration 264, loss = 0.33048580\n",
      "Iteration 265, loss = 0.32940493\n",
      "Iteration 266, loss = 0.32832860\n",
      "Iteration 267, loss = 0.32725673\n",
      "Iteration 268, loss = 0.32618884\n",
      "Iteration 269, loss = 0.32512529\n",
      "Iteration 270, loss = 0.32406563\n",
      "Iteration 271, loss = 0.32300992\n",
      "Iteration 272, loss = 0.32195837\n",
      "Iteration 273, loss = 0.32091119\n",
      "Iteration 274, loss = 0.31986803\n",
      "Iteration 275, loss = 0.31882909\n",
      "Iteration 276, loss = 0.31779446\n",
      "Iteration 277, loss = 0.31676380\n",
      "Iteration 278, loss = 0.31573718\n",
      "Iteration 279, loss = 0.31471477\n",
      "Iteration 280, loss = 0.31369620\n",
      "Iteration 281, loss = 0.31268146\n",
      "Iteration 282, loss = 0.31167076\n",
      "Iteration 283, loss = 0.31066448\n",
      "Iteration 284, loss = 0.30966169\n",
      "Iteration 285, loss = 0.30866287\n",
      "Iteration 286, loss = 0.30766822\n",
      "Iteration 287, loss = 0.30667726\n",
      "Iteration 288, loss = 0.30569000\n",
      "Iteration 289, loss = 0.30470665\n",
      "Iteration 290, loss = 0.30372716\n",
      "Iteration 291, loss = 0.30275138\n",
      "Iteration 292, loss = 0.30177931\n",
      "Iteration 293, loss = 0.30081099\n",
      "Iteration 294, loss = 0.29984634\n",
      "Iteration 295, loss = 0.29888547\n",
      "Iteration 296, loss = 0.29792830\n",
      "Iteration 297, loss = 0.29697476\n",
      "Iteration 298, loss = 0.29602481\n",
      "Iteration 299, loss = 0.29507879\n",
      "Iteration 300, loss = 0.29413634\n",
      "Iteration 301, loss = 0.29319743\n",
      "Iteration 302, loss = 0.29226218\n",
      "Iteration 303, loss = 0.29133049\n",
      "Iteration 304, loss = 0.29040228\n",
      "Iteration 305, loss = 0.28947772\n",
      "Iteration 306, loss = 0.28855660\n",
      "Iteration 307, loss = 0.28763908\n",
      "Iteration 308, loss = 0.28672521\n",
      "Iteration 309, loss = 0.28581482\n",
      "Iteration 310, loss = 0.28490782\n",
      "Iteration 311, loss = 0.28400421\n",
      "Iteration 312, loss = 0.28310415\n",
      "Iteration 313, loss = 0.28220763\n",
      "Iteration 314, loss = 0.28131447\n",
      "Iteration 315, loss = 0.28042473\n",
      "Iteration 316, loss = 0.27953842\n",
      "Iteration 317, loss = 0.27865554\n",
      "Iteration 318, loss = 0.27777603\n",
      "Iteration 319, loss = 0.27689987\n",
      "Iteration 320, loss = 0.27602719\n",
      "Iteration 321, loss = 0.27515781\n",
      "Iteration 322, loss = 0.27429173\n",
      "Iteration 323, loss = 0.27342903\n",
      "Iteration 324, loss = 0.27256958\n",
      "Iteration 325, loss = 0.27171335\n",
      "Iteration 326, loss = 0.27086037\n",
      "Iteration 327, loss = 0.27001078\n",
      "Iteration 328, loss = 0.26916444\n",
      "Iteration 329, loss = 0.26832128\n",
      "Iteration 330, loss = 0.26748142\n",
      "Iteration 331, loss = 0.26664488\n",
      "Iteration 332, loss = 0.26581151\n",
      "Iteration 333, loss = 0.26498132\n",
      "Iteration 334, loss = 0.26415448\n",
      "Iteration 335, loss = 0.26333075\n",
      "Iteration 336, loss = 0.26251029\n",
      "Iteration 337, loss = 0.26169299\n",
      "Iteration 338, loss = 0.26087888\n",
      "Iteration 339, loss = 0.26006792\n",
      "Iteration 340, loss = 0.25926016\n",
      "Iteration 341, loss = 0.25845565\n",
      "Iteration 342, loss = 0.25765430\n",
      "Iteration 343, loss = 0.25685617\n",
      "Iteration 344, loss = 0.25606107\n",
      "Iteration 345, loss = 0.25526918\n",
      "Iteration 346, loss = 0.25448042\n",
      "Iteration 347, loss = 0.25369476\n",
      "Iteration 348, loss = 0.25291236\n",
      "Iteration 349, loss = 0.25213302\n",
      "Iteration 350, loss = 0.25135671\n",
      "Iteration 351, loss = 0.25058352\n",
      "Iteration 352, loss = 0.24981348\n",
      "Iteration 353, loss = 0.24904648\n",
      "Iteration 354, loss = 0.24828252\n",
      "Iteration 355, loss = 0.24752167\n",
      "Iteration 356, loss = 0.24676378\n",
      "Iteration 357, loss = 0.24600894\n",
      "Iteration 358, loss = 0.24525795\n",
      "Iteration 359, loss = 0.24450892\n",
      "Iteration 360, loss = 0.24376241\n",
      "Iteration 361, loss = 0.24301959\n",
      "Iteration 362, loss = 0.24227947\n",
      "Iteration 363, loss = 0.24154307\n",
      "Iteration 364, loss = 0.24080900\n",
      "Iteration 365, loss = 0.24007743\n",
      "Iteration 366, loss = 0.23934946\n",
      "Iteration 367, loss = 0.23862445\n",
      "Iteration 368, loss = 0.23790205\n",
      "Iteration 369, loss = 0.23718248\n",
      "Iteration 370, loss = 0.23646547\n",
      "Iteration 371, loss = 0.23575234\n",
      "Iteration 372, loss = 0.23504191\n",
      "Iteration 373, loss = 0.23433394\n",
      "Iteration 374, loss = 0.23362855\n",
      "Iteration 375, loss = 0.23292527\n",
      "Iteration 376, loss = 0.23222384\n",
      "Iteration 377, loss = 0.23152482\n",
      "Iteration 378, loss = 0.23082337\n",
      "Iteration 379, loss = 0.23011876\n",
      "Iteration 380, loss = 0.22941319\n",
      "Iteration 381, loss = 0.22870779\n",
      "Iteration 382, loss = 0.22800109\n",
      "Iteration 383, loss = 0.22729494\n",
      "Iteration 384, loss = 0.22658937\n",
      "Iteration 385, loss = 0.22588202\n",
      "Iteration 386, loss = 0.22517362\n",
      "Iteration 387, loss = 0.22446348\n",
      "Iteration 388, loss = 0.22375421\n",
      "Iteration 389, loss = 0.22304247\n",
      "Iteration 390, loss = 0.22233112\n",
      "Iteration 391, loss = 0.22161921\n",
      "Iteration 392, loss = 0.22090594\n",
      "Iteration 393, loss = 0.22019245\n",
      "Iteration 394, loss = 0.21947800\n",
      "Iteration 395, loss = 0.21876312\n",
      "Iteration 396, loss = 0.21804852\n",
      "Iteration 397, loss = 0.21733224\n",
      "Iteration 398, loss = 0.21661667\n",
      "Iteration 399, loss = 0.21590070\n",
      "Iteration 400, loss = 0.21518464\n",
      "Iteration 401, loss = 0.21446859\n",
      "Iteration 402, loss = 0.21375279\n",
      "Iteration 403, loss = 0.21303745\n",
      "Iteration 404, loss = 0.21232244\n",
      "Iteration 405, loss = 0.21160834\n",
      "Iteration 406, loss = 0.21089468\n",
      "Iteration 407, loss = 0.21018165\n",
      "Iteration 408, loss = 0.20946977\n",
      "Iteration 409, loss = 0.20875884\n",
      "Iteration 410, loss = 0.20804875\n",
      "Iteration 411, loss = 0.20733970\n",
      "Iteration 412, loss = 0.20663199\n",
      "Iteration 413, loss = 0.20592557\n",
      "Iteration 414, loss = 0.20522055\n",
      "Iteration 415, loss = 0.20451703\n",
      "Iteration 416, loss = 0.20381523\n",
      "Iteration 417, loss = 0.20311522\n",
      "Iteration 418, loss = 0.20241709\n",
      "Iteration 419, loss = 0.20172093\n",
      "Iteration 420, loss = 0.20102683\n",
      "Iteration 421, loss = 0.20033544\n",
      "Iteration 422, loss = 0.19964633\n",
      "Iteration 423, loss = 0.19895952\n",
      "Iteration 424, loss = 0.19827511\n",
      "Iteration 425, loss = 0.19759313\n",
      "Iteration 426, loss = 0.19691363\n",
      "Iteration 427, loss = 0.19623675\n",
      "Iteration 428, loss = 0.19556281\n",
      "Iteration 429, loss = 0.19489145\n",
      "Iteration 430, loss = 0.19422309\n",
      "Iteration 431, loss = 0.19355756\n",
      "Iteration 432, loss = 0.19289490\n",
      "Iteration 433, loss = 0.19223501\n",
      "Iteration 434, loss = 0.19157839\n",
      "Iteration 435, loss = 0.19092429\n",
      "Iteration 436, loss = 0.19027362\n",
      "Iteration 437, loss = 0.18962592\n",
      "Iteration 438, loss = 0.18898123\n",
      "Iteration 439, loss = 0.18833980\n",
      "Iteration 440, loss = 0.18770188\n",
      "Iteration 441, loss = 0.18706711\n",
      "Iteration 442, loss = 0.18643550\n",
      "Iteration 443, loss = 0.18580705\n",
      "Iteration 444, loss = 0.18518175\n",
      "Iteration 445, loss = 0.18455966\n",
      "Iteration 446, loss = 0.18394086\n",
      "Iteration 447, loss = 0.18332527\n",
      "Iteration 448, loss = 0.18271312\n",
      "Iteration 449, loss = 0.18210443\n",
      "Iteration 450, loss = 0.18149893\n",
      "Iteration 451, loss = 0.18089659\n",
      "Iteration 452, loss = 0.18029762\n",
      "Iteration 453, loss = 0.17970178\n",
      "Iteration 454, loss = 0.17910927\n",
      "Iteration 455, loss = 0.17852019\n",
      "Iteration 456, loss = 0.17793437\n",
      "Iteration 457, loss = 0.17735173\n",
      "Iteration 458, loss = 0.17677234\n",
      "Iteration 459, loss = 0.17619627\n",
      "Iteration 460, loss = 0.17562344\n",
      "Iteration 461, loss = 0.17505393\n",
      "Iteration 462, loss = 0.17448765\n",
      "Iteration 463, loss = 0.17392460\n",
      "Iteration 464, loss = 0.17336481\n",
      "Iteration 465, loss = 0.17280826\n",
      "Iteration 466, loss = 0.17225486\n",
      "Iteration 467, loss = 0.17170475\n",
      "Iteration 468, loss = 0.17115780\n",
      "Iteration 469, loss = 0.17061400\n",
      "Iteration 470, loss = 0.17007337\n",
      "Iteration 471, loss = 0.16953583\n",
      "Iteration 472, loss = 0.16900147\n",
      "Iteration 473, loss = 0.16847024\n",
      "Iteration 474, loss = 0.16794211\n",
      "Iteration 475, loss = 0.16741702\n",
      "Iteration 476, loss = 0.16689505\n",
      "Iteration 477, loss = 0.16637614\n",
      "Iteration 478, loss = 0.16586027\n",
      "Iteration 479, loss = 0.16534708\n",
      "Iteration 480, loss = 0.16483673\n",
      "Iteration 481, loss = 0.16432929\n",
      "Iteration 482, loss = 0.16382448\n",
      "Iteration 483, loss = 0.16332229\n",
      "Iteration 484, loss = 0.16282292\n",
      "Iteration 485, loss = 0.16232639\n",
      "Iteration 486, loss = 0.16183242\n",
      "Iteration 487, loss = 0.16134110\n",
      "Iteration 488, loss = 0.16085254\n",
      "Iteration 489, loss = 0.16036677\n",
      "Iteration 490, loss = 0.15988359\n",
      "Iteration 491, loss = 0.15940296\n",
      "Iteration 492, loss = 0.15892473\n",
      "Iteration 493, loss = 0.15844888\n",
      "Iteration 494, loss = 0.15797546\n",
      "Iteration 495, loss = 0.15750479\n",
      "Iteration 496, loss = 0.15703682\n",
      "Iteration 497, loss = 0.15657190\n",
      "Iteration 498, loss = 0.15611020\n",
      "Iteration 499, loss = 0.15565090\n",
      "Iteration 500, loss = 0.15519332\n",
      "Iteration 501, loss = 0.15473813\n",
      "Iteration 502, loss = 0.15428547\n",
      "Iteration 503, loss = 0.15383529\n",
      "Iteration 504, loss = 0.15338718\n",
      "Iteration 505, loss = 0.15294105\n",
      "Iteration 506, loss = 0.15249676\n",
      "Iteration 507, loss = 0.15205468\n",
      "Iteration 508, loss = 0.15161598\n",
      "Iteration 509, loss = 0.15117993\n",
      "Iteration 510, loss = 0.15074647\n",
      "Iteration 511, loss = 0.15031588\n",
      "Iteration 512, loss = 0.14988782\n",
      "Iteration 513, loss = 0.14946231\n",
      "Iteration 514, loss = 0.14903906\n",
      "Iteration 515, loss = 0.14861821\n",
      "Iteration 516, loss = 0.14819983\n",
      "Iteration 517, loss = 0.14778385\n",
      "Iteration 518, loss = 0.14736998\n",
      "Iteration 519, loss = 0.14695848\n",
      "Iteration 520, loss = 0.14654919\n",
      "Iteration 521, loss = 0.14614226\n",
      "Iteration 522, loss = 0.14573818\n",
      "Iteration 523, loss = 0.14533634\n",
      "Iteration 524, loss = 0.14493708\n",
      "Iteration 525, loss = 0.14454020\n",
      "Iteration 526, loss = 0.14414566\n",
      "Iteration 527, loss = 0.14375344\n",
      "Iteration 528, loss = 0.14336404\n",
      "Iteration 529, loss = 0.14297772\n",
      "Iteration 530, loss = 0.14259349\n",
      "Iteration 531, loss = 0.14221146\n",
      "Iteration 532, loss = 0.14183182\n",
      "Iteration 533, loss = 0.14145443\n",
      "Iteration 534, loss = 0.14107926\n",
      "Iteration 535, loss = 0.14070569\n",
      "Iteration 536, loss = 0.14033478\n",
      "Iteration 537, loss = 0.13996607\n",
      "Iteration 538, loss = 0.13959948\n",
      "Iteration 539, loss = 0.13923489\n",
      "Iteration 540, loss = 0.13887201\n",
      "Iteration 541, loss = 0.13851110\n",
      "Iteration 542, loss = 0.13815225\n",
      "Iteration 543, loss = 0.13779568\n",
      "Iteration 544, loss = 0.13744090\n",
      "Iteration 545, loss = 0.13708815\n",
      "Iteration 546, loss = 0.13673774\n",
      "Iteration 547, loss = 0.13638926\n",
      "Iteration 548, loss = 0.13604216\n",
      "Iteration 549, loss = 0.13569677\n",
      "Iteration 550, loss = 0.13535340\n",
      "Iteration 551, loss = 0.13501249\n",
      "Iteration 552, loss = 0.13467351\n",
      "Iteration 553, loss = 0.13433644\n",
      "Iteration 554, loss = 0.13400133\n",
      "Iteration 555, loss = 0.13366875\n",
      "Iteration 556, loss = 0.13333775\n",
      "Iteration 557, loss = 0.13300860\n",
      "Iteration 558, loss = 0.13268170\n",
      "Iteration 559, loss = 0.13235701\n",
      "Iteration 560, loss = 0.13203479\n",
      "Iteration 561, loss = 0.13171459\n",
      "Iteration 562, loss = 0.13139651\n",
      "Iteration 563, loss = 0.13107968\n",
      "Iteration 564, loss = 0.13076436\n",
      "Iteration 565, loss = 0.13045089\n",
      "Iteration 566, loss = 0.13013910\n",
      "Iteration 567, loss = 0.12982907\n",
      "Iteration 568, loss = 0.12952058\n",
      "Iteration 569, loss = 0.12921379\n",
      "Iteration 570, loss = 0.12890901\n",
      "Iteration 571, loss = 0.12860643\n",
      "Iteration 572, loss = 0.12830562\n",
      "Iteration 573, loss = 0.12800642\n",
      "Iteration 574, loss = 0.12770893\n",
      "Iteration 575, loss = 0.12741305\n",
      "Iteration 576, loss = 0.12711892\n",
      "Iteration 577, loss = 0.12682636\n",
      "Iteration 578, loss = 0.12653535\n",
      "Iteration 579, loss = 0.12624589\n",
      "Iteration 580, loss = 0.12595795\n",
      "Iteration 581, loss = 0.12567151\n",
      "Iteration 582, loss = 0.12538657\n",
      "Iteration 583, loss = 0.12510310\n",
      "Iteration 584, loss = 0.12482110\n",
      "Iteration 585, loss = 0.12454055\n",
      "Iteration 586, loss = 0.12426145\n",
      "Iteration 587, loss = 0.12398383\n",
      "Iteration 588, loss = 0.12370798\n",
      "Iteration 589, loss = 0.12343326\n",
      "Iteration 590, loss = 0.12315972\n",
      "Iteration 591, loss = 0.12288800\n",
      "Iteration 592, loss = 0.12261774\n",
      "Iteration 593, loss = 0.12234882\n",
      "Iteration 594, loss = 0.12208117\n",
      "Iteration 595, loss = 0.12181478\n",
      "Iteration 596, loss = 0.12154966\n",
      "Iteration 597, loss = 0.12128598\n",
      "Iteration 598, loss = 0.12102392\n",
      "Iteration 599, loss = 0.12076308\n",
      "Iteration 600, loss = 0.12050346\n",
      "Iteration 601, loss = 0.12024508\n",
      "Iteration 602, loss = 0.11998837\n",
      "Iteration 603, loss = 0.11973285\n",
      "Iteration 604, loss = 0.11947841\n",
      "Iteration 605, loss = 0.11922511\n",
      "Iteration 606, loss = 0.11897344\n",
      "Iteration 607, loss = 0.11872302\n",
      "Iteration 608, loss = 0.11847381\n",
      "Iteration 609, loss = 0.11822582\n",
      "Iteration 610, loss = 0.11797908\n",
      "Iteration 611, loss = 0.11773372\n",
      "Iteration 612, loss = 0.11748937\n",
      "Iteration 613, loss = 0.11724631\n",
      "Iteration 614, loss = 0.11700450\n",
      "Iteration 615, loss = 0.11676386\n",
      "Iteration 616, loss = 0.11652439\n",
      "Iteration 617, loss = 0.11628606\n",
      "Iteration 618, loss = 0.11604890\n",
      "Iteration 619, loss = 0.11581288\n",
      "Iteration 620, loss = 0.11557802\n",
      "Iteration 621, loss = 0.11534431\n",
      "Iteration 622, loss = 0.11511175\n",
      "Iteration 623, loss = 0.11488032\n",
      "Iteration 624, loss = 0.11465001\n",
      "Iteration 625, loss = 0.11442083\n",
      "Iteration 626, loss = 0.11419274\n",
      "Iteration 627, loss = 0.11396576\n",
      "Iteration 628, loss = 0.11373985\n",
      "Iteration 629, loss = 0.11351503\n",
      "Iteration 630, loss = 0.11329127\n",
      "Iteration 631, loss = 0.11306859\n",
      "Iteration 632, loss = 0.11284696\n",
      "Iteration 633, loss = 0.11262639\n",
      "Iteration 634, loss = 0.11240687\n",
      "Iteration 635, loss = 0.11218843\n",
      "Iteration 636, loss = 0.11197104\n",
      "Iteration 637, loss = 0.11175468\n",
      "Iteration 638, loss = 0.11153938\n",
      "Iteration 639, loss = 0.11132509\n",
      "Iteration 640, loss = 0.11111180\n",
      "Iteration 641, loss = 0.11089949\n",
      "Iteration 642, loss = 0.11068817\n",
      "Iteration 643, loss = 0.11047784\n",
      "Iteration 644, loss = 0.11026847\n",
      "Iteration 645, loss = 0.11006008\n",
      "Iteration 646, loss = 0.10985266\n",
      "Iteration 647, loss = 0.10964623\n",
      "Iteration 648, loss = 0.10944076\n",
      "Iteration 649, loss = 0.10923624\n",
      "Iteration 650, loss = 0.10903266\n",
      "Iteration 651, loss = 0.10883003\n",
      "Iteration 652, loss = 0.10862833\n",
      "Iteration 653, loss = 0.10842756\n",
      "Iteration 654, loss = 0.10822771\n",
      "Iteration 655, loss = 0.10802878\n",
      "Iteration 656, loss = 0.10783076\n",
      "Iteration 657, loss = 0.10763363\n",
      "Iteration 658, loss = 0.10743737\n",
      "Iteration 659, loss = 0.10724196\n",
      "Iteration 660, loss = 0.10704741\n",
      "Iteration 661, loss = 0.10685370\n",
      "Iteration 662, loss = 0.10666091\n",
      "Iteration 663, loss = 0.10646879\n",
      "Iteration 664, loss = 0.10627753\n",
      "Iteration 665, loss = 0.10608705\n",
      "Iteration 666, loss = 0.10589719\n",
      "Iteration 667, loss = 0.10570814\n",
      "Iteration 668, loss = 0.10551951\n",
      "Iteration 669, loss = 0.10533159\n",
      "Iteration 670, loss = 0.10514427\n",
      "Iteration 671, loss = 0.10495708\n",
      "Iteration 672, loss = 0.10477033\n",
      "Iteration 673, loss = 0.10458395\n",
      "Iteration 674, loss = 0.10439798\n",
      "Iteration 675, loss = 0.10421248\n",
      "Iteration 676, loss = 0.10402687\n",
      "Iteration 677, loss = 0.10384188\n",
      "Iteration 678, loss = 0.10365710\n",
      "Iteration 679, loss = 0.10347264\n",
      "Iteration 680, loss = 0.10328832\n",
      "Iteration 681, loss = 0.10310450\n",
      "Iteration 682, loss = 0.10292071\n",
      "Iteration 683, loss = 0.10273687\n",
      "Iteration 684, loss = 0.10255412\n",
      "Iteration 685, loss = 0.10237075\n",
      "Iteration 686, loss = 0.10218832\n",
      "Iteration 687, loss = 0.10200579\n",
      "Iteration 688, loss = 0.10182351\n",
      "Iteration 689, loss = 0.10164192\n",
      "Iteration 690, loss = 0.10146020\n",
      "Iteration 691, loss = 0.10127940\n",
      "Iteration 692, loss = 0.10109868\n",
      "Iteration 693, loss = 0.10091803\n",
      "Iteration 694, loss = 0.10073790\n",
      "Iteration 695, loss = 0.10055824\n",
      "Iteration 696, loss = 0.10037887\n",
      "Iteration 697, loss = 0.10019971\n",
      "Iteration 698, loss = 0.10002088\n",
      "Iteration 699, loss = 0.09984263\n",
      "Iteration 700, loss = 0.09966498\n",
      "Iteration 701, loss = 0.09948752\n",
      "Iteration 702, loss = 0.09931050\n",
      "Iteration 703, loss = 0.09913414\n",
      "Iteration 704, loss = 0.09895826\n",
      "Iteration 705, loss = 0.09878287\n",
      "Iteration 706, loss = 0.09860793\n",
      "Iteration 707, loss = 0.09843343\n",
      "Iteration 708, loss = 0.09825984\n",
      "Iteration 709, loss = 0.09808656\n",
      "Iteration 710, loss = 0.09791348\n",
      "Iteration 711, loss = 0.09774122\n",
      "Iteration 712, loss = 0.09756958\n",
      "Iteration 713, loss = 0.09739834\n",
      "Iteration 714, loss = 0.09722754\n",
      "Iteration 715, loss = 0.09705732\n",
      "Iteration 716, loss = 0.09688845\n",
      "Iteration 717, loss = 0.09672006\n",
      "Iteration 718, loss = 0.09655194\n",
      "Iteration 719, loss = 0.09638516\n",
      "Iteration 720, loss = 0.09621901\n",
      "Iteration 721, loss = 0.09605332\n",
      "Iteration 722, loss = 0.09588814\n",
      "Iteration 723, loss = 0.09572356\n",
      "Iteration 724, loss = 0.09555986\n",
      "Iteration 725, loss = 0.09539732\n",
      "Iteration 726, loss = 0.09523525\n",
      "Iteration 727, loss = 0.09507387\n",
      "Iteration 728, loss = 0.09491357\n",
      "Iteration 729, loss = 0.09475388\n",
      "Iteration 730, loss = 0.09459458\n",
      "Iteration 731, loss = 0.09443620\n",
      "Iteration 732, loss = 0.09427821\n",
      "Iteration 733, loss = 0.09412044\n",
      "Iteration 734, loss = 0.09396273\n",
      "Iteration 735, loss = 0.09380511\n",
      "Iteration 736, loss = 0.09364715\n",
      "Iteration 737, loss = 0.09348878\n",
      "Iteration 738, loss = 0.09333020\n",
      "Iteration 739, loss = 0.09317081\n",
      "Iteration 740, loss = 0.09301035\n",
      "Iteration 741, loss = 0.09284888\n",
      "Iteration 742, loss = 0.09268647\n",
      "Iteration 743, loss = 0.09252273\n",
      "Iteration 744, loss = 0.09235724\n",
      "Iteration 745, loss = 0.09219024\n",
      "Iteration 746, loss = 0.09202131\n",
      "Iteration 747, loss = 0.09185027\n",
      "Iteration 748, loss = 0.09167732\n",
      "Iteration 749, loss = 0.09150272\n",
      "Iteration 750, loss = 0.09132625\n",
      "Iteration 751, loss = 0.09114773\n",
      "Iteration 752, loss = 0.09096711\n",
      "Iteration 753, loss = 0.09078463\n",
      "Iteration 754, loss = 0.09060026\n",
      "Iteration 755, loss = 0.09041424\n",
      "Iteration 756, loss = 0.09022650\n",
      "Iteration 757, loss = 0.09003707\n",
      "Iteration 758, loss = 0.08984606\n",
      "Iteration 759, loss = 0.08965351\n",
      "Iteration 760, loss = 0.08945952\n",
      "Iteration 761, loss = 0.08926415\n",
      "Iteration 762, loss = 0.08906750\n",
      "Iteration 763, loss = 0.08886963\n",
      "Iteration 764, loss = 0.08867060\n",
      "Iteration 765, loss = 0.08847051\n",
      "Iteration 766, loss = 0.08826942\n",
      "Iteration 767, loss = 0.08806749\n",
      "Iteration 768, loss = 0.08786471\n",
      "Iteration 769, loss = 0.08766119\n",
      "Iteration 770, loss = 0.08745707\n",
      "Iteration 771, loss = 0.08725240\n",
      "Iteration 772, loss = 0.08704721\n",
      "Iteration 773, loss = 0.08684161\n",
      "Iteration 774, loss = 0.08663571\n",
      "Iteration 775, loss = 0.08642960\n",
      "Iteration 776, loss = 0.08622318\n",
      "Iteration 777, loss = 0.08601656\n",
      "Iteration 778, loss = 0.08580990\n",
      "Iteration 779, loss = 0.08560323\n",
      "Iteration 780, loss = 0.08539662\n",
      "Iteration 781, loss = 0.08519007\n",
      "Iteration 782, loss = 0.08498364\n",
      "Iteration 783, loss = 0.08477747\n",
      "Iteration 784, loss = 0.08457149\n",
      "Iteration 785, loss = 0.08436573\n",
      "Iteration 786, loss = 0.08416031\n",
      "Iteration 787, loss = 0.08395530\n",
      "Iteration 788, loss = 0.08375064\n",
      "Iteration 789, loss = 0.08354649\n",
      "Iteration 790, loss = 0.08334282\n",
      "Iteration 791, loss = 0.08313963\n",
      "Iteration 792, loss = 0.08293695\n",
      "Iteration 793, loss = 0.08273491\n",
      "Iteration 794, loss = 0.08253348\n",
      "Iteration 795, loss = 0.08233267\n",
      "Iteration 796, loss = 0.08213263\n",
      "Iteration 797, loss = 0.08193332\n",
      "Iteration 798, loss = 0.08173472\n",
      "Iteration 799, loss = 0.08153688\n",
      "Iteration 800, loss = 0.08133982\n",
      "Iteration 801, loss = 0.08114356\n",
      "Iteration 802, loss = 0.08094815\n",
      "Iteration 803, loss = 0.08075359\n",
      "Iteration 804, loss = 0.08055992\n",
      "Iteration 805, loss = 0.08036720\n",
      "Iteration 806, loss = 0.08017544\n",
      "Iteration 807, loss = 0.07998495\n",
      "Iteration 808, loss = 0.07979548\n",
      "Iteration 809, loss = 0.07960690\n",
      "Iteration 810, loss = 0.07941926\n",
      "Iteration 811, loss = 0.07923260\n",
      "Iteration 812, loss = 0.07904690\n",
      "Iteration 813, loss = 0.07886215\n",
      "Iteration 814, loss = 0.07867837\n",
      "Iteration 815, loss = 0.07849561\n",
      "Iteration 816, loss = 0.07831387\n",
      "Iteration 817, loss = 0.07813311\n",
      "Iteration 818, loss = 0.07795337\n",
      "Iteration 819, loss = 0.07777466\n",
      "Iteration 820, loss = 0.07759698\n",
      "Iteration 821, loss = 0.07742033\n",
      "Iteration 822, loss = 0.07724471\n",
      "Iteration 823, loss = 0.07707012\n",
      "Iteration 824, loss = 0.07689663\n",
      "Iteration 825, loss = 0.07672420\n",
      "Iteration 826, loss = 0.07655283\n",
      "Iteration 827, loss = 0.07638250\n",
      "Iteration 828, loss = 0.07621322\n",
      "Iteration 829, loss = 0.07604503\n",
      "Iteration 830, loss = 0.07587791\n",
      "Iteration 831, loss = 0.07571186\n",
      "Iteration 832, loss = 0.07554685\n",
      "Iteration 833, loss = 0.07538291\n",
      "Iteration 834, loss = 0.07522000\n",
      "Iteration 835, loss = 0.07505813\n",
      "Iteration 836, loss = 0.07489732\n",
      "Iteration 837, loss = 0.07473754\n",
      "Iteration 838, loss = 0.07457883\n",
      "Iteration 839, loss = 0.07442116\n",
      "Iteration 840, loss = 0.07426453\n",
      "Iteration 841, loss = 0.07410893\n",
      "Iteration 842, loss = 0.07395434\n",
      "Iteration 843, loss = 0.07380084\n",
      "Iteration 844, loss = 0.07364833\n",
      "Iteration 845, loss = 0.07349681\n",
      "Iteration 846, loss = 0.07334626\n",
      "Iteration 847, loss = 0.07319670\n",
      "Iteration 848, loss = 0.07304818\n",
      "Iteration 849, loss = 0.07290065\n",
      "Iteration 850, loss = 0.07275411\n",
      "Iteration 851, loss = 0.07260854\n",
      "Iteration 852, loss = 0.07246394\n",
      "Iteration 853, loss = 0.07232032\n",
      "Iteration 854, loss = 0.07217768\n",
      "Iteration 855, loss = 0.07203600\n",
      "Iteration 856, loss = 0.07189528\n",
      "Iteration 857, loss = 0.07175556\n",
      "Iteration 858, loss = 0.07161678\n",
      "Iteration 859, loss = 0.07147901\n",
      "Iteration 860, loss = 0.07134217\n",
      "Iteration 861, loss = 0.07120623\n",
      "Iteration 862, loss = 0.07107122\n",
      "Iteration 863, loss = 0.07093712\n",
      "Iteration 864, loss = 0.07080393\n",
      "Iteration 865, loss = 0.07067162\n",
      "Iteration 866, loss = 0.07054022\n",
      "Iteration 867, loss = 0.07040970\n",
      "Iteration 868, loss = 0.07028006\n",
      "Iteration 869, loss = 0.07015130\n",
      "Iteration 870, loss = 0.07002340\n",
      "Iteration 871, loss = 0.06989637\n",
      "Iteration 872, loss = 0.06977019\n",
      "Iteration 873, loss = 0.06964485\n",
      "Iteration 874, loss = 0.06952037\n",
      "Iteration 875, loss = 0.06939671\n",
      "Iteration 876, loss = 0.06927389\n",
      "Iteration 877, loss = 0.06915190\n",
      "Iteration 878, loss = 0.06903072\n",
      "Iteration 879, loss = 0.06891035\n",
      "Iteration 880, loss = 0.06879079\n",
      "Iteration 881, loss = 0.06867203\n",
      "Iteration 882, loss = 0.06855406\n",
      "Iteration 883, loss = 0.06843688\n",
      "Iteration 884, loss = 0.06832048\n",
      "Iteration 885, loss = 0.06820485\n",
      "Iteration 886, loss = 0.06809000\n",
      "Iteration 887, loss = 0.06797591\n",
      "Iteration 888, loss = 0.06786258\n",
      "Iteration 889, loss = 0.06775000\n",
      "Iteration 890, loss = 0.06763817\n",
      "Iteration 891, loss = 0.06752708\n",
      "Iteration 892, loss = 0.06741672\n",
      "Iteration 893, loss = 0.06730709\n",
      "Iteration 894, loss = 0.06719818\n",
      "Iteration 895, loss = 0.06708999\n",
      "Iteration 896, loss = 0.06698251\n",
      "Iteration 897, loss = 0.06687573\n",
      "Iteration 898, loss = 0.06676965\n",
      "Iteration 899, loss = 0.06666426\n",
      "Iteration 900, loss = 0.06655957\n",
      "Iteration 901, loss = 0.06645557\n",
      "Iteration 902, loss = 0.06635224\n",
      "Iteration 903, loss = 0.06624957\n",
      "Iteration 904, loss = 0.06614756\n",
      "Iteration 905, loss = 0.06604620\n",
      "Iteration 906, loss = 0.06594552\n",
      "Iteration 907, loss = 0.06584548\n",
      "Iteration 908, loss = 0.06574608\n",
      "Iteration 909, loss = 0.06564731\n",
      "Iteration 910, loss = 0.06554918\n",
      "Iteration 911, loss = 0.06545167\n",
      "Iteration 912, loss = 0.06535479\n",
      "Iteration 913, loss = 0.06525860\n",
      "Iteration 914, loss = 0.06516295\n",
      "Iteration 915, loss = 0.06506801\n",
      "Iteration 916, loss = 0.06497364\n",
      "Iteration 917, loss = 0.06487982\n",
      "Iteration 918, loss = 0.06478667\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.94163030\n",
      "Iteration 2, loss = 1.89706643\n",
      "Iteration 3, loss = 1.85360582\n",
      "Iteration 4, loss = 1.81120620\n",
      "Iteration 5, loss = 1.76986631\n",
      "Iteration 6, loss = 1.72940288\n",
      "Iteration 7, loss = 1.68973291\n",
      "Iteration 8, loss = 1.65090503\n",
      "Iteration 9, loss = 1.61282383\n",
      "Iteration 10, loss = 1.57546901\n",
      "Iteration 11, loss = 1.53894153\n",
      "Iteration 12, loss = 1.50323608\n",
      "Iteration 13, loss = 1.46836897\n",
      "Iteration 14, loss = 1.43436061\n",
      "Iteration 15, loss = 1.40126340\n",
      "Iteration 16, loss = 1.36906430\n",
      "Iteration 17, loss = 1.33782019\n",
      "Iteration 18, loss = 1.30758383\n",
      "Iteration 19, loss = 1.27844544\n",
      "Iteration 20, loss = 1.25043352\n",
      "Iteration 21, loss = 1.22364446\n",
      "Iteration 22, loss = 1.19813967\n",
      "Iteration 23, loss = 1.17397847\n",
      "Iteration 24, loss = 1.15122802\n",
      "Iteration 25, loss = 1.12987500\n",
      "Iteration 26, loss = 1.10996813\n",
      "Iteration 27, loss = 1.09150251\n",
      "Iteration 28, loss = 1.07446544\n",
      "Iteration 29, loss = 1.05881366\n",
      "Iteration 30, loss = 1.04449221\n",
      "Iteration 31, loss = 1.03139868\n",
      "Iteration 32, loss = 1.01944946\n",
      "Iteration 33, loss = 1.00854664\n",
      "Iteration 34, loss = 0.99852832\n",
      "Iteration 35, loss = 0.98926911\n",
      "Iteration 36, loss = 0.98064174\n",
      "Iteration 37, loss = 0.97250386\n",
      "Iteration 38, loss = 0.96471685\n",
      "Iteration 39, loss = 0.95714802\n",
      "Iteration 40, loss = 0.94969529\n",
      "Iteration 41, loss = 0.94225679\n",
      "Iteration 42, loss = 0.93476594\n",
      "Iteration 43, loss = 0.92718691\n",
      "Iteration 44, loss = 0.91957167\n",
      "Iteration 45, loss = 0.91191092\n",
      "Iteration 46, loss = 0.90423249\n",
      "Iteration 47, loss = 0.89655111\n",
      "Iteration 48, loss = 0.88894509\n",
      "Iteration 49, loss = 0.88142332\n",
      "Iteration 50, loss = 0.87394037\n",
      "Iteration 51, loss = 0.86658641\n",
      "Iteration 52, loss = 0.85932518\n",
      "Iteration 53, loss = 0.85212408\n",
      "Iteration 54, loss = 0.84496899\n",
      "Iteration 55, loss = 0.83785458\n",
      "Iteration 56, loss = 0.83083209\n",
      "Iteration 57, loss = 0.82396451\n",
      "Iteration 58, loss = 0.81725308\n",
      "Iteration 59, loss = 0.81075373\n",
      "Iteration 60, loss = 0.80448135\n",
      "Iteration 61, loss = 0.79839765\n",
      "Iteration 62, loss = 0.79254260\n",
      "Iteration 63, loss = 0.78695224\n",
      "Iteration 64, loss = 0.78154479\n",
      "Iteration 65, loss = 0.77633695\n",
      "Iteration 66, loss = 0.77133363\n",
      "Iteration 67, loss = 0.76651703\n",
      "Iteration 68, loss = 0.76181350\n",
      "Iteration 69, loss = 0.75722279\n",
      "Iteration 70, loss = 0.75269895\n",
      "Iteration 71, loss = 0.74822069\n",
      "Iteration 72, loss = 0.74377036\n",
      "Iteration 73, loss = 0.73934253\n",
      "Iteration 74, loss = 0.73493063\n",
      "Iteration 75, loss = 0.73053744\n",
      "Iteration 76, loss = 0.72616949\n",
      "Iteration 77, loss = 0.72183605\n",
      "Iteration 78, loss = 0.71753094\n",
      "Iteration 79, loss = 0.71325682\n",
      "Iteration 80, loss = 0.70902241\n",
      "Iteration 81, loss = 0.70482512\n",
      "Iteration 82, loss = 0.70066747\n",
      "Iteration 83, loss = 0.69655636\n",
      "Iteration 84, loss = 0.69249737\n",
      "Iteration 85, loss = 0.68848274\n",
      "Iteration 86, loss = 0.68450920\n",
      "Iteration 87, loss = 0.68057471\n",
      "Iteration 88, loss = 0.67667598\n",
      "Iteration 89, loss = 0.67282282\n",
      "Iteration 90, loss = 0.66901646\n",
      "Iteration 91, loss = 0.66524529\n",
      "Iteration 92, loss = 0.66151104\n",
      "Iteration 93, loss = 0.65781603\n",
      "Iteration 94, loss = 0.65415973\n",
      "Iteration 95, loss = 0.65054179\n",
      "Iteration 96, loss = 0.64696034\n",
      "Iteration 97, loss = 0.64342923\n",
      "Iteration 98, loss = 0.63993994\n",
      "Iteration 99, loss = 0.63648800\n",
      "Iteration 100, loss = 0.63307357\n",
      "Iteration 101, loss = 0.62969974\n",
      "Iteration 102, loss = 0.62636549\n",
      "Iteration 103, loss = 0.62306715\n",
      "Iteration 104, loss = 0.61980375\n",
      "Iteration 105, loss = 0.61656584\n",
      "Iteration 106, loss = 0.61334942\n",
      "Iteration 107, loss = 0.61016280\n",
      "Iteration 108, loss = 0.60701021\n",
      "Iteration 109, loss = 0.60388620\n",
      "Iteration 110, loss = 0.60079880\n",
      "Iteration 111, loss = 0.59774429\n",
      "Iteration 112, loss = 0.59472760\n",
      "Iteration 113, loss = 0.59173429\n",
      "Iteration 114, loss = 0.58877039\n",
      "Iteration 115, loss = 0.58583368\n",
      "Iteration 116, loss = 0.58292597\n",
      "Iteration 117, loss = 0.58005574\n",
      "Iteration 118, loss = 0.57723339\n",
      "Iteration 119, loss = 0.57444097\n",
      "Iteration 120, loss = 0.57167857\n",
      "Iteration 121, loss = 0.56894631\n",
      "Iteration 122, loss = 0.56624173\n",
      "Iteration 123, loss = 0.56356709\n",
      "Iteration 124, loss = 0.56092755\n",
      "Iteration 125, loss = 0.55831687\n",
      "Iteration 126, loss = 0.55574614\n",
      "Iteration 127, loss = 0.55320946\n",
      "Iteration 128, loss = 0.55071274\n",
      "Iteration 129, loss = 0.54823913\n",
      "Iteration 130, loss = 0.54578810\n",
      "Iteration 131, loss = 0.54336859\n",
      "Iteration 132, loss = 0.54096848\n",
      "Iteration 133, loss = 0.53859012\n",
      "Iteration 134, loss = 0.53623723\n",
      "Iteration 135, loss = 0.53390381\n",
      "Iteration 136, loss = 0.53160171\n",
      "Iteration 137, loss = 0.52932996\n",
      "Iteration 138, loss = 0.52707702\n",
      "Iteration 139, loss = 0.52484548\n",
      "Iteration 140, loss = 0.52263734\n",
      "Iteration 141, loss = 0.52046593\n",
      "Iteration 142, loss = 0.51831160\n",
      "Iteration 143, loss = 0.51617501\n",
      "Iteration 144, loss = 0.51407197\n",
      "Iteration 145, loss = 0.51199779\n",
      "Iteration 146, loss = 0.50994975\n",
      "Iteration 147, loss = 0.50792135\n",
      "Iteration 148, loss = 0.50591533\n",
      "Iteration 149, loss = 0.50393267\n",
      "Iteration 150, loss = 0.50197040\n",
      "Iteration 151, loss = 0.50003088\n",
      "Iteration 152, loss = 0.49811598\n",
      "Iteration 153, loss = 0.49621943\n",
      "Iteration 154, loss = 0.49434245\n",
      "Iteration 155, loss = 0.49248889\n",
      "Iteration 156, loss = 0.49065490\n",
      "Iteration 157, loss = 0.48883665\n",
      "Iteration 158, loss = 0.48703426\n",
      "Iteration 159, loss = 0.48524764\n",
      "Iteration 160, loss = 0.48347728\n",
      "Iteration 161, loss = 0.48172291\n",
      "Iteration 162, loss = 0.47998361\n",
      "Iteration 163, loss = 0.47825893\n",
      "Iteration 164, loss = 0.47655267\n",
      "Iteration 165, loss = 0.47486132\n",
      "Iteration 166, loss = 0.47318400\n",
      "Iteration 167, loss = 0.47152018\n",
      "Iteration 168, loss = 0.46986962\n",
      "Iteration 169, loss = 0.46823209\n",
      "Iteration 170, loss = 0.46660734\n",
      "Iteration 171, loss = 0.46499515\n",
      "Iteration 172, loss = 0.46339546\n",
      "Iteration 173, loss = 0.46180773\n",
      "Iteration 174, loss = 0.46023189\n",
      "Iteration 175, loss = 0.45866833\n",
      "Iteration 176, loss = 0.45711670\n",
      "Iteration 177, loss = 0.45557636\n",
      "Iteration 178, loss = 0.45404734\n",
      "Iteration 179, loss = 0.45252924\n",
      "Iteration 180, loss = 0.45102202\n",
      "Iteration 181, loss = 0.44952518\n",
      "Iteration 182, loss = 0.44803868\n",
      "Iteration 183, loss = 0.44656220\n",
      "Iteration 184, loss = 0.44509595\n",
      "Iteration 185, loss = 0.44363969\n",
      "Iteration 186, loss = 0.44219334\n",
      "Iteration 187, loss = 0.44075741\n",
      "Iteration 188, loss = 0.43933139\n",
      "Iteration 189, loss = 0.43791470\n",
      "Iteration 190, loss = 0.43650656\n",
      "Iteration 191, loss = 0.43510737\n",
      "Iteration 192, loss = 0.43371692\n",
      "Iteration 193, loss = 0.43233507\n",
      "Iteration 194, loss = 0.43096172\n",
      "Iteration 195, loss = 0.42959678\n",
      "Iteration 196, loss = 0.42823989\n",
      "Iteration 197, loss = 0.42689078\n",
      "Iteration 198, loss = 0.42554952\n",
      "Iteration 199, loss = 0.42421606\n",
      "Iteration 200, loss = 0.42289020\n",
      "Iteration 201, loss = 0.42157180\n",
      "Iteration 202, loss = 0.42026074\n",
      "Iteration 203, loss = 0.41895721\n",
      "Iteration 204, loss = 0.41766081\n",
      "Iteration 205, loss = 0.41637124\n",
      "Iteration 206, loss = 0.41508843\n",
      "Iteration 207, loss = 0.41381244\n",
      "Iteration 208, loss = 0.41254316\n",
      "Iteration 209, loss = 0.41128018\n",
      "Iteration 210, loss = 0.41002396\n",
      "Iteration 211, loss = 0.40877419\n",
      "Iteration 212, loss = 0.40753128\n",
      "Iteration 213, loss = 0.40629485\n",
      "Iteration 214, loss = 0.40506444\n",
      "Iteration 215, loss = 0.40383982\n",
      "Iteration 216, loss = 0.40262104\n",
      "Iteration 217, loss = 0.40140827\n",
      "Iteration 218, loss = 0.40020134\n",
      "Iteration 219, loss = 0.39900039\n",
      "Iteration 220, loss = 0.39780587\n",
      "Iteration 221, loss = 0.39661722\n",
      "Iteration 222, loss = 0.39543401\n",
      "Iteration 223, loss = 0.39425622\n",
      "Iteration 224, loss = 0.39308390\n",
      "Iteration 225, loss = 0.39191700\n",
      "Iteration 226, loss = 0.39075548\n",
      "Iteration 227, loss = 0.38959908\n",
      "Iteration 228, loss = 0.38844786\n",
      "Iteration 229, loss = 0.38730179\n",
      "Iteration 230, loss = 0.38616096\n",
      "Iteration 231, loss = 0.38502516\n",
      "Iteration 232, loss = 0.38389434\n",
      "Iteration 233, loss = 0.38276845\n",
      "Iteration 234, loss = 0.38164744\n",
      "Iteration 235, loss = 0.38053133\n",
      "Iteration 236, loss = 0.37942006\n",
      "Iteration 237, loss = 0.37831368\n",
      "Iteration 238, loss = 0.37721208\n",
      "Iteration 239, loss = 0.37611536\n",
      "Iteration 240, loss = 0.37502365\n",
      "Iteration 241, loss = 0.37393648\n",
      "Iteration 242, loss = 0.37285377\n",
      "Iteration 243, loss = 0.37177525\n",
      "Iteration 244, loss = 0.37070093\n",
      "Iteration 245, loss = 0.36963087\n",
      "Iteration 246, loss = 0.36856516\n",
      "Iteration 247, loss = 0.36750373\n",
      "Iteration 248, loss = 0.36644649\n",
      "Iteration 249, loss = 0.36539342\n",
      "Iteration 250, loss = 0.36434446\n",
      "Iteration 251, loss = 0.36329962\n",
      "Iteration 252, loss = 0.36225879\n",
      "Iteration 253, loss = 0.36122189\n",
      "Iteration 254, loss = 0.36018889\n",
      "Iteration 255, loss = 0.35915993\n",
      "Iteration 256, loss = 0.35813488\n",
      "Iteration 257, loss = 0.35711351\n",
      "Iteration 258, loss = 0.35609592\n",
      "Iteration 259, loss = 0.35508226\n",
      "Iteration 260, loss = 0.35407233\n",
      "Iteration 261, loss = 0.35306609\n",
      "Iteration 262, loss = 0.35206407\n",
      "Iteration 263, loss = 0.35106631\n",
      "Iteration 264, loss = 0.35007224\n",
      "Iteration 265, loss = 0.34908229\n",
      "Iteration 266, loss = 0.34809635\n",
      "Iteration 267, loss = 0.34711434\n",
      "Iteration 268, loss = 0.34613553\n",
      "Iteration 269, loss = 0.34515990\n",
      "Iteration 270, loss = 0.34418762\n",
      "Iteration 271, loss = 0.34321898\n",
      "Iteration 272, loss = 0.34225402\n",
      "Iteration 273, loss = 0.34129269\n",
      "Iteration 274, loss = 0.34033485\n",
      "Iteration 275, loss = 0.33938042\n",
      "Iteration 276, loss = 0.33842948\n",
      "Iteration 277, loss = 0.33748236\n",
      "Iteration 278, loss = 0.33653858\n",
      "Iteration 279, loss = 0.33559815\n",
      "Iteration 280, loss = 0.33466107\n",
      "Iteration 281, loss = 0.33372723\n",
      "Iteration 282, loss = 0.33279686\n",
      "Iteration 283, loss = 0.33186975\n",
      "Iteration 284, loss = 0.33094611\n",
      "Iteration 285, loss = 0.33002592\n",
      "Iteration 286, loss = 0.32910906\n",
      "Iteration 287, loss = 0.32819553\n",
      "Iteration 288, loss = 0.32728529\n",
      "Iteration 289, loss = 0.32637829\n",
      "Iteration 290, loss = 0.32547450\n",
      "Iteration 291, loss = 0.32457395\n",
      "Iteration 292, loss = 0.32367661\n",
      "Iteration 293, loss = 0.32278271\n",
      "Iteration 294, loss = 0.32189192\n",
      "Iteration 295, loss = 0.32100425\n",
      "Iteration 296, loss = 0.32011971\n",
      "Iteration 297, loss = 0.31923813\n",
      "Iteration 298, loss = 0.31835965\n",
      "Iteration 299, loss = 0.31748450\n",
      "Iteration 300, loss = 0.31661245\n",
      "Iteration 301, loss = 0.31574395\n",
      "Iteration 302, loss = 0.31487845\n",
      "Iteration 303, loss = 0.31401593\n",
      "Iteration 304, loss = 0.31315639\n",
      "Iteration 305, loss = 0.31229982\n",
      "Iteration 306, loss = 0.31144623\n",
      "Iteration 307, loss = 0.31059573\n",
      "Iteration 308, loss = 0.30974816\n",
      "Iteration 309, loss = 0.30890340\n",
      "Iteration 310, loss = 0.30806178\n",
      "Iteration 311, loss = 0.30722308\n",
      "Iteration 312, loss = 0.30638734\n",
      "Iteration 313, loss = 0.30555448\n",
      "Iteration 314, loss = 0.30472454\n",
      "Iteration 315, loss = 0.30389752\n",
      "Iteration 316, loss = 0.30307337\n",
      "Iteration 317, loss = 0.30225208\n",
      "Iteration 318, loss = 0.30143374\n",
      "Iteration 319, loss = 0.30061814\n",
      "Iteration 320, loss = 0.29980560\n",
      "Iteration 321, loss = 0.29899596\n",
      "Iteration 322, loss = 0.29818912\n",
      "Iteration 323, loss = 0.29738507\n",
      "Iteration 324, loss = 0.29658384\n",
      "Iteration 325, loss = 0.29578543\n",
      "Iteration 326, loss = 0.29498981\n",
      "Iteration 327, loss = 0.29419696\n",
      "Iteration 328, loss = 0.29340687\n",
      "Iteration 329, loss = 0.29261961\n",
      "Iteration 330, loss = 0.29183500\n",
      "Iteration 331, loss = 0.29105333\n",
      "Iteration 332, loss = 0.29027438\n",
      "Iteration 333, loss = 0.28949815\n",
      "Iteration 334, loss = 0.28872463\n",
      "Iteration 335, loss = 0.28795389\n",
      "Iteration 336, loss = 0.28718578\n",
      "Iteration 337, loss = 0.28642046\n",
      "Iteration 338, loss = 0.28565793\n",
      "Iteration 339, loss = 0.28489805\n",
      "Iteration 340, loss = 0.28414083\n",
      "Iteration 341, loss = 0.28338629\n",
      "Iteration 342, loss = 0.28263444\n",
      "Iteration 343, loss = 0.28188527\n",
      "Iteration 344, loss = 0.28113881\n",
      "Iteration 345, loss = 0.28039496\n",
      "Iteration 346, loss = 0.27965380\n",
      "Iteration 347, loss = 0.27891536\n",
      "Iteration 348, loss = 0.27817943\n",
      "Iteration 349, loss = 0.27744628\n",
      "Iteration 350, loss = 0.27671576\n",
      "Iteration 351, loss = 0.27598787\n",
      "Iteration 352, loss = 0.27526268\n",
      "Iteration 353, loss = 0.27454012\n",
      "Iteration 354, loss = 0.27382019\n",
      "Iteration 355, loss = 0.27310286\n",
      "Iteration 356, loss = 0.27238813\n",
      "Iteration 357, loss = 0.27167599\n",
      "Iteration 358, loss = 0.27096663\n",
      "Iteration 359, loss = 0.27025949\n",
      "Iteration 360, loss = 0.26955514\n",
      "Iteration 361, loss = 0.26885332\n",
      "Iteration 362, loss = 0.26815405\n",
      "Iteration 363, loss = 0.26745735\n",
      "Iteration 364, loss = 0.26676323\n",
      "Iteration 365, loss = 0.26607168\n",
      "Iteration 366, loss = 0.26538264\n",
      "Iteration 367, loss = 0.26469616\n",
      "Iteration 368, loss = 0.26401221\n",
      "Iteration 369, loss = 0.26333076\n",
      "Iteration 370, loss = 0.26265184\n",
      "Iteration 371, loss = 0.26197540\n",
      "Iteration 372, loss = 0.26130147\n",
      "Iteration 373, loss = 0.26063003\n",
      "Iteration 374, loss = 0.25996109\n",
      "Iteration 375, loss = 0.25929466\n",
      "Iteration 376, loss = 0.25863084\n",
      "Iteration 377, loss = 0.25796949\n",
      "Iteration 378, loss = 0.25731057\n",
      "Iteration 379, loss = 0.25665405\n",
      "Iteration 380, loss = 0.25600007\n",
      "Iteration 381, loss = 0.25534855\n",
      "Iteration 382, loss = 0.25469944\n",
      "Iteration 383, loss = 0.25405274\n",
      "Iteration 384, loss = 0.25340846\n",
      "Iteration 385, loss = 0.25276659\n",
      "Iteration 386, loss = 0.25212713\n",
      "Iteration 387, loss = 0.25149008\n",
      "Iteration 388, loss = 0.25085549\n",
      "Iteration 389, loss = 0.25022331\n",
      "Iteration 390, loss = 0.24959352\n",
      "Iteration 391, loss = 0.24896617\n",
      "Iteration 392, loss = 0.24834165\n",
      "Iteration 393, loss = 0.24771967\n",
      "Iteration 394, loss = 0.24710011\n",
      "Iteration 395, loss = 0.24648295\n",
      "Iteration 396, loss = 0.24586819\n",
      "Iteration 397, loss = 0.24525586\n",
      "Iteration 398, loss = 0.24464595\n",
      "Iteration 399, loss = 0.24403842\n",
      "Iteration 400, loss = 0.24343323\n",
      "Iteration 401, loss = 0.24283038\n",
      "Iteration 402, loss = 0.24222986\n",
      "Iteration 403, loss = 0.24163164\n",
      "Iteration 404, loss = 0.24103490\n",
      "Iteration 405, loss = 0.24043374\n",
      "Iteration 406, loss = 0.23983208\n",
      "Iteration 407, loss = 0.23923086\n",
      "Iteration 408, loss = 0.23863106\n",
      "Iteration 409, loss = 0.23803232\n",
      "Iteration 410, loss = 0.23743381\n",
      "Iteration 411, loss = 0.23683507\n",
      "Iteration 412, loss = 0.23623577\n",
      "Iteration 413, loss = 0.23563559\n",
      "Iteration 414, loss = 0.23503430\n",
      "Iteration 415, loss = 0.23443177\n",
      "Iteration 416, loss = 0.23382791\n",
      "Iteration 417, loss = 0.23322269\n",
      "Iteration 418, loss = 0.23261615\n",
      "Iteration 419, loss = 0.23200822\n",
      "Iteration 420, loss = 0.23139895\n",
      "Iteration 421, loss = 0.23078839\n",
      "Iteration 422, loss = 0.23017650\n",
      "Iteration 423, loss = 0.22956329\n",
      "Iteration 424, loss = 0.22894878\n",
      "Iteration 425, loss = 0.22833298\n",
      "Iteration 426, loss = 0.22771606\n",
      "Iteration 427, loss = 0.22709806\n",
      "Iteration 428, loss = 0.22647921\n",
      "Iteration 429, loss = 0.22585958\n",
      "Iteration 430, loss = 0.22523934\n",
      "Iteration 431, loss = 0.22461856\n",
      "Iteration 432, loss = 0.22399704\n",
      "Iteration 433, loss = 0.22337487\n",
      "Iteration 434, loss = 0.22275186\n",
      "Iteration 435, loss = 0.22212830\n",
      "Iteration 436, loss = 0.22150439\n",
      "Iteration 437, loss = 0.22088025\n",
      "Iteration 438, loss = 0.22025593\n",
      "Iteration 439, loss = 0.21963142\n",
      "Iteration 440, loss = 0.21900692\n",
      "Iteration 441, loss = 0.21838221\n",
      "Iteration 442, loss = 0.21775756\n",
      "Iteration 443, loss = 0.21713306\n",
      "Iteration 444, loss = 0.21650932\n",
      "Iteration 445, loss = 0.21588641\n",
      "Iteration 446, loss = 0.21526365\n",
      "Iteration 447, loss = 0.21464126\n",
      "Iteration 448, loss = 0.21401987\n",
      "Iteration 449, loss = 0.21339943\n",
      "Iteration 450, loss = 0.21277948\n",
      "Iteration 451, loss = 0.21216001\n",
      "Iteration 452, loss = 0.21154147\n",
      "Iteration 453, loss = 0.21092433\n",
      "Iteration 454, loss = 0.21030834\n",
      "Iteration 455, loss = 0.20969358\n",
      "Iteration 456, loss = 0.20908010\n",
      "Iteration 457, loss = 0.20846814\n",
      "Iteration 458, loss = 0.20785774\n",
      "Iteration 459, loss = 0.20724875\n",
      "Iteration 460, loss = 0.20664131\n",
      "Iteration 461, loss = 0.20603526\n",
      "Iteration 462, loss = 0.20543098\n",
      "Iteration 463, loss = 0.20482848\n",
      "Iteration 464, loss = 0.20422768\n",
      "Iteration 465, loss = 0.20362857\n",
      "Iteration 466, loss = 0.20303138\n",
      "Iteration 467, loss = 0.20243612\n",
      "Iteration 468, loss = 0.20184270\n",
      "Iteration 469, loss = 0.20125105\n",
      "Iteration 470, loss = 0.20066123\n",
      "Iteration 471, loss = 0.20007356\n",
      "Iteration 472, loss = 0.19948790\n",
      "Iteration 473, loss = 0.19890418\n",
      "Iteration 474, loss = 0.19832271\n",
      "Iteration 475, loss = 0.19774360\n",
      "Iteration 476, loss = 0.19716707\n",
      "Iteration 477, loss = 0.19659287\n",
      "Iteration 478, loss = 0.19602129\n",
      "Iteration 479, loss = 0.19545187\n",
      "Iteration 480, loss = 0.19488434\n",
      "Iteration 481, loss = 0.19431941\n",
      "Iteration 482, loss = 0.19375729\n",
      "Iteration 483, loss = 0.19319735\n",
      "Iteration 484, loss = 0.19264000\n",
      "Iteration 485, loss = 0.19208541\n",
      "Iteration 486, loss = 0.19153304\n",
      "Iteration 487, loss = 0.19098344\n",
      "Iteration 488, loss = 0.19043635\n",
      "Iteration 489, loss = 0.18989180\n",
      "Iteration 490, loss = 0.18934976\n",
      "Iteration 491, loss = 0.18881014\n",
      "Iteration 492, loss = 0.18827287\n",
      "Iteration 493, loss = 0.18773835\n",
      "Iteration 494, loss = 0.18720628\n",
      "Iteration 495, loss = 0.18667662\n",
      "Iteration 496, loss = 0.18614939\n",
      "Iteration 497, loss = 0.18562454\n",
      "Iteration 498, loss = 0.18510246\n",
      "Iteration 499, loss = 0.18458310\n",
      "Iteration 500, loss = 0.18406605\n",
      "Iteration 501, loss = 0.18355143\n",
      "Iteration 502, loss = 0.18303912\n",
      "Iteration 503, loss = 0.18252929\n",
      "Iteration 504, loss = 0.18202265\n",
      "Iteration 505, loss = 0.18151840\n",
      "Iteration 506, loss = 0.18101692\n",
      "Iteration 507, loss = 0.18051822\n",
      "Iteration 508, loss = 0.18002183\n",
      "Iteration 509, loss = 0.17952761\n",
      "Iteration 510, loss = 0.17903643\n",
      "Iteration 511, loss = 0.17854773\n",
      "Iteration 512, loss = 0.17806140\n",
      "Iteration 513, loss = 0.17757767\n",
      "Iteration 514, loss = 0.17709661\n",
      "Iteration 515, loss = 0.17661804\n",
      "Iteration 516, loss = 0.17614198\n",
      "Iteration 517, loss = 0.17566851\n",
      "Iteration 518, loss = 0.17519767\n",
      "Iteration 519, loss = 0.17472935\n",
      "Iteration 520, loss = 0.17426343\n",
      "Iteration 521, loss = 0.17380005\n",
      "Iteration 522, loss = 0.17333910\n",
      "Iteration 523, loss = 0.17288046\n",
      "Iteration 524, loss = 0.17242411\n",
      "Iteration 525, loss = 0.17197011\n",
      "Iteration 526, loss = 0.17151839\n",
      "Iteration 527, loss = 0.17106902\n",
      "Iteration 528, loss = 0.17062232\n",
      "Iteration 529, loss = 0.17017806\n",
      "Iteration 530, loss = 0.16973604\n",
      "Iteration 531, loss = 0.16929602\n",
      "Iteration 532, loss = 0.16885820\n",
      "Iteration 533, loss = 0.16842266\n",
      "Iteration 534, loss = 0.16798920\n",
      "Iteration 535, loss = 0.16755780\n",
      "Iteration 536, loss = 0.16712853\n",
      "Iteration 537, loss = 0.16670141\n",
      "Iteration 538, loss = 0.16627619\n",
      "Iteration 539, loss = 0.16585381\n",
      "Iteration 540, loss = 0.16543388\n",
      "Iteration 541, loss = 0.16501597\n",
      "Iteration 542, loss = 0.16460012\n",
      "Iteration 543, loss = 0.16418568\n",
      "Iteration 544, loss = 0.16377347\n",
      "Iteration 545, loss = 0.16336257\n",
      "Iteration 546, loss = 0.16295324\n",
      "Iteration 547, loss = 0.16254535\n",
      "Iteration 548, loss = 0.16213823\n",
      "Iteration 549, loss = 0.16173269\n",
      "Iteration 550, loss = 0.16132780\n",
      "Iteration 551, loss = 0.16092343\n",
      "Iteration 552, loss = 0.16051988\n",
      "Iteration 553, loss = 0.16011669\n",
      "Iteration 554, loss = 0.15971347\n",
      "Iteration 555, loss = 0.15931004\n",
      "Iteration 556, loss = 0.15890662\n",
      "Iteration 557, loss = 0.15850310\n",
      "Iteration 558, loss = 0.15809919\n",
      "Iteration 559, loss = 0.15769483\n",
      "Iteration 560, loss = 0.15729003\n",
      "Iteration 561, loss = 0.15688488\n",
      "Iteration 562, loss = 0.15647913\n",
      "Iteration 563, loss = 0.15607227\n",
      "Iteration 564, loss = 0.15566392\n",
      "Iteration 565, loss = 0.15525356\n",
      "Iteration 566, loss = 0.15484225\n",
      "Iteration 567, loss = 0.15442935\n",
      "Iteration 568, loss = 0.15401392\n",
      "Iteration 569, loss = 0.15359713\n",
      "Iteration 570, loss = 0.15317803\n",
      "Iteration 571, loss = 0.15275630\n",
      "Iteration 572, loss = 0.15233189\n",
      "Iteration 573, loss = 0.15190518\n",
      "Iteration 574, loss = 0.15147648\n",
      "Iteration 575, loss = 0.15104626\n",
      "Iteration 576, loss = 0.15061506\n",
      "Iteration 577, loss = 0.15018286\n",
      "Iteration 578, loss = 0.14974926\n",
      "Iteration 579, loss = 0.14931448\n",
      "Iteration 580, loss = 0.14887895\n",
      "Iteration 581, loss = 0.14844294\n",
      "Iteration 582, loss = 0.14800643\n",
      "Iteration 583, loss = 0.14756972\n",
      "Iteration 584, loss = 0.14713300\n",
      "Iteration 585, loss = 0.14669642\n",
      "Iteration 586, loss = 0.14626006\n",
      "Iteration 587, loss = 0.14582400\n",
      "Iteration 588, loss = 0.14538844\n",
      "Iteration 589, loss = 0.14495378\n",
      "Iteration 590, loss = 0.14451988\n",
      "Iteration 591, loss = 0.14408685\n",
      "Iteration 592, loss = 0.14365486\n",
      "Iteration 593, loss = 0.14322407\n",
      "Iteration 594, loss = 0.14279456\n",
      "Iteration 595, loss = 0.14236641\n",
      "Iteration 596, loss = 0.14193971\n",
      "Iteration 597, loss = 0.14151455\n",
      "Iteration 598, loss = 0.14109100\n",
      "Iteration 599, loss = 0.14066919\n",
      "Iteration 600, loss = 0.14024919\n",
      "Iteration 601, loss = 0.13983103\n",
      "Iteration 602, loss = 0.13941475\n",
      "Iteration 603, loss = 0.13900046\n",
      "Iteration 604, loss = 0.13858820\n",
      "Iteration 605, loss = 0.13817801\n",
      "Iteration 606, loss = 0.13776997\n",
      "Iteration 607, loss = 0.13736410\n",
      "Iteration 608, loss = 0.13696047\n",
      "Iteration 609, loss = 0.13655910\n",
      "Iteration 610, loss = 0.13616011\n",
      "Iteration 611, loss = 0.13576348\n",
      "Iteration 612, loss = 0.13536922\n",
      "Iteration 613, loss = 0.13497737\n",
      "Iteration 614, loss = 0.13458796\n",
      "Iteration 615, loss = 0.13420101\n",
      "Iteration 616, loss = 0.13381660\n",
      "Iteration 617, loss = 0.13343471\n",
      "Iteration 618, loss = 0.13305534\n",
      "Iteration 619, loss = 0.13267848\n",
      "Iteration 620, loss = 0.13230414\n",
      "Iteration 621, loss = 0.13193234\n",
      "Iteration 622, loss = 0.13156316\n",
      "Iteration 623, loss = 0.13119659\n",
      "Iteration 624, loss = 0.13083259\n",
      "Iteration 625, loss = 0.13047137\n",
      "Iteration 626, loss = 0.13011277\n",
      "Iteration 627, loss = 0.12975705\n",
      "Iteration 628, loss = 0.12940393\n",
      "Iteration 629, loss = 0.12905342\n",
      "Iteration 630, loss = 0.12870551\n",
      "Iteration 631, loss = 0.12836021\n",
      "Iteration 632, loss = 0.12801751\n",
      "Iteration 633, loss = 0.12767740\n",
      "Iteration 634, loss = 0.12733989\n",
      "Iteration 635, loss = 0.12700497\n",
      "Iteration 636, loss = 0.12667261\n",
      "Iteration 637, loss = 0.12634283\n",
      "Iteration 638, loss = 0.12601560\n",
      "Iteration 639, loss = 0.12569091\n",
      "Iteration 640, loss = 0.12536879\n",
      "Iteration 641, loss = 0.12504932\n",
      "Iteration 642, loss = 0.12473235\n",
      "Iteration 643, loss = 0.12441787\n",
      "Iteration 644, loss = 0.12410588\n",
      "Iteration 645, loss = 0.12379634\n",
      "Iteration 646, loss = 0.12348932\n",
      "Iteration 647, loss = 0.12318482\n",
      "Iteration 648, loss = 0.12288273\n",
      "Iteration 649, loss = 0.12258303\n",
      "Iteration 650, loss = 0.12228570\n",
      "Iteration 651, loss = 0.12199079\n",
      "Iteration 652, loss = 0.12169832\n",
      "Iteration 653, loss = 0.12140824\n",
      "Iteration 654, loss = 0.12112058\n",
      "Iteration 655, loss = 0.12083539\n",
      "Iteration 656, loss = 0.12055246\n",
      "Iteration 657, loss = 0.12027166\n",
      "Iteration 658, loss = 0.11999282\n",
      "Iteration 659, loss = 0.11971645\n",
      "Iteration 660, loss = 0.11944158\n",
      "Iteration 661, loss = 0.11916939\n",
      "Iteration 662, loss = 0.11889853\n",
      "Iteration 663, loss = 0.11862995\n",
      "Iteration 664, loss = 0.11836230\n",
      "Iteration 665, loss = 0.11809659\n",
      "Iteration 666, loss = 0.11783204\n",
      "Iteration 667, loss = 0.11756873\n",
      "Iteration 668, loss = 0.11730641\n",
      "Iteration 669, loss = 0.11704499\n",
      "Iteration 670, loss = 0.11678444\n",
      "Iteration 671, loss = 0.11652449\n",
      "Iteration 672, loss = 0.11626528\n",
      "Iteration 673, loss = 0.11600668\n",
      "Iteration 674, loss = 0.11574866\n",
      "Iteration 675, loss = 0.11549123\n",
      "Iteration 676, loss = 0.11523432\n",
      "Iteration 677, loss = 0.11497802\n",
      "Iteration 678, loss = 0.11472233\n",
      "Iteration 679, loss = 0.11446737\n",
      "Iteration 680, loss = 0.11421285\n",
      "Iteration 681, loss = 0.11395936\n",
      "Iteration 682, loss = 0.11370630\n",
      "Iteration 683, loss = 0.11345439\n",
      "Iteration 684, loss = 0.11320311\n",
      "Iteration 685, loss = 0.11295324\n",
      "Iteration 686, loss = 0.11270467\n",
      "Iteration 687, loss = 0.11245680\n",
      "Iteration 688, loss = 0.11221023\n",
      "Iteration 689, loss = 0.11196478\n",
      "Iteration 690, loss = 0.11172022\n",
      "Iteration 691, loss = 0.11147709\n",
      "Iteration 692, loss = 0.11123530\n",
      "Iteration 693, loss = 0.11099483\n",
      "Iteration 694, loss = 0.11075578\n",
      "Iteration 695, loss = 0.11051767\n",
      "Iteration 696, loss = 0.11028196\n",
      "Iteration 697, loss = 0.11004662\n",
      "Iteration 698, loss = 0.10981258\n",
      "Iteration 699, loss = 0.10958052\n",
      "Iteration 700, loss = 0.10934974\n",
      "Iteration 701, loss = 0.10912011\n",
      "Iteration 702, loss = 0.10889279\n",
      "Iteration 703, loss = 0.10866653\n",
      "Iteration 704, loss = 0.10844102\n",
      "Iteration 705, loss = 0.10821808\n",
      "Iteration 706, loss = 0.10799666\n",
      "Iteration 707, loss = 0.10777630\n",
      "Iteration 708, loss = 0.10755709\n",
      "Iteration 709, loss = 0.10734063\n",
      "Iteration 710, loss = 0.10712561\n",
      "Iteration 711, loss = 0.10691133\n",
      "Iteration 712, loss = 0.10669861\n",
      "Iteration 713, loss = 0.10648868\n",
      "Iteration 714, loss = 0.10627982\n",
      "Iteration 715, loss = 0.10607210\n",
      "Iteration 716, loss = 0.10586660\n",
      "Iteration 717, loss = 0.10566320\n",
      "Iteration 718, loss = 0.10546083\n",
      "Iteration 719, loss = 0.10525992\n",
      "Iteration 720, loss = 0.10506146\n",
      "Iteration 721, loss = 0.10486445\n",
      "Iteration 722, loss = 0.10466864\n",
      "Iteration 723, loss = 0.10447532\n",
      "Iteration 724, loss = 0.10428388\n",
      "Iteration 725, loss = 0.10409393\n",
      "Iteration 726, loss = 0.10390555\n",
      "Iteration 727, loss = 0.10371887\n",
      "Iteration 728, loss = 0.10353384\n",
      "Iteration 729, loss = 0.10335023\n",
      "Iteration 730, loss = 0.10316793\n",
      "Iteration 731, loss = 0.10298715\n",
      "Iteration 732, loss = 0.10280786\n",
      "Iteration 733, loss = 0.10263016\n",
      "Iteration 734, loss = 0.10245354\n",
      "Iteration 735, loss = 0.10227793\n",
      "Iteration 736, loss = 0.10210346\n",
      "Iteration 737, loss = 0.10193170\n",
      "Iteration 738, loss = 0.10175924\n",
      "Iteration 739, loss = 0.10158995\n",
      "Iteration 740, loss = 0.10142155\n",
      "Iteration 741, loss = 0.10125394\n",
      "Iteration 742, loss = 0.10108699\n",
      "Iteration 743, loss = 0.10092064\n",
      "Iteration 744, loss = 0.10075483\n",
      "Iteration 745, loss = 0.10058983\n",
      "Iteration 746, loss = 0.10042642\n",
      "Iteration 747, loss = 0.10026488\n",
      "Iteration 748, loss = 0.10010297\n",
      "Iteration 749, loss = 0.09994297\n",
      "Iteration 750, loss = 0.09978419\n",
      "Iteration 751, loss = 0.09962582\n",
      "Iteration 752, loss = 0.09946781\n",
      "Iteration 753, loss = 0.09931093\n",
      "Iteration 754, loss = 0.09915478\n",
      "Iteration 755, loss = 0.09899957\n",
      "Iteration 756, loss = 0.09884508\n",
      "Iteration 757, loss = 0.09869129\n",
      "Iteration 758, loss = 0.09853803\n",
      "Iteration 759, loss = 0.09838544\n",
      "Iteration 760, loss = 0.09823357\n",
      "Iteration 761, loss = 0.09808237\n",
      "Iteration 762, loss = 0.09793190\n",
      "Iteration 763, loss = 0.09778216\n",
      "Iteration 764, loss = 0.09763314\n",
      "Iteration 765, loss = 0.09748483\n",
      "Iteration 766, loss = 0.09733716\n",
      "Iteration 767, loss = 0.09719029\n",
      "Iteration 768, loss = 0.09704414\n",
      "Iteration 769, loss = 0.09689870\n",
      "Iteration 770, loss = 0.09675401\n",
      "Iteration 771, loss = 0.09660998\n",
      "Iteration 772, loss = 0.09646686\n",
      "Iteration 773, loss = 0.09632448\n",
      "Iteration 774, loss = 0.09618283\n",
      "Iteration 775, loss = 0.09604206\n",
      "Iteration 776, loss = 0.09590210\n",
      "Iteration 777, loss = 0.09576292\n",
      "Iteration 778, loss = 0.09562465\n",
      "Iteration 779, loss = 0.09548724\n",
      "Iteration 780, loss = 0.09535068\n",
      "Iteration 781, loss = 0.09521503\n",
      "Iteration 782, loss = 0.09508030\n",
      "Iteration 783, loss = 0.09494647\n",
      "Iteration 784, loss = 0.09481347\n",
      "Iteration 785, loss = 0.09468148\n",
      "Iteration 786, loss = 0.09455040\n",
      "Iteration 787, loss = 0.09442020\n",
      "Iteration 788, loss = 0.09429092\n",
      "Iteration 789, loss = 0.09416259\n",
      "Iteration 790, loss = 0.09403522\n",
      "Iteration 791, loss = 0.09390886\n",
      "Iteration 792, loss = 0.09378341\n",
      "Iteration 793, loss = 0.09365896\n",
      "Iteration 794, loss = 0.09353550\n",
      "Iteration 795, loss = 0.09341300\n",
      "Iteration 796, loss = 0.09329145\n",
      "Iteration 797, loss = 0.09317087\n",
      "Iteration 798, loss = 0.09305126\n",
      "Iteration 799, loss = 0.09293260\n",
      "Iteration 800, loss = 0.09281497\n",
      "Iteration 801, loss = 0.09269832\n",
      "Iteration 802, loss = 0.09258263\n",
      "Iteration 803, loss = 0.09246794\n",
      "Iteration 804, loss = 0.09235417\n",
      "Iteration 805, loss = 0.09224140\n",
      "Iteration 806, loss = 0.09212954\n",
      "Iteration 807, loss = 0.09201875\n",
      "Iteration 808, loss = 0.09190884\n",
      "Iteration 809, loss = 0.09179995\n",
      "Iteration 810, loss = 0.09169199\n",
      "Iteration 811, loss = 0.09158499\n",
      "Iteration 812, loss = 0.09147893\n",
      "Iteration 813, loss = 0.09137378\n",
      "Iteration 814, loss = 0.09126961\n",
      "Iteration 815, loss = 0.09116639\n",
      "Iteration 816, loss = 0.09106409\n",
      "Iteration 817, loss = 0.09096270\n",
      "Iteration 818, loss = 0.09086221\n",
      "Iteration 819, loss = 0.09076261\n",
      "Iteration 820, loss = 0.09066394\n",
      "Iteration 821, loss = 0.09056617\n",
      "Iteration 822, loss = 0.09046931\n",
      "Iteration 823, loss = 0.09037333\n",
      "Iteration 824, loss = 0.09027821\n",
      "Iteration 825, loss = 0.09018398\n",
      "Iteration 826, loss = 0.09009063\n",
      "Iteration 827, loss = 0.08999812\n",
      "Iteration 828, loss = 0.08990646\n",
      "Iteration 829, loss = 0.08981567\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91716398\n",
      "Iteration 2, loss = 1.87313059\n",
      "Iteration 3, loss = 1.81385433\n",
      "Iteration 4, loss = 1.74399279\n",
      "Iteration 5, loss = 1.66770202\n",
      "Iteration 6, loss = 1.58837376\n",
      "Iteration 7, loss = 1.50859797\n",
      "Iteration 8, loss = 1.43062368\n",
      "Iteration 9, loss = 1.35639573\n",
      "Iteration 10, loss = 1.28796397\n",
      "Iteration 11, loss = 1.22729996\n",
      "Iteration 12, loss = 1.17623069\n",
      "Iteration 13, loss = 1.13604701\n",
      "Iteration 14, loss = 1.10700034\n",
      "Iteration 15, loss = 1.08805499\n",
      "Iteration 16, loss = 1.07716720\n",
      "Iteration 17, loss = 1.07140957\n",
      "Iteration 18, loss = 1.06801160\n",
      "Iteration 19, loss = 1.06454328\n",
      "Iteration 20, loss = 1.05937107\n",
      "Iteration 21, loss = 1.05161786\n",
      "Iteration 22, loss = 1.04127466\n",
      "Iteration 23, loss = 1.02887273\n",
      "Iteration 24, loss = 1.01525435\n",
      "Iteration 25, loss = 1.00120496\n",
      "Iteration 26, loss = 0.98740206\n",
      "Iteration 27, loss = 0.97426639\n",
      "Iteration 28, loss = 0.96196205\n",
      "Iteration 29, loss = 0.95032633\n",
      "Iteration 30, loss = 0.93922248\n",
      "Iteration 31, loss = 0.92844711\n",
      "Iteration 32, loss = 0.91772250\n",
      "Iteration 33, loss = 0.90681174\n",
      "Iteration 34, loss = 0.89575511\n",
      "Iteration 35, loss = 0.88448211\n",
      "Iteration 36, loss = 0.87304956\n",
      "Iteration 37, loss = 0.86175182\n",
      "Iteration 38, loss = 0.85095307\n",
      "Iteration 39, loss = 0.84080095\n",
      "Iteration 40, loss = 0.83138924\n",
      "Iteration 41, loss = 0.82256944\n",
      "Iteration 42, loss = 0.81432509\n",
      "Iteration 43, loss = 0.80667385\n",
      "Iteration 44, loss = 0.79948590\n",
      "Iteration 45, loss = 0.79251995\n",
      "Iteration 46, loss = 0.78567401\n",
      "Iteration 47, loss = 0.77895334\n",
      "Iteration 48, loss = 0.77225296\n",
      "Iteration 49, loss = 0.76563137\n",
      "Iteration 50, loss = 0.75914668\n",
      "Iteration 51, loss = 0.75287454\n",
      "Iteration 52, loss = 0.74676296\n",
      "Iteration 53, loss = 0.74079107\n",
      "Iteration 54, loss = 0.73501883\n",
      "Iteration 55, loss = 0.72951325\n",
      "Iteration 56, loss = 0.72424516\n",
      "Iteration 57, loss = 0.71923496\n",
      "Iteration 58, loss = 0.71440690\n",
      "Iteration 59, loss = 0.70967573\n",
      "Iteration 60, loss = 0.70504079\n",
      "Iteration 61, loss = 0.70048445\n",
      "Iteration 62, loss = 0.69603236\n",
      "Iteration 63, loss = 0.69168514\n",
      "Iteration 64, loss = 0.68742046\n",
      "Iteration 65, loss = 0.68323648\n",
      "Iteration 66, loss = 0.67912717\n",
      "Iteration 67, loss = 0.67512017\n",
      "Iteration 68, loss = 0.67124067\n",
      "Iteration 69, loss = 0.66745088\n",
      "Iteration 70, loss = 0.66375420\n",
      "Iteration 71, loss = 0.66014505\n",
      "Iteration 72, loss = 0.65659702\n",
      "Iteration 73, loss = 0.65311019\n",
      "Iteration 74, loss = 0.64968122\n",
      "Iteration 75, loss = 0.64632374\n",
      "Iteration 76, loss = 0.64301924\n",
      "Iteration 77, loss = 0.63976109\n",
      "Iteration 78, loss = 0.63654892\n",
      "Iteration 79, loss = 0.63338785\n",
      "Iteration 80, loss = 0.63027351\n",
      "Iteration 81, loss = 0.62721401\n",
      "Iteration 82, loss = 0.62420645\n",
      "Iteration 83, loss = 0.62124546\n",
      "Iteration 84, loss = 0.61833369\n",
      "Iteration 85, loss = 0.61546508\n",
      "Iteration 86, loss = 0.61264004\n",
      "Iteration 87, loss = 0.60985652\n",
      "Iteration 88, loss = 0.60711409\n",
      "Iteration 89, loss = 0.60441033\n",
      "Iteration 90, loss = 0.60174454\n",
      "Iteration 91, loss = 0.59911498\n",
      "Iteration 92, loss = 0.59652071\n",
      "Iteration 93, loss = 0.59396089\n",
      "Iteration 94, loss = 0.59143475\n",
      "Iteration 95, loss = 0.58894160\n",
      "Iteration 96, loss = 0.58648080\n",
      "Iteration 97, loss = 0.58405217\n",
      "Iteration 98, loss = 0.58165439\n",
      "Iteration 99, loss = 0.57928838\n",
      "Iteration 100, loss = 0.57695247\n",
      "Iteration 101, loss = 0.57464589\n",
      "Iteration 102, loss = 0.57236761\n",
      "Iteration 103, loss = 0.57011795\n",
      "Iteration 104, loss = 0.56789664\n",
      "Iteration 105, loss = 0.56570333\n",
      "Iteration 106, loss = 0.56353729\n",
      "Iteration 107, loss = 0.56139798\n",
      "Iteration 108, loss = 0.55928495\n",
      "Iteration 109, loss = 0.55719771\n",
      "Iteration 110, loss = 0.55513577\n",
      "Iteration 111, loss = 0.55309868\n",
      "Iteration 112, loss = 0.55108599\n",
      "Iteration 113, loss = 0.54909732\n",
      "Iteration 114, loss = 0.54713237\n",
      "Iteration 115, loss = 0.54519049\n",
      "Iteration 116, loss = 0.54327125\n",
      "Iteration 117, loss = 0.54137422\n",
      "Iteration 118, loss = 0.53949898\n",
      "Iteration 119, loss = 0.53764683\n",
      "Iteration 120, loss = 0.53581640\n",
      "Iteration 121, loss = 0.53400670\n",
      "Iteration 122, loss = 0.53221735\n",
      "Iteration 123, loss = 0.53044893\n",
      "Iteration 124, loss = 0.52870028\n",
      "Iteration 125, loss = 0.52697096\n",
      "Iteration 126, loss = 0.52526066\n",
      "Iteration 127, loss = 0.52356907\n",
      "Iteration 128, loss = 0.52189589\n",
      "Iteration 129, loss = 0.52024089\n",
      "Iteration 130, loss = 0.51860369\n",
      "Iteration 131, loss = 0.51698394\n",
      "Iteration 132, loss = 0.51538139\n",
      "Iteration 133, loss = 0.51379572\n",
      "Iteration 134, loss = 0.51222665\n",
      "Iteration 135, loss = 0.51067466\n",
      "Iteration 136, loss = 0.50913905\n",
      "Iteration 137, loss = 0.50761928\n",
      "Iteration 138, loss = 0.50611568\n",
      "Iteration 139, loss = 0.50462757\n",
      "Iteration 140, loss = 0.50315495\n",
      "Iteration 141, loss = 0.50169708\n",
      "Iteration 142, loss = 0.50025369\n",
      "Iteration 143, loss = 0.49882510\n",
      "Iteration 144, loss = 0.49741041\n",
      "Iteration 145, loss = 0.49600947\n",
      "Iteration 146, loss = 0.49462209\n",
      "Iteration 147, loss = 0.49324822\n",
      "Iteration 148, loss = 0.49188789\n",
      "Iteration 149, loss = 0.49054052\n",
      "Iteration 150, loss = 0.48920589\n",
      "Iteration 151, loss = 0.48788412\n",
      "Iteration 152, loss = 0.48657444\n",
      "Iteration 153, loss = 0.48527693\n",
      "Iteration 154, loss = 0.48399154\n",
      "Iteration 155, loss = 0.48271791\n",
      "Iteration 156, loss = 0.48145577\n",
      "Iteration 157, loss = 0.48020493\n",
      "Iteration 158, loss = 0.47896522\n",
      "Iteration 159, loss = 0.47773644\n",
      "Iteration 160, loss = 0.47651881\n",
      "Iteration 161, loss = 0.47531179\n",
      "Iteration 162, loss = 0.47411526\n",
      "Iteration 163, loss = 0.47292899\n",
      "Iteration 164, loss = 0.47175275\n",
      "Iteration 165, loss = 0.47058640\n",
      "Iteration 166, loss = 0.46942989\n",
      "Iteration 167, loss = 0.46828285\n",
      "Iteration 168, loss = 0.46714550\n",
      "Iteration 169, loss = 0.46601711\n",
      "Iteration 170, loss = 0.46489793\n",
      "Iteration 171, loss = 0.46378778\n",
      "Iteration 172, loss = 0.46268640\n",
      "Iteration 173, loss = 0.46159375\n",
      "Iteration 174, loss = 0.46050962\n",
      "Iteration 175, loss = 0.45943387\n",
      "Iteration 176, loss = 0.45836644\n",
      "Iteration 177, loss = 0.45730711\n",
      "Iteration 178, loss = 0.45625578\n",
      "Iteration 179, loss = 0.45521234\n",
      "Iteration 180, loss = 0.45417669\n",
      "Iteration 181, loss = 0.45314865\n",
      "Iteration 182, loss = 0.45212810\n",
      "Iteration 183, loss = 0.45111495\n",
      "Iteration 184, loss = 0.45010911\n",
      "Iteration 185, loss = 0.44911063\n",
      "Iteration 186, loss = 0.44811925\n",
      "Iteration 187, loss = 0.44713594\n",
      "Iteration 188, loss = 0.44615958\n",
      "Iteration 189, loss = 0.44519002\n",
      "Iteration 190, loss = 0.44422720\n",
      "Iteration 191, loss = 0.44327107\n",
      "Iteration 192, loss = 0.44232145\n",
      "Iteration 193, loss = 0.44137903\n",
      "Iteration 194, loss = 0.44044272\n",
      "Iteration 195, loss = 0.43951286\n",
      "Iteration 196, loss = 0.43858922\n",
      "Iteration 197, loss = 0.43767169\n",
      "Iteration 198, loss = 0.43676019\n",
      "Iteration 199, loss = 0.43585463\n",
      "Iteration 200, loss = 0.43495493\n",
      "Iteration 201, loss = 0.43406108\n",
      "Iteration 202, loss = 0.43317287\n",
      "Iteration 203, loss = 0.43229030\n",
      "Iteration 204, loss = 0.43141331\n",
      "Iteration 205, loss = 0.43054174\n",
      "Iteration 206, loss = 0.42967555\n",
      "Iteration 207, loss = 0.42881473\n",
      "Iteration 208, loss = 0.42795909\n",
      "Iteration 209, loss = 0.42710862\n",
      "Iteration 210, loss = 0.42626332\n",
      "Iteration 211, loss = 0.42542307\n",
      "Iteration 212, loss = 0.42458791\n",
      "Iteration 213, loss = 0.42375803\n",
      "Iteration 214, loss = 0.42293292\n",
      "Iteration 215, loss = 0.42211256\n",
      "Iteration 216, loss = 0.42129689\n",
      "Iteration 217, loss = 0.42048588\n",
      "Iteration 218, loss = 0.41967948\n",
      "Iteration 219, loss = 0.41887780\n",
      "Iteration 220, loss = 0.41808068\n",
      "Iteration 221, loss = 0.41728786\n",
      "Iteration 222, loss = 0.41649964\n",
      "Iteration 223, loss = 0.41571560\n",
      "Iteration 224, loss = 0.41493591\n",
      "Iteration 225, loss = 0.41416059\n",
      "Iteration 226, loss = 0.41338929\n",
      "Iteration 227, loss = 0.41262218\n",
      "Iteration 228, loss = 0.41185922\n",
      "Iteration 229, loss = 0.41110030\n",
      "Iteration 230, loss = 0.41034553\n",
      "Iteration 231, loss = 0.40959486\n",
      "Iteration 232, loss = 0.40884810\n",
      "Iteration 233, loss = 0.40810528\n",
      "Iteration 234, loss = 0.40736633\n",
      "Iteration 235, loss = 0.40663118\n",
      "Iteration 236, loss = 0.40589969\n",
      "Iteration 237, loss = 0.40517206\n",
      "Iteration 238, loss = 0.40444830\n",
      "Iteration 239, loss = 0.40372815\n",
      "Iteration 240, loss = 0.40301163\n",
      "Iteration 241, loss = 0.40229874\n",
      "Iteration 242, loss = 0.40158962\n",
      "Iteration 243, loss = 0.40088397\n",
      "Iteration 244, loss = 0.40018175\n",
      "Iteration 245, loss = 0.39948292\n",
      "Iteration 246, loss = 0.39878743\n",
      "Iteration 247, loss = 0.39809524\n",
      "Iteration 248, loss = 0.39740632\n",
      "Iteration 249, loss = 0.39672074\n",
      "Iteration 250, loss = 0.39603846\n",
      "Iteration 251, loss = 0.39535935\n",
      "Iteration 252, loss = 0.39468334\n",
      "Iteration 253, loss = 0.39401042\n",
      "Iteration 254, loss = 0.39334057\n",
      "Iteration 255, loss = 0.39267378\n",
      "Iteration 256, loss = 0.39200997\n",
      "Iteration 257, loss = 0.39134911\n",
      "Iteration 258, loss = 0.39069116\n",
      "Iteration 259, loss = 0.39003608\n",
      "Iteration 260, loss = 0.38938385\n",
      "Iteration 261, loss = 0.38873444\n",
      "Iteration 262, loss = 0.38808781\n",
      "Iteration 263, loss = 0.38744393\n",
      "Iteration 264, loss = 0.38680277\n",
      "Iteration 265, loss = 0.38616434\n",
      "Iteration 266, loss = 0.38552859\n",
      "Iteration 267, loss = 0.38489547\n",
      "Iteration 268, loss = 0.38426497\n",
      "Iteration 269, loss = 0.38363706\n",
      "Iteration 270, loss = 0.38301173\n",
      "Iteration 271, loss = 0.38238893\n",
      "Iteration 272, loss = 0.38176864\n",
      "Iteration 273, loss = 0.38115083\n",
      "Iteration 274, loss = 0.38053570\n",
      "Iteration 275, loss = 0.37992308\n",
      "Iteration 276, loss = 0.37931288\n",
      "Iteration 277, loss = 0.37870509\n",
      "Iteration 278, loss = 0.37809966\n",
      "Iteration 279, loss = 0.37749659\n",
      "Iteration 280, loss = 0.37689584\n",
      "Iteration 281, loss = 0.37629739\n",
      "Iteration 282, loss = 0.37570123\n",
      "Iteration 283, loss = 0.37510732\n",
      "Iteration 284, loss = 0.37451585\n",
      "Iteration 285, loss = 0.37392718\n",
      "Iteration 286, loss = 0.37334065\n",
      "Iteration 287, loss = 0.37275625\n",
      "Iteration 288, loss = 0.37217397\n",
      "Iteration 289, loss = 0.37159383\n",
      "Iteration 290, loss = 0.37101580\n",
      "Iteration 291, loss = 0.37043991\n",
      "Iteration 292, loss = 0.36986613\n",
      "Iteration 293, loss = 0.36929443\n",
      "Iteration 294, loss = 0.36872501\n",
      "Iteration 295, loss = 0.36815755\n",
      "Iteration 296, loss = 0.36759206\n",
      "Iteration 297, loss = 0.36702853\n",
      "Iteration 298, loss = 0.36646711\n",
      "Iteration 299, loss = 0.36590772\n",
      "Iteration 300, loss = 0.36535024\n",
      "Iteration 301, loss = 0.36479469\n",
      "Iteration 302, loss = 0.36424107\n",
      "Iteration 303, loss = 0.36368942\n",
      "Iteration 304, loss = 0.36313962\n",
      "Iteration 305, loss = 0.36259209\n",
      "Iteration 306, loss = 0.36204651\n",
      "Iteration 307, loss = 0.36150272\n",
      "Iteration 308, loss = 0.36096078\n",
      "Iteration 309, loss = 0.36042065\n",
      "Iteration 310, loss = 0.35988233\n",
      "Iteration 311, loss = 0.35934586\n",
      "Iteration 312, loss = 0.35881114\n",
      "Iteration 313, loss = 0.35827866\n",
      "Iteration 314, loss = 0.35774813\n",
      "Iteration 315, loss = 0.35721939\n",
      "Iteration 316, loss = 0.35669243\n",
      "Iteration 317, loss = 0.35616728\n",
      "Iteration 318, loss = 0.35564381\n",
      "Iteration 319, loss = 0.35512211\n",
      "Iteration 320, loss = 0.35460215\n",
      "Iteration 321, loss = 0.35408389\n",
      "Iteration 322, loss = 0.35356730\n",
      "Iteration 323, loss = 0.35305241\n",
      "Iteration 324, loss = 0.35253918\n",
      "Iteration 325, loss = 0.35202762\n",
      "Iteration 326, loss = 0.35151787\n",
      "Iteration 327, loss = 0.35100994\n",
      "Iteration 328, loss = 0.35050356\n",
      "Iteration 329, loss = 0.34999875\n",
      "Iteration 330, loss = 0.34949560\n",
      "Iteration 331, loss = 0.34899409\n",
      "Iteration 332, loss = 0.34849433\n",
      "Iteration 333, loss = 0.34799615\n",
      "Iteration 334, loss = 0.34749952\n",
      "Iteration 335, loss = 0.34700443\n",
      "Iteration 336, loss = 0.34651089\n",
      "Iteration 337, loss = 0.34601887\n",
      "Iteration 338, loss = 0.34552836\n",
      "Iteration 339, loss = 0.34503935\n",
      "Iteration 340, loss = 0.34455182\n",
      "Iteration 341, loss = 0.34406584\n",
      "Iteration 342, loss = 0.34358134\n",
      "Iteration 343, loss = 0.34309831\n",
      "Iteration 344, loss = 0.34261705\n",
      "Iteration 345, loss = 0.34213719\n",
      "Iteration 346, loss = 0.34165883\n",
      "Iteration 347, loss = 0.34118186\n",
      "Iteration 348, loss = 0.34070634\n",
      "Iteration 349, loss = 0.34023241\n",
      "Iteration 350, loss = 0.33975987\n",
      "Iteration 351, loss = 0.33928875\n",
      "Iteration 352, loss = 0.33881901\n",
      "Iteration 353, loss = 0.33835066\n",
      "Iteration 354, loss = 0.33788369\n",
      "Iteration 355, loss = 0.33741807\n",
      "Iteration 356, loss = 0.33695387\n",
      "Iteration 357, loss = 0.33649116\n",
      "Iteration 358, loss = 0.33602982\n",
      "Iteration 359, loss = 0.33556987\n",
      "Iteration 360, loss = 0.33511134\n",
      "Iteration 361, loss = 0.33465411\n",
      "Iteration 362, loss = 0.33419820\n",
      "Iteration 363, loss = 0.33374369\n",
      "Iteration 364, loss = 0.33329051\n",
      "Iteration 365, loss = 0.33283864\n",
      "Iteration 366, loss = 0.33238805\n",
      "Iteration 367, loss = 0.33193874\n",
      "Iteration 368, loss = 0.33149067\n",
      "Iteration 369, loss = 0.33104387\n",
      "Iteration 370, loss = 0.33059830\n",
      "Iteration 371, loss = 0.33015396\n",
      "Iteration 372, loss = 0.32971114\n",
      "Iteration 373, loss = 0.32926947\n",
      "Iteration 374, loss = 0.32882910\n",
      "Iteration 375, loss = 0.32839005\n",
      "Iteration 376, loss = 0.32795215\n",
      "Iteration 377, loss = 0.32751556\n",
      "Iteration 378, loss = 0.32708021\n",
      "Iteration 379, loss = 0.32664607\n",
      "Iteration 380, loss = 0.32621308\n",
      "Iteration 381, loss = 0.32578127\n",
      "Iteration 382, loss = 0.32535069\n",
      "Iteration 383, loss = 0.32492121\n",
      "Iteration 384, loss = 0.32449299\n",
      "Iteration 385, loss = 0.32406584\n",
      "Iteration 386, loss = 0.32363988\n",
      "Iteration 387, loss = 0.32321507\n",
      "Iteration 388, loss = 0.32279136\n",
      "Iteration 389, loss = 0.32236879\n",
      "Iteration 390, loss = 0.32194735\n",
      "Iteration 391, loss = 0.32152698\n",
      "Iteration 392, loss = 0.32110781\n",
      "Iteration 393, loss = 0.32068965\n",
      "Iteration 394, loss = 0.32027263\n",
      "Iteration 395, loss = 0.31985667\n",
      "Iteration 396, loss = 0.31944183\n",
      "Iteration 397, loss = 0.31902806\n",
      "Iteration 398, loss = 0.31861536\n",
      "Iteration 399, loss = 0.31820374\n",
      "Iteration 400, loss = 0.31779316\n",
      "Iteration 401, loss = 0.31738370\n",
      "Iteration 402, loss = 0.31697523\n",
      "Iteration 403, loss = 0.31656787\n",
      "Iteration 404, loss = 0.31616150\n",
      "Iteration 405, loss = 0.31575621\n",
      "Iteration 406, loss = 0.31535190\n",
      "Iteration 407, loss = 0.31494872\n",
      "Iteration 408, loss = 0.31454648\n",
      "Iteration 409, loss = 0.31414531\n",
      "Iteration 410, loss = 0.31374518\n",
      "Iteration 411, loss = 0.31334620\n",
      "Iteration 412, loss = 0.31294816\n",
      "Iteration 413, loss = 0.31255123\n",
      "Iteration 414, loss = 0.31215525\n",
      "Iteration 415, loss = 0.31176027\n",
      "Iteration 416, loss = 0.31136649\n",
      "Iteration 417, loss = 0.31097369\n",
      "Iteration 418, loss = 0.31058189\n",
      "Iteration 419, loss = 0.31019103\n",
      "Iteration 420, loss = 0.30980121\n",
      "Iteration 421, loss = 0.30941235\n",
      "Iteration 422, loss = 0.30902449\n",
      "Iteration 423, loss = 0.30863772\n",
      "Iteration 424, loss = 0.30825195\n",
      "Iteration 425, loss = 0.30786717\n",
      "Iteration 426, loss = 0.30748331\n",
      "Iteration 427, loss = 0.30710046\n",
      "Iteration 428, loss = 0.30671852\n",
      "Iteration 429, loss = 0.30633756\n",
      "Iteration 430, loss = 0.30595760\n",
      "Iteration 431, loss = 0.30557861\n",
      "Iteration 432, loss = 0.30520071\n",
      "Iteration 433, loss = 0.30482372\n",
      "Iteration 434, loss = 0.30444765\n",
      "Iteration 435, loss = 0.30407260\n",
      "Iteration 436, loss = 0.30369840\n",
      "Iteration 437, loss = 0.30332516\n",
      "Iteration 438, loss = 0.30295283\n",
      "Iteration 439, loss = 0.30258140\n",
      "Iteration 440, loss = 0.30221088\n",
      "Iteration 441, loss = 0.30184132\n",
      "Iteration 442, loss = 0.30147277\n",
      "Iteration 443, loss = 0.30110514\n",
      "Iteration 444, loss = 0.30073844\n",
      "Iteration 445, loss = 0.30037255\n",
      "Iteration 446, loss = 0.30000764\n",
      "Iteration 447, loss = 0.29964358\n",
      "Iteration 448, loss = 0.29928039\n",
      "Iteration 449, loss = 0.29891812\n",
      "Iteration 450, loss = 0.29855668\n",
      "Iteration 451, loss = 0.29819613\n",
      "Iteration 452, loss = 0.29783645\n",
      "Iteration 453, loss = 0.29747765\n",
      "Iteration 454, loss = 0.29711968\n",
      "Iteration 455, loss = 0.29676270\n",
      "Iteration 456, loss = 0.29640655\n",
      "Iteration 457, loss = 0.29605128\n",
      "Iteration 458, loss = 0.29569685\n",
      "Iteration 459, loss = 0.29534326\n",
      "Iteration 460, loss = 0.29499054\n",
      "Iteration 461, loss = 0.29463870\n",
      "Iteration 462, loss = 0.29428775\n",
      "Iteration 463, loss = 0.29393754\n",
      "Iteration 464, loss = 0.29358827\n",
      "Iteration 465, loss = 0.29323976\n",
      "Iteration 466, loss = 0.29289211\n",
      "Iteration 467, loss = 0.29254529\n",
      "Iteration 468, loss = 0.29219930\n",
      "Iteration 469, loss = 0.29185412\n",
      "Iteration 470, loss = 0.29150974\n",
      "Iteration 471, loss = 0.29116617\n",
      "Iteration 472, loss = 0.29082342\n",
      "Iteration 473, loss = 0.29048149\n",
      "Iteration 474, loss = 0.29014039\n",
      "Iteration 475, loss = 0.28980008\n",
      "Iteration 476, loss = 0.28946056\n",
      "Iteration 477, loss = 0.28912189\n",
      "Iteration 478, loss = 0.28878397\n",
      "Iteration 479, loss = 0.28844701\n",
      "Iteration 480, loss = 0.28811084\n",
      "Iteration 481, loss = 0.28777554\n",
      "Iteration 482, loss = 0.28744104\n",
      "Iteration 483, loss = 0.28710732\n",
      "Iteration 484, loss = 0.28677440\n",
      "Iteration 485, loss = 0.28644225\n",
      "Iteration 486, loss = 0.28611088\n",
      "Iteration 487, loss = 0.28578032\n",
      "Iteration 488, loss = 0.28545051\n",
      "Iteration 489, loss = 0.28512147\n",
      "Iteration 490, loss = 0.28479320\n",
      "Iteration 491, loss = 0.28446569\n",
      "Iteration 492, loss = 0.28413898\n",
      "Iteration 493, loss = 0.28381297\n",
      "Iteration 494, loss = 0.28348774\n",
      "Iteration 495, loss = 0.28316327\n",
      "Iteration 496, loss = 0.28283953\n",
      "Iteration 497, loss = 0.28251654\n",
      "Iteration 498, loss = 0.28219432\n",
      "Iteration 499, loss = 0.28187282\n",
      "Iteration 500, loss = 0.28155207\n",
      "Iteration 501, loss = 0.28123205\n",
      "Iteration 502, loss = 0.28091279\n",
      "Iteration 503, loss = 0.28059422\n",
      "Iteration 504, loss = 0.28027640\n",
      "Iteration 505, loss = 0.27995933\n",
      "Iteration 506, loss = 0.27964297\n",
      "Iteration 507, loss = 0.27932735\n",
      "Iteration 508, loss = 0.27901246\n",
      "Iteration 509, loss = 0.27869828\n",
      "Iteration 510, loss = 0.27838482\n",
      "Iteration 511, loss = 0.27807209\n",
      "Iteration 512, loss = 0.27776013\n",
      "Iteration 513, loss = 0.27744891\n",
      "Iteration 514, loss = 0.27713840\n",
      "Iteration 515, loss = 0.27682866\n",
      "Iteration 516, loss = 0.27651963\n",
      "Iteration 517, loss = 0.27621130\n",
      "Iteration 518, loss = 0.27590368\n",
      "Iteration 519, loss = 0.27559677\n",
      "Iteration 520, loss = 0.27529057\n",
      "Iteration 521, loss = 0.27498507\n",
      "Iteration 522, loss = 0.27468029\n",
      "Iteration 523, loss = 0.27437618\n",
      "Iteration 524, loss = 0.27407278\n",
      "Iteration 525, loss = 0.27377008\n",
      "Iteration 526, loss = 0.27346806\n",
      "Iteration 527, loss = 0.27316673\n",
      "Iteration 528, loss = 0.27286607\n",
      "Iteration 529, loss = 0.27256611\n",
      "Iteration 530, loss = 0.27226682\n",
      "Iteration 531, loss = 0.27196821\n",
      "Iteration 532, loss = 0.27167028\n",
      "Iteration 533, loss = 0.27137301\n",
      "Iteration 534, loss = 0.27107642\n",
      "Iteration 535, loss = 0.27078052\n",
      "Iteration 536, loss = 0.27048525\n",
      "Iteration 537, loss = 0.27019066\n",
      "Iteration 538, loss = 0.26989674\n",
      "Iteration 539, loss = 0.26960347\n",
      "Iteration 540, loss = 0.26931086\n",
      "Iteration 541, loss = 0.26901891\n",
      "Iteration 542, loss = 0.26872761\n",
      "Iteration 543, loss = 0.26843697\n",
      "Iteration 544, loss = 0.26814701\n",
      "Iteration 545, loss = 0.26785765\n",
      "Iteration 546, loss = 0.26756896\n",
      "Iteration 547, loss = 0.26728092\n",
      "Iteration 548, loss = 0.26699352\n",
      "Iteration 549, loss = 0.26670677\n",
      "Iteration 550, loss = 0.26642067\n",
      "Iteration 551, loss = 0.26613520\n",
      "Iteration 552, loss = 0.26585037\n",
      "Iteration 553, loss = 0.26556619\n",
      "Iteration 554, loss = 0.26528269\n",
      "Iteration 555, loss = 0.26499979\n",
      "Iteration 556, loss = 0.26471750\n",
      "Iteration 557, loss = 0.26443585\n",
      "Iteration 558, loss = 0.26415485\n",
      "Iteration 559, loss = 0.26387448\n",
      "Iteration 560, loss = 0.26359474\n",
      "Iteration 561, loss = 0.26331563\n",
      "Iteration 562, loss = 0.26303713\n",
      "Iteration 563, loss = 0.26275925\n",
      "Iteration 564, loss = 0.26248198\n",
      "Iteration 565, loss = 0.26220535\n",
      "Iteration 566, loss = 0.26192931\n",
      "Iteration 567, loss = 0.26165389\n",
      "Iteration 568, loss = 0.26137910\n",
      "Iteration 569, loss = 0.26110490\n",
      "Iteration 570, loss = 0.26083131\n",
      "Iteration 571, loss = 0.26055834\n",
      "Iteration 572, loss = 0.26028599\n",
      "Iteration 573, loss = 0.26001422\n",
      "Iteration 574, loss = 0.25974306\n",
      "Iteration 575, loss = 0.25947252\n",
      "Iteration 576, loss = 0.25920258\n",
      "Iteration 577, loss = 0.25893324\n",
      "Iteration 578, loss = 0.25866449\n",
      "Iteration 579, loss = 0.25839634\n",
      "Iteration 580, loss = 0.25812878\n",
      "Iteration 581, loss = 0.25786181\n",
      "Iteration 582, loss = 0.25759543\n",
      "Iteration 583, loss = 0.25732969\n",
      "Iteration 584, loss = 0.25706457\n",
      "Iteration 585, loss = 0.25680003\n",
      "Iteration 586, loss = 0.25653610\n",
      "Iteration 587, loss = 0.25627274\n",
      "Iteration 588, loss = 0.25600996\n",
      "Iteration 589, loss = 0.25574778\n",
      "Iteration 590, loss = 0.25548616\n",
      "Iteration 591, loss = 0.25522513\n",
      "Iteration 592, loss = 0.25496467\n",
      "Iteration 593, loss = 0.25470480\n",
      "Iteration 594, loss = 0.25444549\n",
      "Iteration 595, loss = 0.25418676\n",
      "Iteration 596, loss = 0.25392861\n",
      "Iteration 597, loss = 0.25367101\n",
      "Iteration 598, loss = 0.25341399\n",
      "Iteration 599, loss = 0.25315753\n",
      "Iteration 600, loss = 0.25290163\n",
      "Iteration 601, loss = 0.25264629\n",
      "Iteration 602, loss = 0.25239153\n",
      "Iteration 603, loss = 0.25213738\n",
      "Iteration 604, loss = 0.25188379\n",
      "Iteration 605, loss = 0.25163078\n",
      "Iteration 606, loss = 0.25137837\n",
      "Iteration 607, loss = 0.25112653\n",
      "Iteration 608, loss = 0.25087525\n",
      "Iteration 609, loss = 0.25062451\n",
      "Iteration 610, loss = 0.25037433\n",
      "Iteration 611, loss = 0.25012472\n",
      "Iteration 612, loss = 0.24987565\n",
      "Iteration 613, loss = 0.24962712\n",
      "Iteration 614, loss = 0.24937914\n",
      "Iteration 615, loss = 0.24913171\n",
      "Iteration 616, loss = 0.24888482\n",
      "Iteration 617, loss = 0.24863848\n",
      "Iteration 618, loss = 0.24839268\n",
      "Iteration 619, loss = 0.24814741\n",
      "Iteration 620, loss = 0.24790273\n",
      "Iteration 621, loss = 0.24765855\n",
      "Iteration 622, loss = 0.24741491\n",
      "Iteration 623, loss = 0.24717180\n",
      "Iteration 624, loss = 0.24692922\n",
      "Iteration 625, loss = 0.24668719\n",
      "Iteration 626, loss = 0.24644571\n",
      "Iteration 627, loss = 0.24620484\n",
      "Iteration 628, loss = 0.24596448\n",
      "Iteration 629, loss = 0.24572465\n",
      "Iteration 630, loss = 0.24548535\n",
      "Iteration 631, loss = 0.24524657\n",
      "Iteration 632, loss = 0.24500832\n",
      "Iteration 633, loss = 0.24477059\n",
      "Iteration 634, loss = 0.24453338\n",
      "Iteration 635, loss = 0.24429669\n",
      "Iteration 636, loss = 0.24406050\n",
      "Iteration 637, loss = 0.24382484\n",
      "Iteration 638, loss = 0.24358968\n",
      "Iteration 639, loss = 0.24335505\n",
      "Iteration 640, loss = 0.24312092\n",
      "Iteration 641, loss = 0.24288735\n",
      "Iteration 642, loss = 0.24265427\n",
      "Iteration 643, loss = 0.24242170\n",
      "Iteration 644, loss = 0.24218963\n",
      "Iteration 645, loss = 0.24195808\n",
      "Iteration 646, loss = 0.24172701\n",
      "Iteration 647, loss = 0.24149646\n",
      "Iteration 648, loss = 0.24126640\n",
      "Iteration 649, loss = 0.24103684\n",
      "Iteration 650, loss = 0.24080777\n",
      "Iteration 651, loss = 0.24057920\n",
      "Iteration 652, loss = 0.24035112\n",
      "Iteration 653, loss = 0.24012354\n",
      "Iteration 654, loss = 0.23989644\n",
      "Iteration 655, loss = 0.23966984\n",
      "Iteration 656, loss = 0.23944372\n",
      "Iteration 657, loss = 0.23921809\n",
      "Iteration 658, loss = 0.23899294\n",
      "Iteration 659, loss = 0.23876828\n",
      "Iteration 660, loss = 0.23854411\n",
      "Iteration 661, loss = 0.23832041\n",
      "Iteration 662, loss = 0.23809720\n",
      "Iteration 663, loss = 0.23787446\n",
      "Iteration 664, loss = 0.23765221\n",
      "Iteration 665, loss = 0.23743042\n",
      "Iteration 666, loss = 0.23720912\n",
      "Iteration 667, loss = 0.23698828\n",
      "Iteration 668, loss = 0.23676792\n",
      "Iteration 669, loss = 0.23654804\n",
      "Iteration 670, loss = 0.23632862\n",
      "Iteration 671, loss = 0.23610968\n",
      "Iteration 672, loss = 0.23589120\n",
      "Iteration 673, loss = 0.23567319\n",
      "Iteration 674, loss = 0.23545565\n",
      "Iteration 675, loss = 0.23523857\n",
      "Iteration 676, loss = 0.23502196\n",
      "Iteration 677, loss = 0.23480584\n",
      "Iteration 678, loss = 0.23459018\n",
      "Iteration 679, loss = 0.23437499\n",
      "Iteration 680, loss = 0.23416027\n",
      "Iteration 681, loss = 0.23394601\n",
      "Iteration 682, loss = 0.23373220\n",
      "Iteration 683, loss = 0.23351888\n",
      "Iteration 684, loss = 0.23330599\n",
      "Iteration 685, loss = 0.23309355\n",
      "Iteration 686, loss = 0.23288158\n",
      "Iteration 687, loss = 0.23267005\n",
      "Iteration 688, loss = 0.23245897\n",
      "Iteration 689, loss = 0.23224835\n",
      "Iteration 690, loss = 0.23203818\n",
      "Iteration 691, loss = 0.23182847\n",
      "Iteration 692, loss = 0.23161919\n",
      "Iteration 693, loss = 0.23141037\n",
      "Iteration 694, loss = 0.23120199\n",
      "Iteration 695, loss = 0.23099404\n",
      "Iteration 696, loss = 0.23078655\n",
      "Iteration 697, loss = 0.23057948\n",
      "Iteration 698, loss = 0.23037287\n",
      "Iteration 699, loss = 0.23016670\n",
      "Iteration 700, loss = 0.22996097\n",
      "Iteration 701, loss = 0.22975570\n",
      "Iteration 702, loss = 0.22955084\n",
      "Iteration 703, loss = 0.22934642\n",
      "Iteration 704, loss = 0.22914244\n",
      "Iteration 705, loss = 0.22893889\n",
      "Iteration 706, loss = 0.22873576\n",
      "Iteration 707, loss = 0.22853308\n",
      "Iteration 708, loss = 0.22833081\n",
      "Iteration 709, loss = 0.22812898\n",
      "Iteration 710, loss = 0.22792757\n",
      "Iteration 711, loss = 0.22772658\n",
      "Iteration 712, loss = 0.22752603\n",
      "Iteration 713, loss = 0.22732589\n",
      "Iteration 714, loss = 0.22712617\n",
      "Iteration 715, loss = 0.22692688\n",
      "Iteration 716, loss = 0.22672801\n",
      "Iteration 717, loss = 0.22652955\n",
      "Iteration 718, loss = 0.22633153\n",
      "Iteration 719, loss = 0.22613392\n",
      "Iteration 720, loss = 0.22593674\n",
      "Iteration 721, loss = 0.22573997\n",
      "Iteration 722, loss = 0.22554361\n",
      "Iteration 723, loss = 0.22534767\n",
      "Iteration 724, loss = 0.22515213\n",
      "Iteration 725, loss = 0.22495702\n",
      "Iteration 726, loss = 0.22476230\n",
      "Iteration 727, loss = 0.22456801\n",
      "Iteration 728, loss = 0.22437411\n",
      "Iteration 729, loss = 0.22418063\n",
      "Iteration 730, loss = 0.22398754\n",
      "Iteration 731, loss = 0.22379487\n",
      "Iteration 732, loss = 0.22360260\n",
      "Iteration 733, loss = 0.22341073\n",
      "Iteration 734, loss = 0.22321926\n",
      "Iteration 735, loss = 0.22302819\n",
      "Iteration 736, loss = 0.22283753\n",
      "Iteration 737, loss = 0.22264726\n",
      "Iteration 738, loss = 0.22245739\n",
      "Iteration 739, loss = 0.22226792\n",
      "Iteration 740, loss = 0.22207884\n",
      "Iteration 741, loss = 0.22189016\n",
      "Iteration 742, loss = 0.22170187\n",
      "Iteration 743, loss = 0.22151398\n",
      "Iteration 744, loss = 0.22132647\n",
      "Iteration 745, loss = 0.22113936\n",
      "Iteration 746, loss = 0.22095264\n",
      "Iteration 747, loss = 0.22076630\n",
      "Iteration 748, loss = 0.22058036\n",
      "Iteration 749, loss = 0.22039480\n",
      "Iteration 750, loss = 0.22020963\n",
      "Iteration 751, loss = 0.22002484\n",
      "Iteration 752, loss = 0.21984044\n",
      "Iteration 753, loss = 0.21965642\n",
      "Iteration 754, loss = 0.21947278\n",
      "Iteration 755, loss = 0.21928953\n",
      "Iteration 756, loss = 0.21910665\n",
      "Iteration 757, loss = 0.21892416\n",
      "Iteration 758, loss = 0.21874204\n",
      "Iteration 759, loss = 0.21856030\n",
      "Iteration 760, loss = 0.21837893\n",
      "Iteration 761, loss = 0.21819795\n",
      "Iteration 762, loss = 0.21801733\n",
      "Iteration 763, loss = 0.21783710\n",
      "Iteration 764, loss = 0.21765723\n",
      "Iteration 765, loss = 0.21747774\n",
      "Iteration 766, loss = 0.21729862\n",
      "Iteration 767, loss = 0.21711987\n",
      "Iteration 768, loss = 0.21694148\n",
      "Iteration 769, loss = 0.21676347\n",
      "Iteration 770, loss = 0.21658582\n",
      "Iteration 771, loss = 0.21640855\n",
      "Iteration 772, loss = 0.21623163\n",
      "Iteration 773, loss = 0.21605508\n",
      "Iteration 774, loss = 0.21587890\n",
      "Iteration 775, loss = 0.21570308\n",
      "Iteration 776, loss = 0.21552762\n",
      "Iteration 777, loss = 0.21535252\n",
      "Iteration 778, loss = 0.21517779\n",
      "Iteration 779, loss = 0.21500341\n",
      "Iteration 780, loss = 0.21482939\n",
      "Iteration 781, loss = 0.21465573\n",
      "Iteration 782, loss = 0.21448242\n",
      "Iteration 783, loss = 0.21430948\n",
      "Iteration 784, loss = 0.21413688\n",
      "Iteration 785, loss = 0.21396465\n",
      "Iteration 786, loss = 0.21379276\n",
      "Iteration 787, loss = 0.21362123\n",
      "Iteration 788, loss = 0.21345005\n",
      "Iteration 789, loss = 0.21327922\n",
      "Iteration 790, loss = 0.21310874\n",
      "Iteration 791, loss = 0.21293861\n",
      "Iteration 792, loss = 0.21276883\n",
      "Iteration 793, loss = 0.21259939\n",
      "Iteration 794, loss = 0.21243031\n",
      "Iteration 795, loss = 0.21226156\n",
      "Iteration 796, loss = 0.21209317\n",
      "Iteration 797, loss = 0.21192511\n",
      "Iteration 798, loss = 0.21175740\n",
      "Iteration 799, loss = 0.21159004\n",
      "Iteration 800, loss = 0.21142301\n",
      "Iteration 801, loss = 0.21125633\n",
      "Iteration 802, loss = 0.21108998\n",
      "Iteration 803, loss = 0.21092397\n",
      "Iteration 804, loss = 0.21075830\n",
      "Iteration 805, loss = 0.21059297\n",
      "Iteration 806, loss = 0.21042798\n",
      "Iteration 807, loss = 0.21026332\n",
      "Iteration 808, loss = 0.21009900\n",
      "Iteration 809, loss = 0.20993501\n",
      "Iteration 810, loss = 0.20977135\n",
      "Iteration 811, loss = 0.20960803\n",
      "Iteration 812, loss = 0.20944504\n",
      "Iteration 813, loss = 0.20928238\n",
      "Iteration 814, loss = 0.20912005\n",
      "Iteration 815, loss = 0.20895805\n",
      "Iteration 816, loss = 0.20879638\n",
      "Iteration 817, loss = 0.20863503\n",
      "Iteration 818, loss = 0.20847401\n",
      "Iteration 819, loss = 0.20831332\n",
      "Iteration 820, loss = 0.20815295\n",
      "Iteration 821, loss = 0.20799291\n",
      "Iteration 822, loss = 0.20783319\n",
      "Iteration 823, loss = 0.20767379\n",
      "Iteration 824, loss = 0.20751472\n",
      "Iteration 825, loss = 0.20735597\n",
      "Iteration 826, loss = 0.20719754\n",
      "Iteration 827, loss = 0.20703942\n",
      "Iteration 828, loss = 0.20688163\n",
      "Iteration 829, loss = 0.20672416\n",
      "Iteration 830, loss = 0.20656700\n",
      "Iteration 831, loss = 0.20641016\n",
      "Iteration 832, loss = 0.20625363\n",
      "Iteration 833, loss = 0.20609742\n",
      "Iteration 834, loss = 0.20594153\n",
      "Iteration 835, loss = 0.20578594\n",
      "Iteration 836, loss = 0.20563067\n",
      "Iteration 837, loss = 0.20547572\n",
      "Iteration 838, loss = 0.20532107\n",
      "Iteration 839, loss = 0.20516674\n",
      "Iteration 840, loss = 0.20501271\n",
      "Iteration 841, loss = 0.20485900\n",
      "Iteration 842, loss = 0.20470559\n",
      "Iteration 843, loss = 0.20455249\n",
      "Iteration 844, loss = 0.20439969\n",
      "Iteration 845, loss = 0.20424721\n",
      "Iteration 846, loss = 0.20409502\n",
      "Iteration 847, loss = 0.20394314\n",
      "Iteration 848, loss = 0.20379157\n",
      "Iteration 849, loss = 0.20364030\n",
      "Iteration 850, loss = 0.20348933\n",
      "Iteration 851, loss = 0.20333866\n",
      "Iteration 852, loss = 0.20318831\n",
      "Iteration 853, loss = 0.20303824\n",
      "Iteration 854, loss = 0.20288847\n",
      "Iteration 855, loss = 0.20273901\n",
      "Iteration 856, loss = 0.20258985\n",
      "Iteration 857, loss = 0.20244097\n",
      "Iteration 858, loss = 0.20229241\n",
      "Iteration 859, loss = 0.20214412\n",
      "Iteration 860, loss = 0.20199615\n",
      "Iteration 861, loss = 0.20184846\n",
      "Iteration 862, loss = 0.20170106\n",
      "Iteration 863, loss = 0.20155396\n",
      "Iteration 864, loss = 0.20140715\n",
      "Iteration 865, loss = 0.20126064\n",
      "Iteration 866, loss = 0.20111441\n",
      "Iteration 867, loss = 0.20096847\n",
      "Iteration 868, loss = 0.20082282\n",
      "Iteration 869, loss = 0.20067746\n",
      "Iteration 870, loss = 0.20053240\n",
      "Iteration 871, loss = 0.20038761\n",
      "Iteration 872, loss = 0.20024311\n",
      "Iteration 873, loss = 0.20009891\n",
      "Iteration 874, loss = 0.19995498\n",
      "Iteration 875, loss = 0.19981134\n",
      "Iteration 876, loss = 0.19966798\n",
      "Iteration 877, loss = 0.19952491\n",
      "Iteration 878, loss = 0.19938211\n",
      "Iteration 879, loss = 0.19923960\n",
      "Iteration 880, loss = 0.19909738\n",
      "Iteration 881, loss = 0.19895545\n",
      "Iteration 882, loss = 0.19881377\n",
      "Iteration 883, loss = 0.19867238\n",
      "Iteration 884, loss = 0.19853127\n",
      "Iteration 885, loss = 0.19839044\n",
      "Iteration 886, loss = 0.19824989\n",
      "Iteration 887, loss = 0.19810961\n",
      "Iteration 888, loss = 0.19796961\n",
      "Iteration 889, loss = 0.19782988\n",
      "Iteration 890, loss = 0.19769042\n",
      "Iteration 891, loss = 0.19755124\n",
      "Iteration 892, loss = 0.19741234\n",
      "Iteration 893, loss = 0.19727370\n",
      "Iteration 894, loss = 0.19713533\n",
      "Iteration 895, loss = 0.19699724\n",
      "Iteration 896, loss = 0.19685942\n",
      "Iteration 897, loss = 0.19672186\n",
      "Iteration 898, loss = 0.19658461\n",
      "Iteration 899, loss = 0.19644757\n",
      "Iteration 900, loss = 0.19631083\n",
      "Iteration 901, loss = 0.19617435\n",
      "Iteration 902, loss = 0.19603813\n",
      "Iteration 903, loss = 0.19590219\n",
      "Iteration 904, loss = 0.19576651\n",
      "Iteration 905, loss = 0.19563109\n",
      "Iteration 906, loss = 0.19549593\n",
      "Iteration 907, loss = 0.19536104\n",
      "Iteration 908, loss = 0.19522641\n",
      "Iteration 909, loss = 0.19509204\n",
      "Iteration 910, loss = 0.19495794\n",
      "Iteration 911, loss = 0.19482409\n",
      "Iteration 912, loss = 0.19469052\n",
      "Iteration 913, loss = 0.19455720\n",
      "Iteration 914, loss = 0.19442414\n",
      "Iteration 915, loss = 0.19429135\n",
      "Iteration 916, loss = 0.19415881\n",
      "Iteration 917, loss = 0.19402652\n",
      "Iteration 918, loss = 0.19389450\n",
      "Iteration 919, loss = 0.19376276\n",
      "Iteration 920, loss = 0.19363122\n",
      "Iteration 921, loss = 0.19349997\n",
      "Iteration 922, loss = 0.19336896\n",
      "Iteration 923, loss = 0.19323821\n",
      "Iteration 924, loss = 0.19310772\n",
      "Iteration 925, loss = 0.19297747\n",
      "Iteration 926, loss = 0.19284748\n",
      "Iteration 927, loss = 0.19271778\n",
      "Iteration 928, loss = 0.19258826\n",
      "Iteration 929, loss = 0.19245903\n",
      "Iteration 930, loss = 0.19233005\n",
      "Iteration 931, loss = 0.19220131\n",
      "Iteration 932, loss = 0.19207282\n",
      "Iteration 933, loss = 0.19194457\n",
      "Iteration 934, loss = 0.19181657\n",
      "Iteration 935, loss = 0.19168882\n",
      "Iteration 936, loss = 0.19156132\n",
      "Iteration 937, loss = 0.19143406\n",
      "Iteration 938, loss = 0.19130705\n",
      "Iteration 939, loss = 0.19118028\n",
      "Iteration 940, loss = 0.19105379\n",
      "Iteration 941, loss = 0.19092750\n",
      "Iteration 942, loss = 0.19080146\n",
      "Iteration 943, loss = 0.19067567\n",
      "Iteration 944, loss = 0.19055013\n",
      "Iteration 945, loss = 0.19042482\n",
      "Iteration 946, loss = 0.19029977\n",
      "Iteration 947, loss = 0.19017497\n",
      "Iteration 948, loss = 0.19005040\n",
      "Iteration 949, loss = 0.18992605\n",
      "Iteration 950, loss = 0.18980198\n",
      "Iteration 951, loss = 0.18967810\n",
      "Iteration 952, loss = 0.18955448\n",
      "Iteration 953, loss = 0.18943109\n",
      "Iteration 954, loss = 0.18930795\n",
      "Iteration 955, loss = 0.18918502\n",
      "Iteration 956, loss = 0.18906234\n",
      "Iteration 957, loss = 0.18893989\n",
      "Iteration 958, loss = 0.18881767\n",
      "Iteration 959, loss = 0.18869570\n",
      "Iteration 960, loss = 0.18857393\n",
      "Iteration 961, loss = 0.18845242\n",
      "Iteration 962, loss = 0.18833114\n",
      "Iteration 963, loss = 0.18821007\n",
      "Iteration 964, loss = 0.18808924\n",
      "Iteration 965, loss = 0.18796864\n",
      "Iteration 966, loss = 0.18784827\n",
      "Iteration 967, loss = 0.18772811\n",
      "Iteration 968, loss = 0.18760821\n",
      "Iteration 969, loss = 0.18748851\n",
      "Iteration 970, loss = 0.18736905\n",
      "Iteration 971, loss = 0.18724980\n",
      "Iteration 972, loss = 0.18713078\n",
      "Iteration 973, loss = 0.18701200\n",
      "Iteration 974, loss = 0.18689344\n",
      "Iteration 975, loss = 0.18677511\n",
      "Iteration 976, loss = 0.18665699\n",
      "Iteration 977, loss = 0.18653911\n",
      "Iteration 978, loss = 0.18642143\n",
      "Iteration 979, loss = 0.18630400\n",
      "Iteration 980, loss = 0.18618676\n",
      "Iteration 981, loss = 0.18606976\n",
      "Iteration 982, loss = 0.18595297\n",
      "Iteration 983, loss = 0.18583640\n",
      "Iteration 984, loss = 0.18572005\n",
      "Iteration 985, loss = 0.18560393\n",
      "Iteration 986, loss = 0.18548802\n",
      "Iteration 987, loss = 0.18537233\n",
      "Iteration 988, loss = 0.18525684\n",
      "Iteration 989, loss = 0.18514159\n",
      "Iteration 990, loss = 0.18502654\n",
      "Iteration 991, loss = 0.18491171\n",
      "Iteration 992, loss = 0.18479709\n",
      "Iteration 993, loss = 0.18468268\n",
      "Iteration 994, loss = 0.18456851\n",
      "Iteration 995, loss = 0.18445452\n",
      "Iteration 996, loss = 0.18434076\n",
      "Iteration 997, loss = 0.18422720\n",
      "Iteration 998, loss = 0.18411387\n",
      "Iteration 999, loss = 0.18400073\n",
      "Iteration 1000, loss = 0.18388783\n",
      "Iteration 1, loss = 1.92564666\n",
      "Iteration 2, loss = 1.88107456\n",
      "Iteration 3, loss = 1.82101339\n",
      "Iteration 4, loss = 1.75008524\n",
      "Iteration 5, loss = 1.67244843\n",
      "Iteration 6, loss = 1.59154726\n",
      "Iteration 7, loss = 1.51009324\n",
      "Iteration 8, loss = 1.43041917\n",
      "Iteration 9, loss = 1.35466016\n",
      "Iteration 10, loss = 1.28493478\n",
      "Iteration 11, loss = 1.22333732\n",
      "Iteration 12, loss = 1.17183741\n",
      "Iteration 13, loss = 1.13174506\n",
      "Iteration 14, loss = 1.10329544\n",
      "Iteration 15, loss = 1.08522187\n",
      "Iteration 16, loss = 1.07529163\n",
      "Iteration 17, loss = 1.07042661\n",
      "Iteration 18, loss = 1.06765570\n",
      "Iteration 19, loss = 1.06447737\n",
      "Iteration 20, loss = 1.05927821\n",
      "Iteration 21, loss = 1.05131848\n",
      "Iteration 22, loss = 1.04067059\n",
      "Iteration 23, loss = 1.02800606\n",
      "Iteration 24, loss = 1.01417583\n",
      "Iteration 25, loss = 1.00004941\n",
      "Iteration 26, loss = 0.98628830\n",
      "Iteration 27, loss = 0.97326051\n",
      "Iteration 28, loss = 0.96110353\n",
      "Iteration 29, loss = 0.94973248\n",
      "Iteration 30, loss = 0.93885771\n",
      "Iteration 31, loss = 0.92826082\n",
      "Iteration 32, loss = 0.91771249\n",
      "Iteration 33, loss = 0.90687638\n",
      "Iteration 34, loss = 0.89580853\n",
      "Iteration 35, loss = 0.88452833\n",
      "Iteration 36, loss = 0.87301887\n",
      "Iteration 37, loss = 0.86152290\n",
      "Iteration 38, loss = 0.85043450\n",
      "Iteration 39, loss = 0.83995684\n",
      "Iteration 40, loss = 0.83012889\n",
      "Iteration 41, loss = 0.82095886\n",
      "Iteration 42, loss = 0.81251023\n",
      "Iteration 43, loss = 0.80479117\n",
      "Iteration 44, loss = 0.79777150\n",
      "Iteration 45, loss = 0.79114850\n",
      "Iteration 46, loss = 0.78466579\n",
      "Iteration 47, loss = 0.77825801\n",
      "Iteration 48, loss = 0.77186891\n",
      "Iteration 49, loss = 0.76553437\n",
      "Iteration 50, loss = 0.75924040\n",
      "Iteration 51, loss = 0.75311652\n",
      "Iteration 52, loss = 0.74719689\n",
      "Iteration 53, loss = 0.74142523\n",
      "Iteration 54, loss = 0.73588054\n",
      "Iteration 55, loss = 0.73056587\n",
      "Iteration 56, loss = 0.72546616\n",
      "Iteration 57, loss = 0.72058323\n",
      "Iteration 58, loss = 0.71586660\n",
      "Iteration 59, loss = 0.71129223\n",
      "Iteration 60, loss = 0.70679519\n",
      "Iteration 61, loss = 0.70235897\n",
      "Iteration 62, loss = 0.69799263\n",
      "Iteration 63, loss = 0.69367378\n",
      "Iteration 64, loss = 0.68944718\n",
      "Iteration 65, loss = 0.68532898\n",
      "Iteration 66, loss = 0.68130008\n",
      "Iteration 67, loss = 0.67737786\n",
      "Iteration 68, loss = 0.67355587\n",
      "Iteration 69, loss = 0.66981374\n",
      "Iteration 70, loss = 0.66615345\n",
      "Iteration 71, loss = 0.66257912\n",
      "Iteration 72, loss = 0.65906681\n",
      "Iteration 73, loss = 0.65560746\n",
      "Iteration 74, loss = 0.65220146\n",
      "Iteration 75, loss = 0.64884982\n",
      "Iteration 76, loss = 0.64554988\n",
      "Iteration 77, loss = 0.64229668\n",
      "Iteration 78, loss = 0.63909021\n",
      "Iteration 79, loss = 0.63593173\n",
      "Iteration 80, loss = 0.63281949\n",
      "Iteration 81, loss = 0.62975035\n",
      "Iteration 82, loss = 0.62672693\n",
      "Iteration 83, loss = 0.62374596\n",
      "Iteration 84, loss = 0.62080493\n",
      "Iteration 85, loss = 0.61790274\n",
      "Iteration 86, loss = 0.61504010\n",
      "Iteration 87, loss = 0.61222005\n",
      "Iteration 88, loss = 0.60944115\n",
      "Iteration 89, loss = 0.60670233\n",
      "Iteration 90, loss = 0.60399903\n",
      "Iteration 91, loss = 0.60133043\n",
      "Iteration 92, loss = 0.59869574\n",
      "Iteration 93, loss = 0.59609426\n",
      "Iteration 94, loss = 0.59352534\n",
      "Iteration 95, loss = 0.59098837\n",
      "Iteration 96, loss = 0.58848225\n",
      "Iteration 97, loss = 0.58600720\n",
      "Iteration 98, loss = 0.58356520\n",
      "Iteration 99, loss = 0.58115540\n",
      "Iteration 100, loss = 0.57877433\n",
      "Iteration 101, loss = 0.57642358\n",
      "Iteration 102, loss = 0.57410368\n",
      "Iteration 103, loss = 0.57181213\n",
      "Iteration 104, loss = 0.56954844\n",
      "Iteration 105, loss = 0.56731220\n",
      "Iteration 106, loss = 0.56510280\n",
      "Iteration 107, loss = 0.56292046\n",
      "Iteration 108, loss = 0.56076461\n",
      "Iteration 109, loss = 0.55863530\n",
      "Iteration 110, loss = 0.55653135\n",
      "Iteration 111, loss = 0.55445241\n",
      "Iteration 112, loss = 0.55239804\n",
      "Iteration 113, loss = 0.55036780\n",
      "Iteration 114, loss = 0.54836129\n",
      "Iteration 115, loss = 0.54637808\n",
      "Iteration 116, loss = 0.54441777\n",
      "Iteration 117, loss = 0.54247995\n",
      "Iteration 118, loss = 0.54056423\n",
      "Iteration 119, loss = 0.53867019\n",
      "Iteration 120, loss = 0.53679746\n",
      "Iteration 121, loss = 0.53494565\n",
      "Iteration 122, loss = 0.53311440\n",
      "Iteration 123, loss = 0.53130339\n",
      "Iteration 124, loss = 0.52951227\n",
      "Iteration 125, loss = 0.52774063\n",
      "Iteration 126, loss = 0.52598812\n",
      "Iteration 127, loss = 0.52425443\n",
      "Iteration 128, loss = 0.52253925\n",
      "Iteration 129, loss = 0.52084333\n",
      "Iteration 130, loss = 0.51916451\n",
      "Iteration 131, loss = 0.51750313\n",
      "Iteration 132, loss = 0.51585943\n",
      "Iteration 133, loss = 0.51423308\n",
      "Iteration 134, loss = 0.51262330\n",
      "Iteration 135, loss = 0.51103003\n",
      "Iteration 136, loss = 0.50945303\n",
      "Iteration 137, loss = 0.50789224\n",
      "Iteration 138, loss = 0.50634683\n",
      "Iteration 139, loss = 0.50481705\n",
      "Iteration 140, loss = 0.50330241\n",
      "Iteration 141, loss = 0.50180270\n",
      "Iteration 142, loss = 0.50031766\n",
      "Iteration 143, loss = 0.49884712\n",
      "Iteration 144, loss = 0.49739079\n",
      "Iteration 145, loss = 0.49594842\n",
      "Iteration 146, loss = 0.49451979\n",
      "Iteration 147, loss = 0.49310466\n",
      "Iteration 148, loss = 0.49170282\n",
      "Iteration 149, loss = 0.49031416\n",
      "Iteration 150, loss = 0.48893877\n",
      "Iteration 151, loss = 0.48757609\n",
      "Iteration 152, loss = 0.48622580\n",
      "Iteration 153, loss = 0.48488810\n",
      "Iteration 154, loss = 0.48356259\n",
      "Iteration 155, loss = 0.48224903\n",
      "Iteration 156, loss = 0.48094728\n",
      "Iteration 157, loss = 0.47965731\n",
      "Iteration 158, loss = 0.47837852\n",
      "Iteration 159, loss = 0.47711083\n",
      "Iteration 160, loss = 0.47585427\n",
      "Iteration 161, loss = 0.47460905\n",
      "Iteration 162, loss = 0.47337501\n",
      "Iteration 163, loss = 0.47215115\n",
      "Iteration 164, loss = 0.47093721\n",
      "Iteration 165, loss = 0.46973381\n",
      "Iteration 166, loss = 0.46854041\n",
      "Iteration 167, loss = 0.46735711\n",
      "Iteration 168, loss = 0.46618365\n",
      "Iteration 169, loss = 0.46501976\n",
      "Iteration 170, loss = 0.46386527\n",
      "Iteration 171, loss = 0.46272001\n",
      "Iteration 172, loss = 0.46158384\n",
      "Iteration 173, loss = 0.46045663\n",
      "Iteration 174, loss = 0.45933824\n",
      "Iteration 175, loss = 0.45822854\n",
      "Iteration 176, loss = 0.45712735\n",
      "Iteration 177, loss = 0.45603461\n",
      "Iteration 178, loss = 0.45495015\n",
      "Iteration 179, loss = 0.45387383\n",
      "Iteration 180, loss = 0.45280568\n",
      "Iteration 181, loss = 0.45174534\n",
      "Iteration 182, loss = 0.45069287\n",
      "Iteration 183, loss = 0.44964804\n",
      "Iteration 184, loss = 0.44861122\n",
      "Iteration 185, loss = 0.44758206\n",
      "Iteration 186, loss = 0.44656048\n",
      "Iteration 187, loss = 0.44554612\n",
      "Iteration 188, loss = 0.44453934\n",
      "Iteration 189, loss = 0.44353946\n",
      "Iteration 190, loss = 0.44254660\n",
      "Iteration 191, loss = 0.44156069\n",
      "Iteration 192, loss = 0.44058149\n",
      "Iteration 193, loss = 0.43960908\n",
      "Iteration 194, loss = 0.43864319\n",
      "Iteration 195, loss = 0.43768382\n",
      "Iteration 196, loss = 0.43673074\n",
      "Iteration 197, loss = 0.43578391\n",
      "Iteration 198, loss = 0.43484339\n",
      "Iteration 199, loss = 0.43390875\n",
      "Iteration 200, loss = 0.43298018\n",
      "Iteration 201, loss = 0.43205758\n",
      "Iteration 202, loss = 0.43114073\n",
      "Iteration 203, loss = 0.43022963\n",
      "Iteration 204, loss = 0.42932418\n",
      "Iteration 205, loss = 0.42842432\n",
      "Iteration 206, loss = 0.42752995\n",
      "Iteration 207, loss = 0.42664102\n",
      "Iteration 208, loss = 0.42575753\n",
      "Iteration 209, loss = 0.42487921\n",
      "Iteration 210, loss = 0.42400615\n",
      "Iteration 211, loss = 0.42313823\n",
      "Iteration 212, loss = 0.42227539\n",
      "Iteration 213, loss = 0.42141756\n",
      "Iteration 214, loss = 0.42056467\n",
      "Iteration 215, loss = 0.41971665\n",
      "Iteration 216, loss = 0.41887344\n",
      "Iteration 217, loss = 0.41803497\n",
      "Iteration 218, loss = 0.41720121\n",
      "Iteration 219, loss = 0.41637232\n",
      "Iteration 220, loss = 0.41554808\n",
      "Iteration 221, loss = 0.41472837\n",
      "Iteration 222, loss = 0.41391314\n",
      "Iteration 223, loss = 0.41310235\n",
      "Iteration 224, loss = 0.41229592\n",
      "Iteration 225, loss = 0.41149380\n",
      "Iteration 226, loss = 0.41069622\n",
      "Iteration 227, loss = 0.40990289\n",
      "Iteration 228, loss = 0.40911371\n",
      "Iteration 229, loss = 0.40832864\n",
      "Iteration 230, loss = 0.40754766\n",
      "Iteration 231, loss = 0.40677065\n",
      "Iteration 232, loss = 0.40599762\n",
      "Iteration 233, loss = 0.40522915\n",
      "Iteration 234, loss = 0.40446490\n",
      "Iteration 235, loss = 0.40370476\n",
      "Iteration 236, loss = 0.40294818\n",
      "Iteration 237, loss = 0.40219515\n",
      "Iteration 238, loss = 0.40144571\n",
      "Iteration 239, loss = 0.40069974\n",
      "Iteration 240, loss = 0.39995731\n",
      "Iteration 241, loss = 0.39921844\n",
      "Iteration 242, loss = 0.39848337\n",
      "Iteration 243, loss = 0.39775178\n",
      "Iteration 244, loss = 0.39702449\n",
      "Iteration 245, loss = 0.39630052\n",
      "Iteration 246, loss = 0.39557992\n",
      "Iteration 247, loss = 0.39486263\n",
      "Iteration 248, loss = 0.39414866\n",
      "Iteration 249, loss = 0.39343796\n",
      "Iteration 250, loss = 0.39273049\n",
      "Iteration 251, loss = 0.39202626\n",
      "Iteration 252, loss = 0.39132517\n",
      "Iteration 253, loss = 0.39062726\n",
      "Iteration 254, loss = 0.38993245\n",
      "Iteration 255, loss = 0.38924072\n",
      "Iteration 256, loss = 0.38855206\n",
      "Iteration 257, loss = 0.38786643\n",
      "Iteration 258, loss = 0.38718379\n",
      "Iteration 259, loss = 0.38650413\n",
      "Iteration 260, loss = 0.38582740\n",
      "Iteration 261, loss = 0.38515358\n",
      "Iteration 262, loss = 0.38448266\n",
      "Iteration 263, loss = 0.38381464\n",
      "Iteration 264, loss = 0.38314947\n",
      "Iteration 265, loss = 0.38248725\n",
      "Iteration 266, loss = 0.38182767\n",
      "Iteration 267, loss = 0.38117082\n",
      "Iteration 268, loss = 0.38051667\n",
      "Iteration 269, loss = 0.37986527\n",
      "Iteration 270, loss = 0.37921652\n",
      "Iteration 271, loss = 0.37857044\n",
      "Iteration 272, loss = 0.37792696\n",
      "Iteration 273, loss = 0.37728602\n",
      "Iteration 274, loss = 0.37664774\n",
      "Iteration 275, loss = 0.37601190\n",
      "Iteration 276, loss = 0.37537860\n",
      "Iteration 277, loss = 0.37474782\n",
      "Iteration 278, loss = 0.37411945\n",
      "Iteration 279, loss = 0.37349367\n",
      "Iteration 280, loss = 0.37287021\n",
      "Iteration 281, loss = 0.37224922\n",
      "Iteration 282, loss = 0.37163063\n",
      "Iteration 283, loss = 0.37101437\n",
      "Iteration 284, loss = 0.37040060\n",
      "Iteration 285, loss = 0.36978901\n",
      "Iteration 286, loss = 0.36917978\n",
      "Iteration 287, loss = 0.36857291\n",
      "Iteration 288, loss = 0.36796813\n",
      "Iteration 289, loss = 0.36736575\n",
      "Iteration 290, loss = 0.36676553\n",
      "Iteration 291, loss = 0.36616747\n",
      "Iteration 292, loss = 0.36557180\n",
      "Iteration 293, loss = 0.36497803\n",
      "Iteration 294, loss = 0.36438676\n",
      "Iteration 295, loss = 0.36379768\n",
      "Iteration 296, loss = 0.36321078\n",
      "Iteration 297, loss = 0.36262599\n",
      "Iteration 298, loss = 0.36204329\n",
      "Iteration 299, loss = 0.36146275\n",
      "Iteration 300, loss = 0.36088416\n",
      "Iteration 301, loss = 0.36030774\n",
      "Iteration 302, loss = 0.35973352\n",
      "Iteration 303, loss = 0.35916148\n",
      "Iteration 304, loss = 0.35859127\n",
      "Iteration 305, loss = 0.35802325\n",
      "Iteration 306, loss = 0.35745735\n",
      "Iteration 307, loss = 0.35689337\n",
      "Iteration 308, loss = 0.35633159\n",
      "Iteration 309, loss = 0.35577161\n",
      "Iteration 310, loss = 0.35521352\n",
      "Iteration 311, loss = 0.35465734\n",
      "Iteration 312, loss = 0.35410334\n",
      "Iteration 313, loss = 0.35355086\n",
      "Iteration 314, loss = 0.35300033\n",
      "Iteration 315, loss = 0.35245164\n",
      "Iteration 316, loss = 0.35190509\n",
      "Iteration 317, loss = 0.35136035\n",
      "Iteration 318, loss = 0.35081758\n",
      "Iteration 319, loss = 0.35027645\n",
      "Iteration 320, loss = 0.34973751\n",
      "Iteration 321, loss = 0.34920065\n",
      "Iteration 322, loss = 0.34866550\n",
      "Iteration 323, loss = 0.34813214\n",
      "Iteration 324, loss = 0.34760069\n",
      "Iteration 325, loss = 0.34707111\n",
      "Iteration 326, loss = 0.34654326\n",
      "Iteration 327, loss = 0.34601731\n",
      "Iteration 328, loss = 0.34549306\n",
      "Iteration 329, loss = 0.34497056\n",
      "Iteration 330, loss = 0.34444963\n",
      "Iteration 331, loss = 0.34393037\n",
      "Iteration 332, loss = 0.34341288\n",
      "Iteration 333, loss = 0.34289709\n",
      "Iteration 334, loss = 0.34238293\n",
      "Iteration 335, loss = 0.34187036\n",
      "Iteration 336, loss = 0.34135938\n",
      "Iteration 337, loss = 0.34084999\n",
      "Iteration 338, loss = 0.34034212\n",
      "Iteration 339, loss = 0.33983593\n",
      "Iteration 340, loss = 0.33933115\n",
      "Iteration 341, loss = 0.33882798\n",
      "Iteration 342, loss = 0.33832650\n",
      "Iteration 343, loss = 0.33782657\n",
      "Iteration 344, loss = 0.33732813\n",
      "Iteration 345, loss = 0.33683116\n",
      "Iteration 346, loss = 0.33633572\n",
      "Iteration 347, loss = 0.33584193\n",
      "Iteration 348, loss = 0.33534962\n",
      "Iteration 349, loss = 0.33485879\n",
      "Iteration 350, loss = 0.33436939\n",
      "Iteration 351, loss = 0.33388144\n",
      "Iteration 352, loss = 0.33339490\n",
      "Iteration 353, loss = 0.33290977\n",
      "Iteration 354, loss = 0.33242608\n",
      "Iteration 355, loss = 0.33194384\n",
      "Iteration 356, loss = 0.33146302\n",
      "Iteration 357, loss = 0.33098354\n",
      "Iteration 358, loss = 0.33050548\n",
      "Iteration 359, loss = 0.33002883\n",
      "Iteration 360, loss = 0.32955369\n",
      "Iteration 361, loss = 0.32907991\n",
      "Iteration 362, loss = 0.32860749\n",
      "Iteration 363, loss = 0.32813640\n",
      "Iteration 364, loss = 0.32766664\n",
      "Iteration 365, loss = 0.32719820\n",
      "Iteration 366, loss = 0.32673108\n",
      "Iteration 367, loss = 0.32626526\n",
      "Iteration 368, loss = 0.32580080\n",
      "Iteration 369, loss = 0.32533772\n",
      "Iteration 370, loss = 0.32487595\n",
      "Iteration 371, loss = 0.32441546\n",
      "Iteration 372, loss = 0.32395633\n",
      "Iteration 373, loss = 0.32349855\n",
      "Iteration 374, loss = 0.32304207\n",
      "Iteration 375, loss = 0.32258681\n",
      "Iteration 376, loss = 0.32213282\n",
      "Iteration 377, loss = 0.32168012\n",
      "Iteration 378, loss = 0.32122881\n",
      "Iteration 379, loss = 0.32077876\n",
      "Iteration 380, loss = 0.32032991\n",
      "Iteration 381, loss = 0.31988230\n",
      "Iteration 382, loss = 0.31943590\n",
      "Iteration 383, loss = 0.31899070\n",
      "Iteration 384, loss = 0.31854672\n",
      "Iteration 385, loss = 0.31810408\n",
      "Iteration 386, loss = 0.31766263\n",
      "Iteration 387, loss = 0.31722236\n",
      "Iteration 388, loss = 0.31678327\n",
      "Iteration 389, loss = 0.31634543\n",
      "Iteration 390, loss = 0.31590883\n",
      "Iteration 391, loss = 0.31547340\n",
      "Iteration 392, loss = 0.31503913\n",
      "Iteration 393, loss = 0.31460620\n",
      "Iteration 394, loss = 0.31417442\n",
      "Iteration 395, loss = 0.31374379\n",
      "Iteration 396, loss = 0.31331430\n",
      "Iteration 397, loss = 0.31288593\n",
      "Iteration 398, loss = 0.31245869\n",
      "Iteration 399, loss = 0.31203256\n",
      "Iteration 400, loss = 0.31160755\n",
      "Iteration 401, loss = 0.31118363\n",
      "Iteration 402, loss = 0.31076082\n",
      "Iteration 403, loss = 0.31033919\n",
      "Iteration 404, loss = 0.30991867\n",
      "Iteration 405, loss = 0.30949930\n",
      "Iteration 406, loss = 0.30908103\n",
      "Iteration 407, loss = 0.30866385\n",
      "Iteration 408, loss = 0.30824773\n",
      "Iteration 409, loss = 0.30783269\n",
      "Iteration 410, loss = 0.30741871\n",
      "Iteration 411, loss = 0.30700578\n",
      "Iteration 412, loss = 0.30659390\n",
      "Iteration 413, loss = 0.30618307\n",
      "Iteration 414, loss = 0.30577328\n",
      "Iteration 415, loss = 0.30536452\n",
      "Iteration 416, loss = 0.30495680\n",
      "Iteration 417, loss = 0.30455010\n",
      "Iteration 418, loss = 0.30414441\n",
      "Iteration 419, loss = 0.30373975\n",
      "Iteration 420, loss = 0.30333610\n",
      "Iteration 421, loss = 0.30293345\n",
      "Iteration 422, loss = 0.30253182\n",
      "Iteration 423, loss = 0.30213120\n",
      "Iteration 424, loss = 0.30173161\n",
      "Iteration 425, loss = 0.30133301\n",
      "Iteration 426, loss = 0.30093542\n",
      "Iteration 427, loss = 0.30053881\n",
      "Iteration 428, loss = 0.30014319\n",
      "Iteration 429, loss = 0.29974855\n",
      "Iteration 430, loss = 0.29935487\n",
      "Iteration 431, loss = 0.29896217\n",
      "Iteration 432, loss = 0.29857043\n",
      "Iteration 433, loss = 0.29817965\n",
      "Iteration 434, loss = 0.29778984\n",
      "Iteration 435, loss = 0.29740099\n",
      "Iteration 436, loss = 0.29701311\n",
      "Iteration 437, loss = 0.29662624\n",
      "Iteration 438, loss = 0.29624032\n",
      "Iteration 439, loss = 0.29585536\n",
      "Iteration 440, loss = 0.29547134\n",
      "Iteration 441, loss = 0.29508827\n",
      "Iteration 442, loss = 0.29470613\n",
      "Iteration 443, loss = 0.29432492\n",
      "Iteration 444, loss = 0.29394463\n",
      "Iteration 445, loss = 0.29356528\n",
      "Iteration 446, loss = 0.29318691\n",
      "Iteration 447, loss = 0.29280947\n",
      "Iteration 448, loss = 0.29243297\n",
      "Iteration 449, loss = 0.29205738\n",
      "Iteration 450, loss = 0.29168271\n",
      "Iteration 451, loss = 0.29130895\n",
      "Iteration 452, loss = 0.29093608\n",
      "Iteration 453, loss = 0.29056412\n",
      "Iteration 454, loss = 0.29019307\n",
      "Iteration 455, loss = 0.28982290\n",
      "Iteration 456, loss = 0.28945363\n",
      "Iteration 457, loss = 0.28908524\n",
      "Iteration 458, loss = 0.28871773\n",
      "Iteration 459, loss = 0.28835111\n",
      "Iteration 460, loss = 0.28798538\n",
      "Iteration 461, loss = 0.28762051\n",
      "Iteration 462, loss = 0.28725652\n",
      "Iteration 463, loss = 0.28689369\n",
      "Iteration 464, loss = 0.28653162\n",
      "Iteration 465, loss = 0.28617036\n",
      "Iteration 466, loss = 0.28581006\n",
      "Iteration 467, loss = 0.28545041\n",
      "Iteration 468, loss = 0.28509169\n",
      "Iteration 469, loss = 0.28473379\n",
      "Iteration 470, loss = 0.28437687\n",
      "Iteration 471, loss = 0.28402082\n",
      "Iteration 472, loss = 0.28366561\n",
      "Iteration 473, loss = 0.28331123\n",
      "Iteration 474, loss = 0.28295768\n",
      "Iteration 475, loss = 0.28260495\n",
      "Iteration 476, loss = 0.28225305\n",
      "Iteration 477, loss = 0.28190210\n",
      "Iteration 478, loss = 0.28155188\n",
      "Iteration 479, loss = 0.28120247\n",
      "Iteration 480, loss = 0.28085395\n",
      "Iteration 481, loss = 0.28050623\n",
      "Iteration 482, loss = 0.28015931\n",
      "Iteration 483, loss = 0.27981323\n",
      "Iteration 484, loss = 0.27946794\n",
      "Iteration 485, loss = 0.27912356\n",
      "Iteration 486, loss = 0.27877986\n",
      "Iteration 487, loss = 0.27843704\n",
      "Iteration 488, loss = 0.27809502\n",
      "Iteration 489, loss = 0.27775389\n",
      "Iteration 490, loss = 0.27741365\n",
      "Iteration 491, loss = 0.27707414\n",
      "Iteration 492, loss = 0.27673551\n",
      "Iteration 493, loss = 0.27639762\n",
      "Iteration 494, loss = 0.27606053\n",
      "Iteration 495, loss = 0.27572429\n",
      "Iteration 496, loss = 0.27538878\n",
      "Iteration 497, loss = 0.27505416\n",
      "Iteration 498, loss = 0.27472026\n",
      "Iteration 499, loss = 0.27438715\n",
      "Iteration 500, loss = 0.27405482\n",
      "Iteration 501, loss = 0.27372338\n",
      "Iteration 502, loss = 0.27339270\n",
      "Iteration 503, loss = 0.27306282\n",
      "Iteration 504, loss = 0.27273368\n",
      "Iteration 505, loss = 0.27240538\n",
      "Iteration 506, loss = 0.27207779\n",
      "Iteration 507, loss = 0.27175100\n",
      "Iteration 508, loss = 0.27142506\n",
      "Iteration 509, loss = 0.27109994\n",
      "Iteration 510, loss = 0.27077555\n",
      "Iteration 511, loss = 0.27045198\n",
      "Iteration 512, loss = 0.27012916\n",
      "Iteration 513, loss = 0.26980723\n",
      "Iteration 514, loss = 0.26948607\n",
      "Iteration 515, loss = 0.26916571\n",
      "Iteration 516, loss = 0.26884610\n",
      "Iteration 517, loss = 0.26852728\n",
      "Iteration 518, loss = 0.26820916\n",
      "Iteration 519, loss = 0.26789187\n",
      "Iteration 520, loss = 0.26757522\n",
      "Iteration 521, loss = 0.26725936\n",
      "Iteration 522, loss = 0.26694421\n",
      "Iteration 523, loss = 0.26662984\n",
      "Iteration 524, loss = 0.26631615\n",
      "Iteration 525, loss = 0.26600322\n",
      "Iteration 526, loss = 0.26569099\n",
      "Iteration 527, loss = 0.26537954\n",
      "Iteration 528, loss = 0.26506895\n",
      "Iteration 529, loss = 0.26475913\n",
      "Iteration 530, loss = 0.26444999\n",
      "Iteration 531, loss = 0.26414161\n",
      "Iteration 532, loss = 0.26383391\n",
      "Iteration 533, loss = 0.26352694\n",
      "Iteration 534, loss = 0.26322068\n",
      "Iteration 535, loss = 0.26291511\n",
      "Iteration 536, loss = 0.26261029\n",
      "Iteration 537, loss = 0.26230613\n",
      "Iteration 538, loss = 0.26200269\n",
      "Iteration 539, loss = 0.26169992\n",
      "Iteration 540, loss = 0.26139793\n",
      "Iteration 541, loss = 0.26109652\n",
      "Iteration 542, loss = 0.26079589\n",
      "Iteration 543, loss = 0.26049589\n",
      "Iteration 544, loss = 0.26019662\n",
      "Iteration 545, loss = 0.25989801\n",
      "Iteration 546, loss = 0.25960008\n",
      "Iteration 547, loss = 0.25930287\n",
      "Iteration 548, loss = 0.25900628\n",
      "Iteration 549, loss = 0.25871045\n",
      "Iteration 550, loss = 0.25841521\n",
      "Iteration 551, loss = 0.25812066\n",
      "Iteration 552, loss = 0.25782683\n",
      "Iteration 553, loss = 0.25753362\n",
      "Iteration 554, loss = 0.25724114\n",
      "Iteration 555, loss = 0.25694936\n",
      "Iteration 556, loss = 0.25665831\n",
      "Iteration 557, loss = 0.25636787\n",
      "Iteration 558, loss = 0.25607812\n",
      "Iteration 559, loss = 0.25578905\n",
      "Iteration 560, loss = 0.25550060\n",
      "Iteration 561, loss = 0.25521282\n",
      "Iteration 562, loss = 0.25492569\n",
      "Iteration 563, loss = 0.25463924\n",
      "Iteration 564, loss = 0.25435341\n",
      "Iteration 565, loss = 0.25406824\n",
      "Iteration 566, loss = 0.25378377\n",
      "Iteration 567, loss = 0.25349992\n",
      "Iteration 568, loss = 0.25321673\n",
      "Iteration 569, loss = 0.25293417\n",
      "Iteration 570, loss = 0.25265226\n",
      "Iteration 571, loss = 0.25237101\n",
      "Iteration 572, loss = 0.25209038\n",
      "Iteration 573, loss = 0.25181041\n",
      "Iteration 574, loss = 0.25153104\n",
      "Iteration 575, loss = 0.25125235\n",
      "Iteration 576, loss = 0.25097428\n",
      "Iteration 577, loss = 0.25069683\n",
      "Iteration 578, loss = 0.25042003\n",
      "Iteration 579, loss = 0.25014383\n",
      "Iteration 580, loss = 0.24986827\n",
      "Iteration 581, loss = 0.24959338\n",
      "Iteration 582, loss = 0.24931916\n",
      "Iteration 583, loss = 0.24904557\n",
      "Iteration 584, loss = 0.24877258\n",
      "Iteration 585, loss = 0.24850018\n",
      "Iteration 586, loss = 0.24822844\n",
      "Iteration 587, loss = 0.24795726\n",
      "Iteration 588, loss = 0.24768669\n",
      "Iteration 589, loss = 0.24741682\n",
      "Iteration 590, loss = 0.24714748\n",
      "Iteration 591, loss = 0.24687880\n",
      "Iteration 592, loss = 0.24661069\n",
      "Iteration 593, loss = 0.24634318\n",
      "Iteration 594, loss = 0.24607632\n",
      "Iteration 595, loss = 0.24581002\n",
      "Iteration 596, loss = 0.24554432\n",
      "Iteration 597, loss = 0.24527925\n",
      "Iteration 598, loss = 0.24501474\n",
      "Iteration 599, loss = 0.24475086\n",
      "Iteration 600, loss = 0.24448754\n",
      "Iteration 601, loss = 0.24422484\n",
      "Iteration 602, loss = 0.24396271\n",
      "Iteration 603, loss = 0.24370117\n",
      "Iteration 604, loss = 0.24344022\n",
      "Iteration 605, loss = 0.24317987\n",
      "Iteration 606, loss = 0.24292007\n",
      "Iteration 607, loss = 0.24266091\n",
      "Iteration 608, loss = 0.24240226\n",
      "Iteration 609, loss = 0.24214424\n",
      "Iteration 610, loss = 0.24188678\n",
      "Iteration 611, loss = 0.24162990\n",
      "Iteration 612, loss = 0.24137359\n",
      "Iteration 613, loss = 0.24111786\n",
      "Iteration 614, loss = 0.24086269\n",
      "Iteration 615, loss = 0.24060811\n",
      "Iteration 616, loss = 0.24035408\n",
      "Iteration 617, loss = 0.24010063\n",
      "Iteration 618, loss = 0.23984775\n",
      "Iteration 619, loss = 0.23959542\n",
      "Iteration 620, loss = 0.23934365\n",
      "Iteration 621, loss = 0.23909248\n",
      "Iteration 622, loss = 0.23884183\n",
      "Iteration 623, loss = 0.23859175\n",
      "Iteration 624, loss = 0.23834225\n",
      "Iteration 625, loss = 0.23809326\n",
      "Iteration 626, loss = 0.23784488\n",
      "Iteration 627, loss = 0.23759702\n",
      "Iteration 628, loss = 0.23734972\n",
      "Iteration 629, loss = 0.23710294\n",
      "Iteration 630, loss = 0.23685678\n",
      "Iteration 631, loss = 0.23661110\n",
      "Iteration 632, loss = 0.23636601\n",
      "Iteration 633, loss = 0.23612144\n",
      "Iteration 634, loss = 0.23587743\n",
      "Iteration 635, loss = 0.23563393\n",
      "Iteration 636, loss = 0.23539102\n",
      "Iteration 637, loss = 0.23514862\n",
      "Iteration 638, loss = 0.23490676\n",
      "Iteration 639, loss = 0.23466546\n",
      "Iteration 640, loss = 0.23442466\n",
      "Iteration 641, loss = 0.23418443\n",
      "Iteration 642, loss = 0.23394470\n",
      "Iteration 643, loss = 0.23370552\n",
      "Iteration 644, loss = 0.23346689\n",
      "Iteration 645, loss = 0.23322876\n",
      "Iteration 646, loss = 0.23299116\n",
      "Iteration 647, loss = 0.23275410\n",
      "Iteration 648, loss = 0.23251757\n",
      "Iteration 649, loss = 0.23228154\n",
      "Iteration 650, loss = 0.23204605\n",
      "Iteration 651, loss = 0.23181108\n",
      "Iteration 652, loss = 0.23157661\n",
      "Iteration 653, loss = 0.23134269\n",
      "Iteration 654, loss = 0.23110927\n",
      "Iteration 655, loss = 0.23087638\n",
      "Iteration 656, loss = 0.23064399\n",
      "Iteration 657, loss = 0.23041212\n",
      "Iteration 658, loss = 0.23018076\n",
      "Iteration 659, loss = 0.22994991\n",
      "Iteration 660, loss = 0.22971958\n",
      "Iteration 661, loss = 0.22948974\n",
      "Iteration 662, loss = 0.22926043\n",
      "Iteration 663, loss = 0.22903162\n",
      "Iteration 664, loss = 0.22880333\n",
      "Iteration 665, loss = 0.22857552\n",
      "Iteration 666, loss = 0.22834827\n",
      "Iteration 667, loss = 0.22812147\n",
      "Iteration 668, loss = 0.22789521\n",
      "Iteration 669, loss = 0.22766943\n",
      "Iteration 670, loss = 0.22744415\n",
      "Iteration 671, loss = 0.22721936\n",
      "Iteration 672, loss = 0.22699510\n",
      "Iteration 673, loss = 0.22677130\n",
      "Iteration 674, loss = 0.22654801\n",
      "Iteration 675, loss = 0.22632524\n",
      "Iteration 676, loss = 0.22610294\n",
      "Iteration 677, loss = 0.22588113\n",
      "Iteration 678, loss = 0.22565983\n",
      "Iteration 679, loss = 0.22543901\n",
      "Iteration 680, loss = 0.22521866\n",
      "Iteration 681, loss = 0.22499880\n",
      "Iteration 682, loss = 0.22477944\n",
      "Iteration 683, loss = 0.22456055\n",
      "Iteration 684, loss = 0.22434214\n",
      "Iteration 685, loss = 0.22412420\n",
      "Iteration 686, loss = 0.22390676\n",
      "Iteration 687, loss = 0.22368978\n",
      "Iteration 688, loss = 0.22347329\n",
      "Iteration 689, loss = 0.22325727\n",
      "Iteration 690, loss = 0.22304170\n",
      "Iteration 691, loss = 0.22282663\n",
      "Iteration 692, loss = 0.22261204\n",
      "Iteration 693, loss = 0.22239788\n",
      "Iteration 694, loss = 0.22218421\n",
      "Iteration 695, loss = 0.22197103\n",
      "Iteration 696, loss = 0.22175830\n",
      "Iteration 697, loss = 0.22154606\n",
      "Iteration 698, loss = 0.22133426\n",
      "Iteration 699, loss = 0.22112295\n",
      "Iteration 700, loss = 0.22091207\n",
      "Iteration 701, loss = 0.22070167\n",
      "Iteration 702, loss = 0.22049173\n",
      "Iteration 703, loss = 0.22028224\n",
      "Iteration 704, loss = 0.22007322\n",
      "Iteration 705, loss = 0.21986464\n",
      "Iteration 706, loss = 0.21965654\n",
      "Iteration 707, loss = 0.21944886\n",
      "Iteration 708, loss = 0.21924165\n",
      "Iteration 709, loss = 0.21903491\n",
      "Iteration 710, loss = 0.21882860\n",
      "Iteration 711, loss = 0.21862274\n",
      "Iteration 712, loss = 0.21841732\n",
      "Iteration 713, loss = 0.21821237\n",
      "Iteration 714, loss = 0.21800784\n",
      "Iteration 715, loss = 0.21780376\n",
      "Iteration 716, loss = 0.21760014\n",
      "Iteration 717, loss = 0.21739695\n",
      "Iteration 718, loss = 0.21719420\n",
      "Iteration 719, loss = 0.21699190\n",
      "Iteration 720, loss = 0.21679003\n",
      "Iteration 721, loss = 0.21658860\n",
      "Iteration 722, loss = 0.21638760\n",
      "Iteration 723, loss = 0.21618705\n",
      "Iteration 724, loss = 0.21598693\n",
      "Iteration 725, loss = 0.21578723\n",
      "Iteration 726, loss = 0.21558799\n",
      "Iteration 727, loss = 0.21538915\n",
      "Iteration 728, loss = 0.21519076\n",
      "Iteration 729, loss = 0.21499279\n",
      "Iteration 730, loss = 0.21479527\n",
      "Iteration 731, loss = 0.21459815\n",
      "Iteration 732, loss = 0.21440146\n",
      "Iteration 733, loss = 0.21420521\n",
      "Iteration 734, loss = 0.21400937\n",
      "Iteration 735, loss = 0.21381395\n",
      "Iteration 736, loss = 0.21361897\n",
      "Iteration 737, loss = 0.21342440\n",
      "Iteration 738, loss = 0.21323025\n",
      "Iteration 739, loss = 0.21303651\n",
      "Iteration 740, loss = 0.21284322\n",
      "Iteration 741, loss = 0.21265030\n",
      "Iteration 742, loss = 0.21245782\n",
      "Iteration 743, loss = 0.21226576\n",
      "Iteration 744, loss = 0.21207411\n",
      "Iteration 745, loss = 0.21188286\n",
      "Iteration 746, loss = 0.21169203\n",
      "Iteration 747, loss = 0.21150162\n",
      "Iteration 748, loss = 0.21131160\n",
      "Iteration 749, loss = 0.21112199\n",
      "Iteration 750, loss = 0.21093281\n",
      "Iteration 751, loss = 0.21074402\n",
      "Iteration 752, loss = 0.21055564\n",
      "Iteration 753, loss = 0.21036765\n",
      "Iteration 754, loss = 0.21018009\n",
      "Iteration 755, loss = 0.20999292\n",
      "Iteration 756, loss = 0.20980614\n",
      "Iteration 757, loss = 0.20961978\n",
      "Iteration 758, loss = 0.20943382\n",
      "Iteration 759, loss = 0.20924825\n",
      "Iteration 760, loss = 0.20906307\n",
      "Iteration 761, loss = 0.20887830\n",
      "Iteration 762, loss = 0.20869392\n",
      "Iteration 763, loss = 0.20850994\n",
      "Iteration 764, loss = 0.20832634\n",
      "Iteration 765, loss = 0.20814316\n",
      "Iteration 766, loss = 0.20796034\n",
      "Iteration 767, loss = 0.20777792\n",
      "Iteration 768, loss = 0.20759590\n",
      "Iteration 769, loss = 0.20741426\n",
      "Iteration 770, loss = 0.20723301\n",
      "Iteration 771, loss = 0.20705214\n",
      "Iteration 772, loss = 0.20687168\n",
      "Iteration 773, loss = 0.20669158\n",
      "Iteration 774, loss = 0.20651186\n",
      "Iteration 775, loss = 0.20633253\n",
      "Iteration 776, loss = 0.20615360\n",
      "Iteration 777, loss = 0.20597502\n",
      "Iteration 778, loss = 0.20579683\n",
      "Iteration 779, loss = 0.20561904\n",
      "Iteration 780, loss = 0.20544161\n",
      "Iteration 781, loss = 0.20526456\n",
      "Iteration 782, loss = 0.20508787\n",
      "Iteration 783, loss = 0.20491159\n",
      "Iteration 784, loss = 0.20473565\n",
      "Iteration 785, loss = 0.20456010\n",
      "Iteration 786, loss = 0.20438493\n",
      "Iteration 787, loss = 0.20421012\n",
      "Iteration 788, loss = 0.20403568\n",
      "Iteration 789, loss = 0.20386160\n",
      "Iteration 790, loss = 0.20368792\n",
      "Iteration 791, loss = 0.20351458\n",
      "Iteration 792, loss = 0.20334161\n",
      "Iteration 793, loss = 0.20316902\n",
      "Iteration 794, loss = 0.20299678\n",
      "Iteration 795, loss = 0.20282491\n",
      "Iteration 796, loss = 0.20265340\n",
      "Iteration 797, loss = 0.20248228\n",
      "Iteration 798, loss = 0.20231148\n",
      "Iteration 799, loss = 0.20214106\n",
      "Iteration 800, loss = 0.20197101\n",
      "Iteration 801, loss = 0.20180129\n",
      "Iteration 802, loss = 0.20163195\n",
      "Iteration 803, loss = 0.20146295\n",
      "Iteration 804, loss = 0.20129434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 805, loss = 0.20112605\n",
      "Iteration 806, loss = 0.20095812\n",
      "Iteration 807, loss = 0.20079056\n",
      "Iteration 808, loss = 0.20062334\n",
      "Iteration 809, loss = 0.20045648\n",
      "Iteration 810, loss = 0.20028996\n",
      "Iteration 811, loss = 0.20012380\n",
      "Iteration 812, loss = 0.19995798\n",
      "Iteration 813, loss = 0.19979251\n",
      "Iteration 814, loss = 0.19962739\n",
      "Iteration 815, loss = 0.19946262\n",
      "Iteration 816, loss = 0.19929819\n",
      "Iteration 817, loss = 0.19913411\n",
      "Iteration 818, loss = 0.19897037\n",
      "Iteration 819, loss = 0.19880698\n",
      "Iteration 820, loss = 0.19864393\n",
      "Iteration 821, loss = 0.19848122\n",
      "Iteration 822, loss = 0.19831887\n",
      "Iteration 823, loss = 0.19815684\n",
      "Iteration 824, loss = 0.19799515\n",
      "Iteration 825, loss = 0.19783381\n",
      "Iteration 826, loss = 0.19767280\n",
      "Iteration 827, loss = 0.19751212\n",
      "Iteration 828, loss = 0.19735178\n",
      "Iteration 829, loss = 0.19719179\n",
      "Iteration 830, loss = 0.19703211\n",
      "Iteration 831, loss = 0.19687277\n",
      "Iteration 832, loss = 0.19671377\n",
      "Iteration 833, loss = 0.19655510\n",
      "Iteration 834, loss = 0.19639676\n",
      "Iteration 835, loss = 0.19623874\n",
      "Iteration 836, loss = 0.19608106\n",
      "Iteration 837, loss = 0.19592370\n",
      "Iteration 838, loss = 0.19576667\n",
      "Iteration 839, loss = 0.19560998\n",
      "Iteration 840, loss = 0.19545359\n",
      "Iteration 841, loss = 0.19529753\n",
      "Iteration 842, loss = 0.19514181\n",
      "Iteration 843, loss = 0.19498640\n",
      "Iteration 844, loss = 0.19483132\n",
      "Iteration 845, loss = 0.19467656\n",
      "Iteration 846, loss = 0.19452212\n",
      "Iteration 847, loss = 0.19436799\n",
      "Iteration 848, loss = 0.19421419\n",
      "Iteration 849, loss = 0.19406070\n",
      "Iteration 850, loss = 0.19390754\n",
      "Iteration 851, loss = 0.19375469\n",
      "Iteration 852, loss = 0.19360215\n",
      "Iteration 853, loss = 0.19344994\n",
      "Iteration 854, loss = 0.19329803\n",
      "Iteration 855, loss = 0.19314644\n",
      "Iteration 856, loss = 0.19299516\n",
      "Iteration 857, loss = 0.19284420\n",
      "Iteration 858, loss = 0.19269354\n",
      "Iteration 859, loss = 0.19254319\n",
      "Iteration 860, loss = 0.19239317\n",
      "Iteration 861, loss = 0.19224343\n",
      "Iteration 862, loss = 0.19209401\n",
      "Iteration 863, loss = 0.19194491\n",
      "Iteration 864, loss = 0.19179610\n",
      "Iteration 865, loss = 0.19164760\n",
      "Iteration 866, loss = 0.19149940\n",
      "Iteration 867, loss = 0.19135152\n",
      "Iteration 868, loss = 0.19120393\n",
      "Iteration 869, loss = 0.19105664\n",
      "Iteration 870, loss = 0.19090967\n",
      "Iteration 871, loss = 0.19076299\n",
      "Iteration 872, loss = 0.19061660\n",
      "Iteration 873, loss = 0.19047052\n",
      "Iteration 874, loss = 0.19032476\n",
      "Iteration 875, loss = 0.19017926\n",
      "Iteration 876, loss = 0.19003408\n",
      "Iteration 877, loss = 0.18988919\n",
      "Iteration 878, loss = 0.18974460\n",
      "Iteration 879, loss = 0.18960030\n",
      "Iteration 880, loss = 0.18945632\n",
      "Iteration 881, loss = 0.18931262\n",
      "Iteration 882, loss = 0.18916921\n",
      "Iteration 883, loss = 0.18902610\n",
      "Iteration 884, loss = 0.18888329\n",
      "Iteration 885, loss = 0.18874076\n",
      "Iteration 886, loss = 0.18859852\n",
      "Iteration 887, loss = 0.18845659\n",
      "Iteration 888, loss = 0.18831493\n",
      "Iteration 889, loss = 0.18817356\n",
      "Iteration 890, loss = 0.18803248\n",
      "Iteration 891, loss = 0.18789169\n",
      "Iteration 892, loss = 0.18775118\n",
      "Iteration 893, loss = 0.18761096\n",
      "Iteration 894, loss = 0.18747103\n",
      "Iteration 895, loss = 0.18733138\n",
      "Iteration 896, loss = 0.18719200\n",
      "Iteration 897, loss = 0.18705292\n",
      "Iteration 898, loss = 0.18691412\n",
      "Iteration 899, loss = 0.18677559\n",
      "Iteration 900, loss = 0.18663735\n",
      "Iteration 901, loss = 0.18649939\n",
      "Iteration 902, loss = 0.18636170\n",
      "Iteration 903, loss = 0.18622430\n",
      "Iteration 904, loss = 0.18608717\n",
      "Iteration 905, loss = 0.18595031\n",
      "Iteration 906, loss = 0.18581375\n",
      "Iteration 907, loss = 0.18567744\n",
      "Iteration 908, loss = 0.18554142\n",
      "Iteration 909, loss = 0.18540567\n",
      "Iteration 910, loss = 0.18527019\n",
      "Iteration 911, loss = 0.18513500\n",
      "Iteration 912, loss = 0.18500005\n",
      "Iteration 913, loss = 0.18486541\n",
      "Iteration 914, loss = 0.18473101\n",
      "Iteration 915, loss = 0.18459690\n",
      "Iteration 916, loss = 0.18446305\n",
      "Iteration 917, loss = 0.18432947\n",
      "Iteration 918, loss = 0.18419616\n",
      "Iteration 919, loss = 0.18406312\n",
      "Iteration 920, loss = 0.18393034\n",
      "Iteration 921, loss = 0.18379782\n",
      "Iteration 922, loss = 0.18366558\n",
      "Iteration 923, loss = 0.18353360\n",
      "Iteration 924, loss = 0.18340188\n",
      "Iteration 925, loss = 0.18327044\n",
      "Iteration 926, loss = 0.18313926\n",
      "Iteration 927, loss = 0.18300834\n",
      "Iteration 928, loss = 0.18287768\n",
      "Iteration 929, loss = 0.18274729\n",
      "Iteration 930, loss = 0.18261716\n",
      "Iteration 931, loss = 0.18248728\n",
      "Iteration 932, loss = 0.18235768\n",
      "Iteration 933, loss = 0.18222831\n",
      "Iteration 934, loss = 0.18209922\n",
      "Iteration 935, loss = 0.18197038\n",
      "Iteration 936, loss = 0.18184179\n",
      "Iteration 937, loss = 0.18171346\n",
      "Iteration 938, loss = 0.18158539\n",
      "Iteration 939, loss = 0.18145757\n",
      "Iteration 940, loss = 0.18133001\n",
      "Iteration 941, loss = 0.18120270\n",
      "Iteration 942, loss = 0.18107564\n",
      "Iteration 943, loss = 0.18094883\n",
      "Iteration 944, loss = 0.18082228\n",
      "Iteration 945, loss = 0.18069597\n",
      "Iteration 946, loss = 0.18056991\n",
      "Iteration 947, loss = 0.18044412\n",
      "Iteration 948, loss = 0.18031855\n",
      "Iteration 949, loss = 0.18019325\n",
      "Iteration 950, loss = 0.18006820\n",
      "Iteration 951, loss = 0.17994338\n",
      "Iteration 952, loss = 0.17981881\n",
      "Iteration 953, loss = 0.17969449\n",
      "Iteration 954, loss = 0.17957042\n",
      "Iteration 955, loss = 0.17944659\n",
      "Iteration 956, loss = 0.17932301\n",
      "Iteration 957, loss = 0.17919966\n",
      "Iteration 958, loss = 0.17907656\n",
      "Iteration 959, loss = 0.17895371\n",
      "Iteration 960, loss = 0.17883109\n",
      "Iteration 961, loss = 0.17870872\n",
      "Iteration 962, loss = 0.17858658\n",
      "Iteration 963, loss = 0.17846469\n",
      "Iteration 964, loss = 0.17834303\n",
      "Iteration 965, loss = 0.17822162\n",
      "Iteration 966, loss = 0.17810044\n",
      "Iteration 967, loss = 0.17797949\n",
      "Iteration 968, loss = 0.17785880\n",
      "Iteration 969, loss = 0.17773833\n",
      "Iteration 970, loss = 0.17761810\n",
      "Iteration 971, loss = 0.17749810\n",
      "Iteration 972, loss = 0.17737834\n",
      "Iteration 973, loss = 0.17725881\n",
      "Iteration 974, loss = 0.17713952\n",
      "Iteration 975, loss = 0.17702046\n",
      "Iteration 976, loss = 0.17690163\n",
      "Iteration 977, loss = 0.17678304\n",
      "Iteration 978, loss = 0.17666466\n",
      "Iteration 979, loss = 0.17654653\n",
      "Iteration 980, loss = 0.17642863\n",
      "Iteration 981, loss = 0.17631095\n",
      "Iteration 982, loss = 0.17619351\n",
      "Iteration 983, loss = 0.17607629\n",
      "Iteration 984, loss = 0.17595930\n",
      "Iteration 985, loss = 0.17584254\n",
      "Iteration 986, loss = 0.17572600\n",
      "Iteration 987, loss = 0.17560969\n",
      "Iteration 988, loss = 0.17549362\n",
      "Iteration 989, loss = 0.17537775\n",
      "Iteration 990, loss = 0.17526212\n",
      "Iteration 991, loss = 0.17514672\n",
      "Iteration 992, loss = 0.17503153\n",
      "Iteration 993, loss = 0.17491658\n",
      "Iteration 994, loss = 0.17480184\n",
      "Iteration 995, loss = 0.17468732\n",
      "Iteration 996, loss = 0.17457302\n",
      "Iteration 997, loss = 0.17445896\n",
      "Iteration 998, loss = 0.17434509\n",
      "Iteration 999, loss = 0.17423146\n",
      "Iteration 1000, loss = 0.17411805\n",
      "Iteration 1, loss = 1.90632821\n",
      "Iteration 2, loss = 1.86307472\n",
      "Iteration 3, loss = 1.80477302\n",
      "Iteration 4, loss = 1.73594792\n",
      "Iteration 5, loss = 1.66064792\n",
      "Iteration 6, loss = 1.58219730\n",
      "Iteration 7, loss = 1.50315280\n",
      "Iteration 8, loss = 1.42572581\n",
      "Iteration 9, loss = 1.35195416\n",
      "Iteration 10, loss = 1.28373555\n",
      "Iteration 11, loss = 1.22308546\n",
      "Iteration 12, loss = 1.17184819\n",
      "Iteration 13, loss = 1.13132138\n",
      "Iteration 14, loss = 1.10178134\n",
      "Iteration 15, loss = 1.08216894\n",
      "Iteration 16, loss = 1.07059221\n",
      "Iteration 17, loss = 1.06418638\n",
      "Iteration 18, loss = 1.06014108\n",
      "Iteration 19, loss = 1.05606350\n",
      "Iteration 20, loss = 1.05040104\n",
      "Iteration 21, loss = 1.04226978\n",
      "Iteration 22, loss = 1.03161972\n",
      "Iteration 23, loss = 1.01894451\n",
      "Iteration 24, loss = 1.00506922\n",
      "Iteration 25, loss = 0.99069161\n",
      "Iteration 26, loss = 0.97650337\n",
      "Iteration 27, loss = 0.96292748\n",
      "Iteration 28, loss = 0.95017062\n",
      "Iteration 29, loss = 0.93817777\n",
      "Iteration 30, loss = 0.92673713\n",
      "Iteration 31, loss = 0.91564634\n",
      "Iteration 32, loss = 0.90466522\n",
      "Iteration 33, loss = 0.89343732\n",
      "Iteration 34, loss = 0.88203946\n",
      "Iteration 35, loss = 0.87049809\n",
      "Iteration 36, loss = 0.85877471\n",
      "Iteration 37, loss = 0.84717928\n",
      "Iteration 38, loss = 0.83590328\n",
      "Iteration 39, loss = 0.82521303\n",
      "Iteration 40, loss = 0.81523123\n",
      "Iteration 41, loss = 0.80597913\n",
      "Iteration 42, loss = 0.79748853\n",
      "Iteration 43, loss = 0.78970429\n",
      "Iteration 44, loss = 0.78259287\n",
      "Iteration 45, loss = 0.77586166\n",
      "Iteration 46, loss = 0.76935714\n",
      "Iteration 47, loss = 0.76294884\n",
      "Iteration 48, loss = 0.75658439\n",
      "Iteration 49, loss = 0.75031111\n",
      "Iteration 50, loss = 0.74413790\n",
      "Iteration 51, loss = 0.73813293\n",
      "Iteration 52, loss = 0.73225967\n",
      "Iteration 53, loss = 0.72657126\n",
      "Iteration 54, loss = 0.72113499\n",
      "Iteration 55, loss = 0.71591916\n",
      "Iteration 56, loss = 0.71090241\n",
      "Iteration 57, loss = 0.70606740\n",
      "Iteration 58, loss = 0.70133515\n",
      "Iteration 59, loss = 0.69670474\n",
      "Iteration 60, loss = 0.69215917\n",
      "Iteration 61, loss = 0.68767828\n",
      "Iteration 62, loss = 0.68328853\n",
      "Iteration 63, loss = 0.67899035\n",
      "Iteration 64, loss = 0.67480037\n",
      "Iteration 65, loss = 0.67073151\n",
      "Iteration 66, loss = 0.66676918\n",
      "Iteration 67, loss = 0.66289264\n",
      "Iteration 68, loss = 0.65913628\n",
      "Iteration 69, loss = 0.65546013\n",
      "Iteration 70, loss = 0.65184591\n",
      "Iteration 71, loss = 0.64830357\n",
      "Iteration 72, loss = 0.64482276\n",
      "Iteration 73, loss = 0.64138948\n",
      "Iteration 74, loss = 0.63800031\n",
      "Iteration 75, loss = 0.63465265\n",
      "Iteration 76, loss = 0.63134920\n",
      "Iteration 77, loss = 0.62809007\n",
      "Iteration 78, loss = 0.62487539\n",
      "Iteration 79, loss = 0.62170574\n",
      "Iteration 80, loss = 0.61858603\n",
      "Iteration 81, loss = 0.61551494\n",
      "Iteration 82, loss = 0.61248987\n",
      "Iteration 83, loss = 0.60950888\n",
      "Iteration 84, loss = 0.60657587\n",
      "Iteration 85, loss = 0.60368833\n",
      "Iteration 86, loss = 0.60084281\n",
      "Iteration 87, loss = 0.59803621\n",
      "Iteration 88, loss = 0.59526739\n",
      "Iteration 89, loss = 0.59253532\n",
      "Iteration 90, loss = 0.58983900\n",
      "Iteration 91, loss = 0.58717750\n",
      "Iteration 92, loss = 0.58454997\n",
      "Iteration 93, loss = 0.58195683\n",
      "Iteration 94, loss = 0.57939727\n",
      "Iteration 95, loss = 0.57687009\n",
      "Iteration 96, loss = 0.57437629\n",
      "Iteration 97, loss = 0.57191567\n",
      "Iteration 98, loss = 0.56948601\n",
      "Iteration 99, loss = 0.56708680\n",
      "Iteration 100, loss = 0.56471747\n",
      "Iteration 101, loss = 0.56237731\n",
      "Iteration 102, loss = 0.56006511\n",
      "Iteration 103, loss = 0.55778117\n",
      "Iteration 104, loss = 0.55552621\n",
      "Iteration 105, loss = 0.55329878\n",
      "Iteration 106, loss = 0.55109822\n",
      "Iteration 107, loss = 0.54892409\n",
      "Iteration 108, loss = 0.54677593\n",
      "Iteration 109, loss = 0.54465330\n",
      "Iteration 110, loss = 0.54255624\n",
      "Iteration 111, loss = 0.54048454\n",
      "Iteration 112, loss = 0.53843707\n",
      "Iteration 113, loss = 0.53641337\n",
      "Iteration 114, loss = 0.53441303\n",
      "Iteration 115, loss = 0.53243565\n",
      "Iteration 116, loss = 0.53048080\n",
      "Iteration 117, loss = 0.52854807\n",
      "Iteration 118, loss = 0.52663707\n",
      "Iteration 119, loss = 0.52474739\n",
      "Iteration 120, loss = 0.52287883\n",
      "Iteration 121, loss = 0.52103094\n",
      "Iteration 122, loss = 0.51920321\n",
      "Iteration 123, loss = 0.51739530\n",
      "Iteration 124, loss = 0.51560712\n",
      "Iteration 125, loss = 0.51383799\n",
      "Iteration 126, loss = 0.51208776\n",
      "Iteration 127, loss = 0.51035602\n",
      "Iteration 128, loss = 0.50864257\n",
      "Iteration 129, loss = 0.50694688\n",
      "Iteration 130, loss = 0.50526917\n",
      "Iteration 131, loss = 0.50360960\n",
      "Iteration 132, loss = 0.50196699\n",
      "Iteration 133, loss = 0.50034108\n",
      "Iteration 134, loss = 0.49873159\n",
      "Iteration 135, loss = 0.49713828\n",
      "Iteration 136, loss = 0.49556087\n",
      "Iteration 137, loss = 0.49399910\n",
      "Iteration 138, loss = 0.49245271\n",
      "Iteration 139, loss = 0.49092144\n",
      "Iteration 140, loss = 0.48940503\n",
      "Iteration 141, loss = 0.48790324\n",
      "Iteration 142, loss = 0.48641581\n",
      "Iteration 143, loss = 0.48494249\n",
      "Iteration 144, loss = 0.48348304\n",
      "Iteration 145, loss = 0.48203721\n",
      "Iteration 146, loss = 0.48060518\n",
      "Iteration 147, loss = 0.47918662\n",
      "Iteration 148, loss = 0.47778098\n",
      "Iteration 149, loss = 0.47638816\n",
      "Iteration 150, loss = 0.47500794\n",
      "Iteration 151, loss = 0.47363996\n",
      "Iteration 152, loss = 0.47228430\n",
      "Iteration 153, loss = 0.47094041\n",
      "Iteration 154, loss = 0.46960835\n",
      "Iteration 155, loss = 0.46828804\n",
      "Iteration 156, loss = 0.46697947\n",
      "Iteration 157, loss = 0.46568206\n",
      "Iteration 158, loss = 0.46439565\n",
      "Iteration 159, loss = 0.46312012\n",
      "Iteration 160, loss = 0.46185518\n",
      "Iteration 161, loss = 0.46060069\n",
      "Iteration 162, loss = 0.45935650\n",
      "Iteration 163, loss = 0.45812260\n",
      "Iteration 164, loss = 0.45689864\n",
      "Iteration 165, loss = 0.45568449\n",
      "Iteration 166, loss = 0.45447999\n",
      "Iteration 167, loss = 0.45328497\n",
      "Iteration 168, loss = 0.45209930\n",
      "Iteration 169, loss = 0.45092283\n",
      "Iteration 170, loss = 0.44975540\n",
      "Iteration 171, loss = 0.44859688\n",
      "Iteration 172, loss = 0.44744714\n",
      "Iteration 173, loss = 0.44630610\n",
      "Iteration 174, loss = 0.44517365\n",
      "Iteration 175, loss = 0.44404955\n",
      "Iteration 176, loss = 0.44293367\n",
      "Iteration 177, loss = 0.44182588\n",
      "Iteration 178, loss = 0.44072605\n",
      "Iteration 179, loss = 0.43963406\n",
      "Iteration 180, loss = 0.43854980\n",
      "Iteration 181, loss = 0.43747313\n",
      "Iteration 182, loss = 0.43640394\n",
      "Iteration 183, loss = 0.43534212\n",
      "Iteration 184, loss = 0.43428755\n",
      "Iteration 185, loss = 0.43324012\n",
      "Iteration 186, loss = 0.43219973\n",
      "Iteration 187, loss = 0.43116627\n",
      "Iteration 188, loss = 0.43013978\n",
      "Iteration 189, loss = 0.42912088\n",
      "Iteration 190, loss = 0.42810866\n",
      "Iteration 191, loss = 0.42710335\n",
      "Iteration 192, loss = 0.42610436\n",
      "Iteration 193, loss = 0.42511163\n",
      "Iteration 194, loss = 0.42412508\n",
      "Iteration 195, loss = 0.42314465\n",
      "Iteration 196, loss = 0.42217027\n",
      "Iteration 197, loss = 0.42120286\n",
      "Iteration 198, loss = 0.42024182\n",
      "Iteration 199, loss = 0.41928673\n",
      "Iteration 200, loss = 0.41833748\n",
      "Iteration 201, loss = 0.41739401\n",
      "Iteration 202, loss = 0.41645623\n",
      "Iteration 203, loss = 0.41552408\n",
      "Iteration 204, loss = 0.41459749\n",
      "Iteration 205, loss = 0.41367638\n",
      "Iteration 206, loss = 0.41276069\n",
      "Iteration 207, loss = 0.41185038\n",
      "Iteration 208, loss = 0.41094535\n",
      "Iteration 209, loss = 0.41004554\n",
      "Iteration 210, loss = 0.40915087\n",
      "Iteration 211, loss = 0.40826128\n",
      "Iteration 212, loss = 0.40737670\n",
      "Iteration 213, loss = 0.40649724\n",
      "Iteration 214, loss = 0.40562263\n",
      "Iteration 215, loss = 0.40475281\n",
      "Iteration 216, loss = 0.40388783\n",
      "Iteration 217, loss = 0.40302787\n",
      "Iteration 218, loss = 0.40217264\n",
      "Iteration 219, loss = 0.40132206\n",
      "Iteration 220, loss = 0.40047641\n",
      "Iteration 221, loss = 0.39963508\n",
      "Iteration 222, loss = 0.39879811\n",
      "Iteration 223, loss = 0.39796581\n",
      "Iteration 224, loss = 0.39713795\n",
      "Iteration 225, loss = 0.39631476\n",
      "Iteration 226, loss = 0.39549593\n",
      "Iteration 227, loss = 0.39468148\n",
      "Iteration 228, loss = 0.39387122\n",
      "Iteration 229, loss = 0.39306501\n",
      "Iteration 230, loss = 0.39226300\n",
      "Iteration 231, loss = 0.39146511\n",
      "Iteration 232, loss = 0.39067102\n",
      "Iteration 233, loss = 0.38988101\n",
      "Iteration 234, loss = 0.38909519\n",
      "Iteration 235, loss = 0.38831315\n",
      "Iteration 236, loss = 0.38753497\n",
      "Iteration 237, loss = 0.38676047\n",
      "Iteration 238, loss = 0.38598978\n",
      "Iteration 239, loss = 0.38522271\n",
      "Iteration 240, loss = 0.38445924\n",
      "Iteration 241, loss = 0.38369938\n",
      "Iteration 242, loss = 0.38294314\n",
      "Iteration 243, loss = 0.38219027\n",
      "Iteration 244, loss = 0.38144089\n",
      "Iteration 245, loss = 0.38069507\n",
      "Iteration 246, loss = 0.37995245\n",
      "Iteration 247, loss = 0.37921324\n",
      "Iteration 248, loss = 0.37847735\n",
      "Iteration 249, loss = 0.37774478\n",
      "Iteration 250, loss = 0.37701543\n",
      "Iteration 251, loss = 0.37628926\n",
      "Iteration 252, loss = 0.37556626\n",
      "Iteration 253, loss = 0.37484641\n",
      "Iteration 254, loss = 0.37412965\n",
      "Iteration 255, loss = 0.37341617\n",
      "Iteration 256, loss = 0.37270593\n",
      "Iteration 257, loss = 0.37199870\n",
      "Iteration 258, loss = 0.37129447\n",
      "Iteration 259, loss = 0.37059320\n",
      "Iteration 260, loss = 0.36989488\n",
      "Iteration 261, loss = 0.36919946\n",
      "Iteration 262, loss = 0.36850690\n",
      "Iteration 263, loss = 0.36781720\n",
      "Iteration 264, loss = 0.36713032\n",
      "Iteration 265, loss = 0.36644622\n",
      "Iteration 266, loss = 0.36576488\n",
      "Iteration 267, loss = 0.36508627\n",
      "Iteration 268, loss = 0.36441042\n",
      "Iteration 269, loss = 0.36373729\n",
      "Iteration 270, loss = 0.36306682\n",
      "Iteration 271, loss = 0.36239898\n",
      "Iteration 272, loss = 0.36173374\n",
      "Iteration 273, loss = 0.36107109\n",
      "Iteration 274, loss = 0.36041098\n",
      "Iteration 275, loss = 0.35975343\n",
      "Iteration 276, loss = 0.35909842\n",
      "Iteration 277, loss = 0.35844590\n",
      "Iteration 278, loss = 0.35779584\n",
      "Iteration 279, loss = 0.35714825\n",
      "Iteration 280, loss = 0.35650305\n",
      "Iteration 281, loss = 0.35586024\n",
      "Iteration 282, loss = 0.35521983\n",
      "Iteration 283, loss = 0.35458175\n",
      "Iteration 284, loss = 0.35394601\n",
      "Iteration 285, loss = 0.35331257\n",
      "Iteration 286, loss = 0.35268143\n",
      "Iteration 287, loss = 0.35205257\n",
      "Iteration 288, loss = 0.35142594\n",
      "Iteration 289, loss = 0.35080165\n",
      "Iteration 290, loss = 0.35018007\n",
      "Iteration 291, loss = 0.34956092\n",
      "Iteration 292, loss = 0.34894403\n",
      "Iteration 293, loss = 0.34832946\n",
      "Iteration 294, loss = 0.34771725\n",
      "Iteration 295, loss = 0.34710723\n",
      "Iteration 296, loss = 0.34649932\n",
      "Iteration 297, loss = 0.34589357\n",
      "Iteration 298, loss = 0.34528993\n",
      "Iteration 299, loss = 0.34468837\n",
      "Iteration 300, loss = 0.34408891\n",
      "Iteration 301, loss = 0.34349162\n",
      "Iteration 302, loss = 0.34289652\n",
      "Iteration 303, loss = 0.34230347\n",
      "Iteration 304, loss = 0.34171243\n",
      "Iteration 305, loss = 0.34112337\n",
      "Iteration 306, loss = 0.34053654\n",
      "Iteration 307, loss = 0.33995176\n",
      "Iteration 308, loss = 0.33936896\n",
      "Iteration 309, loss = 0.33878807\n",
      "Iteration 310, loss = 0.33820911\n",
      "Iteration 311, loss = 0.33763205\n",
      "Iteration 312, loss = 0.33705691\n",
      "Iteration 313, loss = 0.33648361\n",
      "Iteration 314, loss = 0.33591218\n",
      "Iteration 315, loss = 0.33534258\n",
      "Iteration 316, loss = 0.33477481\n",
      "Iteration 317, loss = 0.33420962\n",
      "Iteration 318, loss = 0.33364628\n",
      "Iteration 319, loss = 0.33308468\n",
      "Iteration 320, loss = 0.33252484\n",
      "Iteration 321, loss = 0.33196681\n",
      "Iteration 322, loss = 0.33141072\n",
      "Iteration 323, loss = 0.33085668\n",
      "Iteration 324, loss = 0.33030436\n",
      "Iteration 325, loss = 0.32975380\n",
      "Iteration 326, loss = 0.32920494\n",
      "Iteration 327, loss = 0.32865780\n",
      "Iteration 328, loss = 0.32811237\n",
      "Iteration 329, loss = 0.32756937\n",
      "Iteration 330, loss = 0.32702804\n",
      "Iteration 331, loss = 0.32648852\n",
      "Iteration 332, loss = 0.32595058\n",
      "Iteration 333, loss = 0.32541425\n",
      "Iteration 334, loss = 0.32487960\n",
      "Iteration 335, loss = 0.32434679\n",
      "Iteration 336, loss = 0.32381559\n",
      "Iteration 337, loss = 0.32328598\n",
      "Iteration 338, loss = 0.32275806\n",
      "Iteration 339, loss = 0.32223160\n",
      "Iteration 340, loss = 0.32170680\n",
      "Iteration 341, loss = 0.32118353\n",
      "Iteration 342, loss = 0.32066183\n",
      "Iteration 343, loss = 0.32014170\n",
      "Iteration 344, loss = 0.31962346\n",
      "Iteration 345, loss = 0.31910674\n",
      "Iteration 346, loss = 0.31859164\n",
      "Iteration 347, loss = 0.31807809\n",
      "Iteration 348, loss = 0.31756599\n",
      "Iteration 349, loss = 0.31705542\n",
      "Iteration 350, loss = 0.31654636\n",
      "Iteration 351, loss = 0.31603883\n",
      "Iteration 352, loss = 0.31553285\n",
      "Iteration 353, loss = 0.31502842\n",
      "Iteration 354, loss = 0.31452545\n",
      "Iteration 355, loss = 0.31402394\n",
      "Iteration 356, loss = 0.31352393\n",
      "Iteration 357, loss = 0.31302537\n",
      "Iteration 358, loss = 0.31252822\n",
      "Iteration 359, loss = 0.31203258\n",
      "Iteration 360, loss = 0.31153829\n",
      "Iteration 361, loss = 0.31104548\n",
      "Iteration 362, loss = 0.31055402\n",
      "Iteration 363, loss = 0.31006394\n",
      "Iteration 364, loss = 0.30957523\n",
      "Iteration 365, loss = 0.30908791\n",
      "Iteration 366, loss = 0.30860193\n",
      "Iteration 367, loss = 0.30811733\n",
      "Iteration 368, loss = 0.30763408\n",
      "Iteration 369, loss = 0.30715214\n",
      "Iteration 370, loss = 0.30667154\n",
      "Iteration 371, loss = 0.30619231\n",
      "Iteration 372, loss = 0.30571453\n",
      "Iteration 373, loss = 0.30523808\n",
      "Iteration 374, loss = 0.30476299\n",
      "Iteration 375, loss = 0.30428917\n",
      "Iteration 376, loss = 0.30381665\n",
      "Iteration 377, loss = 0.30334541\n",
      "Iteration 378, loss = 0.30287549\n",
      "Iteration 379, loss = 0.30240681\n",
      "Iteration 380, loss = 0.30193946\n",
      "Iteration 381, loss = 0.30147340\n",
      "Iteration 382, loss = 0.30100856\n",
      "Iteration 383, loss = 0.30054498\n",
      "Iteration 384, loss = 0.30008265\n",
      "Iteration 385, loss = 0.29962161\n",
      "Iteration 386, loss = 0.29916177\n",
      "Iteration 387, loss = 0.29870314\n",
      "Iteration 388, loss = 0.29824576\n",
      "Iteration 389, loss = 0.29778963\n",
      "Iteration 390, loss = 0.29733472\n",
      "Iteration 391, loss = 0.29688102\n",
      "Iteration 392, loss = 0.29642872\n",
      "Iteration 393, loss = 0.29597760\n",
      "Iteration 394, loss = 0.29552769\n",
      "Iteration 395, loss = 0.29507898\n",
      "Iteration 396, loss = 0.29463148\n",
      "Iteration 397, loss = 0.29418514\n",
      "Iteration 398, loss = 0.29373998\n",
      "Iteration 399, loss = 0.29329603\n",
      "Iteration 400, loss = 0.29285339\n",
      "Iteration 401, loss = 0.29241186\n",
      "Iteration 402, loss = 0.29197151\n",
      "Iteration 403, loss = 0.29153236\n",
      "Iteration 404, loss = 0.29109432\n",
      "Iteration 405, loss = 0.29065744\n",
      "Iteration 406, loss = 0.29022176\n",
      "Iteration 407, loss = 0.28978729\n",
      "Iteration 408, loss = 0.28935397\n",
      "Iteration 409, loss = 0.28892177\n",
      "Iteration 410, loss = 0.28849082\n",
      "Iteration 411, loss = 0.28806102\n",
      "Iteration 412, loss = 0.28763237\n",
      "Iteration 413, loss = 0.28720481\n",
      "Iteration 414, loss = 0.28677837\n",
      "Iteration 415, loss = 0.28635303\n",
      "Iteration 416, loss = 0.28592882\n",
      "Iteration 417, loss = 0.28550566\n",
      "Iteration 418, loss = 0.28508361\n",
      "Iteration 419, loss = 0.28466264\n",
      "Iteration 420, loss = 0.28424277\n",
      "Iteration 421, loss = 0.28382395\n",
      "Iteration 422, loss = 0.28340621\n",
      "Iteration 423, loss = 0.28298960\n",
      "Iteration 424, loss = 0.28257409\n",
      "Iteration 425, loss = 0.28215968\n",
      "Iteration 426, loss = 0.28174628\n",
      "Iteration 427, loss = 0.28133396\n",
      "Iteration 428, loss = 0.28092268\n",
      "Iteration 429, loss = 0.28051245\n",
      "Iteration 430, loss = 0.28010326\n",
      "Iteration 431, loss = 0.27969510\n",
      "Iteration 432, loss = 0.27928796\n",
      "Iteration 433, loss = 0.27888185\n",
      "Iteration 434, loss = 0.27847681\n",
      "Iteration 435, loss = 0.27807277\n",
      "Iteration 436, loss = 0.27766976\n",
      "Iteration 437, loss = 0.27726777\n",
      "Iteration 438, loss = 0.27686680\n",
      "Iteration 439, loss = 0.27646696\n",
      "Iteration 440, loss = 0.27606800\n",
      "Iteration 441, loss = 0.27566997\n",
      "Iteration 442, loss = 0.27527303\n",
      "Iteration 443, loss = 0.27487708\n",
      "Iteration 444, loss = 0.27448212\n",
      "Iteration 445, loss = 0.27408813\n",
      "Iteration 446, loss = 0.27369512\n",
      "Iteration 447, loss = 0.27330313\n",
      "Iteration 448, loss = 0.27291218\n",
      "Iteration 449, loss = 0.27252231\n",
      "Iteration 450, loss = 0.27213336\n",
      "Iteration 451, loss = 0.27174541\n",
      "Iteration 452, loss = 0.27135851\n",
      "Iteration 453, loss = 0.27097247\n",
      "Iteration 454, loss = 0.27058742\n",
      "Iteration 455, loss = 0.27020333\n",
      "Iteration 456, loss = 0.26982024\n",
      "Iteration 457, loss = 0.26943805\n",
      "Iteration 458, loss = 0.26905683\n",
      "Iteration 459, loss = 0.26867657\n",
      "Iteration 460, loss = 0.26829725\n",
      "Iteration 461, loss = 0.26791890\n",
      "Iteration 462, loss = 0.26754144\n",
      "Iteration 463, loss = 0.26716492\n",
      "Iteration 464, loss = 0.26678936\n",
      "Iteration 465, loss = 0.26641469\n",
      "Iteration 466, loss = 0.26604095\n",
      "Iteration 467, loss = 0.26566815\n",
      "Iteration 468, loss = 0.26529625\n",
      "Iteration 469, loss = 0.26492525\n",
      "Iteration 470, loss = 0.26455515\n",
      "Iteration 471, loss = 0.26418604\n",
      "Iteration 472, loss = 0.26381774\n",
      "Iteration 473, loss = 0.26345037\n",
      "Iteration 474, loss = 0.26308395\n",
      "Iteration 475, loss = 0.26271837\n",
      "Iteration 476, loss = 0.26235370\n",
      "Iteration 477, loss = 0.26198996\n",
      "Iteration 478, loss = 0.26162710\n",
      "Iteration 479, loss = 0.26126511\n",
      "Iteration 480, loss = 0.26090402\n",
      "Iteration 481, loss = 0.26054384\n",
      "Iteration 482, loss = 0.26018445\n",
      "Iteration 483, loss = 0.25982602\n",
      "Iteration 484, loss = 0.25946847\n",
      "Iteration 485, loss = 0.25911175\n",
      "Iteration 486, loss = 0.25875591\n",
      "Iteration 487, loss = 0.25840096\n",
      "Iteration 488, loss = 0.25804685\n",
      "Iteration 489, loss = 0.25769359\n",
      "Iteration 490, loss = 0.25734123\n",
      "Iteration 491, loss = 0.25698966\n",
      "Iteration 492, loss = 0.25663899\n",
      "Iteration 493, loss = 0.25628914\n",
      "Iteration 494, loss = 0.25594017\n",
      "Iteration 495, loss = 0.25559200\n",
      "Iteration 496, loss = 0.25524470\n",
      "Iteration 497, loss = 0.25489824\n",
      "Iteration 498, loss = 0.25455262\n",
      "Iteration 499, loss = 0.25420781\n",
      "Iteration 500, loss = 0.25386383\n",
      "Iteration 501, loss = 0.25352072\n",
      "Iteration 502, loss = 0.25317842\n",
      "Iteration 503, loss = 0.25283691\n",
      "Iteration 504, loss = 0.25249628\n",
      "Iteration 505, loss = 0.25215640\n",
      "Iteration 506, loss = 0.25181741\n",
      "Iteration 507, loss = 0.25147920\n",
      "Iteration 508, loss = 0.25114179\n",
      "Iteration 509, loss = 0.25080521\n",
      "Iteration 510, loss = 0.25046941\n",
      "Iteration 511, loss = 0.25013447\n",
      "Iteration 512, loss = 0.24980029\n",
      "Iteration 513, loss = 0.24946691\n",
      "Iteration 514, loss = 0.24913437\n",
      "Iteration 515, loss = 0.24880258\n",
      "Iteration 516, loss = 0.24847163\n",
      "Iteration 517, loss = 0.24814144\n",
      "Iteration 518, loss = 0.24781203\n",
      "Iteration 519, loss = 0.24748346\n",
      "Iteration 520, loss = 0.24715564\n",
      "Iteration 521, loss = 0.24682866\n",
      "Iteration 522, loss = 0.24650242\n",
      "Iteration 523, loss = 0.24617696\n",
      "Iteration 524, loss = 0.24585231\n",
      "Iteration 525, loss = 0.24552842\n",
      "Iteration 526, loss = 0.24520532\n",
      "Iteration 527, loss = 0.24488299\n",
      "Iteration 528, loss = 0.24456144\n",
      "Iteration 529, loss = 0.24424065\n",
      "Iteration 530, loss = 0.24392063\n",
      "Iteration 531, loss = 0.24360138\n",
      "Iteration 532, loss = 0.24328286\n",
      "Iteration 533, loss = 0.24296516\n",
      "Iteration 534, loss = 0.24264816\n",
      "Iteration 535, loss = 0.24233193\n",
      "Iteration 536, loss = 0.24201647\n",
      "Iteration 537, loss = 0.24170175\n",
      "Iteration 538, loss = 0.24138780\n",
      "Iteration 539, loss = 0.24107455\n",
      "Iteration 540, loss = 0.24076210\n",
      "Iteration 541, loss = 0.24045035\n",
      "Iteration 542, loss = 0.24013937\n",
      "Iteration 543, loss = 0.23982911\n",
      "Iteration 544, loss = 0.23951959\n",
      "Iteration 545, loss = 0.23921082\n",
      "Iteration 546, loss = 0.23890277\n",
      "Iteration 547, loss = 0.23859545\n",
      "Iteration 548, loss = 0.23828886\n",
      "Iteration 549, loss = 0.23798301\n",
      "Iteration 550, loss = 0.23767786\n",
      "Iteration 551, loss = 0.23737346\n",
      "Iteration 552, loss = 0.23706977\n",
      "Iteration 553, loss = 0.23676679\n",
      "Iteration 554, loss = 0.23646456\n",
      "Iteration 555, loss = 0.23616300\n",
      "Iteration 556, loss = 0.23586220\n",
      "Iteration 557, loss = 0.23556206\n",
      "Iteration 558, loss = 0.23526277\n",
      "Iteration 559, loss = 0.23496402\n",
      "Iteration 560, loss = 0.23466605\n",
      "Iteration 561, loss = 0.23436887\n",
      "Iteration 562, loss = 0.23407238\n",
      "Iteration 563, loss = 0.23377659\n",
      "Iteration 564, loss = 0.23348152\n",
      "Iteration 565, loss = 0.23318712\n",
      "Iteration 566, loss = 0.23289346\n",
      "Iteration 567, loss = 0.23260045\n",
      "Iteration 568, loss = 0.23230819\n",
      "Iteration 569, loss = 0.23201656\n",
      "Iteration 570, loss = 0.23172568\n",
      "Iteration 571, loss = 0.23143545\n",
      "Iteration 572, loss = 0.23114590\n",
      "Iteration 573, loss = 0.23085705\n",
      "Iteration 574, loss = 0.23056887\n",
      "Iteration 575, loss = 0.23028141\n",
      "Iteration 576, loss = 0.22999460\n",
      "Iteration 577, loss = 0.22970850\n",
      "Iteration 578, loss = 0.22942305\n",
      "Iteration 579, loss = 0.22913834\n",
      "Iteration 580, loss = 0.22885432\n",
      "Iteration 581, loss = 0.22857097\n",
      "Iteration 582, loss = 0.22828833\n",
      "Iteration 583, loss = 0.22800633\n",
      "Iteration 584, loss = 0.22772502\n",
      "Iteration 585, loss = 0.22744434\n",
      "Iteration 586, loss = 0.22716438\n",
      "Iteration 587, loss = 0.22688503\n",
      "Iteration 588, loss = 0.22660637\n",
      "Iteration 589, loss = 0.22632834\n",
      "Iteration 590, loss = 0.22605102\n",
      "Iteration 591, loss = 0.22577435\n",
      "Iteration 592, loss = 0.22549839\n",
      "Iteration 593, loss = 0.22522311\n",
      "Iteration 594, loss = 0.22494848\n",
      "Iteration 595, loss = 0.22467450\n",
      "Iteration 596, loss = 0.22440116\n",
      "Iteration 597, loss = 0.22412848\n",
      "Iteration 598, loss = 0.22385642\n",
      "Iteration 599, loss = 0.22358503\n",
      "Iteration 600, loss = 0.22331424\n",
      "Iteration 601, loss = 0.22304412\n",
      "Iteration 602, loss = 0.22277462\n",
      "Iteration 603, loss = 0.22250576\n",
      "Iteration 604, loss = 0.22223751\n",
      "Iteration 605, loss = 0.22196991\n",
      "Iteration 606, loss = 0.22170292\n",
      "Iteration 607, loss = 0.22143657\n",
      "Iteration 608, loss = 0.22117083\n",
      "Iteration 609, loss = 0.22090573\n",
      "Iteration 610, loss = 0.22064124\n",
      "Iteration 611, loss = 0.22037735\n",
      "Iteration 612, loss = 0.22011411\n",
      "Iteration 613, loss = 0.21985150\n",
      "Iteration 614, loss = 0.21958959\n",
      "Iteration 615, loss = 0.21932826\n",
      "Iteration 616, loss = 0.21906757\n",
      "Iteration 617, loss = 0.21880749\n",
      "Iteration 618, loss = 0.21854800\n",
      "Iteration 619, loss = 0.21828912\n",
      "Iteration 620, loss = 0.21803086\n",
      "Iteration 621, loss = 0.21777320\n",
      "Iteration 622, loss = 0.21751613\n",
      "Iteration 623, loss = 0.21725967\n",
      "Iteration 624, loss = 0.21700379\n",
      "Iteration 625, loss = 0.21674854\n",
      "Iteration 626, loss = 0.21649384\n",
      "Iteration 627, loss = 0.21623976\n",
      "Iteration 628, loss = 0.21598626\n",
      "Iteration 629, loss = 0.21573336\n",
      "Iteration 630, loss = 0.21548103\n",
      "Iteration 631, loss = 0.21522930\n",
      "Iteration 632, loss = 0.21497814\n",
      "Iteration 633, loss = 0.21472758\n",
      "Iteration 634, loss = 0.21447760\n",
      "Iteration 635, loss = 0.21422819\n",
      "Iteration 636, loss = 0.21397937\n",
      "Iteration 637, loss = 0.21373110\n",
      "Iteration 638, loss = 0.21348344\n",
      "Iteration 639, loss = 0.21323633\n",
      "Iteration 640, loss = 0.21298980\n",
      "Iteration 641, loss = 0.21274382\n",
      "Iteration 642, loss = 0.21249845\n",
      "Iteration 643, loss = 0.21225361\n",
      "Iteration 644, loss = 0.21200934\n",
      "Iteration 645, loss = 0.21176565\n",
      "Iteration 646, loss = 0.21152251\n",
      "Iteration 647, loss = 0.21127994\n",
      "Iteration 648, loss = 0.21103791\n",
      "Iteration 649, loss = 0.21079647\n",
      "Iteration 650, loss = 0.21055555\n",
      "Iteration 651, loss = 0.21031523\n",
      "Iteration 652, loss = 0.21007543\n",
      "Iteration 653, loss = 0.20983618\n",
      "Iteration 654, loss = 0.20959748\n",
      "Iteration 655, loss = 0.20935936\n",
      "Iteration 656, loss = 0.20912175\n",
      "Iteration 657, loss = 0.20888473\n",
      "Iteration 658, loss = 0.20864822\n",
      "Iteration 659, loss = 0.20841225\n",
      "Iteration 660, loss = 0.20817684\n",
      "Iteration 661, loss = 0.20794196\n",
      "Iteration 662, loss = 0.20770763\n",
      "Iteration 663, loss = 0.20747383\n",
      "Iteration 664, loss = 0.20724058\n",
      "Iteration 665, loss = 0.20700785\n",
      "Iteration 666, loss = 0.20677567\n",
      "Iteration 667, loss = 0.20654400\n",
      "Iteration 668, loss = 0.20631288\n",
      "Iteration 669, loss = 0.20608227\n",
      "Iteration 670, loss = 0.20585221\n",
      "Iteration 671, loss = 0.20562265\n",
      "Iteration 672, loss = 0.20539366\n",
      "Iteration 673, loss = 0.20516514\n",
      "Iteration 674, loss = 0.20493719\n",
      "Iteration 675, loss = 0.20470973\n",
      "Iteration 676, loss = 0.20448279\n",
      "Iteration 677, loss = 0.20425640\n",
      "Iteration 678, loss = 0.20403049\n",
      "Iteration 679, loss = 0.20380511\n",
      "Iteration 680, loss = 0.20358027\n",
      "Iteration 681, loss = 0.20335595\n",
      "Iteration 682, loss = 0.20313212\n",
      "Iteration 683, loss = 0.20290881\n",
      "Iteration 684, loss = 0.20268600\n",
      "Iteration 685, loss = 0.20246372\n",
      "Iteration 686, loss = 0.20224192\n",
      "Iteration 687, loss = 0.20202064\n",
      "Iteration 688, loss = 0.20179986\n",
      "Iteration 689, loss = 0.20157959\n",
      "Iteration 690, loss = 0.20135981\n",
      "Iteration 691, loss = 0.20114053\n",
      "Iteration 692, loss = 0.20092176\n",
      "Iteration 693, loss = 0.20070348\n",
      "Iteration 694, loss = 0.20048570\n",
      "Iteration 695, loss = 0.20026840\n",
      "Iteration 696, loss = 0.20005163\n",
      "Iteration 697, loss = 0.19983531\n",
      "Iteration 698, loss = 0.19961950\n",
      "Iteration 699, loss = 0.19940417\n",
      "Iteration 700, loss = 0.19918935\n",
      "Iteration 701, loss = 0.19897499\n",
      "Iteration 702, loss = 0.19876113\n",
      "Iteration 703, loss = 0.19854774\n",
      "Iteration 704, loss = 0.19833487\n",
      "Iteration 705, loss = 0.19812243\n",
      "Iteration 706, loss = 0.19791051\n",
      "Iteration 707, loss = 0.19769905\n",
      "Iteration 708, loss = 0.19748807\n",
      "Iteration 709, loss = 0.19727759\n",
      "Iteration 710, loss = 0.19706755\n",
      "Iteration 711, loss = 0.19685799\n",
      "Iteration 712, loss = 0.19664892\n",
      "Iteration 713, loss = 0.19644031\n",
      "Iteration 714, loss = 0.19623217\n",
      "Iteration 715, loss = 0.19602451\n",
      "Iteration 716, loss = 0.19581731\n",
      "Iteration 717, loss = 0.19561059\n",
      "Iteration 718, loss = 0.19540431\n",
      "Iteration 719, loss = 0.19519852\n",
      "Iteration 720, loss = 0.19499317\n",
      "Iteration 721, loss = 0.19478831\n",
      "Iteration 722, loss = 0.19458388\n",
      "Iteration 723, loss = 0.19437993\n",
      "Iteration 724, loss = 0.19417642\n",
      "Iteration 725, loss = 0.19397340\n",
      "Iteration 726, loss = 0.19377080\n",
      "Iteration 727, loss = 0.19356868\n",
      "Iteration 728, loss = 0.19336700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 729, loss = 0.19316577\n",
      "Iteration 730, loss = 0.19296501\n",
      "Iteration 731, loss = 0.19276468\n",
      "Iteration 732, loss = 0.19256481\n",
      "Iteration 733, loss = 0.19236538\n",
      "Iteration 734, loss = 0.19216641\n",
      "Iteration 735, loss = 0.19196787\n",
      "Iteration 736, loss = 0.19176979\n",
      "Iteration 737, loss = 0.19157213\n",
      "Iteration 738, loss = 0.19137494\n",
      "Iteration 739, loss = 0.19117819\n",
      "Iteration 740, loss = 0.19098189\n",
      "Iteration 741, loss = 0.19078601\n",
      "Iteration 742, loss = 0.19059059\n",
      "Iteration 743, loss = 0.19039558\n",
      "Iteration 744, loss = 0.19020103\n",
      "Iteration 745, loss = 0.19000689\n",
      "Iteration 746, loss = 0.18981320\n",
      "Iteration 747, loss = 0.18961995\n",
      "Iteration 748, loss = 0.18942711\n",
      "Iteration 749, loss = 0.18923472\n",
      "Iteration 750, loss = 0.18904274\n",
      "Iteration 751, loss = 0.18885119\n",
      "Iteration 752, loss = 0.18866007\n",
      "Iteration 753, loss = 0.18846938\n",
      "Iteration 754, loss = 0.18827910\n",
      "Iteration 755, loss = 0.18808928\n",
      "Iteration 756, loss = 0.18789985\n",
      "Iteration 757, loss = 0.18771084\n",
      "Iteration 758, loss = 0.18752227\n",
      "Iteration 759, loss = 0.18733411\n",
      "Iteration 760, loss = 0.18714638\n",
      "Iteration 761, loss = 0.18695904\n",
      "Iteration 762, loss = 0.18677212\n",
      "Iteration 763, loss = 0.18658562\n",
      "Iteration 764, loss = 0.18639955\n",
      "Iteration 765, loss = 0.18621387\n",
      "Iteration 766, loss = 0.18602861\n",
      "Iteration 767, loss = 0.18584375\n",
      "Iteration 768, loss = 0.18565932\n",
      "Iteration 769, loss = 0.18547527\n",
      "Iteration 770, loss = 0.18529165\n",
      "Iteration 771, loss = 0.18510841\n",
      "Iteration 772, loss = 0.18492559\n",
      "Iteration 773, loss = 0.18474318\n",
      "Iteration 774, loss = 0.18456117\n",
      "Iteration 775, loss = 0.18437957\n",
      "Iteration 776, loss = 0.18419835\n",
      "Iteration 777, loss = 0.18401755\n",
      "Iteration 778, loss = 0.18383714\n",
      "Iteration 779, loss = 0.18365715\n",
      "Iteration 780, loss = 0.18347754\n",
      "Iteration 781, loss = 0.18329835\n",
      "Iteration 782, loss = 0.18311953\n",
      "Iteration 783, loss = 0.18294113\n",
      "Iteration 784, loss = 0.18276310\n",
      "Iteration 785, loss = 0.18258547\n",
      "Iteration 786, loss = 0.18240824\n",
      "Iteration 787, loss = 0.18223138\n",
      "Iteration 788, loss = 0.18205492\n",
      "Iteration 789, loss = 0.18187883\n",
      "Iteration 790, loss = 0.18170315\n",
      "Iteration 791, loss = 0.18152784\n",
      "Iteration 792, loss = 0.18135292\n",
      "Iteration 793, loss = 0.18117838\n",
      "Iteration 794, loss = 0.18100422\n",
      "Iteration 795, loss = 0.18083046\n",
      "Iteration 796, loss = 0.18065705\n",
      "Iteration 797, loss = 0.18048403\n",
      "Iteration 798, loss = 0.18031139\n",
      "Iteration 799, loss = 0.18013913\n",
      "Iteration 800, loss = 0.17996724\n",
      "Iteration 801, loss = 0.17979573\n",
      "Iteration 802, loss = 0.17962460\n",
      "Iteration 803, loss = 0.17945383\n",
      "Iteration 804, loss = 0.17928344\n",
      "Iteration 805, loss = 0.17911341\n",
      "Iteration 806, loss = 0.17894377\n",
      "Iteration 807, loss = 0.17877448\n",
      "Iteration 808, loss = 0.17860556\n",
      "Iteration 809, loss = 0.17843703\n",
      "Iteration 810, loss = 0.17826884\n",
      "Iteration 811, loss = 0.17810102\n",
      "Iteration 812, loss = 0.17793356\n",
      "Iteration 813, loss = 0.17776648\n",
      "Iteration 814, loss = 0.17759974\n",
      "Iteration 815, loss = 0.17743338\n",
      "Iteration 816, loss = 0.17726737\n",
      "Iteration 817, loss = 0.17710172\n",
      "Iteration 818, loss = 0.17693644\n",
      "Iteration 819, loss = 0.17677150\n",
      "Iteration 820, loss = 0.17660693\n",
      "Iteration 821, loss = 0.17644271\n",
      "Iteration 822, loss = 0.17627885\n",
      "Iteration 823, loss = 0.17611534\n",
      "Iteration 824, loss = 0.17595218\n",
      "Iteration 825, loss = 0.17578939\n",
      "Iteration 826, loss = 0.17562692\n",
      "Iteration 827, loss = 0.17546483\n",
      "Iteration 828, loss = 0.17530307\n",
      "Iteration 829, loss = 0.17514166\n",
      "Iteration 830, loss = 0.17498061\n",
      "Iteration 831, loss = 0.17481989\n",
      "Iteration 832, loss = 0.17465953\n",
      "Iteration 833, loss = 0.17449951\n",
      "Iteration 834, loss = 0.17433983\n",
      "Iteration 835, loss = 0.17418050\n",
      "Iteration 836, loss = 0.17402150\n",
      "Iteration 837, loss = 0.17386286\n",
      "Iteration 838, loss = 0.17370454\n",
      "Iteration 839, loss = 0.17354658\n",
      "Iteration 840, loss = 0.17338895\n",
      "Iteration 841, loss = 0.17323165\n",
      "Iteration 842, loss = 0.17307470\n",
      "Iteration 843, loss = 0.17291807\n",
      "Iteration 844, loss = 0.17276179\n",
      "Iteration 845, loss = 0.17260583\n",
      "Iteration 846, loss = 0.17245021\n",
      "Iteration 847, loss = 0.17229493\n",
      "Iteration 848, loss = 0.17213997\n",
      "Iteration 849, loss = 0.17198536\n",
      "Iteration 850, loss = 0.17183105\n",
      "Iteration 851, loss = 0.17167710\n",
      "Iteration 852, loss = 0.17152346\n",
      "Iteration 853, loss = 0.17137016\n",
      "Iteration 854, loss = 0.17121719\n",
      "Iteration 855, loss = 0.17106455\n",
      "Iteration 856, loss = 0.17091222\n",
      "Iteration 857, loss = 0.17076023\n",
      "Iteration 858, loss = 0.17060855\n",
      "Iteration 859, loss = 0.17045721\n",
      "Iteration 860, loss = 0.17030617\n",
      "Iteration 861, loss = 0.17015547\n",
      "Iteration 862, loss = 0.17000508\n",
      "Iteration 863, loss = 0.16985501\n",
      "Iteration 864, loss = 0.16970526\n",
      "Iteration 865, loss = 0.16955582\n",
      "Iteration 866, loss = 0.16940671\n",
      "Iteration 867, loss = 0.16925790\n",
      "Iteration 868, loss = 0.16910942\n",
      "Iteration 869, loss = 0.16896125\n",
      "Iteration 870, loss = 0.16881339\n",
      "Iteration 871, loss = 0.16866584\n",
      "Iteration 872, loss = 0.16851861\n",
      "Iteration 873, loss = 0.16837168\n",
      "Iteration 874, loss = 0.16822507\n",
      "Iteration 875, loss = 0.16807875\n",
      "Iteration 876, loss = 0.16793276\n",
      "Iteration 877, loss = 0.16778708\n",
      "Iteration 878, loss = 0.16764169\n",
      "Iteration 879, loss = 0.16749663\n",
      "Iteration 880, loss = 0.16735185\n",
      "Iteration 881, loss = 0.16720740\n",
      "Iteration 882, loss = 0.16706324\n",
      "Iteration 883, loss = 0.16691938\n",
      "Iteration 884, loss = 0.16677583\n",
      "Iteration 885, loss = 0.16663258\n",
      "Iteration 886, loss = 0.16648962\n",
      "Iteration 887, loss = 0.16634698\n",
      "Iteration 888, loss = 0.16620462\n",
      "Iteration 889, loss = 0.16606256\n",
      "Iteration 890, loss = 0.16592081\n",
      "Iteration 891, loss = 0.16577935\n",
      "Iteration 892, loss = 0.16563819\n",
      "Iteration 893, loss = 0.16549732\n",
      "Iteration 894, loss = 0.16535674\n",
      "Iteration 895, loss = 0.16521646\n",
      "Iteration 896, loss = 0.16507647\n",
      "Iteration 897, loss = 0.16493678\n",
      "Iteration 898, loss = 0.16479738\n",
      "Iteration 899, loss = 0.16465826\n",
      "Iteration 900, loss = 0.16451944\n",
      "Iteration 901, loss = 0.16438090\n",
      "Iteration 902, loss = 0.16424266\n",
      "Iteration 903, loss = 0.16410470\n",
      "Iteration 904, loss = 0.16396704\n",
      "Iteration 905, loss = 0.16382965\n",
      "Iteration 906, loss = 0.16369256\n",
      "Iteration 907, loss = 0.16355575\n",
      "Iteration 908, loss = 0.16341923\n",
      "Iteration 909, loss = 0.16328299\n",
      "Iteration 910, loss = 0.16314703\n",
      "Iteration 911, loss = 0.16301135\n",
      "Iteration 912, loss = 0.16287596\n",
      "Iteration 913, loss = 0.16274085\n",
      "Iteration 914, loss = 0.16260602\n",
      "Iteration 915, loss = 0.16247146\n",
      "Iteration 916, loss = 0.16233718\n",
      "Iteration 917, loss = 0.16220319\n",
      "Iteration 918, loss = 0.16206946\n",
      "Iteration 919, loss = 0.16193606\n",
      "Iteration 920, loss = 0.16180288\n",
      "Iteration 921, loss = 0.16166999\n",
      "Iteration 922, loss = 0.16153737\n",
      "Iteration 923, loss = 0.16140504\n",
      "Iteration 924, loss = 0.16127296\n",
      "Iteration 925, loss = 0.16114117\n",
      "Iteration 926, loss = 0.16100965\n",
      "Iteration 927, loss = 0.16087840\n",
      "Iteration 928, loss = 0.16074741\n",
      "Iteration 929, loss = 0.16061671\n",
      "Iteration 930, loss = 0.16048625\n",
      "Iteration 931, loss = 0.16035608\n",
      "Iteration 932, loss = 0.16022617\n",
      "Iteration 933, loss = 0.16009653\n",
      "Iteration 934, loss = 0.15996715\n",
      "Iteration 935, loss = 0.15983804\n",
      "Iteration 936, loss = 0.15970919\n",
      "Iteration 937, loss = 0.15958060\n",
      "Iteration 938, loss = 0.15945228\n",
      "Iteration 939, loss = 0.15932421\n",
      "Iteration 940, loss = 0.15919642\n",
      "Iteration 941, loss = 0.15906889\n",
      "Iteration 942, loss = 0.15894160\n",
      "Iteration 943, loss = 0.15881458\n",
      "Iteration 944, loss = 0.15868782\n",
      "Iteration 945, loss = 0.15856132\n",
      "Iteration 946, loss = 0.15843508\n",
      "Iteration 947, loss = 0.15830909\n",
      "Iteration 948, loss = 0.15818337\n",
      "Iteration 949, loss = 0.15805788\n",
      "Iteration 950, loss = 0.15793266\n",
      "Iteration 951, loss = 0.15780770\n",
      "Iteration 952, loss = 0.15768299\n",
      "Iteration 953, loss = 0.15755853\n",
      "Iteration 954, loss = 0.15743431\n",
      "Iteration 955, loss = 0.15731037\n",
      "Iteration 956, loss = 0.15718665\n",
      "Iteration 957, loss = 0.15706319\n",
      "Iteration 958, loss = 0.15694000\n",
      "Iteration 959, loss = 0.15681704\n",
      "Iteration 960, loss = 0.15669433\n",
      "Iteration 961, loss = 0.15657187\n",
      "Iteration 962, loss = 0.15644967\n",
      "Iteration 963, loss = 0.15632768\n",
      "Iteration 964, loss = 0.15620596\n",
      "Iteration 965, loss = 0.15608451\n",
      "Iteration 966, loss = 0.15596327\n",
      "Iteration 967, loss = 0.15584232\n",
      "Iteration 968, loss = 0.15572160\n",
      "Iteration 969, loss = 0.15560112\n",
      "Iteration 970, loss = 0.15548090\n",
      "Iteration 971, loss = 0.15536090\n",
      "Iteration 972, loss = 0.15524115\n",
      "Iteration 973, loss = 0.15512164\n",
      "Iteration 974, loss = 0.15500238\n",
      "Iteration 975, loss = 0.15488335\n",
      "Iteration 976, loss = 0.15476455\n",
      "Iteration 977, loss = 0.15464602\n",
      "Iteration 978, loss = 0.15452770\n",
      "Iteration 979, loss = 0.15440963\n",
      "Iteration 980, loss = 0.15429178\n",
      "Iteration 981, loss = 0.15417419\n",
      "Iteration 982, loss = 0.15405681\n",
      "Iteration 983, loss = 0.15393968\n",
      "Iteration 984, loss = 0.15382279\n",
      "Iteration 985, loss = 0.15370612\n",
      "Iteration 986, loss = 0.15358969\n",
      "Iteration 987, loss = 0.15347348\n",
      "Iteration 988, loss = 0.15335751\n",
      "Iteration 989, loss = 0.15324178\n",
      "Iteration 990, loss = 0.15312626\n",
      "Iteration 991, loss = 0.15301099\n",
      "Iteration 992, loss = 0.15289594\n",
      "Iteration 993, loss = 0.15278112\n",
      "Iteration 994, loss = 0.15266652\n",
      "Iteration 995, loss = 0.15255215\n",
      "Iteration 996, loss = 0.15243801\n",
      "Iteration 997, loss = 0.15232410\n",
      "Iteration 998, loss = 0.15221040\n",
      "Iteration 999, loss = 0.15209695\n",
      "Iteration 1000, loss = 0.15198371\n",
      "Iteration 1, loss = 1.92980133\n",
      "Iteration 2, loss = 1.88518426\n",
      "Iteration 3, loss = 1.82506927\n",
      "Iteration 4, loss = 1.75414080\n",
      "Iteration 5, loss = 1.67658784\n",
      "Iteration 6, loss = 1.59584692\n",
      "Iteration 7, loss = 1.51468181\n",
      "Iteration 8, loss = 1.43528533\n",
      "Iteration 9, loss = 1.35978758\n",
      "Iteration 10, loss = 1.29026894\n",
      "Iteration 11, loss = 1.22876565\n",
      "Iteration 12, loss = 1.17721979\n",
      "Iteration 13, loss = 1.13688210\n",
      "Iteration 14, loss = 1.10798197\n",
      "Iteration 15, loss = 1.08931936\n",
      "Iteration 16, loss = 1.07876043\n",
      "Iteration 17, loss = 1.07318294\n",
      "Iteration 18, loss = 1.06973996\n",
      "Iteration 19, loss = 1.06597052\n",
      "Iteration 20, loss = 1.06032669\n",
      "Iteration 21, loss = 1.05198506\n",
      "Iteration 22, loss = 1.04096885\n",
      "Iteration 23, loss = 1.02791836\n",
      "Iteration 24, loss = 1.01365941\n",
      "Iteration 25, loss = 0.99901043\n",
      "Iteration 26, loss = 0.98463310\n",
      "Iteration 27, loss = 0.97097987\n",
      "Iteration 28, loss = 0.95819480\n",
      "Iteration 29, loss = 0.94621547\n",
      "Iteration 30, loss = 0.93479107\n",
      "Iteration 31, loss = 0.92367936\n",
      "Iteration 32, loss = 0.91264274\n",
      "Iteration 33, loss = 0.90138626\n",
      "Iteration 34, loss = 0.88991004\n",
      "Iteration 35, loss = 0.87825485\n",
      "Iteration 36, loss = 0.86640113\n",
      "Iteration 37, loss = 0.85462406\n",
      "Iteration 38, loss = 0.84322924\n",
      "Iteration 39, loss = 0.83245640\n",
      "Iteration 40, loss = 0.82236096\n",
      "Iteration 41, loss = 0.81305514\n",
      "Iteration 42, loss = 0.80458821\n",
      "Iteration 43, loss = 0.79688681\n",
      "Iteration 44, loss = 0.78983377\n",
      "Iteration 45, loss = 0.78306477\n",
      "Iteration 46, loss = 0.77648842\n",
      "Iteration 47, loss = 0.77000034\n",
      "Iteration 48, loss = 0.76360724\n",
      "Iteration 49, loss = 0.75735149\n",
      "Iteration 50, loss = 0.75120680\n",
      "Iteration 51, loss = 0.74519189\n",
      "Iteration 52, loss = 0.73929031\n",
      "Iteration 53, loss = 0.73355469\n",
      "Iteration 54, loss = 0.72803309\n",
      "Iteration 55, loss = 0.72279694\n",
      "Iteration 56, loss = 0.71784722\n",
      "Iteration 57, loss = 0.71308605\n",
      "Iteration 58, loss = 0.70846113\n",
      "Iteration 59, loss = 0.70391450\n",
      "Iteration 60, loss = 0.69943041\n",
      "Iteration 61, loss = 0.69502819\n",
      "Iteration 62, loss = 0.69069976\n",
      "Iteration 63, loss = 0.68644417\n",
      "Iteration 64, loss = 0.68226744\n",
      "Iteration 65, loss = 0.67818157\n",
      "Iteration 66, loss = 0.67417368\n",
      "Iteration 67, loss = 0.67027437\n",
      "Iteration 68, loss = 0.66647544\n",
      "Iteration 69, loss = 0.66274658\n",
      "Iteration 70, loss = 0.65907555\n",
      "Iteration 71, loss = 0.65547952\n",
      "Iteration 72, loss = 0.65194234\n",
      "Iteration 73, loss = 0.64845404\n",
      "Iteration 74, loss = 0.64500881\n",
      "Iteration 75, loss = 0.64160970\n",
      "Iteration 76, loss = 0.63825666\n",
      "Iteration 77, loss = 0.63494790\n",
      "Iteration 78, loss = 0.63168214\n",
      "Iteration 79, loss = 0.62846159\n",
      "Iteration 80, loss = 0.62528923\n",
      "Iteration 81, loss = 0.62216696\n",
      "Iteration 82, loss = 0.61909229\n",
      "Iteration 83, loss = 0.61606725\n",
      "Iteration 84, loss = 0.61309091\n",
      "Iteration 85, loss = 0.61015281\n",
      "Iteration 86, loss = 0.60725234\n",
      "Iteration 87, loss = 0.60438852\n",
      "Iteration 88, loss = 0.60156043\n",
      "Iteration 89, loss = 0.59876720\n",
      "Iteration 90, loss = 0.59600777\n",
      "Iteration 91, loss = 0.59328306\n",
      "Iteration 92, loss = 0.59059557\n",
      "Iteration 93, loss = 0.58794890\n",
      "Iteration 94, loss = 0.58533653\n",
      "Iteration 95, loss = 0.58275686\n",
      "Iteration 96, loss = 0.58020926\n",
      "Iteration 97, loss = 0.57769325\n",
      "Iteration 98, loss = 0.57520836\n",
      "Iteration 99, loss = 0.57275411\n",
      "Iteration 100, loss = 0.57033003\n",
      "Iteration 101, loss = 0.56793565\n",
      "Iteration 102, loss = 0.56557050\n",
      "Iteration 103, loss = 0.56323448\n",
      "Iteration 104, loss = 0.56092681\n",
      "Iteration 105, loss = 0.55864681\n",
      "Iteration 106, loss = 0.55639400\n",
      "Iteration 107, loss = 0.55416795\n",
      "Iteration 108, loss = 0.55196744\n",
      "Iteration 109, loss = 0.54979275\n",
      "Iteration 110, loss = 0.54764344\n",
      "Iteration 111, loss = 0.54551914\n",
      "Iteration 112, loss = 0.54341939\n",
      "Iteration 113, loss = 0.54134389\n",
      "Iteration 114, loss = 0.53929257\n",
      "Iteration 115, loss = 0.53726458\n",
      "Iteration 116, loss = 0.53525951\n",
      "Iteration 117, loss = 0.53327695\n",
      "Iteration 118, loss = 0.53131651\n",
      "Iteration 119, loss = 0.52937781\n",
      "Iteration 120, loss = 0.52746045\n",
      "Iteration 121, loss = 0.52556428\n",
      "Iteration 122, loss = 0.52368880\n",
      "Iteration 123, loss = 0.52183356\n",
      "Iteration 124, loss = 0.51999819\n",
      "Iteration 125, loss = 0.51818234\n",
      "Iteration 126, loss = 0.51638565\n",
      "Iteration 127, loss = 0.51460780\n",
      "Iteration 128, loss = 0.51284843\n",
      "Iteration 129, loss = 0.51110723\n",
      "Iteration 130, loss = 0.50938471\n",
      "Iteration 131, loss = 0.50767998\n",
      "Iteration 132, loss = 0.50599249\n",
      "Iteration 133, loss = 0.50432215\n",
      "Iteration 134, loss = 0.50266852\n",
      "Iteration 135, loss = 0.50103127\n",
      "Iteration 136, loss = 0.49941012\n",
      "Iteration 137, loss = 0.49780481\n",
      "Iteration 138, loss = 0.49621506\n",
      "Iteration 139, loss = 0.49464060\n",
      "Iteration 140, loss = 0.49308118\n",
      "Iteration 141, loss = 0.49153654\n",
      "Iteration 142, loss = 0.49000643\n",
      "Iteration 143, loss = 0.48849059\n",
      "Iteration 144, loss = 0.48698878\n",
      "Iteration 145, loss = 0.48550076\n",
      "Iteration 146, loss = 0.48402629\n",
      "Iteration 147, loss = 0.48256581\n",
      "Iteration 148, loss = 0.48111970\n",
      "Iteration 149, loss = 0.47968657\n",
      "Iteration 150, loss = 0.47826621\n",
      "Iteration 151, loss = 0.47685845\n",
      "Iteration 152, loss = 0.47546307\n",
      "Iteration 153, loss = 0.47407989\n",
      "Iteration 154, loss = 0.47270916\n",
      "Iteration 155, loss = 0.47135005\n",
      "Iteration 156, loss = 0.47000225\n",
      "Iteration 157, loss = 0.46866616\n",
      "Iteration 158, loss = 0.46734169\n",
      "Iteration 159, loss = 0.46602850\n",
      "Iteration 160, loss = 0.46472621\n",
      "Iteration 161, loss = 0.46343466\n",
      "Iteration 162, loss = 0.46215368\n",
      "Iteration 163, loss = 0.46088311\n",
      "Iteration 164, loss = 0.45962294\n",
      "Iteration 165, loss = 0.45837295\n",
      "Iteration 166, loss = 0.45713287\n",
      "Iteration 167, loss = 0.45590264\n",
      "Iteration 168, loss = 0.45468223\n",
      "Iteration 169, loss = 0.45347120\n",
      "Iteration 170, loss = 0.45226951\n",
      "Iteration 171, loss = 0.45107726\n",
      "Iteration 172, loss = 0.44989381\n",
      "Iteration 173, loss = 0.44871951\n",
      "Iteration 174, loss = 0.44755395\n",
      "Iteration 175, loss = 0.44639706\n",
      "Iteration 176, loss = 0.44524872\n",
      "Iteration 177, loss = 0.44410879\n",
      "Iteration 178, loss = 0.44297717\n",
      "Iteration 179, loss = 0.44185376\n",
      "Iteration 180, loss = 0.44073856\n",
      "Iteration 181, loss = 0.43963183\n",
      "Iteration 182, loss = 0.43853296\n",
      "Iteration 183, loss = 0.43744182\n",
      "Iteration 184, loss = 0.43635829\n",
      "Iteration 185, loss = 0.43528228\n",
      "Iteration 186, loss = 0.43421365\n",
      "Iteration 187, loss = 0.43315231\n",
      "Iteration 188, loss = 0.43209814\n",
      "Iteration 189, loss = 0.43105104\n",
      "Iteration 190, loss = 0.43001091\n",
      "Iteration 191, loss = 0.42897763\n",
      "Iteration 192, loss = 0.42795119\n",
      "Iteration 193, loss = 0.42693157\n",
      "Iteration 194, loss = 0.42591943\n",
      "Iteration 195, loss = 0.42491467\n",
      "Iteration 196, loss = 0.42391626\n",
      "Iteration 197, loss = 0.42292397\n",
      "Iteration 198, loss = 0.42193782\n",
      "Iteration 199, loss = 0.42095781\n",
      "Iteration 200, loss = 0.41998374\n",
      "Iteration 201, loss = 0.41901584\n",
      "Iteration 202, loss = 0.41805376\n",
      "Iteration 203, loss = 0.41709785\n",
      "Iteration 204, loss = 0.41614792\n",
      "Iteration 205, loss = 0.41520384\n",
      "Iteration 206, loss = 0.41426546\n",
      "Iteration 207, loss = 0.41333258\n",
      "Iteration 208, loss = 0.41240516\n",
      "Iteration 209, loss = 0.41148316\n",
      "Iteration 210, loss = 0.41056666\n",
      "Iteration 211, loss = 0.40965528\n",
      "Iteration 212, loss = 0.40874931\n",
      "Iteration 213, loss = 0.40784839\n",
      "Iteration 214, loss = 0.40695254\n",
      "Iteration 215, loss = 0.40606170\n",
      "Iteration 216, loss = 0.40517591\n",
      "Iteration 217, loss = 0.40429500\n",
      "Iteration 218, loss = 0.40341892\n",
      "Iteration 219, loss = 0.40254762\n",
      "Iteration 220, loss = 0.40168102\n",
      "Iteration 221, loss = 0.40081914\n",
      "Iteration 222, loss = 0.39996179\n",
      "Iteration 223, loss = 0.39910917\n",
      "Iteration 224, loss = 0.39826136\n",
      "Iteration 225, loss = 0.39741800\n",
      "Iteration 226, loss = 0.39657902\n",
      "Iteration 227, loss = 0.39574445\n",
      "Iteration 228, loss = 0.39491420\n",
      "Iteration 229, loss = 0.39408819\n",
      "Iteration 230, loss = 0.39326646\n",
      "Iteration 231, loss = 0.39244909\n",
      "Iteration 232, loss = 0.39163587\n",
      "Iteration 233, loss = 0.39082666\n",
      "Iteration 234, loss = 0.39002158\n",
      "Iteration 235, loss = 0.38922040\n",
      "Iteration 236, loss = 0.38842321\n",
      "Iteration 237, loss = 0.38762982\n",
      "Iteration 238, loss = 0.38684034\n",
      "Iteration 239, loss = 0.38605459\n",
      "Iteration 240, loss = 0.38527264\n",
      "Iteration 241, loss = 0.38449434\n",
      "Iteration 242, loss = 0.38371974\n",
      "Iteration 243, loss = 0.38294873\n",
      "Iteration 244, loss = 0.38218130\n",
      "Iteration 245, loss = 0.38141747\n",
      "Iteration 246, loss = 0.38065721\n",
      "Iteration 247, loss = 0.37990064\n",
      "Iteration 248, loss = 0.37914740\n",
      "Iteration 249, loss = 0.37839750\n",
      "Iteration 250, loss = 0.37765106\n",
      "Iteration 251, loss = 0.37690790\n",
      "Iteration 252, loss = 0.37616844\n",
      "Iteration 253, loss = 0.37543215\n",
      "Iteration 254, loss = 0.37469937\n",
      "Iteration 255, loss = 0.37396960\n",
      "Iteration 256, loss = 0.37324308\n",
      "Iteration 257, loss = 0.37251978\n",
      "Iteration 258, loss = 0.37179952\n",
      "Iteration 259, loss = 0.37108248\n",
      "Iteration 260, loss = 0.37036841\n",
      "Iteration 261, loss = 0.36965748\n",
      "Iteration 262, loss = 0.36894944\n",
      "Iteration 263, loss = 0.36824453\n",
      "Iteration 264, loss = 0.36754240\n",
      "Iteration 265, loss = 0.36684331\n",
      "Iteration 266, loss = 0.36614699\n",
      "Iteration 267, loss = 0.36545364\n",
      "Iteration 268, loss = 0.36476299\n",
      "Iteration 269, loss = 0.36407523\n",
      "Iteration 270, loss = 0.36339021\n",
      "Iteration 271, loss = 0.36270794\n",
      "Iteration 272, loss = 0.36202843\n",
      "Iteration 273, loss = 0.36135156\n",
      "Iteration 274, loss = 0.36067736\n",
      "Iteration 275, loss = 0.36000592\n",
      "Iteration 276, loss = 0.35933695\n",
      "Iteration 277, loss = 0.35867064\n",
      "Iteration 278, loss = 0.35800699\n",
      "Iteration 279, loss = 0.35734583\n",
      "Iteration 280, loss = 0.35668719\n",
      "Iteration 281, loss = 0.35603107\n",
      "Iteration 282, loss = 0.35537749\n",
      "Iteration 283, loss = 0.35472633\n",
      "Iteration 284, loss = 0.35407764\n",
      "Iteration 285, loss = 0.35343147\n",
      "Iteration 286, loss = 0.35278816\n",
      "Iteration 287, loss = 0.35214755\n",
      "Iteration 288, loss = 0.35150933\n",
      "Iteration 289, loss = 0.35087374\n",
      "Iteration 290, loss = 0.35024062\n",
      "Iteration 291, loss = 0.34960985\n",
      "Iteration 292, loss = 0.34898146\n",
      "Iteration 293, loss = 0.34835537\n",
      "Iteration 294, loss = 0.34773155\n",
      "Iteration 295, loss = 0.34711001\n",
      "Iteration 296, loss = 0.34649074\n",
      "Iteration 297, loss = 0.34587400\n",
      "Iteration 298, loss = 0.34525950\n",
      "Iteration 299, loss = 0.34464718\n",
      "Iteration 300, loss = 0.34403706\n",
      "Iteration 301, loss = 0.34342920\n",
      "Iteration 302, loss = 0.34282369\n",
      "Iteration 303, loss = 0.34222030\n",
      "Iteration 304, loss = 0.34161902\n",
      "Iteration 305, loss = 0.34101986\n",
      "Iteration 306, loss = 0.34042276\n",
      "Iteration 307, loss = 0.33982774\n",
      "Iteration 308, loss = 0.33923474\n",
      "Iteration 309, loss = 0.33864378\n",
      "Iteration 310, loss = 0.33805482\n",
      "Iteration 311, loss = 0.33746791\n",
      "Iteration 312, loss = 0.33688306\n",
      "Iteration 313, loss = 0.33630049\n",
      "Iteration 314, loss = 0.33571987\n",
      "Iteration 315, loss = 0.33514121\n",
      "Iteration 316, loss = 0.33456465\n",
      "Iteration 317, loss = 0.33399009\n",
      "Iteration 318, loss = 0.33341772\n",
      "Iteration 319, loss = 0.33284733\n",
      "Iteration 320, loss = 0.33227882\n",
      "Iteration 321, loss = 0.33171221\n",
      "Iteration 322, loss = 0.33114766\n",
      "Iteration 323, loss = 0.33058514\n",
      "Iteration 324, loss = 0.33002461\n",
      "Iteration 325, loss = 0.32946612\n",
      "Iteration 326, loss = 0.32890951\n",
      "Iteration 327, loss = 0.32835472\n",
      "Iteration 328, loss = 0.32780173\n",
      "Iteration 329, loss = 0.32725052\n",
      "Iteration 330, loss = 0.32670109\n",
      "Iteration 331, loss = 0.32615340\n",
      "Iteration 332, loss = 0.32560745\n",
      "Iteration 333, loss = 0.32506322\n",
      "Iteration 334, loss = 0.32452071\n",
      "Iteration 335, loss = 0.32397989\n",
      "Iteration 336, loss = 0.32344075\n",
      "Iteration 337, loss = 0.32290333\n",
      "Iteration 338, loss = 0.32236780\n",
      "Iteration 339, loss = 0.32183393\n",
      "Iteration 340, loss = 0.32130172\n",
      "Iteration 341, loss = 0.32077112\n",
      "Iteration 342, loss = 0.32024217\n",
      "Iteration 343, loss = 0.31971482\n",
      "Iteration 344, loss = 0.31918907\n",
      "Iteration 345, loss = 0.31866491\n",
      "Iteration 346, loss = 0.31814243\n",
      "Iteration 347, loss = 0.31762164\n",
      "Iteration 348, loss = 0.31710241\n",
      "Iteration 349, loss = 0.31658475\n",
      "Iteration 350, loss = 0.31606864\n",
      "Iteration 351, loss = 0.31555405\n",
      "Iteration 352, loss = 0.31504100\n",
      "Iteration 353, loss = 0.31452946\n",
      "Iteration 354, loss = 0.31401945\n",
      "Iteration 355, loss = 0.31351091\n",
      "Iteration 356, loss = 0.31300386\n",
      "Iteration 357, loss = 0.31249830\n",
      "Iteration 358, loss = 0.31199421\n",
      "Iteration 359, loss = 0.31149171\n",
      "Iteration 360, loss = 0.31099069\n",
      "Iteration 361, loss = 0.31049107\n",
      "Iteration 362, loss = 0.30999287\n",
      "Iteration 363, loss = 0.30949607\n",
      "Iteration 364, loss = 0.30900078\n",
      "Iteration 365, loss = 0.30850696\n",
      "Iteration 366, loss = 0.30801460\n",
      "Iteration 367, loss = 0.30752369\n",
      "Iteration 368, loss = 0.30703417\n",
      "Iteration 369, loss = 0.30654604\n",
      "Iteration 370, loss = 0.30605928\n",
      "Iteration 371, loss = 0.30557400\n",
      "Iteration 372, loss = 0.30509022\n",
      "Iteration 373, loss = 0.30460775\n",
      "Iteration 374, loss = 0.30412665\n",
      "Iteration 375, loss = 0.30364692\n",
      "Iteration 376, loss = 0.30316867\n",
      "Iteration 377, loss = 0.30269178\n",
      "Iteration 378, loss = 0.30221622\n",
      "Iteration 379, loss = 0.30174203\n",
      "Iteration 380, loss = 0.30126912\n",
      "Iteration 381, loss = 0.30079757\n",
      "Iteration 382, loss = 0.30032731\n",
      "Iteration 383, loss = 0.29985834\n",
      "Iteration 384, loss = 0.29939084\n",
      "Iteration 385, loss = 0.29892463\n",
      "Iteration 386, loss = 0.29845976\n",
      "Iteration 387, loss = 0.29799615\n",
      "Iteration 388, loss = 0.29753380\n",
      "Iteration 389, loss = 0.29707277\n",
      "Iteration 390, loss = 0.29661296\n",
      "Iteration 391, loss = 0.29615455\n",
      "Iteration 392, loss = 0.29569739\n",
      "Iteration 393, loss = 0.29524149\n",
      "Iteration 394, loss = 0.29478686\n",
      "Iteration 395, loss = 0.29433344\n",
      "Iteration 396, loss = 0.29388176\n",
      "Iteration 397, loss = 0.29343088\n",
      "Iteration 398, loss = 0.29298127\n",
      "Iteration 399, loss = 0.29253317\n",
      "Iteration 400, loss = 0.29208625\n",
      "Iteration 401, loss = 0.29164052\n",
      "Iteration 402, loss = 0.29119598\n",
      "Iteration 403, loss = 0.29075273\n",
      "Iteration 404, loss = 0.29031058\n",
      "Iteration 405, loss = 0.28986974\n",
      "Iteration 406, loss = 0.28942993\n",
      "Iteration 407, loss = 0.28899143\n",
      "Iteration 408, loss = 0.28855403\n",
      "Iteration 409, loss = 0.28811779\n",
      "Iteration 410, loss = 0.28768280\n",
      "Iteration 411, loss = 0.28724887\n",
      "Iteration 412, loss = 0.28681611\n",
      "Iteration 413, loss = 0.28638454\n",
      "Iteration 414, loss = 0.28595406\n",
      "Iteration 415, loss = 0.28552478\n",
      "Iteration 416, loss = 0.28509652\n",
      "Iteration 417, loss = 0.28466954\n",
      "Iteration 418, loss = 0.28424353\n",
      "Iteration 419, loss = 0.28381875\n",
      "Iteration 420, loss = 0.28339506\n",
      "Iteration 421, loss = 0.28297245\n",
      "Iteration 422, loss = 0.28255106\n",
      "Iteration 423, loss = 0.28213062\n",
      "Iteration 424, loss = 0.28171147\n",
      "Iteration 425, loss = 0.28129327\n",
      "Iteration 426, loss = 0.28087615\n",
      "Iteration 427, loss = 0.28046019\n",
      "Iteration 428, loss = 0.28004525\n",
      "Iteration 429, loss = 0.27963142\n",
      "Iteration 430, loss = 0.27921863\n",
      "Iteration 431, loss = 0.27880696\n",
      "Iteration 432, loss = 0.27839628\n",
      "Iteration 433, loss = 0.27798678\n",
      "Iteration 434, loss = 0.27757823\n",
      "Iteration 435, loss = 0.27717086\n",
      "Iteration 436, loss = 0.27676444\n",
      "Iteration 437, loss = 0.27635926\n",
      "Iteration 438, loss = 0.27595519\n",
      "Iteration 439, loss = 0.27555212\n",
      "Iteration 440, loss = 0.27515020\n",
      "Iteration 441, loss = 0.27474921\n",
      "Iteration 442, loss = 0.27434934\n",
      "Iteration 443, loss = 0.27395041\n",
      "Iteration 444, loss = 0.27355263\n",
      "Iteration 445, loss = 0.27315575\n",
      "Iteration 446, loss = 0.27275995\n",
      "Iteration 447, loss = 0.27236512\n",
      "Iteration 448, loss = 0.27197136\n",
      "Iteration 449, loss = 0.27157853\n",
      "Iteration 450, loss = 0.27118679\n",
      "Iteration 451, loss = 0.27079595\n",
      "Iteration 452, loss = 0.27040622\n",
      "Iteration 453, loss = 0.27001741\n",
      "Iteration 454, loss = 0.26962952\n",
      "Iteration 455, loss = 0.26924273\n",
      "Iteration 456, loss = 0.26885682\n",
      "Iteration 457, loss = 0.26847199\n",
      "Iteration 458, loss = 0.26808806\n",
      "Iteration 459, loss = 0.26770518\n",
      "Iteration 460, loss = 0.26732321\n",
      "Iteration 461, loss = 0.26694224\n",
      "Iteration 462, loss = 0.26656227\n",
      "Iteration 463, loss = 0.26618319\n",
      "Iteration 464, loss = 0.26580512\n",
      "Iteration 465, loss = 0.26542800\n",
      "Iteration 466, loss = 0.26505181\n",
      "Iteration 467, loss = 0.26467660\n",
      "Iteration 468, loss = 0.26430237\n",
      "Iteration 469, loss = 0.26392905\n",
      "Iteration 470, loss = 0.26355669\n",
      "Iteration 471, loss = 0.26318522\n",
      "Iteration 472, loss = 0.26281473\n",
      "Iteration 473, loss = 0.26244513\n",
      "Iteration 474, loss = 0.26207650\n",
      "Iteration 475, loss = 0.26170874\n",
      "Iteration 476, loss = 0.26134199\n",
      "Iteration 477, loss = 0.26097605\n",
      "Iteration 478, loss = 0.26061102\n",
      "Iteration 479, loss = 0.26024696\n",
      "Iteration 480, loss = 0.25988380\n",
      "Iteration 481, loss = 0.25952147\n",
      "Iteration 482, loss = 0.25916016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 483, loss = 0.25879965\n",
      "Iteration 484, loss = 0.25844006\n",
      "Iteration 485, loss = 0.25808132\n",
      "Iteration 486, loss = 0.25772359\n",
      "Iteration 487, loss = 0.25736659\n",
      "Iteration 488, loss = 0.25701058\n",
      "Iteration 489, loss = 0.25665541\n",
      "Iteration 490, loss = 0.25630111\n",
      "Iteration 491, loss = 0.25594768\n",
      "Iteration 492, loss = 0.25559517\n",
      "Iteration 493, loss = 0.25524351\n",
      "Iteration 494, loss = 0.25489272\n",
      "Iteration 495, loss = 0.25454277\n",
      "Iteration 496, loss = 0.25419370\n",
      "Iteration 497, loss = 0.25384551\n",
      "Iteration 498, loss = 0.25349815\n",
      "Iteration 499, loss = 0.25315166\n",
      "Iteration 500, loss = 0.25280596\n",
      "Iteration 501, loss = 0.25246123\n",
      "Iteration 502, loss = 0.25211723\n",
      "Iteration 503, loss = 0.25177412\n",
      "Iteration 504, loss = 0.25143180\n",
      "Iteration 505, loss = 0.25109042\n",
      "Iteration 506, loss = 0.25074978\n",
      "Iteration 507, loss = 0.25041006\n",
      "Iteration 508, loss = 0.25007108\n",
      "Iteration 509, loss = 0.24973294\n",
      "Iteration 510, loss = 0.24939568\n",
      "Iteration 511, loss = 0.24905918\n",
      "Iteration 512, loss = 0.24872356\n",
      "Iteration 513, loss = 0.24838872\n",
      "Iteration 514, loss = 0.24805475\n",
      "Iteration 515, loss = 0.24772151\n",
      "Iteration 516, loss = 0.24738911\n",
      "Iteration 517, loss = 0.24705752\n",
      "Iteration 518, loss = 0.24672679\n",
      "Iteration 519, loss = 0.24639677\n",
      "Iteration 520, loss = 0.24606764\n",
      "Iteration 521, loss = 0.24573924\n",
      "Iteration 522, loss = 0.24541165\n",
      "Iteration 523, loss = 0.24508489\n",
      "Iteration 524, loss = 0.24475887\n",
      "Iteration 525, loss = 0.24443369\n",
      "Iteration 526, loss = 0.24410923\n",
      "Iteration 527, loss = 0.24378569\n",
      "Iteration 528, loss = 0.24346278\n",
      "Iteration 529, loss = 0.24314072\n",
      "Iteration 530, loss = 0.24281941\n",
      "Iteration 531, loss = 0.24249896\n",
      "Iteration 532, loss = 0.24217915\n",
      "Iteration 533, loss = 0.24186026\n",
      "Iteration 534, loss = 0.24154205\n",
      "Iteration 535, loss = 0.24122455\n",
      "Iteration 536, loss = 0.24090804\n",
      "Iteration 537, loss = 0.24059220\n",
      "Iteration 538, loss = 0.24027720\n",
      "Iteration 539, loss = 0.23996284\n",
      "Iteration 540, loss = 0.23964941\n",
      "Iteration 541, loss = 0.23933656\n",
      "Iteration 542, loss = 0.23902460\n",
      "Iteration 543, loss = 0.23871331\n",
      "Iteration 544, loss = 0.23840280\n",
      "Iteration 545, loss = 0.23809307\n",
      "Iteration 546, loss = 0.23778404\n",
      "Iteration 547, loss = 0.23747577\n",
      "Iteration 548, loss = 0.23716820\n",
      "Iteration 549, loss = 0.23686140\n",
      "Iteration 550, loss = 0.23655532\n",
      "Iteration 551, loss = 0.23625004\n",
      "Iteration 552, loss = 0.23594542\n",
      "Iteration 553, loss = 0.23564157\n",
      "Iteration 554, loss = 0.23533840\n",
      "Iteration 555, loss = 0.23503606\n",
      "Iteration 556, loss = 0.23473443\n",
      "Iteration 557, loss = 0.23443358\n",
      "Iteration 558, loss = 0.23413340\n",
      "Iteration 559, loss = 0.23383392\n",
      "Iteration 560, loss = 0.23353524\n",
      "Iteration 561, loss = 0.23323722\n",
      "Iteration 562, loss = 0.23293993\n",
      "Iteration 563, loss = 0.23264342\n",
      "Iteration 564, loss = 0.23234765\n",
      "Iteration 565, loss = 0.23205254\n",
      "Iteration 566, loss = 0.23175820\n",
      "Iteration 567, loss = 0.23146453\n",
      "Iteration 568, loss = 0.23117158\n",
      "Iteration 569, loss = 0.23087932\n",
      "Iteration 570, loss = 0.23058782\n",
      "Iteration 571, loss = 0.23029697\n",
      "Iteration 572, loss = 0.23000683\n",
      "Iteration 573, loss = 0.22971738\n",
      "Iteration 574, loss = 0.22942861\n",
      "Iteration 575, loss = 0.22914058\n",
      "Iteration 576, loss = 0.22885318\n",
      "Iteration 577, loss = 0.22856655\n",
      "Iteration 578, loss = 0.22828055\n",
      "Iteration 579, loss = 0.22799532\n",
      "Iteration 580, loss = 0.22771071\n",
      "Iteration 581, loss = 0.22742678\n",
      "Iteration 582, loss = 0.22714354\n",
      "Iteration 583, loss = 0.22686097\n",
      "Iteration 584, loss = 0.22657905\n",
      "Iteration 585, loss = 0.22629785\n",
      "Iteration 586, loss = 0.22601731\n",
      "Iteration 587, loss = 0.22573746\n",
      "Iteration 588, loss = 0.22545828\n",
      "Iteration 589, loss = 0.22517973\n",
      "Iteration 590, loss = 0.22490196\n",
      "Iteration 591, loss = 0.22462475\n",
      "Iteration 592, loss = 0.22434829\n",
      "Iteration 593, loss = 0.22407242\n",
      "Iteration 594, loss = 0.22379724\n",
      "Iteration 595, loss = 0.22352267\n",
      "Iteration 596, loss = 0.22324880\n",
      "Iteration 597, loss = 0.22297552\n",
      "Iteration 598, loss = 0.22270293\n",
      "Iteration 599, loss = 0.22243097\n",
      "Iteration 600, loss = 0.22215964\n",
      "Iteration 601, loss = 0.22188894\n",
      "Iteration 602, loss = 0.22161892\n",
      "Iteration 603, loss = 0.22134948\n",
      "Iteration 604, loss = 0.22108074\n",
      "Iteration 605, loss = 0.22081258\n",
      "Iteration 606, loss = 0.22054503\n",
      "Iteration 607, loss = 0.22027815\n",
      "Iteration 608, loss = 0.22001195\n",
      "Iteration 609, loss = 0.21974631\n",
      "Iteration 610, loss = 0.21948131\n",
      "Iteration 611, loss = 0.21921697\n",
      "Iteration 612, loss = 0.21895325\n",
      "Iteration 613, loss = 0.21869016\n",
      "Iteration 614, loss = 0.21842769\n",
      "Iteration 615, loss = 0.21816585\n",
      "Iteration 616, loss = 0.21790460\n",
      "Iteration 617, loss = 0.21764398\n",
      "Iteration 618, loss = 0.21738396\n",
      "Iteration 619, loss = 0.21712455\n",
      "Iteration 620, loss = 0.21686574\n",
      "Iteration 621, loss = 0.21660754\n",
      "Iteration 622, loss = 0.21634994\n",
      "Iteration 623, loss = 0.21609294\n",
      "Iteration 624, loss = 0.21583653\n",
      "Iteration 625, loss = 0.21558073\n",
      "Iteration 626, loss = 0.21532551\n",
      "Iteration 627, loss = 0.21507089\n",
      "Iteration 628, loss = 0.21481686\n",
      "Iteration 629, loss = 0.21456342\n",
      "Iteration 630, loss = 0.21431058\n",
      "Iteration 631, loss = 0.21405831\n",
      "Iteration 632, loss = 0.21380665\n",
      "Iteration 633, loss = 0.21355555\n",
      "Iteration 634, loss = 0.21330505\n",
      "Iteration 635, loss = 0.21305511\n",
      "Iteration 636, loss = 0.21280576\n",
      "Iteration 637, loss = 0.21255700\n",
      "Iteration 638, loss = 0.21230883\n",
      "Iteration 639, loss = 0.21206124\n",
      "Iteration 640, loss = 0.21181420\n",
      "Iteration 641, loss = 0.21156775\n",
      "Iteration 642, loss = 0.21132186\n",
      "Iteration 643, loss = 0.21107654\n",
      "Iteration 644, loss = 0.21083178\n",
      "Iteration 645, loss = 0.21058759\n",
      "Iteration 646, loss = 0.21034396\n",
      "Iteration 647, loss = 0.21010090\n",
      "Iteration 648, loss = 0.20985841\n",
      "Iteration 649, loss = 0.20961648\n",
      "Iteration 650, loss = 0.20937510\n",
      "Iteration 651, loss = 0.20913428\n",
      "Iteration 652, loss = 0.20889401\n",
      "Iteration 653, loss = 0.20865430\n",
      "Iteration 654, loss = 0.20841513\n",
      "Iteration 655, loss = 0.20817651\n",
      "Iteration 656, loss = 0.20793844\n",
      "Iteration 657, loss = 0.20770091\n",
      "Iteration 658, loss = 0.20746392\n",
      "Iteration 659, loss = 0.20722748\n",
      "Iteration 660, loss = 0.20699159\n",
      "Iteration 661, loss = 0.20675624\n",
      "Iteration 662, loss = 0.20652142\n",
      "Iteration 663, loss = 0.20628714\n",
      "Iteration 664, loss = 0.20605340\n",
      "Iteration 665, loss = 0.20582019\n",
      "Iteration 666, loss = 0.20558751\n",
      "Iteration 667, loss = 0.20535536\n",
      "Iteration 668, loss = 0.20512375\n",
      "Iteration 669, loss = 0.20489265\n",
      "Iteration 670, loss = 0.20466208\n",
      "Iteration 671, loss = 0.20443204\n",
      "Iteration 672, loss = 0.20420253\n",
      "Iteration 673, loss = 0.20397354\n",
      "Iteration 674, loss = 0.20374507\n",
      "Iteration 675, loss = 0.20351711\n",
      "Iteration 676, loss = 0.20328967\n",
      "Iteration 677, loss = 0.20306275\n",
      "Iteration 678, loss = 0.20283635\n",
      "Iteration 679, loss = 0.20261045\n",
      "Iteration 680, loss = 0.20238507\n",
      "Iteration 681, loss = 0.20216020\n",
      "Iteration 682, loss = 0.20193583\n",
      "Iteration 683, loss = 0.20171198\n",
      "Iteration 684, loss = 0.20148863\n",
      "Iteration 685, loss = 0.20126578\n",
      "Iteration 686, loss = 0.20104344\n",
      "Iteration 687, loss = 0.20082160\n",
      "Iteration 688, loss = 0.20060026\n",
      "Iteration 689, loss = 0.20037942\n",
      "Iteration 690, loss = 0.20015907\n",
      "Iteration 691, loss = 0.19993923\n",
      "Iteration 692, loss = 0.19971988\n",
      "Iteration 693, loss = 0.19950102\n",
      "Iteration 694, loss = 0.19928265\n",
      "Iteration 695, loss = 0.19906478\n",
      "Iteration 696, loss = 0.19884740\n",
      "Iteration 697, loss = 0.19863050\n",
      "Iteration 698, loss = 0.19841410\n",
      "Iteration 699, loss = 0.19819817\n",
      "Iteration 700, loss = 0.19798274\n",
      "Iteration 701, loss = 0.19776779\n",
      "Iteration 702, loss = 0.19755332\n",
      "Iteration 703, loss = 0.19733933\n",
      "Iteration 704, loss = 0.19712582\n",
      "Iteration 705, loss = 0.19691279\n",
      "Iteration 706, loss = 0.19670024\n",
      "Iteration 707, loss = 0.19648816\n",
      "Iteration 708, loss = 0.19627656\n",
      "Iteration 709, loss = 0.19606543\n",
      "Iteration 710, loss = 0.19585477\n",
      "Iteration 711, loss = 0.19564458\n",
      "Iteration 712, loss = 0.19543486\n",
      "Iteration 713, loss = 0.19522561\n",
      "Iteration 714, loss = 0.19501682\n",
      "Iteration 715, loss = 0.19480850\n",
      "Iteration 716, loss = 0.19460065\n",
      "Iteration 717, loss = 0.19439325\n",
      "Iteration 718, loss = 0.19418632\n",
      "Iteration 719, loss = 0.19397985\n",
      "Iteration 720, loss = 0.19377383\n",
      "Iteration 721, loss = 0.19356827\n",
      "Iteration 722, loss = 0.19336317\n",
      "Iteration 723, loss = 0.19315853\n",
      "Iteration 724, loss = 0.19295433\n",
      "Iteration 725, loss = 0.19275060\n",
      "Iteration 726, loss = 0.19254731\n",
      "Iteration 727, loss = 0.19234447\n",
      "Iteration 728, loss = 0.19214208\n",
      "Iteration 729, loss = 0.19194014\n",
      "Iteration 730, loss = 0.19173864\n",
      "Iteration 731, loss = 0.19153759\n",
      "Iteration 732, loss = 0.19133699\n",
      "Iteration 733, loss = 0.19113682\n",
      "Iteration 734, loss = 0.19093712\n",
      "Iteration 735, loss = 0.19073786\n",
      "Iteration 736, loss = 0.19053905\n",
      "Iteration 737, loss = 0.19034068\n",
      "Iteration 738, loss = 0.19014275\n",
      "Iteration 739, loss = 0.18994525\n",
      "Iteration 740, loss = 0.18974819\n",
      "Iteration 741, loss = 0.18955156\n",
      "Iteration 742, loss = 0.18935537\n",
      "Iteration 743, loss = 0.18915961\n",
      "Iteration 744, loss = 0.18896428\n",
      "Iteration 745, loss = 0.18876937\n",
      "Iteration 746, loss = 0.18857490\n",
      "Iteration 747, loss = 0.18838085\n",
      "Iteration 748, loss = 0.18818722\n",
      "Iteration 749, loss = 0.18799402\n",
      "Iteration 750, loss = 0.18780125\n",
      "Iteration 751, loss = 0.18760889\n",
      "Iteration 752, loss = 0.18741696\n",
      "Iteration 753, loss = 0.18722545\n",
      "Iteration 754, loss = 0.18703435\n",
      "Iteration 755, loss = 0.18684367\n",
      "Iteration 756, loss = 0.18665341\n",
      "Iteration 757, loss = 0.18646356\n",
      "Iteration 758, loss = 0.18627413\n",
      "Iteration 759, loss = 0.18608510\n",
      "Iteration 760, loss = 0.18589649\n",
      "Iteration 761, loss = 0.18570829\n",
      "Iteration 762, loss = 0.18552050\n",
      "Iteration 763, loss = 0.18533312\n",
      "Iteration 764, loss = 0.18514615\n",
      "Iteration 765, loss = 0.18495960\n",
      "Iteration 766, loss = 0.18477345\n",
      "Iteration 767, loss = 0.18458770\n",
      "Iteration 768, loss = 0.18440235\n",
      "Iteration 769, loss = 0.18421741\n",
      "Iteration 770, loss = 0.18403286\n",
      "Iteration 771, loss = 0.18384873\n",
      "Iteration 772, loss = 0.18366498\n",
      "Iteration 773, loss = 0.18348163\n",
      "Iteration 774, loss = 0.18329869\n",
      "Iteration 775, loss = 0.18311614\n",
      "Iteration 776, loss = 0.18293398\n",
      "Iteration 777, loss = 0.18275223\n",
      "Iteration 778, loss = 0.18257089\n",
      "Iteration 779, loss = 0.18238993\n",
      "Iteration 780, loss = 0.18220937\n",
      "Iteration 781, loss = 0.18202920\n",
      "Iteration 782, loss = 0.18184942\n",
      "Iteration 783, loss = 0.18167004\n",
      "Iteration 784, loss = 0.18149103\n",
      "Iteration 785, loss = 0.18131242\n",
      "Iteration 786, loss = 0.18113419\n",
      "Iteration 787, loss = 0.18095635\n",
      "Iteration 788, loss = 0.18077888\n",
      "Iteration 789, loss = 0.18060180\n",
      "Iteration 790, loss = 0.18042510\n",
      "Iteration 791, loss = 0.18024877\n",
      "Iteration 792, loss = 0.18007283\n",
      "Iteration 793, loss = 0.17989726\n",
      "Iteration 794, loss = 0.17972207\n",
      "Iteration 795, loss = 0.17954726\n",
      "Iteration 796, loss = 0.17937281\n",
      "Iteration 797, loss = 0.17919874\n",
      "Iteration 798, loss = 0.17902504\n",
      "Iteration 799, loss = 0.17885172\n",
      "Iteration 800, loss = 0.17867876\n",
      "Iteration 801, loss = 0.17850618\n",
      "Iteration 802, loss = 0.17833396\n",
      "Iteration 803, loss = 0.17816211\n",
      "Iteration 804, loss = 0.17799062\n",
      "Iteration 805, loss = 0.17781950\n",
      "Iteration 806, loss = 0.17764874\n",
      "Iteration 807, loss = 0.17747835\n",
      "Iteration 808, loss = 0.17730837\n",
      "Iteration 809, loss = 0.17713867\n",
      "Iteration 810, loss = 0.17696938\n",
      "Iteration 811, loss = 0.17680045\n",
      "Iteration 812, loss = 0.17663187\n",
      "Iteration 813, loss = 0.17646364\n",
      "Iteration 814, loss = 0.17629578\n",
      "Iteration 815, loss = 0.17612826\n",
      "Iteration 816, loss = 0.17596111\n",
      "Iteration 817, loss = 0.17579431\n",
      "Iteration 818, loss = 0.17562787\n",
      "Iteration 819, loss = 0.17546177\n",
      "Iteration 820, loss = 0.17529603\n",
      "Iteration 821, loss = 0.17513063\n",
      "Iteration 822, loss = 0.17496558\n",
      "Iteration 823, loss = 0.17480090\n",
      "Iteration 824, loss = 0.17463654\n",
      "Iteration 825, loss = 0.17447255\n",
      "Iteration 826, loss = 0.17430889\n",
      "Iteration 827, loss = 0.17414557\n",
      "Iteration 828, loss = 0.17398260\n",
      "Iteration 829, loss = 0.17382000\n",
      "Iteration 830, loss = 0.17365770\n",
      "Iteration 831, loss = 0.17349576\n",
      "Iteration 832, loss = 0.17333415\n",
      "Iteration 833, loss = 0.17317289\n",
      "Iteration 834, loss = 0.17301198\n",
      "Iteration 835, loss = 0.17285139\n",
      "Iteration 836, loss = 0.17269114\n",
      "Iteration 837, loss = 0.17253122\n",
      "Iteration 838, loss = 0.17237164\n",
      "Iteration 839, loss = 0.17221239\n",
      "Iteration 840, loss = 0.17205349\n",
      "Iteration 841, loss = 0.17189491\n",
      "Iteration 842, loss = 0.17173666\n",
      "Iteration 843, loss = 0.17157874\n",
      "Iteration 844, loss = 0.17142114\n",
      "Iteration 845, loss = 0.17126391\n",
      "Iteration 846, loss = 0.17110695\n",
      "Iteration 847, loss = 0.17095034\n",
      "Iteration 848, loss = 0.17079407\n",
      "Iteration 849, loss = 0.17063812\n",
      "Iteration 850, loss = 0.17048250\n",
      "Iteration 851, loss = 0.17032718\n",
      "Iteration 852, loss = 0.17017220\n",
      "Iteration 853, loss = 0.17001753\n",
      "Iteration 854, loss = 0.16986319\n",
      "Iteration 855, loss = 0.16970917\n",
      "Iteration 856, loss = 0.16955547\n",
      "Iteration 857, loss = 0.16940208\n",
      "Iteration 858, loss = 0.16924902\n",
      "Iteration 859, loss = 0.16909627\n",
      "Iteration 860, loss = 0.16894384\n",
      "Iteration 861, loss = 0.16879172\n",
      "Iteration 862, loss = 0.16863991\n",
      "Iteration 863, loss = 0.16848843\n",
      "Iteration 864, loss = 0.16833725\n",
      "Iteration 865, loss = 0.16818638\n",
      "Iteration 866, loss = 0.16803582\n",
      "Iteration 867, loss = 0.16788559\n",
      "Iteration 868, loss = 0.16773564\n",
      "Iteration 869, loss = 0.16758601\n",
      "Iteration 870, loss = 0.16743669\n",
      "Iteration 871, loss = 0.16728769\n",
      "Iteration 872, loss = 0.16713900\n",
      "Iteration 873, loss = 0.16699060\n",
      "Iteration 874, loss = 0.16684251\n",
      "Iteration 875, loss = 0.16669475\n",
      "Iteration 876, loss = 0.16654727\n",
      "Iteration 877, loss = 0.16640009\n",
      "Iteration 878, loss = 0.16625323\n",
      "Iteration 879, loss = 0.16610667\n",
      "Iteration 880, loss = 0.16596040\n",
      "Iteration 881, loss = 0.16581442\n",
      "Iteration 882, loss = 0.16566875\n",
      "Iteration 883, loss = 0.16552339\n",
      "Iteration 884, loss = 0.16537831\n",
      "Iteration 885, loss = 0.16523352\n",
      "Iteration 886, loss = 0.16508904\n",
      "Iteration 887, loss = 0.16494486\n",
      "Iteration 888, loss = 0.16480096\n",
      "Iteration 889, loss = 0.16465735\n",
      "Iteration 890, loss = 0.16451406\n",
      "Iteration 891, loss = 0.16437102\n",
      "Iteration 892, loss = 0.16422829\n",
      "Iteration 893, loss = 0.16408586\n",
      "Iteration 894, loss = 0.16394371\n",
      "Iteration 895, loss = 0.16380185\n",
      "Iteration 896, loss = 0.16366027\n",
      "Iteration 897, loss = 0.16351899\n",
      "Iteration 898, loss = 0.16337798\n",
      "Iteration 899, loss = 0.16323726\n",
      "Iteration 900, loss = 0.16309684\n",
      "Iteration 901, loss = 0.16295668\n",
      "Iteration 902, loss = 0.16281681\n",
      "Iteration 903, loss = 0.16267724\n",
      "Iteration 904, loss = 0.16253794\n",
      "Iteration 905, loss = 0.16239892\n",
      "Iteration 906, loss = 0.16226017\n",
      "Iteration 907, loss = 0.16212172\n",
      "Iteration 908, loss = 0.16198354\n",
      "Iteration 909, loss = 0.16184563\n",
      "Iteration 910, loss = 0.16170800\n",
      "Iteration 911, loss = 0.16157065\n",
      "Iteration 912, loss = 0.16143357\n",
      "Iteration 913, loss = 0.16129679\n",
      "Iteration 914, loss = 0.16116025\n",
      "Iteration 915, loss = 0.16102399\n",
      "Iteration 916, loss = 0.16088803\n",
      "Iteration 917, loss = 0.16075232\n",
      "Iteration 918, loss = 0.16061688\n",
      "Iteration 919, loss = 0.16048172\n",
      "Iteration 920, loss = 0.16034682\n",
      "Iteration 921, loss = 0.16021219\n",
      "Iteration 922, loss = 0.16007784\n",
      "Iteration 923, loss = 0.15994376\n",
      "Iteration 924, loss = 0.15980993\n",
      "Iteration 925, loss = 0.15967638\n",
      "Iteration 926, loss = 0.15954309\n",
      "Iteration 927, loss = 0.15941006\n",
      "Iteration 928, loss = 0.15927731\n",
      "Iteration 929, loss = 0.15914482\n",
      "Iteration 930, loss = 0.15901258\n",
      "Iteration 931, loss = 0.15888062\n",
      "Iteration 932, loss = 0.15874891\n",
      "Iteration 933, loss = 0.15861745\n",
      "Iteration 934, loss = 0.15848629\n",
      "Iteration 935, loss = 0.15835535\n",
      "Iteration 936, loss = 0.15822468\n",
      "Iteration 937, loss = 0.15809428\n",
      "Iteration 938, loss = 0.15796413\n",
      "Iteration 939, loss = 0.15783423\n",
      "Iteration 940, loss = 0.15770461\n",
      "Iteration 941, loss = 0.15757521\n",
      "Iteration 942, loss = 0.15744610\n",
      "Iteration 943, loss = 0.15731722\n",
      "Iteration 944, loss = 0.15718860\n",
      "Iteration 945, loss = 0.15706024\n",
      "Iteration 946, loss = 0.15693212\n",
      "Iteration 947, loss = 0.15680426\n",
      "Iteration 948, loss = 0.15667667\n",
      "Iteration 949, loss = 0.15654930\n",
      "Iteration 950, loss = 0.15642218\n",
      "Iteration 951, loss = 0.15629534\n",
      "Iteration 952, loss = 0.15616871\n",
      "Iteration 953, loss = 0.15604237\n",
      "Iteration 954, loss = 0.15591625\n",
      "Iteration 955, loss = 0.15579037\n",
      "Iteration 956, loss = 0.15566475\n",
      "Iteration 957, loss = 0.15553937\n",
      "Iteration 958, loss = 0.15541424\n",
      "Iteration 959, loss = 0.15528936\n",
      "Iteration 960, loss = 0.15516471\n",
      "Iteration 961, loss = 0.15504031\n",
      "Iteration 962, loss = 0.15491615\n",
      "Iteration 963, loss = 0.15479223\n",
      "Iteration 964, loss = 0.15466856\n",
      "Iteration 965, loss = 0.15454512\n",
      "Iteration 966, loss = 0.15442193\n",
      "Iteration 967, loss = 0.15429896\n",
      "Iteration 968, loss = 0.15417624\n",
      "Iteration 969, loss = 0.15405377\n",
      "Iteration 970, loss = 0.15393152\n",
      "Iteration 971, loss = 0.15380952\n",
      "Iteration 972, loss = 0.15368774\n",
      "Iteration 973, loss = 0.15356621\n",
      "Iteration 974, loss = 0.15344492\n",
      "Iteration 975, loss = 0.15332384\n",
      "Iteration 976, loss = 0.15320302\n",
      "Iteration 977, loss = 0.15308241\n",
      "Iteration 978, loss = 0.15296205\n",
      "Iteration 979, loss = 0.15284192\n",
      "Iteration 980, loss = 0.15272201\n",
      "Iteration 981, loss = 0.15260235\n",
      "Iteration 982, loss = 0.15248290\n",
      "Iteration 983, loss = 0.15236368\n",
      "Iteration 984, loss = 0.15224471\n",
      "Iteration 985, loss = 0.15212594\n",
      "Iteration 986, loss = 0.15200743\n",
      "Iteration 987, loss = 0.15188912\n",
      "Iteration 988, loss = 0.15177105\n",
      "Iteration 989, loss = 0.15165320\n",
      "Iteration 990, loss = 0.15153559\n",
      "Iteration 991, loss = 0.15141818\n",
      "Iteration 992, loss = 0.15130101\n",
      "Iteration 993, loss = 0.15118407\n",
      "Iteration 994, loss = 0.15106734\n",
      "Iteration 995, loss = 0.15095085\n",
      "Iteration 996, loss = 0.15083455\n",
      "Iteration 997, loss = 0.15071851\n",
      "Iteration 998, loss = 0.15060267\n",
      "Iteration 999, loss = 0.15048704\n",
      "Iteration 1000, loss = 0.15037167\n",
      "Iteration 1, loss = 1.94163030\n",
      "Iteration 2, loss = 1.89615571\n",
      "Iteration 3, loss = 1.83491840\n",
      "Iteration 4, loss = 1.76269787\n",
      "Iteration 5, loss = 1.68374064\n",
      "Iteration 6, loss = 1.60151846\n",
      "Iteration 7, loss = 1.51868529\n",
      "Iteration 8, loss = 1.43754231\n",
      "Iteration 9, loss = 1.36019293\n",
      "Iteration 10, loss = 1.28884933\n",
      "Iteration 11, loss = 1.22563743\n",
      "Iteration 12, loss = 1.17265472\n",
      "Iteration 13, loss = 1.13133588\n",
      "Iteration 14, loss = 1.10190141\n",
      "Iteration 15, loss = 1.08323529\n",
      "Iteration 16, loss = 1.07296091\n",
      "Iteration 17, loss = 1.06786778\n",
      "Iteration 18, loss = 1.06490439\n",
      "Iteration 19, loss = 1.06156363\n",
      "Iteration 20, loss = 1.05607886\n",
      "Iteration 21, loss = 1.04776072\n",
      "Iteration 22, loss = 1.03668055\n",
      "Iteration 23, loss = 1.02355922\n",
      "Iteration 24, loss = 1.00930333\n",
      "Iteration 25, loss = 0.99479521\n",
      "Iteration 26, loss = 0.98069992\n",
      "Iteration 27, loss = 0.96745197\n",
      "Iteration 28, loss = 0.95512370\n",
      "Iteration 29, loss = 0.94352135\n",
      "Iteration 30, loss = 0.93244964\n",
      "Iteration 31, loss = 0.92164569\n",
      "Iteration 32, loss = 0.91085105\n",
      "Iteration 33, loss = 0.89976796\n",
      "Iteration 34, loss = 0.88853597\n",
      "Iteration 35, loss = 0.87710073\n",
      "Iteration 36, loss = 0.86548524\n",
      "Iteration 37, loss = 0.85402874\n",
      "Iteration 38, loss = 0.84303360\n",
      "Iteration 39, loss = 0.83266484\n",
      "Iteration 40, loss = 0.82297049\n",
      "Iteration 41, loss = 0.81397686\n",
      "Iteration 42, loss = 0.80570171\n",
      "Iteration 43, loss = 0.79812150\n",
      "Iteration 44, loss = 0.79104001\n",
      "Iteration 45, loss = 0.78413796\n",
      "Iteration 46, loss = 0.77736463\n",
      "Iteration 47, loss = 0.77069267\n",
      "Iteration 48, loss = 0.76403061\n",
      "Iteration 49, loss = 0.75741310\n",
      "Iteration 50, loss = 0.75091852\n",
      "Iteration 51, loss = 0.74456333\n",
      "Iteration 52, loss = 0.73833875\n",
      "Iteration 53, loss = 0.73230485\n",
      "Iteration 54, loss = 0.72655178\n",
      "Iteration 55, loss = 0.72102883\n",
      "Iteration 56, loss = 0.71573112\n",
      "Iteration 57, loss = 0.71058470\n",
      "Iteration 58, loss = 0.70556162\n",
      "Iteration 59, loss = 0.70068148\n",
      "Iteration 60, loss = 0.69593954\n",
      "Iteration 61, loss = 0.69129259\n",
      "Iteration 62, loss = 0.68676462\n",
      "Iteration 63, loss = 0.68237026\n",
      "Iteration 64, loss = 0.67809260\n",
      "Iteration 65, loss = 0.67389940\n",
      "Iteration 66, loss = 0.66978567\n",
      "Iteration 67, loss = 0.66576086\n",
      "Iteration 68, loss = 0.66183934\n",
      "Iteration 69, loss = 0.65803048\n",
      "Iteration 70, loss = 0.65431298\n",
      "Iteration 71, loss = 0.65068158\n",
      "Iteration 72, loss = 0.64711803\n",
      "Iteration 73, loss = 0.64362480\n",
      "Iteration 74, loss = 0.64020002\n",
      "Iteration 75, loss = 0.63684732\n",
      "Iteration 76, loss = 0.63354541\n",
      "Iteration 77, loss = 0.63029195\n",
      "Iteration 78, loss = 0.62708300\n",
      "Iteration 79, loss = 0.62392033\n",
      "Iteration 80, loss = 0.62080603\n",
      "Iteration 81, loss = 0.61773708\n",
      "Iteration 82, loss = 0.61471217\n",
      "Iteration 83, loss = 0.61173627\n",
      "Iteration 84, loss = 0.60881572\n",
      "Iteration 85, loss = 0.60594529\n",
      "Iteration 86, loss = 0.60312722\n",
      "Iteration 87, loss = 0.60035312\n",
      "Iteration 88, loss = 0.59762250\n",
      "Iteration 89, loss = 0.59493122\n",
      "Iteration 90, loss = 0.59227550\n",
      "Iteration 91, loss = 0.58965434\n",
      "Iteration 92, loss = 0.58706904\n",
      "Iteration 93, loss = 0.58451610\n",
      "Iteration 94, loss = 0.58199618\n",
      "Iteration 95, loss = 0.57951014\n",
      "Iteration 96, loss = 0.57705561\n",
      "Iteration 97, loss = 0.57463309\n",
      "Iteration 98, loss = 0.57224524\n",
      "Iteration 99, loss = 0.56988892\n",
      "Iteration 100, loss = 0.56756299\n",
      "Iteration 101, loss = 0.56526748\n",
      "Iteration 102, loss = 0.56300204\n",
      "Iteration 103, loss = 0.56076538\n",
      "Iteration 104, loss = 0.55855733\n",
      "Iteration 105, loss = 0.55637709\n",
      "Iteration 106, loss = 0.55422396\n",
      "Iteration 107, loss = 0.55209836\n",
      "Iteration 108, loss = 0.54999957\n",
      "Iteration 109, loss = 0.54792744\n",
      "Iteration 110, loss = 0.54588101\n",
      "Iteration 111, loss = 0.54385998\n",
      "Iteration 112, loss = 0.54186367\n",
      "Iteration 113, loss = 0.53989175\n",
      "Iteration 114, loss = 0.53794388\n",
      "Iteration 115, loss = 0.53601941\n",
      "Iteration 116, loss = 0.53411792\n",
      "Iteration 117, loss = 0.53223899\n",
      "Iteration 118, loss = 0.53038220\n",
      "Iteration 119, loss = 0.52854713\n",
      "Iteration 120, loss = 0.52673337\n",
      "Iteration 121, loss = 0.52494053\n",
      "Iteration 122, loss = 0.52316824\n",
      "Iteration 123, loss = 0.52141691\n",
      "Iteration 124, loss = 0.51968582\n",
      "Iteration 125, loss = 0.51797416\n",
      "Iteration 126, loss = 0.51628160\n",
      "Iteration 127, loss = 0.51460781\n",
      "Iteration 128, loss = 0.51295250\n",
      "Iteration 129, loss = 0.51131537\n",
      "Iteration 130, loss = 0.50969600\n",
      "Iteration 131, loss = 0.50809407\n",
      "Iteration 132, loss = 0.50650926\n",
      "Iteration 133, loss = 0.50494127\n",
      "Iteration 134, loss = 0.50339025\n",
      "Iteration 135, loss = 0.50185568\n",
      "Iteration 136, loss = 0.50033689\n",
      "Iteration 137, loss = 0.49883374\n",
      "Iteration 138, loss = 0.49734587\n",
      "Iteration 139, loss = 0.49587303\n",
      "Iteration 140, loss = 0.49441494\n",
      "Iteration 141, loss = 0.49297134\n",
      "Iteration 142, loss = 0.49154195\n",
      "Iteration 143, loss = 0.49012651\n",
      "Iteration 144, loss = 0.48872484\n",
      "Iteration 145, loss = 0.48733668\n",
      "Iteration 146, loss = 0.48596173\n",
      "Iteration 147, loss = 0.48459975\n",
      "Iteration 148, loss = 0.48325049\n",
      "Iteration 149, loss = 0.48191375\n",
      "Iteration 150, loss = 0.48058930\n",
      "Iteration 151, loss = 0.47927691\n",
      "Iteration 152, loss = 0.47797657\n",
      "Iteration 153, loss = 0.47668798\n",
      "Iteration 154, loss = 0.47541204\n",
      "Iteration 155, loss = 0.47414807\n",
      "Iteration 156, loss = 0.47289525\n",
      "Iteration 157, loss = 0.47165339\n",
      "Iteration 158, loss = 0.47042232\n",
      "Iteration 159, loss = 0.46920185\n",
      "Iteration 160, loss = 0.46799196\n",
      "Iteration 161, loss = 0.46679282\n",
      "Iteration 162, loss = 0.46560427\n",
      "Iteration 163, loss = 0.46442570\n",
      "Iteration 164, loss = 0.46325680\n",
      "Iteration 165, loss = 0.46209746\n",
      "Iteration 166, loss = 0.46094764\n",
      "Iteration 167, loss = 0.45980744\n",
      "Iteration 168, loss = 0.45867643\n",
      "Iteration 169, loss = 0.45755510\n",
      "Iteration 170, loss = 0.45644322\n",
      "Iteration 171, loss = 0.45534003\n",
      "Iteration 172, loss = 0.45424557\n",
      "Iteration 173, loss = 0.45315969\n",
      "Iteration 174, loss = 0.45208231\n",
      "Iteration 175, loss = 0.45101319\n",
      "Iteration 176, loss = 0.44995279\n",
      "Iteration 177, loss = 0.44890097\n",
      "Iteration 178, loss = 0.44785731\n",
      "Iteration 179, loss = 0.44682145\n",
      "Iteration 180, loss = 0.44579336\n",
      "Iteration 181, loss = 0.44477290\n",
      "Iteration 182, loss = 0.44376007\n",
      "Iteration 183, loss = 0.44275451\n",
      "Iteration 184, loss = 0.44175629\n",
      "Iteration 185, loss = 0.44076545\n",
      "Iteration 186, loss = 0.43978169\n",
      "Iteration 187, loss = 0.43880491\n",
      "Iteration 188, loss = 0.43783500\n",
      "Iteration 189, loss = 0.43687184\n",
      "Iteration 190, loss = 0.43591533\n",
      "Iteration 191, loss = 0.43496538\n",
      "Iteration 192, loss = 0.43402188\n",
      "Iteration 193, loss = 0.43308474\n",
      "Iteration 194, loss = 0.43215387\n",
      "Iteration 195, loss = 0.43122918\n",
      "Iteration 196, loss = 0.43031062\n",
      "Iteration 197, loss = 0.42939804\n",
      "Iteration 198, loss = 0.42849145\n",
      "Iteration 199, loss = 0.42759065\n",
      "Iteration 200, loss = 0.42669565\n",
      "Iteration 201, loss = 0.42580630\n",
      "Iteration 202, loss = 0.42492262\n",
      "Iteration 203, loss = 0.42404441\n",
      "Iteration 204, loss = 0.42317230\n",
      "Iteration 205, loss = 0.42230545\n",
      "Iteration 206, loss = 0.42144382\n",
      "Iteration 207, loss = 0.42058783\n",
      "Iteration 208, loss = 0.41973664\n",
      "Iteration 209, loss = 0.41889063\n",
      "Iteration 210, loss = 0.41804987\n",
      "Iteration 211, loss = 0.41721443\n",
      "Iteration 212, loss = 0.41638387\n",
      "Iteration 213, loss = 0.41555815\n",
      "Iteration 214, loss = 0.41473740\n",
      "Iteration 215, loss = 0.41392122\n",
      "Iteration 216, loss = 0.41310986\n",
      "Iteration 217, loss = 0.41230313\n",
      "Iteration 218, loss = 0.41150138\n",
      "Iteration 219, loss = 0.41070441\n",
      "Iteration 220, loss = 0.40991201\n",
      "Iteration 221, loss = 0.40912408\n",
      "Iteration 222, loss = 0.40834053\n",
      "Iteration 223, loss = 0.40756122\n",
      "Iteration 224, loss = 0.40678618\n",
      "Iteration 225, loss = 0.40601531\n",
      "Iteration 226, loss = 0.40524862\n",
      "Iteration 227, loss = 0.40448610\n",
      "Iteration 228, loss = 0.40372775\n",
      "Iteration 229, loss = 0.40297337\n",
      "Iteration 230, loss = 0.40222294\n",
      "Iteration 231, loss = 0.40147646\n",
      "Iteration 232, loss = 0.40073370\n",
      "Iteration 233, loss = 0.39999479\n",
      "Iteration 234, loss = 0.39925957\n",
      "Iteration 235, loss = 0.39852804\n",
      "Iteration 236, loss = 0.39780018\n",
      "Iteration 237, loss = 0.39707588\n",
      "Iteration 238, loss = 0.39635515\n",
      "Iteration 239, loss = 0.39563793\n",
      "Iteration 240, loss = 0.39492417\n",
      "Iteration 241, loss = 0.39421387\n",
      "Iteration 242, loss = 0.39350691\n",
      "Iteration 243, loss = 0.39280333\n",
      "Iteration 244, loss = 0.39210305\n",
      "Iteration 245, loss = 0.39140603\n",
      "Iteration 246, loss = 0.39071249\n",
      "Iteration 247, loss = 0.39002200\n",
      "Iteration 248, loss = 0.38933491\n",
      "Iteration 249, loss = 0.38865103\n",
      "Iteration 250, loss = 0.38797033\n",
      "Iteration 251, loss = 0.38729273\n",
      "Iteration 252, loss = 0.38661824\n",
      "Iteration 253, loss = 0.38594668\n",
      "Iteration 254, loss = 0.38527812\n",
      "Iteration 255, loss = 0.38461257\n",
      "Iteration 256, loss = 0.38394997\n",
      "Iteration 257, loss = 0.38329021\n",
      "Iteration 258, loss = 0.38263336\n",
      "Iteration 259, loss = 0.38197929\n",
      "Iteration 260, loss = 0.38132798\n",
      "Iteration 261, loss = 0.38067949\n",
      "Iteration 262, loss = 0.38003364\n",
      "Iteration 263, loss = 0.37939061\n",
      "Iteration 264, loss = 0.37875015\n",
      "Iteration 265, loss = 0.37811236\n",
      "Iteration 266, loss = 0.37747717\n",
      "Iteration 267, loss = 0.37684461\n",
      "Iteration 268, loss = 0.37621458\n",
      "Iteration 269, loss = 0.37558719\n",
      "Iteration 270, loss = 0.37496230\n",
      "Iteration 271, loss = 0.37433990\n",
      "Iteration 272, loss = 0.37371996\n",
      "Iteration 273, loss = 0.37310246\n",
      "Iteration 274, loss = 0.37248740\n",
      "Iteration 275, loss = 0.37187469\n",
      "Iteration 276, loss = 0.37126440\n",
      "Iteration 277, loss = 0.37065644\n",
      "Iteration 278, loss = 0.37005085\n",
      "Iteration 279, loss = 0.36944757\n",
      "Iteration 280, loss = 0.36884654\n",
      "Iteration 281, loss = 0.36824785\n",
      "Iteration 282, loss = 0.36765200\n",
      "Iteration 283, loss = 0.36705841\n",
      "Iteration 284, loss = 0.36646722\n",
      "Iteration 285, loss = 0.36587835\n",
      "Iteration 286, loss = 0.36529174\n",
      "Iteration 287, loss = 0.36470754\n",
      "Iteration 288, loss = 0.36412577\n",
      "Iteration 289, loss = 0.36354620\n",
      "Iteration 290, loss = 0.36296886\n",
      "Iteration 291, loss = 0.36239361\n",
      "Iteration 292, loss = 0.36182052\n",
      "Iteration 293, loss = 0.36124957\n",
      "Iteration 294, loss = 0.36068067\n",
      "Iteration 295, loss = 0.36011416\n",
      "Iteration 296, loss = 0.35954955\n",
      "Iteration 297, loss = 0.35898685\n",
      "Iteration 298, loss = 0.35842610\n",
      "Iteration 299, loss = 0.35786733\n",
      "Iteration 300, loss = 0.35731059\n",
      "Iteration 301, loss = 0.35675588\n",
      "Iteration 302, loss = 0.35620303\n",
      "Iteration 303, loss = 0.35565224\n",
      "Iteration 304, loss = 0.35510321\n",
      "Iteration 305, loss = 0.35455611\n",
      "Iteration 306, loss = 0.35401084\n",
      "Iteration 307, loss = 0.35346749\n",
      "Iteration 308, loss = 0.35292593\n",
      "Iteration 309, loss = 0.35238621\n",
      "Iteration 310, loss = 0.35184830\n",
      "Iteration 311, loss = 0.35131218\n",
      "Iteration 312, loss = 0.35077790\n",
      "Iteration 313, loss = 0.35024534\n",
      "Iteration 314, loss = 0.34971453\n",
      "Iteration 315, loss = 0.34918547\n",
      "Iteration 316, loss = 0.34865828\n",
      "Iteration 317, loss = 0.34813306\n",
      "Iteration 318, loss = 0.34760958\n",
      "Iteration 319, loss = 0.34708787\n",
      "Iteration 320, loss = 0.34656814\n",
      "Iteration 321, loss = 0.34605004\n",
      "Iteration 322, loss = 0.34553366\n",
      "Iteration 323, loss = 0.34501913\n",
      "Iteration 324, loss = 0.34450633\n",
      "Iteration 325, loss = 0.34399549\n",
      "Iteration 326, loss = 0.34348629\n",
      "Iteration 327, loss = 0.34297872\n",
      "Iteration 328, loss = 0.34247298\n",
      "Iteration 329, loss = 0.34196888\n",
      "Iteration 330, loss = 0.34146633\n",
      "Iteration 331, loss = 0.34096542\n",
      "Iteration 332, loss = 0.34046610\n",
      "Iteration 333, loss = 0.33996832\n",
      "Iteration 334, loss = 0.33947210\n",
      "Iteration 335, loss = 0.33897763\n",
      "Iteration 336, loss = 0.33848469\n",
      "Iteration 337, loss = 0.33799335\n",
      "Iteration 338, loss = 0.33750371\n",
      "Iteration 339, loss = 0.33701555\n",
      "Iteration 340, loss = 0.33652887\n",
      "Iteration 341, loss = 0.33604375\n",
      "Iteration 342, loss = 0.33556007\n",
      "Iteration 343, loss = 0.33507786\n",
      "Iteration 344, loss = 0.33459726\n",
      "Iteration 345, loss = 0.33411820\n",
      "Iteration 346, loss = 0.33364056\n",
      "Iteration 347, loss = 0.33316436\n",
      "Iteration 348, loss = 0.33268959\n",
      "Iteration 349, loss = 0.33221637\n",
      "Iteration 350, loss = 0.33174451\n",
      "Iteration 351, loss = 0.33127407\n",
      "Iteration 352, loss = 0.33080502\n",
      "Iteration 353, loss = 0.33033734\n",
      "Iteration 354, loss = 0.32987101\n",
      "Iteration 355, loss = 0.32940603\n",
      "Iteration 356, loss = 0.32894239\n",
      "Iteration 357, loss = 0.32848010\n",
      "Iteration 358, loss = 0.32801944\n",
      "Iteration 359, loss = 0.32755994\n",
      "Iteration 360, loss = 0.32710188\n",
      "Iteration 361, loss = 0.32664512\n",
      "Iteration 362, loss = 0.32618967\n",
      "Iteration 363, loss = 0.32573550\n",
      "Iteration 364, loss = 0.32528267\n",
      "Iteration 365, loss = 0.32483107\n",
      "Iteration 366, loss = 0.32438076\n",
      "Iteration 367, loss = 0.32393171\n",
      "Iteration 368, loss = 0.32348410\n",
      "Iteration 369, loss = 0.32303773\n",
      "Iteration 370, loss = 0.32259261\n",
      "Iteration 371, loss = 0.32214872\n",
      "Iteration 372, loss = 0.32170607\n",
      "Iteration 373, loss = 0.32126480\n",
      "Iteration 374, loss = 0.32082488\n",
      "Iteration 375, loss = 0.32038606\n",
      "Iteration 376, loss = 0.31994854\n",
      "Iteration 377, loss = 0.31951217\n",
      "Iteration 378, loss = 0.31907703\n",
      "Iteration 379, loss = 0.31864307\n",
      "Iteration 380, loss = 0.31821031\n",
      "Iteration 381, loss = 0.31777870\n",
      "Iteration 382, loss = 0.31734825\n",
      "Iteration 383, loss = 0.31691899\n",
      "Iteration 384, loss = 0.31649086\n",
      "Iteration 385, loss = 0.31606392\n",
      "Iteration 386, loss = 0.31563813\n",
      "Iteration 387, loss = 0.31521344\n",
      "Iteration 388, loss = 0.31478990\n",
      "Iteration 389, loss = 0.31436754\n",
      "Iteration 390, loss = 0.31394625\n",
      "Iteration 391, loss = 0.31352608\n",
      "Iteration 392, loss = 0.31310717\n",
      "Iteration 393, loss = 0.31268936\n",
      "Iteration 394, loss = 0.31227270\n",
      "Iteration 395, loss = 0.31185721\n",
      "Iteration 396, loss = 0.31144294\n",
      "Iteration 397, loss = 0.31102976\n",
      "Iteration 398, loss = 0.31061779\n",
      "Iteration 399, loss = 0.31020675\n",
      "Iteration 400, loss = 0.30979690\n",
      "Iteration 401, loss = 0.30938809\n",
      "Iteration 402, loss = 0.30898033\n",
      "Iteration 403, loss = 0.30857371\n",
      "Iteration 404, loss = 0.30816806\n",
      "Iteration 405, loss = 0.30776357\n",
      "Iteration 406, loss = 0.30736014\n",
      "Iteration 407, loss = 0.30695779\n",
      "Iteration 408, loss = 0.30655657\n",
      "Iteration 409, loss = 0.30615631\n",
      "Iteration 410, loss = 0.30575712\n",
      "Iteration 411, loss = 0.30535894\n",
      "Iteration 412, loss = 0.30496182\n",
      "Iteration 413, loss = 0.30456568\n",
      "Iteration 414, loss = 0.30417063\n",
      "Iteration 415, loss = 0.30377650\n",
      "Iteration 416, loss = 0.30338344\n",
      "Iteration 417, loss = 0.30299135\n",
      "Iteration 418, loss = 0.30260026\n",
      "Iteration 419, loss = 0.30221019\n",
      "Iteration 420, loss = 0.30182106\n",
      "Iteration 421, loss = 0.30143298\n",
      "Iteration 422, loss = 0.30104583\n",
      "Iteration 423, loss = 0.30065970\n",
      "Iteration 424, loss = 0.30027459\n",
      "Iteration 425, loss = 0.29989054\n",
      "Iteration 426, loss = 0.29950748\n",
      "Iteration 427, loss = 0.29912547\n",
      "Iteration 428, loss = 0.29874439\n",
      "Iteration 429, loss = 0.29836432\n",
      "Iteration 430, loss = 0.29798515\n",
      "Iteration 431, loss = 0.29760700\n",
      "Iteration 432, loss = 0.29722975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 433, loss = 0.29685345\n",
      "Iteration 434, loss = 0.29647810\n",
      "Iteration 435, loss = 0.29610369\n",
      "Iteration 436, loss = 0.29573021\n",
      "Iteration 437, loss = 0.29535765\n",
      "Iteration 438, loss = 0.29498604\n",
      "Iteration 439, loss = 0.29461532\n",
      "Iteration 440, loss = 0.29424556\n",
      "Iteration 441, loss = 0.29387666\n",
      "Iteration 442, loss = 0.29350871\n",
      "Iteration 443, loss = 0.29314162\n",
      "Iteration 444, loss = 0.29277551\n",
      "Iteration 445, loss = 0.29241025\n",
      "Iteration 446, loss = 0.29204590\n",
      "Iteration 447, loss = 0.29168242\n",
      "Iteration 448, loss = 0.29131985\n",
      "Iteration 449, loss = 0.29095818\n",
      "Iteration 450, loss = 0.29059736\n",
      "Iteration 451, loss = 0.29023745\n",
      "Iteration 452, loss = 0.28987840\n",
      "Iteration 453, loss = 0.28952023\n",
      "Iteration 454, loss = 0.28916294\n",
      "Iteration 455, loss = 0.28880650\n",
      "Iteration 456, loss = 0.28845096\n",
      "Iteration 457, loss = 0.28809626\n",
      "Iteration 458, loss = 0.28774243\n",
      "Iteration 459, loss = 0.28738947\n",
      "Iteration 460, loss = 0.28703734\n",
      "Iteration 461, loss = 0.28668610\n",
      "Iteration 462, loss = 0.28633566\n",
      "Iteration 463, loss = 0.28598609\n",
      "Iteration 464, loss = 0.28563738\n",
      "Iteration 465, loss = 0.28528949\n",
      "Iteration 466, loss = 0.28494243\n",
      "Iteration 467, loss = 0.28459623\n",
      "Iteration 468, loss = 0.28425084\n",
      "Iteration 469, loss = 0.28390630\n",
      "Iteration 470, loss = 0.28356257\n",
      "Iteration 471, loss = 0.28321966\n",
      "Iteration 472, loss = 0.28287762\n",
      "Iteration 473, loss = 0.28253636\n",
      "Iteration 474, loss = 0.28219590\n",
      "Iteration 475, loss = 0.28185631\n",
      "Iteration 476, loss = 0.28151747\n",
      "Iteration 477, loss = 0.28117949\n",
      "Iteration 478, loss = 0.28084232\n",
      "Iteration 479, loss = 0.28050595\n",
      "Iteration 480, loss = 0.28017039\n",
      "Iteration 481, loss = 0.27983565\n",
      "Iteration 482, loss = 0.27950170\n",
      "Iteration 483, loss = 0.27916854\n",
      "Iteration 484, loss = 0.27883625\n",
      "Iteration 485, loss = 0.27850465\n",
      "Iteration 486, loss = 0.27817393\n",
      "Iteration 487, loss = 0.27784396\n",
      "Iteration 488, loss = 0.27751478\n",
      "Iteration 489, loss = 0.27718635\n",
      "Iteration 490, loss = 0.27685872\n",
      "Iteration 491, loss = 0.27653188\n",
      "Iteration 492, loss = 0.27620580\n",
      "Iteration 493, loss = 0.27588052\n",
      "Iteration 494, loss = 0.27555599\n",
      "Iteration 495, loss = 0.27523224\n",
      "Iteration 496, loss = 0.27490928\n",
      "Iteration 497, loss = 0.27458706\n",
      "Iteration 498, loss = 0.27426560\n",
      "Iteration 499, loss = 0.27394490\n",
      "Iteration 500, loss = 0.27362499\n",
      "Iteration 501, loss = 0.27330580\n",
      "Iteration 502, loss = 0.27298736\n",
      "Iteration 503, loss = 0.27266968\n",
      "Iteration 504, loss = 0.27235276\n",
      "Iteration 505, loss = 0.27203660\n",
      "Iteration 506, loss = 0.27172116\n",
      "Iteration 507, loss = 0.27140652\n",
      "Iteration 508, loss = 0.27109255\n",
      "Iteration 509, loss = 0.27077935\n",
      "Iteration 510, loss = 0.27046692\n",
      "Iteration 511, loss = 0.27015519\n",
      "Iteration 512, loss = 0.26984419\n",
      "Iteration 513, loss = 0.26953392\n",
      "Iteration 514, loss = 0.26922437\n",
      "Iteration 515, loss = 0.26891558\n",
      "Iteration 516, loss = 0.26860750\n",
      "Iteration 517, loss = 0.26830021\n",
      "Iteration 518, loss = 0.26799371\n",
      "Iteration 519, loss = 0.26768790\n",
      "Iteration 520, loss = 0.26738290\n",
      "Iteration 521, loss = 0.26707863\n",
      "Iteration 522, loss = 0.26677505\n",
      "Iteration 523, loss = 0.26647220\n",
      "Iteration 524, loss = 0.26617005\n",
      "Iteration 525, loss = 0.26586861\n",
      "Iteration 526, loss = 0.26556791\n",
      "Iteration 527, loss = 0.26526790\n",
      "Iteration 528, loss = 0.26496862\n",
      "Iteration 529, loss = 0.26466999\n",
      "Iteration 530, loss = 0.26437208\n",
      "Iteration 531, loss = 0.26407490\n",
      "Iteration 532, loss = 0.26377837\n",
      "Iteration 533, loss = 0.26348254\n",
      "Iteration 534, loss = 0.26318740\n",
      "Iteration 535, loss = 0.26289299\n",
      "Iteration 536, loss = 0.26259925\n",
      "Iteration 537, loss = 0.26230623\n",
      "Iteration 538, loss = 0.26201394\n",
      "Iteration 539, loss = 0.26172236\n",
      "Iteration 540, loss = 0.26143148\n",
      "Iteration 541, loss = 0.26114133\n",
      "Iteration 542, loss = 0.26085187\n",
      "Iteration 543, loss = 0.26056309\n",
      "Iteration 544, loss = 0.26027497\n",
      "Iteration 545, loss = 0.25998753\n",
      "Iteration 546, loss = 0.25970077\n",
      "Iteration 547, loss = 0.25941467\n",
      "Iteration 548, loss = 0.25912923\n",
      "Iteration 549, loss = 0.25884447\n",
      "Iteration 550, loss = 0.25856036\n",
      "Iteration 551, loss = 0.25827691\n",
      "Iteration 552, loss = 0.25799411\n",
      "Iteration 553, loss = 0.25771196\n",
      "Iteration 554, loss = 0.25743050\n",
      "Iteration 555, loss = 0.25714965\n",
      "Iteration 556, loss = 0.25686946\n",
      "Iteration 557, loss = 0.25658991\n",
      "Iteration 558, loss = 0.25631103\n",
      "Iteration 559, loss = 0.25603278\n",
      "Iteration 560, loss = 0.25575529\n",
      "Iteration 561, loss = 0.25547848\n",
      "Iteration 562, loss = 0.25520232\n",
      "Iteration 563, loss = 0.25492681\n",
      "Iteration 564, loss = 0.25465193\n",
      "Iteration 565, loss = 0.25437768\n",
      "Iteration 566, loss = 0.25410410\n",
      "Iteration 567, loss = 0.25383111\n",
      "Iteration 568, loss = 0.25355876\n",
      "Iteration 569, loss = 0.25328705\n",
      "Iteration 570, loss = 0.25301597\n",
      "Iteration 571, loss = 0.25274553\n",
      "Iteration 572, loss = 0.25247566\n",
      "Iteration 573, loss = 0.25220643\n",
      "Iteration 574, loss = 0.25193783\n",
      "Iteration 575, loss = 0.25166984\n",
      "Iteration 576, loss = 0.25140246\n",
      "Iteration 577, loss = 0.25113569\n",
      "Iteration 578, loss = 0.25086956\n",
      "Iteration 579, loss = 0.25060400\n",
      "Iteration 580, loss = 0.25033908\n",
      "Iteration 581, loss = 0.25007473\n",
      "Iteration 582, loss = 0.24981102\n",
      "Iteration 583, loss = 0.24954789\n",
      "Iteration 584, loss = 0.24928536\n",
      "Iteration 585, loss = 0.24902343\n",
      "Iteration 586, loss = 0.24876213\n",
      "Iteration 587, loss = 0.24850141\n",
      "Iteration 588, loss = 0.24824128\n",
      "Iteration 589, loss = 0.24798176\n",
      "Iteration 590, loss = 0.24772283\n",
      "Iteration 591, loss = 0.24746448\n",
      "Iteration 592, loss = 0.24720672\n",
      "Iteration 593, loss = 0.24694955\n",
      "Iteration 594, loss = 0.24669295\n",
      "Iteration 595, loss = 0.24643696\n",
      "Iteration 596, loss = 0.24618154\n",
      "Iteration 597, loss = 0.24592671\n",
      "Iteration 598, loss = 0.24567249\n",
      "Iteration 599, loss = 0.24541886\n",
      "Iteration 600, loss = 0.24516579\n",
      "Iteration 601, loss = 0.24491330\n",
      "Iteration 602, loss = 0.24466138\n",
      "Iteration 603, loss = 0.24441006\n",
      "Iteration 604, loss = 0.24415928\n",
      "Iteration 605, loss = 0.24390908\n",
      "Iteration 606, loss = 0.24365944\n",
      "Iteration 607, loss = 0.24341040\n",
      "Iteration 608, loss = 0.24316188\n",
      "Iteration 609, loss = 0.24291394\n",
      "Iteration 610, loss = 0.24266656\n",
      "Iteration 611, loss = 0.24241975\n",
      "Iteration 612, loss = 0.24217354\n",
      "Iteration 613, loss = 0.24192787\n",
      "Iteration 614, loss = 0.24168278\n",
      "Iteration 615, loss = 0.24143826\n",
      "Iteration 616, loss = 0.24119432\n",
      "Iteration 617, loss = 0.24095089\n",
      "Iteration 618, loss = 0.24070804\n",
      "Iteration 619, loss = 0.24046572\n",
      "Iteration 620, loss = 0.24022398\n",
      "Iteration 621, loss = 0.23998276\n",
      "Iteration 622, loss = 0.23974209\n",
      "Iteration 623, loss = 0.23950196\n",
      "Iteration 624, loss = 0.23926236\n",
      "Iteration 625, loss = 0.23902335\n",
      "Iteration 626, loss = 0.23878481\n",
      "Iteration 627, loss = 0.23854684\n",
      "Iteration 628, loss = 0.23830942\n",
      "Iteration 629, loss = 0.23807251\n",
      "Iteration 630, loss = 0.23783614\n",
      "Iteration 631, loss = 0.23760031\n",
      "Iteration 632, loss = 0.23736499\n",
      "Iteration 633, loss = 0.23713023\n",
      "Iteration 634, loss = 0.23689596\n",
      "Iteration 635, loss = 0.23666223\n",
      "Iteration 636, loss = 0.23642903\n",
      "Iteration 637, loss = 0.23619637\n",
      "Iteration 638, loss = 0.23596421\n",
      "Iteration 639, loss = 0.23573257\n",
      "Iteration 640, loss = 0.23550144\n",
      "Iteration 641, loss = 0.23527085\n",
      "Iteration 642, loss = 0.23504076\n",
      "Iteration 643, loss = 0.23481120\n",
      "Iteration 644, loss = 0.23458213\n",
      "Iteration 645, loss = 0.23435358\n",
      "Iteration 646, loss = 0.23412557\n",
      "Iteration 647, loss = 0.23389802\n",
      "Iteration 648, loss = 0.23367101\n",
      "Iteration 649, loss = 0.23344451\n",
      "Iteration 650, loss = 0.23321851\n",
      "Iteration 651, loss = 0.23299300\n",
      "Iteration 652, loss = 0.23276800\n",
      "Iteration 653, loss = 0.23254350\n",
      "Iteration 654, loss = 0.23231951\n",
      "Iteration 655, loss = 0.23209603\n",
      "Iteration 656, loss = 0.23187304\n",
      "Iteration 657, loss = 0.23165055\n",
      "Iteration 658, loss = 0.23142855\n",
      "Iteration 659, loss = 0.23120706\n",
      "Iteration 660, loss = 0.23098606\n",
      "Iteration 661, loss = 0.23076554\n",
      "Iteration 662, loss = 0.23054550\n",
      "Iteration 663, loss = 0.23032599\n",
      "Iteration 664, loss = 0.23010693\n",
      "Iteration 665, loss = 0.22988837\n",
      "Iteration 666, loss = 0.22967030\n",
      "Iteration 667, loss = 0.22945271\n",
      "Iteration 668, loss = 0.22923561\n",
      "Iteration 669, loss = 0.22901898\n",
      "Iteration 670, loss = 0.22880284\n",
      "Iteration 671, loss = 0.22858718\n",
      "Iteration 672, loss = 0.22837200\n",
      "Iteration 673, loss = 0.22815730\n",
      "Iteration 674, loss = 0.22794306\n",
      "Iteration 675, loss = 0.22772932\n",
      "Iteration 676, loss = 0.22751603\n",
      "Iteration 677, loss = 0.22730322\n",
      "Iteration 678, loss = 0.22709088\n",
      "Iteration 679, loss = 0.22687902\n",
      "Iteration 680, loss = 0.22666762\n",
      "Iteration 681, loss = 0.22645669\n",
      "Iteration 682, loss = 0.22624622\n",
      "Iteration 683, loss = 0.22603621\n",
      "Iteration 684, loss = 0.22582668\n",
      "Iteration 685, loss = 0.22561760\n",
      "Iteration 686, loss = 0.22540900\n",
      "Iteration 687, loss = 0.22520083\n",
      "Iteration 688, loss = 0.22499315\n",
      "Iteration 689, loss = 0.22478590\n",
      "Iteration 690, loss = 0.22457912\n",
      "Iteration 691, loss = 0.22437280\n",
      "Iteration 692, loss = 0.22416693\n",
      "Iteration 693, loss = 0.22396150\n",
      "Iteration 694, loss = 0.22375654\n",
      "Iteration 695, loss = 0.22355202\n",
      "Iteration 696, loss = 0.22334796\n",
      "Iteration 697, loss = 0.22314433\n",
      "Iteration 698, loss = 0.22294117\n",
      "Iteration 699, loss = 0.22273844\n",
      "Iteration 700, loss = 0.22253619\n",
      "Iteration 701, loss = 0.22233437\n",
      "Iteration 702, loss = 0.22213300\n",
      "Iteration 703, loss = 0.22193206\n",
      "Iteration 704, loss = 0.22173158\n",
      "Iteration 705, loss = 0.22153152\n",
      "Iteration 706, loss = 0.22133192\n",
      "Iteration 707, loss = 0.22113273\n",
      "Iteration 708, loss = 0.22093401\n",
      "Iteration 709, loss = 0.22073570\n",
      "Iteration 710, loss = 0.22053783\n",
      "Iteration 711, loss = 0.22034040\n",
      "Iteration 712, loss = 0.22014340\n",
      "Iteration 713, loss = 0.21994684\n",
      "Iteration 714, loss = 0.21975069\n",
      "Iteration 715, loss = 0.21955497\n",
      "Iteration 716, loss = 0.21935969\n",
      "Iteration 717, loss = 0.21916483\n",
      "Iteration 718, loss = 0.21897040\n",
      "Iteration 719, loss = 0.21877638\n",
      "Iteration 720, loss = 0.21858280\n",
      "Iteration 721, loss = 0.21838964\n",
      "Iteration 722, loss = 0.21819690\n",
      "Iteration 723, loss = 0.21800456\n",
      "Iteration 724, loss = 0.21781268\n",
      "Iteration 725, loss = 0.21762118\n",
      "Iteration 726, loss = 0.21743010\n",
      "Iteration 727, loss = 0.21723946\n",
      "Iteration 728, loss = 0.21704922\n",
      "Iteration 729, loss = 0.21685939\n",
      "Iteration 730, loss = 0.21666997\n",
      "Iteration 731, loss = 0.21648098\n",
      "Iteration 732, loss = 0.21629238\n",
      "Iteration 733, loss = 0.21610420\n",
      "Iteration 734, loss = 0.21591643\n",
      "Iteration 735, loss = 0.21572907\n",
      "Iteration 736, loss = 0.21554210\n",
      "Iteration 737, loss = 0.21535555\n",
      "Iteration 738, loss = 0.21516939\n",
      "Iteration 739, loss = 0.21498365\n",
      "Iteration 740, loss = 0.21479830\n",
      "Iteration 741, loss = 0.21461335\n",
      "Iteration 742, loss = 0.21442881\n",
      "Iteration 743, loss = 0.21424467\n",
      "Iteration 744, loss = 0.21406092\n",
      "Iteration 745, loss = 0.21387756\n",
      "Iteration 746, loss = 0.21369461\n",
      "Iteration 747, loss = 0.21351205\n",
      "Iteration 748, loss = 0.21332988\n",
      "Iteration 749, loss = 0.21314810\n",
      "Iteration 750, loss = 0.21296674\n",
      "Iteration 751, loss = 0.21278574\n",
      "Iteration 752, loss = 0.21260513\n",
      "Iteration 753, loss = 0.21242493\n",
      "Iteration 754, loss = 0.21224510\n",
      "Iteration 755, loss = 0.21206567\n",
      "Iteration 756, loss = 0.21188661\n",
      "Iteration 757, loss = 0.21170795\n",
      "Iteration 758, loss = 0.21152967\n",
      "Iteration 759, loss = 0.21135177\n",
      "Iteration 760, loss = 0.21117425\n",
      "Iteration 761, loss = 0.21099713\n",
      "Iteration 762, loss = 0.21082037\n",
      "Iteration 763, loss = 0.21064399\n",
      "Iteration 764, loss = 0.21046799\n",
      "Iteration 765, loss = 0.21029238\n",
      "Iteration 766, loss = 0.21011713\n",
      "Iteration 767, loss = 0.20994227\n",
      "Iteration 768, loss = 0.20976778\n",
      "Iteration 769, loss = 0.20959365\n",
      "Iteration 770, loss = 0.20941990\n",
      "Iteration 771, loss = 0.20924654\n",
      "Iteration 772, loss = 0.20907353\n",
      "Iteration 773, loss = 0.20890090\n",
      "Iteration 774, loss = 0.20872862\n",
      "Iteration 775, loss = 0.20855673\n",
      "Iteration 776, loss = 0.20838519\n",
      "Iteration 777, loss = 0.20821402\n",
      "Iteration 778, loss = 0.20804323\n",
      "Iteration 779, loss = 0.20787279\n",
      "Iteration 780, loss = 0.20770271\n",
      "Iteration 781, loss = 0.20753301\n",
      "Iteration 782, loss = 0.20736366\n",
      "Iteration 783, loss = 0.20719466\n",
      "Iteration 784, loss = 0.20702603\n",
      "Iteration 785, loss = 0.20685777\n",
      "Iteration 786, loss = 0.20668986\n",
      "Iteration 787, loss = 0.20652229\n",
      "Iteration 788, loss = 0.20635510\n",
      "Iteration 789, loss = 0.20618825\n",
      "Iteration 790, loss = 0.20602175\n",
      "Iteration 791, loss = 0.20585563\n",
      "Iteration 792, loss = 0.20568985\n",
      "Iteration 793, loss = 0.20552441\n",
      "Iteration 794, loss = 0.20535934\n",
      "Iteration 795, loss = 0.20519462\n",
      "Iteration 796, loss = 0.20503024\n",
      "Iteration 797, loss = 0.20486620\n",
      "Iteration 798, loss = 0.20470252\n",
      "Iteration 799, loss = 0.20453919\n",
      "Iteration 800, loss = 0.20437619\n",
      "Iteration 801, loss = 0.20421356\n",
      "Iteration 802, loss = 0.20405124\n",
      "Iteration 803, loss = 0.20388928\n",
      "Iteration 804, loss = 0.20372767\n",
      "Iteration 805, loss = 0.20356639\n",
      "Iteration 806, loss = 0.20340545\n",
      "Iteration 807, loss = 0.20324484\n",
      "Iteration 808, loss = 0.20308460\n",
      "Iteration 809, loss = 0.20292466\n",
      "Iteration 810, loss = 0.20276509\n",
      "Iteration 811, loss = 0.20260584\n",
      "Iteration 812, loss = 0.20244691\n",
      "Iteration 813, loss = 0.20228834\n",
      "Iteration 814, loss = 0.20213010\n",
      "Iteration 815, loss = 0.20197217\n",
      "Iteration 816, loss = 0.20181459\n",
      "Iteration 817, loss = 0.20165734\n",
      "Iteration 818, loss = 0.20150041\n",
      "Iteration 819, loss = 0.20134381\n",
      "Iteration 820, loss = 0.20118756\n",
      "Iteration 821, loss = 0.20103161\n",
      "Iteration 822, loss = 0.20087601\n",
      "Iteration 823, loss = 0.20072071\n",
      "Iteration 824, loss = 0.20056575\n",
      "Iteration 825, loss = 0.20041112\n",
      "Iteration 826, loss = 0.20025680\n",
      "Iteration 827, loss = 0.20010281\n",
      "Iteration 828, loss = 0.19994914\n",
      "Iteration 829, loss = 0.19979579\n",
      "Iteration 830, loss = 0.19964275\n",
      "Iteration 831, loss = 0.19949006\n",
      "Iteration 832, loss = 0.19933766\n",
      "Iteration 833, loss = 0.19918558\n",
      "Iteration 834, loss = 0.19903383\n",
      "Iteration 835, loss = 0.19888239\n",
      "Iteration 836, loss = 0.19873126\n",
      "Iteration 837, loss = 0.19858045\n",
      "Iteration 838, loss = 0.19842994\n",
      "Iteration 839, loss = 0.19827976\n",
      "Iteration 840, loss = 0.19812989\n",
      "Iteration 841, loss = 0.19798033\n",
      "Iteration 842, loss = 0.19783107\n",
      "Iteration 843, loss = 0.19768214\n",
      "Iteration 844, loss = 0.19753350\n",
      "Iteration 845, loss = 0.19738517\n",
      "Iteration 846, loss = 0.19723716\n",
      "Iteration 847, loss = 0.19708944\n",
      "Iteration 848, loss = 0.19694204\n",
      "Iteration 849, loss = 0.19679493\n",
      "Iteration 850, loss = 0.19664814\n",
      "Iteration 851, loss = 0.19650164\n",
      "Iteration 852, loss = 0.19635545\n",
      "Iteration 853, loss = 0.19620957\n",
      "Iteration 854, loss = 0.19606398\n",
      "Iteration 855, loss = 0.19591869\n",
      "Iteration 856, loss = 0.19577371\n",
      "Iteration 857, loss = 0.19562901\n",
      "Iteration 858, loss = 0.19548463\n",
      "Iteration 859, loss = 0.19534053\n",
      "Iteration 860, loss = 0.19519673\n",
      "Iteration 861, loss = 0.19505324\n",
      "Iteration 862, loss = 0.19491003\n",
      "Iteration 863, loss = 0.19476712\n",
      "Iteration 864, loss = 0.19462450\n",
      "Iteration 865, loss = 0.19448218\n",
      "Iteration 866, loss = 0.19434014\n",
      "Iteration 867, loss = 0.19419841\n",
      "Iteration 868, loss = 0.19405695\n",
      "Iteration 869, loss = 0.19391580\n",
      "Iteration 870, loss = 0.19377492\n",
      "Iteration 871, loss = 0.19363434\n",
      "Iteration 872, loss = 0.19349405\n",
      "Iteration 873, loss = 0.19335404\n",
      "Iteration 874, loss = 0.19321432\n",
      "Iteration 875, loss = 0.19307488\n",
      "Iteration 876, loss = 0.19293573\n",
      "Iteration 877, loss = 0.19279686\n",
      "Iteration 878, loss = 0.19265828\n",
      "Iteration 879, loss = 0.19251999\n",
      "Iteration 880, loss = 0.19238196\n",
      "Iteration 881, loss = 0.19224423\n",
      "Iteration 882, loss = 0.19210677\n",
      "Iteration 883, loss = 0.19196959\n",
      "Iteration 884, loss = 0.19183270\n",
      "Iteration 885, loss = 0.19169608\n",
      "Iteration 886, loss = 0.19155974\n",
      "Iteration 887, loss = 0.19142368\n",
      "Iteration 888, loss = 0.19128789\n",
      "Iteration 889, loss = 0.19115239\n",
      "Iteration 890, loss = 0.19101714\n",
      "Iteration 891, loss = 0.19088219\n",
      "Iteration 892, loss = 0.19074750\n",
      "Iteration 893, loss = 0.19061307\n",
      "Iteration 894, loss = 0.19047895\n",
      "Iteration 895, loss = 0.19034507\n",
      "Iteration 896, loss = 0.19021147\n",
      "Iteration 897, loss = 0.19007813\n",
      "Iteration 898, loss = 0.18994508\n",
      "Iteration 899, loss = 0.18981228\n",
      "Iteration 900, loss = 0.18967976\n",
      "Iteration 901, loss = 0.18954751\n",
      "Iteration 902, loss = 0.18941551\n",
      "Iteration 903, loss = 0.18928381\n",
      "Iteration 904, loss = 0.18915234\n",
      "Iteration 905, loss = 0.18902116\n",
      "Iteration 906, loss = 0.18889022\n",
      "Iteration 907, loss = 0.18875955\n",
      "Iteration 908, loss = 0.18862917\n",
      "Iteration 909, loss = 0.18849902\n",
      "Iteration 910, loss = 0.18836914\n",
      "Iteration 911, loss = 0.18823953\n",
      "Iteration 912, loss = 0.18811018\n",
      "Iteration 913, loss = 0.18798108\n",
      "Iteration 914, loss = 0.18785224\n",
      "Iteration 915, loss = 0.18772366\n",
      "Iteration 916, loss = 0.18759534\n",
      "Iteration 917, loss = 0.18746728\n",
      "Iteration 918, loss = 0.18733946\n",
      "Iteration 919, loss = 0.18721192\n",
      "Iteration 920, loss = 0.18708462\n",
      "Iteration 921, loss = 0.18695758\n",
      "Iteration 922, loss = 0.18683079\n",
      "Iteration 923, loss = 0.18670426\n",
      "Iteration 924, loss = 0.18657798\n",
      "Iteration 925, loss = 0.18645195\n",
      "Iteration 926, loss = 0.18632618\n",
      "Iteration 927, loss = 0.18620065\n",
      "Iteration 928, loss = 0.18607537\n",
      "Iteration 929, loss = 0.18595035\n",
      "Iteration 930, loss = 0.18582557\n",
      "Iteration 931, loss = 0.18570104\n",
      "Iteration 932, loss = 0.18557677\n",
      "Iteration 933, loss = 0.18545272\n",
      "Iteration 934, loss = 0.18532895\n",
      "Iteration 935, loss = 0.18520540\n",
      "Iteration 936, loss = 0.18508211\n",
      "Iteration 937, loss = 0.18495905\n",
      "Iteration 938, loss = 0.18483626\n",
      "Iteration 939, loss = 0.18471369\n",
      "Iteration 940, loss = 0.18459138\n",
      "Iteration 941, loss = 0.18446930\n",
      "Iteration 942, loss = 0.18434748\n",
      "Iteration 943, loss = 0.18422588\n",
      "Iteration 944, loss = 0.18410453\n",
      "Iteration 945, loss = 0.18398341\n",
      "Iteration 946, loss = 0.18386255\n",
      "Iteration 947, loss = 0.18374191\n",
      "Iteration 948, loss = 0.18362154\n",
      "Iteration 949, loss = 0.18350137\n",
      "Iteration 950, loss = 0.18338146\n",
      "Iteration 951, loss = 0.18326177\n",
      "Iteration 952, loss = 0.18314233\n",
      "Iteration 953, loss = 0.18302312\n",
      "Iteration 954, loss = 0.18290416\n",
      "Iteration 955, loss = 0.18278541\n",
      "Iteration 956, loss = 0.18266691\n",
      "Iteration 957, loss = 0.18254864\n",
      "Iteration 958, loss = 0.18243060\n",
      "Iteration 959, loss = 0.18231279\n",
      "Iteration 960, loss = 0.18219522\n",
      "Iteration 961, loss = 0.18207787\n",
      "Iteration 962, loss = 0.18196076\n",
      "Iteration 963, loss = 0.18184387\n",
      "Iteration 964, loss = 0.18172722\n",
      "Iteration 965, loss = 0.18161078\n",
      "Iteration 966, loss = 0.18149461\n",
      "Iteration 967, loss = 0.18137862\n",
      "Iteration 968, loss = 0.18126287\n",
      "Iteration 969, loss = 0.18114735\n",
      "Iteration 970, loss = 0.18103206\n",
      "Iteration 971, loss = 0.18091702\n",
      "Iteration 972, loss = 0.18080217\n",
      "Iteration 973, loss = 0.18068756\n",
      "Iteration 974, loss = 0.18057317\n",
      "Iteration 975, loss = 0.18045901\n",
      "Iteration 976, loss = 0.18034509\n",
      "Iteration 977, loss = 0.18023137\n",
      "Iteration 978, loss = 0.18011787\n",
      "Iteration 979, loss = 0.18000462\n",
      "Iteration 980, loss = 0.17989156\n",
      "Iteration 981, loss = 0.17977872\n",
      "Iteration 982, loss = 0.17966613\n",
      "Iteration 983, loss = 0.17955372\n",
      "Iteration 984, loss = 0.17944156\n",
      "Iteration 985, loss = 0.17932960\n",
      "Iteration 986, loss = 0.17921786\n",
      "Iteration 987, loss = 0.17910633\n",
      "Iteration 988, loss = 0.17899505\n",
      "Iteration 989, loss = 0.17888394\n",
      "Iteration 990, loss = 0.17877308\n",
      "Iteration 991, loss = 0.17866243\n",
      "Iteration 992, loss = 0.17855199\n",
      "Iteration 993, loss = 0.17844178\n",
      "Iteration 994, loss = 0.17833177\n",
      "Iteration 995, loss = 0.17822198\n",
      "Iteration 996, loss = 0.17811238\n",
      "Iteration 997, loss = 0.17800301\n",
      "Iteration 998, loss = 0.17789384\n",
      "Iteration 999, loss = 0.17778489\n",
      "Iteration 1000, loss = 0.17767615\n",
      "Iteration 1, loss = 1.48762646\n",
      "Iteration 2, loss = 6.15821746\n",
      "Iteration 3, loss = 1.40627723\n",
      "Iteration 4, loss = 1.06166550\n",
      "Iteration 5, loss = 1.56767691\n",
      "Iteration 6, loss = 0.75594181\n",
      "Iteration 7, loss = 0.91846541\n",
      "Iteration 8, loss = 0.73890705\n",
      "Iteration 9, loss = 0.54608341\n",
      "Iteration 10, loss = 0.50458009\n",
      "Iteration 11, loss = 0.49168241\n",
      "Iteration 12, loss = 0.44375178\n",
      "Iteration 13, loss = 0.38933385\n",
      "Iteration 14, loss = 0.37698816\n",
      "Iteration 15, loss = 0.36896043\n",
      "Iteration 16, loss = 0.32981335\n",
      "Iteration 17, loss = 0.31047318\n",
      "Iteration 18, loss = 0.30934248\n",
      "Iteration 19, loss = 0.26863843\n",
      "Iteration 20, loss = 0.25669295\n",
      "Iteration 21, loss = 0.24491851\n",
      "Iteration 22, loss = 0.21465917\n",
      "Iteration 23, loss = 0.21974319\n",
      "Iteration 24, loss = 0.19065326\n",
      "Iteration 25, loss = 0.18943989\n",
      "Iteration 26, loss = 0.17377333\n",
      "Iteration 27, loss = 0.16478256\n",
      "Iteration 28, loss = 0.16005211\n",
      "Iteration 29, loss = 0.14854019\n",
      "Iteration 30, loss = 0.14920247\n",
      "Iteration 31, loss = 0.13665447\n",
      "Iteration 32, loss = 0.14064984\n",
      "Iteration 33, loss = 0.12921778\n",
      "Iteration 34, loss = 0.13290695\n",
      "Iteration 35, loss = 0.12367661\n",
      "Iteration 36, loss = 0.12751251\n",
      "Iteration 37, loss = 0.11958079\n",
      "Iteration 38, loss = 0.12254848\n",
      "Iteration 39, loss = 0.11654566\n",
      "Iteration 40, loss = 0.11921043\n",
      "Iteration 41, loss = 0.11391183\n",
      "Iteration 42, loss = 0.11601652\n",
      "Iteration 43, loss = 0.11186149\n",
      "Iteration 44, loss = 0.11379045\n",
      "Iteration 45, loss = 0.11006256\n",
      "Iteration 46, loss = 0.11146611\n",
      "Iteration 47, loss = 0.10852553\n",
      "Iteration 48, loss = 0.10968153\n",
      "Iteration 49, loss = 0.10734939\n",
      "Iteration 50, loss = 0.10780924\n",
      "Iteration 51, loss = 0.10622906\n",
      "Iteration 52, loss = 0.10616828\n",
      "Iteration 53, loss = 0.10540428\n",
      "Iteration 54, loss = 0.10470810\n",
      "Iteration 55, loss = 0.10455611\n",
      "Iteration 56, loss = 0.10339459\n",
      "Iteration 57, loss = 0.10362683\n",
      "Iteration 58, loss = 0.10241331\n",
      "Iteration 59, loss = 0.10266429\n",
      "Iteration 60, loss = 0.10167735\n",
      "Iteration 61, loss = 0.10154854\n",
      "Iteration 62, loss = 0.10104500\n",
      "Iteration 63, loss = 0.10048976\n",
      "Iteration 64, loss = 0.10038609\n",
      "Iteration 65, loss = 0.09963153\n",
      "Iteration 66, loss = 0.09960589\n",
      "Iteration 67, loss = 0.09898019\n",
      "Iteration 68, loss = 0.09869535\n",
      "Iteration 69, loss = 0.09840154\n",
      "Iteration 70, loss = 0.09786020\n",
      "Iteration 71, loss = 0.09772382\n",
      "Iteration 72, loss = 0.09721268\n",
      "Iteration 73, loss = 0.09695416\n",
      "Iteration 74, loss = 0.09666244\n",
      "Iteration 75, loss = 0.09622566\n",
      "Iteration 76, loss = 0.09604602\n",
      "Iteration 77, loss = 0.09564558\n",
      "Iteration 78, loss = 0.09534565\n",
      "Iteration 79, loss = 0.09511107\n",
      "Iteration 80, loss = 0.09472272\n",
      "Iteration 81, loss = 0.09449129\n",
      "Iteration 82, loss = 0.09420181\n",
      "Iteration 83, loss = 0.09387021\n",
      "Iteration 84, loss = 0.09364926\n",
      "Iteration 85, loss = 0.09334272\n",
      "Iteration 86, loss = 0.09305873\n",
      "Iteration 87, loss = 0.09283001\n",
      "Iteration 88, loss = 0.09253159\n",
      "Iteration 89, loss = 0.09227594\n",
      "Iteration 90, loss = 0.09204210\n",
      "Iteration 91, loss = 0.09176033\n",
      "Iteration 92, loss = 0.09151996\n",
      "Iteration 93, loss = 0.09128760\n",
      "Iteration 94, loss = 0.09102264\n",
      "Iteration 95, loss = 0.09079092\n",
      "Iteration 96, loss = 0.09056454\n",
      "Iteration 97, loss = 0.09031440\n",
      "Iteration 98, loss = 0.09008823\n",
      "Iteration 99, loss = 0.08986965\n",
      "Iteration 100, loss = 0.08963283\n",
      "Iteration 101, loss = 0.08941088\n",
      "Iteration 102, loss = 0.08919996\n",
      "Iteration 103, loss = 0.08897606\n",
      "Iteration 104, loss = 0.08875841\n",
      "Iteration 105, loss = 0.08855378\n",
      "Iteration 106, loss = 0.08834266\n",
      "Iteration 107, loss = 0.08813074\n",
      "Iteration 108, loss = 0.08793028\n",
      "Iteration 109, loss = 0.08773060\n",
      "Iteration 110, loss = 0.08752683\n",
      "Iteration 111, loss = 0.08732928\n",
      "Iteration 112, loss = 0.08713775\n",
      "Iteration 113, loss = 0.08694403\n",
      "Iteration 114, loss = 0.08675072\n",
      "Iteration 115, loss = 0.08656363\n",
      "Iteration 116, loss = 0.08637907\n",
      "Iteration 117, loss = 0.08619317\n",
      "Iteration 118, loss = 0.08600955\n",
      "Iteration 119, loss = 0.08583068\n",
      "Iteration 120, loss = 0.08565354\n",
      "Iteration 121, loss = 0.08547623\n",
      "Iteration 122, loss = 0.08530114\n",
      "Iteration 123, loss = 0.08512965\n",
      "Iteration 124, loss = 0.08495987\n",
      "Iteration 125, loss = 0.08479052\n",
      "Iteration 126, loss = 0.08462284\n",
      "Iteration 127, loss = 0.08445799\n",
      "Iteration 128, loss = 0.08429520\n",
      "Iteration 129, loss = 0.08413339\n",
      "Iteration 130, loss = 0.08397274\n",
      "Iteration 131, loss = 0.08381414\n",
      "Iteration 132, loss = 0.08365771\n",
      "Iteration 133, loss = 0.08350279\n",
      "Iteration 134, loss = 0.08334894\n",
      "Iteration 135, loss = 0.08319643\n",
      "Iteration 136, loss = 0.08304571\n",
      "Iteration 137, loss = 0.08289679\n",
      "Iteration 138, loss = 0.08274932\n",
      "Iteration 139, loss = 0.08260305\n",
      "Iteration 140, loss = 0.08245800\n",
      "Iteration 141, loss = 0.08231440\n",
      "Iteration 142, loss = 0.08217234\n",
      "Iteration 143, loss = 0.08203176\n",
      "Iteration 144, loss = 0.08189249\n",
      "Iteration 145, loss = 0.08175441\n",
      "Iteration 146, loss = 0.08161759\n",
      "Iteration 147, loss = 0.08148209\n",
      "Iteration 148, loss = 0.08134791\n",
      "Iteration 149, loss = 0.08121505\n",
      "Iteration 150, loss = 0.08108346\n",
      "Iteration 151, loss = 0.08095309\n",
      "Iteration 152, loss = 0.08082388\n",
      "Iteration 153, loss = 0.08069583\n",
      "Iteration 154, loss = 0.08056891\n",
      "Iteration 155, loss = 0.08044312\n",
      "Iteration 156, loss = 0.08031847\n",
      "Iteration 157, loss = 0.08019494\n",
      "Iteration 158, loss = 0.08007257\n",
      "Iteration 159, loss = 0.07995132\n",
      "Iteration 160, loss = 0.07983116\n",
      "Iteration 161, loss = 0.07971207\n",
      "Iteration 162, loss = 0.07959404\n",
      "Iteration 163, loss = 0.07947706\n",
      "Iteration 164, loss = 0.07936112\n",
      "Iteration 165, loss = 0.07924620\n",
      "Iteration 166, loss = 0.07913233\n",
      "Iteration 167, loss = 0.07901950\n",
      "Iteration 168, loss = 0.07890773\n",
      "Iteration 169, loss = 0.07879710\n",
      "Iteration 170, loss = 0.07868779\n",
      "Iteration 171, loss = 0.07858021\n",
      "Iteration 172, loss = 0.07847534\n",
      "Iteration 173, loss = 0.07837532\n",
      "Iteration 174, loss = 0.07828595\n",
      "Iteration 175, loss = 0.07821948\n",
      "Iteration 176, loss = 0.07821428\n",
      "Iteration 177, loss = 0.07834295\n",
      "Iteration 178, loss = 0.07889509\n",
      "Iteration 179, loss = 0.08022992\n",
      "Iteration 180, loss = 0.08472145\n",
      "Iteration 181, loss = 0.09119532\n",
      "Iteration 182, loss = 0.11398409\n",
      "Iteration 183, loss = 0.10327969\n",
      "Iteration 184, loss = 0.10326214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47705517\n",
      "Iteration 2, loss = 6.21312686\n",
      "Iteration 3, loss = 1.41030512\n",
      "Iteration 4, loss = 1.05862602\n",
      "Iteration 5, loss = 1.50349088\n",
      "Iteration 6, loss = 0.79629236\n",
      "Iteration 7, loss = 0.83441146\n",
      "Iteration 8, loss = 0.71521704\n",
      "Iteration 9, loss = 0.54729226\n",
      "Iteration 10, loss = 0.54415640\n",
      "Iteration 11, loss = 0.53723144\n",
      "Iteration 12, loss = 0.45684335\n",
      "Iteration 13, loss = 0.39914816\n",
      "Iteration 14, loss = 0.41767712\n",
      "Iteration 15, loss = 0.43284026\n",
      "Iteration 16, loss = 0.39757949\n",
      "Iteration 17, loss = 0.33763392\n",
      "Iteration 18, loss = 0.33360980\n",
      "Iteration 19, loss = 0.33249371\n",
      "Iteration 20, loss = 0.28359387\n",
      "Iteration 21, loss = 0.27051087\n",
      "Iteration 22, loss = 0.26883924\n",
      "Iteration 23, loss = 0.22658514\n",
      "Iteration 24, loss = 0.21969320\n",
      "Iteration 25, loss = 0.21736293\n",
      "Iteration 26, loss = 0.18007119\n",
      "Iteration 27, loss = 0.18971542\n",
      "Iteration 28, loss = 0.16543727\n",
      "Iteration 29, loss = 0.15759390\n",
      "Iteration 30, loss = 0.15845847\n",
      "Iteration 31, loss = 0.13772123\n",
      "Iteration 32, loss = 0.14712208\n",
      "Iteration 33, loss = 0.12934715\n",
      "Iteration 34, loss = 0.13301391\n",
      "Iteration 35, loss = 0.12607856\n",
      "Iteration 36, loss = 0.12105201\n",
      "Iteration 37, loss = 0.12370841\n",
      "Iteration 38, loss = 0.11327336\n",
      "Iteration 39, loss = 0.11939428\n",
      "Iteration 40, loss = 0.11004288\n",
      "Iteration 41, loss = 0.11345381\n",
      "Iteration 42, loss = 0.10922393\n",
      "Iteration 43, loss = 0.10805039\n",
      "Iteration 44, loss = 0.10812376\n",
      "Iteration 45, loss = 0.10454556\n",
      "Iteration 46, loss = 0.10664447\n",
      "Iteration 47, loss = 0.10215515\n",
      "Iteration 48, loss = 0.10478270\n",
      "Iteration 49, loss = 0.10087399\n",
      "Iteration 50, loss = 0.10255416\n",
      "Iteration 51, loss = 0.10019185\n",
      "Iteration 52, loss = 0.10059872\n",
      "Iteration 53, loss = 0.09939701\n",
      "Iteration 54, loss = 0.09902258\n",
      "Iteration 55, loss = 0.09872490\n",
      "Iteration 56, loss = 0.09764924\n",
      "Iteration 57, loss = 0.09790310\n",
      "Iteration 58, loss = 0.09666634\n",
      "Iteration 59, loss = 0.09705219\n",
      "Iteration 60, loss = 0.09576570\n",
      "Iteration 61, loss = 0.09624186\n",
      "Iteration 62, loss = 0.09504931\n",
      "Iteration 63, loss = 0.09541915\n",
      "Iteration 64, loss = 0.09437046\n",
      "Iteration 65, loss = 0.09468716\n",
      "Iteration 66, loss = 0.09375783\n",
      "Iteration 67, loss = 0.09396294\n",
      "Iteration 68, loss = 0.09316867\n",
      "Iteration 69, loss = 0.09331373\n",
      "Iteration 70, loss = 0.09261232\n",
      "Iteration 71, loss = 0.09267506\n",
      "Iteration 72, loss = 0.09207217\n",
      "Iteration 73, loss = 0.09208554\n",
      "Iteration 74, loss = 0.09156087\n",
      "Iteration 75, loss = 0.09150707\n",
      "Iteration 76, loss = 0.09106194\n",
      "Iteration 77, loss = 0.09095893\n",
      "Iteration 78, loss = 0.09058977\n",
      "Iteration 79, loss = 0.09042621\n",
      "Iteration 80, loss = 0.09012604\n",
      "Iteration 81, loss = 0.08991339\n",
      "Iteration 82, loss = 0.08967895\n",
      "Iteration 83, loss = 0.08942345\n",
      "Iteration 84, loss = 0.08923668\n",
      "Iteration 85, loss = 0.08895283\n",
      "Iteration 86, loss = 0.08879698\n",
      "Iteration 87, loss = 0.08850806\n",
      "Iteration 88, loss = 0.08836031\n",
      "Iteration 89, loss = 0.08808483\n",
      "Iteration 90, loss = 0.08792326\n",
      "Iteration 91, loss = 0.08767957\n",
      "Iteration 92, loss = 0.08749373\n",
      "Iteration 93, loss = 0.08728701\n",
      "Iteration 94, loss = 0.08707633\n",
      "Iteration 95, loss = 0.08689857\n",
      "Iteration 96, loss = 0.08667571\n",
      "Iteration 97, loss = 0.08650966\n",
      "Iteration 98, loss = 0.08629384\n",
      "Iteration 99, loss = 0.08612228\n",
      "Iteration 100, loss = 0.08592653\n",
      "Iteration 101, loss = 0.08574091\n",
      "Iteration 102, loss = 0.08556603\n",
      "Iteration 103, loss = 0.08537207\n",
      "Iteration 104, loss = 0.08520682\n",
      "Iteration 105, loss = 0.08501815\n",
      "Iteration 106, loss = 0.08484966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 107, loss = 0.08467526\n",
      "Iteration 108, loss = 0.08449955\n",
      "Iteration 109, loss = 0.08433695\n",
      "Iteration 110, loss = 0.08416118\n",
      "Iteration 111, loss = 0.08400055\n",
      "Iteration 112, loss = 0.08383400\n",
      "Iteration 113, loss = 0.08366970\n",
      "Iteration 114, loss = 0.08351263\n",
      "Iteration 115, loss = 0.08334852\n",
      "Iteration 116, loss = 0.08319404\n",
      "Iteration 117, loss = 0.08303673\n",
      "Iteration 118, loss = 0.08288054\n",
      "Iteration 119, loss = 0.08273011\n",
      "Iteration 120, loss = 0.08257563\n",
      "Iteration 121, loss = 0.08242675\n",
      "Iteration 122, loss = 0.08227832\n",
      "Iteration 123, loss = 0.08212927\n",
      "Iteration 124, loss = 0.08198518\n",
      "Iteration 125, loss = 0.08183955\n",
      "Iteration 126, loss = 0.08169616\n",
      "Iteration 127, loss = 0.08155542\n",
      "Iteration 128, loss = 0.08141370\n",
      "Iteration 129, loss = 0.08127513\n",
      "Iteration 130, loss = 0.08113762\n",
      "Iteration 131, loss = 0.08100025\n",
      "Iteration 132, loss = 0.08086584\n",
      "Iteration 133, loss = 0.08073179\n",
      "Iteration 134, loss = 0.08059870\n",
      "Iteration 135, loss = 0.08046790\n",
      "Iteration 136, loss = 0.08033739\n",
      "Iteration 137, loss = 0.08020821\n",
      "Iteration 138, loss = 0.08008085\n",
      "Iteration 139, loss = 0.07995393\n",
      "Iteration 140, loss = 0.07982840\n",
      "Iteration 141, loss = 0.07970444\n",
      "Iteration 142, loss = 0.07958102\n",
      "Iteration 143, loss = 0.07945894\n",
      "Iteration 144, loss = 0.07933823\n",
      "Iteration 145, loss = 0.07921818\n",
      "Iteration 146, loss = 0.07909936\n",
      "Iteration 147, loss = 0.07898183\n",
      "Iteration 148, loss = 0.07886504\n",
      "Iteration 149, loss = 0.07874936\n",
      "Iteration 150, loss = 0.07863489\n",
      "Iteration 151, loss = 0.07852126\n",
      "Iteration 152, loss = 0.07840862\n",
      "Iteration 153, loss = 0.07829714\n",
      "Iteration 154, loss = 0.07818653\n",
      "Iteration 155, loss = 0.07807682\n",
      "Iteration 156, loss = 0.07796820\n",
      "Iteration 157, loss = 0.07786051\n",
      "Iteration 158, loss = 0.07775366\n",
      "Iteration 159, loss = 0.07764781\n",
      "Iteration 160, loss = 0.07754292\n",
      "Iteration 161, loss = 0.07743886\n",
      "Iteration 162, loss = 0.07733570\n",
      "Iteration 163, loss = 0.07723349\n",
      "Iteration 164, loss = 0.07713214\n",
      "Iteration 165, loss = 0.07703164\n",
      "Iteration 166, loss = 0.07693204\n",
      "Iteration 167, loss = 0.07683331\n",
      "Iteration 168, loss = 0.07673541\n",
      "Iteration 169, loss = 0.07663832\n",
      "Iteration 170, loss = 0.07654209\n",
      "Iteration 171, loss = 0.07644669\n",
      "Iteration 172, loss = 0.07635207\n",
      "Iteration 173, loss = 0.07625826\n",
      "Iteration 174, loss = 0.07616526\n",
      "Iteration 175, loss = 0.07607305\n",
      "Iteration 176, loss = 0.07598161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48410645\n",
      "Iteration 2, loss = 6.14206204\n",
      "Iteration 3, loss = 1.40124252\n",
      "Iteration 4, loss = 1.05406316\n",
      "Iteration 5, loss = 1.57627794\n",
      "Iteration 6, loss = 0.74734847\n",
      "Iteration 7, loss = 0.90759428\n",
      "Iteration 8, loss = 0.72326031\n",
      "Iteration 9, loss = 0.53870897\n",
      "Iteration 10, loss = 0.49234008\n",
      "Iteration 11, loss = 0.47858155\n",
      "Iteration 12, loss = 0.44495377\n",
      "Iteration 13, loss = 0.39209010\n",
      "Iteration 14, loss = 0.36257410\n",
      "Iteration 15, loss = 0.36412975\n",
      "Iteration 16, loss = 0.34903967\n",
      "Iteration 17, loss = 0.30868608\n",
      "Iteration 18, loss = 0.29232460\n",
      "Iteration 19, loss = 0.28861827\n",
      "Iteration 20, loss = 0.24986895\n",
      "Iteration 21, loss = 0.23316082\n",
      "Iteration 22, loss = 0.22785907\n",
      "Iteration 23, loss = 0.19584233\n",
      "Iteration 24, loss = 0.18882006\n",
      "Iteration 25, loss = 0.17979777\n",
      "Iteration 26, loss = 0.15681988\n",
      "Iteration 27, loss = 0.15881222\n",
      "Iteration 28, loss = 0.14244631\n",
      "Iteration 29, loss = 0.13373847\n",
      "Iteration 30, loss = 0.13323249\n",
      "Iteration 31, loss = 0.11823703\n",
      "Iteration 32, loss = 0.11967806\n",
      "Iteration 33, loss = 0.11270401\n",
      "Iteration 34, loss = 0.10603605\n",
      "Iteration 35, loss = 0.10779405\n",
      "Iteration 36, loss = 0.09937082\n",
      "Iteration 37, loss = 0.09887537\n",
      "Iteration 38, loss = 0.09508903\n",
      "Iteration 39, loss = 0.09040438\n",
      "Iteration 40, loss = 0.09365668\n",
      "Iteration 41, loss = 0.08805858\n",
      "Iteration 42, loss = 0.08866182\n",
      "Iteration 43, loss = 0.08887348\n",
      "Iteration 44, loss = 0.08496373\n",
      "Iteration 45, loss = 0.08720330\n",
      "Iteration 46, loss = 0.08459575\n",
      "Iteration 47, loss = 0.08402277\n",
      "Iteration 48, loss = 0.08479594\n",
      "Iteration 49, loss = 0.08223028\n",
      "Iteration 50, loss = 0.08318994\n",
      "Iteration 51, loss = 0.08212083\n",
      "Iteration 52, loss = 0.08115750\n",
      "Iteration 53, loss = 0.08181281\n",
      "Iteration 54, loss = 0.08019774\n",
      "Iteration 55, loss = 0.08044925\n",
      "Iteration 56, loss = 0.07998860\n",
      "Iteration 57, loss = 0.07909060\n",
      "Iteration 58, loss = 0.07945697\n",
      "Iteration 59, loss = 0.07841573\n",
      "Iteration 60, loss = 0.07837007\n",
      "Iteration 61, loss = 0.07807513\n",
      "Iteration 62, loss = 0.07737332\n",
      "Iteration 63, loss = 0.07750149\n",
      "Iteration 64, loss = 0.07677569\n",
      "Iteration 65, loss = 0.07663804\n",
      "Iteration 66, loss = 0.07635412\n",
      "Iteration 67, loss = 0.07583557\n",
      "Iteration 68, loss = 0.07580820\n",
      "Iteration 69, loss = 0.07525972\n",
      "Iteration 70, loss = 0.07510584\n",
      "Iteration 71, loss = 0.07480178\n",
      "Iteration 72, loss = 0.07442413\n",
      "Iteration 73, loss = 0.07429880\n",
      "Iteration 74, loss = 0.07386599\n",
      "Iteration 75, loss = 0.07371134\n",
      "Iteration 76, loss = 0.07339718\n",
      "Iteration 77, loss = 0.07311838\n",
      "Iteration 78, loss = 0.07293092\n",
      "Iteration 79, loss = 0.07258777\n",
      "Iteration 80, loss = 0.07242613\n",
      "Iteration 81, loss = 0.07212260\n",
      "Iteration 82, loss = 0.07190712\n",
      "Iteration 83, loss = 0.07168247\n",
      "Iteration 84, loss = 0.07141351\n",
      "Iteration 85, loss = 0.07123420\n",
      "Iteration 86, loss = 0.07096159\n",
      "Iteration 87, loss = 0.07077639\n",
      "Iteration 88, loss = 0.07053912\n",
      "Iteration 89, loss = 0.07032577\n",
      "Iteration 90, loss = 0.07012696\n",
      "Iteration 91, loss = 0.06989648\n",
      "Iteration 92, loss = 0.06971587\n",
      "Iteration 93, loss = 0.06949030\n",
      "Iteration 94, loss = 0.06930770\n",
      "Iteration 95, loss = 0.06910125\n",
      "Iteration 96, loss = 0.06890890\n",
      "Iteration 97, loss = 0.06872223\n",
      "Iteration 98, loss = 0.06852399\n",
      "Iteration 99, loss = 0.06834918\n",
      "Iteration 100, loss = 0.06815362\n",
      "Iteration 101, loss = 0.06798190\n",
      "Iteration 102, loss = 0.06779551\n",
      "Iteration 103, loss = 0.06762229\n",
      "Iteration 104, loss = 0.06744649\n",
      "Iteration 105, loss = 0.06727189\n",
      "Iteration 106, loss = 0.06710473\n",
      "Iteration 107, loss = 0.06693136\n",
      "Iteration 108, loss = 0.06676960\n",
      "Iteration 109, loss = 0.06660014\n",
      "Iteration 110, loss = 0.06644107\n",
      "Iteration 111, loss = 0.06627711\n",
      "Iteration 112, loss = 0.06611955\n",
      "Iteration 113, loss = 0.06596125\n",
      "Iteration 114, loss = 0.06580518\n",
      "Iteration 115, loss = 0.06565186\n",
      "Iteration 116, loss = 0.06549790\n",
      "Iteration 117, loss = 0.06534857\n",
      "Iteration 118, loss = 0.06519739\n",
      "Iteration 119, loss = 0.06505115\n",
      "Iteration 120, loss = 0.06490323\n",
      "Iteration 121, loss = 0.06475954\n",
      "Iteration 122, loss = 0.06461504\n",
      "Iteration 123, loss = 0.06447362\n",
      "Iteration 124, loss = 0.06433246\n",
      "Iteration 125, loss = 0.06419328\n",
      "Iteration 126, loss = 0.06405523\n",
      "Iteration 127, loss = 0.06391832\n",
      "Iteration 128, loss = 0.06378312\n",
      "Iteration 129, loss = 0.06364854\n",
      "Iteration 130, loss = 0.06351596\n",
      "Iteration 131, loss = 0.06338375\n",
      "Iteration 132, loss = 0.06325358\n",
      "Iteration 133, loss = 0.06312378\n",
      "Iteration 134, loss = 0.06299588\n",
      "Iteration 135, loss = 0.06286840\n",
      "Iteration 136, loss = 0.06274265\n",
      "Iteration 137, loss = 0.06261747\n",
      "Iteration 138, loss = 0.06249384\n",
      "Iteration 139, loss = 0.06237088\n",
      "Iteration 140, loss = 0.06224928\n",
      "Iteration 141, loss = 0.06212847\n",
      "Iteration 142, loss = 0.06200887\n",
      "Iteration 143, loss = 0.06189013\n",
      "Iteration 144, loss = 0.06177249\n",
      "Iteration 145, loss = 0.06165575\n",
      "Iteration 146, loss = 0.06154003\n",
      "Iteration 147, loss = 0.06142522\n",
      "Iteration 148, loss = 0.06131138\n",
      "Iteration 149, loss = 0.06119846\n",
      "Iteration 150, loss = 0.06108644\n",
      "Iteration 151, loss = 0.06097535\n",
      "Iteration 152, loss = 0.06086514\n",
      "Iteration 153, loss = 0.06075582\n",
      "Iteration 154, loss = 0.06064736\n",
      "Iteration 155, loss = 0.06053978\n",
      "Iteration 156, loss = 0.06043304\n",
      "Iteration 157, loss = 0.06032725\n",
      "Iteration 158, loss = 0.06022234\n",
      "Iteration 159, loss = 0.06011828\n",
      "Iteration 160, loss = 0.06001503\n",
      "Iteration 161, loss = 0.05991261\n",
      "Iteration 162, loss = 0.05981098\n",
      "Iteration 163, loss = 0.05971016\n",
      "Iteration 164, loss = 0.05961012\n",
      "Iteration 165, loss = 0.05951094\n",
      "Iteration 166, loss = 0.05941258\n",
      "Iteration 167, loss = 0.05931496\n",
      "Iteration 168, loss = 0.05921811\n",
      "Iteration 169, loss = 0.05912201\n",
      "Iteration 170, loss = 0.05902665\n",
      "Iteration 171, loss = 0.05893202\n",
      "Iteration 172, loss = 0.05883812\n",
      "Iteration 173, loss = 0.05874493\n",
      "Iteration 174, loss = 0.05865246\n",
      "Iteration 175, loss = 0.05856068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48086253\n",
      "Iteration 2, loss = 6.17958358\n",
      "Iteration 3, loss = 1.41501411\n",
      "Iteration 4, loss = 1.04441986\n",
      "Iteration 5, loss = 1.45322877\n",
      "Iteration 6, loss = 0.81971655\n",
      "Iteration 7, loss = 0.81595909\n",
      "Iteration 8, loss = 0.71429571\n",
      "Iteration 9, loss = 0.55740610\n",
      "Iteration 10, loss = 0.54711220\n",
      "Iteration 11, loss = 0.52875947\n",
      "Iteration 12, loss = 0.44928749\n",
      "Iteration 13, loss = 0.40077234\n",
      "Iteration 14, loss = 0.41779514\n",
      "Iteration 15, loss = 0.42089622\n",
      "Iteration 16, loss = 0.36937648\n",
      "Iteration 17, loss = 0.31478123\n",
      "Iteration 18, loss = 0.31918790\n",
      "Iteration 19, loss = 0.30733517\n",
      "Iteration 20, loss = 0.26626756\n",
      "Iteration 21, loss = 0.24751933\n",
      "Iteration 22, loss = 0.22641799\n",
      "Iteration 23, loss = 0.19470955\n",
      "Iteration 24, loss = 0.19472849\n",
      "Iteration 25, loss = 0.17482384\n",
      "Iteration 26, loss = 0.16221585\n",
      "Iteration 27, loss = 0.15921342\n",
      "Iteration 28, loss = 0.13776692\n",
      "Iteration 29, loss = 0.13781932\n",
      "Iteration 30, loss = 0.12265678\n",
      "Iteration 31, loss = 0.11887187\n",
      "Iteration 32, loss = 0.11450087\n",
      "Iteration 33, loss = 0.10527406\n",
      "Iteration 34, loss = 0.10718495\n",
      "Iteration 35, loss = 0.09738716\n",
      "Iteration 36, loss = 0.09921071\n",
      "Iteration 37, loss = 0.09314963\n",
      "Iteration 38, loss = 0.09200094\n",
      "Iteration 39, loss = 0.09011596\n",
      "Iteration 40, loss = 0.08662569\n",
      "Iteration 41, loss = 0.08730829\n",
      "Iteration 42, loss = 0.08292956\n",
      "Iteration 43, loss = 0.08427015\n",
      "Iteration 44, loss = 0.08068535\n",
      "Iteration 45, loss = 0.08111634\n",
      "Iteration 46, loss = 0.07923829\n",
      "Iteration 47, loss = 0.07836218\n",
      "Iteration 48, loss = 0.07783095\n",
      "Iteration 49, loss = 0.07627119\n",
      "Iteration 50, loss = 0.07643366\n",
      "Iteration 51, loss = 0.07459103\n",
      "Iteration 52, loss = 0.07501956\n",
      "Iteration 53, loss = 0.07331197\n",
      "Iteration 54, loss = 0.07358464\n",
      "Iteration 55, loss = 0.07224483\n",
      "Iteration 56, loss = 0.07228119\n",
      "Iteration 57, loss = 0.07125055\n",
      "Iteration 58, loss = 0.07108333\n",
      "Iteration 59, loss = 0.07034807\n",
      "Iteration 60, loss = 0.07000317\n",
      "Iteration 61, loss = 0.06945098\n",
      "Iteration 62, loss = 0.06903843\n",
      "Iteration 63, loss = 0.06860464\n",
      "Iteration 64, loss = 0.06814410\n",
      "Iteration 65, loss = 0.06777754\n",
      "Iteration 66, loss = 0.06732993\n",
      "Iteration 67, loss = 0.06699056\n",
      "Iteration 68, loss = 0.06655763\n",
      "Iteration 69, loss = 0.06623013\n",
      "Iteration 70, loss = 0.06583575\n",
      "Iteration 71, loss = 0.06550417\n",
      "Iteration 72, loss = 0.06514258\n",
      "Iteration 73, loss = 0.06480349\n",
      "Iteration 74, loss = 0.06448241\n",
      "Iteration 75, loss = 0.06413355\n",
      "Iteration 76, loss = 0.06384447\n",
      "Iteration 77, loss = 0.06348683\n",
      "Iteration 78, loss = 0.06322798\n",
      "Iteration 79, loss = 0.06286912\n",
      "Iteration 80, loss = 0.06263000\n",
      "Iteration 81, loss = 0.06227525\n",
      "Iteration 82, loss = 0.06204535\n",
      "Iteration 83, loss = 0.06170641\n",
      "Iteration 84, loss = 0.06147533\n",
      "Iteration 85, loss = 0.06116005\n",
      "Iteration 86, loss = 0.06091772\n",
      "Iteration 87, loss = 0.06063232\n",
      "Iteration 88, loss = 0.06037568\n",
      "Iteration 89, loss = 0.06011985\n",
      "Iteration 90, loss = 0.05985190\n",
      "Iteration 91, loss = 0.05961760\n",
      "Iteration 92, loss = 0.05934776\n",
      "Iteration 93, loss = 0.05912272\n",
      "Iteration 94, loss = 0.05886338\n",
      "Iteration 95, loss = 0.05863630\n",
      "Iteration 96, loss = 0.05839535\n",
      "Iteration 97, loss = 0.05816135\n",
      "Iteration 98, loss = 0.05793827\n",
      "Iteration 99, loss = 0.05770189\n",
      "Iteration 100, loss = 0.05748832\n",
      "Iteration 101, loss = 0.05725914\n",
      "Iteration 102, loss = 0.05704582\n",
      "Iteration 103, loss = 0.05683013\n",
      "Iteration 104, loss = 0.05661439\n",
      "Iteration 105, loss = 0.05640999\n",
      "Iteration 106, loss = 0.05619725\n",
      "Iteration 107, loss = 0.05599670\n",
      "Iteration 108, loss = 0.05579337\n",
      "Iteration 109, loss = 0.05559248\n",
      "Iteration 110, loss = 0.05539857\n",
      "Iteration 111, loss = 0.05520057\n",
      "Iteration 112, loss = 0.05501063\n",
      "Iteration 113, loss = 0.05482035\n",
      "Iteration 114, loss = 0.05463143\n",
      "Iteration 115, loss = 0.05444832\n",
      "Iteration 116, loss = 0.05426339\n",
      "Iteration 117, loss = 0.05408319\n",
      "Iteration 118, loss = 0.05390519\n",
      "Iteration 119, loss = 0.05372727\n",
      "Iteration 120, loss = 0.05355413\n",
      "Iteration 121, loss = 0.05338137\n",
      "Iteration 122, loss = 0.05321057\n",
      "Iteration 123, loss = 0.05304328\n",
      "Iteration 124, loss = 0.05287623\n",
      "Iteration 125, loss = 0.05271196\n",
      "Iteration 126, loss = 0.05255008\n",
      "Iteration 127, loss = 0.05238888\n",
      "Iteration 128, loss = 0.05223053\n",
      "Iteration 129, loss = 0.05207394\n",
      "Iteration 130, loss = 0.05191843\n",
      "Iteration 131, loss = 0.05176553\n",
      "Iteration 132, loss = 0.05161412\n",
      "Iteration 133, loss = 0.05146403\n",
      "Iteration 134, loss = 0.05131627\n",
      "Iteration 135, loss = 0.05116997\n",
      "Iteration 136, loss = 0.05102513\n",
      "Iteration 137, loss = 0.05088246\n",
      "Iteration 138, loss = 0.05074122\n",
      "Iteration 139, loss = 0.05060135\n",
      "Iteration 140, loss = 0.05046338\n",
      "Iteration 141, loss = 0.05032687\n",
      "Iteration 142, loss = 0.05019166\n",
      "Iteration 143, loss = 0.05005817\n",
      "Iteration 144, loss = 0.04992616\n",
      "Iteration 145, loss = 0.04979539\n",
      "Iteration 146, loss = 0.04966618\n",
      "Iteration 147, loss = 0.04953844\n",
      "Iteration 148, loss = 0.04941192\n",
      "Iteration 149, loss = 0.04928680\n",
      "Iteration 150, loss = 0.04916312\n",
      "Iteration 151, loss = 0.04904067\n",
      "Iteration 152, loss = 0.04891948\n",
      "Iteration 153, loss = 0.04879965\n",
      "Iteration 154, loss = 0.04868107\n",
      "Iteration 155, loss = 0.04856366\n",
      "Iteration 156, loss = 0.04844751\n",
      "Iteration 157, loss = 0.04833260\n",
      "Iteration 158, loss = 0.04821885\n",
      "Iteration 159, loss = 0.04810623\n",
      "Iteration 160, loss = 0.04799480\n",
      "Iteration 161, loss = 0.04788451\n",
      "Iteration 162, loss = 0.04777532\n",
      "Iteration 163, loss = 0.04766723\n",
      "Iteration 164, loss = 0.04756029\n",
      "Iteration 165, loss = 0.04745442\n",
      "Iteration 166, loss = 0.04734958\n",
      "Iteration 167, loss = 0.04724577\n",
      "Iteration 168, loss = 0.04714300\n",
      "Iteration 169, loss = 0.04704124\n",
      "Iteration 170, loss = 0.04694045\n",
      "Iteration 171, loss = 0.04684064\n",
      "Iteration 172, loss = 0.04674181\n",
      "Iteration 173, loss = 0.04664392\n",
      "Iteration 174, loss = 0.04654695\n",
      "Iteration 175, loss = 0.04645091\n",
      "Iteration 176, loss = 0.04635577\n",
      "Iteration 177, loss = 0.04626153\n",
      "Iteration 178, loss = 0.04616818\n",
      "Iteration 179, loss = 0.04607568\n",
      "Iteration 180, loss = 0.04598405\n",
      "Iteration 181, loss = 0.04589325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49174295\n",
      "Iteration 2, loss = 6.08383987\n",
      "Iteration 3, loss = 1.15466405\n",
      "Iteration 4, loss = 1.30663148\n",
      "Iteration 5, loss = 1.11799620\n",
      "Iteration 6, loss = 0.71332946\n",
      "Iteration 7, loss = 0.91302855\n",
      "Iteration 8, loss = 0.54700897\n",
      "Iteration 9, loss = 0.59083764\n",
      "Iteration 10, loss = 0.53380108\n",
      "Iteration 11, loss = 0.49563637\n",
      "Iteration 12, loss = 0.42841994\n",
      "Iteration 13, loss = 0.40741116\n",
      "Iteration 14, loss = 0.38311392\n",
      "Iteration 15, loss = 0.35830108\n",
      "Iteration 16, loss = 0.34155182\n",
      "Iteration 17, loss = 0.30852803\n",
      "Iteration 18, loss = 0.29182281\n",
      "Iteration 19, loss = 0.26585173\n",
      "Iteration 20, loss = 0.24331080\n",
      "Iteration 21, loss = 0.22851763\n",
      "Iteration 22, loss = 0.20750054\n",
      "Iteration 23, loss = 0.19865869\n",
      "Iteration 24, loss = 0.18108259\n",
      "Iteration 25, loss = 0.17439476\n",
      "Iteration 26, loss = 0.16205423\n",
      "Iteration 27, loss = 0.15629768\n",
      "Iteration 28, loss = 0.14821076\n",
      "Iteration 29, loss = 0.14319078\n",
      "Iteration 30, loss = 0.13761490\n",
      "Iteration 31, loss = 0.13299670\n",
      "Iteration 32, loss = 0.12878047\n",
      "Iteration 33, loss = 0.12662515\n",
      "Iteration 34, loss = 0.12272644\n",
      "Iteration 35, loss = 0.11934652\n",
      "Iteration 36, loss = 0.11761627\n",
      "Iteration 37, loss = 0.11505631\n",
      "Iteration 38, loss = 0.11370638\n",
      "Iteration 39, loss = 0.11213165\n",
      "Iteration 40, loss = 0.11084473\n",
      "Iteration 41, loss = 0.11001334\n",
      "Iteration 42, loss = 0.10856830\n",
      "Iteration 43, loss = 0.10829417\n",
      "Iteration 44, loss = 0.10667136\n",
      "Iteration 45, loss = 0.10666802\n",
      "Iteration 46, loss = 0.10493836\n",
      "Iteration 47, loss = 0.10495955\n",
      "Iteration 48, loss = 0.10352619\n",
      "Iteration 49, loss = 0.10337543\n",
      "Iteration 50, loss = 0.10247784\n",
      "Iteration 51, loss = 0.10193384\n",
      "Iteration 52, loss = 0.10160011\n",
      "Iteration 53, loss = 0.10073700\n",
      "Iteration 54, loss = 0.10067252\n",
      "Iteration 55, loss = 0.09984498\n",
      "Iteration 56, loss = 0.09963499\n",
      "Iteration 57, loss = 0.09913268\n",
      "Iteration 58, loss = 0.09856866\n",
      "Iteration 59, loss = 0.09836467\n",
      "Iteration 60, loss = 0.09768963\n",
      "Iteration 61, loss = 0.09742200\n",
      "Iteration 62, loss = 0.09699649\n",
      "Iteration 63, loss = 0.09650082\n",
      "Iteration 64, loss = 0.09627602\n",
      "Iteration 65, loss = 0.09578253\n",
      "Iteration 66, loss = 0.09546735\n",
      "Iteration 67, loss = 0.09517371\n",
      "Iteration 68, loss = 0.09473076\n",
      "Iteration 69, loss = 0.09448514\n",
      "Iteration 70, loss = 0.09413661\n",
      "Iteration 71, loss = 0.09374272\n",
      "Iteration 72, loss = 0.09345117\n",
      "Iteration 73, loss = 0.09304261\n",
      "Iteration 74, loss = 0.09249707\n",
      "Iteration 75, loss = 0.09294494\n",
      "Iteration 76, loss = 0.09684682\n",
      "Iteration 77, loss = 0.09828076\n",
      "Iteration 78, loss = 0.09214820\n",
      "Iteration 79, loss = 0.09298191\n",
      "Iteration 80, loss = 0.09487374\n",
      "Iteration 81, loss = 0.09091526\n",
      "Iteration 82, loss = 0.09274762\n",
      "Iteration 83, loss = 0.09352549\n",
      "Iteration 84, loss = 0.09005475\n",
      "Iteration 85, loss = 0.09240598\n",
      "Iteration 86, loss = 0.09251818\n",
      "Iteration 87, loss = 0.08936211\n",
      "Iteration 88, loss = 0.09188118\n",
      "Iteration 89, loss = 0.09168925\n",
      "Iteration 90, loss = 0.08870274\n",
      "Iteration 91, loss = 0.09122148\n",
      "Iteration 92, loss = 0.09095937\n",
      "Iteration 93, loss = 0.08805147\n",
      "Iteration 94, loss = 0.09049037\n",
      "Iteration 95, loss = 0.09030812\n",
      "Iteration 96, loss = 0.08741203\n",
      "Iteration 97, loss = 0.08972732\n",
      "Iteration 98, loss = 0.08971200\n",
      "Iteration 99, loss = 0.08679282\n",
      "Iteration 100, loss = 0.08893670\n",
      "Iteration 101, loss = 0.08914772\n",
      "Iteration 102, loss = 0.08620414\n",
      "Iteration 103, loss = 0.08811887\n",
      "Iteration 104, loss = 0.08858864\n",
      "Iteration 105, loss = 0.08565595\n",
      "Iteration 106, loss = 0.08726780\n",
      "Iteration 107, loss = 0.08800121\n",
      "Iteration 108, loss = 0.08515728\n",
      "Iteration 109, loss = 0.08637716\n",
      "Iteration 110, loss = 0.08734379\n",
      "Iteration 111, loss = 0.08471432\n",
      "Iteration 112, loss = 0.08545059\n",
      "Iteration 113, loss = 0.08657261\n",
      "Iteration 114, loss = 0.08432803\n",
      "Iteration 115, loss = 0.08451702\n",
      "Iteration 116, loss = 0.08565873\n",
      "Iteration 117, loss = 0.08398739\n",
      "Iteration 118, loss = 0.08364665\n",
      "Iteration 119, loss = 0.08461539\n",
      "Iteration 120, loss = 0.08365763\n",
      "Iteration 121, loss = 0.08294847\n",
      "Iteration 122, loss = 0.08353427\n",
      "Iteration 123, loss = 0.08327311\n",
      "Iteration 124, loss = 0.08251122\n",
      "Iteration 125, loss = 0.08257960\n",
      "Iteration 126, loss = 0.08275514\n",
      "Iteration 127, loss = 0.08228595\n",
      "Iteration 128, loss = 0.08191470\n",
      "Iteration 129, loss = 0.08209099\n",
      "Iteration 130, loss = 0.08203343\n",
      "Iteration 131, loss = 0.08155787\n",
      "Iteration 132, loss = 0.08142487\n",
      "Iteration 133, loss = 0.08153520\n",
      "Iteration 134, loss = 0.08131169\n",
      "Iteration 135, loss = 0.08097993\n",
      "Iteration 136, loss = 0.08090857\n",
      "Iteration 137, loss = 0.08091725\n",
      "Iteration 138, loss = 0.08073166\n",
      "Iteration 139, loss = 0.08047022\n",
      "Iteration 140, loss = 0.08038196\n",
      "Iteration 141, loss = 0.08036242\n",
      "Iteration 142, loss = 0.08020605\n",
      "Iteration 143, loss = 0.07999164\n",
      "Iteration 144, loss = 0.07986900\n",
      "Iteration 145, loss = 0.07981927\n",
      "Iteration 146, loss = 0.07972052\n",
      "Iteration 147, loss = 0.07954517\n",
      "Iteration 148, loss = 0.07939264\n",
      "Iteration 149, loss = 0.07930646\n",
      "Iteration 150, loss = 0.07923179\n",
      "Iteration 151, loss = 0.07911494\n",
      "Iteration 152, loss = 0.07896570\n",
      "Iteration 153, loss = 0.07883784\n",
      "Iteration 154, loss = 0.07874686\n",
      "Iteration 155, loss = 0.07866348\n",
      "Iteration 156, loss = 0.07855936\n",
      "Iteration 157, loss = 0.07843352\n",
      "Iteration 158, loss = 0.07831116\n",
      "Iteration 159, loss = 0.07820778\n",
      "Iteration 160, loss = 0.07811898\n",
      "Iteration 161, loss = 0.07802974\n",
      "Iteration 162, loss = 0.07792893\n",
      "Iteration 163, loss = 0.07781963\n",
      "Iteration 164, loss = 0.07770977\n",
      "Iteration 165, loss = 0.07760723\n",
      "Iteration 166, loss = 0.07751333\n",
      "Iteration 167, loss = 0.07742449\n",
      "Iteration 168, loss = 0.07733602\n",
      "Iteration 169, loss = 0.07724442\n",
      "Iteration 170, loss = 0.07714988\n",
      "Iteration 171, loss = 0.07705319\n",
      "Iteration 172, loss = 0.07695685\n",
      "Iteration 173, loss = 0.07686203\n",
      "Iteration 174, loss = 0.07676961\n",
      "Iteration 175, loss = 0.07667958\n",
      "Iteration 176, loss = 0.07659166\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48762646\n",
      "Iteration 2, loss = 1.67599084\n",
      "Iteration 3, loss = 1.66382714\n",
      "Iteration 4, loss = 1.02755535\n",
      "Iteration 5, loss = 1.24755732\n",
      "Iteration 6, loss = 0.62250028\n",
      "Iteration 7, loss = 0.56591440\n",
      "Iteration 8, loss = 0.50405036\n",
      "Iteration 9, loss = 0.45915505\n",
      "Iteration 10, loss = 0.42908856\n",
      "Iteration 11, loss = 0.40417721\n",
      "Iteration 12, loss = 0.38124751\n",
      "Iteration 13, loss = 0.35856057\n",
      "Iteration 14, loss = 0.33520741\n",
      "Iteration 15, loss = 0.31109027\n",
      "Iteration 16, loss = 0.28712078\n",
      "Iteration 17, loss = 0.26600662\n",
      "Iteration 18, loss = 0.28884814\n",
      "Iteration 19, loss = 0.87787156\n",
      "Iteration 20, loss = 2.39955618\n",
      "Iteration 21, loss = 0.89196916\n",
      "Iteration 22, loss = 0.37683979\n",
      "Iteration 23, loss = 0.36084103\n",
      "Iteration 24, loss = 0.34492340\n",
      "Iteration 25, loss = 0.33234927\n",
      "Iteration 26, loss = 0.32334704\n",
      "Iteration 27, loss = 0.31259661\n",
      "Iteration 28, loss = 0.30065919\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47705517\n",
      "Iteration 2, loss = 1.64766522\n",
      "Iteration 3, loss = 1.67573023\n",
      "Iteration 4, loss = 1.10824182\n",
      "Iteration 5, loss = 1.32746669\n",
      "Iteration 6, loss = 0.63640802\n",
      "Iteration 7, loss = 0.57045700\n",
      "Iteration 8, loss = 0.50602812\n",
      "Iteration 9, loss = 0.46554345\n",
      "Iteration 10, loss = 0.43416931\n",
      "Iteration 11, loss = 0.40712887\n",
      "Iteration 12, loss = 0.38459684\n",
      "Iteration 13, loss = 0.36286785\n",
      "Iteration 14, loss = 0.34772144\n",
      "Iteration 15, loss = 0.37294608\n",
      "Iteration 16, loss = 0.63402179\n",
      "Iteration 17, loss = 1.26514328\n",
      "Iteration 18, loss = 0.35514532\n",
      "Iteration 19, loss = 0.35436159\n",
      "Iteration 20, loss = 0.30288022\n",
      "Iteration 21, loss = 0.28077313\n",
      "Iteration 22, loss = 0.25894989\n",
      "Iteration 23, loss = 0.24094905\n",
      "Iteration 24, loss = 0.24393650\n",
      "Iteration 25, loss = 0.33849918\n",
      "Iteration 26, loss = 0.93269866\n",
      "Iteration 27, loss = 1.12160194\n",
      "Iteration 28, loss = 0.94099772\n",
      "Iteration 29, loss = 0.30436148\n",
      "Iteration 30, loss = 0.30361014\n",
      "Iteration 31, loss = 0.26891062\n",
      "Iteration 32, loss = 0.25202211\n",
      "Iteration 33, loss = 0.23919603\n",
      "Iteration 34, loss = 0.22757772\n",
      "Iteration 35, loss = 0.21441803\n",
      "Iteration 36, loss = 0.20064846\n",
      "Iteration 37, loss = 0.18719178\n",
      "Iteration 38, loss = 0.17472211\n",
      "Iteration 39, loss = 0.16364942\n",
      "Iteration 40, loss = 0.15407345\n",
      "Iteration 41, loss = 0.14592514\n",
      "Iteration 42, loss = 0.13900108\n",
      "Iteration 43, loss = 0.13318023\n",
      "Iteration 44, loss = 0.12826631\n",
      "Iteration 45, loss = 0.12446755\n",
      "Iteration 46, loss = 0.12264766\n",
      "Iteration 47, loss = 0.12935973\n",
      "Iteration 48, loss = 0.18685187\n",
      "Iteration 49, loss = 0.54996575\n",
      "Iteration 50, loss = 3.62433992\n",
      "Iteration 51, loss = 1.34195099\n",
      "Iteration 52, loss = 1.03531063\n",
      "Iteration 53, loss = 0.55409471\n",
      "Iteration 54, loss = 0.41337382\n",
      "Iteration 55, loss = 0.31620999\n",
      "Iteration 56, loss = 0.32566866\n",
      "Iteration 57, loss = 0.31148464\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48410645\n",
      "Iteration 2, loss = 1.64759435\n",
      "Iteration 3, loss = 1.63600662\n",
      "Iteration 4, loss = 0.96085698\n",
      "Iteration 5, loss = 1.24752546\n",
      "Iteration 6, loss = 0.59860221\n",
      "Iteration 7, loss = 0.53089514\n",
      "Iteration 8, loss = 0.47626814\n",
      "Iteration 9, loss = 0.43459843\n",
      "Iteration 10, loss = 0.40207171\n",
      "Iteration 11, loss = 0.37381350\n",
      "Iteration 12, loss = 0.34633597\n",
      "Iteration 13, loss = 0.31863072\n",
      "Iteration 14, loss = 0.29078979\n",
      "Iteration 15, loss = 0.26443792\n",
      "Iteration 16, loss = 0.25965791\n",
      "Iteration 17, loss = 0.51046760\n",
      "Iteration 18, loss = 2.04010419\n",
      "Iteration 19, loss = 1.46018142\n",
      "Iteration 20, loss = 0.31487176\n",
      "Iteration 21, loss = 0.37425393\n",
      "Iteration 22, loss = 0.31886444\n",
      "Iteration 23, loss = 0.30228609\n",
      "Iteration 24, loss = 0.28705367\n",
      "Iteration 25, loss = 0.26540087\n",
      "Iteration 26, loss = 0.24223563\n",
      "Iteration 27, loss = 0.22112071\n",
      "Iteration 28, loss = 0.20083785\n",
      "Iteration 29, loss = 0.18219865\n",
      "Iteration 30, loss = 0.16663620\n",
      "Iteration 31, loss = 0.15357092\n",
      "Iteration 32, loss = 0.14534559\n",
      "Iteration 33, loss = 0.15053737\n",
      "Iteration 34, loss = 0.23006052\n",
      "Iteration 35, loss = 0.99247626\n",
      "Iteration 36, loss = 2.28671439\n",
      "Iteration 37, loss = 0.59135897\n",
      "Iteration 38, loss = 0.40836012\n",
      "Iteration 39, loss = 0.43563949\n",
      "Iteration 40, loss = 0.42715767\n",
      "Iteration 41, loss = 0.43678505\n",
      "Iteration 42, loss = 0.67447107\n",
      "Iteration 43, loss = 1.26935954\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48086253\n",
      "Iteration 2, loss = 1.65717716\n",
      "Iteration 3, loss = 1.66785437\n",
      "Iteration 4, loss = 1.08150368\n",
      "Iteration 5, loss = 1.31520528\n",
      "Iteration 6, loss = 0.62841080\n",
      "Iteration 7, loss = 0.57017333\n",
      "Iteration 8, loss = 0.50090639\n",
      "Iteration 9, loss = 0.45465352\n",
      "Iteration 10, loss = 0.42007807\n",
      "Iteration 11, loss = 0.39121145\n",
      "Iteration 12, loss = 0.36364414\n",
      "Iteration 13, loss = 0.33460731\n",
      "Iteration 14, loss = 0.30750242\n",
      "Iteration 15, loss = 0.28460195\n",
      "Iteration 16, loss = 0.32373448\n",
      "Iteration 17, loss = 0.94442950\n",
      "Iteration 18, loss = 1.76362653\n",
      "Iteration 19, loss = 0.83534130\n",
      "Iteration 20, loss = 0.36770251\n",
      "Iteration 21, loss = 0.35629030\n",
      "Iteration 22, loss = 0.33393929\n",
      "Iteration 23, loss = 0.32061634\n",
      "Iteration 24, loss = 0.30668412\n",
      "Iteration 25, loss = 0.28726206\n",
      "Iteration 26, loss = 0.26329254\n",
      "Iteration 27, loss = 0.24106423\n",
      "Iteration 28, loss = 0.21828965\n",
      "Iteration 29, loss = 0.19605082\n",
      "Iteration 30, loss = 0.17627854\n",
      "Iteration 31, loss = 0.16020549\n",
      "Iteration 32, loss = 0.14712198\n",
      "Iteration 33, loss = 0.13741154\n",
      "Iteration 34, loss = 0.13332888\n",
      "Iteration 35, loss = 0.16321947\n",
      "Iteration 36, loss = 0.48829853\n",
      "Iteration 37, loss = 2.41380078\n",
      "Iteration 38, loss = 0.57022494\n",
      "Iteration 39, loss = 0.31202435\n",
      "Iteration 40, loss = 0.27189806\n",
      "Iteration 41, loss = 0.20808029\n",
      "Iteration 42, loss = 0.21235561\n",
      "Iteration 43, loss = 0.22808467\n",
      "Iteration 44, loss = 0.34956662\n",
      "Iteration 45, loss = 0.51386688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49174295\n",
      "Iteration 2, loss = 1.69296771\n",
      "Iteration 3, loss = 1.69870405\n",
      "Iteration 4, loss = 1.07511183\n",
      "Iteration 5, loss = 1.29173821\n",
      "Iteration 6, loss = 0.64161300\n",
      "Iteration 7, loss = 0.57844565\n",
      "Iteration 8, loss = 0.51077914\n",
      "Iteration 9, loss = 0.46695122\n",
      "Iteration 10, loss = 0.43674951\n",
      "Iteration 11, loss = 0.41172133\n",
      "Iteration 12, loss = 0.38863373\n",
      "Iteration 13, loss = 0.36565544\n",
      "Iteration 14, loss = 0.34143505\n",
      "Iteration 15, loss = 0.31600080\n",
      "Iteration 16, loss = 0.28994208\n",
      "Iteration 17, loss = 0.26469160\n",
      "Iteration 18, loss = 0.24284936\n",
      "Iteration 19, loss = 0.26474688\n",
      "Iteration 20, loss = 0.89887048\n",
      "Iteration 21, loss = 2.94742519\n",
      "Iteration 22, loss = 0.84966473\n",
      "Iteration 23, loss = 0.41305030\n",
      "Iteration 24, loss = 0.33577150\n",
      "Iteration 25, loss = 0.33553256\n",
      "Iteration 26, loss = 0.32059915\n",
      "Iteration 27, loss = 0.31029415\n",
      "Iteration 28, loss = 0.29946781\n",
      "Iteration 29, loss = 0.28837906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48762646\n",
      "Iteration 2, loss = 1.23656474\n",
      "Iteration 3, loss = 1.08521818\n",
      "Iteration 4, loss = 0.99173175\n",
      "Iteration 5, loss = 0.92634460\n",
      "Iteration 6, loss = 0.86402220\n",
      "Iteration 7, loss = 0.79344095\n",
      "Iteration 8, loss = 0.72720746\n",
      "Iteration 9, loss = 0.67654418\n",
      "Iteration 10, loss = 0.64203449\n",
      "Iteration 11, loss = 0.61214574\n",
      "Iteration 12, loss = 0.57961916\n",
      "Iteration 13, loss = 0.54765644\n",
      "Iteration 14, loss = 0.52000057\n",
      "Iteration 15, loss = 0.49565715\n",
      "Iteration 16, loss = 0.47302438\n",
      "Iteration 17, loss = 0.45423104\n",
      "Iteration 18, loss = 0.43932644\n",
      "Iteration 19, loss = 0.42383172\n",
      "Iteration 20, loss = 0.40647242\n",
      "Iteration 21, loss = 0.39155911\n",
      "Iteration 22, loss = 0.37847471\n",
      "Iteration 23, loss = 0.36497322\n",
      "Iteration 24, loss = 0.35211110\n",
      "Iteration 25, loss = 0.34109328\n",
      "Iteration 26, loss = 0.32960785\n",
      "Iteration 27, loss = 0.31703690\n",
      "Iteration 28, loss = 0.30512955\n",
      "Iteration 29, loss = 0.29365742\n",
      "Iteration 30, loss = 0.28276194\n",
      "Iteration 31, loss = 0.27337063\n",
      "Iteration 32, loss = 0.26420430\n",
      "Iteration 33, loss = 0.25454451\n",
      "Iteration 34, loss = 0.24542835\n",
      "Iteration 35, loss = 0.23627314\n",
      "Iteration 36, loss = 0.22710923\n",
      "Iteration 37, loss = 0.21855408\n",
      "Iteration 38, loss = 0.20985864\n",
      "Iteration 39, loss = 0.20184381\n",
      "Iteration 40, loss = 0.19376746\n",
      "Iteration 41, loss = 0.18627064\n",
      "Iteration 42, loss = 0.17872624\n",
      "Iteration 43, loss = 0.17173684\n",
      "Iteration 44, loss = 0.16475789\n",
      "Iteration 45, loss = 0.15834162\n",
      "Iteration 46, loss = 0.15202776\n",
      "Iteration 47, loss = 0.14620721\n",
      "Iteration 48, loss = 0.14079918\n",
      "Iteration 49, loss = 0.13552457\n",
      "Iteration 50, loss = 0.13075457\n",
      "Iteration 51, loss = 0.12636088\n",
      "Iteration 52, loss = 0.12213378\n",
      "Iteration 53, loss = 0.11826484\n",
      "Iteration 54, loss = 0.11481922\n",
      "Iteration 55, loss = 0.11159466\n",
      "Iteration 56, loss = 0.10859276\n",
      "Iteration 57, loss = 0.10577605\n",
      "Iteration 58, loss = 0.10324844\n",
      "Iteration 59, loss = 0.10096668\n",
      "Iteration 60, loss = 0.09893525\n",
      "Iteration 61, loss = 0.09715764\n",
      "Iteration 62, loss = 0.09552589\n",
      "Iteration 63, loss = 0.09390750\n",
      "Iteration 64, loss = 0.09227456\n",
      "Iteration 65, loss = 0.09097479\n",
      "Iteration 66, loss = 0.08998537\n",
      "Iteration 67, loss = 0.08901841\n",
      "Iteration 68, loss = 0.08794594\n",
      "Iteration 69, loss = 0.08691272\n",
      "Iteration 70, loss = 0.08619262\n",
      "Iteration 71, loss = 0.08562549\n",
      "Iteration 72, loss = 0.08495766\n",
      "Iteration 73, loss = 0.08421943\n",
      "Iteration 74, loss = 0.08360353\n",
      "Iteration 75, loss = 0.08317685\n",
      "Iteration 76, loss = 0.08280599\n",
      "Iteration 77, loss = 0.08235367\n",
      "Iteration 78, loss = 0.08185807\n",
      "Iteration 79, loss = 0.08142873\n",
      "Iteration 80, loss = 0.08111805\n",
      "Iteration 81, loss = 0.08086957\n",
      "Iteration 82, loss = 0.08059883\n",
      "Iteration 83, loss = 0.08028049\n",
      "Iteration 84, loss = 0.07993860\n",
      "Iteration 85, loss = 0.07964116\n",
      "Iteration 86, loss = 0.07941097\n",
      "Iteration 87, loss = 0.07922736\n",
      "Iteration 88, loss = 0.07906092\n",
      "Iteration 89, loss = 0.07888376\n",
      "Iteration 90, loss = 0.07868841\n",
      "Iteration 91, loss = 0.07845890\n",
      "Iteration 92, loss = 0.07822450\n",
      "Iteration 93, loss = 0.07800585\n",
      "Iteration 94, loss = 0.07781941\n",
      "Iteration 95, loss = 0.07766352\n",
      "Iteration 96, loss = 0.07754009\n",
      "Iteration 97, loss = 0.07746897\n",
      "Iteration 98, loss = 0.07743010\n",
      "Iteration 99, loss = 0.07742005\n",
      "Iteration 100, loss = 0.07732720\n",
      "Iteration 101, loss = 0.07711546\n",
      "Iteration 102, loss = 0.07677597\n",
      "Iteration 103, loss = 0.07653876\n",
      "Iteration 104, loss = 0.07648244\n",
      "Iteration 105, loss = 0.07649843\n",
      "Iteration 106, loss = 0.07644182\n",
      "Iteration 107, loss = 0.07622748\n",
      "Iteration 108, loss = 0.07599544\n",
      "Iteration 109, loss = 0.07587182\n",
      "Iteration 110, loss = 0.07584843\n",
      "Iteration 111, loss = 0.07581634\n",
      "Iteration 112, loss = 0.07568508\n",
      "Iteration 113, loss = 0.07550552\n",
      "Iteration 114, loss = 0.07536309\n",
      "Iteration 115, loss = 0.07529670\n",
      "Iteration 116, loss = 0.07525973\n",
      "Iteration 117, loss = 0.07518441\n",
      "Iteration 118, loss = 0.07506250\n",
      "Iteration 119, loss = 0.07492365\n",
      "Iteration 120, loss = 0.07481633\n",
      "Iteration 121, loss = 0.07474849\n",
      "Iteration 122, loss = 0.07469501\n",
      "Iteration 123, loss = 0.07462911\n",
      "Iteration 124, loss = 0.07453561\n",
      "Iteration 125, loss = 0.07442805\n",
      "Iteration 126, loss = 0.07432247\n",
      "Iteration 127, loss = 0.07423257\n",
      "Iteration 128, loss = 0.07415929\n",
      "Iteration 129, loss = 0.07409650\n",
      "Iteration 130, loss = 0.07403787\n",
      "Iteration 131, loss = 0.07397896\n",
      "Iteration 132, loss = 0.07392285\n",
      "Iteration 133, loss = 0.07386567\n",
      "Iteration 134, loss = 0.07381750\n",
      "Iteration 135, loss = 0.07376636\n",
      "Iteration 136, loss = 0.07372539\n",
      "Iteration 137, loss = 0.07366580\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47705517\n",
      "Iteration 2, loss = 1.23360476\n",
      "Iteration 3, loss = 1.08067148\n",
      "Iteration 4, loss = 0.99422921\n",
      "Iteration 5, loss = 0.93026402\n",
      "Iteration 6, loss = 0.86747655\n",
      "Iteration 7, loss = 0.79695092\n",
      "Iteration 8, loss = 0.73196230\n",
      "Iteration 9, loss = 0.68323897\n",
      "Iteration 10, loss = 0.64836308\n",
      "Iteration 11, loss = 0.61717858\n",
      "Iteration 12, loss = 0.58354468\n",
      "Iteration 13, loss = 0.55113819\n",
      "Iteration 14, loss = 0.52297465\n",
      "Iteration 15, loss = 0.49866760\n",
      "Iteration 16, loss = 0.47637679\n",
      "Iteration 17, loss = 0.45813719\n",
      "Iteration 18, loss = 0.44297493\n",
      "Iteration 19, loss = 0.42702268\n",
      "Iteration 20, loss = 0.40897183\n",
      "Iteration 21, loss = 0.39243168\n",
      "Iteration 22, loss = 0.37858073\n",
      "Iteration 23, loss = 0.36433778\n",
      "Iteration 24, loss = 0.35142268\n",
      "Iteration 25, loss = 0.34011110\n",
      "Iteration 26, loss = 0.32730154\n",
      "Iteration 27, loss = 0.31402532\n",
      "Iteration 28, loss = 0.30177826\n",
      "Iteration 29, loss = 0.29016321\n",
      "Iteration 30, loss = 0.27980892\n",
      "Iteration 31, loss = 0.27019054\n",
      "Iteration 32, loss = 0.26009744\n",
      "Iteration 33, loss = 0.24996014\n",
      "Iteration 34, loss = 0.24046113\n",
      "Iteration 35, loss = 0.23063286\n",
      "Iteration 36, loss = 0.22128655\n",
      "Iteration 37, loss = 0.21213039\n",
      "Iteration 38, loss = 0.20322658\n",
      "Iteration 39, loss = 0.19479131\n",
      "Iteration 40, loss = 0.18645891\n",
      "Iteration 41, loss = 0.17871559\n",
      "Iteration 42, loss = 0.17092842\n",
      "Iteration 43, loss = 0.16377282\n",
      "Iteration 44, loss = 0.15669556\n",
      "Iteration 45, loss = 0.15011016\n",
      "Iteration 46, loss = 0.14380154\n",
      "Iteration 47, loss = 0.13784375\n",
      "Iteration 48, loss = 0.13243616\n",
      "Iteration 49, loss = 0.12722206\n",
      "Iteration 50, loss = 0.12247042\n",
      "Iteration 51, loss = 0.11814235\n",
      "Iteration 52, loss = 0.11400919\n",
      "Iteration 53, loss = 0.11021465\n",
      "Iteration 54, loss = 0.10680631\n",
      "Iteration 55, loss = 0.10367424\n",
      "Iteration 56, loss = 0.10080572\n",
      "Iteration 57, loss = 0.09810657\n",
      "Iteration 58, loss = 0.09565566\n",
      "Iteration 59, loss = 0.09351010\n",
      "Iteration 60, loss = 0.09159301\n",
      "Iteration 61, loss = 0.08990614\n",
      "Iteration 62, loss = 0.08837751\n",
      "Iteration 63, loss = 0.08693785\n",
      "Iteration 64, loss = 0.08547163\n",
      "Iteration 65, loss = 0.08423656\n",
      "Iteration 66, loss = 0.08330173\n",
      "Iteration 67, loss = 0.08246651\n",
      "Iteration 68, loss = 0.08158116\n",
      "Iteration 69, loss = 0.08066492\n",
      "Iteration 70, loss = 0.07995462\n",
      "Iteration 71, loss = 0.07944355\n",
      "Iteration 72, loss = 0.07894449\n",
      "Iteration 73, loss = 0.07837202\n",
      "Iteration 74, loss = 0.07780139\n",
      "Iteration 75, loss = 0.07737403\n",
      "Iteration 76, loss = 0.07706880\n",
      "Iteration 77, loss = 0.07677162\n",
      "Iteration 78, loss = 0.07642263\n",
      "Iteration 79, loss = 0.07604611\n",
      "Iteration 80, loss = 0.07573761\n",
      "Iteration 81, loss = 0.07552149\n",
      "Iteration 82, loss = 0.07535297\n",
      "Iteration 83, loss = 0.07517691\n",
      "Iteration 84, loss = 0.07495400\n",
      "Iteration 85, loss = 0.07471488\n",
      "Iteration 86, loss = 0.07450109\n",
      "Iteration 87, loss = 0.07434113\n",
      "Iteration 88, loss = 0.07422333\n",
      "Iteration 89, loss = 0.07411976\n",
      "Iteration 90, loss = 0.07401313\n",
      "Iteration 91, loss = 0.07387961\n",
      "Iteration 92, loss = 0.07372978\n",
      "Iteration 93, loss = 0.07356961\n",
      "Iteration 94, loss = 0.07342561\n",
      "Iteration 95, loss = 0.07330673\n",
      "Iteration 96, loss = 0.07321125\n",
      "Iteration 97, loss = 0.07313163\n",
      "Iteration 98, loss = 0.07306029\n",
      "Iteration 99, loss = 0.07299644\n",
      "Iteration 100, loss = 0.07292621\n",
      "Iteration 101, loss = 0.07285272\n",
      "Iteration 102, loss = 0.07274803\n",
      "Iteration 103, loss = 0.07262745\n",
      "Iteration 104, loss = 0.07249064\n",
      "Iteration 105, loss = 0.07236800\n",
      "Iteration 106, loss = 0.07227025\n",
      "Iteration 107, loss = 0.07219725\n",
      "Iteration 108, loss = 0.07214077\n",
      "Iteration 109, loss = 0.07209018\n",
      "Iteration 110, loss = 0.07204155\n",
      "Iteration 111, loss = 0.07197932\n",
      "Iteration 112, loss = 0.07190766\n",
      "Iteration 113, loss = 0.07181032\n",
      "Iteration 114, loss = 0.07170470\n",
      "Iteration 115, loss = 0.07159637\n",
      "Iteration 116, loss = 0.07150077\n",
      "Iteration 117, loss = 0.07142188\n",
      "Iteration 118, loss = 0.07135735\n",
      "Iteration 119, loss = 0.07130286\n",
      "Iteration 120, loss = 0.07125598\n",
      "Iteration 121, loss = 0.07121863\n",
      "Iteration 122, loss = 0.07118913\n",
      "Iteration 123, loss = 0.07117951\n",
      "Iteration 124, loss = 0.07116596\n",
      "Iteration 125, loss = 0.07115870\n",
      "Iteration 126, loss = 0.07107817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48410645\n",
      "Iteration 2, loss = 1.23533066\n",
      "Iteration 3, loss = 1.08499961\n",
      "Iteration 4, loss = 0.98847314\n",
      "Iteration 5, loss = 0.92115040\n",
      "Iteration 6, loss = 0.86012722\n",
      "Iteration 7, loss = 0.79200685\n",
      "Iteration 8, loss = 0.72818655\n",
      "Iteration 9, loss = 0.67950369\n",
      "Iteration 10, loss = 0.64416378\n",
      "Iteration 11, loss = 0.61117829\n",
      "Iteration 12, loss = 0.57574545\n",
      "Iteration 13, loss = 0.54232525\n",
      "Iteration 14, loss = 0.51350241\n",
      "Iteration 15, loss = 0.48801572\n",
      "Iteration 16, loss = 0.46482474\n",
      "Iteration 17, loss = 0.44553659\n",
      "Iteration 18, loss = 0.43011852\n",
      "Iteration 19, loss = 0.41492458\n",
      "Iteration 20, loss = 0.39716313\n",
      "Iteration 21, loss = 0.37898477\n",
      "Iteration 22, loss = 0.36429464\n",
      "Iteration 23, loss = 0.35030410\n",
      "Iteration 24, loss = 0.33646605\n",
      "Iteration 25, loss = 0.32432543\n",
      "Iteration 26, loss = 0.31190733\n",
      "Iteration 27, loss = 0.29875064\n",
      "Iteration 28, loss = 0.28575383\n",
      "Iteration 29, loss = 0.27349325\n",
      "Iteration 30, loss = 0.26160296\n",
      "Iteration 31, loss = 0.25114300\n",
      "Iteration 32, loss = 0.24112610\n",
      "Iteration 33, loss = 0.23067815\n",
      "Iteration 34, loss = 0.22051937\n",
      "Iteration 35, loss = 0.21066495\n",
      "Iteration 36, loss = 0.20066968\n",
      "Iteration 37, loss = 0.19136187\n",
      "Iteration 38, loss = 0.18208193\n",
      "Iteration 39, loss = 0.17335564\n",
      "Iteration 40, loss = 0.16473375\n",
      "Iteration 41, loss = 0.15668717\n",
      "Iteration 42, loss = 0.14887578\n",
      "Iteration 43, loss = 0.14145726\n",
      "Iteration 44, loss = 0.13431938\n",
      "Iteration 45, loss = 0.12761456\n",
      "Iteration 46, loss = 0.12124208\n",
      "Iteration 47, loss = 0.11536862\n",
      "Iteration 48, loss = 0.10979182\n",
      "Iteration 49, loss = 0.10470698\n",
      "Iteration 50, loss = 0.09993947\n",
      "Iteration 51, loss = 0.09553557\n",
      "Iteration 52, loss = 0.09158286\n",
      "Iteration 53, loss = 0.08785205\n",
      "Iteration 54, loss = 0.08444013\n",
      "Iteration 55, loss = 0.08138745\n",
      "Iteration 56, loss = 0.07861312\n",
      "Iteration 57, loss = 0.07605989\n",
      "Iteration 58, loss = 0.07379095\n",
      "Iteration 59, loss = 0.07177511\n",
      "Iteration 60, loss = 0.06997401\n",
      "Iteration 61, loss = 0.06834804\n",
      "Iteration 62, loss = 0.06686773\n",
      "Iteration 63, loss = 0.06555732\n",
      "Iteration 64, loss = 0.06440388\n",
      "Iteration 65, loss = 0.06338646\n",
      "Iteration 66, loss = 0.06250029\n",
      "Iteration 67, loss = 0.06173084\n",
      "Iteration 68, loss = 0.06102335\n",
      "Iteration 69, loss = 0.06036788\n",
      "Iteration 70, loss = 0.05977556\n",
      "Iteration 71, loss = 0.05927563\n",
      "Iteration 72, loss = 0.05886315\n",
      "Iteration 73, loss = 0.05849985\n",
      "Iteration 74, loss = 0.05815639\n",
      "Iteration 75, loss = 0.05781808\n",
      "Iteration 76, loss = 0.05750661\n",
      "Iteration 77, loss = 0.05724598\n",
      "Iteration 78, loss = 0.05702606\n",
      "Iteration 79, loss = 0.05682585\n",
      "Iteration 80, loss = 0.05662238\n",
      "Iteration 81, loss = 0.05641939\n",
      "Iteration 82, loss = 0.05623354\n",
      "Iteration 83, loss = 0.05607522\n",
      "Iteration 84, loss = 0.05593669\n",
      "Iteration 85, loss = 0.05580354\n",
      "Iteration 86, loss = 0.05566662\n",
      "Iteration 87, loss = 0.05552666\n",
      "Iteration 88, loss = 0.05539287\n",
      "Iteration 89, loss = 0.05527234\n",
      "Iteration 90, loss = 0.05516418\n",
      "Iteration 91, loss = 0.05506332\n",
      "Iteration 92, loss = 0.05496263\n",
      "Iteration 93, loss = 0.05485929\n",
      "Iteration 94, loss = 0.05475453\n",
      "Iteration 95, loss = 0.05465234\n",
      "Iteration 96, loss = 0.05455558\n",
      "Iteration 97, loss = 0.05446464\n",
      "Iteration 98, loss = 0.05437809\n",
      "Iteration 99, loss = 0.05429406\n",
      "Iteration 100, loss = 0.05421099\n",
      "Iteration 101, loss = 0.05412793\n",
      "Iteration 102, loss = 0.05404467\n",
      "Iteration 103, loss = 0.05396138\n",
      "Iteration 104, loss = 0.05387835\n",
      "Iteration 105, loss = 0.05379632\n",
      "Iteration 106, loss = 0.05371573\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48086253\n",
      "Iteration 2, loss = 1.23473898\n",
      "Iteration 3, loss = 1.07985780\n",
      "Iteration 4, loss = 0.99117602\n",
      "Iteration 5, loss = 0.92601102\n",
      "Iteration 6, loss = 0.86234976\n",
      "Iteration 7, loss = 0.79102258\n",
      "Iteration 8, loss = 0.72454497\n",
      "Iteration 9, loss = 0.67399180\n",
      "Iteration 10, loss = 0.63877006\n",
      "Iteration 11, loss = 0.60786464\n",
      "Iteration 12, loss = 0.57407537\n",
      "Iteration 13, loss = 0.54076063\n",
      "Iteration 14, loss = 0.51202583\n",
      "Iteration 15, loss = 0.48755250\n",
      "Iteration 16, loss = 0.46450532\n",
      "Iteration 17, loss = 0.44499361\n",
      "Iteration 18, loss = 0.42829841\n",
      "Iteration 19, loss = 0.41060506\n",
      "Iteration 20, loss = 0.39317345\n",
      "Iteration 21, loss = 0.37766282\n",
      "Iteration 22, loss = 0.36307684\n",
      "Iteration 23, loss = 0.34840919\n",
      "Iteration 24, loss = 0.33490982\n",
      "Iteration 25, loss = 0.32256317\n",
      "Iteration 26, loss = 0.30933145\n",
      "Iteration 27, loss = 0.29531756\n",
      "Iteration 28, loss = 0.28209711\n",
      "Iteration 29, loss = 0.27063646\n",
      "Iteration 30, loss = 0.26005356\n",
      "Iteration 31, loss = 0.24977669\n",
      "Iteration 32, loss = 0.23953283\n",
      "Iteration 33, loss = 0.22924070\n",
      "Iteration 34, loss = 0.21919274\n",
      "Iteration 35, loss = 0.20922366\n",
      "Iteration 36, loss = 0.19947487\n",
      "Iteration 37, loss = 0.19009142\n",
      "Iteration 38, loss = 0.18090846\n",
      "Iteration 39, loss = 0.17217592\n",
      "Iteration 40, loss = 0.16369817\n",
      "Iteration 41, loss = 0.15563512\n",
      "Iteration 42, loss = 0.14781953\n",
      "Iteration 43, loss = 0.14032470\n",
      "Iteration 44, loss = 0.13313918\n",
      "Iteration 45, loss = 0.12638810\n",
      "Iteration 46, loss = 0.11993468\n",
      "Iteration 47, loss = 0.11388096\n",
      "Iteration 48, loss = 0.10820550\n",
      "Iteration 49, loss = 0.10285513\n",
      "Iteration 50, loss = 0.09790777\n",
      "Iteration 51, loss = 0.09331576\n",
      "Iteration 52, loss = 0.08901920\n",
      "Iteration 53, loss = 0.08508260\n",
      "Iteration 54, loss = 0.08144257\n",
      "Iteration 55, loss = 0.07809942\n",
      "Iteration 56, loss = 0.07505628\n",
      "Iteration 57, loss = 0.07227293\n",
      "Iteration 58, loss = 0.06981408\n",
      "Iteration 59, loss = 0.06753853\n",
      "Iteration 60, loss = 0.06543945\n",
      "Iteration 61, loss = 0.06344834\n",
      "Iteration 62, loss = 0.06172226\n",
      "Iteration 63, loss = 0.06023697\n",
      "Iteration 64, loss = 0.05886582\n",
      "Iteration 65, loss = 0.05754750\n",
      "Iteration 66, loss = 0.05628040\n",
      "Iteration 67, loss = 0.05518451\n",
      "Iteration 68, loss = 0.05426147\n",
      "Iteration 69, loss = 0.05343879\n",
      "Iteration 70, loss = 0.05265088\n",
      "Iteration 71, loss = 0.05184591\n",
      "Iteration 72, loss = 0.05110315\n",
      "Iteration 73, loss = 0.05047957\n",
      "Iteration 74, loss = 0.04996080\n",
      "Iteration 75, loss = 0.04950022\n",
      "Iteration 76, loss = 0.04903320\n",
      "Iteration 77, loss = 0.04855186\n",
      "Iteration 78, loss = 0.04807371\n",
      "Iteration 79, loss = 0.04766218\n",
      "Iteration 80, loss = 0.04733028\n",
      "Iteration 81, loss = 0.04704965\n",
      "Iteration 82, loss = 0.04679130\n",
      "Iteration 83, loss = 0.04651271\n",
      "Iteration 84, loss = 0.04621162\n",
      "Iteration 85, loss = 0.04589521\n",
      "Iteration 86, loss = 0.04561574\n",
      "Iteration 87, loss = 0.04539111\n",
      "Iteration 88, loss = 0.04520855\n",
      "Iteration 89, loss = 0.04504789\n",
      "Iteration 90, loss = 0.04488226\n",
      "Iteration 91, loss = 0.04470357\n",
      "Iteration 92, loss = 0.04448659\n",
      "Iteration 93, loss = 0.04426386\n",
      "Iteration 94, loss = 0.04406065\n",
      "Iteration 95, loss = 0.04389702\n",
      "Iteration 96, loss = 0.04376678\n",
      "Iteration 97, loss = 0.04365432\n",
      "Iteration 98, loss = 0.04354909\n",
      "Iteration 99, loss = 0.04342887\n",
      "Iteration 100, loss = 0.04329427\n",
      "Iteration 101, loss = 0.04312854\n",
      "Iteration 102, loss = 0.04296120\n",
      "Iteration 103, loss = 0.04280889\n",
      "Iteration 104, loss = 0.04268546\n",
      "Iteration 105, loss = 0.04258744\n",
      "Iteration 106, loss = 0.04250342\n",
      "Iteration 107, loss = 0.04242847\n",
      "Iteration 108, loss = 0.04234824\n",
      "Iteration 109, loss = 0.04226233\n",
      "Iteration 110, loss = 0.04214979\n",
      "Iteration 111, loss = 0.04202435\n",
      "Iteration 112, loss = 0.04189019\n",
      "Iteration 113, loss = 0.04177077\n",
      "Iteration 114, loss = 0.04167401\n",
      "Iteration 115, loss = 0.04159770\n",
      "Iteration 116, loss = 0.04153435\n",
      "Iteration 117, loss = 0.04147576\n",
      "Iteration 118, loss = 0.04141908\n",
      "Iteration 119, loss = 0.04135088\n",
      "Iteration 120, loss = 0.04127556\n",
      "Iteration 121, loss = 0.04117911\n",
      "Iteration 122, loss = 0.04107739\n",
      "Iteration 123, loss = 0.04097484\n",
      "Iteration 124, loss = 0.04088466\n",
      "Iteration 125, loss = 0.04081008\n",
      "Iteration 126, loss = 0.04074758\n",
      "Iteration 127, loss = 0.04069447\n",
      "Iteration 128, loss = 0.04064581\n",
      "Iteration 129, loss = 0.04060159\n",
      "Iteration 130, loss = 0.04055509\n",
      "Iteration 131, loss = 0.04051135\n",
      "Iteration 132, loss = 0.04045482\n",
      "Iteration 133, loss = 0.04039470\n",
      "Iteration 134, loss = 0.04031492\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49174295\n",
      "Iteration 2, loss = 1.24126068\n",
      "Iteration 3, loss = 1.08003592\n",
      "Iteration 4, loss = 0.99074829\n",
      "Iteration 5, loss = 0.92572514\n",
      "Iteration 6, loss = 0.86200977\n",
      "Iteration 7, loss = 0.79018830\n",
      "Iteration 8, loss = 0.72314797\n",
      "Iteration 9, loss = 0.67223048\n",
      "Iteration 10, loss = 0.63761892\n",
      "Iteration 11, loss = 0.60748155\n",
      "Iteration 12, loss = 0.57480771\n",
      "Iteration 13, loss = 0.54322351\n",
      "Iteration 14, loss = 0.51570977\n",
      "Iteration 15, loss = 0.49079658\n",
      "Iteration 16, loss = 0.46879340\n",
      "Iteration 17, loss = 0.45211262\n",
      "Iteration 18, loss = 0.43808782\n",
      "Iteration 19, loss = 0.42196414\n",
      "Iteration 20, loss = 0.40476936\n",
      "Iteration 21, loss = 0.39067168\n",
      "Iteration 22, loss = 0.37735072\n",
      "Iteration 23, loss = 0.36370710\n",
      "Iteration 24, loss = 0.35198873\n",
      "Iteration 25, loss = 0.34140726\n",
      "Iteration 26, loss = 0.32954658\n",
      "Iteration 27, loss = 0.31790075\n",
      "Iteration 28, loss = 0.30719520\n",
      "Iteration 29, loss = 0.29571552\n",
      "Iteration 30, loss = 0.28432361\n",
      "Iteration 31, loss = 0.27358505\n",
      "Iteration 32, loss = 0.26333058\n",
      "Iteration 33, loss = 0.25381338\n",
      "Iteration 34, loss = 0.24496929\n",
      "Iteration 35, loss = 0.23560205\n",
      "Iteration 36, loss = 0.22649009\n",
      "Iteration 37, loss = 0.21769176\n",
      "Iteration 38, loss = 0.20894313\n",
      "Iteration 39, loss = 0.20084348\n",
      "Iteration 40, loss = 0.19272588\n",
      "Iteration 41, loss = 0.18523505\n",
      "Iteration 42, loss = 0.17771984\n",
      "Iteration 43, loss = 0.17078927\n",
      "Iteration 44, loss = 0.16386945\n",
      "Iteration 45, loss = 0.15751986\n",
      "Iteration 46, loss = 0.15120906\n",
      "Iteration 47, loss = 0.14547107\n",
      "Iteration 48, loss = 0.13994591\n",
      "Iteration 49, loss = 0.13476314\n",
      "Iteration 50, loss = 0.13007937\n",
      "Iteration 51, loss = 0.12557895\n",
      "Iteration 52, loss = 0.12140029\n",
      "Iteration 53, loss = 0.11768898\n",
      "Iteration 54, loss = 0.11423461\n",
      "Iteration 55, loss = 0.11090947\n",
      "Iteration 56, loss = 0.10792415\n",
      "Iteration 57, loss = 0.10530496\n",
      "Iteration 58, loss = 0.10291689\n",
      "Iteration 59, loss = 0.10069614\n",
      "Iteration 60, loss = 0.09856394\n",
      "Iteration 61, loss = 0.09662404\n",
      "Iteration 62, loss = 0.09492928\n",
      "Iteration 63, loss = 0.09345696\n",
      "Iteration 64, loss = 0.09216177\n",
      "Iteration 65, loss = 0.09095931\n",
      "Iteration 66, loss = 0.08975817\n",
      "Iteration 67, loss = 0.08853454\n",
      "Iteration 68, loss = 0.08749964\n",
      "Iteration 69, loss = 0.08671491\n",
      "Iteration 70, loss = 0.08605243\n",
      "Iteration 71, loss = 0.08536945\n",
      "Iteration 72, loss = 0.08458904\n",
      "Iteration 73, loss = 0.08386164\n",
      "Iteration 74, loss = 0.08331282\n",
      "Iteration 75, loss = 0.08290470\n",
      "Iteration 76, loss = 0.08252192\n",
      "Iteration 77, loss = 0.08206187\n",
      "Iteration 78, loss = 0.08155246\n",
      "Iteration 79, loss = 0.08108642\n",
      "Iteration 80, loss = 0.08074123\n",
      "Iteration 81, loss = 0.08048770\n",
      "Iteration 82, loss = 0.08024293\n",
      "Iteration 83, loss = 0.07995865\n",
      "Iteration 84, loss = 0.07961452\n",
      "Iteration 85, loss = 0.07928076\n",
      "Iteration 86, loss = 0.07900922\n",
      "Iteration 87, loss = 0.07880824\n",
      "Iteration 88, loss = 0.07864860\n",
      "Iteration 89, loss = 0.07849338\n",
      "Iteration 90, loss = 0.07832118\n",
      "Iteration 91, loss = 0.07810692\n",
      "Iteration 92, loss = 0.07787483\n",
      "Iteration 93, loss = 0.07765187\n",
      "Iteration 94, loss = 0.07746752\n",
      "Iteration 95, loss = 0.07732295\n",
      "Iteration 96, loss = 0.07720533\n",
      "Iteration 97, loss = 0.07710246\n",
      "Iteration 98, loss = 0.07699939\n",
      "Iteration 99, loss = 0.07689395\n",
      "Iteration 100, loss = 0.07675905\n",
      "Iteration 101, loss = 0.07660413\n",
      "Iteration 102, loss = 0.07642431\n",
      "Iteration 103, loss = 0.07625321\n",
      "Iteration 104, loss = 0.07610705\n",
      "Iteration 105, loss = 0.07599184\n",
      "Iteration 106, loss = 0.07590035\n",
      "Iteration 107, loss = 0.07582245\n",
      "Iteration 108, loss = 0.07575302\n",
      "Iteration 109, loss = 0.07568055\n",
      "Iteration 110, loss = 0.07560601\n",
      "Iteration 111, loss = 0.07550391\n",
      "Iteration 112, loss = 0.07538288\n",
      "Iteration 113, loss = 0.07523506\n",
      "Iteration 114, loss = 0.07509008\n",
      "Iteration 115, loss = 0.07496290\n",
      "Iteration 116, loss = 0.07486322\n",
      "Iteration 117, loss = 0.07478695\n",
      "Iteration 118, loss = 0.07472524\n",
      "Iteration 119, loss = 0.07467098\n",
      "Iteration 120, loss = 0.07461422\n",
      "Iteration 121, loss = 0.07455665\n",
      "Iteration 122, loss = 0.07448028\n",
      "Iteration 123, loss = 0.07439324\n",
      "Iteration 124, loss = 0.07428245\n",
      "Iteration 125, loss = 0.07416768\n",
      "Iteration 126, loss = 0.07405239\n",
      "Iteration 127, loss = 0.07394993\n",
      "Iteration 128, loss = 0.07386205\n",
      "Iteration 129, loss = 0.07378727\n",
      "Iteration 130, loss = 0.07372266\n",
      "Iteration 131, loss = 0.07366758\n",
      "Iteration 132, loss = 0.07362742\n",
      "Iteration 133, loss = 0.07361093\n",
      "Iteration 134, loss = 0.07364653\n",
      "Iteration 135, loss = 0.07372477\n",
      "Iteration 136, loss = 0.07385793\n",
      "Iteration 137, loss = 0.07381074\n",
      "Iteration 138, loss = 0.07357509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48762646\n",
      "Iteration 2, loss = 1.35470632\n",
      "Iteration 3, loss = 1.20781357\n",
      "Iteration 4, loss = 1.09587557\n",
      "Iteration 5, loss = 1.03315165\n",
      "Iteration 6, loss = 0.98464934\n",
      "Iteration 7, loss = 0.92354956\n",
      "Iteration 8, loss = 0.85782788\n",
      "Iteration 9, loss = 0.79882466\n",
      "Iteration 10, loss = 0.74802128\n",
      "Iteration 11, loss = 0.70342928\n",
      "Iteration 12, loss = 0.66459326\n",
      "Iteration 13, loss = 0.63102446\n",
      "Iteration 14, loss = 0.60216382\n",
      "Iteration 15, loss = 0.57663847\n",
      "Iteration 16, loss = 0.55364079\n",
      "Iteration 17, loss = 0.53281372\n",
      "Iteration 18, loss = 0.51406284\n",
      "Iteration 19, loss = 0.49723402\n",
      "Iteration 20, loss = 0.48210990\n",
      "Iteration 21, loss = 0.46843593\n",
      "Iteration 22, loss = 0.45594942\n",
      "Iteration 23, loss = 0.44440323\n",
      "Iteration 24, loss = 0.43370869\n",
      "Iteration 25, loss = 0.42356145\n",
      "Iteration 26, loss = 0.41456710\n",
      "Iteration 27, loss = 0.40653777\n",
      "Iteration 28, loss = 0.39899485\n",
      "Iteration 29, loss = 0.39180649\n",
      "Iteration 30, loss = 0.38489487\n",
      "Iteration 31, loss = 0.37813147\n",
      "Iteration 32, loss = 0.37156679\n",
      "Iteration 33, loss = 0.36523220\n",
      "Iteration 34, loss = 0.35913518\n",
      "Iteration 35, loss = 0.35320117\n",
      "Iteration 36, loss = 0.34770778\n",
      "Iteration 37, loss = 0.34261430\n",
      "Iteration 38, loss = 0.33782372\n",
      "Iteration 39, loss = 0.33323810\n",
      "Iteration 40, loss = 0.32878406\n",
      "Iteration 41, loss = 0.32445831\n",
      "Iteration 42, loss = 0.32024272\n",
      "Iteration 43, loss = 0.31613889\n",
      "Iteration 44, loss = 0.31214770\n",
      "Iteration 45, loss = 0.30826688\n",
      "Iteration 46, loss = 0.30447437\n",
      "Iteration 47, loss = 0.30076215\n",
      "Iteration 48, loss = 0.29712786\n",
      "Iteration 49, loss = 0.29357146\n",
      "Iteration 50, loss = 0.29009289\n",
      "Iteration 51, loss = 0.28669362\n",
      "Iteration 52, loss = 0.28337217\n",
      "Iteration 53, loss = 0.28012181\n",
      "Iteration 54, loss = 0.27694409\n",
      "Iteration 55, loss = 0.27383114\n",
      "Iteration 56, loss = 0.27077981\n",
      "Iteration 57, loss = 0.26779569\n",
      "Iteration 58, loss = 0.26488105\n",
      "Iteration 59, loss = 0.26200837\n",
      "Iteration 60, loss = 0.25917071\n",
      "Iteration 61, loss = 0.25634901\n",
      "Iteration 62, loss = 0.25354144\n",
      "Iteration 63, loss = 0.25072014\n",
      "Iteration 64, loss = 0.24787371\n",
      "Iteration 65, loss = 0.24504640\n",
      "Iteration 66, loss = 0.24218877\n",
      "Iteration 67, loss = 0.23922628\n",
      "Iteration 68, loss = 0.23667674\n",
      "Iteration 69, loss = 0.23444236\n",
      "Iteration 70, loss = 0.23234349\n",
      "Iteration 71, loss = 0.23031154\n",
      "Iteration 72, loss = 0.22830967\n",
      "Iteration 73, loss = 0.22633099\n",
      "Iteration 74, loss = 0.22437875\n",
      "Iteration 75, loss = 0.22245522\n",
      "Iteration 76, loss = 0.22056326\n",
      "Iteration 77, loss = 0.21869545\n",
      "Iteration 78, loss = 0.21685534\n",
      "Iteration 79, loss = 0.21504427\n",
      "Iteration 80, loss = 0.21326380\n",
      "Iteration 81, loss = 0.21151474\n",
      "Iteration 82, loss = 0.20979742\n",
      "Iteration 83, loss = 0.20810975\n",
      "Iteration 84, loss = 0.20645220\n",
      "Iteration 85, loss = 0.20482562\n",
      "Iteration 86, loss = 0.20322813\n",
      "Iteration 87, loss = 0.20166329\n",
      "Iteration 88, loss = 0.20012990\n",
      "Iteration 89, loss = 0.19864089\n",
      "Iteration 90, loss = 0.19718277\n",
      "Iteration 91, loss = 0.19575930\n",
      "Iteration 92, loss = 0.19437535\n",
      "Iteration 93, loss = 0.19302272\n",
      "Iteration 94, loss = 0.19169834\n",
      "Iteration 95, loss = 0.19039742\n",
      "Iteration 96, loss = 0.18911877\n",
      "Iteration 97, loss = 0.18786130\n",
      "Iteration 98, loss = 0.18662491\n",
      "Iteration 99, loss = 0.18540946\n",
      "Iteration 100, loss = 0.18421483\n",
      "Iteration 101, loss = 0.18304223\n",
      "Iteration 102, loss = 0.18189437\n",
      "Iteration 103, loss = 0.18076842\n",
      "Iteration 104, loss = 0.17966327\n",
      "Iteration 105, loss = 0.17857889\n",
      "Iteration 106, loss = 0.17751566\n",
      "Iteration 107, loss = 0.17647227\n",
      "Iteration 108, loss = 0.17545046\n",
      "Iteration 109, loss = 0.17444579\n",
      "Iteration 110, loss = 0.17345814\n",
      "Iteration 111, loss = 0.17248731\n",
      "Iteration 112, loss = 0.17153381\n",
      "Iteration 113, loss = 0.17059709\n",
      "Iteration 114, loss = 0.16967832\n",
      "Iteration 115, loss = 0.16877558\n",
      "Iteration 116, loss = 0.16788849\n",
      "Iteration 117, loss = 0.16701660\n",
      "Iteration 118, loss = 0.16615952\n",
      "Iteration 119, loss = 0.16531696\n",
      "Iteration 120, loss = 0.16448862\n",
      "Iteration 121, loss = 0.16367414\n",
      "Iteration 122, loss = 0.16287323\n",
      "Iteration 123, loss = 0.16208564\n",
      "Iteration 124, loss = 0.16131142\n",
      "Iteration 125, loss = 0.16055011\n",
      "Iteration 126, loss = 0.15980103\n",
      "Iteration 127, loss = 0.15906431\n",
      "Iteration 128, loss = 0.15833978\n",
      "Iteration 129, loss = 0.15762712\n",
      "Iteration 130, loss = 0.15692569\n",
      "Iteration 131, loss = 0.15623582\n",
      "Iteration 132, loss = 0.15555761\n",
      "Iteration 133, loss = 0.15488966\n",
      "Iteration 134, loss = 0.15423252\n",
      "Iteration 135, loss = 0.15358580\n",
      "Iteration 136, loss = 0.15294896\n",
      "Iteration 137, loss = 0.15232217\n",
      "Iteration 138, loss = 0.15170463\n",
      "Iteration 139, loss = 0.15109633\n",
      "Iteration 140, loss = 0.15049815\n",
      "Iteration 141, loss = 0.14990787\n",
      "Iteration 142, loss = 0.14932722\n",
      "Iteration 143, loss = 0.14875553\n",
      "Iteration 144, loss = 0.14819220\n",
      "Iteration 145, loss = 0.14763718\n",
      "Iteration 146, loss = 0.14709045\n",
      "Iteration 147, loss = 0.14655166\n",
      "Iteration 148, loss = 0.14602083\n",
      "Iteration 149, loss = 0.14549780\n",
      "Iteration 150, loss = 0.14498286\n",
      "Iteration 151, loss = 0.14447593\n",
      "Iteration 152, loss = 0.14397604\n",
      "Iteration 153, loss = 0.14348312\n",
      "Iteration 154, loss = 0.14299706\n",
      "Iteration 155, loss = 0.14251803\n",
      "Iteration 156, loss = 0.14204526\n",
      "Iteration 157, loss = 0.14157943\n",
      "Iteration 158, loss = 0.14112018\n",
      "Iteration 159, loss = 0.14066717\n",
      "Iteration 160, loss = 0.14022035\n",
      "Iteration 161, loss = 0.13977966\n",
      "Iteration 162, loss = 0.13934494\n",
      "Iteration 163, loss = 0.13891598\n",
      "Iteration 164, loss = 0.13849298\n",
      "Iteration 165, loss = 0.13807596\n",
      "Iteration 166, loss = 0.13766265\n",
      "Iteration 167, loss = 0.13725359\n",
      "Iteration 168, loss = 0.13684990\n",
      "Iteration 169, loss = 0.13645128\n",
      "Iteration 170, loss = 0.13605801\n",
      "Iteration 171, loss = 0.13567015\n",
      "Iteration 172, loss = 0.13528622\n",
      "Iteration 173, loss = 0.13490682\n",
      "Iteration 174, loss = 0.13453607\n",
      "Iteration 175, loss = 0.13417061\n",
      "Iteration 176, loss = 0.13381003\n",
      "Iteration 177, loss = 0.13345494\n",
      "Iteration 178, loss = 0.13310473\n",
      "Iteration 179, loss = 0.13275903\n",
      "Iteration 180, loss = 0.13241778\n",
      "Iteration 181, loss = 0.13208044\n",
      "Iteration 182, loss = 0.13174707\n",
      "Iteration 183, loss = 0.13141711\n",
      "Iteration 184, loss = 0.13109111\n",
      "Iteration 185, loss = 0.13076852\n",
      "Iteration 186, loss = 0.13044963\n",
      "Iteration 187, loss = 0.13013455\n",
      "Iteration 188, loss = 0.12982313\n",
      "Iteration 189, loss = 0.12951553\n",
      "Iteration 190, loss = 0.12921158\n",
      "Iteration 191, loss = 0.12891101\n",
      "Iteration 192, loss = 0.12861436\n",
      "Iteration 193, loss = 0.12832075\n",
      "Iteration 194, loss = 0.12803107\n",
      "Iteration 195, loss = 0.12774445\n",
      "Iteration 196, loss = 0.12746117\n",
      "Iteration 197, loss = 0.12718105\n",
      "Iteration 198, loss = 0.12690398\n",
      "Iteration 199, loss = 0.12663012\n",
      "Iteration 200, loss = 0.12635923\n",
      "Iteration 201, loss = 0.12609126\n",
      "Iteration 202, loss = 0.12582630\n",
      "Iteration 203, loss = 0.12556422\n",
      "Iteration 204, loss = 0.12530483\n",
      "Iteration 205, loss = 0.12504828\n",
      "Iteration 206, loss = 0.12479469\n",
      "Iteration 207, loss = 0.12454364\n",
      "Iteration 208, loss = 0.12429523\n",
      "Iteration 209, loss = 0.12404942\n",
      "Iteration 210, loss = 0.12380631\n",
      "Iteration 211, loss = 0.12356571\n",
      "Iteration 212, loss = 0.12332755\n",
      "Iteration 213, loss = 0.12309198\n",
      "Iteration 214, loss = 0.12285878\n",
      "Iteration 215, loss = 0.12262804\n",
      "Iteration 216, loss = 0.12239980\n",
      "Iteration 217, loss = 0.12217376\n",
      "Iteration 218, loss = 0.12195004\n",
      "Iteration 219, loss = 0.12172857\n",
      "Iteration 220, loss = 0.12150937\n",
      "Iteration 221, loss = 0.12129230\n",
      "Iteration 222, loss = 0.12107742\n",
      "Iteration 223, loss = 0.12086468\n",
      "Iteration 224, loss = 0.12065404\n",
      "Iteration 225, loss = 0.12044550\n",
      "Iteration 226, loss = 0.12023907\n",
      "Iteration 227, loss = 0.12003451\n",
      "Iteration 228, loss = 0.11983208\n",
      "Iteration 229, loss = 0.11963149\n",
      "Iteration 230, loss = 0.11943288\n",
      "Iteration 231, loss = 0.11923620\n",
      "Iteration 232, loss = 0.11904134\n",
      "Iteration 233, loss = 0.11884838\n",
      "Iteration 234, loss = 0.11865719\n",
      "Iteration 235, loss = 0.11846777\n",
      "Iteration 236, loss = 0.11828020\n",
      "Iteration 237, loss = 0.11809430\n",
      "Iteration 238, loss = 0.11791010\n",
      "Iteration 239, loss = 0.11772768\n",
      "Iteration 240, loss = 0.11754684\n",
      "Iteration 241, loss = 0.11736772\n",
      "Iteration 242, loss = 0.11719017\n",
      "Iteration 243, loss = 0.11701429\n",
      "Iteration 244, loss = 0.11683994\n",
      "Iteration 245, loss = 0.11666715\n",
      "Iteration 246, loss = 0.11649598\n",
      "Iteration 247, loss = 0.11632625\n",
      "Iteration 248, loss = 0.11615801\n",
      "Iteration 249, loss = 0.11599131\n",
      "Iteration 250, loss = 0.11582604\n",
      "Iteration 251, loss = 0.11566232\n",
      "Iteration 252, loss = 0.11549990\n",
      "Iteration 253, loss = 0.11533901\n",
      "Iteration 254, loss = 0.11517948\n",
      "Iteration 255, loss = 0.11502121\n",
      "Iteration 256, loss = 0.11486443\n",
      "Iteration 257, loss = 0.11470886\n",
      "Iteration 258, loss = 0.11455468\n",
      "Iteration 259, loss = 0.11440173\n",
      "Iteration 260, loss = 0.11425009\n",
      "Iteration 261, loss = 0.11409969\n",
      "Iteration 262, loss = 0.11395055\n",
      "Iteration 263, loss = 0.11380264\n",
      "Iteration 264, loss = 0.11365602\n",
      "Iteration 265, loss = 0.11351050\n",
      "Iteration 266, loss = 0.11336653\n",
      "Iteration 267, loss = 0.11322373\n",
      "Iteration 268, loss = 0.11308199\n",
      "Iteration 269, loss = 0.11294136\n",
      "Iteration 270, loss = 0.11280183\n",
      "Iteration 271, loss = 0.11266336\n",
      "Iteration 272, loss = 0.11252619\n",
      "Iteration 273, loss = 0.11238981\n",
      "Iteration 274, loss = 0.11225490\n",
      "Iteration 275, loss = 0.11212092\n",
      "Iteration 276, loss = 0.11198805\n",
      "Iteration 277, loss = 0.11185610\n",
      "Iteration 278, loss = 0.11172536\n",
      "Iteration 279, loss = 0.11159554\n",
      "Iteration 280, loss = 0.11146681\n",
      "Iteration 281, loss = 0.11133910\n",
      "Iteration 282, loss = 0.11121231\n",
      "Iteration 283, loss = 0.11108656\n",
      "Iteration 284, loss = 0.11096180\n",
      "Iteration 285, loss = 0.11083798\n",
      "Iteration 286, loss = 0.11071513\n",
      "Iteration 287, loss = 0.11059319\n",
      "Iteration 288, loss = 0.11047211\n",
      "Iteration 289, loss = 0.11035208\n",
      "Iteration 290, loss = 0.11023287\n",
      "Iteration 291, loss = 0.11011458\n",
      "Iteration 292, loss = 0.10999719\n",
      "Iteration 293, loss = 0.10988068\n",
      "Iteration 294, loss = 0.10976497\n",
      "Iteration 295, loss = 0.10965020\n",
      "Iteration 296, loss = 0.10953622\n",
      "Iteration 297, loss = 0.10942319\n",
      "Iteration 298, loss = 0.10931087\n",
      "Iteration 299, loss = 0.10919938\n",
      "Iteration 300, loss = 0.10908870\n",
      "Iteration 301, loss = 0.10897891\n",
      "Iteration 302, loss = 0.10886990\n",
      "Iteration 303, loss = 0.10876161\n",
      "Iteration 304, loss = 0.10865409\n",
      "Iteration 305, loss = 0.10854747\n",
      "Iteration 306, loss = 0.10844146\n",
      "Iteration 307, loss = 0.10833627\n",
      "Iteration 308, loss = 0.10823188\n",
      "Iteration 309, loss = 0.10812824\n",
      "Iteration 310, loss = 0.10802518\n",
      "Iteration 311, loss = 0.10792296\n",
      "Iteration 312, loss = 0.10782136\n",
      "Iteration 313, loss = 0.10772067\n",
      "Iteration 314, loss = 0.10762056\n",
      "Iteration 315, loss = 0.10752112\n",
      "Iteration 316, loss = 0.10742231\n",
      "Iteration 317, loss = 0.10732426\n",
      "Iteration 318, loss = 0.10722697\n",
      "Iteration 319, loss = 0.10713017\n",
      "Iteration 320, loss = 0.10703415\n",
      "Iteration 321, loss = 0.10693879\n",
      "Iteration 322, loss = 0.10684394\n",
      "Iteration 323, loss = 0.10674981\n",
      "Iteration 324, loss = 0.10665638\n",
      "Iteration 325, loss = 0.10656355\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47705517\n",
      "Iteration 2, loss = 1.35012459\n",
      "Iteration 3, loss = 1.20900168\n",
      "Iteration 4, loss = 1.10086237\n",
      "Iteration 5, loss = 1.03876308\n",
      "Iteration 6, loss = 0.99083615\n",
      "Iteration 7, loss = 0.93024279\n",
      "Iteration 8, loss = 0.86428215\n",
      "Iteration 9, loss = 0.80562693\n",
      "Iteration 10, loss = 0.75430211\n",
      "Iteration 11, loss = 0.70945946\n",
      "Iteration 12, loss = 0.67056661\n",
      "Iteration 13, loss = 0.63705574\n",
      "Iteration 14, loss = 0.60808229\n",
      "Iteration 15, loss = 0.58211260\n",
      "Iteration 16, loss = 0.55850394\n",
      "Iteration 17, loss = 0.53705944\n",
      "Iteration 18, loss = 0.51762158\n",
      "Iteration 19, loss = 0.49991340\n",
      "Iteration 20, loss = 0.48376981\n",
      "Iteration 21, loss = 0.46895494\n",
      "Iteration 22, loss = 0.45571524\n",
      "Iteration 23, loss = 0.44433375\n",
      "Iteration 24, loss = 0.43395336\n",
      "Iteration 25, loss = 0.42426073\n",
      "Iteration 26, loss = 0.41511635\n",
      "Iteration 27, loss = 0.40645602\n",
      "Iteration 28, loss = 0.39825066\n",
      "Iteration 29, loss = 0.39049060\n",
      "Iteration 30, loss = 0.38314781\n",
      "Iteration 31, loss = 0.37611210\n",
      "Iteration 32, loss = 0.36934534\n",
      "Iteration 33, loss = 0.36282573\n",
      "Iteration 34, loss = 0.35658307\n",
      "Iteration 35, loss = 0.35040402\n",
      "Iteration 36, loss = 0.34447724\n",
      "Iteration 37, loss = 0.33904305\n",
      "Iteration 38, loss = 0.33400918\n",
      "Iteration 39, loss = 0.32922628\n",
      "Iteration 40, loss = 0.32460449\n",
      "Iteration 41, loss = 0.32010577\n",
      "Iteration 42, loss = 0.31571082\n",
      "Iteration 43, loss = 0.31142252\n",
      "Iteration 44, loss = 0.30723407\n",
      "Iteration 45, loss = 0.30314207\n",
      "Iteration 46, loss = 0.29914297\n",
      "Iteration 47, loss = 0.29523291\n",
      "Iteration 48, loss = 0.29141137\n",
      "Iteration 49, loss = 0.28767401\n",
      "Iteration 50, loss = 0.28402520\n",
      "Iteration 51, loss = 0.28046607\n",
      "Iteration 52, loss = 0.27698528\n",
      "Iteration 53, loss = 0.27358048\n",
      "Iteration 54, loss = 0.27024918\n",
      "Iteration 55, loss = 0.26699300\n",
      "Iteration 56, loss = 0.26380857\n",
      "Iteration 57, loss = 0.26069278\n",
      "Iteration 58, loss = 0.25764765\n",
      "Iteration 59, loss = 0.25467243\n",
      "Iteration 60, loss = 0.25175805\n",
      "Iteration 61, loss = 0.24888636\n",
      "Iteration 62, loss = 0.24604138\n",
      "Iteration 63, loss = 0.24321776\n",
      "Iteration 64, loss = 0.24039348\n",
      "Iteration 65, loss = 0.23753796\n",
      "Iteration 66, loss = 0.23461711\n",
      "Iteration 67, loss = 0.23176033\n",
      "Iteration 68, loss = 0.22879824\n",
      "Iteration 69, loss = 0.22609271\n",
      "Iteration 70, loss = 0.22381805\n",
      "Iteration 71, loss = 0.22167460\n",
      "Iteration 72, loss = 0.21960892\n",
      "Iteration 73, loss = 0.21757768\n",
      "Iteration 74, loss = 0.21557171\n",
      "Iteration 75, loss = 0.21359190\n",
      "Iteration 76, loss = 0.21164013\n",
      "Iteration 77, loss = 0.20971786\n",
      "Iteration 78, loss = 0.20782755\n",
      "Iteration 79, loss = 0.20596924\n",
      "Iteration 80, loss = 0.20414332\n",
      "Iteration 81, loss = 0.20234947\n",
      "Iteration 82, loss = 0.20058790\n",
      "Iteration 83, loss = 0.19885834\n",
      "Iteration 84, loss = 0.19716176\n",
      "Iteration 85, loss = 0.19549620\n",
      "Iteration 86, loss = 0.19386197\n",
      "Iteration 87, loss = 0.19225844\n",
      "Iteration 88, loss = 0.19068992\n",
      "Iteration 89, loss = 0.18915504\n",
      "Iteration 90, loss = 0.18766334\n",
      "Iteration 91, loss = 0.18620372\n",
      "Iteration 92, loss = 0.18478370\n",
      "Iteration 93, loss = 0.18340159\n",
      "Iteration 94, loss = 0.18204834\n",
      "Iteration 95, loss = 0.18072312\n",
      "Iteration 96, loss = 0.17942939\n",
      "Iteration 97, loss = 0.17815947\n",
      "Iteration 98, loss = 0.17691143\n",
      "Iteration 99, loss = 0.17568369\n",
      "Iteration 100, loss = 0.17447681\n",
      "Iteration 101, loss = 0.17329199\n",
      "Iteration 102, loss = 0.17213303\n",
      "Iteration 103, loss = 0.17099702\n",
      "Iteration 104, loss = 0.16988489\n",
      "Iteration 105, loss = 0.16879487\n",
      "Iteration 106, loss = 0.16772499\n",
      "Iteration 107, loss = 0.16667561\n",
      "Iteration 108, loss = 0.16564578\n",
      "Iteration 109, loss = 0.16463538\n",
      "Iteration 110, loss = 0.16364446\n",
      "Iteration 111, loss = 0.16267258\n",
      "Iteration 112, loss = 0.16171913\n",
      "Iteration 113, loss = 0.16078305\n",
      "Iteration 114, loss = 0.15986437\n",
      "Iteration 115, loss = 0.15896257\n",
      "Iteration 116, loss = 0.15807677\n",
      "Iteration 117, loss = 0.15720668\n",
      "Iteration 118, loss = 0.15635250\n",
      "Iteration 119, loss = 0.15551382\n",
      "Iteration 120, loss = 0.15468949\n",
      "Iteration 121, loss = 0.15387958\n",
      "Iteration 122, loss = 0.15308399\n",
      "Iteration 123, loss = 0.15230233\n",
      "Iteration 124, loss = 0.15153464\n",
      "Iteration 125, loss = 0.15077966\n",
      "Iteration 126, loss = 0.15003624\n",
      "Iteration 127, loss = 0.14930613\n",
      "Iteration 128, loss = 0.14858807\n",
      "Iteration 129, loss = 0.14788346\n",
      "Iteration 130, loss = 0.14719111\n",
      "Iteration 131, loss = 0.14651084\n",
      "Iteration 132, loss = 0.14584216\n",
      "Iteration 133, loss = 0.14518556\n",
      "Iteration 134, loss = 0.14454005\n",
      "Iteration 135, loss = 0.14390544\n",
      "Iteration 136, loss = 0.14328112\n",
      "Iteration 137, loss = 0.14266701\n",
      "Iteration 138, loss = 0.14206306\n",
      "Iteration 139, loss = 0.14146913\n",
      "Iteration 140, loss = 0.14088465\n",
      "Iteration 141, loss = 0.14030968\n",
      "Iteration 142, loss = 0.13974393\n",
      "Iteration 143, loss = 0.13918694\n",
      "Iteration 144, loss = 0.13863913\n",
      "Iteration 145, loss = 0.13809939\n",
      "Iteration 146, loss = 0.13756793\n",
      "Iteration 147, loss = 0.13704297\n",
      "Iteration 148, loss = 0.13652359\n",
      "Iteration 149, loss = 0.13601198\n",
      "Iteration 150, loss = 0.13550782\n",
      "Iteration 151, loss = 0.13501137\n",
      "Iteration 152, loss = 0.13452137\n",
      "Iteration 153, loss = 0.13403761\n",
      "Iteration 154, loss = 0.13356123\n",
      "Iteration 155, loss = 0.13309205\n",
      "Iteration 156, loss = 0.13263086\n",
      "Iteration 157, loss = 0.13218091\n",
      "Iteration 158, loss = 0.13174033\n",
      "Iteration 159, loss = 0.13130876\n",
      "Iteration 160, loss = 0.13088039\n",
      "Iteration 161, loss = 0.13045409\n",
      "Iteration 162, loss = 0.13003115\n",
      "Iteration 163, loss = 0.12961347\n",
      "Iteration 164, loss = 0.12920399\n",
      "Iteration 165, loss = 0.12880263\n",
      "Iteration 166, loss = 0.12840475\n",
      "Iteration 167, loss = 0.12801328\n",
      "Iteration 168, loss = 0.12762485\n",
      "Iteration 169, loss = 0.12725320\n",
      "Iteration 170, loss = 0.12688527\n",
      "Iteration 171, loss = 0.12652119\n",
      "Iteration 172, loss = 0.12616040\n",
      "Iteration 173, loss = 0.12580321\n",
      "Iteration 174, loss = 0.12544946\n",
      "Iteration 175, loss = 0.12509938\n",
      "Iteration 176, loss = 0.12475415\n",
      "Iteration 177, loss = 0.12441269\n",
      "Iteration 178, loss = 0.12407612\n",
      "Iteration 179, loss = 0.12374663\n",
      "Iteration 180, loss = 0.12342019\n",
      "Iteration 181, loss = 0.12309907\n",
      "Iteration 182, loss = 0.12278228\n",
      "Iteration 183, loss = 0.12246793\n",
      "Iteration 184, loss = 0.12215957\n",
      "Iteration 185, loss = 0.12185273\n",
      "Iteration 186, loss = 0.12155101\n",
      "Iteration 187, loss = 0.12125276\n",
      "Iteration 188, loss = 0.12095828\n",
      "Iteration 189, loss = 0.12066788\n",
      "Iteration 190, loss = 0.12037974\n",
      "Iteration 191, loss = 0.12009583\n",
      "Iteration 192, loss = 0.11981541\n",
      "Iteration 193, loss = 0.11953825\n",
      "Iteration 194, loss = 0.11926393\n",
      "Iteration 195, loss = 0.11899318\n",
      "Iteration 196, loss = 0.11872607\n",
      "Iteration 197, loss = 0.11846141\n",
      "Iteration 198, loss = 0.11820019\n",
      "Iteration 199, loss = 0.11794179\n",
      "Iteration 200, loss = 0.11768671\n",
      "Iteration 201, loss = 0.11743425\n",
      "Iteration 202, loss = 0.11718476\n",
      "Iteration 203, loss = 0.11693800\n",
      "Iteration 204, loss = 0.11669445\n",
      "Iteration 205, loss = 0.11645310\n",
      "Iteration 206, loss = 0.11621460\n",
      "Iteration 207, loss = 0.11597905\n",
      "Iteration 208, loss = 0.11574576\n",
      "Iteration 209, loss = 0.11551518\n",
      "Iteration 210, loss = 0.11528709\n",
      "Iteration 211, loss = 0.11506168\n",
      "Iteration 212, loss = 0.11483842\n",
      "Iteration 213, loss = 0.11461767\n",
      "Iteration 214, loss = 0.11439952\n",
      "Iteration 215, loss = 0.11418335\n",
      "Iteration 216, loss = 0.11396972\n",
      "Iteration 217, loss = 0.11375829\n",
      "Iteration 218, loss = 0.11354890\n",
      "Iteration 219, loss = 0.11334230\n",
      "Iteration 220, loss = 0.11313728\n",
      "Iteration 221, loss = 0.11293433\n",
      "Iteration 222, loss = 0.11273400\n",
      "Iteration 223, loss = 0.11253521\n",
      "Iteration 224, loss = 0.11233902\n",
      "Iteration 225, loss = 0.11214436\n",
      "Iteration 226, loss = 0.11195207\n",
      "Iteration 227, loss = 0.11176142\n",
      "Iteration 228, loss = 0.11157244\n",
      "Iteration 229, loss = 0.11138618\n",
      "Iteration 230, loss = 0.11120107\n",
      "Iteration 231, loss = 0.11101784\n",
      "Iteration 232, loss = 0.11083645\n",
      "Iteration 233, loss = 0.11065735\n",
      "Iteration 234, loss = 0.11047938\n",
      "Iteration 235, loss = 0.11030341\n",
      "Iteration 236, loss = 0.11012895\n",
      "Iteration 237, loss = 0.10995660\n",
      "Iteration 238, loss = 0.10978537\n",
      "Iteration 239, loss = 0.10961661\n",
      "Iteration 240, loss = 0.10944871\n",
      "Iteration 241, loss = 0.10928244\n",
      "Iteration 242, loss = 0.10911797\n",
      "Iteration 243, loss = 0.10895506\n",
      "Iteration 244, loss = 0.10879344\n",
      "Iteration 245, loss = 0.10863374\n",
      "Iteration 246, loss = 0.10847506\n",
      "Iteration 247, loss = 0.10831854\n",
      "Iteration 248, loss = 0.10816275\n",
      "Iteration 249, loss = 0.10800878\n",
      "Iteration 250, loss = 0.10785587\n",
      "Iteration 251, loss = 0.10770493\n",
      "Iteration 252, loss = 0.10755469\n",
      "Iteration 253, loss = 0.10740657\n",
      "Iteration 254, loss = 0.10725923\n",
      "Iteration 255, loss = 0.10711308\n",
      "Iteration 256, loss = 0.10696880\n",
      "Iteration 257, loss = 0.10682519\n",
      "Iteration 258, loss = 0.10668364\n",
      "Iteration 259, loss = 0.10654250\n",
      "Iteration 260, loss = 0.10640308\n",
      "Iteration 261, loss = 0.10626458\n",
      "Iteration 262, loss = 0.10612746\n",
      "Iteration 263, loss = 0.10599147\n",
      "Iteration 264, loss = 0.10585655\n",
      "Iteration 265, loss = 0.10572319\n",
      "Iteration 266, loss = 0.10559048\n",
      "Iteration 267, loss = 0.10545922\n",
      "Iteration 268, loss = 0.10532900\n",
      "Iteration 269, loss = 0.10519985\n",
      "Iteration 270, loss = 0.10507188\n",
      "Iteration 271, loss = 0.10494476\n",
      "Iteration 272, loss = 0.10481922\n",
      "Iteration 273, loss = 0.10469400\n",
      "Iteration 274, loss = 0.10457043\n",
      "Iteration 275, loss = 0.10444757\n",
      "Iteration 276, loss = 0.10432561\n",
      "Iteration 277, loss = 0.10420517\n",
      "Iteration 278, loss = 0.10408507\n",
      "Iteration 279, loss = 0.10396631\n",
      "Iteration 280, loss = 0.10384853\n",
      "Iteration 281, loss = 0.10373150\n",
      "Iteration 282, loss = 0.10361553\n",
      "Iteration 283, loss = 0.10350058\n",
      "Iteration 284, loss = 0.10338630\n",
      "Iteration 285, loss = 0.10327328\n",
      "Iteration 286, loss = 0.10316078\n",
      "Iteration 287, loss = 0.10304924\n",
      "Iteration 288, loss = 0.10293902\n",
      "Iteration 289, loss = 0.10282923\n",
      "Iteration 290, loss = 0.10272017\n",
      "Iteration 291, loss = 0.10261265\n",
      "Iteration 292, loss = 0.10250501\n",
      "Iteration 293, loss = 0.10239916\n",
      "Iteration 294, loss = 0.10229361\n",
      "Iteration 295, loss = 0.10218875\n",
      "Iteration 296, loss = 0.10208459\n",
      "Iteration 297, loss = 0.10198214\n",
      "Iteration 298, loss = 0.10187925\n",
      "Iteration 299, loss = 0.10177802\n",
      "Iteration 300, loss = 0.10167719\n",
      "Iteration 301, loss = 0.10157700\n",
      "Iteration 302, loss = 0.10147760\n",
      "Iteration 303, loss = 0.10137924\n",
      "Iteration 304, loss = 0.10128127\n",
      "Iteration 305, loss = 0.10118423\n",
      "Iteration 306, loss = 0.10108786\n",
      "Iteration 307, loss = 0.10099205\n",
      "Iteration 308, loss = 0.10089722\n",
      "Iteration 309, loss = 0.10080286\n",
      "Iteration 310, loss = 0.10070915\n",
      "Iteration 311, loss = 0.10061638\n",
      "Iteration 312, loss = 0.10052416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48410645\n",
      "Iteration 2, loss = 1.35341740\n",
      "Iteration 3, loss = 1.20792366\n",
      "Iteration 4, loss = 1.09572296\n",
      "Iteration 5, loss = 1.03049913\n",
      "Iteration 6, loss = 0.97987460\n",
      "Iteration 7, loss = 0.91687234\n",
      "Iteration 8, loss = 0.84857942\n",
      "Iteration 9, loss = 0.78833216\n",
      "Iteration 10, loss = 0.73587537\n",
      "Iteration 11, loss = 0.69134850\n",
      "Iteration 12, loss = 0.65293663\n",
      "Iteration 13, loss = 0.61983510\n",
      "Iteration 14, loss = 0.59080553\n",
      "Iteration 15, loss = 0.56502403\n",
      "Iteration 16, loss = 0.54176874\n",
      "Iteration 17, loss = 0.52071651\n",
      "Iteration 18, loss = 0.50171320\n",
      "Iteration 19, loss = 0.48467500\n",
      "Iteration 20, loss = 0.46922343\n",
      "Iteration 21, loss = 0.45512917\n",
      "Iteration 22, loss = 0.44237131\n",
      "Iteration 23, loss = 0.43056063\n",
      "Iteration 24, loss = 0.41936744\n",
      "Iteration 25, loss = 0.40873882\n",
      "Iteration 26, loss = 0.39912983\n",
      "Iteration 27, loss = 0.39060738\n",
      "Iteration 28, loss = 0.38256480\n",
      "Iteration 29, loss = 0.37483506\n",
      "Iteration 30, loss = 0.36735091\n",
      "Iteration 31, loss = 0.36003122\n",
      "Iteration 32, loss = 0.35287234\n",
      "Iteration 33, loss = 0.34597079\n",
      "Iteration 34, loss = 0.33921183\n",
      "Iteration 35, loss = 0.33285729\n",
      "Iteration 36, loss = 0.32708105\n",
      "Iteration 37, loss = 0.32171098\n",
      "Iteration 38, loss = 0.31660789\n",
      "Iteration 39, loss = 0.31163589\n",
      "Iteration 40, loss = 0.30677245\n",
      "Iteration 41, loss = 0.30203617\n",
      "Iteration 42, loss = 0.29741090\n",
      "Iteration 43, loss = 0.29289233\n",
      "Iteration 44, loss = 0.28849403\n",
      "Iteration 45, loss = 0.28420021\n",
      "Iteration 46, loss = 0.28000177\n",
      "Iteration 47, loss = 0.27589779\n",
      "Iteration 48, loss = 0.27188732\n",
      "Iteration 49, loss = 0.26796930\n",
      "Iteration 50, loss = 0.26414680\n",
      "Iteration 51, loss = 0.26041650\n",
      "Iteration 52, loss = 0.25677904\n",
      "Iteration 53, loss = 0.25322279\n",
      "Iteration 54, loss = 0.24974658\n",
      "Iteration 55, loss = 0.24634674\n",
      "Iteration 56, loss = 0.24302609\n",
      "Iteration 57, loss = 0.23978408\n",
      "Iteration 58, loss = 0.23660400\n",
      "Iteration 59, loss = 0.23348343\n",
      "Iteration 60, loss = 0.23040738\n",
      "Iteration 61, loss = 0.22735336\n",
      "Iteration 62, loss = 0.22430125\n",
      "Iteration 63, loss = 0.22121824\n",
      "Iteration 64, loss = 0.21813218\n",
      "Iteration 65, loss = 0.21509115\n",
      "Iteration 66, loss = 0.21201185\n",
      "Iteration 67, loss = 0.20911860\n",
      "Iteration 68, loss = 0.20663741\n",
      "Iteration 69, loss = 0.20432730\n",
      "Iteration 70, loss = 0.20210528\n",
      "Iteration 71, loss = 0.19992370\n",
      "Iteration 72, loss = 0.19777302\n",
      "Iteration 73, loss = 0.19565534\n",
      "Iteration 74, loss = 0.19357067\n",
      "Iteration 75, loss = 0.19151997\n",
      "Iteration 76, loss = 0.18950307\n",
      "Iteration 77, loss = 0.18752290\n",
      "Iteration 78, loss = 0.18557973\n",
      "Iteration 79, loss = 0.18367345\n",
      "Iteration 80, loss = 0.18180541\n",
      "Iteration 81, loss = 0.17997302\n",
      "Iteration 82, loss = 0.17817802\n",
      "Iteration 83, loss = 0.17641804\n",
      "Iteration 84, loss = 0.17469308\n",
      "Iteration 85, loss = 0.17300381\n",
      "Iteration 86, loss = 0.17134969\n",
      "Iteration 87, loss = 0.16973395\n",
      "Iteration 88, loss = 0.16815940\n",
      "Iteration 89, loss = 0.16662593\n",
      "Iteration 90, loss = 0.16513121\n",
      "Iteration 91, loss = 0.16367807\n",
      "Iteration 92, loss = 0.16226381\n",
      "Iteration 93, loss = 0.16088411\n",
      "Iteration 94, loss = 0.15953091\n",
      "Iteration 95, loss = 0.15820259\n",
      "Iteration 96, loss = 0.15689776\n",
      "Iteration 97, loss = 0.15561646\n",
      "Iteration 98, loss = 0.15435827\n",
      "Iteration 99, loss = 0.15312581\n",
      "Iteration 100, loss = 0.15191745\n",
      "Iteration 101, loss = 0.15073656\n",
      "Iteration 102, loss = 0.14958031\n",
      "Iteration 103, loss = 0.14844779\n",
      "Iteration 104, loss = 0.14734048\n",
      "Iteration 105, loss = 0.14625568\n",
      "Iteration 106, loss = 0.14519289\n",
      "Iteration 107, loss = 0.14415222\n",
      "Iteration 108, loss = 0.14313060\n",
      "Iteration 109, loss = 0.14212794\n",
      "Iteration 110, loss = 0.14114429\n",
      "Iteration 111, loss = 0.14017996\n",
      "Iteration 112, loss = 0.13923516\n",
      "Iteration 113, loss = 0.13830808\n",
      "Iteration 114, loss = 0.13739885\n",
      "Iteration 115, loss = 0.13650708\n",
      "Iteration 116, loss = 0.13563184\n",
      "Iteration 117, loss = 0.13477255\n",
      "Iteration 118, loss = 0.13392909\n",
      "Iteration 119, loss = 0.13310096\n",
      "Iteration 120, loss = 0.13228768\n",
      "Iteration 121, loss = 0.13149039\n",
      "Iteration 122, loss = 0.13070624\n",
      "Iteration 123, loss = 0.12993590\n",
      "Iteration 124, loss = 0.12917936\n",
      "Iteration 125, loss = 0.12843686\n",
      "Iteration 126, loss = 0.12770827\n",
      "Iteration 127, loss = 0.12699229\n",
      "Iteration 128, loss = 0.12628908\n",
      "Iteration 129, loss = 0.12559896\n",
      "Iteration 130, loss = 0.12492003\n",
      "Iteration 131, loss = 0.12425345\n",
      "Iteration 132, loss = 0.12359865\n",
      "Iteration 133, loss = 0.12295551\n",
      "Iteration 134, loss = 0.12232301\n",
      "Iteration 135, loss = 0.12170136\n",
      "Iteration 136, loss = 0.12109054\n",
      "Iteration 137, loss = 0.12049031\n",
      "Iteration 138, loss = 0.11990066\n",
      "Iteration 139, loss = 0.11932023\n",
      "Iteration 140, loss = 0.11874986\n",
      "Iteration 141, loss = 0.11818853\n",
      "Iteration 142, loss = 0.11763662\n",
      "Iteration 143, loss = 0.11709353\n",
      "Iteration 144, loss = 0.11655915\n",
      "Iteration 145, loss = 0.11603359\n",
      "Iteration 146, loss = 0.11551502\n",
      "Iteration 147, loss = 0.11500279\n",
      "Iteration 148, loss = 0.11449769\n",
      "Iteration 149, loss = 0.11400059\n",
      "Iteration 150, loss = 0.11351097\n",
      "Iteration 151, loss = 0.11302887\n",
      "Iteration 152, loss = 0.11255198\n",
      "Iteration 153, loss = 0.11208424\n",
      "Iteration 154, loss = 0.11162768\n",
      "Iteration 155, loss = 0.11117909\n",
      "Iteration 156, loss = 0.11073951\n",
      "Iteration 157, loss = 0.11030791\n",
      "Iteration 158, loss = 0.10988285\n",
      "Iteration 159, loss = 0.10946344\n",
      "Iteration 160, loss = 0.10904935\n",
      "Iteration 161, loss = 0.10864041\n",
      "Iteration 162, loss = 0.10823646\n",
      "Iteration 163, loss = 0.10783797\n",
      "Iteration 164, loss = 0.10744542\n",
      "Iteration 165, loss = 0.10705827\n",
      "Iteration 166, loss = 0.10667647\n",
      "Iteration 167, loss = 0.10630021\n",
      "Iteration 168, loss = 0.10592956\n",
      "Iteration 169, loss = 0.10556495\n",
      "Iteration 170, loss = 0.10520548\n",
      "Iteration 171, loss = 0.10485101\n",
      "Iteration 172, loss = 0.10450125\n",
      "Iteration 173, loss = 0.10415630\n",
      "Iteration 174, loss = 0.10381578\n",
      "Iteration 175, loss = 0.10347986\n",
      "Iteration 176, loss = 0.10314832\n",
      "Iteration 177, loss = 0.10282106\n",
      "Iteration 178, loss = 0.10249817\n",
      "Iteration 179, loss = 0.10217934\n",
      "Iteration 180, loss = 0.10186474\n",
      "Iteration 181, loss = 0.10155408\n",
      "Iteration 182, loss = 0.10124773\n",
      "Iteration 183, loss = 0.10094511\n",
      "Iteration 184, loss = 0.10064629\n",
      "Iteration 185, loss = 0.10035133\n",
      "Iteration 186, loss = 0.10005986\n",
      "Iteration 187, loss = 0.09977202\n",
      "Iteration 188, loss = 0.09948756\n",
      "Iteration 189, loss = 0.09920673\n",
      "Iteration 190, loss = 0.09892922\n",
      "Iteration 191, loss = 0.09865507\n",
      "Iteration 192, loss = 0.09838436\n",
      "Iteration 193, loss = 0.09811679\n",
      "Iteration 194, loss = 0.09785251\n",
      "Iteration 195, loss = 0.09759131\n",
      "Iteration 196, loss = 0.09733335\n",
      "Iteration 197, loss = 0.09707833\n",
      "Iteration 198, loss = 0.09682637\n",
      "Iteration 199, loss = 0.09657756\n",
      "Iteration 200, loss = 0.09633162\n",
      "Iteration 201, loss = 0.09608860\n",
      "Iteration 202, loss = 0.09584832\n",
      "Iteration 203, loss = 0.09561084\n",
      "Iteration 204, loss = 0.09537609\n",
      "Iteration 205, loss = 0.09514401\n",
      "Iteration 206, loss = 0.09491464\n",
      "Iteration 207, loss = 0.09468775\n",
      "Iteration 208, loss = 0.09446348\n",
      "Iteration 209, loss = 0.09424169\n",
      "Iteration 210, loss = 0.09402240\n",
      "Iteration 211, loss = 0.09380555\n",
      "Iteration 212, loss = 0.09359103\n",
      "Iteration 213, loss = 0.09337897\n",
      "Iteration 214, loss = 0.09316914\n",
      "Iteration 215, loss = 0.09296165\n",
      "Iteration 216, loss = 0.09275635\n",
      "Iteration 217, loss = 0.09255331\n",
      "Iteration 218, loss = 0.09235241\n",
      "Iteration 219, loss = 0.09215369\n",
      "Iteration 220, loss = 0.09195708\n",
      "Iteration 221, loss = 0.09176251\n",
      "Iteration 222, loss = 0.09157010\n",
      "Iteration 223, loss = 0.09137957\n",
      "Iteration 224, loss = 0.09119111\n",
      "Iteration 225, loss = 0.09100454\n",
      "Iteration 226, loss = 0.09081996\n",
      "Iteration 227, loss = 0.09063723\n",
      "Iteration 228, loss = 0.09045640\n",
      "Iteration 229, loss = 0.09027740\n",
      "Iteration 230, loss = 0.09010021\n",
      "Iteration 231, loss = 0.08992485\n",
      "Iteration 232, loss = 0.08975118\n",
      "Iteration 233, loss = 0.08957935\n",
      "Iteration 234, loss = 0.08940913\n",
      "Iteration 235, loss = 0.08924065\n",
      "Iteration 236, loss = 0.08907380\n",
      "Iteration 237, loss = 0.08890863\n",
      "Iteration 238, loss = 0.08874501\n",
      "Iteration 239, loss = 0.08858308\n",
      "Iteration 240, loss = 0.08842262\n",
      "Iteration 241, loss = 0.08826383\n",
      "Iteration 242, loss = 0.08810647\n",
      "Iteration 243, loss = 0.08795060\n",
      "Iteration 244, loss = 0.08779632\n",
      "Iteration 245, loss = 0.08764353\n",
      "Iteration 246, loss = 0.08749223\n",
      "Iteration 247, loss = 0.08734239\n",
      "Iteration 248, loss = 0.08719396\n",
      "Iteration 249, loss = 0.08704689\n",
      "Iteration 250, loss = 0.08690126\n",
      "Iteration 251, loss = 0.08675691\n",
      "Iteration 252, loss = 0.08661397\n",
      "Iteration 253, loss = 0.08647227\n",
      "Iteration 254, loss = 0.08633192\n",
      "Iteration 255, loss = 0.08619281\n",
      "Iteration 256, loss = 0.08605501\n",
      "Iteration 257, loss = 0.08591841\n",
      "Iteration 258, loss = 0.08578310\n",
      "Iteration 259, loss = 0.08564895\n",
      "Iteration 260, loss = 0.08551604\n",
      "Iteration 261, loss = 0.08538433\n",
      "Iteration 262, loss = 0.08525384\n",
      "Iteration 263, loss = 0.08512455\n",
      "Iteration 264, loss = 0.08499648\n",
      "Iteration 265, loss = 0.08486947\n",
      "Iteration 266, loss = 0.08474364\n",
      "Iteration 267, loss = 0.08461885\n",
      "Iteration 268, loss = 0.08449521\n",
      "Iteration 269, loss = 0.08437262\n",
      "Iteration 270, loss = 0.08425114\n",
      "Iteration 271, loss = 0.08413066\n",
      "Iteration 272, loss = 0.08401127\n",
      "Iteration 273, loss = 0.08389283\n",
      "Iteration 274, loss = 0.08377542\n",
      "Iteration 275, loss = 0.08365897\n",
      "Iteration 276, loss = 0.08354356\n",
      "Iteration 277, loss = 0.08342908\n",
      "Iteration 278, loss = 0.08331556\n",
      "Iteration 279, loss = 0.08320296\n",
      "Iteration 280, loss = 0.08309136\n",
      "Iteration 281, loss = 0.08298061\n",
      "Iteration 282, loss = 0.08287080\n",
      "Iteration 283, loss = 0.08276193\n",
      "Iteration 284, loss = 0.08265390\n",
      "Iteration 285, loss = 0.08254675\n",
      "Iteration 286, loss = 0.08244051\n",
      "Iteration 287, loss = 0.08233515\n",
      "Iteration 288, loss = 0.08223060\n",
      "Iteration 289, loss = 0.08212686\n",
      "Iteration 290, loss = 0.08202394\n",
      "Iteration 291, loss = 0.08192184\n",
      "Iteration 292, loss = 0.08182055\n",
      "Iteration 293, loss = 0.08172003\n",
      "Iteration 294, loss = 0.08162038\n",
      "Iteration 295, loss = 0.08152158\n",
      "Iteration 296, loss = 0.08142348\n",
      "Iteration 297, loss = 0.08132614\n",
      "Iteration 298, loss = 0.08122955\n",
      "Iteration 299, loss = 0.08113368\n",
      "Iteration 300, loss = 0.08103870\n",
      "Iteration 301, loss = 0.08094421\n",
      "Iteration 302, loss = 0.08085054\n",
      "Iteration 303, loss = 0.08075757\n",
      "Iteration 304, loss = 0.08066529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48086253\n",
      "Iteration 2, loss = 1.35081329\n",
      "Iteration 3, loss = 1.20634404\n",
      "Iteration 4, loss = 1.09588858\n",
      "Iteration 5, loss = 1.03436606\n",
      "Iteration 6, loss = 0.98636287\n",
      "Iteration 7, loss = 0.92558714\n",
      "Iteration 8, loss = 0.85905698\n",
      "Iteration 9, loss = 0.79921516\n",
      "Iteration 10, loss = 0.74703155\n",
      "Iteration 11, loss = 0.70167012\n",
      "Iteration 12, loss = 0.66213993\n",
      "Iteration 13, loss = 0.62830065\n",
      "Iteration 14, loss = 0.59876019\n",
      "Iteration 15, loss = 0.57219540\n",
      "Iteration 16, loss = 0.54813945\n",
      "Iteration 17, loss = 0.52639799\n",
      "Iteration 18, loss = 0.50675463\n",
      "Iteration 19, loss = 0.48917597\n",
      "Iteration 20, loss = 0.47346996\n",
      "Iteration 21, loss = 0.45923127\n",
      "Iteration 22, loss = 0.44636099\n",
      "Iteration 23, loss = 0.43466868\n",
      "Iteration 24, loss = 0.42393555\n",
      "Iteration 25, loss = 0.41402427\n",
      "Iteration 26, loss = 0.40472035\n",
      "Iteration 27, loss = 0.39589570\n",
      "Iteration 28, loss = 0.38750504\n",
      "Iteration 29, loss = 0.37950980\n",
      "Iteration 30, loss = 0.37191231\n",
      "Iteration 31, loss = 0.36465816\n",
      "Iteration 32, loss = 0.35758512\n",
      "Iteration 33, loss = 0.35073756\n",
      "Iteration 34, loss = 0.34415114\n",
      "Iteration 35, loss = 0.33774096\n",
      "Iteration 36, loss = 0.33159905\n",
      "Iteration 37, loss = 0.32592278\n",
      "Iteration 38, loss = 0.32058089\n",
      "Iteration 39, loss = 0.31547493\n",
      "Iteration 40, loss = 0.31055636\n",
      "Iteration 41, loss = 0.30578601\n",
      "Iteration 42, loss = 0.30114241\n",
      "Iteration 43, loss = 0.29660994\n",
      "Iteration 44, loss = 0.29219302\n",
      "Iteration 45, loss = 0.28787858\n",
      "Iteration 46, loss = 0.28366461\n",
      "Iteration 47, loss = 0.27954762\n",
      "Iteration 48, loss = 0.27552846\n",
      "Iteration 49, loss = 0.27160022\n",
      "Iteration 50, loss = 0.26776245\n",
      "Iteration 51, loss = 0.26401291\n",
      "Iteration 52, loss = 0.26035239\n",
      "Iteration 53, loss = 0.25677601\n",
      "Iteration 54, loss = 0.25328101\n",
      "Iteration 55, loss = 0.24986475\n",
      "Iteration 56, loss = 0.24653024\n",
      "Iteration 57, loss = 0.24327113\n",
      "Iteration 58, loss = 0.24008942\n",
      "Iteration 59, loss = 0.23696657\n",
      "Iteration 60, loss = 0.23389278\n",
      "Iteration 61, loss = 0.23087033\n",
      "Iteration 62, loss = 0.22789559\n",
      "Iteration 63, loss = 0.22491541\n",
      "Iteration 64, loss = 0.22192968\n",
      "Iteration 65, loss = 0.21887045\n",
      "Iteration 66, loss = 0.21586672\n",
      "Iteration 67, loss = 0.21273358\n",
      "Iteration 68, loss = 0.20972317\n",
      "Iteration 69, loss = 0.20724295\n",
      "Iteration 70, loss = 0.20500360\n",
      "Iteration 71, loss = 0.20280532\n",
      "Iteration 72, loss = 0.20064537\n",
      "Iteration 73, loss = 0.19852171\n",
      "Iteration 74, loss = 0.19642858\n",
      "Iteration 75, loss = 0.19436620\n",
      "Iteration 76, loss = 0.19233616\n",
      "Iteration 77, loss = 0.19033906\n",
      "Iteration 78, loss = 0.18837567\n",
      "Iteration 79, loss = 0.18644579\n",
      "Iteration 80, loss = 0.18454907\n",
      "Iteration 81, loss = 0.18268809\n",
      "Iteration 82, loss = 0.18086145\n",
      "Iteration 83, loss = 0.17906931\n",
      "Iteration 84, loss = 0.17731228\n",
      "Iteration 85, loss = 0.17558874\n",
      "Iteration 86, loss = 0.17390307\n",
      "Iteration 87, loss = 0.17225057\n",
      "Iteration 88, loss = 0.17063130\n",
      "Iteration 89, loss = 0.16904463\n",
      "Iteration 90, loss = 0.16749403\n",
      "Iteration 91, loss = 0.16599957\n",
      "Iteration 92, loss = 0.16454567\n",
      "Iteration 93, loss = 0.16312876\n",
      "Iteration 94, loss = 0.16174899\n",
      "Iteration 95, loss = 0.16038637\n",
      "Iteration 96, loss = 0.15903780\n",
      "Iteration 97, loss = 0.15771124\n",
      "Iteration 98, loss = 0.15641157\n",
      "Iteration 99, loss = 0.15514060\n",
      "Iteration 100, loss = 0.15389856\n",
      "Iteration 101, loss = 0.15268247\n",
      "Iteration 102, loss = 0.15149064\n",
      "Iteration 103, loss = 0.15032258\n",
      "Iteration 104, loss = 0.14917660\n",
      "Iteration 105, loss = 0.14805206\n",
      "Iteration 106, loss = 0.14694905\n",
      "Iteration 107, loss = 0.14586588\n",
      "Iteration 108, loss = 0.14480205\n",
      "Iteration 109, loss = 0.14375873\n",
      "Iteration 110, loss = 0.14273482\n",
      "Iteration 111, loss = 0.14172974\n",
      "Iteration 112, loss = 0.14074356\n",
      "Iteration 113, loss = 0.13977515\n",
      "Iteration 114, loss = 0.13882403\n",
      "Iteration 115, loss = 0.13788967\n",
      "Iteration 116, loss = 0.13697208\n",
      "Iteration 117, loss = 0.13607085\n",
      "Iteration 118, loss = 0.13518623\n",
      "Iteration 119, loss = 0.13431658\n",
      "Iteration 120, loss = 0.13346123\n",
      "Iteration 121, loss = 0.13262005\n",
      "Iteration 122, loss = 0.13179336\n",
      "Iteration 123, loss = 0.13098037\n",
      "Iteration 124, loss = 0.13018104\n",
      "Iteration 125, loss = 0.12939497\n",
      "Iteration 126, loss = 0.12862198\n",
      "Iteration 127, loss = 0.12786154\n",
      "Iteration 128, loss = 0.12711349\n",
      "Iteration 129, loss = 0.12637770\n",
      "Iteration 130, loss = 0.12565424\n",
      "Iteration 131, loss = 0.12494320\n",
      "Iteration 132, loss = 0.12424356\n",
      "Iteration 133, loss = 0.12355547\n",
      "Iteration 134, loss = 0.12287792\n",
      "Iteration 135, loss = 0.12221092\n",
      "Iteration 136, loss = 0.12155418\n",
      "Iteration 137, loss = 0.12090825\n",
      "Iteration 138, loss = 0.12027202\n",
      "Iteration 139, loss = 0.11964536\n",
      "Iteration 140, loss = 0.11902897\n",
      "Iteration 141, loss = 0.11842137\n",
      "Iteration 142, loss = 0.11782265\n",
      "Iteration 143, loss = 0.11723315\n",
      "Iteration 144, loss = 0.11665135\n",
      "Iteration 145, loss = 0.11607502\n",
      "Iteration 146, loss = 0.11550704\n",
      "Iteration 147, loss = 0.11494737\n",
      "Iteration 148, loss = 0.11439579\n",
      "Iteration 149, loss = 0.11385228\n",
      "Iteration 150, loss = 0.11331428\n",
      "Iteration 151, loss = 0.11278831\n",
      "Iteration 152, loss = 0.11227179\n",
      "Iteration 153, loss = 0.11176304\n",
      "Iteration 154, loss = 0.11126409\n",
      "Iteration 155, loss = 0.11077230\n",
      "Iteration 156, loss = 0.11028834\n",
      "Iteration 157, loss = 0.10981053\n",
      "Iteration 158, loss = 0.10933848\n",
      "Iteration 159, loss = 0.10887288\n",
      "Iteration 160, loss = 0.10841350\n",
      "Iteration 161, loss = 0.10796054\n",
      "Iteration 162, loss = 0.10751283\n",
      "Iteration 163, loss = 0.10707100\n",
      "Iteration 164, loss = 0.10663464\n",
      "Iteration 165, loss = 0.10620400\n",
      "Iteration 166, loss = 0.10577910\n",
      "Iteration 167, loss = 0.10535992\n",
      "Iteration 168, loss = 0.10494617\n",
      "Iteration 169, loss = 0.10453689\n",
      "Iteration 170, loss = 0.10413278\n",
      "Iteration 171, loss = 0.10373377\n",
      "Iteration 172, loss = 0.10333957\n",
      "Iteration 173, loss = 0.10295058\n",
      "Iteration 174, loss = 0.10256636\n",
      "Iteration 175, loss = 0.10218747\n",
      "Iteration 176, loss = 0.10181316\n",
      "Iteration 177, loss = 0.10144352\n",
      "Iteration 178, loss = 0.10107841\n",
      "Iteration 179, loss = 0.10071778\n",
      "Iteration 180, loss = 0.10036153\n",
      "Iteration 181, loss = 0.10000940\n",
      "Iteration 182, loss = 0.09966147\n",
      "Iteration 183, loss = 0.09931752\n",
      "Iteration 184, loss = 0.09897773\n",
      "Iteration 185, loss = 0.09864157\n",
      "Iteration 186, loss = 0.09830929\n",
      "Iteration 187, loss = 0.09798098\n",
      "Iteration 188, loss = 0.09765631\n",
      "Iteration 189, loss = 0.09733549\n",
      "Iteration 190, loss = 0.09701836\n",
      "Iteration 191, loss = 0.09670493\n",
      "Iteration 192, loss = 0.09639491\n",
      "Iteration 193, loss = 0.09608827\n",
      "Iteration 194, loss = 0.09578512\n",
      "Iteration 195, loss = 0.09548527\n",
      "Iteration 196, loss = 0.09518871\n",
      "Iteration 197, loss = 0.09489547\n",
      "Iteration 198, loss = 0.09460548\n",
      "Iteration 199, loss = 0.09431858\n",
      "Iteration 200, loss = 0.09403474\n",
      "Iteration 201, loss = 0.09375409\n",
      "Iteration 202, loss = 0.09347625\n",
      "Iteration 203, loss = 0.09320150\n",
      "Iteration 204, loss = 0.09292960\n",
      "Iteration 205, loss = 0.09266053\n",
      "Iteration 206, loss = 0.09239425\n",
      "Iteration 207, loss = 0.09213074\n",
      "Iteration 208, loss = 0.09187012\n",
      "Iteration 209, loss = 0.09161193\n",
      "Iteration 210, loss = 0.09135647\n",
      "Iteration 211, loss = 0.09110377\n",
      "Iteration 212, loss = 0.09085360\n",
      "Iteration 213, loss = 0.09060592\n",
      "Iteration 214, loss = 0.09036069\n",
      "Iteration 215, loss = 0.09011810\n",
      "Iteration 216, loss = 0.08987774\n",
      "Iteration 217, loss = 0.08963985\n",
      "Iteration 218, loss = 0.08940435\n",
      "Iteration 219, loss = 0.08917119\n",
      "Iteration 220, loss = 0.08894041\n",
      "Iteration 221, loss = 0.08871189\n",
      "Iteration 222, loss = 0.08848557\n",
      "Iteration 223, loss = 0.08826160\n",
      "Iteration 224, loss = 0.08803959\n",
      "Iteration 225, loss = 0.08781982\n",
      "Iteration 226, loss = 0.08760208\n",
      "Iteration 227, loss = 0.08738658\n",
      "Iteration 228, loss = 0.08717294\n",
      "Iteration 229, loss = 0.08696152\n",
      "Iteration 230, loss = 0.08675185\n",
      "Iteration 231, loss = 0.08654427\n",
      "Iteration 232, loss = 0.08633869\n",
      "Iteration 233, loss = 0.08613494\n",
      "Iteration 234, loss = 0.08593304\n",
      "Iteration 235, loss = 0.08573298\n",
      "Iteration 236, loss = 0.08553487\n",
      "Iteration 237, loss = 0.08533848\n",
      "Iteration 238, loss = 0.08514386\n",
      "Iteration 239, loss = 0.08495101\n",
      "Iteration 240, loss = 0.08475990\n",
      "Iteration 241, loss = 0.08457052\n",
      "Iteration 242, loss = 0.08438292\n",
      "Iteration 243, loss = 0.08419681\n",
      "Iteration 244, loss = 0.08401247\n",
      "Iteration 245, loss = 0.08382969\n",
      "Iteration 246, loss = 0.08364853\n",
      "Iteration 247, loss = 0.08346894\n",
      "Iteration 248, loss = 0.08329090\n",
      "Iteration 249, loss = 0.08311437\n",
      "Iteration 250, loss = 0.08293939\n",
      "Iteration 251, loss = 0.08276585\n",
      "Iteration 252, loss = 0.08259396\n",
      "Iteration 253, loss = 0.08242332\n",
      "Iteration 254, loss = 0.08225421\n",
      "Iteration 255, loss = 0.08208654\n",
      "Iteration 256, loss = 0.08192027\n",
      "Iteration 257, loss = 0.08175548\n",
      "Iteration 258, loss = 0.08159194\n",
      "Iteration 259, loss = 0.08142978\n",
      "Iteration 260, loss = 0.08126897\n",
      "Iteration 261, loss = 0.08110961\n",
      "Iteration 262, loss = 0.08095134\n",
      "Iteration 263, loss = 0.08079446\n",
      "Iteration 264, loss = 0.08063886\n",
      "Iteration 265, loss = 0.08048452\n",
      "Iteration 266, loss = 0.08033142\n",
      "Iteration 267, loss = 0.08017955\n",
      "Iteration 268, loss = 0.08002889\n",
      "Iteration 269, loss = 0.07987942\n",
      "Iteration 270, loss = 0.07973115\n",
      "Iteration 271, loss = 0.07958403\n",
      "Iteration 272, loss = 0.07943809\n",
      "Iteration 273, loss = 0.07929340\n",
      "Iteration 274, loss = 0.07914968\n",
      "Iteration 275, loss = 0.07900713\n",
      "Iteration 276, loss = 0.07886572\n",
      "Iteration 277, loss = 0.07872539\n",
      "Iteration 278, loss = 0.07858614\n",
      "Iteration 279, loss = 0.07844795\n",
      "Iteration 280, loss = 0.07831082\n",
      "Iteration 281, loss = 0.07817472\n",
      "Iteration 282, loss = 0.07803963\n",
      "Iteration 283, loss = 0.07790556\n",
      "Iteration 284, loss = 0.07777263\n",
      "Iteration 285, loss = 0.07764049\n",
      "Iteration 286, loss = 0.07750943\n",
      "Iteration 287, loss = 0.07737934\n",
      "Iteration 288, loss = 0.07725019\n",
      "Iteration 289, loss = 0.07712199\n",
      "Iteration 290, loss = 0.07699473\n",
      "Iteration 291, loss = 0.07686837\n",
      "Iteration 292, loss = 0.07674292\n",
      "Iteration 293, loss = 0.07661837\n",
      "Iteration 294, loss = 0.07649473\n",
      "Iteration 295, loss = 0.07637202\n",
      "Iteration 296, loss = 0.07625016\n",
      "Iteration 297, loss = 0.07612917\n",
      "Iteration 298, loss = 0.07600902\n",
      "Iteration 299, loss = 0.07588971\n",
      "Iteration 300, loss = 0.07577125\n",
      "Iteration 301, loss = 0.07565360\n",
      "Iteration 302, loss = 0.07553680\n",
      "Iteration 303, loss = 0.07542082\n",
      "Iteration 304, loss = 0.07530564\n",
      "Iteration 305, loss = 0.07519130\n",
      "Iteration 306, loss = 0.07507775\n",
      "Iteration 307, loss = 0.07496495\n",
      "Iteration 308, loss = 0.07485292\n",
      "Iteration 309, loss = 0.07474165\n",
      "Iteration 310, loss = 0.07463114\n",
      "Iteration 311, loss = 0.07452138\n",
      "Iteration 312, loss = 0.07441234\n",
      "Iteration 313, loss = 0.07430404\n",
      "Iteration 314, loss = 0.07419645\n",
      "Iteration 315, loss = 0.07408958\n",
      "Iteration 316, loss = 0.07398361\n",
      "Iteration 317, loss = 0.07387809\n",
      "Iteration 318, loss = 0.07377343\n",
      "Iteration 319, loss = 0.07366946\n",
      "Iteration 320, loss = 0.07356618\n",
      "Iteration 321, loss = 0.07346357\n",
      "Iteration 322, loss = 0.07336162\n",
      "Iteration 323, loss = 0.07326033\n",
      "Iteration 324, loss = 0.07315969\n",
      "Iteration 325, loss = 0.07305973\n",
      "Iteration 326, loss = 0.07296043\n",
      "Iteration 327, loss = 0.07286177\n",
      "Iteration 328, loss = 0.07276377\n",
      "Iteration 329, loss = 0.07266638\n",
      "Iteration 330, loss = 0.07256963\n",
      "Iteration 331, loss = 0.07247349\n",
      "Iteration 332, loss = 0.07237796\n",
      "Iteration 333, loss = 0.07228304\n",
      "Iteration 334, loss = 0.07218871\n",
      "Iteration 335, loss = 0.07209497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49174295\n",
      "Iteration 2, loss = 1.35744472\n",
      "Iteration 3, loss = 1.20940635\n",
      "Iteration 4, loss = 1.09712272\n",
      "Iteration 5, loss = 1.03381891\n",
      "Iteration 6, loss = 0.98350683\n",
      "Iteration 7, loss = 0.92040085\n",
      "Iteration 8, loss = 0.85320129\n",
      "Iteration 9, loss = 0.79446959\n",
      "Iteration 10, loss = 0.74332522\n",
      "Iteration 11, loss = 0.69870310\n",
      "Iteration 12, loss = 0.65916375\n",
      "Iteration 13, loss = 0.62480745\n",
      "Iteration 14, loss = 0.59515636\n",
      "Iteration 15, loss = 0.56888986\n",
      "Iteration 16, loss = 0.54593845\n",
      "Iteration 17, loss = 0.52544265\n",
      "Iteration 18, loss = 0.50709081\n",
      "Iteration 19, loss = 0.49040893\n",
      "Iteration 20, loss = 0.47551439\n",
      "Iteration 21, loss = 0.46204720\n",
      "Iteration 22, loss = 0.44988746\n",
      "Iteration 23, loss = 0.43889724\n",
      "Iteration 24, loss = 0.42867830\n",
      "Iteration 25, loss = 0.41893191\n",
      "Iteration 26, loss = 0.40971321\n",
      "Iteration 27, loss = 0.40094440\n",
      "Iteration 28, loss = 0.39294392\n",
      "Iteration 29, loss = 0.38557536\n",
      "Iteration 30, loss = 0.37853734\n",
      "Iteration 31, loss = 0.37163590\n",
      "Iteration 32, loss = 0.36490931\n",
      "Iteration 33, loss = 0.35843449\n",
      "Iteration 34, loss = 0.35212064\n",
      "Iteration 35, loss = 0.34618578\n",
      "Iteration 36, loss = 0.34077935\n",
      "Iteration 37, loss = 0.33571316\n",
      "Iteration 38, loss = 0.33091453\n",
      "Iteration 39, loss = 0.32627591\n",
      "Iteration 40, loss = 0.32177626\n",
      "Iteration 41, loss = 0.31737469\n",
      "Iteration 42, loss = 0.31307075\n",
      "Iteration 43, loss = 0.30886050\n",
      "Iteration 44, loss = 0.30475812\n",
      "Iteration 45, loss = 0.30074607\n",
      "Iteration 46, loss = 0.29682260\n",
      "Iteration 47, loss = 0.29299729\n",
      "Iteration 48, loss = 0.28925696\n",
      "Iteration 49, loss = 0.28560525\n",
      "Iteration 50, loss = 0.28204906\n",
      "Iteration 51, loss = 0.27857394\n",
      "Iteration 52, loss = 0.27517780\n",
      "Iteration 53, loss = 0.27186074\n",
      "Iteration 54, loss = 0.26862095\n",
      "Iteration 55, loss = 0.26544997\n",
      "Iteration 56, loss = 0.26235333\n",
      "Iteration 57, loss = 0.25933831\n",
      "Iteration 58, loss = 0.25639552\n",
      "Iteration 59, loss = 0.25353204\n",
      "Iteration 60, loss = 0.25073029\n",
      "Iteration 61, loss = 0.24797534\n",
      "Iteration 62, loss = 0.24524530\n",
      "Iteration 63, loss = 0.24250526\n",
      "Iteration 64, loss = 0.23974098\n",
      "Iteration 65, loss = 0.23697382\n",
      "Iteration 66, loss = 0.23424714\n",
      "Iteration 67, loss = 0.23153104\n",
      "Iteration 68, loss = 0.22874424\n",
      "Iteration 69, loss = 0.22619210\n",
      "Iteration 70, loss = 0.22394680\n",
      "Iteration 71, loss = 0.22186048\n",
      "Iteration 72, loss = 0.21986026\n",
      "Iteration 73, loss = 0.21789135\n",
      "Iteration 74, loss = 0.21594888\n",
      "Iteration 75, loss = 0.21403330\n",
      "Iteration 76, loss = 0.21214780\n",
      "Iteration 77, loss = 0.21029467\n",
      "Iteration 78, loss = 0.20847288\n",
      "Iteration 79, loss = 0.20668310\n",
      "Iteration 80, loss = 0.20492582\n",
      "Iteration 81, loss = 0.20320161\n",
      "Iteration 82, loss = 0.20150953\n",
      "Iteration 83, loss = 0.19984941\n",
      "Iteration 84, loss = 0.19822130\n",
      "Iteration 85, loss = 0.19662437\n",
      "Iteration 86, loss = 0.19505743\n",
      "Iteration 87, loss = 0.19352041\n",
      "Iteration 88, loss = 0.19202080\n",
      "Iteration 89, loss = 0.19056210\n",
      "Iteration 90, loss = 0.18914448\n",
      "Iteration 91, loss = 0.18776399\n",
      "Iteration 92, loss = 0.18641386\n",
      "Iteration 93, loss = 0.18509974\n",
      "Iteration 94, loss = 0.18381045\n",
      "Iteration 95, loss = 0.18254609\n",
      "Iteration 96, loss = 0.18130446\n",
      "Iteration 97, loss = 0.18008113\n",
      "Iteration 98, loss = 0.17888213\n",
      "Iteration 99, loss = 0.17770731\n",
      "Iteration 100, loss = 0.17655496\n",
      "Iteration 101, loss = 0.17542484\n",
      "Iteration 102, loss = 0.17432179\n",
      "Iteration 103, loss = 0.17324119\n",
      "Iteration 104, loss = 0.17218146\n",
      "Iteration 105, loss = 0.17114217\n",
      "Iteration 106, loss = 0.17012309\n",
      "Iteration 107, loss = 0.16912368\n",
      "Iteration 108, loss = 0.16814384\n",
      "Iteration 109, loss = 0.16718081\n",
      "Iteration 110, loss = 0.16623578\n",
      "Iteration 111, loss = 0.16530875\n",
      "Iteration 112, loss = 0.16439893\n",
      "Iteration 113, loss = 0.16350552\n",
      "Iteration 114, loss = 0.16262834\n",
      "Iteration 115, loss = 0.16176758\n",
      "Iteration 116, loss = 0.16092153\n",
      "Iteration 117, loss = 0.16009085\n",
      "Iteration 118, loss = 0.15927481\n",
      "Iteration 119, loss = 0.15847314\n",
      "Iteration 120, loss = 0.15768572\n",
      "Iteration 121, loss = 0.15691220\n",
      "Iteration 122, loss = 0.15615208\n",
      "Iteration 123, loss = 0.15540520\n",
      "Iteration 124, loss = 0.15467138\n",
      "Iteration 125, loss = 0.15395020\n",
      "Iteration 126, loss = 0.15324137\n",
      "Iteration 127, loss = 0.15254486\n",
      "Iteration 128, loss = 0.15186144\n",
      "Iteration 129, loss = 0.15118833\n",
      "Iteration 130, loss = 0.15052759\n",
      "Iteration 131, loss = 0.14987776\n",
      "Iteration 132, loss = 0.14923901\n",
      "Iteration 133, loss = 0.14861096\n",
      "Iteration 134, loss = 0.14799382\n",
      "Iteration 135, loss = 0.14738690\n",
      "Iteration 136, loss = 0.14678977\n",
      "Iteration 137, loss = 0.14620234\n",
      "Iteration 138, loss = 0.14562545\n",
      "Iteration 139, loss = 0.14505763\n",
      "Iteration 140, loss = 0.14449992\n",
      "Iteration 141, loss = 0.14395155\n",
      "Iteration 142, loss = 0.14341252\n",
      "Iteration 143, loss = 0.14288172\n",
      "Iteration 144, loss = 0.14236052\n",
      "Iteration 145, loss = 0.14184716\n",
      "Iteration 146, loss = 0.14134258\n",
      "Iteration 147, loss = 0.14084801\n",
      "Iteration 148, loss = 0.14036327\n",
      "Iteration 149, loss = 0.13988514\n",
      "Iteration 150, loss = 0.13941432\n",
      "Iteration 151, loss = 0.13894961\n",
      "Iteration 152, loss = 0.13849169\n",
      "Iteration 153, loss = 0.13804037\n",
      "Iteration 154, loss = 0.13759535\n",
      "Iteration 155, loss = 0.13715678\n",
      "Iteration 156, loss = 0.13672460\n",
      "Iteration 157, loss = 0.13630054\n",
      "Iteration 158, loss = 0.13588256\n",
      "Iteration 159, loss = 0.13547110\n",
      "Iteration 160, loss = 0.13506474\n",
      "Iteration 161, loss = 0.13466452\n",
      "Iteration 162, loss = 0.13426958\n",
      "Iteration 163, loss = 0.13388040\n",
      "Iteration 164, loss = 0.13349659\n",
      "Iteration 165, loss = 0.13311835\n",
      "Iteration 166, loss = 0.13274529\n",
      "Iteration 167, loss = 0.13237727\n",
      "Iteration 168, loss = 0.13201455\n",
      "Iteration 169, loss = 0.13165665\n",
      "Iteration 170, loss = 0.13130458\n",
      "Iteration 171, loss = 0.13095576\n",
      "Iteration 172, loss = 0.13061265\n",
      "Iteration 173, loss = 0.13027364\n",
      "Iteration 174, loss = 0.12993892\n",
      "Iteration 175, loss = 0.12960950\n",
      "Iteration 176, loss = 0.12928335\n",
      "Iteration 177, loss = 0.12896203\n",
      "Iteration 178, loss = 0.12864469\n",
      "Iteration 179, loss = 0.12833141\n",
      "Iteration 180, loss = 0.12802209\n",
      "Iteration 181, loss = 0.12771727\n",
      "Iteration 182, loss = 0.12741575\n",
      "Iteration 183, loss = 0.12711828\n",
      "Iteration 184, loss = 0.12682485\n",
      "Iteration 185, loss = 0.12653479\n",
      "Iteration 186, loss = 0.12624824\n",
      "Iteration 187, loss = 0.12596515\n",
      "Iteration 188, loss = 0.12568564\n",
      "Iteration 189, loss = 0.12540966\n",
      "Iteration 190, loss = 0.12513691\n",
      "Iteration 191, loss = 0.12486739\n",
      "Iteration 192, loss = 0.12460165\n",
      "Iteration 193, loss = 0.12433829\n",
      "Iteration 194, loss = 0.12407842\n",
      "Iteration 195, loss = 0.12382193\n",
      "Iteration 196, loss = 0.12356821\n",
      "Iteration 197, loss = 0.12331740\n",
      "Iteration 198, loss = 0.12306945\n",
      "Iteration 199, loss = 0.12282430\n",
      "Iteration 200, loss = 0.12258245\n",
      "Iteration 201, loss = 0.12234271\n",
      "Iteration 202, loss = 0.12210597\n",
      "Iteration 203, loss = 0.12187218\n",
      "Iteration 204, loss = 0.12164081\n",
      "Iteration 205, loss = 0.12141204\n",
      "Iteration 206, loss = 0.12118580\n",
      "Iteration 207, loss = 0.12096202\n",
      "Iteration 208, loss = 0.12074126\n",
      "Iteration 209, loss = 0.12052213\n",
      "Iteration 210, loss = 0.12030582\n",
      "Iteration 211, loss = 0.12009198\n",
      "Iteration 212, loss = 0.11988033\n",
      "Iteration 213, loss = 0.11967090\n",
      "Iteration 214, loss = 0.11946365\n",
      "Iteration 215, loss = 0.11925857\n",
      "Iteration 216, loss = 0.11905618\n",
      "Iteration 217, loss = 0.11885523\n",
      "Iteration 218, loss = 0.11865667\n",
      "Iteration 219, loss = 0.11846046\n",
      "Iteration 220, loss = 0.11826600\n",
      "Iteration 221, loss = 0.11807361\n",
      "Iteration 222, loss = 0.11788318\n",
      "Iteration 223, loss = 0.11769467\n",
      "Iteration 224, loss = 0.11750849\n",
      "Iteration 225, loss = 0.11732368\n",
      "Iteration 226, loss = 0.11714096\n",
      "Iteration 227, loss = 0.11696004\n",
      "Iteration 228, loss = 0.11678120\n",
      "Iteration 229, loss = 0.11660394\n",
      "Iteration 230, loss = 0.11642841\n",
      "Iteration 231, loss = 0.11625459\n",
      "Iteration 232, loss = 0.11608241\n",
      "Iteration 233, loss = 0.11591241\n",
      "Iteration 234, loss = 0.11574341\n",
      "Iteration 235, loss = 0.11557631\n",
      "Iteration 236, loss = 0.11541105\n",
      "Iteration 237, loss = 0.11524719\n",
      "Iteration 238, loss = 0.11508494\n",
      "Iteration 239, loss = 0.11492422\n",
      "Iteration 240, loss = 0.11476500\n",
      "Iteration 241, loss = 0.11460747\n",
      "Iteration 242, loss = 0.11445128\n",
      "Iteration 243, loss = 0.11429657\n",
      "Iteration 244, loss = 0.11414329\n",
      "Iteration 245, loss = 0.11399147\n",
      "Iteration 246, loss = 0.11384118\n",
      "Iteration 247, loss = 0.11369218\n",
      "Iteration 248, loss = 0.11354448\n",
      "Iteration 249, loss = 0.11339810\n",
      "Iteration 250, loss = 0.11325325\n",
      "Iteration 251, loss = 0.11310956\n",
      "Iteration 252, loss = 0.11296717\n",
      "Iteration 253, loss = 0.11282602\n",
      "Iteration 254, loss = 0.11268628\n",
      "Iteration 255, loss = 0.11254775\n",
      "Iteration 256, loss = 0.11241039\n",
      "Iteration 257, loss = 0.11227421\n",
      "Iteration 258, loss = 0.11213920\n",
      "Iteration 259, loss = 0.11200556\n",
      "Iteration 260, loss = 0.11187293\n",
      "Iteration 261, loss = 0.11174143\n",
      "Iteration 262, loss = 0.11161110\n",
      "Iteration 263, loss = 0.11148184\n",
      "Iteration 264, loss = 0.11135390\n",
      "Iteration 265, loss = 0.11122691\n",
      "Iteration 266, loss = 0.11110098\n",
      "Iteration 267, loss = 0.11097607\n",
      "Iteration 268, loss = 0.11085219\n",
      "Iteration 269, loss = 0.11072983\n",
      "Iteration 270, loss = 0.11060777\n",
      "Iteration 271, loss = 0.11048704\n",
      "Iteration 272, loss = 0.11036737\n",
      "Iteration 273, loss = 0.11024874\n",
      "Iteration 274, loss = 0.11013101\n",
      "Iteration 275, loss = 0.11001422\n",
      "Iteration 276, loss = 0.10989832\n",
      "Iteration 277, loss = 0.10978335\n",
      "Iteration 278, loss = 0.10966939\n",
      "Iteration 279, loss = 0.10955641\n",
      "Iteration 280, loss = 0.10944427\n",
      "Iteration 281, loss = 0.10933301\n",
      "Iteration 282, loss = 0.10922260\n",
      "Iteration 283, loss = 0.10911334\n",
      "Iteration 284, loss = 0.10900459\n",
      "Iteration 285, loss = 0.10889683\n",
      "Iteration 286, loss = 0.10878987\n",
      "Iteration 287, loss = 0.10868374\n",
      "Iteration 288, loss = 0.10857865\n",
      "Iteration 289, loss = 0.10847422\n",
      "Iteration 290, loss = 0.10837055\n",
      "Iteration 291, loss = 0.10826765\n",
      "Iteration 292, loss = 0.10816552\n",
      "Iteration 293, loss = 0.10806427\n",
      "Iteration 294, loss = 0.10796379\n",
      "Iteration 295, loss = 0.10786401\n",
      "Iteration 296, loss = 0.10776496\n",
      "Iteration 297, loss = 0.10766662\n",
      "Iteration 298, loss = 0.10756924\n",
      "Iteration 299, loss = 0.10747235\n",
      "Iteration 300, loss = 0.10737625\n",
      "Iteration 301, loss = 0.10728083\n",
      "Iteration 302, loss = 0.10718608\n",
      "Iteration 303, loss = 0.10709221\n",
      "Iteration 304, loss = 0.10699886\n",
      "Iteration 305, loss = 0.10690622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48762646\n",
      "Iteration 2, loss = 1.45807064\n",
      "Iteration 3, loss = 1.42935304\n",
      "Iteration 4, loss = 1.40157758\n",
      "Iteration 5, loss = 1.37470964\n",
      "Iteration 6, loss = 1.34868508\n",
      "Iteration 7, loss = 1.32335864\n",
      "Iteration 8, loss = 1.29871510\n",
      "Iteration 9, loss = 1.27492256\n",
      "Iteration 10, loss = 1.25202889\n",
      "Iteration 11, loss = 1.22995304\n",
      "Iteration 12, loss = 1.20875052\n",
      "Iteration 13, loss = 1.18867466\n",
      "Iteration 14, loss = 1.16962964\n",
      "Iteration 15, loss = 1.15138124\n",
      "Iteration 16, loss = 1.13389030\n",
      "Iteration 17, loss = 1.11701469\n",
      "Iteration 18, loss = 1.10069723\n",
      "Iteration 19, loss = 1.08493152\n",
      "Iteration 20, loss = 1.06973471\n",
      "Iteration 21, loss = 1.05511956\n",
      "Iteration 22, loss = 1.04098713\n",
      "Iteration 23, loss = 1.02725460\n",
      "Iteration 24, loss = 1.01390784\n",
      "Iteration 25, loss = 1.00095351\n",
      "Iteration 26, loss = 0.98832568\n",
      "Iteration 27, loss = 0.97597344\n",
      "Iteration 28, loss = 0.96393407\n",
      "Iteration 29, loss = 0.95212521\n",
      "Iteration 30, loss = 0.94054710\n",
      "Iteration 31, loss = 0.92918203\n",
      "Iteration 32, loss = 0.91800562\n",
      "Iteration 33, loss = 0.90707109\n",
      "Iteration 34, loss = 0.89644862\n",
      "Iteration 35, loss = 0.88614569\n",
      "Iteration 36, loss = 0.87609101\n",
      "Iteration 37, loss = 0.86630510\n",
      "Iteration 38, loss = 0.85670379\n",
      "Iteration 39, loss = 0.84732725\n",
      "Iteration 40, loss = 0.83820194\n",
      "Iteration 41, loss = 0.82935671\n",
      "Iteration 42, loss = 0.82066423\n",
      "Iteration 43, loss = 0.81210105\n",
      "Iteration 44, loss = 0.80369451\n",
      "Iteration 45, loss = 0.79549796\n",
      "Iteration 46, loss = 0.78746996\n",
      "Iteration 47, loss = 0.77958665\n",
      "Iteration 48, loss = 0.77184473\n",
      "Iteration 49, loss = 0.76425638\n",
      "Iteration 50, loss = 0.75681548\n",
      "Iteration 51, loss = 0.74949882\n",
      "Iteration 52, loss = 0.74231560\n",
      "Iteration 53, loss = 0.73528830\n",
      "Iteration 54, loss = 0.72838821\n",
      "Iteration 55, loss = 0.72161492\n",
      "Iteration 56, loss = 0.71497457\n",
      "Iteration 57, loss = 0.70849049\n",
      "Iteration 58, loss = 0.70214668\n",
      "Iteration 59, loss = 0.69594734\n",
      "Iteration 60, loss = 0.68988243\n",
      "Iteration 61, loss = 0.68396632\n",
      "Iteration 62, loss = 0.67820156\n",
      "Iteration 63, loss = 0.67257988\n",
      "Iteration 64, loss = 0.66708098\n",
      "Iteration 65, loss = 0.66167641\n",
      "Iteration 66, loss = 0.65634942\n",
      "Iteration 67, loss = 0.65109608\n",
      "Iteration 68, loss = 0.64591614\n",
      "Iteration 69, loss = 0.64079300\n",
      "Iteration 70, loss = 0.63572519\n",
      "Iteration 71, loss = 0.63072618\n",
      "Iteration 72, loss = 0.62578227\n",
      "Iteration 73, loss = 0.62089962\n",
      "Iteration 74, loss = 0.61608369\n",
      "Iteration 75, loss = 0.61133723\n",
      "Iteration 76, loss = 0.60664089\n",
      "Iteration 77, loss = 0.60197879\n",
      "Iteration 78, loss = 0.59737483\n",
      "Iteration 79, loss = 0.59278571\n",
      "Iteration 80, loss = 0.58825733\n",
      "Iteration 81, loss = 0.58377501\n",
      "Iteration 82, loss = 0.57925890\n",
      "Iteration 83, loss = 0.57479490\n",
      "Iteration 84, loss = 0.57014835\n",
      "Iteration 85, loss = 0.56545264\n",
      "Iteration 86, loss = 0.56098833\n",
      "Iteration 87, loss = 0.55663484\n",
      "Iteration 88, loss = 0.55234315\n",
      "Iteration 89, loss = 0.54809910\n",
      "Iteration 90, loss = 0.54397932\n",
      "Iteration 91, loss = 0.53998574\n",
      "Iteration 92, loss = 0.53615195\n",
      "Iteration 93, loss = 0.53239329\n",
      "Iteration 94, loss = 0.52871235\n",
      "Iteration 95, loss = 0.52508188\n",
      "Iteration 96, loss = 0.52153168\n",
      "Iteration 97, loss = 0.51797991\n",
      "Iteration 98, loss = 0.51442640\n",
      "Iteration 99, loss = 0.51090590\n",
      "Iteration 100, loss = 0.50747168\n",
      "Iteration 101, loss = 0.50412760\n",
      "Iteration 102, loss = 0.50083052\n",
      "Iteration 103, loss = 0.49755791\n",
      "Iteration 104, loss = 0.49431074\n",
      "Iteration 105, loss = 0.49109223\n",
      "Iteration 106, loss = 0.48790345\n",
      "Iteration 107, loss = 0.48474027\n",
      "Iteration 108, loss = 0.48160977\n",
      "Iteration 109, loss = 0.47851483\n",
      "Iteration 110, loss = 0.47545353\n",
      "Iteration 111, loss = 0.47241607\n",
      "Iteration 112, loss = 0.46940594\n",
      "Iteration 113, loss = 0.46642490\n",
      "Iteration 114, loss = 0.46347545\n",
      "Iteration 115, loss = 0.46055858\n",
      "Iteration 116, loss = 0.45766997\n",
      "Iteration 117, loss = 0.45481451\n",
      "Iteration 118, loss = 0.45198233\n",
      "Iteration 119, loss = 0.44917588\n",
      "Iteration 120, loss = 0.44639538\n",
      "Iteration 121, loss = 0.44363969\n",
      "Iteration 122, loss = 0.44090661\n",
      "Iteration 123, loss = 0.43818378\n",
      "Iteration 124, loss = 0.43545664\n",
      "Iteration 125, loss = 0.43271596\n",
      "Iteration 126, loss = 0.42996144\n",
      "Iteration 127, loss = 0.42717201\n",
      "Iteration 128, loss = 0.42437936\n",
      "Iteration 129, loss = 0.42160246\n",
      "Iteration 130, loss = 0.41878005\n",
      "Iteration 131, loss = 0.41593507\n",
      "Iteration 132, loss = 0.41306881\n",
      "Iteration 133, loss = 0.41022061\n",
      "Iteration 134, loss = 0.40739409\n",
      "Iteration 135, loss = 0.40468696\n",
      "Iteration 136, loss = 0.40211644\n",
      "Iteration 137, loss = 0.39965288\n",
      "Iteration 138, loss = 0.39728102\n",
      "Iteration 139, loss = 0.39501162\n",
      "Iteration 140, loss = 0.39276149\n",
      "Iteration 141, loss = 0.39055388\n",
      "Iteration 142, loss = 0.38837257\n",
      "Iteration 143, loss = 0.38620641\n",
      "Iteration 144, loss = 0.38404765\n",
      "Iteration 145, loss = 0.38188954\n",
      "Iteration 146, loss = 0.37974162\n",
      "Iteration 147, loss = 0.37759739\n",
      "Iteration 148, loss = 0.37545538\n",
      "Iteration 149, loss = 0.37331514\n",
      "Iteration 150, loss = 0.37117819\n",
      "Iteration 151, loss = 0.36904929\n",
      "Iteration 152, loss = 0.36693684\n",
      "Iteration 153, loss = 0.36483001\n",
      "Iteration 154, loss = 0.36273992\n",
      "Iteration 155, loss = 0.36065941\n",
      "Iteration 156, loss = 0.35859332\n",
      "Iteration 157, loss = 0.35655147\n",
      "Iteration 158, loss = 0.35452879\n",
      "Iteration 159, loss = 0.35252777\n",
      "Iteration 160, loss = 0.35055313\n",
      "Iteration 161, loss = 0.34859764\n",
      "Iteration 162, loss = 0.34665603\n",
      "Iteration 163, loss = 0.34472723\n",
      "Iteration 164, loss = 0.34281114\n",
      "Iteration 165, loss = 0.34091050\n",
      "Iteration 166, loss = 0.33902229\n",
      "Iteration 167, loss = 0.33714515\n",
      "Iteration 168, loss = 0.33528029\n",
      "Iteration 169, loss = 0.33342645\n",
      "Iteration 170, loss = 0.33158565\n",
      "Iteration 171, loss = 0.32975684\n",
      "Iteration 172, loss = 0.32793941\n",
      "Iteration 173, loss = 0.32613704\n",
      "Iteration 174, loss = 0.32435055\n",
      "Iteration 175, loss = 0.32257773\n",
      "Iteration 176, loss = 0.32081489\n",
      "Iteration 177, loss = 0.31906243\n",
      "Iteration 178, loss = 0.31732051\n",
      "Iteration 179, loss = 0.31558867\n",
      "Iteration 180, loss = 0.31386810\n",
      "Iteration 181, loss = 0.31215829\n",
      "Iteration 182, loss = 0.31046094\n",
      "Iteration 183, loss = 0.30877370\n",
      "Iteration 184, loss = 0.30709597\n",
      "Iteration 185, loss = 0.30542919\n",
      "Iteration 186, loss = 0.30376941\n",
      "Iteration 187, loss = 0.30211807\n",
      "Iteration 188, loss = 0.30047853\n",
      "Iteration 189, loss = 0.29885116\n",
      "Iteration 190, loss = 0.29723777\n",
      "Iteration 191, loss = 0.29563855\n",
      "Iteration 192, loss = 0.29405252\n",
      "Iteration 193, loss = 0.29247659\n",
      "Iteration 194, loss = 0.29091206\n",
      "Iteration 195, loss = 0.28935841\n",
      "Iteration 196, loss = 0.28781540\n",
      "Iteration 197, loss = 0.28628287\n",
      "Iteration 198, loss = 0.28475927\n",
      "Iteration 199, loss = 0.28324492\n",
      "Iteration 200, loss = 0.28174180\n",
      "Iteration 201, loss = 0.28024921\n",
      "Iteration 202, loss = 0.27876703\n",
      "Iteration 203, loss = 0.27729545\n",
      "Iteration 204, loss = 0.27583386\n",
      "Iteration 205, loss = 0.27438228\n",
      "Iteration 206, loss = 0.27294190\n",
      "Iteration 207, loss = 0.27151292\n",
      "Iteration 208, loss = 0.27009400\n",
      "Iteration 209, loss = 0.26868590\n",
      "Iteration 210, loss = 0.26728862\n",
      "Iteration 211, loss = 0.26590042\n",
      "Iteration 212, loss = 0.26452376\n",
      "Iteration 213, loss = 0.26315734\n",
      "Iteration 214, loss = 0.26180063\n",
      "Iteration 215, loss = 0.26045456\n",
      "Iteration 216, loss = 0.25911894\n",
      "Iteration 217, loss = 0.25779291\n",
      "Iteration 218, loss = 0.25647804\n",
      "Iteration 219, loss = 0.25517259\n",
      "Iteration 220, loss = 0.25387640\n",
      "Iteration 221, loss = 0.25259037\n",
      "Iteration 222, loss = 0.25131483\n",
      "Iteration 223, loss = 0.25004840\n",
      "Iteration 224, loss = 0.24879297\n",
      "Iteration 225, loss = 0.24754679\n",
      "Iteration 226, loss = 0.24631024\n",
      "Iteration 227, loss = 0.24508387\n",
      "Iteration 228, loss = 0.24386666\n",
      "Iteration 229, loss = 0.24265916\n",
      "Iteration 230, loss = 0.24146095\n",
      "Iteration 231, loss = 0.24027217\n",
      "Iteration 232, loss = 0.23909260\n",
      "Iteration 233, loss = 0.23792242\n",
      "Iteration 234, loss = 0.23676214\n",
      "Iteration 235, loss = 0.23561091\n",
      "Iteration 236, loss = 0.23446882\n",
      "Iteration 237, loss = 0.23333558\n",
      "Iteration 238, loss = 0.23221124\n",
      "Iteration 239, loss = 0.23109605\n",
      "Iteration 240, loss = 0.22998984\n",
      "Iteration 241, loss = 0.22889240\n",
      "Iteration 242, loss = 0.22780379\n",
      "Iteration 243, loss = 0.22672384\n",
      "Iteration 244, loss = 0.22565252\n",
      "Iteration 245, loss = 0.22459034\n",
      "Iteration 246, loss = 0.22353691\n",
      "Iteration 247, loss = 0.22249199\n",
      "Iteration 248, loss = 0.22145548\n",
      "Iteration 249, loss = 0.22042697\n",
      "Iteration 250, loss = 0.21940671\n",
      "Iteration 251, loss = 0.21839473\n",
      "Iteration 252, loss = 0.21739108\n",
      "Iteration 253, loss = 0.21639548\n",
      "Iteration 254, loss = 0.21540787\n",
      "Iteration 255, loss = 0.21442831\n",
      "Iteration 256, loss = 0.21345700\n",
      "Iteration 257, loss = 0.21249324\n",
      "Iteration 258, loss = 0.21153775\n",
      "Iteration 259, loss = 0.21059017\n",
      "Iteration 260, loss = 0.20965048\n",
      "Iteration 261, loss = 0.20871808\n",
      "Iteration 262, loss = 0.20779413\n",
      "Iteration 263, loss = 0.20687722\n",
      "Iteration 264, loss = 0.20596766\n",
      "Iteration 265, loss = 0.20506646\n",
      "Iteration 266, loss = 0.20417198\n",
      "Iteration 267, loss = 0.20328414\n",
      "Iteration 268, loss = 0.20240465\n",
      "Iteration 269, loss = 0.20153192\n",
      "Iteration 270, loss = 0.20066584\n",
      "Iteration 271, loss = 0.19980727\n",
      "Iteration 272, loss = 0.19895553\n",
      "Iteration 273, loss = 0.19811121\n",
      "Iteration 274, loss = 0.19727419\n",
      "Iteration 275, loss = 0.19644347\n",
      "Iteration 276, loss = 0.19561942\n",
      "Iteration 277, loss = 0.19480284\n",
      "Iteration 278, loss = 0.19399324\n",
      "Iteration 279, loss = 0.19318980\n",
      "Iteration 280, loss = 0.19239303\n",
      "Iteration 281, loss = 0.19160263\n",
      "Iteration 282, loss = 0.19081780\n",
      "Iteration 283, loss = 0.19003646\n",
      "Iteration 284, loss = 0.18926203\n",
      "Iteration 285, loss = 0.18849264\n",
      "Iteration 286, loss = 0.18772746\n",
      "Iteration 287, loss = 0.18696905\n",
      "Iteration 288, loss = 0.18621533\n",
      "Iteration 289, loss = 0.18546464\n",
      "Iteration 290, loss = 0.18472340\n",
      "Iteration 291, loss = 0.18398867\n",
      "Iteration 292, loss = 0.18325872\n",
      "Iteration 293, loss = 0.18253437\n",
      "Iteration 294, loss = 0.18181517\n",
      "Iteration 295, loss = 0.18110003\n",
      "Iteration 296, loss = 0.18038763\n",
      "Iteration 297, loss = 0.17967437\n",
      "Iteration 298, loss = 0.17896220\n",
      "Iteration 299, loss = 0.17825225\n",
      "Iteration 300, loss = 0.17754423\n",
      "Iteration 301, loss = 0.17683020\n",
      "Iteration 302, loss = 0.17610940\n",
      "Iteration 303, loss = 0.17538720\n",
      "Iteration 304, loss = 0.17466229\n",
      "Iteration 305, loss = 0.17393938\n",
      "Iteration 306, loss = 0.17322533\n",
      "Iteration 307, loss = 0.17251530\n",
      "Iteration 308, loss = 0.17180445\n",
      "Iteration 309, loss = 0.17108526\n",
      "Iteration 310, loss = 0.17039311\n",
      "Iteration 311, loss = 0.16969640\n",
      "Iteration 312, loss = 0.16897165\n",
      "Iteration 313, loss = 0.16822948\n",
      "Iteration 314, loss = 0.16747904\n",
      "Iteration 315, loss = 0.16673206\n",
      "Iteration 316, loss = 0.16599959\n",
      "Iteration 317, loss = 0.16529580\n",
      "Iteration 318, loss = 0.16462949\n",
      "Iteration 319, loss = 0.16400073\n",
      "Iteration 320, loss = 0.16342474\n",
      "Iteration 321, loss = 0.16287928\n",
      "Iteration 322, loss = 0.16235070\n",
      "Iteration 323, loss = 0.16182563\n",
      "Iteration 324, loss = 0.16128911\n",
      "Iteration 325, loss = 0.16077313\n",
      "Iteration 326, loss = 0.16026369\n",
      "Iteration 327, loss = 0.15975080\n",
      "Iteration 328, loss = 0.15923670\n",
      "Iteration 329, loss = 0.15872135\n",
      "Iteration 330, loss = 0.15820425\n",
      "Iteration 331, loss = 0.15768700\n",
      "Iteration 332, loss = 0.15717429\n",
      "Iteration 333, loss = 0.15666395\n",
      "Iteration 334, loss = 0.15615009\n",
      "Iteration 335, loss = 0.15563754\n",
      "Iteration 336, loss = 0.15512909\n",
      "Iteration 337, loss = 0.15462340\n",
      "Iteration 338, loss = 0.15411859\n",
      "Iteration 339, loss = 0.15361462\n",
      "Iteration 340, loss = 0.15311210\n",
      "Iteration 341, loss = 0.15261171\n",
      "Iteration 342, loss = 0.15211482\n",
      "Iteration 343, loss = 0.15162259\n",
      "Iteration 344, loss = 0.15113399\n",
      "Iteration 345, loss = 0.15064745\n",
      "Iteration 346, loss = 0.15016320\n",
      "Iteration 347, loss = 0.14968247\n",
      "Iteration 348, loss = 0.14920562\n",
      "Iteration 349, loss = 0.14873206\n",
      "Iteration 350, loss = 0.14826062\n",
      "Iteration 351, loss = 0.14779253\n",
      "Iteration 352, loss = 0.14732720\n",
      "Iteration 353, loss = 0.14686441\n",
      "Iteration 354, loss = 0.14640496\n",
      "Iteration 355, loss = 0.14594833\n",
      "Iteration 356, loss = 0.14549597\n",
      "Iteration 357, loss = 0.14504582\n",
      "Iteration 358, loss = 0.14459789\n",
      "Iteration 359, loss = 0.14415269\n",
      "Iteration 360, loss = 0.14370927\n",
      "Iteration 361, loss = 0.14326778\n",
      "Iteration 362, loss = 0.14282886\n",
      "Iteration 363, loss = 0.14239243\n",
      "Iteration 364, loss = 0.14195874\n",
      "Iteration 365, loss = 0.14152754\n",
      "Iteration 366, loss = 0.14109847\n",
      "Iteration 367, loss = 0.14067164\n",
      "Iteration 368, loss = 0.14024939\n",
      "Iteration 369, loss = 0.13983001\n",
      "Iteration 370, loss = 0.13941344\n",
      "Iteration 371, loss = 0.13899941\n",
      "Iteration 372, loss = 0.13858796\n",
      "Iteration 373, loss = 0.13817890\n",
      "Iteration 374, loss = 0.13777259\n",
      "Iteration 375, loss = 0.13736920\n",
      "Iteration 376, loss = 0.13696836\n",
      "Iteration 377, loss = 0.13657012\n",
      "Iteration 378, loss = 0.13617467\n",
      "Iteration 379, loss = 0.13578211\n",
      "Iteration 380, loss = 0.13539208\n",
      "Iteration 381, loss = 0.13500483\n",
      "Iteration 382, loss = 0.13462045\n",
      "Iteration 383, loss = 0.13423877\n",
      "Iteration 384, loss = 0.13385976\n",
      "Iteration 385, loss = 0.13348359\n",
      "Iteration 386, loss = 0.13311025\n",
      "Iteration 387, loss = 0.13273949\n",
      "Iteration 388, loss = 0.13237169\n",
      "Iteration 389, loss = 0.13200660\n",
      "Iteration 390, loss = 0.13164403\n",
      "Iteration 391, loss = 0.13128411\n",
      "Iteration 392, loss = 0.13092654\n",
      "Iteration 393, loss = 0.13057139\n",
      "Iteration 394, loss = 0.13021819\n",
      "Iteration 395, loss = 0.12986683\n",
      "Iteration 396, loss = 0.12951712\n",
      "Iteration 397, loss = 0.12916919\n",
      "Iteration 398, loss = 0.12882273\n",
      "Iteration 399, loss = 0.12847745\n",
      "Iteration 400, loss = 0.12813328\n",
      "Iteration 401, loss = 0.12779021\n",
      "Iteration 402, loss = 0.12744813\n",
      "Iteration 403, loss = 0.12710710\n",
      "Iteration 404, loss = 0.12676719\n",
      "Iteration 405, loss = 0.12642839\n",
      "Iteration 406, loss = 0.12609060\n",
      "Iteration 407, loss = 0.12575386\n",
      "Iteration 408, loss = 0.12541824\n",
      "Iteration 409, loss = 0.12508362\n",
      "Iteration 410, loss = 0.12474998\n",
      "Iteration 411, loss = 0.12441753\n",
      "Iteration 412, loss = 0.12408642\n",
      "Iteration 413, loss = 0.12375641\n",
      "Iteration 414, loss = 0.12342779\n",
      "Iteration 415, loss = 0.12310044\n",
      "Iteration 416, loss = 0.12277443\n",
      "Iteration 417, loss = 0.12244978\n",
      "Iteration 418, loss = 0.12212656\n",
      "Iteration 419, loss = 0.12180499\n",
      "Iteration 420, loss = 0.12148484\n",
      "Iteration 421, loss = 0.12116625\n",
      "Iteration 422, loss = 0.12084918\n",
      "Iteration 423, loss = 0.12053341\n",
      "Iteration 424, loss = 0.12021938\n",
      "Iteration 425, loss = 0.11990682\n",
      "Iteration 426, loss = 0.11959547\n",
      "Iteration 427, loss = 0.11928513\n",
      "Iteration 428, loss = 0.11897599\n",
      "Iteration 429, loss = 0.11866792\n",
      "Iteration 430, loss = 0.11836090\n",
      "Iteration 431, loss = 0.11805495\n",
      "Iteration 432, loss = 0.11775077\n",
      "Iteration 433, loss = 0.11744758\n",
      "Iteration 434, loss = 0.11714612\n",
      "Iteration 435, loss = 0.11684573\n",
      "Iteration 436, loss = 0.11654628\n",
      "Iteration 437, loss = 0.11624779\n",
      "Iteration 438, loss = 0.11595005\n",
      "Iteration 439, loss = 0.11565341\n",
      "Iteration 440, loss = 0.11535762\n",
      "Iteration 441, loss = 0.11506295\n",
      "Iteration 442, loss = 0.11476951\n",
      "Iteration 443, loss = 0.11447726\n",
      "Iteration 444, loss = 0.11418633\n",
      "Iteration 445, loss = 0.11389629\n",
      "Iteration 446, loss = 0.11360747\n",
      "Iteration 447, loss = 0.11331980\n",
      "Iteration 448, loss = 0.11303331\n",
      "Iteration 449, loss = 0.11274821\n",
      "Iteration 450, loss = 0.11246449\n",
      "Iteration 451, loss = 0.11218238\n",
      "Iteration 452, loss = 0.11190168\n",
      "Iteration 453, loss = 0.11162240\n",
      "Iteration 454, loss = 0.11134526\n",
      "Iteration 455, loss = 0.11106990\n",
      "Iteration 456, loss = 0.11079601\n",
      "Iteration 457, loss = 0.11052358\n",
      "Iteration 458, loss = 0.11025272\n",
      "Iteration 459, loss = 0.10998384\n",
      "Iteration 460, loss = 0.10971673\n",
      "Iteration 461, loss = 0.10945128\n",
      "Iteration 462, loss = 0.10918771\n",
      "Iteration 463, loss = 0.10892606\n",
      "Iteration 464, loss = 0.10866623\n",
      "Iteration 465, loss = 0.10840843\n",
      "Iteration 466, loss = 0.10815264\n",
      "Iteration 467, loss = 0.10789893\n",
      "Iteration 468, loss = 0.10764723\n",
      "Iteration 469, loss = 0.10739742\n",
      "Iteration 470, loss = 0.10714957\n",
      "Iteration 471, loss = 0.10690368\n",
      "Iteration 472, loss = 0.10665973\n",
      "Iteration 473, loss = 0.10641791\n",
      "Iteration 474, loss = 0.10617825\n",
      "Iteration 475, loss = 0.10594057\n",
      "Iteration 476, loss = 0.10570497\n",
      "Iteration 477, loss = 0.10547142\n",
      "Iteration 478, loss = 0.10523989\n",
      "Iteration 479, loss = 0.10501046\n",
      "Iteration 480, loss = 0.10478306\n",
      "Iteration 481, loss = 0.10455780\n",
      "Iteration 482, loss = 0.10433460\n",
      "Iteration 483, loss = 0.10411337\n",
      "Iteration 484, loss = 0.10389420\n",
      "Iteration 485, loss = 0.10367722\n",
      "Iteration 486, loss = 0.10346236\n",
      "Iteration 487, loss = 0.10324956\n",
      "Iteration 488, loss = 0.10303875\n",
      "Iteration 489, loss = 0.10282997\n",
      "Iteration 490, loss = 0.10262317\n",
      "Iteration 491, loss = 0.10241841\n",
      "Iteration 492, loss = 0.10221571\n",
      "Iteration 493, loss = 0.10201501\n",
      "Iteration 494, loss = 0.10181622\n",
      "Iteration 495, loss = 0.10161941\n",
      "Iteration 496, loss = 0.10142412\n",
      "Iteration 497, loss = 0.10123025\n",
      "Iteration 498, loss = 0.10103799\n",
      "Iteration 499, loss = 0.10084737\n",
      "Iteration 500, loss = 0.10065846\n",
      "Iteration 501, loss = 0.10047136\n",
      "Iteration 502, loss = 0.10028584\n",
      "Iteration 503, loss = 0.10010193\n",
      "Iteration 504, loss = 0.09991967\n",
      "Iteration 505, loss = 0.09974024\n",
      "Iteration 506, loss = 0.09956279\n",
      "Iteration 507, loss = 0.09938717\n",
      "Iteration 508, loss = 0.09921323\n",
      "Iteration 509, loss = 0.09904121\n",
      "Iteration 510, loss = 0.09887106\n",
      "Iteration 511, loss = 0.09870289\n",
      "Iteration 512, loss = 0.09853651\n",
      "Iteration 513, loss = 0.09837185\n",
      "Iteration 514, loss = 0.09820881\n",
      "Iteration 515, loss = 0.09804742\n",
      "Iteration 516, loss = 0.09788765\n",
      "Iteration 517, loss = 0.09772948\n",
      "Iteration 518, loss = 0.09757321\n",
      "Iteration 519, loss = 0.09741845\n",
      "Iteration 520, loss = 0.09726515\n",
      "Iteration 521, loss = 0.09711325\n",
      "Iteration 522, loss = 0.09696284\n",
      "Iteration 523, loss = 0.09681393\n",
      "Iteration 524, loss = 0.09666644\n",
      "Iteration 525, loss = 0.09652058\n",
      "Iteration 526, loss = 0.09637614\n",
      "Iteration 527, loss = 0.09623318\n",
      "Iteration 528, loss = 0.09609149\n",
      "Iteration 529, loss = 0.09595119\n",
      "Iteration 530, loss = 0.09581217\n",
      "Iteration 531, loss = 0.09567476\n",
      "Iteration 532, loss = 0.09553832\n",
      "Iteration 533, loss = 0.09540355\n",
      "Iteration 534, loss = 0.09526986\n",
      "Iteration 535, loss = 0.09513760\n",
      "Iteration 536, loss = 0.09500653\n",
      "Iteration 537, loss = 0.09487697\n",
      "Iteration 538, loss = 0.09474858\n",
      "Iteration 539, loss = 0.09462164\n",
      "Iteration 540, loss = 0.09449584\n",
      "Iteration 541, loss = 0.09437175\n",
      "Iteration 542, loss = 0.09424833\n",
      "Iteration 543, loss = 0.09412660\n",
      "Iteration 544, loss = 0.09400543\n",
      "Iteration 545, loss = 0.09388609\n",
      "Iteration 546, loss = 0.09376700\n",
      "Iteration 547, loss = 0.09364968\n",
      "Iteration 548, loss = 0.09353306\n",
      "Iteration 549, loss = 0.09341775\n",
      "Iteration 550, loss = 0.09330328\n",
      "Iteration 551, loss = 0.09319002\n",
      "Iteration 552, loss = 0.09307771\n",
      "Iteration 553, loss = 0.09296653\n",
      "Iteration 554, loss = 0.09285640\n",
      "Iteration 555, loss = 0.09274727\n",
      "Iteration 556, loss = 0.09263904\n",
      "Iteration 557, loss = 0.09253190\n",
      "Iteration 558, loss = 0.09242560\n",
      "Iteration 559, loss = 0.09232030\n",
      "Iteration 560, loss = 0.09221586\n",
      "Iteration 561, loss = 0.09211244\n",
      "Iteration 562, loss = 0.09200988\n",
      "Iteration 563, loss = 0.09190822\n",
      "Iteration 564, loss = 0.09180748\n",
      "Iteration 565, loss = 0.09170758\n",
      "Iteration 566, loss = 0.09160860\n",
      "Iteration 567, loss = 0.09151060\n",
      "Iteration 568, loss = 0.09141350\n",
      "Iteration 569, loss = 0.09131731\n",
      "Iteration 570, loss = 0.09122207\n",
      "Iteration 571, loss = 0.09112761\n",
      "Iteration 572, loss = 0.09103397\n",
      "Iteration 573, loss = 0.09094113\n",
      "Iteration 574, loss = 0.09084913\n",
      "Iteration 575, loss = 0.09075786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47705517\n",
      "Iteration 2, loss = 1.44829403\n",
      "Iteration 3, loss = 1.42029541\n",
      "Iteration 4, loss = 1.39318462\n",
      "Iteration 5, loss = 1.36701846\n",
      "Iteration 6, loss = 1.34160451\n",
      "Iteration 7, loss = 1.31684159\n",
      "Iteration 8, loss = 1.29274792\n",
      "Iteration 9, loss = 1.26945737\n",
      "Iteration 10, loss = 1.24705978\n",
      "Iteration 11, loss = 1.22550787\n",
      "Iteration 12, loss = 1.20479259\n",
      "Iteration 13, loss = 1.18516225\n",
      "Iteration 14, loss = 1.16656161\n",
      "Iteration 15, loss = 1.14870233\n",
      "Iteration 16, loss = 1.13150719\n",
      "Iteration 17, loss = 1.11486448\n",
      "Iteration 18, loss = 1.09876003\n",
      "Iteration 19, loss = 1.08319824\n",
      "Iteration 20, loss = 1.06832635\n",
      "Iteration 21, loss = 1.05409252\n",
      "Iteration 22, loss = 1.04032650\n",
      "Iteration 23, loss = 1.02696246\n",
      "Iteration 24, loss = 1.01407218\n",
      "Iteration 25, loss = 1.00150299\n",
      "Iteration 26, loss = 0.98918104\n",
      "Iteration 27, loss = 0.97712368\n",
      "Iteration 28, loss = 0.96533392\n",
      "Iteration 29, loss = 0.95377553\n",
      "Iteration 30, loss = 0.94249394\n",
      "Iteration 31, loss = 0.93144504\n",
      "Iteration 32, loss = 0.92059150\n",
      "Iteration 33, loss = 0.90995191\n",
      "Iteration 34, loss = 0.89958853\n",
      "Iteration 35, loss = 0.88947540\n",
      "Iteration 36, loss = 0.87957441\n",
      "Iteration 37, loss = 0.86991875\n",
      "Iteration 38, loss = 0.86045471\n",
      "Iteration 39, loss = 0.85123926\n",
      "Iteration 40, loss = 0.84224884\n",
      "Iteration 41, loss = 0.83348241\n",
      "Iteration 42, loss = 0.82488246\n",
      "Iteration 43, loss = 0.81639812\n",
      "Iteration 44, loss = 0.80809412\n",
      "Iteration 45, loss = 0.79998216\n",
      "Iteration 46, loss = 0.79203494\n",
      "Iteration 47, loss = 0.78423638\n",
      "Iteration 48, loss = 0.77656046\n",
      "Iteration 49, loss = 0.76900835\n",
      "Iteration 50, loss = 0.76156816\n",
      "Iteration 51, loss = 0.75422717\n",
      "Iteration 52, loss = 0.74700400\n",
      "Iteration 53, loss = 0.73991132\n",
      "Iteration 54, loss = 0.73294245\n",
      "Iteration 55, loss = 0.72606671\n",
      "Iteration 56, loss = 0.71929340\n",
      "Iteration 57, loss = 0.71265399\n",
      "Iteration 58, loss = 0.70616233\n",
      "Iteration 59, loss = 0.69982722\n",
      "Iteration 60, loss = 0.69366489\n",
      "Iteration 61, loss = 0.68768801\n",
      "Iteration 62, loss = 0.68188091\n",
      "Iteration 63, loss = 0.67623364\n",
      "Iteration 64, loss = 0.67072018\n",
      "Iteration 65, loss = 0.66530892\n",
      "Iteration 66, loss = 0.65998882\n",
      "Iteration 67, loss = 0.65475847\n",
      "Iteration 68, loss = 0.64960544\n",
      "Iteration 69, loss = 0.64448878\n",
      "Iteration 70, loss = 0.63932522\n",
      "Iteration 71, loss = 0.63415469\n",
      "Iteration 72, loss = 0.62893063\n",
      "Iteration 73, loss = 0.62359068\n",
      "Iteration 74, loss = 0.61834293\n",
      "Iteration 75, loss = 0.61330749\n",
      "Iteration 76, loss = 0.60835569\n",
      "Iteration 77, loss = 0.60350679\n",
      "Iteration 78, loss = 0.59876682\n",
      "Iteration 79, loss = 0.59413954\n",
      "Iteration 80, loss = 0.58963311\n",
      "Iteration 81, loss = 0.58530770\n",
      "Iteration 82, loss = 0.58108174\n",
      "Iteration 83, loss = 0.57694470\n",
      "Iteration 84, loss = 0.57288514\n",
      "Iteration 85, loss = 0.56889097\n",
      "Iteration 86, loss = 0.56492062\n",
      "Iteration 87, loss = 0.56097355\n",
      "Iteration 88, loss = 0.55705845\n",
      "Iteration 89, loss = 0.55320558\n",
      "Iteration 90, loss = 0.54941907\n",
      "Iteration 91, loss = 0.54566937\n",
      "Iteration 92, loss = 0.54195673\n",
      "Iteration 93, loss = 0.53828795\n",
      "Iteration 94, loss = 0.53465753\n",
      "Iteration 95, loss = 0.53106206\n",
      "Iteration 96, loss = 0.52750386\n",
      "Iteration 97, loss = 0.52394044\n",
      "Iteration 98, loss = 0.52037199\n",
      "Iteration 99, loss = 0.51680442\n",
      "Iteration 100, loss = 0.51326936\n",
      "Iteration 101, loss = 0.50983210\n",
      "Iteration 102, loss = 0.50647970\n",
      "Iteration 103, loss = 0.50317074\n",
      "Iteration 104, loss = 0.49988810\n",
      "Iteration 105, loss = 0.49662961\n",
      "Iteration 106, loss = 0.49340307\n",
      "Iteration 107, loss = 0.49020386\n",
      "Iteration 108, loss = 0.48703471\n",
      "Iteration 109, loss = 0.48389133\n",
      "Iteration 110, loss = 0.48076778\n",
      "Iteration 111, loss = 0.47767347\n",
      "Iteration 112, loss = 0.47460701\n",
      "Iteration 113, loss = 0.47155855\n",
      "Iteration 114, loss = 0.46853113\n",
      "Iteration 115, loss = 0.46552085\n",
      "Iteration 116, loss = 0.46253154\n",
      "Iteration 117, loss = 0.45956414\n",
      "Iteration 118, loss = 0.45661019\n",
      "Iteration 119, loss = 0.45367089\n",
      "Iteration 120, loss = 0.45073185\n",
      "Iteration 121, loss = 0.44778847\n",
      "Iteration 122, loss = 0.44484136\n",
      "Iteration 123, loss = 0.44187695\n",
      "Iteration 124, loss = 0.43886461\n",
      "Iteration 125, loss = 0.43582071\n",
      "Iteration 126, loss = 0.43279740\n",
      "Iteration 127, loss = 0.42970812\n",
      "Iteration 128, loss = 0.42664726\n",
      "Iteration 129, loss = 0.42354090\n",
      "Iteration 130, loss = 0.42041211\n",
      "Iteration 131, loss = 0.41733570\n",
      "Iteration 132, loss = 0.41439339\n",
      "Iteration 133, loss = 0.41157583\n",
      "Iteration 134, loss = 0.40897280\n",
      "Iteration 135, loss = 0.40647489\n",
      "Iteration 136, loss = 0.40406275\n",
      "Iteration 137, loss = 0.40168095\n",
      "Iteration 138, loss = 0.39935046\n",
      "Iteration 139, loss = 0.39705396\n",
      "Iteration 140, loss = 0.39474726\n",
      "Iteration 141, loss = 0.39243858\n",
      "Iteration 142, loss = 0.39012766\n",
      "Iteration 143, loss = 0.38782180\n",
      "Iteration 144, loss = 0.38552253\n",
      "Iteration 145, loss = 0.38322665\n",
      "Iteration 146, loss = 0.38094592\n",
      "Iteration 147, loss = 0.37866807\n",
      "Iteration 148, loss = 0.37640245\n",
      "Iteration 149, loss = 0.37414100\n",
      "Iteration 150, loss = 0.37189365\n",
      "Iteration 151, loss = 0.36965327\n",
      "Iteration 152, loss = 0.36743509\n",
      "Iteration 153, loss = 0.36524070\n",
      "Iteration 154, loss = 0.36307141\n",
      "Iteration 155, loss = 0.36092684\n",
      "Iteration 156, loss = 0.35879917\n",
      "Iteration 157, loss = 0.35669030\n",
      "Iteration 158, loss = 0.35459692\n",
      "Iteration 159, loss = 0.35252425\n",
      "Iteration 160, loss = 0.35046591\n",
      "Iteration 161, loss = 0.34842492\n",
      "Iteration 162, loss = 0.34639700\n",
      "Iteration 163, loss = 0.34438344\n",
      "Iteration 164, loss = 0.34237993\n",
      "Iteration 165, loss = 0.34039919\n",
      "Iteration 166, loss = 0.33843113\n",
      "Iteration 167, loss = 0.33646828\n",
      "Iteration 168, loss = 0.33451479\n",
      "Iteration 169, loss = 0.33257377\n",
      "Iteration 170, loss = 0.33064545\n",
      "Iteration 171, loss = 0.32872977\n",
      "Iteration 172, loss = 0.32682390\n",
      "Iteration 173, loss = 0.32492726\n",
      "Iteration 174, loss = 0.32304025\n",
      "Iteration 175, loss = 0.32116319\n",
      "Iteration 176, loss = 0.31929706\n",
      "Iteration 177, loss = 0.31744101\n",
      "Iteration 178, loss = 0.31559413\n",
      "Iteration 179, loss = 0.31375864\n",
      "Iteration 180, loss = 0.31193339\n",
      "Iteration 181, loss = 0.31012710\n",
      "Iteration 182, loss = 0.30833644\n",
      "Iteration 183, loss = 0.30655744\n",
      "Iteration 184, loss = 0.30479094\n",
      "Iteration 185, loss = 0.30303910\n",
      "Iteration 186, loss = 0.30130668\n",
      "Iteration 187, loss = 0.29958547\n",
      "Iteration 188, loss = 0.29787614\n",
      "Iteration 189, loss = 0.29617497\n",
      "Iteration 190, loss = 0.29447864\n",
      "Iteration 191, loss = 0.29279244\n",
      "Iteration 192, loss = 0.29111807\n",
      "Iteration 193, loss = 0.28945511\n",
      "Iteration 194, loss = 0.28780870\n",
      "Iteration 195, loss = 0.28617552\n",
      "Iteration 196, loss = 0.28455418\n",
      "Iteration 197, loss = 0.28294368\n",
      "Iteration 198, loss = 0.28134407\n",
      "Iteration 199, loss = 0.27975630\n",
      "Iteration 200, loss = 0.27817929\n",
      "Iteration 201, loss = 0.27661286\n",
      "Iteration 202, loss = 0.27505718\n",
      "Iteration 203, loss = 0.27351252\n",
      "Iteration 204, loss = 0.27197794\n",
      "Iteration 205, loss = 0.27045476\n",
      "Iteration 206, loss = 0.26894467\n",
      "Iteration 207, loss = 0.26744681\n",
      "Iteration 208, loss = 0.26595951\n",
      "Iteration 209, loss = 0.26448354\n",
      "Iteration 210, loss = 0.26301865\n",
      "Iteration 211, loss = 0.26156414\n",
      "Iteration 212, loss = 0.26011954\n",
      "Iteration 213, loss = 0.25868661\n",
      "Iteration 214, loss = 0.25726429\n",
      "Iteration 215, loss = 0.25585268\n",
      "Iteration 216, loss = 0.25445276\n",
      "Iteration 217, loss = 0.25306329\n",
      "Iteration 218, loss = 0.25168425\n",
      "Iteration 219, loss = 0.25031606\n",
      "Iteration 220, loss = 0.24895826\n",
      "Iteration 221, loss = 0.24761080\n",
      "Iteration 222, loss = 0.24627393\n",
      "Iteration 223, loss = 0.24494963\n",
      "Iteration 224, loss = 0.24363613\n",
      "Iteration 225, loss = 0.24233352\n",
      "Iteration 226, loss = 0.24104143\n",
      "Iteration 227, loss = 0.23975944\n",
      "Iteration 228, loss = 0.23848770\n",
      "Iteration 229, loss = 0.23722617\n",
      "Iteration 230, loss = 0.23597488\n",
      "Iteration 231, loss = 0.23473429\n",
      "Iteration 232, loss = 0.23350343\n",
      "Iteration 233, loss = 0.23228202\n",
      "Iteration 234, loss = 0.23107000\n",
      "Iteration 235, loss = 0.22986785\n",
      "Iteration 236, loss = 0.22867543\n",
      "Iteration 237, loss = 0.22749253\n",
      "Iteration 238, loss = 0.22632016\n",
      "Iteration 239, loss = 0.22515711\n",
      "Iteration 240, loss = 0.22400338\n",
      "Iteration 241, loss = 0.22285903\n",
      "Iteration 242, loss = 0.22172557\n",
      "Iteration 243, loss = 0.22060037\n",
      "Iteration 244, loss = 0.21948409\n",
      "Iteration 245, loss = 0.21837763\n",
      "Iteration 246, loss = 0.21728078\n",
      "Iteration 247, loss = 0.21619304\n",
      "Iteration 248, loss = 0.21511398\n",
      "Iteration 249, loss = 0.21404445\n",
      "Iteration 250, loss = 0.21298349\n",
      "Iteration 251, loss = 0.21193188\n",
      "Iteration 252, loss = 0.21088876\n",
      "Iteration 253, loss = 0.20985423\n",
      "Iteration 254, loss = 0.20882825\n",
      "Iteration 255, loss = 0.20781114\n",
      "Iteration 256, loss = 0.20680382\n",
      "Iteration 257, loss = 0.20580458\n",
      "Iteration 258, loss = 0.20481341\n",
      "Iteration 259, loss = 0.20383026\n",
      "Iteration 260, loss = 0.20285656\n",
      "Iteration 261, loss = 0.20189010\n",
      "Iteration 262, loss = 0.20093131\n",
      "Iteration 263, loss = 0.19998089\n",
      "Iteration 264, loss = 0.19903839\n",
      "Iteration 265, loss = 0.19810420\n",
      "Iteration 266, loss = 0.19717682\n",
      "Iteration 267, loss = 0.19625492\n",
      "Iteration 268, loss = 0.19533925\n",
      "Iteration 269, loss = 0.19443062\n",
      "Iteration 270, loss = 0.19352833\n",
      "Iteration 271, loss = 0.19263177\n",
      "Iteration 272, loss = 0.19173911\n",
      "Iteration 273, loss = 0.19085259\n",
      "Iteration 274, loss = 0.18997254\n",
      "Iteration 275, loss = 0.18909679\n",
      "Iteration 276, loss = 0.18822364\n",
      "Iteration 277, loss = 0.18735632\n",
      "Iteration 278, loss = 0.18649537\n",
      "Iteration 279, loss = 0.18563651\n",
      "Iteration 280, loss = 0.18477466\n",
      "Iteration 281, loss = 0.18391579\n",
      "Iteration 282, loss = 0.18306117\n",
      "Iteration 283, loss = 0.18219046\n",
      "Iteration 284, loss = 0.18131453\n",
      "Iteration 285, loss = 0.18043046\n",
      "Iteration 286, loss = 0.17953560\n",
      "Iteration 287, loss = 0.17865070\n",
      "Iteration 288, loss = 0.17776537\n",
      "Iteration 289, loss = 0.17687681\n",
      "Iteration 290, loss = 0.17599775\n",
      "Iteration 291, loss = 0.17515364\n",
      "Iteration 292, loss = 0.17428573\n",
      "Iteration 293, loss = 0.17339455\n",
      "Iteration 294, loss = 0.17248434\n",
      "Iteration 295, loss = 0.17157843\n",
      "Iteration 296, loss = 0.17070964\n",
      "Iteration 297, loss = 0.16987279\n",
      "Iteration 298, loss = 0.16908287\n",
      "Iteration 299, loss = 0.16832211\n",
      "Iteration 300, loss = 0.16763732\n",
      "Iteration 301, loss = 0.16699571\n",
      "Iteration 302, loss = 0.16636146\n",
      "Iteration 303, loss = 0.16573215\n",
      "Iteration 304, loss = 0.16511519\n",
      "Iteration 305, loss = 0.16449982\n",
      "Iteration 306, loss = 0.16388514\n",
      "Iteration 307, loss = 0.16327237\n",
      "Iteration 308, loss = 0.16266080\n",
      "Iteration 309, loss = 0.16205066\n",
      "Iteration 310, loss = 0.16144201\n",
      "Iteration 311, loss = 0.16083618\n",
      "Iteration 312, loss = 0.16023240\n",
      "Iteration 313, loss = 0.15963191\n",
      "Iteration 314, loss = 0.15903391\n",
      "Iteration 315, loss = 0.15843905\n",
      "Iteration 316, loss = 0.15784864\n",
      "Iteration 317, loss = 0.15726293\n",
      "Iteration 318, loss = 0.15668081\n",
      "Iteration 319, loss = 0.15610294\n",
      "Iteration 320, loss = 0.15552777\n",
      "Iteration 321, loss = 0.15496093\n",
      "Iteration 322, loss = 0.15440224\n",
      "Iteration 323, loss = 0.15384781\n",
      "Iteration 324, loss = 0.15329788\n",
      "Iteration 325, loss = 0.15275346\n",
      "Iteration 326, loss = 0.15221531\n",
      "Iteration 327, loss = 0.15168321\n",
      "Iteration 328, loss = 0.15115770\n",
      "Iteration 329, loss = 0.15063655\n",
      "Iteration 330, loss = 0.15011484\n",
      "Iteration 331, loss = 0.14957434\n",
      "Iteration 332, loss = 0.14906955\n",
      "Iteration 333, loss = 0.14857597\n",
      "Iteration 334, loss = 0.14808108\n",
      "Iteration 335, loss = 0.14758619\n",
      "Iteration 336, loss = 0.14709192\n",
      "Iteration 337, loss = 0.14659851\n",
      "Iteration 338, loss = 0.14610561\n",
      "Iteration 339, loss = 0.14561416\n",
      "Iteration 340, loss = 0.14512522\n",
      "Iteration 341, loss = 0.14463866\n",
      "Iteration 342, loss = 0.14415687\n",
      "Iteration 343, loss = 0.14368561\n",
      "Iteration 344, loss = 0.14320837\n",
      "Iteration 345, loss = 0.14273327\n",
      "Iteration 346, loss = 0.14226636\n",
      "Iteration 347, loss = 0.14180317\n",
      "Iteration 348, loss = 0.14134212\n",
      "Iteration 349, loss = 0.14088308\n",
      "Iteration 350, loss = 0.14042555\n",
      "Iteration 351, loss = 0.13996982\n",
      "Iteration 352, loss = 0.13951631\n",
      "Iteration 353, loss = 0.13906495\n",
      "Iteration 354, loss = 0.13861621\n",
      "Iteration 355, loss = 0.13817066\n",
      "Iteration 356, loss = 0.13772829\n",
      "Iteration 357, loss = 0.13728761\n",
      "Iteration 358, loss = 0.13684814\n",
      "Iteration 359, loss = 0.13641222\n",
      "Iteration 360, loss = 0.13597900\n",
      "Iteration 361, loss = 0.13554865\n",
      "Iteration 362, loss = 0.13512045\n",
      "Iteration 363, loss = 0.13469427\n",
      "Iteration 364, loss = 0.13427037\n",
      "Iteration 365, loss = 0.13384925\n",
      "Iteration 366, loss = 0.13343095\n",
      "Iteration 367, loss = 0.13301544\n",
      "Iteration 368, loss = 0.13260181\n",
      "Iteration 369, loss = 0.13219053\n",
      "Iteration 370, loss = 0.13178217\n",
      "Iteration 371, loss = 0.13137647\n",
      "Iteration 372, loss = 0.13097314\n",
      "Iteration 373, loss = 0.13057226\n",
      "Iteration 374, loss = 0.13017405\n",
      "Iteration 375, loss = 0.12977855\n",
      "Iteration 376, loss = 0.12938564\n",
      "Iteration 377, loss = 0.12899523\n",
      "Iteration 378, loss = 0.12860759\n",
      "Iteration 379, loss = 0.12822258\n",
      "Iteration 380, loss = 0.12783976\n",
      "Iteration 381, loss = 0.12745983\n",
      "Iteration 382, loss = 0.12708262\n",
      "Iteration 383, loss = 0.12670803\n",
      "Iteration 384, loss = 0.12633598\n",
      "Iteration 385, loss = 0.12596646\n",
      "Iteration 386, loss = 0.12559954\n",
      "Iteration 387, loss = 0.12523519\n",
      "Iteration 388, loss = 0.12487335\n",
      "Iteration 389, loss = 0.12451411\n",
      "Iteration 390, loss = 0.12415761\n",
      "Iteration 391, loss = 0.12380322\n",
      "Iteration 392, loss = 0.12345177\n",
      "Iteration 393, loss = 0.12310265\n",
      "Iteration 394, loss = 0.12275540\n",
      "Iteration 395, loss = 0.12241074\n",
      "Iteration 396, loss = 0.12206790\n",
      "Iteration 397, loss = 0.12172667\n",
      "Iteration 398, loss = 0.12138705\n",
      "Iteration 399, loss = 0.12104909\n",
      "Iteration 400, loss = 0.12071231\n",
      "Iteration 401, loss = 0.12037659\n",
      "Iteration 402, loss = 0.12004184\n",
      "Iteration 403, loss = 0.11970808\n",
      "Iteration 404, loss = 0.11937554\n",
      "Iteration 405, loss = 0.11904431\n",
      "Iteration 406, loss = 0.11871401\n",
      "Iteration 407, loss = 0.11838508\n",
      "Iteration 408, loss = 0.11805728\n",
      "Iteration 409, loss = 0.11773013\n",
      "Iteration 410, loss = 0.11740377\n",
      "Iteration 411, loss = 0.11707854\n",
      "Iteration 412, loss = 0.11675453\n",
      "Iteration 413, loss = 0.11643193\n",
      "Iteration 414, loss = 0.11611034\n",
      "Iteration 415, loss = 0.11578983\n",
      "Iteration 416, loss = 0.11547077\n",
      "Iteration 417, loss = 0.11515295\n",
      "Iteration 418, loss = 0.11483643\n",
      "Iteration 419, loss = 0.11452133\n",
      "Iteration 420, loss = 0.11420769\n",
      "Iteration 421, loss = 0.11389550\n",
      "Iteration 422, loss = 0.11358475\n",
      "Iteration 423, loss = 0.11327544\n",
      "Iteration 424, loss = 0.11296756\n",
      "Iteration 425, loss = 0.11266143\n",
      "Iteration 426, loss = 0.11235621\n",
      "Iteration 427, loss = 0.11205217\n",
      "Iteration 428, loss = 0.11174925\n",
      "Iteration 429, loss = 0.11144749\n",
      "Iteration 430, loss = 0.11114700\n",
      "Iteration 431, loss = 0.11084789\n",
      "Iteration 432, loss = 0.11054990\n",
      "Iteration 433, loss = 0.11025286\n",
      "Iteration 434, loss = 0.10995673\n",
      "Iteration 435, loss = 0.10966147\n",
      "Iteration 436, loss = 0.10936718\n",
      "Iteration 437, loss = 0.10907368\n",
      "Iteration 438, loss = 0.10878114\n",
      "Iteration 439, loss = 0.10848973\n",
      "Iteration 440, loss = 0.10819904\n",
      "Iteration 441, loss = 0.10790911\n",
      "Iteration 442, loss = 0.10762054\n",
      "Iteration 443, loss = 0.10733311\n",
      "Iteration 444, loss = 0.10704684\n",
      "Iteration 445, loss = 0.10676157\n",
      "Iteration 446, loss = 0.10647748\n",
      "Iteration 447, loss = 0.10619459\n",
      "Iteration 448, loss = 0.10591299\n",
      "Iteration 449, loss = 0.10563274\n",
      "Iteration 450, loss = 0.10535392\n",
      "Iteration 451, loss = 0.10507658\n",
      "Iteration 452, loss = 0.10480073\n",
      "Iteration 453, loss = 0.10452644\n",
      "Iteration 454, loss = 0.10425380\n",
      "Iteration 455, loss = 0.10398269\n",
      "Iteration 456, loss = 0.10371340\n",
      "Iteration 457, loss = 0.10344586\n",
      "Iteration 458, loss = 0.10318005\n",
      "Iteration 459, loss = 0.10291611\n",
      "Iteration 460, loss = 0.10265404\n",
      "Iteration 461, loss = 0.10239376\n",
      "Iteration 462, loss = 0.10213541\n",
      "Iteration 463, loss = 0.10187905\n",
      "Iteration 464, loss = 0.10162468\n",
      "Iteration 465, loss = 0.10137226\n",
      "Iteration 466, loss = 0.10112206\n",
      "Iteration 467, loss = 0.10087383\n",
      "Iteration 468, loss = 0.10062787\n",
      "Iteration 469, loss = 0.10038393\n",
      "Iteration 470, loss = 0.10014210\n",
      "Iteration 471, loss = 0.09990235\n",
      "Iteration 472, loss = 0.09966468\n",
      "Iteration 473, loss = 0.09942919\n",
      "Iteration 474, loss = 0.09919575\n",
      "Iteration 475, loss = 0.09896447\n",
      "Iteration 476, loss = 0.09873534\n",
      "Iteration 477, loss = 0.09850833\n",
      "Iteration 478, loss = 0.09828343\n",
      "Iteration 479, loss = 0.09806071\n",
      "Iteration 480, loss = 0.09784015\n",
      "Iteration 481, loss = 0.09762169\n",
      "Iteration 482, loss = 0.09740547\n",
      "Iteration 483, loss = 0.09719147\n",
      "Iteration 484, loss = 0.09697962\n",
      "Iteration 485, loss = 0.09676985\n",
      "Iteration 486, loss = 0.09656216\n",
      "Iteration 487, loss = 0.09635658\n",
      "Iteration 488, loss = 0.09615307\n",
      "Iteration 489, loss = 0.09595165\n",
      "Iteration 490, loss = 0.09575230\n",
      "Iteration 491, loss = 0.09555527\n",
      "Iteration 492, loss = 0.09536027\n",
      "Iteration 493, loss = 0.09516728\n",
      "Iteration 494, loss = 0.09497650\n",
      "Iteration 495, loss = 0.09478765\n",
      "Iteration 496, loss = 0.09460069\n",
      "Iteration 497, loss = 0.09441559\n",
      "Iteration 498, loss = 0.09423241\n",
      "Iteration 499, loss = 0.09405120\n",
      "Iteration 500, loss = 0.09387186\n",
      "Iteration 501, loss = 0.09369428\n",
      "Iteration 502, loss = 0.09351874\n",
      "Iteration 503, loss = 0.09334507\n",
      "Iteration 504, loss = 0.09317333\n",
      "Iteration 505, loss = 0.09300332\n",
      "Iteration 506, loss = 0.09283526\n",
      "Iteration 507, loss = 0.09266887\n",
      "Iteration 508, loss = 0.09250430\n",
      "Iteration 509, loss = 0.09234159\n",
      "Iteration 510, loss = 0.09218056\n",
      "Iteration 511, loss = 0.09202127\n",
      "Iteration 512, loss = 0.09186364\n",
      "Iteration 513, loss = 0.09170794\n",
      "Iteration 514, loss = 0.09155389\n",
      "Iteration 515, loss = 0.09140141\n",
      "Iteration 516, loss = 0.09125057\n",
      "Iteration 517, loss = 0.09110130\n",
      "Iteration 518, loss = 0.09095370\n",
      "Iteration 519, loss = 0.09080772\n",
      "Iteration 520, loss = 0.09066334\n",
      "Iteration 521, loss = 0.09052047\n",
      "Iteration 522, loss = 0.09037912\n",
      "Iteration 523, loss = 0.09023933\n",
      "Iteration 524, loss = 0.09010099\n",
      "Iteration 525, loss = 0.08996414\n",
      "Iteration 526, loss = 0.08982881\n",
      "Iteration 527, loss = 0.08969497\n",
      "Iteration 528, loss = 0.08956251\n",
      "Iteration 529, loss = 0.08943149\n",
      "Iteration 530, loss = 0.08930179\n",
      "Iteration 531, loss = 0.08917343\n",
      "Iteration 532, loss = 0.08904645\n",
      "Iteration 533, loss = 0.08892083\n",
      "Iteration 534, loss = 0.08879649\n",
      "Iteration 535, loss = 0.08867344\n",
      "Iteration 536, loss = 0.08855166\n",
      "Iteration 537, loss = 0.08843112\n",
      "Iteration 538, loss = 0.08831190\n",
      "Iteration 539, loss = 0.08819396\n",
      "Iteration 540, loss = 0.08807685\n",
      "Iteration 541, loss = 0.08796048\n",
      "Iteration 542, loss = 0.08784486\n",
      "Iteration 543, loss = 0.08773012\n",
      "Iteration 544, loss = 0.08761619\n",
      "Iteration 545, loss = 0.08750317\n",
      "Iteration 546, loss = 0.08739101\n",
      "Iteration 547, loss = 0.08727972\n",
      "Iteration 548, loss = 0.08716946\n",
      "Iteration 549, loss = 0.08706015\n",
      "Iteration 550, loss = 0.08695258\n",
      "Iteration 551, loss = 0.08684702\n",
      "Iteration 552, loss = 0.08674260\n",
      "Iteration 553, loss = 0.08663933\n",
      "Iteration 554, loss = 0.08653736\n",
      "Iteration 555, loss = 0.08643662\n",
      "Iteration 556, loss = 0.08633689\n",
      "Iteration 557, loss = 0.08623825\n",
      "Iteration 558, loss = 0.08614097\n",
      "Iteration 559, loss = 0.08604459\n",
      "Iteration 560, loss = 0.08594921\n",
      "Iteration 561, loss = 0.08585459\n",
      "Iteration 562, loss = 0.08576094\n",
      "Iteration 563, loss = 0.08566812\n",
      "Iteration 564, loss = 0.08557609\n",
      "Iteration 565, loss = 0.08548496\n",
      "Iteration 566, loss = 0.08539469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48410645\n",
      "Iteration 2, loss = 1.45483051\n",
      "Iteration 3, loss = 1.42637119\n",
      "Iteration 4, loss = 1.39878816\n",
      "Iteration 5, loss = 1.37212407\n",
      "Iteration 6, loss = 1.34629895\n",
      "Iteration 7, loss = 1.32115228\n",
      "Iteration 8, loss = 1.29672099\n",
      "Iteration 9, loss = 1.27308988\n",
      "Iteration 10, loss = 1.25027877\n",
      "Iteration 11, loss = 1.22824449\n",
      "Iteration 12, loss = 1.20705794\n",
      "Iteration 13, loss = 1.18695462\n",
      "Iteration 14, loss = 1.16791924\n",
      "Iteration 15, loss = 1.14969120\n",
      "Iteration 16, loss = 1.13215846\n",
      "Iteration 17, loss = 1.11524287\n",
      "Iteration 18, loss = 1.09889439\n",
      "Iteration 19, loss = 1.08301134\n",
      "Iteration 20, loss = 1.06758345\n",
      "Iteration 21, loss = 1.05273809\n",
      "Iteration 22, loss = 1.03839828\n",
      "Iteration 23, loss = 1.02448386\n",
      "Iteration 24, loss = 1.01102205\n",
      "Iteration 25, loss = 0.99787970\n",
      "Iteration 26, loss = 0.98505212\n",
      "Iteration 27, loss = 0.97257278\n",
      "Iteration 28, loss = 0.96044402\n",
      "Iteration 29, loss = 0.94860421\n",
      "Iteration 30, loss = 0.93707605\n",
      "Iteration 31, loss = 0.92580660\n",
      "Iteration 32, loss = 0.91479272\n",
      "Iteration 33, loss = 0.90402010\n",
      "Iteration 34, loss = 0.89355373\n",
      "Iteration 35, loss = 0.88338896\n",
      "Iteration 36, loss = 0.87351350\n",
      "Iteration 37, loss = 0.86400795\n",
      "Iteration 38, loss = 0.85482124\n",
      "Iteration 39, loss = 0.84602083\n",
      "Iteration 40, loss = 0.83749757\n",
      "Iteration 41, loss = 0.82926818\n",
      "Iteration 42, loss = 0.82122401\n",
      "Iteration 43, loss = 0.81338129\n",
      "Iteration 44, loss = 0.80574439\n",
      "Iteration 45, loss = 0.79831726\n",
      "Iteration 46, loss = 0.79106192\n",
      "Iteration 47, loss = 0.78395270\n",
      "Iteration 48, loss = 0.77698523\n",
      "Iteration 49, loss = 0.77015433\n",
      "Iteration 50, loss = 0.76343826\n",
      "Iteration 51, loss = 0.75682193\n",
      "Iteration 52, loss = 0.75031441\n",
      "Iteration 53, loss = 0.74390690\n",
      "Iteration 54, loss = 0.73759176\n",
      "Iteration 55, loss = 0.73133584\n",
      "Iteration 56, loss = 0.72513261\n",
      "Iteration 57, loss = 0.71898659\n",
      "Iteration 58, loss = 0.71286586\n",
      "Iteration 59, loss = 0.70674674\n",
      "Iteration 60, loss = 0.70062448\n",
      "Iteration 61, loss = 0.69448752\n",
      "Iteration 62, loss = 0.68835783\n",
      "Iteration 63, loss = 0.68221830\n",
      "Iteration 64, loss = 0.67607921\n",
      "Iteration 65, loss = 0.67000337\n",
      "Iteration 66, loss = 0.66397680\n",
      "Iteration 67, loss = 0.65802378\n",
      "Iteration 68, loss = 0.65213357\n",
      "Iteration 69, loss = 0.64630491\n",
      "Iteration 70, loss = 0.64052892\n",
      "Iteration 71, loss = 0.63483577\n",
      "Iteration 72, loss = 0.62921175\n",
      "Iteration 73, loss = 0.62368768\n",
      "Iteration 74, loss = 0.61825399\n",
      "Iteration 75, loss = 0.61292028\n",
      "Iteration 76, loss = 0.60771728\n",
      "Iteration 77, loss = 0.60270860\n",
      "Iteration 78, loss = 0.59785327\n",
      "Iteration 79, loss = 0.59313472\n",
      "Iteration 80, loss = 0.58854443\n",
      "Iteration 81, loss = 0.58404863\n",
      "Iteration 82, loss = 0.57963280\n",
      "Iteration 83, loss = 0.57525391\n",
      "Iteration 84, loss = 0.57094341\n",
      "Iteration 85, loss = 0.56669964\n",
      "Iteration 86, loss = 0.56251259\n",
      "Iteration 87, loss = 0.55837329\n",
      "Iteration 88, loss = 0.55427766\n",
      "Iteration 89, loss = 0.55023127\n",
      "Iteration 90, loss = 0.54624442\n",
      "Iteration 91, loss = 0.54231563\n",
      "Iteration 92, loss = 0.53844719\n",
      "Iteration 93, loss = 0.53463412\n",
      "Iteration 94, loss = 0.53087071\n",
      "Iteration 95, loss = 0.52713868\n",
      "Iteration 96, loss = 0.52341503\n",
      "Iteration 97, loss = 0.51969532\n",
      "Iteration 98, loss = 0.51598508\n",
      "Iteration 99, loss = 0.51230417\n",
      "Iteration 100, loss = 0.50871444\n",
      "Iteration 101, loss = 0.50522235\n",
      "Iteration 102, loss = 0.50178322\n",
      "Iteration 103, loss = 0.49837506\n",
      "Iteration 104, loss = 0.49499590\n",
      "Iteration 105, loss = 0.49164715\n",
      "Iteration 106, loss = 0.48833143\n",
      "Iteration 107, loss = 0.48504539\n",
      "Iteration 108, loss = 0.48178988\n",
      "Iteration 109, loss = 0.47856407\n",
      "Iteration 110, loss = 0.47536649\n",
      "Iteration 111, loss = 0.47219340\n",
      "Iteration 112, loss = 0.46904547\n",
      "Iteration 113, loss = 0.46592113\n",
      "Iteration 114, loss = 0.46281570\n",
      "Iteration 115, loss = 0.45972129\n",
      "Iteration 116, loss = 0.45663697\n",
      "Iteration 117, loss = 0.45357159\n",
      "Iteration 118, loss = 0.45051542\n",
      "Iteration 119, loss = 0.44744782\n",
      "Iteration 120, loss = 0.44435978\n",
      "Iteration 121, loss = 0.44125838\n",
      "Iteration 122, loss = 0.43810107\n",
      "Iteration 123, loss = 0.43494940\n",
      "Iteration 124, loss = 0.43178621\n",
      "Iteration 125, loss = 0.42855243\n",
      "Iteration 126, loss = 0.42531792\n",
      "Iteration 127, loss = 0.42212317\n",
      "Iteration 128, loss = 0.41893802\n",
      "Iteration 129, loss = 0.41585362\n",
      "Iteration 130, loss = 0.41285324\n",
      "Iteration 131, loss = 0.41007267\n",
      "Iteration 132, loss = 0.40746026\n",
      "Iteration 133, loss = 0.40495871\n",
      "Iteration 134, loss = 0.40250049\n",
      "Iteration 135, loss = 0.40007779\n",
      "Iteration 136, loss = 0.39768742\n",
      "Iteration 137, loss = 0.39530276\n",
      "Iteration 138, loss = 0.39290105\n",
      "Iteration 139, loss = 0.39050619\n",
      "Iteration 140, loss = 0.38811349\n",
      "Iteration 141, loss = 0.38571720\n",
      "Iteration 142, loss = 0.38332866\n",
      "Iteration 143, loss = 0.38094816\n",
      "Iteration 144, loss = 0.37857158\n",
      "Iteration 145, loss = 0.37621400\n",
      "Iteration 146, loss = 0.37386393\n",
      "Iteration 147, loss = 0.37152357\n",
      "Iteration 148, loss = 0.36919455\n",
      "Iteration 149, loss = 0.36687317\n",
      "Iteration 150, loss = 0.36457244\n",
      "Iteration 151, loss = 0.36230138\n",
      "Iteration 152, loss = 0.36004806\n",
      "Iteration 153, loss = 0.35780991\n",
      "Iteration 154, loss = 0.35559396\n",
      "Iteration 155, loss = 0.35340240\n",
      "Iteration 156, loss = 0.35122417\n",
      "Iteration 157, loss = 0.34906200\n",
      "Iteration 158, loss = 0.34692624\n",
      "Iteration 159, loss = 0.34481660\n",
      "Iteration 160, loss = 0.34273050\n",
      "Iteration 161, loss = 0.34066047\n",
      "Iteration 162, loss = 0.33859718\n",
      "Iteration 163, loss = 0.33655338\n",
      "Iteration 164, loss = 0.33452274\n",
      "Iteration 165, loss = 0.33249764\n",
      "Iteration 166, loss = 0.33048340\n",
      "Iteration 167, loss = 0.32848011\n",
      "Iteration 168, loss = 0.32648677\n",
      "Iteration 169, loss = 0.32450781\n",
      "Iteration 170, loss = 0.32254660\n",
      "Iteration 171, loss = 0.32059971\n",
      "Iteration 172, loss = 0.31866419\n",
      "Iteration 173, loss = 0.31673830\n",
      "Iteration 174, loss = 0.31482505\n",
      "Iteration 175, loss = 0.31292572\n",
      "Iteration 176, loss = 0.31103328\n",
      "Iteration 177, loss = 0.30915183\n",
      "Iteration 178, loss = 0.30727917\n",
      "Iteration 179, loss = 0.30541699\n",
      "Iteration 180, loss = 0.30356468\n",
      "Iteration 181, loss = 0.30172129\n",
      "Iteration 182, loss = 0.29988731\n",
      "Iteration 183, loss = 0.29806471\n",
      "Iteration 184, loss = 0.29625533\n",
      "Iteration 185, loss = 0.29445612\n",
      "Iteration 186, loss = 0.29267128\n",
      "Iteration 187, loss = 0.29089972\n",
      "Iteration 188, loss = 0.28914046\n",
      "Iteration 189, loss = 0.28739427\n",
      "Iteration 190, loss = 0.28566294\n",
      "Iteration 191, loss = 0.28394583\n",
      "Iteration 192, loss = 0.28223899\n",
      "Iteration 193, loss = 0.28053978\n",
      "Iteration 194, loss = 0.27885009\n",
      "Iteration 195, loss = 0.27717008\n",
      "Iteration 196, loss = 0.27549989\n",
      "Iteration 197, loss = 0.27384098\n",
      "Iteration 198, loss = 0.27219556\n",
      "Iteration 199, loss = 0.27056202\n",
      "Iteration 200, loss = 0.26893976\n",
      "Iteration 201, loss = 0.26732822\n",
      "Iteration 202, loss = 0.26572778\n",
      "Iteration 203, loss = 0.26413882\n",
      "Iteration 204, loss = 0.26256270\n",
      "Iteration 205, loss = 0.26099718\n",
      "Iteration 206, loss = 0.25944246\n",
      "Iteration 207, loss = 0.25789881\n",
      "Iteration 208, loss = 0.25636581\n",
      "Iteration 209, loss = 0.25484392\n",
      "Iteration 210, loss = 0.25333265\n",
      "Iteration 211, loss = 0.25183209\n",
      "Iteration 212, loss = 0.25034251\n",
      "Iteration 213, loss = 0.24886336\n",
      "Iteration 214, loss = 0.24739560\n",
      "Iteration 215, loss = 0.24593821\n",
      "Iteration 216, loss = 0.24449109\n",
      "Iteration 217, loss = 0.24305442\n",
      "Iteration 218, loss = 0.24162899\n",
      "Iteration 219, loss = 0.24021442\n",
      "Iteration 220, loss = 0.23881048\n",
      "Iteration 221, loss = 0.23741699\n",
      "Iteration 222, loss = 0.23603441\n",
      "Iteration 223, loss = 0.23466242\n",
      "Iteration 224, loss = 0.23330026\n",
      "Iteration 225, loss = 0.23194825\n",
      "Iteration 226, loss = 0.23060641\n",
      "Iteration 227, loss = 0.22927504\n",
      "Iteration 228, loss = 0.22795349\n",
      "Iteration 229, loss = 0.22664194\n",
      "Iteration 230, loss = 0.22534103\n",
      "Iteration 231, loss = 0.22404994\n",
      "Iteration 232, loss = 0.22276855\n",
      "Iteration 233, loss = 0.22149700\n",
      "Iteration 234, loss = 0.22023533\n",
      "Iteration 235, loss = 0.21898334\n",
      "Iteration 236, loss = 0.21774119\n",
      "Iteration 237, loss = 0.21650825\n",
      "Iteration 238, loss = 0.21528503\n",
      "Iteration 239, loss = 0.21407185\n",
      "Iteration 240, loss = 0.21286836\n",
      "Iteration 241, loss = 0.21167439\n",
      "Iteration 242, loss = 0.21048990\n",
      "Iteration 243, loss = 0.20931497\n",
      "Iteration 244, loss = 0.20814925\n",
      "Iteration 245, loss = 0.20699346\n",
      "Iteration 246, loss = 0.20584663\n",
      "Iteration 247, loss = 0.20470871\n",
      "Iteration 248, loss = 0.20357963\n",
      "Iteration 249, loss = 0.20245939\n",
      "Iteration 250, loss = 0.20134817\n",
      "Iteration 251, loss = 0.20024574\n",
      "Iteration 252, loss = 0.19915282\n",
      "Iteration 253, loss = 0.19806831\n",
      "Iteration 254, loss = 0.19699236\n",
      "Iteration 255, loss = 0.19592525\n",
      "Iteration 256, loss = 0.19486669\n",
      "Iteration 257, loss = 0.19381659\n",
      "Iteration 258, loss = 0.19277525\n",
      "Iteration 259, loss = 0.19174223\n",
      "Iteration 260, loss = 0.19071743\n",
      "Iteration 261, loss = 0.18970120\n",
      "Iteration 262, loss = 0.18869212\n",
      "Iteration 263, loss = 0.18769133\n",
      "Iteration 264, loss = 0.18669877\n",
      "Iteration 265, loss = 0.18571418\n",
      "Iteration 266, loss = 0.18473505\n",
      "Iteration 267, loss = 0.18376246\n",
      "Iteration 268, loss = 0.18279552\n",
      "Iteration 269, loss = 0.18183513\n",
      "Iteration 270, loss = 0.18088273\n",
      "Iteration 271, loss = 0.17993504\n",
      "Iteration 272, loss = 0.17899282\n",
      "Iteration 273, loss = 0.17806036\n",
      "Iteration 274, loss = 0.17713662\n",
      "Iteration 275, loss = 0.17621810\n",
      "Iteration 276, loss = 0.17530650\n",
      "Iteration 277, loss = 0.17440215\n",
      "Iteration 278, loss = 0.17350192\n",
      "Iteration 279, loss = 0.17259534\n",
      "Iteration 280, loss = 0.17168740\n",
      "Iteration 281, loss = 0.17077963\n",
      "Iteration 282, loss = 0.16987105\n",
      "Iteration 283, loss = 0.16895376\n",
      "Iteration 284, loss = 0.16803749\n",
      "Iteration 285, loss = 0.16711685\n",
      "Iteration 286, loss = 0.16619905\n",
      "Iteration 287, loss = 0.16528109\n",
      "Iteration 288, loss = 0.16436066\n",
      "Iteration 289, loss = 0.16343315\n",
      "Iteration 290, loss = 0.16251354\n",
      "Iteration 291, loss = 0.16160676\n",
      "Iteration 292, loss = 0.16069881\n",
      "Iteration 293, loss = 0.15977971\n",
      "Iteration 294, loss = 0.15884026\n",
      "Iteration 295, loss = 0.15790715\n",
      "Iteration 296, loss = 0.15696382\n",
      "Iteration 297, loss = 0.15604064\n",
      "Iteration 298, loss = 0.15516386\n",
      "Iteration 299, loss = 0.15432652\n",
      "Iteration 300, loss = 0.15357896\n",
      "Iteration 301, loss = 0.15286591\n",
      "Iteration 302, loss = 0.15216115\n",
      "Iteration 303, loss = 0.15146150\n",
      "Iteration 304, loss = 0.15076847\n",
      "Iteration 305, loss = 0.15007557\n",
      "Iteration 306, loss = 0.14938143\n",
      "Iteration 307, loss = 0.14868847\n",
      "Iteration 308, loss = 0.14799535\n",
      "Iteration 309, loss = 0.14730234\n",
      "Iteration 310, loss = 0.14660953\n",
      "Iteration 311, loss = 0.14591745\n",
      "Iteration 312, loss = 0.14522656\n",
      "Iteration 313, loss = 0.14453683\n",
      "Iteration 314, loss = 0.14384951\n",
      "Iteration 315, loss = 0.14316428\n",
      "Iteration 316, loss = 0.14248229\n",
      "Iteration 317, loss = 0.14180372\n",
      "Iteration 318, loss = 0.14112861\n",
      "Iteration 319, loss = 0.14045672\n",
      "Iteration 320, loss = 0.13978862\n",
      "Iteration 321, loss = 0.13912474\n",
      "Iteration 322, loss = 0.13846608\n",
      "Iteration 323, loss = 0.13781622\n",
      "Iteration 324, loss = 0.13717049\n",
      "Iteration 325, loss = 0.13652918\n",
      "Iteration 326, loss = 0.13589425\n",
      "Iteration 327, loss = 0.13526543\n",
      "Iteration 328, loss = 0.13464143\n",
      "Iteration 329, loss = 0.13402287\n",
      "Iteration 330, loss = 0.13341070\n",
      "Iteration 331, loss = 0.13280338\n",
      "Iteration 332, loss = 0.13220072\n",
      "Iteration 333, loss = 0.13160278\n",
      "Iteration 334, loss = 0.13101302\n",
      "Iteration 335, loss = 0.13042794\n",
      "Iteration 336, loss = 0.12984734\n",
      "Iteration 337, loss = 0.12927155\n",
      "Iteration 338, loss = 0.12870014\n",
      "Iteration 339, loss = 0.12813253\n",
      "Iteration 340, loss = 0.12756898\n",
      "Iteration 341, loss = 0.12700885\n",
      "Iteration 342, loss = 0.12645299\n",
      "Iteration 343, loss = 0.12590244\n",
      "Iteration 344, loss = 0.12535611\n",
      "Iteration 345, loss = 0.12481470\n",
      "Iteration 346, loss = 0.12427765\n",
      "Iteration 347, loss = 0.12374480\n",
      "Iteration 348, loss = 0.12321828\n",
      "Iteration 349, loss = 0.12269636\n",
      "Iteration 350, loss = 0.12217873\n",
      "Iteration 351, loss = 0.12166631\n",
      "Iteration 352, loss = 0.12115768\n",
      "Iteration 353, loss = 0.12065316\n",
      "Iteration 354, loss = 0.12015282\n",
      "Iteration 355, loss = 0.11965701\n",
      "Iteration 356, loss = 0.11916524\n",
      "Iteration 357, loss = 0.11867759\n",
      "Iteration 358, loss = 0.11819469\n",
      "Iteration 359, loss = 0.11771593\n",
      "Iteration 360, loss = 0.11724127\n",
      "Iteration 361, loss = 0.11677092\n",
      "Iteration 362, loss = 0.11630502\n",
      "Iteration 363, loss = 0.11584296\n",
      "Iteration 364, loss = 0.11538492\n",
      "Iteration 365, loss = 0.11493138\n",
      "Iteration 366, loss = 0.11448206\n",
      "Iteration 367, loss = 0.11403665\n",
      "Iteration 368, loss = 0.11359533\n",
      "Iteration 369, loss = 0.11315859\n",
      "Iteration 370, loss = 0.11272520\n",
      "Iteration 371, loss = 0.11229584\n",
      "Iteration 372, loss = 0.11187059\n",
      "Iteration 373, loss = 0.11144907\n",
      "Iteration 374, loss = 0.11103174\n",
      "Iteration 375, loss = 0.11061794\n",
      "Iteration 376, loss = 0.11020796\n",
      "Iteration 377, loss = 0.10980198\n",
      "Iteration 378, loss = 0.10939954\n",
      "Iteration 379, loss = 0.10900061\n",
      "Iteration 380, loss = 0.10860555\n",
      "Iteration 381, loss = 0.10821421\n",
      "Iteration 382, loss = 0.10782607\n",
      "Iteration 383, loss = 0.10744187\n",
      "Iteration 384, loss = 0.10706119\n",
      "Iteration 385, loss = 0.10668371\n",
      "Iteration 386, loss = 0.10630965\n",
      "Iteration 387, loss = 0.10593903\n",
      "Iteration 388, loss = 0.10557224\n",
      "Iteration 389, loss = 0.10520813\n",
      "Iteration 390, loss = 0.10484758\n",
      "Iteration 391, loss = 0.10449014\n",
      "Iteration 392, loss = 0.10413567\n",
      "Iteration 393, loss = 0.10378404\n",
      "Iteration 394, loss = 0.10343515\n",
      "Iteration 395, loss = 0.10308854\n",
      "Iteration 396, loss = 0.10274426\n",
      "Iteration 397, loss = 0.10240241\n",
      "Iteration 398, loss = 0.10206239\n",
      "Iteration 399, loss = 0.10172408\n",
      "Iteration 400, loss = 0.10138736\n",
      "Iteration 401, loss = 0.10105202\n",
      "Iteration 402, loss = 0.10071805\n",
      "Iteration 403, loss = 0.10038555\n",
      "Iteration 404, loss = 0.10005449\n",
      "Iteration 405, loss = 0.09972482\n",
      "Iteration 406, loss = 0.09939642\n",
      "Iteration 407, loss = 0.09906923\n",
      "Iteration 408, loss = 0.09874342\n",
      "Iteration 409, loss = 0.09841891\n",
      "Iteration 410, loss = 0.09809567\n",
      "Iteration 411, loss = 0.09777426\n",
      "Iteration 412, loss = 0.09745345\n",
      "Iteration 413, loss = 0.09713452\n",
      "Iteration 414, loss = 0.09681691\n",
      "Iteration 415, loss = 0.09650081\n",
      "Iteration 416, loss = 0.09618623\n",
      "Iteration 417, loss = 0.09587321\n",
      "Iteration 418, loss = 0.09556160\n",
      "Iteration 419, loss = 0.09525179\n",
      "Iteration 420, loss = 0.09494343\n",
      "Iteration 421, loss = 0.09463667\n",
      "Iteration 422, loss = 0.09433152\n",
      "Iteration 423, loss = 0.09402786\n",
      "Iteration 424, loss = 0.09372590\n",
      "Iteration 425, loss = 0.09342514\n",
      "Iteration 426, loss = 0.09312549\n",
      "Iteration 427, loss = 0.09282698\n",
      "Iteration 428, loss = 0.09252978\n",
      "Iteration 429, loss = 0.09223373\n",
      "Iteration 430, loss = 0.09193887\n",
      "Iteration 431, loss = 0.09164523\n",
      "Iteration 432, loss = 0.09135303\n",
      "Iteration 433, loss = 0.09106226\n",
      "Iteration 434, loss = 0.09077254\n",
      "Iteration 435, loss = 0.09048367\n",
      "Iteration 436, loss = 0.09019577\n",
      "Iteration 437, loss = 0.08990874\n",
      "Iteration 438, loss = 0.08962273\n",
      "Iteration 439, loss = 0.08933760\n",
      "Iteration 440, loss = 0.08905382\n",
      "Iteration 441, loss = 0.08877121\n",
      "Iteration 442, loss = 0.08848945\n",
      "Iteration 443, loss = 0.08820864\n",
      "Iteration 444, loss = 0.08792922\n",
      "Iteration 445, loss = 0.08765098\n",
      "Iteration 446, loss = 0.08737397\n",
      "Iteration 447, loss = 0.08709825\n",
      "Iteration 448, loss = 0.08682390\n",
      "Iteration 449, loss = 0.08655094\n",
      "Iteration 450, loss = 0.08627943\n",
      "Iteration 451, loss = 0.08600936\n",
      "Iteration 452, loss = 0.08574085\n",
      "Iteration 453, loss = 0.08547391\n",
      "Iteration 454, loss = 0.08520872\n",
      "Iteration 455, loss = 0.08494516\n",
      "Iteration 456, loss = 0.08468328\n",
      "Iteration 457, loss = 0.08442316\n",
      "Iteration 458, loss = 0.08416474\n",
      "Iteration 459, loss = 0.08390821\n",
      "Iteration 460, loss = 0.08365350\n",
      "Iteration 461, loss = 0.08340068\n",
      "Iteration 462, loss = 0.08314977\n",
      "Iteration 463, loss = 0.08290083\n",
      "Iteration 464, loss = 0.08265386\n",
      "Iteration 465, loss = 0.08240888\n",
      "Iteration 466, loss = 0.08216592\n",
      "Iteration 467, loss = 0.08192511\n",
      "Iteration 468, loss = 0.08168644\n",
      "Iteration 469, loss = 0.08144982\n",
      "Iteration 470, loss = 0.08121528\n",
      "Iteration 471, loss = 0.08098277\n",
      "Iteration 472, loss = 0.08075231\n",
      "Iteration 473, loss = 0.08052396\n",
      "Iteration 474, loss = 0.08029767\n",
      "Iteration 475, loss = 0.08007350\n",
      "Iteration 476, loss = 0.07985160\n",
      "Iteration 477, loss = 0.07963183\n",
      "Iteration 478, loss = 0.07941414\n",
      "Iteration 479, loss = 0.07919855\n",
      "Iteration 480, loss = 0.07898503\n",
      "Iteration 481, loss = 0.07877358\n",
      "Iteration 482, loss = 0.07856419\n",
      "Iteration 483, loss = 0.07835695\n",
      "Iteration 484, loss = 0.07815178\n",
      "Iteration 485, loss = 0.07794866\n",
      "Iteration 486, loss = 0.07774758\n",
      "Iteration 487, loss = 0.07754854\n",
      "Iteration 488, loss = 0.07735162\n",
      "Iteration 489, loss = 0.07715671\n",
      "Iteration 490, loss = 0.07696378\n",
      "Iteration 491, loss = 0.07677290\n",
      "Iteration 492, loss = 0.07658402\n",
      "Iteration 493, loss = 0.07639710\n",
      "Iteration 494, loss = 0.07621210\n",
      "Iteration 495, loss = 0.07602908\n",
      "Iteration 496, loss = 0.07584800\n",
      "Iteration 497, loss = 0.07566882\n",
      "Iteration 498, loss = 0.07549151\n",
      "Iteration 499, loss = 0.07531608\n",
      "Iteration 500, loss = 0.07514249\n",
      "Iteration 501, loss = 0.07497079\n",
      "Iteration 502, loss = 0.07480096\n",
      "Iteration 503, loss = 0.07463292\n",
      "Iteration 504, loss = 0.07446665\n",
      "Iteration 505, loss = 0.07430217\n",
      "Iteration 506, loss = 0.07413946\n",
      "Iteration 507, loss = 0.07397853\n",
      "Iteration 508, loss = 0.07381937\n",
      "Iteration 509, loss = 0.07366199\n",
      "Iteration 510, loss = 0.07350640\n",
      "Iteration 511, loss = 0.07335243\n",
      "Iteration 512, loss = 0.07320007\n",
      "Iteration 513, loss = 0.07304920\n",
      "Iteration 514, loss = 0.07290014\n",
      "Iteration 515, loss = 0.07275269\n",
      "Iteration 516, loss = 0.07260690\n",
      "Iteration 517, loss = 0.07246269\n",
      "Iteration 518, loss = 0.07231998\n",
      "Iteration 519, loss = 0.07217885\n",
      "Iteration 520, loss = 0.07203918\n",
      "Iteration 521, loss = 0.07190103\n",
      "Iteration 522, loss = 0.07176442\n",
      "Iteration 523, loss = 0.07162920\n",
      "Iteration 524, loss = 0.07149545\n",
      "Iteration 525, loss = 0.07136316\n",
      "Iteration 526, loss = 0.07123224\n",
      "Iteration 527, loss = 0.07110273\n",
      "Iteration 528, loss = 0.07097458\n",
      "Iteration 529, loss = 0.07084777\n",
      "Iteration 530, loss = 0.07072230\n",
      "Iteration 531, loss = 0.07059819\n",
      "Iteration 532, loss = 0.07047539\n",
      "Iteration 533, loss = 0.07035392\n",
      "Iteration 534, loss = 0.07023374\n",
      "Iteration 535, loss = 0.07011486\n",
      "Iteration 536, loss = 0.06999723\n",
      "Iteration 537, loss = 0.06988080\n",
      "Iteration 538, loss = 0.06976558\n",
      "Iteration 539, loss = 0.06965155\n",
      "Iteration 540, loss = 0.06953870\n",
      "Iteration 541, loss = 0.06942702\n",
      "Iteration 542, loss = 0.06931653\n",
      "Iteration 543, loss = 0.06920715\n",
      "Iteration 544, loss = 0.06909888\n",
      "Iteration 545, loss = 0.06899181\n",
      "Iteration 546, loss = 0.06888571\n",
      "Iteration 547, loss = 0.06878066\n",
      "Iteration 548, loss = 0.06867666\n",
      "Iteration 549, loss = 0.06857377\n",
      "Iteration 550, loss = 0.06847185\n",
      "Iteration 551, loss = 0.06837099\n",
      "Iteration 552, loss = 0.06827112\n",
      "Iteration 553, loss = 0.06817227\n",
      "Iteration 554, loss = 0.06807390\n",
      "Iteration 555, loss = 0.06797531\n",
      "Iteration 556, loss = 0.06787722\n",
      "Iteration 557, loss = 0.06777968\n",
      "Iteration 558, loss = 0.06768273\n",
      "Iteration 559, loss = 0.06758640\n",
      "Iteration 560, loss = 0.06749071\n",
      "Iteration 561, loss = 0.06739579\n",
      "Iteration 562, loss = 0.06730154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48086253\n",
      "Iteration 2, loss = 1.45168950\n",
      "Iteration 3, loss = 1.42335152\n",
      "Iteration 4, loss = 1.39594052\n",
      "Iteration 5, loss = 1.36945525\n",
      "Iteration 6, loss = 1.34380149\n",
      "Iteration 7, loss = 1.31885283\n",
      "Iteration 8, loss = 1.29466654\n",
      "Iteration 9, loss = 1.27125206\n",
      "Iteration 10, loss = 1.24871342\n",
      "Iteration 11, loss = 1.22693336\n",
      "Iteration 12, loss = 1.20603022\n",
      "Iteration 13, loss = 1.18619227\n",
      "Iteration 14, loss = 1.16746238\n",
      "Iteration 15, loss = 1.14948880\n",
      "Iteration 16, loss = 1.13216282\n",
      "Iteration 17, loss = 1.11543218\n",
      "Iteration 18, loss = 1.09924615\n",
      "Iteration 19, loss = 1.08352541\n",
      "Iteration 20, loss = 1.06830879\n",
      "Iteration 21, loss = 1.05369307\n",
      "Iteration 22, loss = 1.03962029\n",
      "Iteration 23, loss = 1.02594869\n",
      "Iteration 24, loss = 1.01268808\n",
      "Iteration 25, loss = 0.99983181\n",
      "Iteration 26, loss = 0.98732277\n",
      "Iteration 27, loss = 0.97507210\n",
      "Iteration 28, loss = 0.96309377\n",
      "Iteration 29, loss = 0.95134013\n",
      "Iteration 30, loss = 0.93987363\n",
      "Iteration 31, loss = 0.92865771\n",
      "Iteration 32, loss = 0.91764115\n",
      "Iteration 33, loss = 0.90685870\n",
      "Iteration 34, loss = 0.89634869\n",
      "Iteration 35, loss = 0.88610453\n",
      "Iteration 36, loss = 0.87609683\n",
      "Iteration 37, loss = 0.86637053\n",
      "Iteration 38, loss = 0.85681569\n",
      "Iteration 39, loss = 0.84750451\n",
      "Iteration 40, loss = 0.83843194\n",
      "Iteration 41, loss = 0.82958076\n",
      "Iteration 42, loss = 0.82086480\n",
      "Iteration 43, loss = 0.81227508\n",
      "Iteration 44, loss = 0.80385458\n",
      "Iteration 45, loss = 0.79564564\n",
      "Iteration 46, loss = 0.78760906\n",
      "Iteration 47, loss = 0.77973089\n",
      "Iteration 48, loss = 0.77197040\n",
      "Iteration 49, loss = 0.76432860\n",
      "Iteration 50, loss = 0.75681042\n",
      "Iteration 51, loss = 0.74942025\n",
      "Iteration 52, loss = 0.74214876\n",
      "Iteration 53, loss = 0.73502238\n",
      "Iteration 54, loss = 0.72801973\n",
      "Iteration 55, loss = 0.72115165\n",
      "Iteration 56, loss = 0.71440587\n",
      "Iteration 57, loss = 0.70779676\n",
      "Iteration 58, loss = 0.70135803\n",
      "Iteration 59, loss = 0.69507749\n",
      "Iteration 60, loss = 0.68898399\n",
      "Iteration 61, loss = 0.68304475\n",
      "Iteration 62, loss = 0.67723474\n",
      "Iteration 63, loss = 0.67156703\n",
      "Iteration 64, loss = 0.66601069\n",
      "Iteration 65, loss = 0.66051157\n",
      "Iteration 66, loss = 0.65508824\n",
      "Iteration 67, loss = 0.64974372\n",
      "Iteration 68, loss = 0.64448911\n",
      "Iteration 69, loss = 0.63930870\n",
      "Iteration 70, loss = 0.63417428\n",
      "Iteration 71, loss = 0.62908935\n",
      "Iteration 72, loss = 0.62406236\n",
      "Iteration 73, loss = 0.61908810\n",
      "Iteration 74, loss = 0.61416724\n",
      "Iteration 75, loss = 0.60929781\n",
      "Iteration 76, loss = 0.60447128\n",
      "Iteration 77, loss = 0.59969171\n",
      "Iteration 78, loss = 0.59497561\n",
      "Iteration 79, loss = 0.59027823\n",
      "Iteration 80, loss = 0.58562180\n",
      "Iteration 81, loss = 0.58097622\n",
      "Iteration 82, loss = 0.57638947\n",
      "Iteration 83, loss = 0.57181431\n",
      "Iteration 84, loss = 0.56719533\n",
      "Iteration 85, loss = 0.56264413\n",
      "Iteration 86, loss = 0.55793168\n",
      "Iteration 87, loss = 0.55318565\n",
      "Iteration 88, loss = 0.54862862\n",
      "Iteration 89, loss = 0.54410857\n",
      "Iteration 90, loss = 0.53967411\n",
      "Iteration 91, loss = 0.53531774\n",
      "Iteration 92, loss = 0.53111761\n",
      "Iteration 93, loss = 0.52708184\n",
      "Iteration 94, loss = 0.52317839\n",
      "Iteration 95, loss = 0.51935989\n",
      "Iteration 96, loss = 0.51559160\n",
      "Iteration 97, loss = 0.51187348\n",
      "Iteration 98, loss = 0.50816224\n",
      "Iteration 99, loss = 0.50448526\n",
      "Iteration 100, loss = 0.50089899\n",
      "Iteration 101, loss = 0.49741384\n",
      "Iteration 102, loss = 0.49397697\n",
      "Iteration 103, loss = 0.49055805\n",
      "Iteration 104, loss = 0.48714666\n",
      "Iteration 105, loss = 0.48374958\n",
      "Iteration 106, loss = 0.48037867\n",
      "Iteration 107, loss = 0.47704252\n",
      "Iteration 108, loss = 0.47374047\n",
      "Iteration 109, loss = 0.47046884\n",
      "Iteration 110, loss = 0.46723358\n",
      "Iteration 111, loss = 0.46402605\n",
      "Iteration 112, loss = 0.46083318\n",
      "Iteration 113, loss = 0.45766329\n",
      "Iteration 114, loss = 0.45451713\n",
      "Iteration 115, loss = 0.45138782\n",
      "Iteration 116, loss = 0.44827542\n",
      "Iteration 117, loss = 0.44517799\n",
      "Iteration 118, loss = 0.44209521\n",
      "Iteration 119, loss = 0.43901024\n",
      "Iteration 120, loss = 0.43589526\n",
      "Iteration 121, loss = 0.43274670\n",
      "Iteration 122, loss = 0.42955296\n",
      "Iteration 123, loss = 0.42632568\n",
      "Iteration 124, loss = 0.42312450\n",
      "Iteration 125, loss = 0.41991482\n",
      "Iteration 126, loss = 0.41667956\n",
      "Iteration 127, loss = 0.41343090\n",
      "Iteration 128, loss = 0.41024515\n",
      "Iteration 129, loss = 0.40713552\n",
      "Iteration 130, loss = 0.40414847\n",
      "Iteration 131, loss = 0.40133330\n",
      "Iteration 132, loss = 0.39870607\n",
      "Iteration 133, loss = 0.39611976\n",
      "Iteration 134, loss = 0.39357468\n",
      "Iteration 135, loss = 0.39109345\n",
      "Iteration 136, loss = 0.38865539\n",
      "Iteration 137, loss = 0.38624573\n",
      "Iteration 138, loss = 0.38386100\n",
      "Iteration 139, loss = 0.38146055\n",
      "Iteration 140, loss = 0.37910260\n",
      "Iteration 141, loss = 0.37674976\n",
      "Iteration 142, loss = 0.37438697\n",
      "Iteration 143, loss = 0.37202516\n",
      "Iteration 144, loss = 0.36969757\n",
      "Iteration 145, loss = 0.36735154\n",
      "Iteration 146, loss = 0.36502108\n",
      "Iteration 147, loss = 0.36272673\n",
      "Iteration 148, loss = 0.36043582\n",
      "Iteration 149, loss = 0.35814645\n",
      "Iteration 150, loss = 0.35588379\n",
      "Iteration 151, loss = 0.35365012\n",
      "Iteration 152, loss = 0.35143121\n",
      "Iteration 153, loss = 0.34923381\n",
      "Iteration 154, loss = 0.34707912\n",
      "Iteration 155, loss = 0.34494161\n",
      "Iteration 156, loss = 0.34281331\n",
      "Iteration 157, loss = 0.34071291\n",
      "Iteration 158, loss = 0.33863853\n",
      "Iteration 159, loss = 0.33657221\n",
      "Iteration 160, loss = 0.33451865\n",
      "Iteration 161, loss = 0.33248806\n",
      "Iteration 162, loss = 0.33046691\n",
      "Iteration 163, loss = 0.32845293\n",
      "Iteration 164, loss = 0.32646001\n",
      "Iteration 165, loss = 0.32448281\n",
      "Iteration 166, loss = 0.32251629\n",
      "Iteration 167, loss = 0.32055826\n",
      "Iteration 168, loss = 0.31861293\n",
      "Iteration 169, loss = 0.31668675\n",
      "Iteration 170, loss = 0.31477148\n",
      "Iteration 171, loss = 0.31286614\n",
      "Iteration 172, loss = 0.31097162\n",
      "Iteration 173, loss = 0.30909501\n",
      "Iteration 174, loss = 0.30723103\n",
      "Iteration 175, loss = 0.30537756\n",
      "Iteration 176, loss = 0.30353456\n",
      "Iteration 177, loss = 0.30170341\n",
      "Iteration 178, loss = 0.29988482\n",
      "Iteration 179, loss = 0.29807663\n",
      "Iteration 180, loss = 0.29627873\n",
      "Iteration 181, loss = 0.29449256\n",
      "Iteration 182, loss = 0.29271779\n",
      "Iteration 183, loss = 0.29095339\n",
      "Iteration 184, loss = 0.28920226\n",
      "Iteration 185, loss = 0.28746039\n",
      "Iteration 186, loss = 0.28572575\n",
      "Iteration 187, loss = 0.28400079\n",
      "Iteration 188, loss = 0.28228449\n",
      "Iteration 189, loss = 0.28057830\n",
      "Iteration 190, loss = 0.27888276\n",
      "Iteration 191, loss = 0.27719931\n",
      "Iteration 192, loss = 0.27552888\n",
      "Iteration 193, loss = 0.27387112\n",
      "Iteration 194, loss = 0.27222866\n",
      "Iteration 195, loss = 0.27060014\n",
      "Iteration 196, loss = 0.26898422\n",
      "Iteration 197, loss = 0.26737993\n",
      "Iteration 198, loss = 0.26578495\n",
      "Iteration 199, loss = 0.26419993\n",
      "Iteration 200, loss = 0.26262269\n",
      "Iteration 201, loss = 0.26105472\n",
      "Iteration 202, loss = 0.25949947\n",
      "Iteration 203, loss = 0.25795577\n",
      "Iteration 204, loss = 0.25642350\n",
      "Iteration 205, loss = 0.25490357\n",
      "Iteration 206, loss = 0.25339619\n",
      "Iteration 207, loss = 0.25189911\n",
      "Iteration 208, loss = 0.25041257\n",
      "Iteration 209, loss = 0.24893641\n",
      "Iteration 210, loss = 0.24747039\n",
      "Iteration 211, loss = 0.24601473\n",
      "Iteration 212, loss = 0.24456919\n",
      "Iteration 213, loss = 0.24313370\n",
      "Iteration 214, loss = 0.24170898\n",
      "Iteration 215, loss = 0.24029487\n",
      "Iteration 216, loss = 0.23889105\n",
      "Iteration 217, loss = 0.23750221\n",
      "Iteration 218, loss = 0.23612344\n",
      "Iteration 219, loss = 0.23475372\n",
      "Iteration 220, loss = 0.23339274\n",
      "Iteration 221, loss = 0.23204231\n",
      "Iteration 222, loss = 0.23070440\n",
      "Iteration 223, loss = 0.22937617\n",
      "Iteration 224, loss = 0.22805811\n",
      "Iteration 225, loss = 0.22675002\n",
      "Iteration 226, loss = 0.22545169\n",
      "Iteration 227, loss = 0.22416351\n",
      "Iteration 228, loss = 0.22288511\n",
      "Iteration 229, loss = 0.22161632\n",
      "Iteration 230, loss = 0.22035744\n",
      "Iteration 231, loss = 0.21910959\n",
      "Iteration 232, loss = 0.21787093\n",
      "Iteration 233, loss = 0.21664131\n",
      "Iteration 234, loss = 0.21542121\n",
      "Iteration 235, loss = 0.21421073\n",
      "Iteration 236, loss = 0.21301022\n",
      "Iteration 237, loss = 0.21181887\n",
      "Iteration 238, loss = 0.21063718\n",
      "Iteration 239, loss = 0.20946540\n",
      "Iteration 240, loss = 0.20830294\n",
      "Iteration 241, loss = 0.20714969\n",
      "Iteration 242, loss = 0.20600542\n",
      "Iteration 243, loss = 0.20487025\n",
      "Iteration 244, loss = 0.20374393\n",
      "Iteration 245, loss = 0.20262648\n",
      "Iteration 246, loss = 0.20151811\n",
      "Iteration 247, loss = 0.20041847\n",
      "Iteration 248, loss = 0.19932757\n",
      "Iteration 249, loss = 0.19824547\n",
      "Iteration 250, loss = 0.19717320\n",
      "Iteration 251, loss = 0.19610914\n",
      "Iteration 252, loss = 0.19505335\n",
      "Iteration 253, loss = 0.19400620\n",
      "Iteration 254, loss = 0.19296775\n",
      "Iteration 255, loss = 0.19193762\n",
      "Iteration 256, loss = 0.19091573\n",
      "Iteration 257, loss = 0.18990212\n",
      "Iteration 258, loss = 0.18889666\n",
      "Iteration 259, loss = 0.18789950\n",
      "Iteration 260, loss = 0.18691047\n",
      "Iteration 261, loss = 0.18592890\n",
      "Iteration 262, loss = 0.18495343\n",
      "Iteration 263, loss = 0.18398560\n",
      "Iteration 264, loss = 0.18302375\n",
      "Iteration 265, loss = 0.18206806\n",
      "Iteration 266, loss = 0.18111976\n",
      "Iteration 267, loss = 0.18017817\n",
      "Iteration 268, loss = 0.17924377\n",
      "Iteration 269, loss = 0.17831607\n",
      "Iteration 270, loss = 0.17739361\n",
      "Iteration 271, loss = 0.17647775\n",
      "Iteration 272, loss = 0.17556733\n",
      "Iteration 273, loss = 0.17466325\n",
      "Iteration 274, loss = 0.17376001\n",
      "Iteration 275, loss = 0.17285899\n",
      "Iteration 276, loss = 0.17195469\n",
      "Iteration 277, loss = 0.17104927\n",
      "Iteration 278, loss = 0.17014500\n",
      "Iteration 279, loss = 0.16923140\n",
      "Iteration 280, loss = 0.16831053\n",
      "Iteration 281, loss = 0.16738066\n",
      "Iteration 282, loss = 0.16644148\n",
      "Iteration 283, loss = 0.16550423\n",
      "Iteration 284, loss = 0.16456649\n",
      "Iteration 285, loss = 0.16361474\n",
      "Iteration 286, loss = 0.16265479\n",
      "Iteration 287, loss = 0.16169349\n",
      "Iteration 288, loss = 0.16070403\n",
      "Iteration 289, loss = 0.15970475\n",
      "Iteration 290, loss = 0.15870865\n",
      "Iteration 291, loss = 0.15774015\n",
      "Iteration 292, loss = 0.15682294\n",
      "Iteration 293, loss = 0.15597067\n",
      "Iteration 294, loss = 0.15517701\n",
      "Iteration 295, loss = 0.15446679\n",
      "Iteration 296, loss = 0.15377227\n",
      "Iteration 297, loss = 0.15308727\n",
      "Iteration 298, loss = 0.15240657\n",
      "Iteration 299, loss = 0.15172647\n",
      "Iteration 300, loss = 0.15104164\n",
      "Iteration 301, loss = 0.15035524\n",
      "Iteration 302, loss = 0.14966945\n",
      "Iteration 303, loss = 0.14898270\n",
      "Iteration 304, loss = 0.14829465\n",
      "Iteration 305, loss = 0.14760643\n",
      "Iteration 306, loss = 0.14691860\n",
      "Iteration 307, loss = 0.14623142\n",
      "Iteration 308, loss = 0.14554504\n",
      "Iteration 309, loss = 0.14486031\n",
      "Iteration 310, loss = 0.14417795\n",
      "Iteration 311, loss = 0.14349711\n",
      "Iteration 312, loss = 0.14281878\n",
      "Iteration 313, loss = 0.14214313\n",
      "Iteration 314, loss = 0.14146975\n",
      "Iteration 315, loss = 0.14079932\n",
      "Iteration 316, loss = 0.14013321\n",
      "Iteration 317, loss = 0.13947217\n",
      "Iteration 318, loss = 0.13881665\n",
      "Iteration 319, loss = 0.13816759\n",
      "Iteration 320, loss = 0.13752340\n",
      "Iteration 321, loss = 0.13688490\n",
      "Iteration 322, loss = 0.13625263\n",
      "Iteration 323, loss = 0.13562456\n",
      "Iteration 324, loss = 0.13500074\n",
      "Iteration 325, loss = 0.13438306\n",
      "Iteration 326, loss = 0.13377616\n",
      "Iteration 327, loss = 0.13317691\n",
      "Iteration 328, loss = 0.13258063\n",
      "Iteration 329, loss = 0.13198544\n",
      "Iteration 330, loss = 0.13139185\n",
      "Iteration 331, loss = 0.13080001\n",
      "Iteration 332, loss = 0.13021024\n",
      "Iteration 333, loss = 0.12962285\n",
      "Iteration 334, loss = 0.12903923\n",
      "Iteration 335, loss = 0.12846190\n",
      "Iteration 336, loss = 0.12789199\n",
      "Iteration 337, loss = 0.12732756\n",
      "Iteration 338, loss = 0.12676741\n",
      "Iteration 339, loss = 0.12621162\n",
      "Iteration 340, loss = 0.12565996\n",
      "Iteration 341, loss = 0.12511197\n",
      "Iteration 342, loss = 0.12456748\n",
      "Iteration 343, loss = 0.12402638\n",
      "Iteration 344, loss = 0.12348926\n",
      "Iteration 345, loss = 0.12295616\n",
      "Iteration 346, loss = 0.12242660\n",
      "Iteration 347, loss = 0.12190085\n",
      "Iteration 348, loss = 0.12138008\n",
      "Iteration 349, loss = 0.12086352\n",
      "Iteration 350, loss = 0.12035083\n",
      "Iteration 351, loss = 0.11984212\n",
      "Iteration 352, loss = 0.11933742\n",
      "Iteration 353, loss = 0.11883663\n",
      "Iteration 354, loss = 0.11834043\n",
      "Iteration 355, loss = 0.11784876\n",
      "Iteration 356, loss = 0.11736074\n",
      "Iteration 357, loss = 0.11687635\n",
      "Iteration 358, loss = 0.11639556\n",
      "Iteration 359, loss = 0.11591845\n",
      "Iteration 360, loss = 0.11544499\n",
      "Iteration 361, loss = 0.11497547\n",
      "Iteration 362, loss = 0.11451049\n",
      "Iteration 363, loss = 0.11404932\n",
      "Iteration 364, loss = 0.11359183\n",
      "Iteration 365, loss = 0.11313797\n",
      "Iteration 366, loss = 0.11268797\n",
      "Iteration 367, loss = 0.11224155\n",
      "Iteration 368, loss = 0.11179867\n",
      "Iteration 369, loss = 0.11135954\n",
      "Iteration 370, loss = 0.11092402\n",
      "Iteration 371, loss = 0.11049185\n",
      "Iteration 372, loss = 0.11006334\n",
      "Iteration 373, loss = 0.10963836\n",
      "Iteration 374, loss = 0.10921699\n",
      "Iteration 375, loss = 0.10879942\n",
      "Iteration 376, loss = 0.10838522\n",
      "Iteration 377, loss = 0.10797443\n",
      "Iteration 378, loss = 0.10756691\n",
      "Iteration 379, loss = 0.10716270\n",
      "Iteration 380, loss = 0.10676192\n",
      "Iteration 381, loss = 0.10636451\n",
      "Iteration 382, loss = 0.10597047\n",
      "Iteration 383, loss = 0.10557972\n",
      "Iteration 384, loss = 0.10519215\n",
      "Iteration 385, loss = 0.10480779\n",
      "Iteration 386, loss = 0.10442659\n",
      "Iteration 387, loss = 0.10404834\n",
      "Iteration 388, loss = 0.10367334\n",
      "Iteration 389, loss = 0.10330144\n",
      "Iteration 390, loss = 0.10293237\n",
      "Iteration 391, loss = 0.10256647\n",
      "Iteration 392, loss = 0.10220329\n",
      "Iteration 393, loss = 0.10184272\n",
      "Iteration 394, loss = 0.10148472\n",
      "Iteration 395, loss = 0.10112899\n",
      "Iteration 396, loss = 0.10077547\n",
      "Iteration 397, loss = 0.10042381\n",
      "Iteration 398, loss = 0.10007407\n",
      "Iteration 399, loss = 0.09972587\n",
      "Iteration 400, loss = 0.09937909\n",
      "Iteration 401, loss = 0.09903365\n",
      "Iteration 402, loss = 0.09868947\n",
      "Iteration 403, loss = 0.09834663\n",
      "Iteration 404, loss = 0.09800507\n",
      "Iteration 405, loss = 0.09766477\n",
      "Iteration 406, loss = 0.09732551\n",
      "Iteration 407, loss = 0.09698730\n",
      "Iteration 408, loss = 0.09665009\n",
      "Iteration 409, loss = 0.09631406\n",
      "Iteration 410, loss = 0.09597898\n",
      "Iteration 411, loss = 0.09564519\n",
      "Iteration 412, loss = 0.09531255\n",
      "Iteration 413, loss = 0.09498111\n",
      "Iteration 414, loss = 0.09465085\n",
      "Iteration 415, loss = 0.09432183\n",
      "Iteration 416, loss = 0.09399408\n",
      "Iteration 417, loss = 0.09366761\n",
      "Iteration 418, loss = 0.09334246\n",
      "Iteration 419, loss = 0.09301884\n",
      "Iteration 420, loss = 0.09269644\n",
      "Iteration 421, loss = 0.09237560\n",
      "Iteration 422, loss = 0.09205609\n",
      "Iteration 423, loss = 0.09173768\n",
      "Iteration 424, loss = 0.09142089\n",
      "Iteration 425, loss = 0.09110539\n",
      "Iteration 426, loss = 0.09079093\n",
      "Iteration 427, loss = 0.09047758\n",
      "Iteration 428, loss = 0.09016538\n",
      "Iteration 429, loss = 0.08985436\n",
      "Iteration 430, loss = 0.08954425\n",
      "Iteration 431, loss = 0.08923531\n",
      "Iteration 432, loss = 0.08892728\n",
      "Iteration 433, loss = 0.08862007\n",
      "Iteration 434, loss = 0.08831364\n",
      "Iteration 435, loss = 0.08800798\n",
      "Iteration 436, loss = 0.08770305\n",
      "Iteration 437, loss = 0.08739885\n",
      "Iteration 438, loss = 0.08709560\n",
      "Iteration 439, loss = 0.08679293\n",
      "Iteration 440, loss = 0.08649106\n",
      "Iteration 441, loss = 0.08619004\n",
      "Iteration 442, loss = 0.08588981\n",
      "Iteration 443, loss = 0.08559046\n",
      "Iteration 444, loss = 0.08529183\n",
      "Iteration 445, loss = 0.08499420\n",
      "Iteration 446, loss = 0.08469758\n",
      "Iteration 447, loss = 0.08440204\n",
      "Iteration 448, loss = 0.08410731\n",
      "Iteration 449, loss = 0.08381383\n",
      "Iteration 450, loss = 0.08352140\n",
      "Iteration 451, loss = 0.08323005\n",
      "Iteration 452, loss = 0.08294000\n",
      "Iteration 453, loss = 0.08265125\n",
      "Iteration 454, loss = 0.08236384\n",
      "Iteration 455, loss = 0.08207781\n",
      "Iteration 456, loss = 0.08179330\n",
      "Iteration 457, loss = 0.08151027\n",
      "Iteration 458, loss = 0.08122874\n",
      "Iteration 459, loss = 0.08094871\n",
      "Iteration 460, loss = 0.08067022\n",
      "Iteration 461, loss = 0.08039333\n",
      "Iteration 462, loss = 0.08011812\n",
      "Iteration 463, loss = 0.07984460\n",
      "Iteration 464, loss = 0.07957274\n",
      "Iteration 465, loss = 0.07930259\n",
      "Iteration 466, loss = 0.07903418\n",
      "Iteration 467, loss = 0.07876769\n",
      "Iteration 468, loss = 0.07850286\n",
      "Iteration 469, loss = 0.07823989\n",
      "Iteration 470, loss = 0.07797876\n",
      "Iteration 471, loss = 0.07771945\n",
      "Iteration 472, loss = 0.07746199\n",
      "Iteration 473, loss = 0.07720646\n",
      "Iteration 474, loss = 0.07695276\n",
      "Iteration 475, loss = 0.07670108\n",
      "Iteration 476, loss = 0.07645124\n",
      "Iteration 477, loss = 0.07620329\n",
      "Iteration 478, loss = 0.07595741\n",
      "Iteration 479, loss = 0.07571329\n",
      "Iteration 480, loss = 0.07547126\n",
      "Iteration 481, loss = 0.07523123\n",
      "Iteration 482, loss = 0.07499303\n",
      "Iteration 483, loss = 0.07475689\n",
      "Iteration 484, loss = 0.07452271\n",
      "Iteration 485, loss = 0.07429057\n",
      "Iteration 486, loss = 0.07406075\n",
      "Iteration 487, loss = 0.07383292\n",
      "Iteration 488, loss = 0.07360705\n",
      "Iteration 489, loss = 0.07338305\n",
      "Iteration 490, loss = 0.07316091\n",
      "Iteration 491, loss = 0.07294085\n",
      "Iteration 492, loss = 0.07272268\n",
      "Iteration 493, loss = 0.07250639\n",
      "Iteration 494, loss = 0.07229193\n",
      "Iteration 495, loss = 0.07207944\n",
      "Iteration 496, loss = 0.07186883\n",
      "Iteration 497, loss = 0.07166016\n",
      "Iteration 498, loss = 0.07145337\n",
      "Iteration 499, loss = 0.07124840\n",
      "Iteration 500, loss = 0.07104532\n",
      "Iteration 501, loss = 0.07084405\n",
      "Iteration 502, loss = 0.07064460\n",
      "Iteration 503, loss = 0.07044700\n",
      "Iteration 504, loss = 0.07025120\n",
      "Iteration 505, loss = 0.07005717\n",
      "Iteration 506, loss = 0.06986488\n",
      "Iteration 507, loss = 0.06967431\n",
      "Iteration 508, loss = 0.06948549\n",
      "Iteration 509, loss = 0.06929834\n",
      "Iteration 510, loss = 0.06911289\n",
      "Iteration 511, loss = 0.06892912\n",
      "Iteration 512, loss = 0.06874702\n",
      "Iteration 513, loss = 0.06856657\n",
      "Iteration 514, loss = 0.06838779\n",
      "Iteration 515, loss = 0.06821070\n",
      "Iteration 516, loss = 0.06803518\n",
      "Iteration 517, loss = 0.06786117\n",
      "Iteration 518, loss = 0.06768872\n",
      "Iteration 519, loss = 0.06751783\n",
      "Iteration 520, loss = 0.06734848\n",
      "Iteration 521, loss = 0.06718064\n",
      "Iteration 522, loss = 0.06701432\n",
      "Iteration 523, loss = 0.06684946\n",
      "Iteration 524, loss = 0.06668607\n",
      "Iteration 525, loss = 0.06652413\n",
      "Iteration 526, loss = 0.06636361\n",
      "Iteration 527, loss = 0.06620451\n",
      "Iteration 528, loss = 0.06604681\n",
      "Iteration 529, loss = 0.06589050\n",
      "Iteration 530, loss = 0.06573554\n",
      "Iteration 531, loss = 0.06558196\n",
      "Iteration 532, loss = 0.06542969\n",
      "Iteration 533, loss = 0.06527877\n",
      "Iteration 534, loss = 0.06512924\n",
      "Iteration 535, loss = 0.06498101\n",
      "Iteration 536, loss = 0.06483407\n",
      "Iteration 537, loss = 0.06468839\n",
      "Iteration 538, loss = 0.06454400\n",
      "Iteration 539, loss = 0.06440086\n",
      "Iteration 540, loss = 0.06425894\n",
      "Iteration 541, loss = 0.06411824\n",
      "Iteration 542, loss = 0.06397872\n",
      "Iteration 543, loss = 0.06384039\n",
      "Iteration 544, loss = 0.06370323\n",
      "Iteration 545, loss = 0.06356721\n",
      "Iteration 546, loss = 0.06343234\n",
      "Iteration 547, loss = 0.06329865\n",
      "Iteration 548, loss = 0.06316600\n",
      "Iteration 549, loss = 0.06303450\n",
      "Iteration 550, loss = 0.06290410\n",
      "Iteration 551, loss = 0.06277482\n",
      "Iteration 552, loss = 0.06264662\n",
      "Iteration 553, loss = 0.06251949\n",
      "Iteration 554, loss = 0.06239339\n",
      "Iteration 555, loss = 0.06226832\n",
      "Iteration 556, loss = 0.06214427\n",
      "Iteration 557, loss = 0.06202123\n",
      "Iteration 558, loss = 0.06189923\n",
      "Iteration 559, loss = 0.06177824\n",
      "Iteration 560, loss = 0.06165826\n",
      "Iteration 561, loss = 0.06153928\n",
      "Iteration 562, loss = 0.06142126\n",
      "Iteration 563, loss = 0.06130421\n",
      "Iteration 564, loss = 0.06118814\n",
      "Iteration 565, loss = 0.06107297\n",
      "Iteration 566, loss = 0.06095883\n",
      "Iteration 567, loss = 0.06084552\n",
      "Iteration 568, loss = 0.06073326\n",
      "Iteration 569, loss = 0.06062194\n",
      "Iteration 570, loss = 0.06051152\n",
      "Iteration 571, loss = 0.06040198\n",
      "Iteration 572, loss = 0.06029336\n",
      "Iteration 573, loss = 0.06018556\n",
      "Iteration 574, loss = 0.06007860\n",
      "Iteration 575, loss = 0.05997257\n",
      "Iteration 576, loss = 0.05986729\n",
      "Iteration 577, loss = 0.05976289\n",
      "Iteration 578, loss = 0.05965921\n",
      "Iteration 579, loss = 0.05955645\n",
      "Iteration 580, loss = 0.05945436\n",
      "Iteration 581, loss = 0.05935315\n",
      "Iteration 582, loss = 0.05925261\n",
      "Iteration 583, loss = 0.05915292\n",
      "Iteration 584, loss = 0.05905391\n",
      "Iteration 585, loss = 0.05895586\n",
      "Iteration 586, loss = 0.05885851\n",
      "Iteration 587, loss = 0.05876182\n",
      "Iteration 588, loss = 0.05866598\n",
      "Iteration 589, loss = 0.05857072\n",
      "Iteration 590, loss = 0.05847631\n",
      "Iteration 591, loss = 0.05838251\n",
      "Iteration 592, loss = 0.05828947\n",
      "Iteration 593, loss = 0.05819714\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49174295\n",
      "Iteration 2, loss = 1.46211109\n",
      "Iteration 3, loss = 1.43328691\n",
      "Iteration 4, loss = 1.40544449\n",
      "Iteration 5, loss = 1.37858816\n",
      "Iteration 6, loss = 1.35254150\n",
      "Iteration 7, loss = 1.32720236\n",
      "Iteration 8, loss = 1.30252209\n",
      "Iteration 9, loss = 1.27860234\n",
      "Iteration 10, loss = 1.25556334\n",
      "Iteration 11, loss = 1.23331567\n",
      "Iteration 12, loss = 1.21192025\n",
      "Iteration 13, loss = 1.19158888\n",
      "Iteration 14, loss = 1.17230811\n",
      "Iteration 15, loss = 1.15379169\n",
      "Iteration 16, loss = 1.13598303\n",
      "Iteration 17, loss = 1.11878158\n",
      "Iteration 18, loss = 1.10218775\n",
      "Iteration 19, loss = 1.08624380\n",
      "Iteration 20, loss = 1.07089268\n",
      "Iteration 21, loss = 1.05617065\n",
      "Iteration 22, loss = 1.04196642\n",
      "Iteration 23, loss = 1.02811644\n",
      "Iteration 24, loss = 1.01467924\n",
      "Iteration 25, loss = 1.00158668\n",
      "Iteration 26, loss = 0.98877491\n",
      "Iteration 27, loss = 0.97629072\n",
      "Iteration 28, loss = 0.96415087\n",
      "Iteration 29, loss = 0.95224164\n",
      "Iteration 30, loss = 0.94063007\n",
      "Iteration 31, loss = 0.92923238\n",
      "Iteration 32, loss = 0.91797539\n",
      "Iteration 33, loss = 0.90693822\n",
      "Iteration 34, loss = 0.89623284\n",
      "Iteration 35, loss = 0.88579098\n",
      "Iteration 36, loss = 0.87560845\n",
      "Iteration 37, loss = 0.86570651\n",
      "Iteration 38, loss = 0.85601435\n",
      "Iteration 39, loss = 0.84656563\n",
      "Iteration 40, loss = 0.83737179\n",
      "Iteration 41, loss = 0.82843827\n",
      "Iteration 42, loss = 0.81963826\n",
      "Iteration 43, loss = 0.81098926\n",
      "Iteration 44, loss = 0.80254076\n",
      "Iteration 45, loss = 0.79428922\n",
      "Iteration 46, loss = 0.78619452\n",
      "Iteration 47, loss = 0.77827440\n",
      "Iteration 48, loss = 0.77050580\n",
      "Iteration 49, loss = 0.76286085\n",
      "Iteration 50, loss = 0.75533561\n",
      "Iteration 51, loss = 0.74793558\n",
      "Iteration 52, loss = 0.74066276\n",
      "Iteration 53, loss = 0.73352547\n",
      "Iteration 54, loss = 0.72652042\n",
      "Iteration 55, loss = 0.71963558\n",
      "Iteration 56, loss = 0.71286787\n",
      "Iteration 57, loss = 0.70625429\n",
      "Iteration 58, loss = 0.69978530\n",
      "Iteration 59, loss = 0.69347761\n",
      "Iteration 60, loss = 0.68734825\n",
      "Iteration 61, loss = 0.68136364\n",
      "Iteration 62, loss = 0.67552744\n",
      "Iteration 63, loss = 0.66983245\n",
      "Iteration 64, loss = 0.66429249\n",
      "Iteration 65, loss = 0.65886078\n",
      "Iteration 66, loss = 0.65352884\n",
      "Iteration 67, loss = 0.64828580\n",
      "Iteration 68, loss = 0.64312507\n",
      "Iteration 69, loss = 0.63804733\n",
      "Iteration 70, loss = 0.63304550\n",
      "Iteration 71, loss = 0.62812012\n",
      "Iteration 72, loss = 0.62326627\n",
      "Iteration 73, loss = 0.61843149\n",
      "Iteration 74, loss = 0.61366313\n",
      "Iteration 75, loss = 0.60892530\n",
      "Iteration 76, loss = 0.60409063\n",
      "Iteration 77, loss = 0.59920615\n",
      "Iteration 78, loss = 0.59425684\n",
      "Iteration 79, loss = 0.58920025\n",
      "Iteration 80, loss = 0.58424295\n",
      "Iteration 81, loss = 0.57929902\n",
      "Iteration 82, loss = 0.57442247\n",
      "Iteration 83, loss = 0.56971919\n",
      "Iteration 84, loss = 0.56519247\n",
      "Iteration 85, loss = 0.56082830\n",
      "Iteration 86, loss = 0.55667783\n",
      "Iteration 87, loss = 0.55271716\n",
      "Iteration 88, loss = 0.54884792\n",
      "Iteration 89, loss = 0.54511435\n",
      "Iteration 90, loss = 0.54145429\n",
      "Iteration 91, loss = 0.53784028\n",
      "Iteration 92, loss = 0.53427685\n",
      "Iteration 93, loss = 0.53076957\n",
      "Iteration 94, loss = 0.52731608\n",
      "Iteration 95, loss = 0.52390152\n",
      "Iteration 96, loss = 0.52049526\n",
      "Iteration 97, loss = 0.51708180\n",
      "Iteration 98, loss = 0.51366081\n",
      "Iteration 99, loss = 0.51026645\n",
      "Iteration 100, loss = 0.50694026\n",
      "Iteration 101, loss = 0.50371725\n",
      "Iteration 102, loss = 0.50055530\n",
      "Iteration 103, loss = 0.49742442\n",
      "Iteration 104, loss = 0.49432156\n",
      "Iteration 105, loss = 0.49124523\n",
      "Iteration 106, loss = 0.48819835\n",
      "Iteration 107, loss = 0.48517948\n",
      "Iteration 108, loss = 0.48218806\n",
      "Iteration 109, loss = 0.47922420\n",
      "Iteration 110, loss = 0.47628905\n",
      "Iteration 111, loss = 0.47338238\n",
      "Iteration 112, loss = 0.47050421\n",
      "Iteration 113, loss = 0.46765688\n",
      "Iteration 114, loss = 0.46483724\n",
      "Iteration 115, loss = 0.46204582\n",
      "Iteration 116, loss = 0.45928185\n",
      "Iteration 117, loss = 0.45654454\n",
      "Iteration 118, loss = 0.45382563\n",
      "Iteration 119, loss = 0.45113233\n",
      "Iteration 120, loss = 0.44846196\n",
      "Iteration 121, loss = 0.44580651\n",
      "Iteration 122, loss = 0.44316908\n",
      "Iteration 123, loss = 0.44054448\n",
      "Iteration 124, loss = 0.43793445\n",
      "Iteration 125, loss = 0.43532733\n",
      "Iteration 126, loss = 0.43272672\n",
      "Iteration 127, loss = 0.43013246\n",
      "Iteration 128, loss = 0.42752466\n",
      "Iteration 129, loss = 0.42490465\n",
      "Iteration 130, loss = 0.42225214\n",
      "Iteration 131, loss = 0.41955534\n",
      "Iteration 132, loss = 0.41680470\n",
      "Iteration 133, loss = 0.41401053\n",
      "Iteration 134, loss = 0.41119327\n",
      "Iteration 135, loss = 0.40839052\n",
      "Iteration 136, loss = 0.40556358\n",
      "Iteration 137, loss = 0.40278341\n",
      "Iteration 138, loss = 0.40001776\n",
      "Iteration 139, loss = 0.39735242\n",
      "Iteration 140, loss = 0.39480988\n",
      "Iteration 141, loss = 0.39245350\n",
      "Iteration 142, loss = 0.39021185\n",
      "Iteration 143, loss = 0.38802677\n",
      "Iteration 144, loss = 0.38590054\n",
      "Iteration 145, loss = 0.38383566\n",
      "Iteration 146, loss = 0.38179394\n",
      "Iteration 147, loss = 0.37975185\n",
      "Iteration 148, loss = 0.37771793\n",
      "Iteration 149, loss = 0.37568269\n",
      "Iteration 150, loss = 0.37365149\n",
      "Iteration 151, loss = 0.37161819\n",
      "Iteration 152, loss = 0.36957824\n",
      "Iteration 153, loss = 0.36755327\n",
      "Iteration 154, loss = 0.36552896\n",
      "Iteration 155, loss = 0.36351218\n",
      "Iteration 156, loss = 0.36151187\n",
      "Iteration 157, loss = 0.35951045\n",
      "Iteration 158, loss = 0.35753437\n",
      "Iteration 159, loss = 0.35557872\n",
      "Iteration 160, loss = 0.35363303\n",
      "Iteration 161, loss = 0.35169744\n",
      "Iteration 162, loss = 0.34977810\n",
      "Iteration 163, loss = 0.34787995\n",
      "Iteration 164, loss = 0.34600150\n",
      "Iteration 165, loss = 0.34414736\n",
      "Iteration 166, loss = 0.34230659\n",
      "Iteration 167, loss = 0.34048002\n",
      "Iteration 168, loss = 0.33866962\n",
      "Iteration 169, loss = 0.33687422\n",
      "Iteration 170, loss = 0.33509095\n",
      "Iteration 171, loss = 0.33331410\n",
      "Iteration 172, loss = 0.33154987\n",
      "Iteration 173, loss = 0.32979749\n",
      "Iteration 174, loss = 0.32805510\n",
      "Iteration 175, loss = 0.32632242\n",
      "Iteration 176, loss = 0.32460103\n",
      "Iteration 177, loss = 0.32288955\n",
      "Iteration 178, loss = 0.32118854\n",
      "Iteration 179, loss = 0.31949772\n",
      "Iteration 180, loss = 0.31781730\n",
      "Iteration 181, loss = 0.31614704\n",
      "Iteration 182, loss = 0.31448710\n",
      "Iteration 183, loss = 0.31283685\n",
      "Iteration 184, loss = 0.31119791\n",
      "Iteration 185, loss = 0.30957028\n",
      "Iteration 186, loss = 0.30795403\n",
      "Iteration 187, loss = 0.30634706\n",
      "Iteration 188, loss = 0.30475027\n",
      "Iteration 189, loss = 0.30316515\n",
      "Iteration 190, loss = 0.30158984\n",
      "Iteration 191, loss = 0.30002533\n",
      "Iteration 192, loss = 0.29847275\n",
      "Iteration 193, loss = 0.29693178\n",
      "Iteration 194, loss = 0.29540047\n",
      "Iteration 195, loss = 0.29387884\n",
      "Iteration 196, loss = 0.29236725\n",
      "Iteration 197, loss = 0.29086694\n",
      "Iteration 198, loss = 0.28937647\n",
      "Iteration 199, loss = 0.28789590\n",
      "Iteration 200, loss = 0.28642555\n",
      "Iteration 201, loss = 0.28496485\n",
      "Iteration 202, loss = 0.28351377\n",
      "Iteration 203, loss = 0.28207259\n",
      "Iteration 204, loss = 0.28064134\n",
      "Iteration 205, loss = 0.27921992\n",
      "Iteration 206, loss = 0.27780915\n",
      "Iteration 207, loss = 0.27640816\n",
      "Iteration 208, loss = 0.27501646\n",
      "Iteration 209, loss = 0.27363425\n",
      "Iteration 210, loss = 0.27226239\n",
      "Iteration 211, loss = 0.27089985\n",
      "Iteration 212, loss = 0.26954671\n",
      "Iteration 213, loss = 0.26820349\n",
      "Iteration 214, loss = 0.26686936\n",
      "Iteration 215, loss = 0.26554428\n",
      "Iteration 216, loss = 0.26422991\n",
      "Iteration 217, loss = 0.26292395\n",
      "Iteration 218, loss = 0.26162626\n",
      "Iteration 219, loss = 0.26033964\n",
      "Iteration 220, loss = 0.25906247\n",
      "Iteration 221, loss = 0.25779419\n",
      "Iteration 222, loss = 0.25653574\n",
      "Iteration 223, loss = 0.25528585\n",
      "Iteration 224, loss = 0.25404516\n",
      "Iteration 225, loss = 0.25281496\n",
      "Iteration 226, loss = 0.25159282\n",
      "Iteration 227, loss = 0.25037895\n",
      "Iteration 228, loss = 0.24917566\n",
      "Iteration 229, loss = 0.24798076\n",
      "Iteration 230, loss = 0.24679416\n",
      "Iteration 231, loss = 0.24561635\n",
      "Iteration 232, loss = 0.24444907\n",
      "Iteration 233, loss = 0.24328863\n",
      "Iteration 234, loss = 0.24213753\n",
      "Iteration 235, loss = 0.24099628\n",
      "Iteration 236, loss = 0.23986297\n",
      "Iteration 237, loss = 0.23873750\n",
      "Iteration 238, loss = 0.23762017\n",
      "Iteration 239, loss = 0.23651225\n",
      "Iteration 240, loss = 0.23541284\n",
      "Iteration 241, loss = 0.23432162\n",
      "Iteration 242, loss = 0.23323931\n",
      "Iteration 243, loss = 0.23216378\n",
      "Iteration 244, loss = 0.23109783\n",
      "Iteration 245, loss = 0.23003987\n",
      "Iteration 246, loss = 0.22898962\n",
      "Iteration 247, loss = 0.22794765\n",
      "Iteration 248, loss = 0.22691373\n",
      "Iteration 249, loss = 0.22588796\n",
      "Iteration 250, loss = 0.22487019\n",
      "Iteration 251, loss = 0.22386023\n",
      "Iteration 252, loss = 0.22285808\n",
      "Iteration 253, loss = 0.22186364\n",
      "Iteration 254, loss = 0.22087699\n",
      "Iteration 255, loss = 0.21989812\n",
      "Iteration 256, loss = 0.21892681\n",
      "Iteration 257, loss = 0.21796306\n",
      "Iteration 258, loss = 0.21700675\n",
      "Iteration 259, loss = 0.21605833\n",
      "Iteration 260, loss = 0.21511667\n",
      "Iteration 261, loss = 0.21418276\n",
      "Iteration 262, loss = 0.21325609\n",
      "Iteration 263, loss = 0.21233682\n",
      "Iteration 264, loss = 0.21142534\n",
      "Iteration 265, loss = 0.21052089\n",
      "Iteration 266, loss = 0.20962348\n",
      "Iteration 267, loss = 0.20873349\n",
      "Iteration 268, loss = 0.20785012\n",
      "Iteration 269, loss = 0.20697398\n",
      "Iteration 270, loss = 0.20610461\n",
      "Iteration 271, loss = 0.20524213\n",
      "Iteration 272, loss = 0.20438649\n",
      "Iteration 273, loss = 0.20353765\n",
      "Iteration 274, loss = 0.20269551\n",
      "Iteration 275, loss = 0.20186002\n",
      "Iteration 276, loss = 0.20103148\n",
      "Iteration 277, loss = 0.20020893\n",
      "Iteration 278, loss = 0.19939330\n",
      "Iteration 279, loss = 0.19858409\n",
      "Iteration 280, loss = 0.19778124\n",
      "Iteration 281, loss = 0.19698505\n",
      "Iteration 282, loss = 0.19619426\n",
      "Iteration 283, loss = 0.19540890\n",
      "Iteration 284, loss = 0.19463187\n",
      "Iteration 285, loss = 0.19386058\n",
      "Iteration 286, loss = 0.19309153\n",
      "Iteration 287, loss = 0.19233105\n",
      "Iteration 288, loss = 0.19157705\n",
      "Iteration 289, loss = 0.19082537\n",
      "Iteration 290, loss = 0.19007833\n",
      "Iteration 291, loss = 0.18933777\n",
      "Iteration 292, loss = 0.18860043\n",
      "Iteration 293, loss = 0.18786674\n",
      "Iteration 294, loss = 0.18713776\n",
      "Iteration 295, loss = 0.18641296\n",
      "Iteration 296, loss = 0.18569024\n",
      "Iteration 297, loss = 0.18497145\n",
      "Iteration 298, loss = 0.18425676\n",
      "Iteration 299, loss = 0.18354438\n",
      "Iteration 300, loss = 0.18283606\n",
      "Iteration 301, loss = 0.18213129\n",
      "Iteration 302, loss = 0.18142862\n",
      "Iteration 303, loss = 0.18072840\n",
      "Iteration 304, loss = 0.18003218\n",
      "Iteration 305, loss = 0.17933923\n",
      "Iteration 306, loss = 0.17864869\n",
      "Iteration 307, loss = 0.17796181\n",
      "Iteration 308, loss = 0.17727900\n",
      "Iteration 309, loss = 0.17659828\n",
      "Iteration 310, loss = 0.17591971\n",
      "Iteration 311, loss = 0.17524507\n",
      "Iteration 312, loss = 0.17457464\n",
      "Iteration 313, loss = 0.17390741\n",
      "Iteration 314, loss = 0.17324327\n",
      "Iteration 315, loss = 0.17258269\n",
      "Iteration 316, loss = 0.17192596\n",
      "Iteration 317, loss = 0.17127279\n",
      "Iteration 318, loss = 0.17062253\n",
      "Iteration 319, loss = 0.16997556\n",
      "Iteration 320, loss = 0.16933300\n",
      "Iteration 321, loss = 0.16869398\n",
      "Iteration 322, loss = 0.16805896\n",
      "Iteration 323, loss = 0.16742827\n",
      "Iteration 324, loss = 0.16680131\n",
      "Iteration 325, loss = 0.16617719\n",
      "Iteration 326, loss = 0.16555647\n",
      "Iteration 327, loss = 0.16493976\n",
      "Iteration 328, loss = 0.16432730\n",
      "Iteration 329, loss = 0.16371917\n",
      "Iteration 330, loss = 0.16311416\n",
      "Iteration 331, loss = 0.16251350\n",
      "Iteration 332, loss = 0.16191691\n",
      "Iteration 333, loss = 0.16132433\n",
      "Iteration 334, loss = 0.16073518\n",
      "Iteration 335, loss = 0.16015030\n",
      "Iteration 336, loss = 0.15956962\n",
      "Iteration 337, loss = 0.15899287\n",
      "Iteration 338, loss = 0.15842006\n",
      "Iteration 339, loss = 0.15785118\n",
      "Iteration 340, loss = 0.15728650\n",
      "Iteration 341, loss = 0.15672647\n",
      "Iteration 342, loss = 0.15617018\n",
      "Iteration 343, loss = 0.15561780\n",
      "Iteration 344, loss = 0.15507108\n",
      "Iteration 345, loss = 0.15453055\n",
      "Iteration 346, loss = 0.15399383\n",
      "Iteration 347, loss = 0.15346065\n",
      "Iteration 348, loss = 0.15293118\n",
      "Iteration 349, loss = 0.15240603\n",
      "Iteration 350, loss = 0.15188538\n",
      "Iteration 351, loss = 0.15136862\n",
      "Iteration 352, loss = 0.15085558\n",
      "Iteration 353, loss = 0.15034710\n",
      "Iteration 354, loss = 0.14984334\n",
      "Iteration 355, loss = 0.14934289\n",
      "Iteration 356, loss = 0.14884682\n",
      "Iteration 357, loss = 0.14835478\n",
      "Iteration 358, loss = 0.14786656\n",
      "Iteration 359, loss = 0.14738241\n",
      "Iteration 360, loss = 0.14690273\n",
      "Iteration 361, loss = 0.14642751\n",
      "Iteration 362, loss = 0.14595582\n",
      "Iteration 363, loss = 0.14548672\n",
      "Iteration 364, loss = 0.14502078\n",
      "Iteration 365, loss = 0.14455895\n",
      "Iteration 366, loss = 0.14409960\n",
      "Iteration 367, loss = 0.14364303\n",
      "Iteration 368, loss = 0.14318932\n",
      "Iteration 369, loss = 0.14273798\n",
      "Iteration 370, loss = 0.14228976\n",
      "Iteration 371, loss = 0.14184479\n",
      "Iteration 372, loss = 0.14140245\n",
      "Iteration 373, loss = 0.14096345\n",
      "Iteration 374, loss = 0.14052772\n",
      "Iteration 375, loss = 0.14008895\n",
      "Iteration 376, loss = 0.13964883\n",
      "Iteration 377, loss = 0.13920970\n",
      "Iteration 378, loss = 0.13877253\n",
      "Iteration 379, loss = 0.13833697\n",
      "Iteration 380, loss = 0.13790251\n",
      "Iteration 381, loss = 0.13746370\n",
      "Iteration 382, loss = 0.13703430\n",
      "Iteration 383, loss = 0.13660956\n",
      "Iteration 384, loss = 0.13618746\n",
      "Iteration 385, loss = 0.13576856\n",
      "Iteration 386, loss = 0.13534402\n",
      "Iteration 387, loss = 0.13491959\n",
      "Iteration 388, loss = 0.13450856\n",
      "Iteration 389, loss = 0.13410998\n",
      "Iteration 390, loss = 0.13370280\n",
      "Iteration 391, loss = 0.13328828\n",
      "Iteration 392, loss = 0.13286756\n",
      "Iteration 393, loss = 0.13243835\n",
      "Iteration 394, loss = 0.13200285\n",
      "Iteration 395, loss = 0.13157043\n",
      "Iteration 396, loss = 0.13113525\n",
      "Iteration 397, loss = 0.13071875\n",
      "Iteration 398, loss = 0.13030968\n",
      "Iteration 399, loss = 0.12991225\n",
      "Iteration 400, loss = 0.12953207\n",
      "Iteration 401, loss = 0.12915905\n",
      "Iteration 402, loss = 0.12879724\n",
      "Iteration 403, loss = 0.12845501\n",
      "Iteration 404, loss = 0.12812449\n",
      "Iteration 405, loss = 0.12779481\n",
      "Iteration 406, loss = 0.12746509\n",
      "Iteration 407, loss = 0.12713593\n",
      "Iteration 408, loss = 0.12680764\n",
      "Iteration 409, loss = 0.12647922\n",
      "Iteration 410, loss = 0.12615110\n",
      "Iteration 411, loss = 0.12582437\n",
      "Iteration 412, loss = 0.12549796\n",
      "Iteration 413, loss = 0.12517115\n",
      "Iteration 414, loss = 0.12484419\n",
      "Iteration 415, loss = 0.12451737\n",
      "Iteration 416, loss = 0.12419044\n",
      "Iteration 417, loss = 0.12386379\n",
      "Iteration 418, loss = 0.12353771\n",
      "Iteration 419, loss = 0.12321262\n",
      "Iteration 420, loss = 0.12288957\n",
      "Iteration 421, loss = 0.12256751\n",
      "Iteration 422, loss = 0.12224655\n",
      "Iteration 423, loss = 0.12192641\n",
      "Iteration 424, loss = 0.12160761\n",
      "Iteration 425, loss = 0.12128977\n",
      "Iteration 426, loss = 0.12097341\n",
      "Iteration 427, loss = 0.12065800\n",
      "Iteration 428, loss = 0.12034334\n",
      "Iteration 429, loss = 0.12002956\n",
      "Iteration 430, loss = 0.11971649\n",
      "Iteration 431, loss = 0.11940427\n",
      "Iteration 432, loss = 0.11909347\n",
      "Iteration 433, loss = 0.11878370\n",
      "Iteration 434, loss = 0.11847470\n",
      "Iteration 435, loss = 0.11816631\n",
      "Iteration 436, loss = 0.11785850\n",
      "Iteration 437, loss = 0.11755126\n",
      "Iteration 438, loss = 0.11724466\n",
      "Iteration 439, loss = 0.11693906\n",
      "Iteration 440, loss = 0.11663563\n",
      "Iteration 441, loss = 0.11633394\n",
      "Iteration 442, loss = 0.11603308\n",
      "Iteration 443, loss = 0.11573289\n",
      "Iteration 444, loss = 0.11543356\n",
      "Iteration 445, loss = 0.11513513\n",
      "Iteration 446, loss = 0.11483769\n",
      "Iteration 447, loss = 0.11454122\n",
      "Iteration 448, loss = 0.11424595\n",
      "Iteration 449, loss = 0.11395160\n",
      "Iteration 450, loss = 0.11365847\n",
      "Iteration 451, loss = 0.11336659\n",
      "Iteration 452, loss = 0.11307604\n",
      "Iteration 453, loss = 0.11278675\n",
      "Iteration 454, loss = 0.11249895\n",
      "Iteration 455, loss = 0.11221259\n",
      "Iteration 456, loss = 0.11192784\n",
      "Iteration 457, loss = 0.11164466\n",
      "Iteration 458, loss = 0.11136317\n",
      "Iteration 459, loss = 0.11108342\n",
      "Iteration 460, loss = 0.11080627\n",
      "Iteration 461, loss = 0.11053076\n",
      "Iteration 462, loss = 0.11025694\n",
      "Iteration 463, loss = 0.10998507\n",
      "Iteration 464, loss = 0.10971509\n",
      "Iteration 465, loss = 0.10944695\n",
      "Iteration 466, loss = 0.10918069\n",
      "Iteration 467, loss = 0.10891631\n",
      "Iteration 468, loss = 0.10865397\n",
      "Iteration 469, loss = 0.10839368\n",
      "Iteration 470, loss = 0.10813557\n",
      "Iteration 471, loss = 0.10787947\n",
      "Iteration 472, loss = 0.10762530\n",
      "Iteration 473, loss = 0.10737327\n",
      "Iteration 474, loss = 0.10712372\n",
      "Iteration 475, loss = 0.10687613\n",
      "Iteration 476, loss = 0.10663054\n",
      "Iteration 477, loss = 0.10638690\n",
      "Iteration 478, loss = 0.10614529\n",
      "Iteration 479, loss = 0.10590597\n",
      "Iteration 480, loss = 0.10566884\n",
      "Iteration 481, loss = 0.10543400\n",
      "Iteration 482, loss = 0.10520126\n",
      "Iteration 483, loss = 0.10497058\n",
      "Iteration 484, loss = 0.10474203\n",
      "Iteration 485, loss = 0.10451557\n",
      "Iteration 486, loss = 0.10429119\n",
      "Iteration 487, loss = 0.10406884\n",
      "Iteration 488, loss = 0.10384869\n",
      "Iteration 489, loss = 0.10363068\n",
      "Iteration 490, loss = 0.10341478\n",
      "Iteration 491, loss = 0.10320088\n",
      "Iteration 492, loss = 0.10298902\n",
      "Iteration 493, loss = 0.10277925\n",
      "Iteration 494, loss = 0.10257159\n",
      "Iteration 495, loss = 0.10236591\n",
      "Iteration 496, loss = 0.10216223\n",
      "Iteration 497, loss = 0.10196049\n",
      "Iteration 498, loss = 0.10176087\n",
      "Iteration 499, loss = 0.10156317\n",
      "Iteration 500, loss = 0.10136731\n",
      "Iteration 501, loss = 0.10117352\n",
      "Iteration 502, loss = 0.10098202\n",
      "Iteration 503, loss = 0.10079238\n",
      "Iteration 504, loss = 0.10060463\n",
      "Iteration 505, loss = 0.10041870\n",
      "Iteration 506, loss = 0.10023475\n",
      "Iteration 507, loss = 0.10005266\n",
      "Iteration 508, loss = 0.09987229\n",
      "Iteration 509, loss = 0.09969366\n",
      "Iteration 510, loss = 0.09951680\n",
      "Iteration 511, loss = 0.09934177\n",
      "Iteration 512, loss = 0.09916844\n",
      "Iteration 513, loss = 0.09899690\n",
      "Iteration 514, loss = 0.09882700\n",
      "Iteration 515, loss = 0.09865878\n",
      "Iteration 516, loss = 0.09849225\n",
      "Iteration 517, loss = 0.09832738\n",
      "Iteration 518, loss = 0.09816418\n",
      "Iteration 519, loss = 0.09800261\n",
      "Iteration 520, loss = 0.09784262\n",
      "Iteration 521, loss = 0.09768421\n",
      "Iteration 522, loss = 0.09752736\n",
      "Iteration 523, loss = 0.09737204\n",
      "Iteration 524, loss = 0.09721833\n",
      "Iteration 525, loss = 0.09706611\n",
      "Iteration 526, loss = 0.09691539\n",
      "Iteration 527, loss = 0.09676614\n",
      "Iteration 528, loss = 0.09661833\n",
      "Iteration 529, loss = 0.09647199\n",
      "Iteration 530, loss = 0.09632706\n",
      "Iteration 531, loss = 0.09618346\n",
      "Iteration 532, loss = 0.09604132\n",
      "Iteration 533, loss = 0.09590058\n",
      "Iteration 534, loss = 0.09576119\n",
      "Iteration 535, loss = 0.09562309\n",
      "Iteration 536, loss = 0.09548628\n",
      "Iteration 537, loss = 0.09535076\n",
      "Iteration 538, loss = 0.09521656\n",
      "Iteration 539, loss = 0.09508359\n",
      "Iteration 540, loss = 0.09495197\n",
      "Iteration 541, loss = 0.09482158\n",
      "Iteration 542, loss = 0.09469242\n",
      "Iteration 543, loss = 0.09456447\n",
      "Iteration 544, loss = 0.09443774\n",
      "Iteration 545, loss = 0.09431224\n",
      "Iteration 546, loss = 0.09418789\n",
      "Iteration 547, loss = 0.09406475\n",
      "Iteration 548, loss = 0.09394278\n",
      "Iteration 549, loss = 0.09382194\n",
      "Iteration 550, loss = 0.09370223\n",
      "Iteration 551, loss = 0.09358367\n",
      "Iteration 552, loss = 0.09346621\n",
      "Iteration 553, loss = 0.09334987\n",
      "Iteration 554, loss = 0.09323464\n",
      "Iteration 555, loss = 0.09312047\n",
      "Iteration 556, loss = 0.09300726\n",
      "Iteration 557, loss = 0.09289511\n",
      "Iteration 558, loss = 0.09278403\n",
      "Iteration 559, loss = 0.09267373\n",
      "Iteration 560, loss = 0.09256407\n",
      "Iteration 561, loss = 0.09245505\n",
      "Iteration 562, loss = 0.09234672\n",
      "Iteration 563, loss = 0.09223909\n",
      "Iteration 564, loss = 0.09213213\n",
      "Iteration 565, loss = 0.09202589\n",
      "Iteration 566, loss = 0.09192043\n",
      "Iteration 567, loss = 0.09181568\n",
      "Iteration 568, loss = 0.09171166\n",
      "Iteration 569, loss = 0.09160835\n",
      "Iteration 570, loss = 0.09150596\n",
      "Iteration 571, loss = 0.09140582\n",
      "Iteration 572, loss = 0.09130686\n",
      "Iteration 573, loss = 0.09120891\n",
      "Iteration 574, loss = 0.09111180\n",
      "Iteration 575, loss = 0.09101584\n",
      "Iteration 576, loss = 0.09092080\n",
      "Iteration 577, loss = 0.09082666\n",
      "Iteration 578, loss = 0.09073329\n",
      "Iteration 579, loss = 0.09064075\n",
      "Iteration 580, loss = 0.09054902\n",
      "Iteration 581, loss = 0.09045812\n",
      "Iteration 582, loss = 0.09036803\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48762646\n",
      "Iteration 2, loss = 1.47371141\n",
      "Iteration 3, loss = 1.45417536\n",
      "Iteration 4, loss = 1.42995974\n",
      "Iteration 5, loss = 1.40197366\n",
      "Iteration 6, loss = 1.37106191\n",
      "Iteration 7, loss = 1.33814295\n",
      "Iteration 8, loss = 1.30426142\n",
      "Iteration 9, loss = 1.27039389\n",
      "Iteration 10, loss = 1.23772986\n",
      "Iteration 11, loss = 1.20659852\n",
      "Iteration 12, loss = 1.17739010\n",
      "Iteration 13, loss = 1.15049083\n",
      "Iteration 14, loss = 1.12616724\n",
      "Iteration 15, loss = 1.10480581\n",
      "Iteration 16, loss = 1.08609340\n",
      "Iteration 17, loss = 1.06952768\n",
      "Iteration 18, loss = 1.05482189\n",
      "Iteration 19, loss = 1.04163892\n",
      "Iteration 20, loss = 1.02942412\n",
      "Iteration 21, loss = 1.01772555\n",
      "Iteration 22, loss = 1.00615888\n",
      "Iteration 23, loss = 0.99448202\n",
      "Iteration 24, loss = 0.98262362\n",
      "Iteration 25, loss = 0.97059261\n",
      "Iteration 26, loss = 0.95844889\n",
      "Iteration 27, loss = 0.94633258\n",
      "Iteration 28, loss = 0.93428182\n",
      "Iteration 29, loss = 0.92232085\n",
      "Iteration 30, loss = 0.91058077\n",
      "Iteration 31, loss = 0.89909106\n",
      "Iteration 32, loss = 0.88788004\n",
      "Iteration 33, loss = 0.87706972\n",
      "Iteration 34, loss = 0.86678895\n",
      "Iteration 35, loss = 0.85698123\n",
      "Iteration 36, loss = 0.84752930\n",
      "Iteration 37, loss = 0.83834119\n",
      "Iteration 38, loss = 0.82947861\n",
      "Iteration 39, loss = 0.82091319\n",
      "Iteration 40, loss = 0.81255875\n",
      "Iteration 41, loss = 0.80443383\n",
      "Iteration 42, loss = 0.79650816\n",
      "Iteration 43, loss = 0.78888922\n",
      "Iteration 44, loss = 0.78147695\n",
      "Iteration 45, loss = 0.77433013\n",
      "Iteration 46, loss = 0.76745950\n",
      "Iteration 47, loss = 0.76077477\n",
      "Iteration 48, loss = 0.75424616\n",
      "Iteration 49, loss = 0.74788958\n",
      "Iteration 50, loss = 0.74176848\n",
      "Iteration 51, loss = 0.73581701\n",
      "Iteration 52, loss = 0.73002430\n",
      "Iteration 53, loss = 0.72439991\n",
      "Iteration 54, loss = 0.71892218\n",
      "Iteration 55, loss = 0.71358192\n",
      "Iteration 56, loss = 0.70836802\n",
      "Iteration 57, loss = 0.70326033\n",
      "Iteration 58, loss = 0.69826828\n",
      "Iteration 59, loss = 0.69338577\n",
      "Iteration 60, loss = 0.68861731\n",
      "Iteration 61, loss = 0.68398399\n",
      "Iteration 62, loss = 0.67949987\n",
      "Iteration 63, loss = 0.67514052\n",
      "Iteration 64, loss = 0.67087289\n",
      "Iteration 65, loss = 0.66669259\n",
      "Iteration 66, loss = 0.66260665\n",
      "Iteration 67, loss = 0.65860021\n",
      "Iteration 68, loss = 0.65467484\n",
      "Iteration 69, loss = 0.65082778\n",
      "Iteration 70, loss = 0.64705559\n",
      "Iteration 71, loss = 0.64335630\n",
      "Iteration 72, loss = 0.63972524\n",
      "Iteration 73, loss = 0.63616113\n",
      "Iteration 74, loss = 0.63266268\n",
      "Iteration 75, loss = 0.62922854\n",
      "Iteration 76, loss = 0.62585535\n",
      "Iteration 77, loss = 0.62254462\n",
      "Iteration 78, loss = 0.61929200\n",
      "Iteration 79, loss = 0.61609588\n",
      "Iteration 80, loss = 0.61295503\n",
      "Iteration 81, loss = 0.60987098\n",
      "Iteration 82, loss = 0.60683946\n",
      "Iteration 83, loss = 0.60385891\n",
      "Iteration 84, loss = 0.60093047\n",
      "Iteration 85, loss = 0.59805224\n",
      "Iteration 86, loss = 0.59522207\n",
      "Iteration 87, loss = 0.59243910\n",
      "Iteration 88, loss = 0.58970354\n",
      "Iteration 89, loss = 0.58701878\n",
      "Iteration 90, loss = 0.58437900\n",
      "Iteration 91, loss = 0.58178157\n",
      "Iteration 92, loss = 0.57922577\n",
      "Iteration 93, loss = 0.57670997\n",
      "Iteration 94, loss = 0.57423316\n",
      "Iteration 95, loss = 0.57179446\n",
      "Iteration 96, loss = 0.56939293\n",
      "Iteration 97, loss = 0.56702805\n",
      "Iteration 98, loss = 0.56470067\n",
      "Iteration 99, loss = 0.56241096\n",
      "Iteration 100, loss = 0.56015960\n",
      "Iteration 101, loss = 0.55794292\n",
      "Iteration 102, loss = 0.55575984\n",
      "Iteration 103, loss = 0.55360811\n",
      "Iteration 104, loss = 0.55148381\n",
      "Iteration 105, loss = 0.54938916\n",
      "Iteration 106, loss = 0.54732504\n",
      "Iteration 107, loss = 0.54529083\n",
      "Iteration 108, loss = 0.54328549\n",
      "Iteration 109, loss = 0.54130493\n",
      "Iteration 110, loss = 0.53935057\n",
      "Iteration 111, loss = 0.53742229\n",
      "Iteration 112, loss = 0.53552195\n",
      "Iteration 113, loss = 0.53365131\n",
      "Iteration 114, loss = 0.53180514\n",
      "Iteration 115, loss = 0.52998286\n",
      "Iteration 116, loss = 0.52818185\n",
      "Iteration 117, loss = 0.52640275\n",
      "Iteration 118, loss = 0.52464468\n",
      "Iteration 119, loss = 0.52290439\n",
      "Iteration 120, loss = 0.52117453\n",
      "Iteration 121, loss = 0.51945703\n",
      "Iteration 122, loss = 0.51774216\n",
      "Iteration 123, loss = 0.51602753\n",
      "Iteration 124, loss = 0.51432477\n",
      "Iteration 125, loss = 0.51265276\n",
      "Iteration 126, loss = 0.51099064\n",
      "Iteration 127, loss = 0.50933779\n",
      "Iteration 128, loss = 0.50769595\n",
      "Iteration 129, loss = 0.50607668\n",
      "Iteration 130, loss = 0.50446399\n",
      "Iteration 131, loss = 0.50285367\n",
      "Iteration 132, loss = 0.50124484\n",
      "Iteration 133, loss = 0.49965722\n",
      "Iteration 134, loss = 0.49810280\n",
      "Iteration 135, loss = 0.49655827\n",
      "Iteration 136, loss = 0.49502946\n",
      "Iteration 137, loss = 0.49352852\n",
      "Iteration 138, loss = 0.49207845\n",
      "Iteration 139, loss = 0.49066202\n",
      "Iteration 140, loss = 0.48927807\n",
      "Iteration 141, loss = 0.48791806\n",
      "Iteration 142, loss = 0.48659204\n",
      "Iteration 143, loss = 0.48529602\n",
      "Iteration 144, loss = 0.48401602\n",
      "Iteration 145, loss = 0.48274109\n",
      "Iteration 146, loss = 0.48147262\n",
      "Iteration 147, loss = 0.48021112\n",
      "Iteration 148, loss = 0.47895723\n",
      "Iteration 149, loss = 0.47771164\n",
      "Iteration 150, loss = 0.47647452\n",
      "Iteration 151, loss = 0.47524668\n",
      "Iteration 152, loss = 0.47402808\n",
      "Iteration 153, loss = 0.47281901\n",
      "Iteration 154, loss = 0.47161976\n",
      "Iteration 155, loss = 0.47043178\n",
      "Iteration 156, loss = 0.46925676\n",
      "Iteration 157, loss = 0.46809336\n",
      "Iteration 158, loss = 0.46694351\n",
      "Iteration 159, loss = 0.46580998\n",
      "Iteration 160, loss = 0.46469156\n",
      "Iteration 161, loss = 0.46358143\n",
      "Iteration 162, loss = 0.46247949\n",
      "Iteration 163, loss = 0.46138545\n",
      "Iteration 164, loss = 0.46029936\n",
      "Iteration 165, loss = 0.45922128\n",
      "Iteration 166, loss = 0.45815093\n",
      "Iteration 167, loss = 0.45708919\n",
      "Iteration 168, loss = 0.45603892\n",
      "Iteration 169, loss = 0.45499836\n",
      "Iteration 170, loss = 0.45396624\n",
      "Iteration 171, loss = 0.45294296\n",
      "Iteration 172, loss = 0.45192845\n",
      "Iteration 173, loss = 0.45091993\n",
      "Iteration 174, loss = 0.44991859\n",
      "Iteration 175, loss = 0.44892581\n",
      "Iteration 176, loss = 0.44794057\n",
      "Iteration 177, loss = 0.44696374\n",
      "Iteration 178, loss = 0.44599425\n",
      "Iteration 179, loss = 0.44503193\n",
      "Iteration 180, loss = 0.44407685\n",
      "Iteration 181, loss = 0.44312906\n",
      "Iteration 182, loss = 0.44218861\n",
      "Iteration 183, loss = 0.44125514\n",
      "Iteration 184, loss = 0.44032842\n",
      "Iteration 185, loss = 0.43940839\n",
      "Iteration 186, loss = 0.43849492\n",
      "Iteration 187, loss = 0.43758806\n",
      "Iteration 188, loss = 0.43668768\n",
      "Iteration 189, loss = 0.43579387\n",
      "Iteration 190, loss = 0.43490630\n",
      "Iteration 191, loss = 0.43402483\n",
      "Iteration 192, loss = 0.43314939\n",
      "Iteration 193, loss = 0.43227979\n",
      "Iteration 194, loss = 0.43141602\n",
      "Iteration 195, loss = 0.43055791\n",
      "Iteration 196, loss = 0.42970546\n",
      "Iteration 197, loss = 0.42885848\n",
      "Iteration 198, loss = 0.42801696\n",
      "Iteration 199, loss = 0.42718110\n",
      "Iteration 200, loss = 0.42635060\n",
      "Iteration 201, loss = 0.42552533\n",
      "Iteration 202, loss = 0.42470643\n",
      "Iteration 203, loss = 0.42389301\n",
      "Iteration 204, loss = 0.42308463\n",
      "Iteration 205, loss = 0.42228122\n",
      "Iteration 206, loss = 0.42148273\n",
      "Iteration 207, loss = 0.42068911\n",
      "Iteration 208, loss = 0.41990040\n",
      "Iteration 209, loss = 0.41911632\n",
      "Iteration 210, loss = 0.41833706\n",
      "Iteration 211, loss = 0.41756243\n",
      "Iteration 212, loss = 0.41679271\n",
      "Iteration 213, loss = 0.41602772\n",
      "Iteration 214, loss = 0.41526745\n",
      "Iteration 215, loss = 0.41451190\n",
      "Iteration 216, loss = 0.41376058\n",
      "Iteration 217, loss = 0.41301355\n",
      "Iteration 218, loss = 0.41227072\n",
      "Iteration 219, loss = 0.41153208\n",
      "Iteration 220, loss = 0.41079759\n",
      "Iteration 221, loss = 0.41006701\n",
      "Iteration 222, loss = 0.40934041\n",
      "Iteration 223, loss = 0.40861782\n",
      "Iteration 224, loss = 0.40789915\n",
      "Iteration 225, loss = 0.40718434\n",
      "Iteration 226, loss = 0.40647334\n",
      "Iteration 227, loss = 0.40576611\n",
      "Iteration 228, loss = 0.40506258\n",
      "Iteration 229, loss = 0.40436271\n",
      "Iteration 230, loss = 0.40366624\n",
      "Iteration 231, loss = 0.40297245\n",
      "Iteration 232, loss = 0.40228205\n",
      "Iteration 233, loss = 0.40159508\n",
      "Iteration 234, loss = 0.40091146\n",
      "Iteration 235, loss = 0.40023144\n",
      "Iteration 236, loss = 0.39955480\n",
      "Iteration 237, loss = 0.39888144\n",
      "Iteration 238, loss = 0.39821128\n",
      "Iteration 239, loss = 0.39754434\n",
      "Iteration 240, loss = 0.39688056\n",
      "Iteration 241, loss = 0.39621956\n",
      "Iteration 242, loss = 0.39556078\n",
      "Iteration 243, loss = 0.39490498\n",
      "Iteration 244, loss = 0.39425211\n",
      "Iteration 245, loss = 0.39360215\n",
      "Iteration 246, loss = 0.39295514\n",
      "Iteration 247, loss = 0.39231096\n",
      "Iteration 248, loss = 0.39166977\n",
      "Iteration 249, loss = 0.39103143\n",
      "Iteration 250, loss = 0.39039591\n",
      "Iteration 251, loss = 0.38976321\n",
      "Iteration 252, loss = 0.38913333\n",
      "Iteration 253, loss = 0.38850620\n",
      "Iteration 254, loss = 0.38788179\n",
      "Iteration 255, loss = 0.38726005\n",
      "Iteration 256, loss = 0.38664095\n",
      "Iteration 257, loss = 0.38602450\n",
      "Iteration 258, loss = 0.38541115\n",
      "Iteration 259, loss = 0.38480113\n",
      "Iteration 260, loss = 0.38419341\n",
      "Iteration 261, loss = 0.38358815\n",
      "Iteration 262, loss = 0.38298555\n",
      "Iteration 263, loss = 0.38238586\n",
      "Iteration 264, loss = 0.38178870\n",
      "Iteration 265, loss = 0.38119412\n",
      "Iteration 266, loss = 0.38060223\n",
      "Iteration 267, loss = 0.38001263\n",
      "Iteration 268, loss = 0.37942541\n",
      "Iteration 269, loss = 0.37883989\n",
      "Iteration 270, loss = 0.37825602\n",
      "Iteration 271, loss = 0.37767430\n",
      "Iteration 272, loss = 0.37709489\n",
      "Iteration 273, loss = 0.37651777\n",
      "Iteration 274, loss = 0.37594287\n",
      "Iteration 275, loss = 0.37537024\n",
      "Iteration 276, loss = 0.37479985\n",
      "Iteration 277, loss = 0.37423168\n",
      "Iteration 278, loss = 0.37366572\n",
      "Iteration 279, loss = 0.37310197\n",
      "Iteration 280, loss = 0.37254036\n",
      "Iteration 281, loss = 0.37198092\n",
      "Iteration 282, loss = 0.37142362\n",
      "Iteration 283, loss = 0.37086844\n",
      "Iteration 284, loss = 0.37031539\n",
      "Iteration 285, loss = 0.36976442\n",
      "Iteration 286, loss = 0.36921553\n",
      "Iteration 287, loss = 0.36866869\n",
      "Iteration 288, loss = 0.36812421\n",
      "Iteration 289, loss = 0.36758417\n",
      "Iteration 290, loss = 0.36704591\n",
      "Iteration 291, loss = 0.36650839\n",
      "Iteration 292, loss = 0.36597286\n",
      "Iteration 293, loss = 0.36543929\n",
      "Iteration 294, loss = 0.36490776\n",
      "Iteration 295, loss = 0.36437805\n",
      "Iteration 296, loss = 0.36385031\n",
      "Iteration 297, loss = 0.36332436\n",
      "Iteration 298, loss = 0.36279860\n",
      "Iteration 299, loss = 0.36227412\n",
      "Iteration 300, loss = 0.36174751\n",
      "Iteration 301, loss = 0.36121979\n",
      "Iteration 302, loss = 0.36069127\n",
      "Iteration 303, loss = 0.36016359\n",
      "Iteration 304, loss = 0.35963523\n",
      "Iteration 305, loss = 0.35910701\n",
      "Iteration 306, loss = 0.35857964\n",
      "Iteration 307, loss = 0.35805093\n",
      "Iteration 308, loss = 0.35752019\n",
      "Iteration 309, loss = 0.35698811\n",
      "Iteration 310, loss = 0.35646015\n",
      "Iteration 311, loss = 0.35593108\n",
      "Iteration 312, loss = 0.35540235\n",
      "Iteration 313, loss = 0.35487452\n",
      "Iteration 314, loss = 0.35434628\n",
      "Iteration 315, loss = 0.35381617\n",
      "Iteration 316, loss = 0.35328595\n",
      "Iteration 317, loss = 0.35276014\n",
      "Iteration 318, loss = 0.35224130\n",
      "Iteration 319, loss = 0.35172034\n",
      "Iteration 320, loss = 0.35119907\n",
      "Iteration 321, loss = 0.35067489\n",
      "Iteration 322, loss = 0.35015180\n",
      "Iteration 323, loss = 0.34963351\n",
      "Iteration 324, loss = 0.34911838\n",
      "Iteration 325, loss = 0.34860442\n",
      "Iteration 326, loss = 0.34809056\n",
      "Iteration 327, loss = 0.34757759\n",
      "Iteration 328, loss = 0.34706230\n",
      "Iteration 329, loss = 0.34654866\n",
      "Iteration 330, loss = 0.34603226\n",
      "Iteration 331, loss = 0.34551784\n",
      "Iteration 332, loss = 0.34500360\n",
      "Iteration 333, loss = 0.34448663\n",
      "Iteration 334, loss = 0.34396311\n",
      "Iteration 335, loss = 0.34343291\n",
      "Iteration 336, loss = 0.34290578\n",
      "Iteration 337, loss = 0.34238595\n",
      "Iteration 338, loss = 0.34187627\n",
      "Iteration 339, loss = 0.34136858\n",
      "Iteration 340, loss = 0.34086241\n",
      "Iteration 341, loss = 0.34035861\n",
      "Iteration 342, loss = 0.33986496\n",
      "Iteration 343, loss = 0.33938739\n",
      "Iteration 344, loss = 0.33891172\n",
      "Iteration 345, loss = 0.33844287\n",
      "Iteration 346, loss = 0.33797744\n",
      "Iteration 347, loss = 0.33751687\n",
      "Iteration 348, loss = 0.33705886\n",
      "Iteration 349, loss = 0.33660465\n",
      "Iteration 350, loss = 0.33615363\n",
      "Iteration 351, loss = 0.33570469\n",
      "Iteration 352, loss = 0.33525931\n",
      "Iteration 353, loss = 0.33481700\n",
      "Iteration 354, loss = 0.33437709\n",
      "Iteration 355, loss = 0.33394004\n",
      "Iteration 356, loss = 0.33350585\n",
      "Iteration 357, loss = 0.33307365\n",
      "Iteration 358, loss = 0.33264285\n",
      "Iteration 359, loss = 0.33221360\n",
      "Iteration 360, loss = 0.33178564\n",
      "Iteration 361, loss = 0.33136048\n",
      "Iteration 362, loss = 0.33093788\n",
      "Iteration 363, loss = 0.33051681\n",
      "Iteration 364, loss = 0.33009680\n",
      "Iteration 365, loss = 0.32967903\n",
      "Iteration 366, loss = 0.32926242\n",
      "Iteration 367, loss = 0.32884685\n",
      "Iteration 368, loss = 0.32843245\n",
      "Iteration 369, loss = 0.32801912\n",
      "Iteration 370, loss = 0.32760697\n",
      "Iteration 371, loss = 0.32719598\n",
      "Iteration 372, loss = 0.32678596\n",
      "Iteration 373, loss = 0.32637710\n",
      "Iteration 374, loss = 0.32596932\n",
      "Iteration 375, loss = 0.32556253\n",
      "Iteration 376, loss = 0.32515689\n",
      "Iteration 377, loss = 0.32475230\n",
      "Iteration 378, loss = 0.32434876\n",
      "Iteration 379, loss = 0.32394639\n",
      "Iteration 380, loss = 0.32354494\n",
      "Iteration 381, loss = 0.32314457\n",
      "Iteration 382, loss = 0.32274531\n",
      "Iteration 383, loss = 0.32234707\n",
      "Iteration 384, loss = 0.32194979\n",
      "Iteration 385, loss = 0.32155359\n",
      "Iteration 386, loss = 0.32115838\n",
      "Iteration 387, loss = 0.32076430\n",
      "Iteration 388, loss = 0.32037108\n",
      "Iteration 389, loss = 0.31997889\n",
      "Iteration 390, loss = 0.31958774\n",
      "Iteration 391, loss = 0.31919764\n",
      "Iteration 392, loss = 0.31880858\n",
      "Iteration 393, loss = 0.31842043\n",
      "Iteration 394, loss = 0.31803331\n",
      "Iteration 395, loss = 0.31764715\n",
      "Iteration 396, loss = 0.31726194\n",
      "Iteration 397, loss = 0.31687778\n",
      "Iteration 398, loss = 0.31649457\n",
      "Iteration 399, loss = 0.31611236\n",
      "Iteration 400, loss = 0.31573102\n",
      "Iteration 401, loss = 0.31535063\n",
      "Iteration 402, loss = 0.31497122\n",
      "Iteration 403, loss = 0.31459294\n",
      "Iteration 404, loss = 0.31421562\n",
      "Iteration 405, loss = 0.31383919\n",
      "Iteration 406, loss = 0.31346370\n",
      "Iteration 407, loss = 0.31308910\n",
      "Iteration 408, loss = 0.31271538\n",
      "Iteration 409, loss = 0.31234257\n",
      "Iteration 410, loss = 0.31197080\n",
      "Iteration 411, loss = 0.31159997\n",
      "Iteration 412, loss = 0.31123008\n",
      "Iteration 413, loss = 0.31086100\n",
      "Iteration 414, loss = 0.31049288\n",
      "Iteration 415, loss = 0.31012560\n",
      "Iteration 416, loss = 0.30975919\n",
      "Iteration 417, loss = 0.30939371\n",
      "Iteration 418, loss = 0.30902904\n",
      "Iteration 419, loss = 0.30866540\n",
      "Iteration 420, loss = 0.30830276\n",
      "Iteration 421, loss = 0.30794109\n",
      "Iteration 422, loss = 0.30758034\n",
      "Iteration 423, loss = 0.30722039\n",
      "Iteration 424, loss = 0.30686141\n",
      "Iteration 425, loss = 0.30650331\n",
      "Iteration 426, loss = 0.30614615\n",
      "Iteration 427, loss = 0.30578973\n",
      "Iteration 428, loss = 0.30543430\n",
      "Iteration 429, loss = 0.30507966\n",
      "Iteration 430, loss = 0.30472584\n",
      "Iteration 431, loss = 0.30437285\n",
      "Iteration 432, loss = 0.30402069\n",
      "Iteration 433, loss = 0.30366936\n",
      "Iteration 434, loss = 0.30331883\n",
      "Iteration 435, loss = 0.30296911\n",
      "Iteration 436, loss = 0.30262024\n",
      "Iteration 437, loss = 0.30227211\n",
      "Iteration 438, loss = 0.30192479\n",
      "Iteration 439, loss = 0.30157830\n",
      "Iteration 440, loss = 0.30123257\n",
      "Iteration 441, loss = 0.30088762\n",
      "Iteration 442, loss = 0.30054351\n",
      "Iteration 443, loss = 0.30020010\n",
      "Iteration 444, loss = 0.29985752\n",
      "Iteration 445, loss = 0.29951567\n",
      "Iteration 446, loss = 0.29917465\n",
      "Iteration 447, loss = 0.29883432\n",
      "Iteration 448, loss = 0.29849481\n",
      "Iteration 449, loss = 0.29815603\n",
      "Iteration 450, loss = 0.29781800\n",
      "Iteration 451, loss = 0.29748076\n",
      "Iteration 452, loss = 0.29714426\n",
      "Iteration 453, loss = 0.29680848\n",
      "Iteration 454, loss = 0.29647347\n",
      "Iteration 455, loss = 0.29613917\n",
      "Iteration 456, loss = 0.29580565\n",
      "Iteration 457, loss = 0.29547282\n",
      "Iteration 458, loss = 0.29514079\n",
      "Iteration 459, loss = 0.29480959\n",
      "Iteration 460, loss = 0.29447915\n",
      "Iteration 461, loss = 0.29414969\n",
      "Iteration 462, loss = 0.29382113\n",
      "Iteration 463, loss = 0.29349336\n",
      "Iteration 464, loss = 0.29316625\n",
      "Iteration 465, loss = 0.29283987\n",
      "Iteration 466, loss = 0.29251417\n",
      "Iteration 467, loss = 0.29218929\n",
      "Iteration 468, loss = 0.29186501\n",
      "Iteration 469, loss = 0.29154148\n",
      "Iteration 470, loss = 0.29121871\n",
      "Iteration 471, loss = 0.29089661\n",
      "Iteration 472, loss = 0.29057520\n",
      "Iteration 473, loss = 0.29025451\n",
      "Iteration 474, loss = 0.28993452\n",
      "Iteration 475, loss = 0.28961521\n",
      "Iteration 476, loss = 0.28929661\n",
      "Iteration 477, loss = 0.28897866\n",
      "Iteration 478, loss = 0.28866151\n",
      "Iteration 479, loss = 0.28834492\n",
      "Iteration 480, loss = 0.28802905\n",
      "Iteration 481, loss = 0.28771409\n",
      "Iteration 482, loss = 0.28740005\n",
      "Iteration 483, loss = 0.28708650\n",
      "Iteration 484, loss = 0.28677370\n",
      "Iteration 485, loss = 0.28646155\n",
      "Iteration 486, loss = 0.28615013\n",
      "Iteration 487, loss = 0.28583934\n",
      "Iteration 488, loss = 0.28552920\n",
      "Iteration 489, loss = 0.28521973\n",
      "Iteration 490, loss = 0.28491089\n",
      "Iteration 491, loss = 0.28460284\n",
      "Iteration 492, loss = 0.28429528\n",
      "Iteration 493, loss = 0.28398850\n",
      "Iteration 494, loss = 0.28368234\n",
      "Iteration 495, loss = 0.28337696\n",
      "Iteration 496, loss = 0.28307203\n",
      "Iteration 497, loss = 0.28276785\n",
      "Iteration 498, loss = 0.28246430\n",
      "Iteration 499, loss = 0.28216141\n",
      "Iteration 500, loss = 0.28185911\n",
      "Iteration 501, loss = 0.28155766\n",
      "Iteration 502, loss = 0.28125657\n",
      "Iteration 503, loss = 0.28095626\n",
      "Iteration 504, loss = 0.28065653\n",
      "Iteration 505, loss = 0.28035744\n",
      "Iteration 506, loss = 0.28005920\n",
      "Iteration 507, loss = 0.27976161\n",
      "Iteration 508, loss = 0.27946460\n",
      "Iteration 509, loss = 0.27916821\n",
      "Iteration 510, loss = 0.27887249\n",
      "Iteration 511, loss = 0.27857733\n",
      "Iteration 512, loss = 0.27828284\n",
      "Iteration 513, loss = 0.27798895\n",
      "Iteration 514, loss = 0.27769567\n",
      "Iteration 515, loss = 0.27740292\n",
      "Iteration 516, loss = 0.27711066\n",
      "Iteration 517, loss = 0.27681900\n",
      "Iteration 518, loss = 0.27652799\n",
      "Iteration 519, loss = 0.27623749\n",
      "Iteration 520, loss = 0.27594760\n",
      "Iteration 521, loss = 0.27565830\n",
      "Iteration 522, loss = 0.27536960\n",
      "Iteration 523, loss = 0.27508152\n",
      "Iteration 524, loss = 0.27479420\n",
      "Iteration 525, loss = 0.27450743\n",
      "Iteration 526, loss = 0.27422132\n",
      "Iteration 527, loss = 0.27393582\n",
      "Iteration 528, loss = 0.27365094\n",
      "Iteration 529, loss = 0.27336660\n",
      "Iteration 530, loss = 0.27308286\n",
      "Iteration 531, loss = 0.27279972\n",
      "Iteration 532, loss = 0.27251714\n",
      "Iteration 533, loss = 0.27223519\n",
      "Iteration 534, loss = 0.27195376\n",
      "Iteration 535, loss = 0.27167292\n",
      "Iteration 536, loss = 0.27139266\n",
      "Iteration 537, loss = 0.27111293\n",
      "Iteration 538, loss = 0.27083392\n",
      "Iteration 539, loss = 0.27055530\n",
      "Iteration 540, loss = 0.27027734\n",
      "Iteration 541, loss = 0.26999995\n",
      "Iteration 542, loss = 0.26972314\n",
      "Iteration 543, loss = 0.26944686\n",
      "Iteration 544, loss = 0.26917117\n",
      "Iteration 545, loss = 0.26889601\n",
      "Iteration 546, loss = 0.26862141\n",
      "Iteration 547, loss = 0.26834743\n",
      "Iteration 548, loss = 0.26807395\n",
      "Iteration 549, loss = 0.26780105\n",
      "Iteration 550, loss = 0.26752869\n",
      "Iteration 551, loss = 0.26725690\n",
      "Iteration 552, loss = 0.26698566\n",
      "Iteration 553, loss = 0.26671498\n",
      "Iteration 554, loss = 0.26644483\n",
      "Iteration 555, loss = 0.26617524\n",
      "Iteration 556, loss = 0.26590619\n",
      "Iteration 557, loss = 0.26563767\n",
      "Iteration 558, loss = 0.26536981\n",
      "Iteration 559, loss = 0.26510248\n",
      "Iteration 560, loss = 0.26483572\n",
      "Iteration 561, loss = 0.26456955\n",
      "Iteration 562, loss = 0.26430388\n",
      "Iteration 563, loss = 0.26403876\n",
      "Iteration 564, loss = 0.26377419\n",
      "Iteration 565, loss = 0.26351015\n",
      "Iteration 566, loss = 0.26324669\n",
      "Iteration 567, loss = 0.26298371\n",
      "Iteration 568, loss = 0.26272127\n",
      "Iteration 569, loss = 0.26245935\n",
      "Iteration 570, loss = 0.26219802\n",
      "Iteration 571, loss = 0.26193717\n",
      "Iteration 572, loss = 0.26167686\n",
      "Iteration 573, loss = 0.26141709\n",
      "Iteration 574, loss = 0.26115791\n",
      "Iteration 575, loss = 0.26089944\n",
      "Iteration 576, loss = 0.26064156\n",
      "Iteration 577, loss = 0.26038417\n",
      "Iteration 578, loss = 0.26012735\n",
      "Iteration 579, loss = 0.25987103\n",
      "Iteration 580, loss = 0.25961528\n",
      "Iteration 581, loss = 0.25936006\n",
      "Iteration 582, loss = 0.25910532\n",
      "Iteration 583, loss = 0.25885114\n",
      "Iteration 584, loss = 0.25859740\n",
      "Iteration 585, loss = 0.25834433\n",
      "Iteration 586, loss = 0.25809162\n",
      "Iteration 587, loss = 0.25783951\n",
      "Iteration 588, loss = 0.25758789\n",
      "Iteration 589, loss = 0.25733673\n",
      "Iteration 590, loss = 0.25708616\n",
      "Iteration 591, loss = 0.25683607\n",
      "Iteration 592, loss = 0.25658654\n",
      "Iteration 593, loss = 0.25633749\n",
      "Iteration 594, loss = 0.25608897\n",
      "Iteration 595, loss = 0.25584097\n",
      "Iteration 596, loss = 0.25559345\n",
      "Iteration 597, loss = 0.25534643\n",
      "Iteration 598, loss = 0.25509990\n",
      "Iteration 599, loss = 0.25485389\n",
      "Iteration 600, loss = 0.25460837\n",
      "Iteration 601, loss = 0.25436334\n",
      "Iteration 602, loss = 0.25411877\n",
      "Iteration 603, loss = 0.25387474\n",
      "Iteration 604, loss = 0.25363112\n",
      "Iteration 605, loss = 0.25338810\n",
      "Iteration 606, loss = 0.25314556\n",
      "Iteration 607, loss = 0.25290343\n",
      "Iteration 608, loss = 0.25266183\n",
      "Iteration 609, loss = 0.25242069\n",
      "Iteration 610, loss = 0.25218011\n",
      "Iteration 611, loss = 0.25193992\n",
      "Iteration 612, loss = 0.25170030\n",
      "Iteration 613, loss = 0.25146118\n",
      "Iteration 614, loss = 0.25122259\n",
      "Iteration 615, loss = 0.25098448\n",
      "Iteration 616, loss = 0.25074690\n",
      "Iteration 617, loss = 0.25050978\n",
      "Iteration 618, loss = 0.25027318\n",
      "Iteration 619, loss = 0.25003700\n",
      "Iteration 620, loss = 0.24980135\n",
      "Iteration 621, loss = 0.24956614\n",
      "Iteration 622, loss = 0.24933145\n",
      "Iteration 623, loss = 0.24909725\n",
      "Iteration 624, loss = 0.24886350\n",
      "Iteration 625, loss = 0.24863028\n",
      "Iteration 626, loss = 0.24839749\n",
      "Iteration 627, loss = 0.24816517\n",
      "Iteration 628, loss = 0.24793337\n",
      "Iteration 629, loss = 0.24770197\n",
      "Iteration 630, loss = 0.24747104\n",
      "Iteration 631, loss = 0.24724058\n",
      "Iteration 632, loss = 0.24701058\n",
      "Iteration 633, loss = 0.24678102\n",
      "Iteration 634, loss = 0.24655195\n",
      "Iteration 635, loss = 0.24632332\n",
      "Iteration 636, loss = 0.24609520\n",
      "Iteration 637, loss = 0.24586763\n",
      "Iteration 638, loss = 0.24564050\n",
      "Iteration 639, loss = 0.24541384\n",
      "Iteration 640, loss = 0.24518759\n",
      "Iteration 641, loss = 0.24496187\n",
      "Iteration 642, loss = 0.24473653\n",
      "Iteration 643, loss = 0.24451165\n",
      "Iteration 644, loss = 0.24428723\n",
      "Iteration 645, loss = 0.24406323\n",
      "Iteration 646, loss = 0.24383972\n",
      "Iteration 647, loss = 0.24361660\n",
      "Iteration 648, loss = 0.24339395\n",
      "Iteration 649, loss = 0.24317174\n",
      "Iteration 650, loss = 0.24294994\n",
      "Iteration 651, loss = 0.24272866\n",
      "Iteration 652, loss = 0.24250777\n",
      "Iteration 653, loss = 0.24228737\n",
      "Iteration 654, loss = 0.24206738\n",
      "Iteration 655, loss = 0.24184783\n",
      "Iteration 656, loss = 0.24162875\n",
      "Iteration 657, loss = 0.24141005\n",
      "Iteration 658, loss = 0.24119179\n",
      "Iteration 659, loss = 0.24097398\n",
      "Iteration 660, loss = 0.24075657\n",
      "Iteration 661, loss = 0.24053963\n",
      "Iteration 662, loss = 0.24032312\n",
      "Iteration 663, loss = 0.24010709\n",
      "Iteration 664, loss = 0.23989146\n",
      "Iteration 665, loss = 0.23967622\n",
      "Iteration 666, loss = 0.23946140\n",
      "Iteration 667, loss = 0.23924702\n",
      "Iteration 668, loss = 0.23903302\n",
      "Iteration 669, loss = 0.23881950\n",
      "Iteration 670, loss = 0.23860637\n",
      "Iteration 671, loss = 0.23839367\n",
      "Iteration 672, loss = 0.23818138\n",
      "Iteration 673, loss = 0.23796948\n",
      "Iteration 674, loss = 0.23775804\n",
      "Iteration 675, loss = 0.23754700\n",
      "Iteration 676, loss = 0.23733635\n",
      "Iteration 677, loss = 0.23712614\n",
      "Iteration 678, loss = 0.23691632\n",
      "Iteration 679, loss = 0.23670692\n",
      "Iteration 680, loss = 0.23649794\n",
      "Iteration 681, loss = 0.23628933\n",
      "Iteration 682, loss = 0.23608117\n",
      "Iteration 683, loss = 0.23587339\n",
      "Iteration 684, loss = 0.23566601\n",
      "Iteration 685, loss = 0.23545906\n",
      "Iteration 686, loss = 0.23525253\n",
      "Iteration 687, loss = 0.23504646\n",
      "Iteration 688, loss = 0.23484095\n",
      "Iteration 689, loss = 0.23463585\n",
      "Iteration 690, loss = 0.23443116\n",
      "Iteration 691, loss = 0.23422692\n",
      "Iteration 692, loss = 0.23402308\n",
      "Iteration 693, loss = 0.23381965\n",
      "Iteration 694, loss = 0.23361662\n",
      "Iteration 695, loss = 0.23341399\n",
      "Iteration 696, loss = 0.23321176\n",
      "Iteration 697, loss = 0.23300993\n",
      "Iteration 698, loss = 0.23280850\n",
      "Iteration 699, loss = 0.23260750\n",
      "Iteration 700, loss = 0.23240683\n",
      "Iteration 701, loss = 0.23220660\n",
      "Iteration 702, loss = 0.23200674\n",
      "Iteration 703, loss = 0.23180726\n",
      "Iteration 704, loss = 0.23160821\n",
      "Iteration 705, loss = 0.23140950\n",
      "Iteration 706, loss = 0.23121120\n",
      "Iteration 707, loss = 0.23101328\n",
      "Iteration 708, loss = 0.23081574\n",
      "Iteration 709, loss = 0.23061858\n",
      "Iteration 710, loss = 0.23042180\n",
      "Iteration 711, loss = 0.23022543\n",
      "Iteration 712, loss = 0.23002940\n",
      "Iteration 713, loss = 0.22983377\n",
      "Iteration 714, loss = 0.22963852\n",
      "Iteration 715, loss = 0.22944365\n",
      "Iteration 716, loss = 0.22924920\n",
      "Iteration 717, loss = 0.22905513\n",
      "Iteration 718, loss = 0.22886144\n",
      "Iteration 719, loss = 0.22866814\n",
      "Iteration 720, loss = 0.22847524\n",
      "Iteration 721, loss = 0.22828271\n",
      "Iteration 722, loss = 0.22809056\n",
      "Iteration 723, loss = 0.22789879\n",
      "Iteration 724, loss = 0.22770737\n",
      "Iteration 725, loss = 0.22751633\n",
      "Iteration 726, loss = 0.22732568\n",
      "Iteration 727, loss = 0.22713539\n",
      "Iteration 728, loss = 0.22694550\n",
      "Iteration 729, loss = 0.22675596\n",
      "Iteration 730, loss = 0.22656675\n",
      "Iteration 731, loss = 0.22637790\n",
      "Iteration 732, loss = 0.22618941\n",
      "Iteration 733, loss = 0.22600127\n",
      "Iteration 734, loss = 0.22581350\n",
      "Iteration 735, loss = 0.22562608\n",
      "Iteration 736, loss = 0.22543902\n",
      "Iteration 737, loss = 0.22525232\n",
      "Iteration 738, loss = 0.22506599\n",
      "Iteration 739, loss = 0.22488000\n",
      "Iteration 740, loss = 0.22469436\n",
      "Iteration 741, loss = 0.22450909\n",
      "Iteration 742, loss = 0.22432416\n",
      "Iteration 743, loss = 0.22413959\n",
      "Iteration 744, loss = 0.22395538\n",
      "Iteration 745, loss = 0.22377153\n",
      "Iteration 746, loss = 0.22358803\n",
      "Iteration 747, loss = 0.22340488\n",
      "Iteration 748, loss = 0.22322209\n",
      "Iteration 749, loss = 0.22303963\n",
      "Iteration 750, loss = 0.22285754\n",
      "Iteration 751, loss = 0.22267580\n",
      "Iteration 752, loss = 0.22249439\n",
      "Iteration 753, loss = 0.22231328\n",
      "Iteration 754, loss = 0.22213250\n",
      "Iteration 755, loss = 0.22195208\n",
      "Iteration 756, loss = 0.22177198\n",
      "Iteration 757, loss = 0.22159223\n",
      "Iteration 758, loss = 0.22141281\n",
      "Iteration 759, loss = 0.22123374\n",
      "Iteration 760, loss = 0.22105500\n",
      "Iteration 761, loss = 0.22087660\n",
      "Iteration 762, loss = 0.22069854\n",
      "Iteration 763, loss = 0.22052082\n",
      "Iteration 764, loss = 0.22034342\n",
      "Iteration 765, loss = 0.22016637\n",
      "Iteration 766, loss = 0.21998966\n",
      "Iteration 767, loss = 0.21981328\n",
      "Iteration 768, loss = 0.21963724\n",
      "Iteration 769, loss = 0.21946155\n",
      "Iteration 770, loss = 0.21928619\n",
      "Iteration 771, loss = 0.21911117\n",
      "Iteration 772, loss = 0.21893649\n",
      "Iteration 773, loss = 0.21876216\n",
      "Iteration 774, loss = 0.21858824\n",
      "Iteration 775, loss = 0.21841464\n",
      "Iteration 776, loss = 0.21824137\n",
      "Iteration 777, loss = 0.21806838\n",
      "Iteration 778, loss = 0.21789571\n",
      "Iteration 779, loss = 0.21772338\n",
      "Iteration 780, loss = 0.21755138\n",
      "Iteration 781, loss = 0.21737970\n",
      "Iteration 782, loss = 0.21720834\n",
      "Iteration 783, loss = 0.21703730\n",
      "Iteration 784, loss = 0.21686658\n",
      "Iteration 785, loss = 0.21669617\n",
      "Iteration 786, loss = 0.21652609\n",
      "Iteration 787, loss = 0.21635632\n",
      "Iteration 788, loss = 0.21618688\n",
      "Iteration 789, loss = 0.21601775\n",
      "Iteration 790, loss = 0.21584894\n",
      "Iteration 791, loss = 0.21568045\n",
      "Iteration 792, loss = 0.21551227\n",
      "Iteration 793, loss = 0.21534441\n",
      "Iteration 794, loss = 0.21517687\n",
      "Iteration 795, loss = 0.21500964\n",
      "Iteration 796, loss = 0.21484273\n",
      "Iteration 797, loss = 0.21467613\n",
      "Iteration 798, loss = 0.21450985\n",
      "Iteration 799, loss = 0.21434388\n",
      "Iteration 800, loss = 0.21417822\n",
      "Iteration 801, loss = 0.21401288\n",
      "Iteration 802, loss = 0.21384784\n",
      "Iteration 803, loss = 0.21368311\n",
      "Iteration 804, loss = 0.21351871\n",
      "Iteration 805, loss = 0.21335459\n",
      "Iteration 806, loss = 0.21319080\n",
      "Iteration 807, loss = 0.21302731\n",
      "Iteration 808, loss = 0.21286413\n",
      "Iteration 809, loss = 0.21270126\n",
      "Iteration 810, loss = 0.21253870\n",
      "Iteration 811, loss = 0.21237643\n",
      "Iteration 812, loss = 0.21221449\n",
      "Iteration 813, loss = 0.21205284\n",
      "Iteration 814, loss = 0.21189149\n",
      "Iteration 815, loss = 0.21173044\n",
      "Iteration 816, loss = 0.21156972\n",
      "Iteration 817, loss = 0.21140929\n",
      "Iteration 818, loss = 0.21124918\n",
      "Iteration 819, loss = 0.21108936\n",
      "Iteration 820, loss = 0.21092984\n",
      "Iteration 821, loss = 0.21077060\n",
      "Iteration 822, loss = 0.21061163\n",
      "Iteration 823, loss = 0.21045297\n",
      "Iteration 824, loss = 0.21029459\n",
      "Iteration 825, loss = 0.21013651\n",
      "Iteration 826, loss = 0.20997873\n",
      "Iteration 827, loss = 0.20982124\n",
      "Iteration 828, loss = 0.20966405\n",
      "Iteration 829, loss = 0.20950715\n",
      "Iteration 830, loss = 0.20935055\n",
      "Iteration 831, loss = 0.20919424\n",
      "Iteration 832, loss = 0.20903821\n",
      "Iteration 833, loss = 0.20888248\n",
      "Iteration 834, loss = 0.20872704\n",
      "Iteration 835, loss = 0.20857189\n",
      "Iteration 836, loss = 0.20841702\n",
      "Iteration 837, loss = 0.20826244\n",
      "Iteration 838, loss = 0.20810819\n",
      "Iteration 839, loss = 0.20795421\n",
      "Iteration 840, loss = 0.20780051\n",
      "Iteration 841, loss = 0.20764711\n",
      "Iteration 842, loss = 0.20749406\n",
      "Iteration 843, loss = 0.20734130\n",
      "Iteration 844, loss = 0.20718884\n",
      "Iteration 845, loss = 0.20703666\n",
      "Iteration 846, loss = 0.20688476\n",
      "Iteration 847, loss = 0.20673314\n",
      "Iteration 848, loss = 0.20658181\n",
      "Iteration 849, loss = 0.20643078\n",
      "Iteration 850, loss = 0.20628001\n",
      "Iteration 851, loss = 0.20612953\n",
      "Iteration 852, loss = 0.20597934\n",
      "Iteration 853, loss = 0.20582939\n",
      "Iteration 854, loss = 0.20567969\n",
      "Iteration 855, loss = 0.20553027\n",
      "Iteration 856, loss = 0.20538112\n",
      "Iteration 857, loss = 0.20523223\n",
      "Iteration 858, loss = 0.20508362\n",
      "Iteration 859, loss = 0.20493530\n",
      "Iteration 860, loss = 0.20478723\n",
      "Iteration 861, loss = 0.20463944\n",
      "Iteration 862, loss = 0.20449193\n",
      "Iteration 863, loss = 0.20434468\n",
      "Iteration 864, loss = 0.20419770\n",
      "Iteration 865, loss = 0.20405102\n",
      "Iteration 866, loss = 0.20390459\n",
      "Iteration 867, loss = 0.20375844\n",
      "Iteration 868, loss = 0.20361257\n",
      "Iteration 869, loss = 0.20346697\n",
      "Iteration 870, loss = 0.20332164\n",
      "Iteration 871, loss = 0.20317657\n",
      "Iteration 872, loss = 0.20303178\n",
      "Iteration 873, loss = 0.20288724\n",
      "Iteration 874, loss = 0.20274299\n",
      "Iteration 875, loss = 0.20259898\n",
      "Iteration 876, loss = 0.20245525\n",
      "Iteration 877, loss = 0.20231179\n",
      "Iteration 878, loss = 0.20216858\n",
      "Iteration 879, loss = 0.20202565\n",
      "Iteration 880, loss = 0.20188291\n",
      "Iteration 881, loss = 0.20174043\n",
      "Iteration 882, loss = 0.20159820\n",
      "Iteration 883, loss = 0.20145623\n",
      "Iteration 884, loss = 0.20131452\n",
      "Iteration 885, loss = 0.20117307\n",
      "Iteration 886, loss = 0.20103186\n",
      "Iteration 887, loss = 0.20089092\n",
      "Iteration 888, loss = 0.20075024\n",
      "Iteration 889, loss = 0.20060981\n",
      "Iteration 890, loss = 0.20046964\n",
      "Iteration 891, loss = 0.20032971\n",
      "Iteration 892, loss = 0.20019005\n",
      "Iteration 893, loss = 0.20005064\n",
      "Iteration 894, loss = 0.19991148\n",
      "Iteration 895, loss = 0.19977258\n",
      "Iteration 896, loss = 0.19963392\n",
      "Iteration 897, loss = 0.19949553\n",
      "Iteration 898, loss = 0.19935738\n",
      "Iteration 899, loss = 0.19921948\n",
      "Iteration 900, loss = 0.19908185\n",
      "Iteration 901, loss = 0.19894444\n",
      "Iteration 902, loss = 0.19880726\n",
      "Iteration 903, loss = 0.19867022\n",
      "Iteration 904, loss = 0.19853343\n",
      "Iteration 905, loss = 0.19839690\n",
      "Iteration 906, loss = 0.19826067\n",
      "Iteration 907, loss = 0.19812469\n",
      "Iteration 908, loss = 0.19798884\n",
      "Iteration 909, loss = 0.19785318\n",
      "Iteration 910, loss = 0.19771773\n",
      "Iteration 911, loss = 0.19758254\n",
      "Iteration 912, loss = 0.19744759\n",
      "Iteration 913, loss = 0.19731288\n",
      "Iteration 914, loss = 0.19717840\n",
      "Iteration 915, loss = 0.19704417\n",
      "Iteration 916, loss = 0.19691017\n",
      "Iteration 917, loss = 0.19677642\n",
      "Iteration 918, loss = 0.19664292\n",
      "Iteration 919, loss = 0.19650967\n",
      "Iteration 920, loss = 0.19637666\n",
      "Iteration 921, loss = 0.19624388\n",
      "Iteration 922, loss = 0.19611134\n",
      "Iteration 923, loss = 0.19597896\n",
      "Iteration 924, loss = 0.19584676\n",
      "Iteration 925, loss = 0.19571479\n",
      "Iteration 926, loss = 0.19558305\n",
      "Iteration 927, loss = 0.19545156\n",
      "Iteration 928, loss = 0.19532068\n",
      "Iteration 929, loss = 0.19519005\n",
      "Iteration 930, loss = 0.19505965\n",
      "Iteration 931, loss = 0.19492949\n",
      "Iteration 932, loss = 0.19479955\n",
      "Iteration 933, loss = 0.19466985\n",
      "Iteration 934, loss = 0.19454037\n",
      "Iteration 935, loss = 0.19441114\n",
      "Iteration 936, loss = 0.19428213\n",
      "Iteration 937, loss = 0.19415334\n",
      "Iteration 938, loss = 0.19402480\n",
      "Iteration 939, loss = 0.19389647\n",
      "Iteration 940, loss = 0.19376838\n",
      "Iteration 941, loss = 0.19364051\n",
      "Iteration 942, loss = 0.19351287\n",
      "Iteration 943, loss = 0.19338548\n",
      "Iteration 944, loss = 0.19325847\n",
      "Iteration 945, loss = 0.19313170\n",
      "Iteration 946, loss = 0.19300516\n",
      "Iteration 947, loss = 0.19287896\n",
      "Iteration 948, loss = 0.19275311\n",
      "Iteration 949, loss = 0.19262750\n",
      "Iteration 950, loss = 0.19250212\n",
      "Iteration 951, loss = 0.19237699\n",
      "Iteration 952, loss = 0.19225211\n",
      "Iteration 953, loss = 0.19212746\n",
      "Iteration 954, loss = 0.19200304\n",
      "Iteration 955, loss = 0.19187883\n",
      "Iteration 956, loss = 0.19175480\n",
      "Iteration 957, loss = 0.19163095\n",
      "Iteration 958, loss = 0.19150733\n",
      "Iteration 959, loss = 0.19138390\n",
      "Iteration 960, loss = 0.19126070\n",
      "Iteration 961, loss = 0.19113771\n",
      "Iteration 962, loss = 0.19101493\n",
      "Iteration 963, loss = 0.19089235\n",
      "Iteration 964, loss = 0.19077001\n",
      "Iteration 965, loss = 0.19064794\n",
      "Iteration 966, loss = 0.19052609\n",
      "Iteration 967, loss = 0.19040444\n",
      "Iteration 968, loss = 0.19028299\n",
      "Iteration 969, loss = 0.19016175\n",
      "Iteration 970, loss = 0.19004075\n",
      "Iteration 971, loss = 0.18991996\n",
      "Iteration 972, loss = 0.18979940\n",
      "Iteration 973, loss = 0.18967904\n",
      "Iteration 974, loss = 0.18955889\n",
      "Iteration 975, loss = 0.18943896\n",
      "Iteration 976, loss = 0.18931924\n",
      "Iteration 977, loss = 0.18919974\n",
      "Iteration 978, loss = 0.18908045\n",
      "Iteration 979, loss = 0.18896136\n",
      "Iteration 980, loss = 0.18884250\n",
      "Iteration 981, loss = 0.18872383\n",
      "Iteration 982, loss = 0.18860538\n",
      "Iteration 983, loss = 0.18848715\n",
      "Iteration 984, loss = 0.18836911\n",
      "Iteration 985, loss = 0.18825129\n",
      "Iteration 986, loss = 0.18813367\n",
      "Iteration 987, loss = 0.18801627\n",
      "Iteration 988, loss = 0.18789907\n",
      "Iteration 989, loss = 0.18778207\n",
      "Iteration 990, loss = 0.18766529\n",
      "Iteration 991, loss = 0.18754871\n",
      "Iteration 992, loss = 0.18743236\n",
      "Iteration 993, loss = 0.18731621\n",
      "Iteration 994, loss = 0.18720027\n",
      "Iteration 995, loss = 0.18708453\n",
      "Iteration 996, loss = 0.18696898\n",
      "Iteration 997, loss = 0.18685367\n",
      "Iteration 998, loss = 0.18673854\n",
      "Iteration 999, loss = 0.18662363\n",
      "Iteration 1000, loss = 0.18650891\n",
      "Iteration 1, loss = 1.47705517\n",
      "Iteration 2, loss = 1.46378490\n",
      "Iteration 3, loss = 1.44514590\n",
      "Iteration 4, loss = 1.42199086\n",
      "Iteration 5, loss = 1.39519945\n",
      "Iteration 6, loss = 1.36565324\n",
      "Iteration 7, loss = 1.33418575\n",
      "Iteration 8, loss = 1.30176684\n",
      "Iteration 9, loss = 1.26927787\n",
      "Iteration 10, loss = 1.23773357\n",
      "Iteration 11, loss = 1.20769911\n",
      "Iteration 12, loss = 1.17951770\n",
      "Iteration 13, loss = 1.15366073\n",
      "Iteration 14, loss = 1.13024606\n",
      "Iteration 15, loss = 1.10941162\n",
      "Iteration 16, loss = 1.09115060\n",
      "Iteration 17, loss = 1.07497767\n",
      "Iteration 18, loss = 1.06031179\n",
      "Iteration 19, loss = 1.04705854\n",
      "Iteration 20, loss = 1.03492705\n",
      "Iteration 21, loss = 1.02338747\n",
      "Iteration 22, loss = 1.01198774\n",
      "Iteration 23, loss = 1.00046999\n",
      "Iteration 24, loss = 0.98885846\n",
      "Iteration 25, loss = 0.97712599\n",
      "Iteration 26, loss = 0.96531045\n",
      "Iteration 27, loss = 0.95340782\n",
      "Iteration 28, loss = 0.94151458\n",
      "Iteration 29, loss = 0.92982323\n",
      "Iteration 30, loss = 0.91830689\n",
      "Iteration 31, loss = 0.90699844\n",
      "Iteration 32, loss = 0.89598272\n",
      "Iteration 33, loss = 0.88526090\n",
      "Iteration 34, loss = 0.87491600\n",
      "Iteration 35, loss = 0.86492664\n",
      "Iteration 36, loss = 0.85526085\n",
      "Iteration 37, loss = 0.84593614\n",
      "Iteration 38, loss = 0.83698085\n",
      "Iteration 39, loss = 0.82829155\n",
      "Iteration 40, loss = 0.81980507\n",
      "Iteration 41, loss = 0.81152836\n",
      "Iteration 42, loss = 0.80347114\n",
      "Iteration 43, loss = 0.79572433\n",
      "Iteration 44, loss = 0.78828516\n",
      "Iteration 45, loss = 0.78107172\n",
      "Iteration 46, loss = 0.77411473\n",
      "Iteration 47, loss = 0.76741768\n",
      "Iteration 48, loss = 0.76088529\n",
      "Iteration 49, loss = 0.75452093\n",
      "Iteration 50, loss = 0.74836537\n",
      "Iteration 51, loss = 0.74238997\n",
      "Iteration 52, loss = 0.73658476\n",
      "Iteration 53, loss = 0.73093386\n",
      "Iteration 54, loss = 0.72540346\n",
      "Iteration 55, loss = 0.71998162\n",
      "Iteration 56, loss = 0.71467252\n",
      "Iteration 57, loss = 0.70947979\n",
      "Iteration 58, loss = 0.70443749\n",
      "Iteration 59, loss = 0.69954384\n",
      "Iteration 60, loss = 0.69480555\n",
      "Iteration 61, loss = 0.69021244\n",
      "Iteration 62, loss = 0.68574865\n",
      "Iteration 63, loss = 0.68137200\n",
      "Iteration 64, loss = 0.67708366\n",
      "Iteration 65, loss = 0.67287560\n",
      "Iteration 66, loss = 0.66874710\n",
      "Iteration 67, loss = 0.66469599\n",
      "Iteration 68, loss = 0.66071892\n",
      "Iteration 69, loss = 0.65681359\n",
      "Iteration 70, loss = 0.65298225\n",
      "Iteration 71, loss = 0.64921776\n",
      "Iteration 72, loss = 0.64551761\n",
      "Iteration 73, loss = 0.64188409\n",
      "Iteration 74, loss = 0.63831867\n",
      "Iteration 75, loss = 0.63481501\n",
      "Iteration 76, loss = 0.63137642\n",
      "Iteration 77, loss = 0.62799885\n",
      "Iteration 78, loss = 0.62467929\n",
      "Iteration 79, loss = 0.62141375\n",
      "Iteration 80, loss = 0.61820553\n",
      "Iteration 81, loss = 0.61505523\n",
      "Iteration 82, loss = 0.61195412\n",
      "Iteration 83, loss = 0.60890072\n",
      "Iteration 84, loss = 0.60589579\n",
      "Iteration 85, loss = 0.60293784\n",
      "Iteration 86, loss = 0.60003122\n",
      "Iteration 87, loss = 0.59717143\n",
      "Iteration 88, loss = 0.59435225\n",
      "Iteration 89, loss = 0.59157320\n",
      "Iteration 90, loss = 0.58883258\n",
      "Iteration 91, loss = 0.58612751\n",
      "Iteration 92, loss = 0.58345961\n",
      "Iteration 93, loss = 0.58083487\n",
      "Iteration 94, loss = 0.57824354\n",
      "Iteration 95, loss = 0.57568143\n",
      "Iteration 96, loss = 0.57314659\n",
      "Iteration 97, loss = 0.57064984\n",
      "Iteration 98, loss = 0.56817761\n",
      "Iteration 99, loss = 0.56572663\n",
      "Iteration 100, loss = 0.56330742\n",
      "Iteration 101, loss = 0.56092647\n",
      "Iteration 102, loss = 0.55857503\n",
      "Iteration 103, loss = 0.55625519\n",
      "Iteration 104, loss = 0.55395752\n",
      "Iteration 105, loss = 0.55169741\n",
      "Iteration 106, loss = 0.54949913\n",
      "Iteration 107, loss = 0.54733431\n",
      "Iteration 108, loss = 0.54519219\n",
      "Iteration 109, loss = 0.54309259\n",
      "Iteration 110, loss = 0.54102482\n",
      "Iteration 111, loss = 0.53900706\n",
      "Iteration 112, loss = 0.53703187\n",
      "Iteration 113, loss = 0.53508658\n",
      "Iteration 114, loss = 0.53316175\n",
      "Iteration 115, loss = 0.53126736\n",
      "Iteration 116, loss = 0.52939451\n",
      "Iteration 117, loss = 0.52754393\n",
      "Iteration 118, loss = 0.52573015\n",
      "Iteration 119, loss = 0.52393578\n",
      "Iteration 120, loss = 0.52215977\n",
      "Iteration 121, loss = 0.52040166\n",
      "Iteration 122, loss = 0.51866167\n",
      "Iteration 123, loss = 0.51693899\n",
      "Iteration 124, loss = 0.51523531\n",
      "Iteration 125, loss = 0.51355111\n",
      "Iteration 126, loss = 0.51188678\n",
      "Iteration 127, loss = 0.51024677\n",
      "Iteration 128, loss = 0.50863020\n",
      "Iteration 129, loss = 0.50703349\n",
      "Iteration 130, loss = 0.50545497\n",
      "Iteration 131, loss = 0.50389367\n",
      "Iteration 132, loss = 0.50234920\n",
      "Iteration 133, loss = 0.50082120\n",
      "Iteration 134, loss = 0.49930951\n",
      "Iteration 135, loss = 0.49781396\n",
      "Iteration 136, loss = 0.49633469\n",
      "Iteration 137, loss = 0.49487407\n",
      "Iteration 138, loss = 0.49343025\n",
      "Iteration 139, loss = 0.49200206\n",
      "Iteration 140, loss = 0.49058897\n",
      "Iteration 141, loss = 0.48919046\n",
      "Iteration 142, loss = 0.48780657\n",
      "Iteration 143, loss = 0.48643679\n",
      "Iteration 144, loss = 0.48508080\n",
      "Iteration 145, loss = 0.48373840\n",
      "Iteration 146, loss = 0.48240935\n",
      "Iteration 147, loss = 0.48109339\n",
      "Iteration 148, loss = 0.47979079\n",
      "Iteration 149, loss = 0.47850090\n",
      "Iteration 150, loss = 0.47722336\n",
      "Iteration 151, loss = 0.47595791\n",
      "Iteration 152, loss = 0.47470503\n",
      "Iteration 153, loss = 0.47346448\n",
      "Iteration 154, loss = 0.47223553\n",
      "Iteration 155, loss = 0.47101795\n",
      "Iteration 156, loss = 0.46981184\n",
      "Iteration 157, loss = 0.46861720\n",
      "Iteration 158, loss = 0.46743343\n",
      "Iteration 159, loss = 0.46626020\n",
      "Iteration 160, loss = 0.46509748\n",
      "Iteration 161, loss = 0.46394495\n",
      "Iteration 162, loss = 0.46280273\n",
      "Iteration 163, loss = 0.46167041\n",
      "Iteration 164, loss = 0.46054781\n",
      "Iteration 165, loss = 0.45943474\n",
      "Iteration 166, loss = 0.45833105\n",
      "Iteration 167, loss = 0.45723668\n",
      "Iteration 168, loss = 0.45615149\n",
      "Iteration 169, loss = 0.45507529\n",
      "Iteration 170, loss = 0.45400842\n",
      "Iteration 171, loss = 0.45295036\n",
      "Iteration 172, loss = 0.45190093\n",
      "Iteration 173, loss = 0.45085976\n",
      "Iteration 174, loss = 0.44982673\n",
      "Iteration 175, loss = 0.44880190\n",
      "Iteration 176, loss = 0.44778506\n",
      "Iteration 177, loss = 0.44677620\n",
      "Iteration 178, loss = 0.44577467\n",
      "Iteration 179, loss = 0.44478010\n",
      "Iteration 180, loss = 0.44379342\n",
      "Iteration 181, loss = 0.44281388\n",
      "Iteration 182, loss = 0.44184122\n",
      "Iteration 183, loss = 0.44087579\n",
      "Iteration 184, loss = 0.43991782\n",
      "Iteration 185, loss = 0.43896668\n",
      "Iteration 186, loss = 0.43802227\n",
      "Iteration 187, loss = 0.43708500\n",
      "Iteration 188, loss = 0.43615429\n",
      "Iteration 189, loss = 0.43523008\n",
      "Iteration 190, loss = 0.43431235\n",
      "Iteration 191, loss = 0.43340073\n",
      "Iteration 192, loss = 0.43249540\n",
      "Iteration 193, loss = 0.43159623\n",
      "Iteration 194, loss = 0.43070313\n",
      "Iteration 195, loss = 0.42981596\n",
      "Iteration 196, loss = 0.42893462\n",
      "Iteration 197, loss = 0.42805937\n",
      "Iteration 198, loss = 0.42718996\n",
      "Iteration 199, loss = 0.42632614\n",
      "Iteration 200, loss = 0.42546781\n",
      "Iteration 201, loss = 0.42461491\n",
      "Iteration 202, loss = 0.42376733\n",
      "Iteration 203, loss = 0.42292500\n",
      "Iteration 204, loss = 0.42208783\n",
      "Iteration 205, loss = 0.42125575\n",
      "Iteration 206, loss = 0.42042867\n",
      "Iteration 207, loss = 0.41960656\n",
      "Iteration 208, loss = 0.41878945\n",
      "Iteration 209, loss = 0.41797708\n",
      "Iteration 210, loss = 0.41716939\n",
      "Iteration 211, loss = 0.41636631\n",
      "Iteration 212, loss = 0.41556777\n",
      "Iteration 213, loss = 0.41477382\n",
      "Iteration 214, loss = 0.41398523\n",
      "Iteration 215, loss = 0.41320088\n",
      "Iteration 216, loss = 0.41242085\n",
      "Iteration 217, loss = 0.41164511\n",
      "Iteration 218, loss = 0.41087404\n",
      "Iteration 219, loss = 0.41010715\n",
      "Iteration 220, loss = 0.40934446\n",
      "Iteration 221, loss = 0.40858593\n",
      "Iteration 222, loss = 0.40783137\n",
      "Iteration 223, loss = 0.40708086\n",
      "Iteration 224, loss = 0.40633432\n",
      "Iteration 225, loss = 0.40559187\n",
      "Iteration 226, loss = 0.40485343\n",
      "Iteration 227, loss = 0.40411869\n",
      "Iteration 228, loss = 0.40338794\n",
      "Iteration 229, loss = 0.40266113\n",
      "Iteration 230, loss = 0.40193831\n",
      "Iteration 231, loss = 0.40121918\n",
      "Iteration 232, loss = 0.40050378\n",
      "Iteration 233, loss = 0.39979195\n",
      "Iteration 234, loss = 0.39908367\n",
      "Iteration 235, loss = 0.39837898\n",
      "Iteration 236, loss = 0.39767769\n",
      "Iteration 237, loss = 0.39697989\n",
      "Iteration 238, loss = 0.39628550\n",
      "Iteration 239, loss = 0.39559450\n",
      "Iteration 240, loss = 0.39490676\n",
      "Iteration 241, loss = 0.39422235\n",
      "Iteration 242, loss = 0.39354117\n",
      "Iteration 243, loss = 0.39286318\n",
      "Iteration 244, loss = 0.39218836\n",
      "Iteration 245, loss = 0.39151658\n",
      "Iteration 246, loss = 0.39084788\n",
      "Iteration 247, loss = 0.39018247\n",
      "Iteration 248, loss = 0.38952013\n",
      "Iteration 249, loss = 0.38886071\n",
      "Iteration 250, loss = 0.38820346\n",
      "Iteration 251, loss = 0.38754910\n",
      "Iteration 252, loss = 0.38689790\n",
      "Iteration 253, loss = 0.38624981\n",
      "Iteration 254, loss = 0.38560448\n",
      "Iteration 255, loss = 0.38496209\n",
      "Iteration 256, loss = 0.38432233\n",
      "Iteration 257, loss = 0.38368533\n",
      "Iteration 258, loss = 0.38305104\n",
      "Iteration 259, loss = 0.38241945\n",
      "Iteration 260, loss = 0.38179044\n",
      "Iteration 261, loss = 0.38116310\n",
      "Iteration 262, loss = 0.38053858\n",
      "Iteration 263, loss = 0.37991606\n",
      "Iteration 264, loss = 0.37929505\n",
      "Iteration 265, loss = 0.37867635\n",
      "Iteration 266, loss = 0.37805988\n",
      "Iteration 267, loss = 0.37744586\n",
      "Iteration 268, loss = 0.37683419\n",
      "Iteration 269, loss = 0.37622490\n",
      "Iteration 270, loss = 0.37561802\n",
      "Iteration 271, loss = 0.37501331\n",
      "Iteration 272, loss = 0.37441100\n",
      "Iteration 273, loss = 0.37381088\n",
      "Iteration 274, loss = 0.37321312\n",
      "Iteration 275, loss = 0.37261691\n",
      "Iteration 276, loss = 0.37202231\n",
      "Iteration 277, loss = 0.37142914\n",
      "Iteration 278, loss = 0.37083791\n",
      "Iteration 279, loss = 0.37024865\n",
      "Iteration 280, loss = 0.36966155\n",
      "Iteration 281, loss = 0.36907638\n",
      "Iteration 282, loss = 0.36849326\n",
      "Iteration 283, loss = 0.36791243\n",
      "Iteration 284, loss = 0.36733329\n",
      "Iteration 285, loss = 0.36675654\n",
      "Iteration 286, loss = 0.36618172\n",
      "Iteration 287, loss = 0.36560900\n",
      "Iteration 288, loss = 0.36503824\n",
      "Iteration 289, loss = 0.36446962\n",
      "Iteration 290, loss = 0.36390308\n",
      "Iteration 291, loss = 0.36333761\n",
      "Iteration 292, loss = 0.36277379\n",
      "Iteration 293, loss = 0.36221194\n",
      "Iteration 294, loss = 0.36165205\n",
      "Iteration 295, loss = 0.36109733\n",
      "Iteration 296, loss = 0.36054411\n",
      "Iteration 297, loss = 0.35999102\n",
      "Iteration 298, loss = 0.35943974\n",
      "Iteration 299, loss = 0.35889039\n",
      "Iteration 300, loss = 0.35834263\n",
      "Iteration 301, loss = 0.35779508\n",
      "Iteration 302, loss = 0.35724755\n",
      "Iteration 303, loss = 0.35669816\n",
      "Iteration 304, loss = 0.35614433\n",
      "Iteration 305, loss = 0.35559011\n",
      "Iteration 306, loss = 0.35503657\n",
      "Iteration 307, loss = 0.35448375\n",
      "Iteration 308, loss = 0.35393144\n",
      "Iteration 309, loss = 0.35337850\n",
      "Iteration 310, loss = 0.35282373\n",
      "Iteration 311, loss = 0.35227089\n",
      "Iteration 312, loss = 0.35171819\n",
      "Iteration 313, loss = 0.35116653\n",
      "Iteration 314, loss = 0.35061535\n",
      "Iteration 315, loss = 0.35006260\n",
      "Iteration 316, loss = 0.34950860\n",
      "Iteration 317, loss = 0.34895547\n",
      "Iteration 318, loss = 0.34840873\n",
      "Iteration 319, loss = 0.34786407\n",
      "Iteration 320, loss = 0.34731906\n",
      "Iteration 321, loss = 0.34677312\n",
      "Iteration 322, loss = 0.34623124\n",
      "Iteration 323, loss = 0.34569474\n",
      "Iteration 324, loss = 0.34516143\n",
      "Iteration 325, loss = 0.34462887\n",
      "Iteration 326, loss = 0.34409750\n",
      "Iteration 327, loss = 0.34356764\n",
      "Iteration 328, loss = 0.34303536\n",
      "Iteration 329, loss = 0.34250196\n",
      "Iteration 330, loss = 0.34196852\n",
      "Iteration 331, loss = 0.34143042\n",
      "Iteration 332, loss = 0.34089163\n",
      "Iteration 333, loss = 0.34034855\n",
      "Iteration 334, loss = 0.33979865\n",
      "Iteration 335, loss = 0.33924330\n",
      "Iteration 336, loss = 0.33868675\n",
      "Iteration 337, loss = 0.33813855\n",
      "Iteration 338, loss = 0.33759602\n",
      "Iteration 339, loss = 0.33704823\n",
      "Iteration 340, loss = 0.33650858\n",
      "Iteration 341, loss = 0.33597376\n",
      "Iteration 342, loss = 0.33545367\n",
      "Iteration 343, loss = 0.33494200\n",
      "Iteration 344, loss = 0.33443852\n",
      "Iteration 345, loss = 0.33393931\n",
      "Iteration 346, loss = 0.33344291\n",
      "Iteration 347, loss = 0.33295281\n",
      "Iteration 348, loss = 0.33247169\n",
      "Iteration 349, loss = 0.33200004\n",
      "Iteration 350, loss = 0.33153599\n",
      "Iteration 351, loss = 0.33107521\n",
      "Iteration 352, loss = 0.33061750\n",
      "Iteration 353, loss = 0.33016309\n",
      "Iteration 354, loss = 0.32971141\n",
      "Iteration 355, loss = 0.32926116\n",
      "Iteration 356, loss = 0.32881232\n",
      "Iteration 357, loss = 0.32836536\n",
      "Iteration 358, loss = 0.32792188\n",
      "Iteration 359, loss = 0.32748008\n",
      "Iteration 360, loss = 0.32703997\n",
      "Iteration 361, loss = 0.32660082\n",
      "Iteration 362, loss = 0.32616259\n",
      "Iteration 363, loss = 0.32572537\n",
      "Iteration 364, loss = 0.32528916\n",
      "Iteration 365, loss = 0.32485400\n",
      "Iteration 366, loss = 0.32441989\n",
      "Iteration 367, loss = 0.32398683\n",
      "Iteration 368, loss = 0.32355483\n",
      "Iteration 369, loss = 0.32312392\n",
      "Iteration 370, loss = 0.32269418\n",
      "Iteration 371, loss = 0.32226577\n",
      "Iteration 372, loss = 0.32183834\n",
      "Iteration 373, loss = 0.32141206\n",
      "Iteration 374, loss = 0.32098683\n",
      "Iteration 375, loss = 0.32056268\n",
      "Iteration 376, loss = 0.32013960\n",
      "Iteration 377, loss = 0.31971760\n",
      "Iteration 378, loss = 0.31929673\n",
      "Iteration 379, loss = 0.31887692\n",
      "Iteration 380, loss = 0.31845819\n",
      "Iteration 381, loss = 0.31804055\n",
      "Iteration 382, loss = 0.31762392\n",
      "Iteration 383, loss = 0.31720840\n",
      "Iteration 384, loss = 0.31679389\n",
      "Iteration 385, loss = 0.31638044\n",
      "Iteration 386, loss = 0.31596803\n",
      "Iteration 387, loss = 0.31555668\n",
      "Iteration 388, loss = 0.31514663\n",
      "Iteration 389, loss = 0.31473783\n",
      "Iteration 390, loss = 0.31433005\n",
      "Iteration 391, loss = 0.31392339\n",
      "Iteration 392, loss = 0.31351785\n",
      "Iteration 393, loss = 0.31311334\n",
      "Iteration 394, loss = 0.31270986\n",
      "Iteration 395, loss = 0.31230739\n",
      "Iteration 396, loss = 0.31190608\n",
      "Iteration 397, loss = 0.31150600\n",
      "Iteration 398, loss = 0.31110701\n",
      "Iteration 399, loss = 0.31070892\n",
      "Iteration 400, loss = 0.31031173\n",
      "Iteration 401, loss = 0.30991546\n",
      "Iteration 402, loss = 0.30952014\n",
      "Iteration 403, loss = 0.30912572\n",
      "Iteration 404, loss = 0.30873235\n",
      "Iteration 405, loss = 0.30834002\n",
      "Iteration 406, loss = 0.30794865\n",
      "Iteration 407, loss = 0.30755821\n",
      "Iteration 408, loss = 0.30716873\n",
      "Iteration 409, loss = 0.30678015\n",
      "Iteration 410, loss = 0.30639254\n",
      "Iteration 411, loss = 0.30600631\n",
      "Iteration 412, loss = 0.30562091\n",
      "Iteration 413, loss = 0.30523641\n",
      "Iteration 414, loss = 0.30485289\n",
      "Iteration 415, loss = 0.30446988\n",
      "Iteration 416, loss = 0.30408809\n",
      "Iteration 417, loss = 0.30370731\n",
      "Iteration 418, loss = 0.30332724\n",
      "Iteration 419, loss = 0.30294808\n",
      "Iteration 420, loss = 0.30256977\n",
      "Iteration 421, loss = 0.30219235\n",
      "Iteration 422, loss = 0.30181604\n",
      "Iteration 423, loss = 0.30144061\n",
      "Iteration 424, loss = 0.30106604\n",
      "Iteration 425, loss = 0.30069233\n",
      "Iteration 426, loss = 0.30031944\n",
      "Iteration 427, loss = 0.29994741\n",
      "Iteration 428, loss = 0.29957627\n",
      "Iteration 429, loss = 0.29920607\n",
      "Iteration 430, loss = 0.29883672\n",
      "Iteration 431, loss = 0.29846845\n",
      "Iteration 432, loss = 0.29810102\n",
      "Iteration 433, loss = 0.29773442\n",
      "Iteration 434, loss = 0.29736878\n",
      "Iteration 435, loss = 0.29700384\n",
      "Iteration 436, loss = 0.29663989\n",
      "Iteration 437, loss = 0.29627676\n",
      "Iteration 438, loss = 0.29591444\n",
      "Iteration 439, loss = 0.29555301\n",
      "Iteration 440, loss = 0.29519233\n",
      "Iteration 441, loss = 0.29483252\n",
      "Iteration 442, loss = 0.29447355\n",
      "Iteration 443, loss = 0.29411536\n",
      "Iteration 444, loss = 0.29375804\n",
      "Iteration 445, loss = 0.29340149\n",
      "Iteration 446, loss = 0.29304576\n",
      "Iteration 447, loss = 0.29269097\n",
      "Iteration 448, loss = 0.29233682\n",
      "Iteration 449, loss = 0.29198355\n",
      "Iteration 450, loss = 0.29163111\n",
      "Iteration 451, loss = 0.29127944\n",
      "Iteration 452, loss = 0.29092856\n",
      "Iteration 453, loss = 0.29057848\n",
      "Iteration 454, loss = 0.29022920\n",
      "Iteration 455, loss = 0.28988067\n",
      "Iteration 456, loss = 0.28953295\n",
      "Iteration 457, loss = 0.28918601\n",
      "Iteration 458, loss = 0.28883983\n",
      "Iteration 459, loss = 0.28849442\n",
      "Iteration 460, loss = 0.28814981\n",
      "Iteration 461, loss = 0.28780595\n",
      "Iteration 462, loss = 0.28746281\n",
      "Iteration 463, loss = 0.28712047\n",
      "Iteration 464, loss = 0.28677889\n",
      "Iteration 465, loss = 0.28643809\n",
      "Iteration 466, loss = 0.28609798\n",
      "Iteration 467, loss = 0.28575866\n",
      "Iteration 468, loss = 0.28542008\n",
      "Iteration 469, loss = 0.28508244\n",
      "Iteration 470, loss = 0.28474550\n",
      "Iteration 471, loss = 0.28440936\n",
      "Iteration 472, loss = 0.28407401\n",
      "Iteration 473, loss = 0.28373936\n",
      "Iteration 474, loss = 0.28340552\n",
      "Iteration 475, loss = 0.28307233\n",
      "Iteration 476, loss = 0.28274007\n",
      "Iteration 477, loss = 0.28240847\n",
      "Iteration 478, loss = 0.28207772\n",
      "Iteration 479, loss = 0.28174773\n",
      "Iteration 480, loss = 0.28141853\n",
      "Iteration 481, loss = 0.28109008\n",
      "Iteration 482, loss = 0.28076236\n",
      "Iteration 483, loss = 0.28043536\n",
      "Iteration 484, loss = 0.28010911\n",
      "Iteration 485, loss = 0.27978357\n",
      "Iteration 486, loss = 0.27945874\n",
      "Iteration 487, loss = 0.27913463\n",
      "Iteration 488, loss = 0.27881123\n",
      "Iteration 489, loss = 0.27848856\n",
      "Iteration 490, loss = 0.27816657\n",
      "Iteration 491, loss = 0.27784530\n",
      "Iteration 492, loss = 0.27752473\n",
      "Iteration 493, loss = 0.27720486\n",
      "Iteration 494, loss = 0.27688569\n",
      "Iteration 495, loss = 0.27656724\n",
      "Iteration 496, loss = 0.27624946\n",
      "Iteration 497, loss = 0.27593236\n",
      "Iteration 498, loss = 0.27561596\n",
      "Iteration 499, loss = 0.27530023\n",
      "Iteration 500, loss = 0.27498519\n",
      "Iteration 501, loss = 0.27467086\n",
      "Iteration 502, loss = 0.27435716\n",
      "Iteration 503, loss = 0.27404414\n",
      "Iteration 504, loss = 0.27373180\n",
      "Iteration 505, loss = 0.27342025\n",
      "Iteration 506, loss = 0.27310941\n",
      "Iteration 507, loss = 0.27279924\n",
      "Iteration 508, loss = 0.27248973\n",
      "Iteration 509, loss = 0.27218091\n",
      "Iteration 510, loss = 0.27187272\n",
      "Iteration 511, loss = 0.27156522\n",
      "Iteration 512, loss = 0.27125837\n",
      "Iteration 513, loss = 0.27095219\n",
      "Iteration 514, loss = 0.27064666\n",
      "Iteration 515, loss = 0.27034177\n",
      "Iteration 516, loss = 0.27003755\n",
      "Iteration 517, loss = 0.26973396\n",
      "Iteration 518, loss = 0.26943103\n",
      "Iteration 519, loss = 0.26912874\n",
      "Iteration 520, loss = 0.26882709\n",
      "Iteration 521, loss = 0.26852610\n",
      "Iteration 522, loss = 0.26822571\n",
      "Iteration 523, loss = 0.26792600\n",
      "Iteration 524, loss = 0.26762689\n",
      "Iteration 525, loss = 0.26732842\n",
      "Iteration 526, loss = 0.26703059\n",
      "Iteration 527, loss = 0.26673337\n",
      "Iteration 528, loss = 0.26643678\n",
      "Iteration 529, loss = 0.26614097\n",
      "Iteration 530, loss = 0.26584590\n",
      "Iteration 531, loss = 0.26555144\n",
      "Iteration 532, loss = 0.26525768\n",
      "Iteration 533, loss = 0.26496460\n",
      "Iteration 534, loss = 0.26467217\n",
      "Iteration 535, loss = 0.26438035\n",
      "Iteration 536, loss = 0.26408916\n",
      "Iteration 537, loss = 0.26379859\n",
      "Iteration 538, loss = 0.26350866\n",
      "Iteration 539, loss = 0.26321932\n",
      "Iteration 540, loss = 0.26293060\n",
      "Iteration 541, loss = 0.26264248\n",
      "Iteration 542, loss = 0.26235496\n",
      "Iteration 543, loss = 0.26206804\n",
      "Iteration 544, loss = 0.26178173\n",
      "Iteration 545, loss = 0.26149601\n",
      "Iteration 546, loss = 0.26121089\n",
      "Iteration 547, loss = 0.26092639\n",
      "Iteration 548, loss = 0.26064261\n",
      "Iteration 549, loss = 0.26035945\n",
      "Iteration 550, loss = 0.26007689\n",
      "Iteration 551, loss = 0.25979495\n",
      "Iteration 552, loss = 0.25951358\n",
      "Iteration 553, loss = 0.25923282\n",
      "Iteration 554, loss = 0.25895266\n",
      "Iteration 555, loss = 0.25867308\n",
      "Iteration 556, loss = 0.25839407\n",
      "Iteration 557, loss = 0.25811568\n",
      "Iteration 558, loss = 0.25783783\n",
      "Iteration 559, loss = 0.25756060\n",
      "Iteration 560, loss = 0.25728399\n",
      "Iteration 561, loss = 0.25700799\n",
      "Iteration 562, loss = 0.25673253\n",
      "Iteration 563, loss = 0.25645767\n",
      "Iteration 564, loss = 0.25618336\n",
      "Iteration 565, loss = 0.25590965\n",
      "Iteration 566, loss = 0.25563649\n",
      "Iteration 567, loss = 0.25536391\n",
      "Iteration 568, loss = 0.25509190\n",
      "Iteration 569, loss = 0.25482056\n",
      "Iteration 570, loss = 0.25454979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 571, loss = 0.25427959\n",
      "Iteration 572, loss = 0.25400995\n",
      "Iteration 573, loss = 0.25374087\n",
      "Iteration 574, loss = 0.25347236\n",
      "Iteration 575, loss = 0.25320440\n",
      "Iteration 576, loss = 0.25293699\n",
      "Iteration 577, loss = 0.25267019\n",
      "Iteration 578, loss = 0.25240394\n",
      "Iteration 579, loss = 0.25213823\n",
      "Iteration 580, loss = 0.25187307\n",
      "Iteration 581, loss = 0.25160848\n",
      "Iteration 582, loss = 0.25134447\n",
      "Iteration 583, loss = 0.25108101\n",
      "Iteration 584, loss = 0.25081812\n",
      "Iteration 585, loss = 0.25055575\n",
      "Iteration 586, loss = 0.25029394\n",
      "Iteration 587, loss = 0.25003267\n",
      "Iteration 588, loss = 0.24977192\n",
      "Iteration 589, loss = 0.24951171\n",
      "Iteration 590, loss = 0.24925205\n",
      "Iteration 591, loss = 0.24899290\n",
      "Iteration 592, loss = 0.24873432\n",
      "Iteration 593, loss = 0.24847624\n",
      "Iteration 594, loss = 0.24821870\n",
      "Iteration 595, loss = 0.24796168\n",
      "Iteration 596, loss = 0.24770520\n",
      "Iteration 597, loss = 0.24744923\n",
      "Iteration 598, loss = 0.24719380\n",
      "Iteration 599, loss = 0.24693890\n",
      "Iteration 600, loss = 0.24668453\n",
      "Iteration 601, loss = 0.24643068\n",
      "Iteration 602, loss = 0.24617734\n",
      "Iteration 603, loss = 0.24592452\n",
      "Iteration 604, loss = 0.24567222\n",
      "Iteration 605, loss = 0.24542043\n",
      "Iteration 606, loss = 0.24516916\n",
      "Iteration 607, loss = 0.24491841\n",
      "Iteration 608, loss = 0.24466816\n",
      "Iteration 609, loss = 0.24441842\n",
      "Iteration 610, loss = 0.24416919\n",
      "Iteration 611, loss = 0.24392046\n",
      "Iteration 612, loss = 0.24367224\n",
      "Iteration 613, loss = 0.24342463\n",
      "Iteration 614, loss = 0.24317775\n",
      "Iteration 615, loss = 0.24293140\n",
      "Iteration 616, loss = 0.24268555\n",
      "Iteration 617, loss = 0.24244021\n",
      "Iteration 618, loss = 0.24219538\n",
      "Iteration 619, loss = 0.24195123\n",
      "Iteration 620, loss = 0.24170756\n",
      "Iteration 621, loss = 0.24146439\n",
      "Iteration 622, loss = 0.24122170\n",
      "Iteration 623, loss = 0.24097950\n",
      "Iteration 624, loss = 0.24073793\n",
      "Iteration 625, loss = 0.24049681\n",
      "Iteration 626, loss = 0.24025611\n",
      "Iteration 627, loss = 0.24001585\n",
      "Iteration 628, loss = 0.23977631\n",
      "Iteration 629, loss = 0.23953723\n",
      "Iteration 630, loss = 0.23929867\n",
      "Iteration 631, loss = 0.23906057\n",
      "Iteration 632, loss = 0.23882292\n",
      "Iteration 633, loss = 0.23858577\n",
      "Iteration 634, loss = 0.23834908\n",
      "Iteration 635, loss = 0.23811295\n",
      "Iteration 636, loss = 0.23787726\n",
      "Iteration 637, loss = 0.23764198\n",
      "Iteration 638, loss = 0.23740715\n",
      "Iteration 639, loss = 0.23717278\n",
      "Iteration 640, loss = 0.23693899\n",
      "Iteration 641, loss = 0.23670554\n",
      "Iteration 642, loss = 0.23647260\n",
      "Iteration 643, loss = 0.23624016\n",
      "Iteration 644, loss = 0.23600820\n",
      "Iteration 645, loss = 0.23577673\n",
      "Iteration 646, loss = 0.23554574\n",
      "Iteration 647, loss = 0.23531520\n",
      "Iteration 648, loss = 0.23508513\n",
      "Iteration 649, loss = 0.23485559\n",
      "Iteration 650, loss = 0.23462646\n",
      "Iteration 651, loss = 0.23439779\n",
      "Iteration 652, loss = 0.23416967\n",
      "Iteration 653, loss = 0.23394188\n",
      "Iteration 654, loss = 0.23371461\n",
      "Iteration 655, loss = 0.23348790\n",
      "Iteration 656, loss = 0.23326156\n",
      "Iteration 657, loss = 0.23303569\n",
      "Iteration 658, loss = 0.23281028\n",
      "Iteration 659, loss = 0.23258539\n",
      "Iteration 660, loss = 0.23236086\n",
      "Iteration 661, loss = 0.23213682\n",
      "Iteration 662, loss = 0.23191325\n",
      "Iteration 663, loss = 0.23169012\n",
      "Iteration 664, loss = 0.23146745\n",
      "Iteration 665, loss = 0.23124520\n",
      "Iteration 666, loss = 0.23102340\n",
      "Iteration 667, loss = 0.23080209\n",
      "Iteration 668, loss = 0.23058120\n",
      "Iteration 669, loss = 0.23036073\n",
      "Iteration 670, loss = 0.23014075\n",
      "Iteration 671, loss = 0.22992118\n",
      "Iteration 672, loss = 0.22970205\n",
      "Iteration 673, loss = 0.22948337\n",
      "Iteration 674, loss = 0.22926515\n",
      "Iteration 675, loss = 0.22904732\n",
      "Iteration 676, loss = 0.22882994\n",
      "Iteration 677, loss = 0.22861302\n",
      "Iteration 678, loss = 0.22839649\n",
      "Iteration 679, loss = 0.22818039\n",
      "Iteration 680, loss = 0.22796477\n",
      "Iteration 681, loss = 0.22774959\n",
      "Iteration 682, loss = 0.22753481\n",
      "Iteration 683, loss = 0.22732047\n",
      "Iteration 684, loss = 0.22710662\n",
      "Iteration 685, loss = 0.22689315\n",
      "Iteration 686, loss = 0.22668008\n",
      "Iteration 687, loss = 0.22646748\n",
      "Iteration 688, loss = 0.22625533\n",
      "Iteration 689, loss = 0.22604360\n",
      "Iteration 690, loss = 0.22583231\n",
      "Iteration 691, loss = 0.22562145\n",
      "Iteration 692, loss = 0.22541100\n",
      "Iteration 693, loss = 0.22520096\n",
      "Iteration 694, loss = 0.22499140\n",
      "Iteration 695, loss = 0.22478225\n",
      "Iteration 696, loss = 0.22457352\n",
      "Iteration 697, loss = 0.22436529\n",
      "Iteration 698, loss = 0.22415740\n",
      "Iteration 699, loss = 0.22394995\n",
      "Iteration 700, loss = 0.22374295\n",
      "Iteration 701, loss = 0.22353635\n",
      "Iteration 702, loss = 0.22333017\n",
      "Iteration 703, loss = 0.22312439\n",
      "Iteration 704, loss = 0.22291905\n",
      "Iteration 705, loss = 0.22271410\n",
      "Iteration 706, loss = 0.22250958\n",
      "Iteration 707, loss = 0.22230550\n",
      "Iteration 708, loss = 0.22210177\n",
      "Iteration 709, loss = 0.22189848\n",
      "Iteration 710, loss = 0.22169566\n",
      "Iteration 711, loss = 0.22149312\n",
      "Iteration 712, loss = 0.22129109\n",
      "Iteration 713, loss = 0.22108945\n",
      "Iteration 714, loss = 0.22088819\n",
      "Iteration 715, loss = 0.22068735\n",
      "Iteration 716, loss = 0.22048697\n",
      "Iteration 717, loss = 0.22028687\n",
      "Iteration 718, loss = 0.22008726\n",
      "Iteration 719, loss = 0.21988804\n",
      "Iteration 720, loss = 0.21968920\n",
      "Iteration 721, loss = 0.21949074\n",
      "Iteration 722, loss = 0.21929271\n",
      "Iteration 723, loss = 0.21909506\n",
      "Iteration 724, loss = 0.21889778\n",
      "Iteration 725, loss = 0.21870097\n",
      "Iteration 726, loss = 0.21850447\n",
      "Iteration 727, loss = 0.21830838\n",
      "Iteration 728, loss = 0.21811273\n",
      "Iteration 729, loss = 0.21791743\n",
      "Iteration 730, loss = 0.21772250\n",
      "Iteration 731, loss = 0.21752799\n",
      "Iteration 732, loss = 0.21733388\n",
      "Iteration 733, loss = 0.21714019\n",
      "Iteration 734, loss = 0.21694697\n",
      "Iteration 735, loss = 0.21675404\n",
      "Iteration 736, loss = 0.21656158\n",
      "Iteration 737, loss = 0.21636951\n",
      "Iteration 738, loss = 0.21617779\n",
      "Iteration 739, loss = 0.21598646\n",
      "Iteration 740, loss = 0.21579554\n",
      "Iteration 741, loss = 0.21560496\n",
      "Iteration 742, loss = 0.21541478\n",
      "Iteration 743, loss = 0.21522500\n",
      "Iteration 744, loss = 0.21503560\n",
      "Iteration 745, loss = 0.21484651\n",
      "Iteration 746, loss = 0.21465792\n",
      "Iteration 747, loss = 0.21446957\n",
      "Iteration 748, loss = 0.21428170\n",
      "Iteration 749, loss = 0.21409417\n",
      "Iteration 750, loss = 0.21390699\n",
      "Iteration 751, loss = 0.21372016\n",
      "Iteration 752, loss = 0.21353380\n",
      "Iteration 753, loss = 0.21334771\n",
      "Iteration 754, loss = 0.21316205\n",
      "Iteration 755, loss = 0.21297672\n",
      "Iteration 756, loss = 0.21279178\n",
      "Iteration 757, loss = 0.21260720\n",
      "Iteration 758, loss = 0.21242298\n",
      "Iteration 759, loss = 0.21223913\n",
      "Iteration 760, loss = 0.21205566\n",
      "Iteration 761, loss = 0.21187253\n",
      "Iteration 762, loss = 0.21168976\n",
      "Iteration 763, loss = 0.21150741\n",
      "Iteration 764, loss = 0.21132534\n",
      "Iteration 765, loss = 0.21114367\n",
      "Iteration 766, loss = 0.21096236\n",
      "Iteration 767, loss = 0.21078139\n",
      "Iteration 768, loss = 0.21060081\n",
      "Iteration 769, loss = 0.21042055\n",
      "Iteration 770, loss = 0.21024068\n",
      "Iteration 771, loss = 0.21006118\n",
      "Iteration 772, loss = 0.20988202\n",
      "Iteration 773, loss = 0.20970321\n",
      "Iteration 774, loss = 0.20952477\n",
      "Iteration 775, loss = 0.20934665\n",
      "Iteration 776, loss = 0.20916896\n",
      "Iteration 777, loss = 0.20899152\n",
      "Iteration 778, loss = 0.20881450\n",
      "Iteration 779, loss = 0.20863780\n",
      "Iteration 780, loss = 0.20846145\n",
      "Iteration 781, loss = 0.20828545\n",
      "Iteration 782, loss = 0.20810981\n",
      "Iteration 783, loss = 0.20793448\n",
      "Iteration 784, loss = 0.20775956\n",
      "Iteration 785, loss = 0.20758491\n",
      "Iteration 786, loss = 0.20741067\n",
      "Iteration 787, loss = 0.20723676\n",
      "Iteration 788, loss = 0.20706319\n",
      "Iteration 789, loss = 0.20688998\n",
      "Iteration 790, loss = 0.20671711\n",
      "Iteration 791, loss = 0.20654456\n",
      "Iteration 792, loss = 0.20637243\n",
      "Iteration 793, loss = 0.20620052\n",
      "Iteration 794, loss = 0.20602907\n",
      "Iteration 795, loss = 0.20585788\n",
      "Iteration 796, loss = 0.20568702\n",
      "Iteration 797, loss = 0.20551650\n",
      "Iteration 798, loss = 0.20534635\n",
      "Iteration 799, loss = 0.20517648\n",
      "Iteration 800, loss = 0.20500705\n",
      "Iteration 801, loss = 0.20483782\n",
      "Iteration 802, loss = 0.20466903\n",
      "Iteration 803, loss = 0.20450051\n",
      "Iteration 804, loss = 0.20433233\n",
      "Iteration 805, loss = 0.20416448\n",
      "Iteration 806, loss = 0.20399695\n",
      "Iteration 807, loss = 0.20382977\n",
      "Iteration 808, loss = 0.20366290\n",
      "Iteration 809, loss = 0.20349634\n",
      "Iteration 810, loss = 0.20333017\n",
      "Iteration 811, loss = 0.20316425\n",
      "Iteration 812, loss = 0.20299871\n",
      "Iteration 813, loss = 0.20283347\n",
      "Iteration 814, loss = 0.20266857\n",
      "Iteration 815, loss = 0.20250402\n",
      "Iteration 816, loss = 0.20233974\n",
      "Iteration 817, loss = 0.20217585\n",
      "Iteration 818, loss = 0.20201223\n",
      "Iteration 819, loss = 0.20184893\n",
      "Iteration 820, loss = 0.20168599\n",
      "Iteration 821, loss = 0.20152331\n",
      "Iteration 822, loss = 0.20136092\n",
      "Iteration 823, loss = 0.20119890\n",
      "Iteration 824, loss = 0.20103713\n",
      "Iteration 825, loss = 0.20087569\n",
      "Iteration 826, loss = 0.20071458\n",
      "Iteration 827, loss = 0.20055375\n",
      "Iteration 828, loss = 0.20039329\n",
      "Iteration 829, loss = 0.20023305\n",
      "Iteration 830, loss = 0.20007320\n",
      "Iteration 831, loss = 0.19991363\n",
      "Iteration 832, loss = 0.19975433\n",
      "Iteration 833, loss = 0.19959543\n",
      "Iteration 834, loss = 0.19943673\n",
      "Iteration 835, loss = 0.19927842\n",
      "Iteration 836, loss = 0.19912037\n",
      "Iteration 837, loss = 0.19896263\n",
      "Iteration 838, loss = 0.19880524\n",
      "Iteration 839, loss = 0.19864808\n",
      "Iteration 840, loss = 0.19849130\n",
      "Iteration 841, loss = 0.19833478\n",
      "Iteration 842, loss = 0.19817854\n",
      "Iteration 843, loss = 0.19802268\n",
      "Iteration 844, loss = 0.19786703\n",
      "Iteration 845, loss = 0.19771175\n",
      "Iteration 846, loss = 0.19755674\n",
      "Iteration 847, loss = 0.19740205\n",
      "Iteration 848, loss = 0.19724769\n",
      "Iteration 849, loss = 0.19709355\n",
      "Iteration 850, loss = 0.19693981\n",
      "Iteration 851, loss = 0.19678634\n",
      "Iteration 852, loss = 0.19663317\n",
      "Iteration 853, loss = 0.19648036\n",
      "Iteration 854, loss = 0.19632780\n",
      "Iteration 855, loss = 0.19617552\n",
      "Iteration 856, loss = 0.19602362\n",
      "Iteration 857, loss = 0.19587193\n",
      "Iteration 858, loss = 0.19572058\n",
      "Iteration 859, loss = 0.19556951\n",
      "Iteration 860, loss = 0.19541871\n",
      "Iteration 861, loss = 0.19526825\n",
      "Iteration 862, loss = 0.19511804\n",
      "Iteration 863, loss = 0.19496816\n",
      "Iteration 864, loss = 0.19481855\n",
      "Iteration 865, loss = 0.19466929\n",
      "Iteration 866, loss = 0.19452033\n",
      "Iteration 867, loss = 0.19437162\n",
      "Iteration 868, loss = 0.19422322\n",
      "Iteration 869, loss = 0.19407509\n",
      "Iteration 870, loss = 0.19392722\n",
      "Iteration 871, loss = 0.19377970\n",
      "Iteration 872, loss = 0.19363240\n",
      "Iteration 873, loss = 0.19348540\n",
      "Iteration 874, loss = 0.19333870\n",
      "Iteration 875, loss = 0.19319225\n",
      "Iteration 876, loss = 0.19304614\n",
      "Iteration 877, loss = 0.19290024\n",
      "Iteration 878, loss = 0.19275469\n",
      "Iteration 879, loss = 0.19260937\n",
      "Iteration 880, loss = 0.19246433\n",
      "Iteration 881, loss = 0.19231960\n",
      "Iteration 882, loss = 0.19217509\n",
      "Iteration 883, loss = 0.19203093\n",
      "Iteration 884, loss = 0.19188700\n",
      "Iteration 885, loss = 0.19174335\n",
      "Iteration 886, loss = 0.19160000\n",
      "Iteration 887, loss = 0.19145691\n",
      "Iteration 888, loss = 0.19131408\n",
      "Iteration 889, loss = 0.19117155\n",
      "Iteration 890, loss = 0.19102927\n",
      "Iteration 891, loss = 0.19088732\n",
      "Iteration 892, loss = 0.19074557\n",
      "Iteration 893, loss = 0.19060413\n",
      "Iteration 894, loss = 0.19046296\n",
      "Iteration 895, loss = 0.19032202\n",
      "Iteration 896, loss = 0.19018142\n",
      "Iteration 897, loss = 0.19004103\n",
      "Iteration 898, loss = 0.18990098\n",
      "Iteration 899, loss = 0.18976115\n",
      "Iteration 900, loss = 0.18962156\n",
      "Iteration 901, loss = 0.18948232\n",
      "Iteration 902, loss = 0.18934326\n",
      "Iteration 903, loss = 0.18920451\n",
      "Iteration 904, loss = 0.18906600\n",
      "Iteration 905, loss = 0.18892774\n",
      "Iteration 906, loss = 0.18878976\n",
      "Iteration 907, loss = 0.18865200\n",
      "Iteration 908, loss = 0.18851453\n",
      "Iteration 909, loss = 0.18837728\n",
      "Iteration 910, loss = 0.18824034\n",
      "Iteration 911, loss = 0.18810362\n",
      "Iteration 912, loss = 0.18796718\n",
      "Iteration 913, loss = 0.18783099\n",
      "Iteration 914, loss = 0.18769505\n",
      "Iteration 915, loss = 0.18755937\n",
      "Iteration 916, loss = 0.18742396\n",
      "Iteration 917, loss = 0.18728879\n",
      "Iteration 918, loss = 0.18715389\n",
      "Iteration 919, loss = 0.18701923\n",
      "Iteration 920, loss = 0.18688485\n",
      "Iteration 921, loss = 0.18675067\n",
      "Iteration 922, loss = 0.18661681\n",
      "Iteration 923, loss = 0.18648315\n",
      "Iteration 924, loss = 0.18634967\n",
      "Iteration 925, loss = 0.18621640\n",
      "Iteration 926, loss = 0.18608333\n",
      "Iteration 927, loss = 0.18595052\n",
      "Iteration 928, loss = 0.18581793\n",
      "Iteration 929, loss = 0.18568559\n",
      "Iteration 930, loss = 0.18555352\n",
      "Iteration 931, loss = 0.18542163\n",
      "Iteration 932, loss = 0.18529005\n",
      "Iteration 933, loss = 0.18515863\n",
      "Iteration 934, loss = 0.18502754\n",
      "Iteration 935, loss = 0.18489664\n",
      "Iteration 936, loss = 0.18476596\n",
      "Iteration 937, loss = 0.18463560\n",
      "Iteration 938, loss = 0.18450540\n",
      "Iteration 939, loss = 0.18437549\n",
      "Iteration 940, loss = 0.18424579\n",
      "Iteration 941, loss = 0.18411631\n",
      "Iteration 942, loss = 0.18398701\n",
      "Iteration 943, loss = 0.18385794\n",
      "Iteration 944, loss = 0.18372911\n",
      "Iteration 945, loss = 0.18360049\n",
      "Iteration 946, loss = 0.18347215\n",
      "Iteration 947, loss = 0.18334397\n",
      "Iteration 948, loss = 0.18321608\n",
      "Iteration 949, loss = 0.18308840\n",
      "Iteration 950, loss = 0.18296091\n",
      "Iteration 951, loss = 0.18283370\n",
      "Iteration 952, loss = 0.18270664\n",
      "Iteration 953, loss = 0.18257983\n",
      "Iteration 954, loss = 0.18245325\n",
      "Iteration 955, loss = 0.18232675\n",
      "Iteration 956, loss = 0.18220028\n",
      "Iteration 957, loss = 0.18207409\n",
      "Iteration 958, loss = 0.18194802\n",
      "Iteration 959, loss = 0.18182220\n",
      "Iteration 960, loss = 0.18169658\n",
      "Iteration 961, loss = 0.18157114\n",
      "Iteration 962, loss = 0.18144597\n",
      "Iteration 963, loss = 0.18132094\n",
      "Iteration 964, loss = 0.18119620\n",
      "Iteration 965, loss = 0.18107161\n",
      "Iteration 966, loss = 0.18094735\n",
      "Iteration 967, loss = 0.18082346\n",
      "Iteration 968, loss = 0.18069969\n",
      "Iteration 969, loss = 0.18057621\n",
      "Iteration 970, loss = 0.18045289\n",
      "Iteration 971, loss = 0.18032983\n",
      "Iteration 972, loss = 0.18020697\n",
      "Iteration 973, loss = 0.18008436\n",
      "Iteration 974, loss = 0.17996194\n",
      "Iteration 975, loss = 0.17983980\n",
      "Iteration 976, loss = 0.17971784\n",
      "Iteration 977, loss = 0.17959616\n",
      "Iteration 978, loss = 0.17947470\n",
      "Iteration 979, loss = 0.17935339\n",
      "Iteration 980, loss = 0.17923199\n",
      "Iteration 981, loss = 0.17911072\n",
      "Iteration 982, loss = 0.17898969\n",
      "Iteration 983, loss = 0.17886881\n",
      "Iteration 984, loss = 0.17874819\n",
      "Iteration 985, loss = 0.17862771\n",
      "Iteration 986, loss = 0.17850702\n",
      "Iteration 987, loss = 0.17838604\n",
      "Iteration 988, loss = 0.17826518\n",
      "Iteration 989, loss = 0.17814445\n",
      "Iteration 990, loss = 0.17802382\n",
      "Iteration 991, loss = 0.17790340\n",
      "Iteration 992, loss = 0.17778306\n",
      "Iteration 993, loss = 0.17766281\n",
      "Iteration 994, loss = 0.17754255\n",
      "Iteration 995, loss = 0.17742241\n",
      "Iteration 996, loss = 0.17730219\n",
      "Iteration 997, loss = 0.17718185\n",
      "Iteration 998, loss = 0.17706164\n",
      "Iteration 999, loss = 0.17694156\n",
      "Iteration 1000, loss = 0.17682144\n",
      "Iteration 1, loss = 1.48410645\n",
      "Iteration 2, loss = 1.47044523\n",
      "Iteration 3, loss = 1.45126172\n",
      "Iteration 4, loss = 1.42744657\n",
      "Iteration 5, loss = 1.39986533\n",
      "Iteration 6, loss = 1.36941349\n",
      "Iteration 7, loss = 1.33697026\n",
      "Iteration 8, loss = 1.30351419\n",
      "Iteration 9, loss = 1.26994450\n",
      "Iteration 10, loss = 1.23745599\n",
      "Iteration 11, loss = 1.20655649\n",
      "Iteration 12, loss = 1.17753473\n",
      "Iteration 13, loss = 1.15073807\n",
      "Iteration 14, loss = 1.12635486\n",
      "Iteration 15, loss = 1.10464816\n",
      "Iteration 16, loss = 1.08558177\n",
      "Iteration 17, loss = 1.06849209\n",
      "Iteration 18, loss = 1.05329889\n",
      "Iteration 19, loss = 1.03958644\n",
      "Iteration 20, loss = 1.02688454\n",
      "Iteration 21, loss = 1.01474837\n",
      "Iteration 22, loss = 1.00281896\n",
      "Iteration 23, loss = 0.99084484\n",
      "Iteration 24, loss = 0.97875674\n",
      "Iteration 25, loss = 0.96654711\n",
      "Iteration 26, loss = 0.95419554\n",
      "Iteration 27, loss = 0.94172207\n",
      "Iteration 28, loss = 0.92931418\n",
      "Iteration 29, loss = 0.91704159\n",
      "Iteration 30, loss = 0.90499860\n",
      "Iteration 31, loss = 0.89328575\n",
      "Iteration 32, loss = 0.88180968\n",
      "Iteration 33, loss = 0.87060231\n",
      "Iteration 34, loss = 0.85981635\n",
      "Iteration 35, loss = 0.84952670\n",
      "Iteration 36, loss = 0.83966462\n",
      "Iteration 37, loss = 0.83013160\n",
      "Iteration 38, loss = 0.82097497\n",
      "Iteration 39, loss = 0.81211935\n",
      "Iteration 40, loss = 0.80341946\n",
      "Iteration 41, loss = 0.79498465\n",
      "Iteration 42, loss = 0.78685909\n",
      "Iteration 43, loss = 0.77912698\n",
      "Iteration 44, loss = 0.77168587\n",
      "Iteration 45, loss = 0.76454803\n",
      "Iteration 46, loss = 0.75768759\n",
      "Iteration 47, loss = 0.75105299\n",
      "Iteration 48, loss = 0.74458405\n",
      "Iteration 49, loss = 0.73828500\n",
      "Iteration 50, loss = 0.73219850\n",
      "Iteration 51, loss = 0.72627127\n",
      "Iteration 52, loss = 0.72049016\n",
      "Iteration 53, loss = 0.71483656\n",
      "Iteration 54, loss = 0.70929081\n",
      "Iteration 55, loss = 0.70385134\n",
      "Iteration 56, loss = 0.69852853\n",
      "Iteration 57, loss = 0.69332791\n",
      "Iteration 58, loss = 0.68824511\n",
      "Iteration 59, loss = 0.68330438\n",
      "Iteration 60, loss = 0.67849310\n",
      "Iteration 61, loss = 0.67382051\n",
      "Iteration 62, loss = 0.66929240\n",
      "Iteration 63, loss = 0.66489183\n",
      "Iteration 64, loss = 0.66058615\n",
      "Iteration 65, loss = 0.65637297\n",
      "Iteration 66, loss = 0.65224917\n",
      "Iteration 67, loss = 0.64821293\n",
      "Iteration 68, loss = 0.64426451\n",
      "Iteration 69, loss = 0.64039666\n",
      "Iteration 70, loss = 0.63660517\n",
      "Iteration 71, loss = 0.63288073\n",
      "Iteration 72, loss = 0.62921665\n",
      "Iteration 73, loss = 0.62561299\n",
      "Iteration 74, loss = 0.62207065\n",
      "Iteration 75, loss = 0.61859309\n",
      "Iteration 76, loss = 0.61517760\n",
      "Iteration 77, loss = 0.61182525\n",
      "Iteration 78, loss = 0.60853240\n",
      "Iteration 79, loss = 0.60529785\n",
      "Iteration 80, loss = 0.60212420\n",
      "Iteration 81, loss = 0.59900660\n",
      "Iteration 82, loss = 0.59594395\n",
      "Iteration 83, loss = 0.59293413\n",
      "Iteration 84, loss = 0.58997503\n",
      "Iteration 85, loss = 0.58706498\n",
      "Iteration 86, loss = 0.58420285\n",
      "Iteration 87, loss = 0.58138770\n",
      "Iteration 88, loss = 0.57861958\n",
      "Iteration 89, loss = 0.57589885\n",
      "Iteration 90, loss = 0.57322345\n",
      "Iteration 91, loss = 0.57059610\n",
      "Iteration 92, loss = 0.56801265\n",
      "Iteration 93, loss = 0.56547220\n",
      "Iteration 94, loss = 0.56297297\n",
      "Iteration 95, loss = 0.56051064\n",
      "Iteration 96, loss = 0.55808400\n",
      "Iteration 97, loss = 0.55569221\n",
      "Iteration 98, loss = 0.55333546\n",
      "Iteration 99, loss = 0.55101646\n",
      "Iteration 100, loss = 0.54873256\n",
      "Iteration 101, loss = 0.54648201\n",
      "Iteration 102, loss = 0.54426557\n",
      "Iteration 103, loss = 0.54208236\n",
      "Iteration 104, loss = 0.53993119\n",
      "Iteration 105, loss = 0.53781141\n",
      "Iteration 106, loss = 0.53572265\n",
      "Iteration 107, loss = 0.53366244\n",
      "Iteration 108, loss = 0.53162994\n",
      "Iteration 109, loss = 0.52962467\n",
      "Iteration 110, loss = 0.52764433\n",
      "Iteration 111, loss = 0.52568845\n",
      "Iteration 112, loss = 0.52375751\n",
      "Iteration 113, loss = 0.52185170\n",
      "Iteration 114, loss = 0.51996996\n",
      "Iteration 115, loss = 0.51811497\n",
      "Iteration 116, loss = 0.51628244\n",
      "Iteration 117, loss = 0.51446836\n",
      "Iteration 118, loss = 0.51267523\n",
      "Iteration 119, loss = 0.51090070\n",
      "Iteration 120, loss = 0.50914424\n",
      "Iteration 121, loss = 0.50740323\n",
      "Iteration 122, loss = 0.50567426\n",
      "Iteration 123, loss = 0.50395075\n",
      "Iteration 124, loss = 0.50223719\n",
      "Iteration 125, loss = 0.50053170\n",
      "Iteration 126, loss = 0.49883381\n",
      "Iteration 127, loss = 0.49714570\n",
      "Iteration 128, loss = 0.49546733\n",
      "Iteration 129, loss = 0.49378371\n",
      "Iteration 130, loss = 0.49210654\n",
      "Iteration 131, loss = 0.49043681\n",
      "Iteration 132, loss = 0.48878559\n",
      "Iteration 133, loss = 0.48713509\n",
      "Iteration 134, loss = 0.48549645\n",
      "Iteration 135, loss = 0.48390582\n",
      "Iteration 136, loss = 0.48233034\n",
      "Iteration 137, loss = 0.48075760\n",
      "Iteration 138, loss = 0.47921455\n",
      "Iteration 139, loss = 0.47769958\n",
      "Iteration 140, loss = 0.47623291\n",
      "Iteration 141, loss = 0.47479666\n",
      "Iteration 142, loss = 0.47339159\n",
      "Iteration 143, loss = 0.47201892\n",
      "Iteration 144, loss = 0.47068502\n",
      "Iteration 145, loss = 0.46936532\n",
      "Iteration 146, loss = 0.46805401\n",
      "Iteration 147, loss = 0.46675428\n",
      "Iteration 148, loss = 0.46546569\n",
      "Iteration 149, loss = 0.46419005\n",
      "Iteration 150, loss = 0.46292066\n",
      "Iteration 151, loss = 0.46165692\n",
      "Iteration 152, loss = 0.46039955\n",
      "Iteration 153, loss = 0.45914917\n",
      "Iteration 154, loss = 0.45790750\n",
      "Iteration 155, loss = 0.45667977\n",
      "Iteration 156, loss = 0.45546441\n",
      "Iteration 157, loss = 0.45425984\n",
      "Iteration 158, loss = 0.45306647\n",
      "Iteration 159, loss = 0.45188469\n",
      "Iteration 160, loss = 0.45071301\n",
      "Iteration 161, loss = 0.44955129\n",
      "Iteration 162, loss = 0.44839946\n",
      "Iteration 163, loss = 0.44725735\n",
      "Iteration 164, loss = 0.44612517\n",
      "Iteration 165, loss = 0.44500335\n",
      "Iteration 166, loss = 0.44389186\n",
      "Iteration 167, loss = 0.44278947\n",
      "Iteration 168, loss = 0.44169612\n",
      "Iteration 169, loss = 0.44061172\n",
      "Iteration 170, loss = 0.43953598\n",
      "Iteration 171, loss = 0.43846877\n",
      "Iteration 172, loss = 0.43740985\n",
      "Iteration 173, loss = 0.43635903\n",
      "Iteration 174, loss = 0.43531552\n",
      "Iteration 175, loss = 0.43427931\n",
      "Iteration 176, loss = 0.43325027\n",
      "Iteration 177, loss = 0.43222862\n",
      "Iteration 178, loss = 0.43121455\n",
      "Iteration 179, loss = 0.43020765\n",
      "Iteration 180, loss = 0.42920781\n",
      "Iteration 181, loss = 0.42821489\n",
      "Iteration 182, loss = 0.42722877\n",
      "Iteration 183, loss = 0.42624968\n",
      "Iteration 184, loss = 0.42527786\n",
      "Iteration 185, loss = 0.42431262\n",
      "Iteration 186, loss = 0.42335384\n",
      "Iteration 187, loss = 0.42240139\n",
      "Iteration 188, loss = 0.42145523\n",
      "Iteration 189, loss = 0.42051529\n",
      "Iteration 190, loss = 0.41958185\n",
      "Iteration 191, loss = 0.41865438\n",
      "Iteration 192, loss = 0.41773273\n",
      "Iteration 193, loss = 0.41681682\n",
      "Iteration 194, loss = 0.41590672\n",
      "Iteration 195, loss = 0.41500198\n",
      "Iteration 196, loss = 0.41410261\n",
      "Iteration 197, loss = 0.41320892\n",
      "Iteration 198, loss = 0.41232152\n",
      "Iteration 199, loss = 0.41143943\n",
      "Iteration 200, loss = 0.41056257\n",
      "Iteration 201, loss = 0.40969171\n",
      "Iteration 202, loss = 0.40882592\n",
      "Iteration 203, loss = 0.40796549\n",
      "Iteration 204, loss = 0.40711058\n",
      "Iteration 205, loss = 0.40626043\n",
      "Iteration 206, loss = 0.40541511\n",
      "Iteration 207, loss = 0.40457464\n",
      "Iteration 208, loss = 0.40373898\n",
      "Iteration 209, loss = 0.40290805\n",
      "Iteration 210, loss = 0.40208197\n",
      "Iteration 211, loss = 0.40126072\n",
      "Iteration 212, loss = 0.40044419\n",
      "Iteration 213, loss = 0.39963235\n",
      "Iteration 214, loss = 0.39882494\n",
      "Iteration 215, loss = 0.39802208\n",
      "Iteration 216, loss = 0.39722367\n",
      "Iteration 217, loss = 0.39642947\n",
      "Iteration 218, loss = 0.39563968\n",
      "Iteration 219, loss = 0.39485400\n",
      "Iteration 220, loss = 0.39407262\n",
      "Iteration 221, loss = 0.39329536\n",
      "Iteration 222, loss = 0.39252215\n",
      "Iteration 223, loss = 0.39175356\n",
      "Iteration 224, loss = 0.39098888\n",
      "Iteration 225, loss = 0.39022825\n",
      "Iteration 226, loss = 0.38947142\n",
      "Iteration 227, loss = 0.38871851\n",
      "Iteration 228, loss = 0.38796937\n",
      "Iteration 229, loss = 0.38722396\n",
      "Iteration 230, loss = 0.38648222\n",
      "Iteration 231, loss = 0.38574455\n",
      "Iteration 232, loss = 0.38500989\n",
      "Iteration 233, loss = 0.38427825\n",
      "Iteration 234, loss = 0.38355008\n",
      "Iteration 235, loss = 0.38282532\n",
      "Iteration 236, loss = 0.38210399\n",
      "Iteration 237, loss = 0.38138601\n",
      "Iteration 238, loss = 0.38067127\n",
      "Iteration 239, loss = 0.37995984\n",
      "Iteration 240, loss = 0.37925164\n",
      "Iteration 241, loss = 0.37854663\n",
      "Iteration 242, loss = 0.37784471\n",
      "Iteration 243, loss = 0.37714598\n",
      "Iteration 244, loss = 0.37645037\n",
      "Iteration 245, loss = 0.37575795\n",
      "Iteration 246, loss = 0.37506899\n",
      "Iteration 247, loss = 0.37438288\n",
      "Iteration 248, loss = 0.37369983\n",
      "Iteration 249, loss = 0.37301968\n",
      "Iteration 250, loss = 0.37234241\n",
      "Iteration 251, loss = 0.37166808\n",
      "Iteration 252, loss = 0.37099666\n",
      "Iteration 253, loss = 0.37032768\n",
      "Iteration 254, loss = 0.36966066\n",
      "Iteration 255, loss = 0.36899641\n",
      "Iteration 256, loss = 0.36833412\n",
      "Iteration 257, loss = 0.36767383\n",
      "Iteration 258, loss = 0.36701608\n",
      "Iteration 259, loss = 0.36636092\n",
      "Iteration 260, loss = 0.36570818\n",
      "Iteration 261, loss = 0.36505796\n",
      "Iteration 262, loss = 0.36441035\n",
      "Iteration 263, loss = 0.36376502\n",
      "Iteration 264, loss = 0.36312228\n",
      "Iteration 265, loss = 0.36248212\n",
      "Iteration 266, loss = 0.36184478\n",
      "Iteration 267, loss = 0.36120987\n",
      "Iteration 268, loss = 0.36057659\n",
      "Iteration 269, loss = 0.35994503\n",
      "Iteration 270, loss = 0.35931423\n",
      "Iteration 271, loss = 0.35868558\n",
      "Iteration 272, loss = 0.35805765\n",
      "Iteration 273, loss = 0.35743126\n",
      "Iteration 274, loss = 0.35680695\n",
      "Iteration 275, loss = 0.35618474\n",
      "Iteration 276, loss = 0.35556467\n",
      "Iteration 277, loss = 0.35494674\n",
      "Iteration 278, loss = 0.35433077\n",
      "Iteration 279, loss = 0.35371694\n",
      "Iteration 280, loss = 0.35310511\n",
      "Iteration 281, loss = 0.35249537\n",
      "Iteration 282, loss = 0.35188773\n",
      "Iteration 283, loss = 0.35128157\n",
      "Iteration 284, loss = 0.35067679\n",
      "Iteration 285, loss = 0.35007393\n",
      "Iteration 286, loss = 0.34947436\n",
      "Iteration 287, loss = 0.34887934\n",
      "Iteration 288, loss = 0.34828493\n",
      "Iteration 289, loss = 0.34769121\n",
      "Iteration 290, loss = 0.34709932\n",
      "Iteration 291, loss = 0.34650951\n",
      "Iteration 292, loss = 0.34592079\n",
      "Iteration 293, loss = 0.34533317\n",
      "Iteration 294, loss = 0.34474490\n",
      "Iteration 295, loss = 0.34415447\n",
      "Iteration 296, loss = 0.34356395\n",
      "Iteration 297, loss = 0.34297263\n",
      "Iteration 298, loss = 0.34238187\n",
      "Iteration 299, loss = 0.34179169\n",
      "Iteration 300, loss = 0.34119783\n",
      "Iteration 301, loss = 0.34060325\n",
      "Iteration 302, loss = 0.34001172\n",
      "Iteration 303, loss = 0.33941917\n",
      "Iteration 304, loss = 0.33882761\n",
      "Iteration 305, loss = 0.33823616\n",
      "Iteration 306, loss = 0.33764059\n",
      "Iteration 307, loss = 0.33704300\n",
      "Iteration 308, loss = 0.33644602\n",
      "Iteration 309, loss = 0.33584981\n",
      "Iteration 310, loss = 0.33525449\n",
      "Iteration 311, loss = 0.33465914\n",
      "Iteration 312, loss = 0.33406645\n",
      "Iteration 313, loss = 0.33347925\n",
      "Iteration 314, loss = 0.33289546\n",
      "Iteration 315, loss = 0.33231269\n",
      "Iteration 316, loss = 0.33173056\n",
      "Iteration 317, loss = 0.33114958\n",
      "Iteration 318, loss = 0.33056861\n",
      "Iteration 319, loss = 0.32998478\n",
      "Iteration 320, loss = 0.32940143\n",
      "Iteration 321, loss = 0.32881897\n",
      "Iteration 322, loss = 0.32823564\n",
      "Iteration 323, loss = 0.32764960\n",
      "Iteration 324, loss = 0.32706069\n",
      "Iteration 325, loss = 0.32647475\n",
      "Iteration 326, loss = 0.32589825\n",
      "Iteration 327, loss = 0.32532692\n",
      "Iteration 328, loss = 0.32476203\n",
      "Iteration 329, loss = 0.32420264\n",
      "Iteration 330, loss = 0.32364561\n",
      "Iteration 331, loss = 0.32309622\n",
      "Iteration 332, loss = 0.32255068\n",
      "Iteration 333, loss = 0.32201248\n",
      "Iteration 334, loss = 0.32147805\n",
      "Iteration 335, loss = 0.32095002\n",
      "Iteration 336, loss = 0.32042826\n",
      "Iteration 337, loss = 0.31991508\n",
      "Iteration 338, loss = 0.31941052\n",
      "Iteration 339, loss = 0.31891597\n",
      "Iteration 340, loss = 0.31842280\n",
      "Iteration 341, loss = 0.31793300\n",
      "Iteration 342, loss = 0.31744560\n",
      "Iteration 343, loss = 0.31696107\n",
      "Iteration 344, loss = 0.31647809\n",
      "Iteration 345, loss = 0.31599656\n",
      "Iteration 346, loss = 0.31551650\n",
      "Iteration 347, loss = 0.31503861\n",
      "Iteration 348, loss = 0.31456362\n",
      "Iteration 349, loss = 0.31408990\n",
      "Iteration 350, loss = 0.31361777\n",
      "Iteration 351, loss = 0.31314700\n",
      "Iteration 352, loss = 0.31267739\n",
      "Iteration 353, loss = 0.31220867\n",
      "Iteration 354, loss = 0.31174115\n",
      "Iteration 355, loss = 0.31127471\n",
      "Iteration 356, loss = 0.31080950\n",
      "Iteration 357, loss = 0.31034542\n",
      "Iteration 358, loss = 0.30988251\n",
      "Iteration 359, loss = 0.30942070\n",
      "Iteration 360, loss = 0.30896005\n",
      "Iteration 361, loss = 0.30850060\n",
      "Iteration 362, loss = 0.30804225\n",
      "Iteration 363, loss = 0.30758514\n",
      "Iteration 364, loss = 0.30712916\n",
      "Iteration 365, loss = 0.30667435\n",
      "Iteration 366, loss = 0.30622098\n",
      "Iteration 367, loss = 0.30576920\n",
      "Iteration 368, loss = 0.30531872\n",
      "Iteration 369, loss = 0.30486935\n",
      "Iteration 370, loss = 0.30442133\n",
      "Iteration 371, loss = 0.30397440\n",
      "Iteration 372, loss = 0.30352874\n",
      "Iteration 373, loss = 0.30308443\n",
      "Iteration 374, loss = 0.30264151\n",
      "Iteration 375, loss = 0.30219972\n",
      "Iteration 376, loss = 0.30175927\n",
      "Iteration 377, loss = 0.30131994\n",
      "Iteration 378, loss = 0.30088171\n",
      "Iteration 379, loss = 0.30044465\n",
      "Iteration 380, loss = 0.30000859\n",
      "Iteration 381, loss = 0.29957369\n",
      "Iteration 382, loss = 0.29913986\n",
      "Iteration 383, loss = 0.29870710\n",
      "Iteration 384, loss = 0.29827551\n",
      "Iteration 385, loss = 0.29784493\n",
      "Iteration 386, loss = 0.29741539\n",
      "Iteration 387, loss = 0.29698708\n",
      "Iteration 388, loss = 0.29655968\n",
      "Iteration 389, loss = 0.29613342\n",
      "Iteration 390, loss = 0.29570840\n",
      "Iteration 391, loss = 0.29528433\n",
      "Iteration 392, loss = 0.29486141\n",
      "Iteration 393, loss = 0.29443951\n",
      "Iteration 394, loss = 0.29401875\n",
      "Iteration 395, loss = 0.29359897\n",
      "Iteration 396, loss = 0.29318029\n",
      "Iteration 397, loss = 0.29276279\n",
      "Iteration 398, loss = 0.29234631\n",
      "Iteration 399, loss = 0.29193100\n",
      "Iteration 400, loss = 0.29151689\n",
      "Iteration 401, loss = 0.29110394\n",
      "Iteration 402, loss = 0.29069216\n",
      "Iteration 403, loss = 0.29028142\n",
      "Iteration 404, loss = 0.28987171\n",
      "Iteration 405, loss = 0.28946303\n",
      "Iteration 406, loss = 0.28905533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 407, loss = 0.28864867\n",
      "Iteration 408, loss = 0.28824302\n",
      "Iteration 409, loss = 0.28783843\n",
      "Iteration 410, loss = 0.28743471\n",
      "Iteration 411, loss = 0.28703203\n",
      "Iteration 412, loss = 0.28663037\n",
      "Iteration 413, loss = 0.28622963\n",
      "Iteration 414, loss = 0.28582990\n",
      "Iteration 415, loss = 0.28543115\n",
      "Iteration 416, loss = 0.28503337\n",
      "Iteration 417, loss = 0.28463689\n",
      "Iteration 418, loss = 0.28424155\n",
      "Iteration 419, loss = 0.28384706\n",
      "Iteration 420, loss = 0.28345354\n",
      "Iteration 421, loss = 0.28306097\n",
      "Iteration 422, loss = 0.28266927\n",
      "Iteration 423, loss = 0.28227850\n",
      "Iteration 424, loss = 0.28188869\n",
      "Iteration 425, loss = 0.28149972\n",
      "Iteration 426, loss = 0.28111171\n",
      "Iteration 427, loss = 0.28072460\n",
      "Iteration 428, loss = 0.28033840\n",
      "Iteration 429, loss = 0.27995310\n",
      "Iteration 430, loss = 0.27956876\n",
      "Iteration 431, loss = 0.27918520\n",
      "Iteration 432, loss = 0.27880258\n",
      "Iteration 433, loss = 0.27842093\n",
      "Iteration 434, loss = 0.27804001\n",
      "Iteration 435, loss = 0.27766023\n",
      "Iteration 436, loss = 0.27728099\n",
      "Iteration 437, loss = 0.27690297\n",
      "Iteration 438, loss = 0.27652591\n",
      "Iteration 439, loss = 0.27614970\n",
      "Iteration 440, loss = 0.27577435\n",
      "Iteration 441, loss = 0.27539970\n",
      "Iteration 442, loss = 0.27502609\n",
      "Iteration 443, loss = 0.27465355\n",
      "Iteration 444, loss = 0.27428195\n",
      "Iteration 445, loss = 0.27391126\n",
      "Iteration 446, loss = 0.27354142\n",
      "Iteration 447, loss = 0.27317248\n",
      "Iteration 448, loss = 0.27280438\n",
      "Iteration 449, loss = 0.27243716\n",
      "Iteration 450, loss = 0.27207076\n",
      "Iteration 451, loss = 0.27170527\n",
      "Iteration 452, loss = 0.27134060\n",
      "Iteration 453, loss = 0.27097676\n",
      "Iteration 454, loss = 0.27061376\n",
      "Iteration 455, loss = 0.27025159\n",
      "Iteration 456, loss = 0.26989033\n",
      "Iteration 457, loss = 0.26952986\n",
      "Iteration 458, loss = 0.26917019\n",
      "Iteration 459, loss = 0.26881137\n",
      "Iteration 460, loss = 0.26845342\n",
      "Iteration 461, loss = 0.26809624\n",
      "Iteration 462, loss = 0.26773987\n",
      "Iteration 463, loss = 0.26738434\n",
      "Iteration 464, loss = 0.26702961\n",
      "Iteration 465, loss = 0.26667566\n",
      "Iteration 466, loss = 0.26632256\n",
      "Iteration 467, loss = 0.26597026\n",
      "Iteration 468, loss = 0.26561874\n",
      "Iteration 469, loss = 0.26526801\n",
      "Iteration 470, loss = 0.26491808\n",
      "Iteration 471, loss = 0.26456897\n",
      "Iteration 472, loss = 0.26422061\n",
      "Iteration 473, loss = 0.26387306\n",
      "Iteration 474, loss = 0.26352630\n",
      "Iteration 475, loss = 0.26318031\n",
      "Iteration 476, loss = 0.26283510\n",
      "Iteration 477, loss = 0.26249068\n",
      "Iteration 478, loss = 0.26214701\n",
      "Iteration 479, loss = 0.26180414\n",
      "Iteration 480, loss = 0.26146202\n",
      "Iteration 481, loss = 0.26112068\n",
      "Iteration 482, loss = 0.26078008\n",
      "Iteration 483, loss = 0.26044030\n",
      "Iteration 484, loss = 0.26010121\n",
      "Iteration 485, loss = 0.25976295\n",
      "Iteration 486, loss = 0.25942540\n",
      "Iteration 487, loss = 0.25908867\n",
      "Iteration 488, loss = 0.25875269\n",
      "Iteration 489, loss = 0.25841748\n",
      "Iteration 490, loss = 0.25808302\n",
      "Iteration 491, loss = 0.25774932\n",
      "Iteration 492, loss = 0.25741641\n",
      "Iteration 493, loss = 0.25708414\n",
      "Iteration 494, loss = 0.25675266\n",
      "Iteration 495, loss = 0.25642191\n",
      "Iteration 496, loss = 0.25609193\n",
      "Iteration 497, loss = 0.25576262\n",
      "Iteration 498, loss = 0.25543412\n",
      "Iteration 499, loss = 0.25510628\n",
      "Iteration 500, loss = 0.25477922\n",
      "Iteration 501, loss = 0.25445284\n",
      "Iteration 502, loss = 0.25412719\n",
      "Iteration 503, loss = 0.25380225\n",
      "Iteration 504, loss = 0.25347804\n",
      "Iteration 505, loss = 0.25315455\n",
      "Iteration 506, loss = 0.25283176\n",
      "Iteration 507, loss = 0.25250971\n",
      "Iteration 508, loss = 0.25218837\n",
      "Iteration 509, loss = 0.25186770\n",
      "Iteration 510, loss = 0.25154778\n",
      "Iteration 511, loss = 0.25122850\n",
      "Iteration 512, loss = 0.25090994\n",
      "Iteration 513, loss = 0.25059210\n",
      "Iteration 514, loss = 0.25027495\n",
      "Iteration 515, loss = 0.24995853\n",
      "Iteration 516, loss = 0.24964274\n",
      "Iteration 517, loss = 0.24932765\n",
      "Iteration 518, loss = 0.24901325\n",
      "Iteration 519, loss = 0.24869954\n",
      "Iteration 520, loss = 0.24838651\n",
      "Iteration 521, loss = 0.24807417\n",
      "Iteration 522, loss = 0.24776250\n",
      "Iteration 523, loss = 0.24745151\n",
      "Iteration 524, loss = 0.24714130\n",
      "Iteration 525, loss = 0.24683162\n",
      "Iteration 526, loss = 0.24652266\n",
      "Iteration 527, loss = 0.24621462\n",
      "Iteration 528, loss = 0.24590734\n",
      "Iteration 529, loss = 0.24560073\n",
      "Iteration 530, loss = 0.24529479\n",
      "Iteration 531, loss = 0.24498954\n",
      "Iteration 532, loss = 0.24468496\n",
      "Iteration 533, loss = 0.24438106\n",
      "Iteration 534, loss = 0.24407785\n",
      "Iteration 535, loss = 0.24377526\n",
      "Iteration 536, loss = 0.24347337\n",
      "Iteration 537, loss = 0.24317213\n",
      "Iteration 538, loss = 0.24287167\n",
      "Iteration 539, loss = 0.24257202\n",
      "Iteration 540, loss = 0.24227302\n",
      "Iteration 541, loss = 0.24197468\n",
      "Iteration 542, loss = 0.24167708\n",
      "Iteration 543, loss = 0.24138017\n",
      "Iteration 544, loss = 0.24108390\n",
      "Iteration 545, loss = 0.24078829\n",
      "Iteration 546, loss = 0.24049333\n",
      "Iteration 547, loss = 0.24019900\n",
      "Iteration 548, loss = 0.23990532\n",
      "Iteration 549, loss = 0.23961231\n",
      "Iteration 550, loss = 0.23931991\n",
      "Iteration 551, loss = 0.23902815\n",
      "Iteration 552, loss = 0.23873710\n",
      "Iteration 553, loss = 0.23844668\n",
      "Iteration 554, loss = 0.23815687\n",
      "Iteration 555, loss = 0.23786769\n",
      "Iteration 556, loss = 0.23757912\n",
      "Iteration 557, loss = 0.23729119\n",
      "Iteration 558, loss = 0.23700391\n",
      "Iteration 559, loss = 0.23671730\n",
      "Iteration 560, loss = 0.23643136\n",
      "Iteration 561, loss = 0.23614616\n",
      "Iteration 562, loss = 0.23586148\n",
      "Iteration 563, loss = 0.23557749\n",
      "Iteration 564, loss = 0.23529415\n",
      "Iteration 565, loss = 0.23501141\n",
      "Iteration 566, loss = 0.23472930\n",
      "Iteration 567, loss = 0.23444780\n",
      "Iteration 568, loss = 0.23416691\n",
      "Iteration 569, loss = 0.23388663\n",
      "Iteration 570, loss = 0.23360696\n",
      "Iteration 571, loss = 0.23332787\n",
      "Iteration 572, loss = 0.23304943\n",
      "Iteration 573, loss = 0.23277163\n",
      "Iteration 574, loss = 0.23249440\n",
      "Iteration 575, loss = 0.23221780\n",
      "Iteration 576, loss = 0.23194178\n",
      "Iteration 577, loss = 0.23166636\n",
      "Iteration 578, loss = 0.23139152\n",
      "Iteration 579, loss = 0.23111728\n",
      "Iteration 580, loss = 0.23084361\n",
      "Iteration 581, loss = 0.23057057\n",
      "Iteration 582, loss = 0.23029807\n",
      "Iteration 583, loss = 0.23002616\n",
      "Iteration 584, loss = 0.22975483\n",
      "Iteration 585, loss = 0.22948409\n",
      "Iteration 586, loss = 0.22921398\n",
      "Iteration 587, loss = 0.22894452\n",
      "Iteration 588, loss = 0.22867559\n",
      "Iteration 589, loss = 0.22840725\n",
      "Iteration 590, loss = 0.22813949\n",
      "Iteration 591, loss = 0.22787231\n",
      "Iteration 592, loss = 0.22760569\n",
      "Iteration 593, loss = 0.22733965\n",
      "Iteration 594, loss = 0.22707417\n",
      "Iteration 595, loss = 0.22680927\n",
      "Iteration 596, loss = 0.22654490\n",
      "Iteration 597, loss = 0.22628111\n",
      "Iteration 598, loss = 0.22601790\n",
      "Iteration 599, loss = 0.22575533\n",
      "Iteration 600, loss = 0.22549337\n",
      "Iteration 601, loss = 0.22523200\n",
      "Iteration 602, loss = 0.22497118\n",
      "Iteration 603, loss = 0.22471099\n",
      "Iteration 604, loss = 0.22445127\n",
      "Iteration 605, loss = 0.22419212\n",
      "Iteration 606, loss = 0.22393352\n",
      "Iteration 607, loss = 0.22367546\n",
      "Iteration 608, loss = 0.22341802\n",
      "Iteration 609, loss = 0.22316115\n",
      "Iteration 610, loss = 0.22290491\n",
      "Iteration 611, loss = 0.22264921\n",
      "Iteration 612, loss = 0.22239404\n",
      "Iteration 613, loss = 0.22213948\n",
      "Iteration 614, loss = 0.22188537\n",
      "Iteration 615, loss = 0.22163181\n",
      "Iteration 616, loss = 0.22137881\n",
      "Iteration 617, loss = 0.22112637\n",
      "Iteration 618, loss = 0.22087441\n",
      "Iteration 619, loss = 0.22062299\n",
      "Iteration 620, loss = 0.22037210\n",
      "Iteration 621, loss = 0.22012178\n",
      "Iteration 622, loss = 0.21987200\n",
      "Iteration 623, loss = 0.21962274\n",
      "Iteration 624, loss = 0.21937399\n",
      "Iteration 625, loss = 0.21912579\n",
      "Iteration 626, loss = 0.21887809\n",
      "Iteration 627, loss = 0.21863099\n",
      "Iteration 628, loss = 0.21838429\n",
      "Iteration 629, loss = 0.21813820\n",
      "Iteration 630, loss = 0.21789264\n",
      "Iteration 631, loss = 0.21764767\n",
      "Iteration 632, loss = 0.21740322\n",
      "Iteration 633, loss = 0.21715929\n",
      "Iteration 634, loss = 0.21691583\n",
      "Iteration 635, loss = 0.21667290\n",
      "Iteration 636, loss = 0.21643056\n",
      "Iteration 637, loss = 0.21618868\n",
      "Iteration 638, loss = 0.21594731\n",
      "Iteration 639, loss = 0.21570646\n",
      "Iteration 640, loss = 0.21546615\n",
      "Iteration 641, loss = 0.21522631\n",
      "Iteration 642, loss = 0.21498698\n",
      "Iteration 643, loss = 0.21474819\n",
      "Iteration 644, loss = 0.21450998\n",
      "Iteration 645, loss = 0.21427240\n",
      "Iteration 646, loss = 0.21403531\n",
      "Iteration 647, loss = 0.21379879\n",
      "Iteration 648, loss = 0.21356277\n",
      "Iteration 649, loss = 0.21332722\n",
      "Iteration 650, loss = 0.21309218\n",
      "Iteration 651, loss = 0.21285773\n",
      "Iteration 652, loss = 0.21262368\n",
      "Iteration 653, loss = 0.21239013\n",
      "Iteration 654, loss = 0.21215718\n",
      "Iteration 655, loss = 0.21192460\n",
      "Iteration 656, loss = 0.21169256\n",
      "Iteration 657, loss = 0.21146105\n",
      "Iteration 658, loss = 0.21123001\n",
      "Iteration 659, loss = 0.21099943\n",
      "Iteration 660, loss = 0.21076933\n",
      "Iteration 661, loss = 0.21053977\n",
      "Iteration 662, loss = 0.21031066\n",
      "Iteration 663, loss = 0.21008203\n",
      "Iteration 664, loss = 0.20985388\n",
      "Iteration 665, loss = 0.20962626\n",
      "Iteration 666, loss = 0.20939905\n",
      "Iteration 667, loss = 0.20917234\n",
      "Iteration 668, loss = 0.20894615\n",
      "Iteration 669, loss = 0.20872039\n",
      "Iteration 670, loss = 0.20849511\n",
      "Iteration 671, loss = 0.20827030\n",
      "Iteration 672, loss = 0.20804604\n",
      "Iteration 673, loss = 0.20782213\n",
      "Iteration 674, loss = 0.20759874\n",
      "Iteration 675, loss = 0.20737586\n",
      "Iteration 676, loss = 0.20715343\n",
      "Iteration 677, loss = 0.20693143\n",
      "Iteration 678, loss = 0.20670994\n",
      "Iteration 679, loss = 0.20648886\n",
      "Iteration 680, loss = 0.20626833\n",
      "Iteration 681, loss = 0.20604818\n",
      "Iteration 682, loss = 0.20582854\n",
      "Iteration 683, loss = 0.20560943\n",
      "Iteration 684, loss = 0.20539072\n",
      "Iteration 685, loss = 0.20517249\n",
      "Iteration 686, loss = 0.20495471\n",
      "Iteration 687, loss = 0.20473737\n",
      "Iteration 688, loss = 0.20452056\n",
      "Iteration 689, loss = 0.20430412\n",
      "Iteration 690, loss = 0.20408815\n",
      "Iteration 691, loss = 0.20387270\n",
      "Iteration 692, loss = 0.20365762\n",
      "Iteration 693, loss = 0.20344303\n",
      "Iteration 694, loss = 0.20322887\n",
      "Iteration 695, loss = 0.20301517\n",
      "Iteration 696, loss = 0.20280196\n",
      "Iteration 697, loss = 0.20258912\n",
      "Iteration 698, loss = 0.20237676\n",
      "Iteration 699, loss = 0.20216484\n",
      "Iteration 700, loss = 0.20195337\n",
      "Iteration 701, loss = 0.20174239\n",
      "Iteration 702, loss = 0.20153179\n",
      "Iteration 703, loss = 0.20132168\n",
      "Iteration 704, loss = 0.20111201\n",
      "Iteration 705, loss = 0.20090276\n",
      "Iteration 706, loss = 0.20069398\n",
      "Iteration 707, loss = 0.20048560\n",
      "Iteration 708, loss = 0.20027774\n",
      "Iteration 709, loss = 0.20007023\n",
      "Iteration 710, loss = 0.19986317\n",
      "Iteration 711, loss = 0.19965664\n",
      "Iteration 712, loss = 0.19945046\n",
      "Iteration 713, loss = 0.19924473\n",
      "Iteration 714, loss = 0.19903941\n",
      "Iteration 715, loss = 0.19883451\n",
      "Iteration 716, loss = 0.19863010\n",
      "Iteration 717, loss = 0.19842604\n",
      "Iteration 718, loss = 0.19822247\n",
      "Iteration 719, loss = 0.19801929\n",
      "Iteration 720, loss = 0.19781655\n",
      "Iteration 721, loss = 0.19761421\n",
      "Iteration 722, loss = 0.19741229\n",
      "Iteration 723, loss = 0.19721081\n",
      "Iteration 724, loss = 0.19700975\n",
      "Iteration 725, loss = 0.19680913\n",
      "Iteration 726, loss = 0.19660893\n",
      "Iteration 727, loss = 0.19640917\n",
      "Iteration 728, loss = 0.19620986\n",
      "Iteration 729, loss = 0.19601089\n",
      "Iteration 730, loss = 0.19581236\n",
      "Iteration 731, loss = 0.19561429\n",
      "Iteration 732, loss = 0.19541660\n",
      "Iteration 733, loss = 0.19521933\n",
      "Iteration 734, loss = 0.19502248\n",
      "Iteration 735, loss = 0.19482603\n",
      "Iteration 736, loss = 0.19463004\n",
      "Iteration 737, loss = 0.19443437\n",
      "Iteration 738, loss = 0.19423917\n",
      "Iteration 739, loss = 0.19404431\n",
      "Iteration 740, loss = 0.19384994\n",
      "Iteration 741, loss = 0.19365589\n",
      "Iteration 742, loss = 0.19346231\n",
      "Iteration 743, loss = 0.19326907\n",
      "Iteration 744, loss = 0.19307631\n",
      "Iteration 745, loss = 0.19288388\n",
      "Iteration 746, loss = 0.19269187\n",
      "Iteration 747, loss = 0.19250025\n",
      "Iteration 748, loss = 0.19230904\n",
      "Iteration 749, loss = 0.19211825\n",
      "Iteration 750, loss = 0.19192780\n",
      "Iteration 751, loss = 0.19173776\n",
      "Iteration 752, loss = 0.19154812\n",
      "Iteration 753, loss = 0.19135890\n",
      "Iteration 754, loss = 0.19117005\n",
      "Iteration 755, loss = 0.19098156\n",
      "Iteration 756, loss = 0.19079349\n",
      "Iteration 757, loss = 0.19060577\n",
      "Iteration 758, loss = 0.19041853\n",
      "Iteration 759, loss = 0.19023157\n",
      "Iteration 760, loss = 0.19004505\n",
      "Iteration 761, loss = 0.18985889\n",
      "Iteration 762, loss = 0.18967315\n",
      "Iteration 763, loss = 0.18948775\n",
      "Iteration 764, loss = 0.18930278\n",
      "Iteration 765, loss = 0.18911813\n",
      "Iteration 766, loss = 0.18893388\n",
      "Iteration 767, loss = 0.18875008\n",
      "Iteration 768, loss = 0.18856655\n",
      "Iteration 769, loss = 0.18838342\n",
      "Iteration 770, loss = 0.18820074\n",
      "Iteration 771, loss = 0.18801838\n",
      "Iteration 772, loss = 0.18783636\n",
      "Iteration 773, loss = 0.18765472\n",
      "Iteration 774, loss = 0.18747341\n",
      "Iteration 775, loss = 0.18729252\n",
      "Iteration 776, loss = 0.18711202\n",
      "Iteration 777, loss = 0.18693179\n",
      "Iteration 778, loss = 0.18675197\n",
      "Iteration 779, loss = 0.18657259\n",
      "Iteration 780, loss = 0.18639354\n",
      "Iteration 781, loss = 0.18621488\n",
      "Iteration 782, loss = 0.18603657\n",
      "Iteration 783, loss = 0.18585863\n",
      "Iteration 784, loss = 0.18568105\n",
      "Iteration 785, loss = 0.18550386\n",
      "Iteration 786, loss = 0.18532701\n",
      "Iteration 787, loss = 0.18515051\n",
      "Iteration 788, loss = 0.18497440\n",
      "Iteration 789, loss = 0.18479865\n",
      "Iteration 790, loss = 0.18462325\n",
      "Iteration 791, loss = 0.18444820\n",
      "Iteration 792, loss = 0.18427352\n",
      "Iteration 793, loss = 0.18409918\n",
      "Iteration 794, loss = 0.18392525\n",
      "Iteration 795, loss = 0.18375166\n",
      "Iteration 796, loss = 0.18357844\n",
      "Iteration 797, loss = 0.18340562\n",
      "Iteration 798, loss = 0.18323314\n",
      "Iteration 799, loss = 0.18306101\n",
      "Iteration 800, loss = 0.18288926\n",
      "Iteration 801, loss = 0.18271783\n",
      "Iteration 802, loss = 0.18254675\n",
      "Iteration 803, loss = 0.18237605\n",
      "Iteration 804, loss = 0.18220567\n",
      "Iteration 805, loss = 0.18203568\n",
      "Iteration 806, loss = 0.18186597\n",
      "Iteration 807, loss = 0.18169667\n",
      "Iteration 808, loss = 0.18152770\n",
      "Iteration 809, loss = 0.18135906\n",
      "Iteration 810, loss = 0.18119078\n",
      "Iteration 811, loss = 0.18102282\n",
      "Iteration 812, loss = 0.18085526\n",
      "Iteration 813, loss = 0.18068808\n",
      "Iteration 814, loss = 0.18052120\n",
      "Iteration 815, loss = 0.18035468\n",
      "Iteration 816, loss = 0.18018851\n",
      "Iteration 817, loss = 0.18002268\n",
      "Iteration 818, loss = 0.17985720\n",
      "Iteration 819, loss = 0.17969202\n",
      "Iteration 820, loss = 0.17952720\n",
      "Iteration 821, loss = 0.17936273\n",
      "Iteration 822, loss = 0.17919857\n",
      "Iteration 823, loss = 0.17903476\n",
      "Iteration 824, loss = 0.17887128\n",
      "Iteration 825, loss = 0.17870812\n",
      "Iteration 826, loss = 0.17854532\n",
      "Iteration 827, loss = 0.17838285\n",
      "Iteration 828, loss = 0.17822070\n",
      "Iteration 829, loss = 0.17805889\n",
      "Iteration 830, loss = 0.17789741\n",
      "Iteration 831, loss = 0.17773625\n",
      "Iteration 832, loss = 0.17757544\n",
      "Iteration 833, loss = 0.17741492\n",
      "Iteration 834, loss = 0.17725473\n",
      "Iteration 835, loss = 0.17709492\n",
      "Iteration 836, loss = 0.17693538\n",
      "Iteration 837, loss = 0.17677618\n",
      "Iteration 838, loss = 0.17661730\n",
      "Iteration 839, loss = 0.17645873\n",
      "Iteration 840, loss = 0.17630051\n",
      "Iteration 841, loss = 0.17614258\n",
      "Iteration 842, loss = 0.17598498\n",
      "Iteration 843, loss = 0.17582769\n",
      "Iteration 844, loss = 0.17567074\n",
      "Iteration 845, loss = 0.17551409\n",
      "Iteration 846, loss = 0.17535775\n",
      "Iteration 847, loss = 0.17520170\n",
      "Iteration 848, loss = 0.17504601\n",
      "Iteration 849, loss = 0.17489062\n",
      "Iteration 850, loss = 0.17473553\n",
      "Iteration 851, loss = 0.17458074\n",
      "Iteration 852, loss = 0.17442630\n",
      "Iteration 853, loss = 0.17427216\n",
      "Iteration 854, loss = 0.17411831\n",
      "Iteration 855, loss = 0.17396479\n",
      "Iteration 856, loss = 0.17381155\n",
      "Iteration 857, loss = 0.17365864\n",
      "Iteration 858, loss = 0.17350604\n",
      "Iteration 859, loss = 0.17335373\n",
      "Iteration 860, loss = 0.17320174\n",
      "Iteration 861, loss = 0.17305003\n",
      "Iteration 862, loss = 0.17289868\n",
      "Iteration 863, loss = 0.17274757\n",
      "Iteration 864, loss = 0.17259678\n",
      "Iteration 865, loss = 0.17244630\n",
      "Iteration 866, loss = 0.17229614\n",
      "Iteration 867, loss = 0.17214626\n",
      "Iteration 868, loss = 0.17199669\n",
      "Iteration 869, loss = 0.17184740\n",
      "Iteration 870, loss = 0.17169840\n",
      "Iteration 871, loss = 0.17154973\n",
      "Iteration 872, loss = 0.17140135\n",
      "Iteration 873, loss = 0.17125324\n",
      "Iteration 874, loss = 0.17110544\n",
      "Iteration 875, loss = 0.17095796\n",
      "Iteration 876, loss = 0.17081075\n",
      "Iteration 877, loss = 0.17066383\n",
      "Iteration 878, loss = 0.17051720\n",
      "Iteration 879, loss = 0.17037086\n",
      "Iteration 880, loss = 0.17022487\n",
      "Iteration 881, loss = 0.17007909\n",
      "Iteration 882, loss = 0.16993363\n",
      "Iteration 883, loss = 0.16978845\n",
      "Iteration 884, loss = 0.16964360\n",
      "Iteration 885, loss = 0.16949900\n",
      "Iteration 886, loss = 0.16935469\n",
      "Iteration 887, loss = 0.16921066\n",
      "Iteration 888, loss = 0.16906692\n",
      "Iteration 889, loss = 0.16892349\n",
      "Iteration 890, loss = 0.16878034\n",
      "Iteration 891, loss = 0.16863745\n",
      "Iteration 892, loss = 0.16849486\n",
      "Iteration 893, loss = 0.16835252\n",
      "Iteration 894, loss = 0.16821053\n",
      "Iteration 895, loss = 0.16806874\n",
      "Iteration 896, loss = 0.16792727\n",
      "Iteration 897, loss = 0.16778608\n",
      "Iteration 898, loss = 0.16764516\n",
      "Iteration 899, loss = 0.16750452\n",
      "Iteration 900, loss = 0.16736415\n",
      "Iteration 901, loss = 0.16722406\n",
      "Iteration 902, loss = 0.16708424\n",
      "Iteration 903, loss = 0.16694471\n",
      "Iteration 904, loss = 0.16680545\n",
      "Iteration 905, loss = 0.16666645\n",
      "Iteration 906, loss = 0.16652773\n",
      "Iteration 907, loss = 0.16638931\n",
      "Iteration 908, loss = 0.16625113\n",
      "Iteration 909, loss = 0.16611323\n",
      "Iteration 910, loss = 0.16597559\n",
      "Iteration 911, loss = 0.16583826\n",
      "Iteration 912, loss = 0.16570117\n",
      "Iteration 913, loss = 0.16556434\n",
      "Iteration 914, loss = 0.16542777\n",
      "Iteration 915, loss = 0.16529147\n",
      "Iteration 916, loss = 0.16515546\n",
      "Iteration 917, loss = 0.16501970\n",
      "Iteration 918, loss = 0.16488419\n",
      "Iteration 919, loss = 0.16474895\n",
      "Iteration 920, loss = 0.16461397\n",
      "Iteration 921, loss = 0.16447928\n",
      "Iteration 922, loss = 0.16434483\n",
      "Iteration 923, loss = 0.16421064\n",
      "Iteration 924, loss = 0.16407671\n",
      "Iteration 925, loss = 0.16394308\n",
      "Iteration 926, loss = 0.16380966\n",
      "Iteration 927, loss = 0.16367652\n",
      "Iteration 928, loss = 0.16354355\n",
      "Iteration 929, loss = 0.16341079\n",
      "Iteration 930, loss = 0.16327825\n",
      "Iteration 931, loss = 0.16314594\n",
      "Iteration 932, loss = 0.16301388\n",
      "Iteration 933, loss = 0.16288205\n",
      "Iteration 934, loss = 0.16275053\n",
      "Iteration 935, loss = 0.16261919\n",
      "Iteration 936, loss = 0.16248813\n",
      "Iteration 937, loss = 0.16235732\n",
      "Iteration 938, loss = 0.16222677\n",
      "Iteration 939, loss = 0.16209647\n",
      "Iteration 940, loss = 0.16196640\n",
      "Iteration 941, loss = 0.16183658\n",
      "Iteration 942, loss = 0.16170700\n",
      "Iteration 943, loss = 0.16157772\n",
      "Iteration 944, loss = 0.16144862\n",
      "Iteration 945, loss = 0.16131980\n",
      "Iteration 946, loss = 0.16119133\n",
      "Iteration 947, loss = 0.16106338\n",
      "Iteration 948, loss = 0.16093567\n",
      "Iteration 949, loss = 0.16080822\n",
      "Iteration 950, loss = 0.16068100\n",
      "Iteration 951, loss = 0.16055403\n",
      "Iteration 952, loss = 0.16042735\n",
      "Iteration 953, loss = 0.16030085\n",
      "Iteration 954, loss = 0.16017462\n",
      "Iteration 955, loss = 0.16004869\n",
      "Iteration 956, loss = 0.15992293\n",
      "Iteration 957, loss = 0.15979745\n",
      "Iteration 958, loss = 0.15967220\n",
      "Iteration 959, loss = 0.15954719\n",
      "Iteration 960, loss = 0.15942247\n",
      "Iteration 961, loss = 0.15929792\n",
      "Iteration 962, loss = 0.15917363\n",
      "Iteration 963, loss = 0.15904961\n",
      "Iteration 964, loss = 0.15892581\n",
      "Iteration 965, loss = 0.15880225\n",
      "Iteration 966, loss = 0.15867892\n",
      "Iteration 967, loss = 0.15855583\n",
      "Iteration 968, loss = 0.15843301\n",
      "Iteration 969, loss = 0.15831037\n",
      "Iteration 970, loss = 0.15818799\n",
      "Iteration 971, loss = 0.15806586\n",
      "Iteration 972, loss = 0.15794397\n",
      "Iteration 973, loss = 0.15782229\n",
      "Iteration 974, loss = 0.15770085\n",
      "Iteration 975, loss = 0.15757963\n",
      "Iteration 976, loss = 0.15745867\n",
      "Iteration 977, loss = 0.15733792\n",
      "Iteration 978, loss = 0.15721740\n",
      "Iteration 979, loss = 0.15709711\n",
      "Iteration 980, loss = 0.15697709\n",
      "Iteration 981, loss = 0.15685724\n",
      "Iteration 982, loss = 0.15673765\n",
      "Iteration 983, loss = 0.15661828\n",
      "Iteration 984, loss = 0.15649917\n",
      "Iteration 985, loss = 0.15638025\n",
      "Iteration 986, loss = 0.15626156\n",
      "Iteration 987, loss = 0.15614310\n",
      "Iteration 988, loss = 0.15602487\n",
      "Iteration 989, loss = 0.15590687\n",
      "Iteration 990, loss = 0.15578909\n",
      "Iteration 991, loss = 0.15567153\n",
      "Iteration 992, loss = 0.15555423\n",
      "Iteration 993, loss = 0.15543710\n",
      "Iteration 994, loss = 0.15532021\n",
      "Iteration 995, loss = 0.15520354\n",
      "Iteration 996, loss = 0.15508706\n",
      "Iteration 997, loss = 0.15497078\n",
      "Iteration 998, loss = 0.15485470\n",
      "Iteration 999, loss = 0.15473883\n",
      "Iteration 1000, loss = 0.15462321\n",
      "Iteration 1, loss = 1.48086253\n",
      "Iteration 2, loss = 1.46726543\n",
      "Iteration 3, loss = 1.44815339\n",
      "Iteration 4, loss = 1.42442838\n",
      "Iteration 5, loss = 1.39695429\n",
      "Iteration 6, loss = 1.36663728\n",
      "Iteration 7, loss = 1.33439489\n",
      "Iteration 8, loss = 1.30117824\n",
      "Iteration 9, loss = 1.26782317\n",
      "Iteration 10, loss = 1.23552571\n",
      "Iteration 11, loss = 1.20487621\n",
      "Iteration 12, loss = 1.17604309\n",
      "Iteration 13, loss = 1.14957240\n",
      "Iteration 14, loss = 1.12559176\n",
      "Iteration 15, loss = 1.10442006\n",
      "Iteration 16, loss = 1.08594465\n",
      "Iteration 17, loss = 1.06966032\n",
      "Iteration 18, loss = 1.05524199\n",
      "Iteration 19, loss = 1.04218829\n",
      "Iteration 20, loss = 1.03016984\n",
      "Iteration 21, loss = 1.01861807\n",
      "Iteration 22, loss = 1.00713610\n",
      "Iteration 23, loss = 0.99553662\n",
      "Iteration 24, loss = 0.98385790\n",
      "Iteration 25, loss = 0.97207235\n",
      "Iteration 26, loss = 0.96019370\n",
      "Iteration 27, loss = 0.94828825\n",
      "Iteration 28, loss = 0.93642900\n",
      "Iteration 29, loss = 0.92482846\n",
      "Iteration 30, loss = 0.91342053\n",
      "Iteration 31, loss = 0.90224842\n",
      "Iteration 32, loss = 0.89133621\n",
      "Iteration 33, loss = 0.88069594\n",
      "Iteration 34, loss = 0.87038962\n",
      "Iteration 35, loss = 0.86043220\n",
      "Iteration 36, loss = 0.85077349\n",
      "Iteration 37, loss = 0.84146314\n",
      "Iteration 38, loss = 0.83242061\n",
      "Iteration 39, loss = 0.82357661\n",
      "Iteration 40, loss = 0.81486482\n",
      "Iteration 41, loss = 0.80635064\n",
      "Iteration 42, loss = 0.79799501\n",
      "Iteration 43, loss = 0.78994108\n",
      "Iteration 44, loss = 0.78214620\n",
      "Iteration 45, loss = 0.77465344\n",
      "Iteration 46, loss = 0.76740757\n",
      "Iteration 47, loss = 0.76043088\n",
      "Iteration 48, loss = 0.75370128\n",
      "Iteration 49, loss = 0.74720726\n",
      "Iteration 50, loss = 0.74098738\n",
      "Iteration 51, loss = 0.73497066\n",
      "Iteration 52, loss = 0.72913536\n",
      "Iteration 53, loss = 0.72348707\n",
      "Iteration 54, loss = 0.71797754\n",
      "Iteration 55, loss = 0.71260491\n",
      "Iteration 56, loss = 0.70736614\n",
      "Iteration 57, loss = 0.70225014\n",
      "Iteration 58, loss = 0.69724916\n",
      "Iteration 59, loss = 0.69235838\n",
      "Iteration 60, loss = 0.68757301\n",
      "Iteration 61, loss = 0.68289728\n",
      "Iteration 62, loss = 0.67832683\n",
      "Iteration 63, loss = 0.67385433\n",
      "Iteration 64, loss = 0.66947483\n",
      "Iteration 65, loss = 0.66518102\n",
      "Iteration 66, loss = 0.66096980\n",
      "Iteration 67, loss = 0.65684024\n",
      "Iteration 68, loss = 0.65278706\n",
      "Iteration 69, loss = 0.64881102\n",
      "Iteration 70, loss = 0.64491185\n",
      "Iteration 71, loss = 0.64109127\n",
      "Iteration 72, loss = 0.63735051\n",
      "Iteration 73, loss = 0.63367991\n",
      "Iteration 74, loss = 0.63007907\n",
      "Iteration 75, loss = 0.62654530\n",
      "Iteration 76, loss = 0.62307456\n",
      "Iteration 77, loss = 0.61965988\n",
      "Iteration 78, loss = 0.61630148\n",
      "Iteration 79, loss = 0.61299251\n",
      "Iteration 80, loss = 0.60973302\n",
      "Iteration 81, loss = 0.60652640\n",
      "Iteration 82, loss = 0.60336998\n",
      "Iteration 83, loss = 0.60026396\n",
      "Iteration 84, loss = 0.59721660\n",
      "Iteration 85, loss = 0.59421551\n",
      "Iteration 86, loss = 0.59126090\n",
      "Iteration 87, loss = 0.58835359\n",
      "Iteration 88, loss = 0.58549000\n",
      "Iteration 89, loss = 0.58266912\n",
      "Iteration 90, loss = 0.57988720\n",
      "Iteration 91, loss = 0.57714559\n",
      "Iteration 92, loss = 0.57443931\n",
      "Iteration 93, loss = 0.57177569\n",
      "Iteration 94, loss = 0.56915523\n",
      "Iteration 95, loss = 0.56656953\n",
      "Iteration 96, loss = 0.56402858\n",
      "Iteration 97, loss = 0.56153649\n",
      "Iteration 98, loss = 0.55908271\n",
      "Iteration 99, loss = 0.55666270\n",
      "Iteration 100, loss = 0.55427624\n",
      "Iteration 101, loss = 0.55192523\n",
      "Iteration 102, loss = 0.54960995\n",
      "Iteration 103, loss = 0.54732944\n",
      "Iteration 104, loss = 0.54508254\n",
      "Iteration 105, loss = 0.54286797\n",
      "Iteration 106, loss = 0.54068922\n",
      "Iteration 107, loss = 0.53855628\n",
      "Iteration 108, loss = 0.53646766\n",
      "Iteration 109, loss = 0.53441623\n",
      "Iteration 110, loss = 0.53239751\n",
      "Iteration 111, loss = 0.53040769\n",
      "Iteration 112, loss = 0.52844550\n",
      "Iteration 113, loss = 0.52651541\n",
      "Iteration 114, loss = 0.52461000\n",
      "Iteration 115, loss = 0.52272882\n",
      "Iteration 116, loss = 0.52086938\n",
      "Iteration 117, loss = 0.51903036\n",
      "Iteration 118, loss = 0.51721098\n",
      "Iteration 119, loss = 0.51541147\n",
      "Iteration 120, loss = 0.51363299\n",
      "Iteration 121, loss = 0.51187550\n",
      "Iteration 122, loss = 0.51013846\n",
      "Iteration 123, loss = 0.50842100\n",
      "Iteration 124, loss = 0.50672506\n",
      "Iteration 125, loss = 0.50505010\n",
      "Iteration 126, loss = 0.50339646\n",
      "Iteration 127, loss = 0.50176202\n",
      "Iteration 128, loss = 0.50014636\n",
      "Iteration 129, loss = 0.49854916\n",
      "Iteration 130, loss = 0.49696955\n",
      "Iteration 131, loss = 0.49540720\n",
      "Iteration 132, loss = 0.49386174\n",
      "Iteration 133, loss = 0.49233441\n",
      "Iteration 134, loss = 0.49082329\n",
      "Iteration 135, loss = 0.48932821\n",
      "Iteration 136, loss = 0.48784939\n",
      "Iteration 137, loss = 0.48638595\n",
      "Iteration 138, loss = 0.48493764\n",
      "Iteration 139, loss = 0.48350434\n",
      "Iteration 140, loss = 0.48208587\n",
      "Iteration 141, loss = 0.48068158\n",
      "Iteration 142, loss = 0.47929174\n",
      "Iteration 143, loss = 0.47791574\n",
      "Iteration 144, loss = 0.47655308\n",
      "Iteration 145, loss = 0.47520341\n",
      "Iteration 146, loss = 0.47386677\n",
      "Iteration 147, loss = 0.47254255\n",
      "Iteration 148, loss = 0.47123071\n",
      "Iteration 149, loss = 0.46993138\n",
      "Iteration 150, loss = 0.46864404\n",
      "Iteration 151, loss = 0.46736858\n",
      "Iteration 152, loss = 0.46610482\n",
      "Iteration 153, loss = 0.46485239\n",
      "Iteration 154, loss = 0.46361152\n",
      "Iteration 155, loss = 0.46238166\n",
      "Iteration 156, loss = 0.46116259\n",
      "Iteration 157, loss = 0.45995406\n",
      "Iteration 158, loss = 0.45875595\n",
      "Iteration 159, loss = 0.45756794\n",
      "Iteration 160, loss = 0.45638965\n",
      "Iteration 161, loss = 0.45522098\n",
      "Iteration 162, loss = 0.45406190\n",
      "Iteration 163, loss = 0.45291235\n",
      "Iteration 164, loss = 0.45177217\n",
      "Iteration 165, loss = 0.45064136\n",
      "Iteration 166, loss = 0.44951949\n",
      "Iteration 167, loss = 0.44840659\n",
      "Iteration 168, loss = 0.44730218\n",
      "Iteration 169, loss = 0.44620621\n",
      "Iteration 170, loss = 0.44511856\n",
      "Iteration 171, loss = 0.44403919\n",
      "Iteration 172, loss = 0.44296787\n",
      "Iteration 173, loss = 0.44190499\n",
      "Iteration 174, loss = 0.44084985\n",
      "Iteration 175, loss = 0.43980341\n",
      "Iteration 176, loss = 0.43876466\n",
      "Iteration 177, loss = 0.43773367\n",
      "Iteration 178, loss = 0.43671014\n",
      "Iteration 179, loss = 0.43569396\n",
      "Iteration 180, loss = 0.43468503\n",
      "Iteration 181, loss = 0.43368341\n",
      "Iteration 182, loss = 0.43268904\n",
      "Iteration 183, loss = 0.43170166\n",
      "Iteration 184, loss = 0.43072103\n",
      "Iteration 185, loss = 0.42974719\n",
      "Iteration 186, loss = 0.42878005\n",
      "Iteration 187, loss = 0.42781940\n",
      "Iteration 188, loss = 0.42686541\n",
      "Iteration 189, loss = 0.42591744\n",
      "Iteration 190, loss = 0.42497668\n",
      "Iteration 191, loss = 0.42404212\n",
      "Iteration 192, loss = 0.42311410\n",
      "Iteration 193, loss = 0.42219128\n",
      "Iteration 194, loss = 0.42127485\n",
      "Iteration 195, loss = 0.42036421\n",
      "Iteration 196, loss = 0.41945937\n",
      "Iteration 197, loss = 0.41856017\n",
      "Iteration 198, loss = 0.41766663\n",
      "Iteration 199, loss = 0.41677856\n",
      "Iteration 200, loss = 0.41589589\n",
      "Iteration 201, loss = 0.41501869\n",
      "Iteration 202, loss = 0.41414663\n",
      "Iteration 203, loss = 0.41327991\n",
      "Iteration 204, loss = 0.41241833\n",
      "Iteration 205, loss = 0.41156178\n",
      "Iteration 206, loss = 0.41071031\n",
      "Iteration 207, loss = 0.40986370\n",
      "Iteration 208, loss = 0.40902193\n",
      "Iteration 209, loss = 0.40818500\n",
      "Iteration 210, loss = 0.40735276\n",
      "Iteration 211, loss = 0.40652523\n",
      "Iteration 212, loss = 0.40570217\n",
      "Iteration 213, loss = 0.40488385\n",
      "Iteration 214, loss = 0.40406984\n",
      "Iteration 215, loss = 0.40326030\n",
      "Iteration 216, loss = 0.40245526\n",
      "Iteration 217, loss = 0.40165449\n",
      "Iteration 218, loss = 0.40085794\n",
      "Iteration 219, loss = 0.40006568\n",
      "Iteration 220, loss = 0.39927764\n",
      "Iteration 221, loss = 0.39849381\n",
      "Iteration 222, loss = 0.39771409\n",
      "Iteration 223, loss = 0.39693821\n",
      "Iteration 224, loss = 0.39616634\n",
      "Iteration 225, loss = 0.39539846\n",
      "Iteration 226, loss = 0.39463476\n",
      "Iteration 227, loss = 0.39387514\n",
      "Iteration 228, loss = 0.39311948\n",
      "Iteration 229, loss = 0.39236809\n",
      "Iteration 230, loss = 0.39162052\n",
      "Iteration 231, loss = 0.39087707\n",
      "Iteration 232, loss = 0.39013675\n",
      "Iteration 233, loss = 0.38940041\n",
      "Iteration 234, loss = 0.38866727\n",
      "Iteration 235, loss = 0.38793783\n",
      "Iteration 236, loss = 0.38721171\n",
      "Iteration 237, loss = 0.38648914\n",
      "Iteration 238, loss = 0.38577006\n",
      "Iteration 239, loss = 0.38505430\n",
      "Iteration 240, loss = 0.38434190\n",
      "Iteration 241, loss = 0.38363282\n",
      "Iteration 242, loss = 0.38292691\n",
      "Iteration 243, loss = 0.38222425\n",
      "Iteration 244, loss = 0.38152485\n",
      "Iteration 245, loss = 0.38082854\n",
      "Iteration 246, loss = 0.38013535\n",
      "Iteration 247, loss = 0.37944530\n",
      "Iteration 248, loss = 0.37875840\n",
      "Iteration 249, loss = 0.37807442\n",
      "Iteration 250, loss = 0.37739338\n",
      "Iteration 251, loss = 0.37671547\n",
      "Iteration 252, loss = 0.37604023\n",
      "Iteration 253, loss = 0.37536793\n",
      "Iteration 254, loss = 0.37469786\n",
      "Iteration 255, loss = 0.37403058\n",
      "Iteration 256, loss = 0.37336610\n",
      "Iteration 257, loss = 0.37270426\n",
      "Iteration 258, loss = 0.37204521\n",
      "Iteration 259, loss = 0.37138876\n",
      "Iteration 260, loss = 0.37073511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 261, loss = 0.37008396\n",
      "Iteration 262, loss = 0.36943558\n",
      "Iteration 263, loss = 0.36878970\n",
      "Iteration 264, loss = 0.36814636\n",
      "Iteration 265, loss = 0.36750558\n",
      "Iteration 266, loss = 0.36686755\n",
      "Iteration 267, loss = 0.36623185\n",
      "Iteration 268, loss = 0.36559869\n",
      "Iteration 269, loss = 0.36496739\n",
      "Iteration 270, loss = 0.36433817\n",
      "Iteration 271, loss = 0.36371136\n",
      "Iteration 272, loss = 0.36308682\n",
      "Iteration 273, loss = 0.36246460\n",
      "Iteration 274, loss = 0.36184473\n",
      "Iteration 275, loss = 0.36122715\n",
      "Iteration 276, loss = 0.36061184\n",
      "Iteration 277, loss = 0.35999884\n",
      "Iteration 278, loss = 0.35938810\n",
      "Iteration 279, loss = 0.35877952\n",
      "Iteration 280, loss = 0.35817327\n",
      "Iteration 281, loss = 0.35756905\n",
      "Iteration 282, loss = 0.35696593\n",
      "Iteration 283, loss = 0.35636492\n",
      "Iteration 284, loss = 0.35576567\n",
      "Iteration 285, loss = 0.35516763\n",
      "Iteration 286, loss = 0.35457155\n",
      "Iteration 287, loss = 0.35397734\n",
      "Iteration 288, loss = 0.35338523\n",
      "Iteration 289, loss = 0.35279505\n",
      "Iteration 290, loss = 0.35220679\n",
      "Iteration 291, loss = 0.35162053\n",
      "Iteration 292, loss = 0.35103620\n",
      "Iteration 293, loss = 0.35045388\n",
      "Iteration 294, loss = 0.34987350\n",
      "Iteration 295, loss = 0.34929508\n",
      "Iteration 296, loss = 0.34871854\n",
      "Iteration 297, loss = 0.34814349\n",
      "Iteration 298, loss = 0.34756948\n",
      "Iteration 299, loss = 0.34699734\n",
      "Iteration 300, loss = 0.34642638\n",
      "Iteration 301, loss = 0.34585564\n",
      "Iteration 302, loss = 0.34528561\n",
      "Iteration 303, loss = 0.34471708\n",
      "Iteration 304, loss = 0.34415017\n",
      "Iteration 305, loss = 0.34358482\n",
      "Iteration 306, loss = 0.34302112\n",
      "Iteration 307, loss = 0.34245909\n",
      "Iteration 308, loss = 0.34189870\n",
      "Iteration 309, loss = 0.34134003\n",
      "Iteration 310, loss = 0.34078300\n",
      "Iteration 311, loss = 0.34022770\n",
      "Iteration 312, loss = 0.33967410\n",
      "Iteration 313, loss = 0.33912216\n",
      "Iteration 314, loss = 0.33857200\n",
      "Iteration 315, loss = 0.33802279\n",
      "Iteration 316, loss = 0.33747479\n",
      "Iteration 317, loss = 0.33692844\n",
      "Iteration 318, loss = 0.33638370\n",
      "Iteration 319, loss = 0.33584064\n",
      "Iteration 320, loss = 0.33529914\n",
      "Iteration 321, loss = 0.33475873\n",
      "Iteration 322, loss = 0.33421977\n",
      "Iteration 323, loss = 0.33368211\n",
      "Iteration 324, loss = 0.33314241\n",
      "Iteration 325, loss = 0.33259992\n",
      "Iteration 326, loss = 0.33205526\n",
      "Iteration 327, loss = 0.33150809\n",
      "Iteration 328, loss = 0.33096019\n",
      "Iteration 329, loss = 0.33041130\n",
      "Iteration 330, loss = 0.32986263\n",
      "Iteration 331, loss = 0.32931267\n",
      "Iteration 332, loss = 0.32876348\n",
      "Iteration 333, loss = 0.32821873\n",
      "Iteration 334, loss = 0.32767341\n",
      "Iteration 335, loss = 0.32712856\n",
      "Iteration 336, loss = 0.32658245\n",
      "Iteration 337, loss = 0.32603315\n",
      "Iteration 338, loss = 0.32548230\n",
      "Iteration 339, loss = 0.32493641\n",
      "Iteration 340, loss = 0.32439632\n",
      "Iteration 341, loss = 0.32385512\n",
      "Iteration 342, loss = 0.32331341\n",
      "Iteration 343, loss = 0.32277636\n",
      "Iteration 344, loss = 0.32224182\n",
      "Iteration 345, loss = 0.32170903\n",
      "Iteration 346, loss = 0.32117793\n",
      "Iteration 347, loss = 0.32064850\n",
      "Iteration 348, loss = 0.32012051\n",
      "Iteration 349, loss = 0.31959077\n",
      "Iteration 350, loss = 0.31906533\n",
      "Iteration 351, loss = 0.31853872\n",
      "Iteration 352, loss = 0.31801184\n",
      "Iteration 353, loss = 0.31748525\n",
      "Iteration 354, loss = 0.31695783\n",
      "Iteration 355, loss = 0.31642468\n",
      "Iteration 356, loss = 0.31588661\n",
      "Iteration 357, loss = 0.31535200\n",
      "Iteration 358, loss = 0.31482292\n",
      "Iteration 359, loss = 0.31430604\n",
      "Iteration 360, loss = 0.31379253\n",
      "Iteration 361, loss = 0.31328235\n",
      "Iteration 362, loss = 0.31277491\n",
      "Iteration 363, loss = 0.31227340\n",
      "Iteration 364, loss = 0.31178640\n",
      "Iteration 365, loss = 0.31130283\n",
      "Iteration 366, loss = 0.31081990\n",
      "Iteration 367, loss = 0.31034223\n",
      "Iteration 368, loss = 0.30986738\n",
      "Iteration 369, loss = 0.30939713\n",
      "Iteration 370, loss = 0.30893036\n",
      "Iteration 371, loss = 0.30846762\n",
      "Iteration 372, loss = 0.30800711\n",
      "Iteration 373, loss = 0.30755084\n",
      "Iteration 374, loss = 0.30710103\n",
      "Iteration 375, loss = 0.30665238\n",
      "Iteration 376, loss = 0.30620518\n",
      "Iteration 377, loss = 0.30575932\n",
      "Iteration 378, loss = 0.30531483\n",
      "Iteration 379, loss = 0.30487208\n",
      "Iteration 380, loss = 0.30443123\n",
      "Iteration 381, loss = 0.30399164\n",
      "Iteration 382, loss = 0.30355331\n",
      "Iteration 383, loss = 0.30311615\n",
      "Iteration 384, loss = 0.30268030\n",
      "Iteration 385, loss = 0.30224561\n",
      "Iteration 386, loss = 0.30181249\n",
      "Iteration 387, loss = 0.30138052\n",
      "Iteration 388, loss = 0.30095002\n",
      "Iteration 389, loss = 0.30052087\n",
      "Iteration 390, loss = 0.30009278\n",
      "Iteration 391, loss = 0.29966590\n",
      "Iteration 392, loss = 0.29924001\n",
      "Iteration 393, loss = 0.29881531\n",
      "Iteration 394, loss = 0.29839164\n",
      "Iteration 395, loss = 0.29796907\n",
      "Iteration 396, loss = 0.29754757\n",
      "Iteration 397, loss = 0.29712718\n",
      "Iteration 398, loss = 0.29670780\n",
      "Iteration 399, loss = 0.29628950\n",
      "Iteration 400, loss = 0.29587224\n",
      "Iteration 401, loss = 0.29545603\n",
      "Iteration 402, loss = 0.29504087\n",
      "Iteration 403, loss = 0.29462670\n",
      "Iteration 404, loss = 0.29421359\n",
      "Iteration 405, loss = 0.29380148\n",
      "Iteration 406, loss = 0.29339042\n",
      "Iteration 407, loss = 0.29298032\n",
      "Iteration 408, loss = 0.29257161\n",
      "Iteration 409, loss = 0.29216377\n",
      "Iteration 410, loss = 0.29175686\n",
      "Iteration 411, loss = 0.29135093\n",
      "Iteration 412, loss = 0.29094588\n",
      "Iteration 413, loss = 0.29054181\n",
      "Iteration 414, loss = 0.29013888\n",
      "Iteration 415, loss = 0.28973697\n",
      "Iteration 416, loss = 0.28933595\n",
      "Iteration 417, loss = 0.28893591\n",
      "Iteration 418, loss = 0.28853682\n",
      "Iteration 419, loss = 0.28813863\n",
      "Iteration 420, loss = 0.28774154\n",
      "Iteration 421, loss = 0.28734533\n",
      "Iteration 422, loss = 0.28695008\n",
      "Iteration 423, loss = 0.28655580\n",
      "Iteration 424, loss = 0.28616243\n",
      "Iteration 425, loss = 0.28577002\n",
      "Iteration 426, loss = 0.28537855\n",
      "Iteration 427, loss = 0.28498798\n",
      "Iteration 428, loss = 0.28459838\n",
      "Iteration 429, loss = 0.28420964\n",
      "Iteration 430, loss = 0.28382190\n",
      "Iteration 431, loss = 0.28343501\n",
      "Iteration 432, loss = 0.28304903\n",
      "Iteration 433, loss = 0.28266397\n",
      "Iteration 434, loss = 0.28227985\n",
      "Iteration 435, loss = 0.28189660\n",
      "Iteration 436, loss = 0.28151423\n",
      "Iteration 437, loss = 0.28113281\n",
      "Iteration 438, loss = 0.28075222\n",
      "Iteration 439, loss = 0.28037258\n",
      "Iteration 440, loss = 0.27999374\n",
      "Iteration 441, loss = 0.27961587\n",
      "Iteration 442, loss = 0.27923881\n",
      "Iteration 443, loss = 0.27886271\n",
      "Iteration 444, loss = 0.27848741\n",
      "Iteration 445, loss = 0.27811295\n",
      "Iteration 446, loss = 0.27773939\n",
      "Iteration 447, loss = 0.27736673\n",
      "Iteration 448, loss = 0.27699488\n",
      "Iteration 449, loss = 0.27662394\n",
      "Iteration 450, loss = 0.27625380\n",
      "Iteration 451, loss = 0.27588455\n",
      "Iteration 452, loss = 0.27551613\n",
      "Iteration 453, loss = 0.27514855\n",
      "Iteration 454, loss = 0.27478183\n",
      "Iteration 455, loss = 0.27441593\n",
      "Iteration 456, loss = 0.27405088\n",
      "Iteration 457, loss = 0.27368665\n",
      "Iteration 458, loss = 0.27332330\n",
      "Iteration 459, loss = 0.27296066\n",
      "Iteration 460, loss = 0.27259899\n",
      "Iteration 461, loss = 0.27223799\n",
      "Iteration 462, loss = 0.27187797\n",
      "Iteration 463, loss = 0.27151863\n",
      "Iteration 464, loss = 0.27116016\n",
      "Iteration 465, loss = 0.27080247\n",
      "Iteration 466, loss = 0.27044562\n",
      "Iteration 467, loss = 0.27008956\n",
      "Iteration 468, loss = 0.26973433\n",
      "Iteration 469, loss = 0.26937986\n",
      "Iteration 470, loss = 0.26902619\n",
      "Iteration 471, loss = 0.26867339\n",
      "Iteration 472, loss = 0.26832127\n",
      "Iteration 473, loss = 0.26797005\n",
      "Iteration 474, loss = 0.26761951\n",
      "Iteration 475, loss = 0.26726985\n",
      "Iteration 476, loss = 0.26692088\n",
      "Iteration 477, loss = 0.26657278\n",
      "Iteration 478, loss = 0.26622540\n",
      "Iteration 479, loss = 0.26587879\n",
      "Iteration 480, loss = 0.26553303\n",
      "Iteration 481, loss = 0.26518802\n",
      "Iteration 482, loss = 0.26484395\n",
      "Iteration 483, loss = 0.26450069\n",
      "Iteration 484, loss = 0.26415825\n",
      "Iteration 485, loss = 0.26381652\n",
      "Iteration 486, loss = 0.26347558\n",
      "Iteration 487, loss = 0.26313546\n",
      "Iteration 488, loss = 0.26279600\n",
      "Iteration 489, loss = 0.26245736\n",
      "Iteration 490, loss = 0.26211947\n",
      "Iteration 491, loss = 0.26178226\n",
      "Iteration 492, loss = 0.26144594\n",
      "Iteration 493, loss = 0.26111041\n",
      "Iteration 494, loss = 0.26077579\n",
      "Iteration 495, loss = 0.26044185\n",
      "Iteration 496, loss = 0.26010884\n",
      "Iteration 497, loss = 0.25977648\n",
      "Iteration 498, loss = 0.25944491\n",
      "Iteration 499, loss = 0.25911414\n",
      "Iteration 500, loss = 0.25878403\n",
      "Iteration 501, loss = 0.25845476\n",
      "Iteration 502, loss = 0.25812612\n",
      "Iteration 503, loss = 0.25779828\n",
      "Iteration 504, loss = 0.25747119\n",
      "Iteration 505, loss = 0.25714495\n",
      "Iteration 506, loss = 0.25681958\n",
      "Iteration 507, loss = 0.25649494\n",
      "Iteration 508, loss = 0.25617105\n",
      "Iteration 509, loss = 0.25584794\n",
      "Iteration 510, loss = 0.25552562\n",
      "Iteration 511, loss = 0.25520395\n",
      "Iteration 512, loss = 0.25488308\n",
      "Iteration 513, loss = 0.25456287\n",
      "Iteration 514, loss = 0.25424348\n",
      "Iteration 515, loss = 0.25392471\n",
      "Iteration 516, loss = 0.25360662\n",
      "Iteration 517, loss = 0.25328935\n",
      "Iteration 518, loss = 0.25297268\n",
      "Iteration 519, loss = 0.25265673\n",
      "Iteration 520, loss = 0.25234148\n",
      "Iteration 521, loss = 0.25202696\n",
      "Iteration 522, loss = 0.25171306\n",
      "Iteration 523, loss = 0.25139997\n",
      "Iteration 524, loss = 0.25108758\n",
      "Iteration 525, loss = 0.25077588\n",
      "Iteration 526, loss = 0.25046494\n",
      "Iteration 527, loss = 0.25015469\n",
      "Iteration 528, loss = 0.24984515\n",
      "Iteration 529, loss = 0.24953632\n",
      "Iteration 530, loss = 0.24922814\n",
      "Iteration 531, loss = 0.24892062\n",
      "Iteration 532, loss = 0.24861378\n",
      "Iteration 533, loss = 0.24830762\n",
      "Iteration 534, loss = 0.24800213\n",
      "Iteration 535, loss = 0.24769728\n",
      "Iteration 536, loss = 0.24739323\n",
      "Iteration 537, loss = 0.24708977\n",
      "Iteration 538, loss = 0.24678700\n",
      "Iteration 539, loss = 0.24648488\n",
      "Iteration 540, loss = 0.24618347\n",
      "Iteration 541, loss = 0.24588262\n",
      "Iteration 542, loss = 0.24558251\n",
      "Iteration 543, loss = 0.24528300\n",
      "Iteration 544, loss = 0.24498432\n",
      "Iteration 545, loss = 0.24468645\n",
      "Iteration 546, loss = 0.24438929\n",
      "Iteration 547, loss = 0.24409268\n",
      "Iteration 548, loss = 0.24379675\n",
      "Iteration 549, loss = 0.24350141\n",
      "Iteration 550, loss = 0.24320679\n",
      "Iteration 551, loss = 0.24291271\n",
      "Iteration 552, loss = 0.24261929\n",
      "Iteration 553, loss = 0.24232650\n",
      "Iteration 554, loss = 0.24203438\n",
      "Iteration 555, loss = 0.24174280\n",
      "Iteration 556, loss = 0.24145193\n",
      "Iteration 557, loss = 0.24116162\n",
      "Iteration 558, loss = 0.24087198\n",
      "Iteration 559, loss = 0.24058299\n",
      "Iteration 560, loss = 0.24029467\n",
      "Iteration 561, loss = 0.24000690\n",
      "Iteration 562, loss = 0.23971980\n",
      "Iteration 563, loss = 0.23943329\n",
      "Iteration 564, loss = 0.23914745\n",
      "Iteration 565, loss = 0.23886218\n",
      "Iteration 566, loss = 0.23857753\n",
      "Iteration 567, loss = 0.23829352\n",
      "Iteration 568, loss = 0.23801003\n",
      "Iteration 569, loss = 0.23772725\n",
      "Iteration 570, loss = 0.23744501\n",
      "Iteration 571, loss = 0.23716339\n",
      "Iteration 572, loss = 0.23688234\n",
      "Iteration 573, loss = 0.23660195\n",
      "Iteration 574, loss = 0.23632210\n",
      "Iteration 575, loss = 0.23604283\n",
      "Iteration 576, loss = 0.23576422\n",
      "Iteration 577, loss = 0.23548617\n",
      "Iteration 578, loss = 0.23520879\n",
      "Iteration 579, loss = 0.23493193\n",
      "Iteration 580, loss = 0.23465574\n",
      "Iteration 581, loss = 0.23438008\n",
      "Iteration 582, loss = 0.23410505\n",
      "Iteration 583, loss = 0.23383057\n",
      "Iteration 584, loss = 0.23355670\n",
      "Iteration 585, loss = 0.23328337\n",
      "Iteration 586, loss = 0.23301067\n",
      "Iteration 587, loss = 0.23273854\n",
      "Iteration 588, loss = 0.23246697\n",
      "Iteration 589, loss = 0.23219610\n",
      "Iteration 590, loss = 0.23192587\n",
      "Iteration 591, loss = 0.23165622\n",
      "Iteration 592, loss = 0.23138713\n",
      "Iteration 593, loss = 0.23111865\n",
      "Iteration 594, loss = 0.23085072\n",
      "Iteration 595, loss = 0.23058336\n",
      "Iteration 596, loss = 0.23031664\n",
      "Iteration 597, loss = 0.23005044\n",
      "Iteration 598, loss = 0.22978488\n",
      "Iteration 599, loss = 0.22951982\n",
      "Iteration 600, loss = 0.22925537\n",
      "Iteration 601, loss = 0.22899144\n",
      "Iteration 602, loss = 0.22872811\n",
      "Iteration 603, loss = 0.22846535\n",
      "Iteration 604, loss = 0.22820309\n",
      "Iteration 605, loss = 0.22794140\n",
      "Iteration 606, loss = 0.22768030\n",
      "Iteration 607, loss = 0.22741970\n",
      "Iteration 608, loss = 0.22715971\n",
      "Iteration 609, loss = 0.22690019\n",
      "Iteration 610, loss = 0.22664132\n",
      "Iteration 611, loss = 0.22638290\n",
      "Iteration 612, loss = 0.22612503\n",
      "Iteration 613, loss = 0.22586775\n",
      "Iteration 614, loss = 0.22561100\n",
      "Iteration 615, loss = 0.22535483\n",
      "Iteration 616, loss = 0.22509923\n",
      "Iteration 617, loss = 0.22484413\n",
      "Iteration 618, loss = 0.22458966\n",
      "Iteration 619, loss = 0.22433563\n",
      "Iteration 620, loss = 0.22408220\n",
      "Iteration 621, loss = 0.22382930\n",
      "Iteration 622, loss = 0.22357693\n",
      "Iteration 623, loss = 0.22332506\n",
      "Iteration 624, loss = 0.22307379\n",
      "Iteration 625, loss = 0.22282297\n",
      "Iteration 626, loss = 0.22257274\n",
      "Iteration 627, loss = 0.22232298\n",
      "Iteration 628, loss = 0.22207380\n",
      "Iteration 629, loss = 0.22182519\n",
      "Iteration 630, loss = 0.22157706\n",
      "Iteration 631, loss = 0.22132946\n",
      "Iteration 632, loss = 0.22108242\n",
      "Iteration 633, loss = 0.22083582\n",
      "Iteration 634, loss = 0.22058978\n",
      "Iteration 635, loss = 0.22034424\n",
      "Iteration 636, loss = 0.22009922\n",
      "Iteration 637, loss = 0.21985473\n",
      "Iteration 638, loss = 0.21961072\n",
      "Iteration 639, loss = 0.21936721\n",
      "Iteration 640, loss = 0.21912429\n",
      "Iteration 641, loss = 0.21888179\n",
      "Iteration 642, loss = 0.21863985\n",
      "Iteration 643, loss = 0.21839841\n",
      "Iteration 644, loss = 0.21815740\n",
      "Iteration 645, loss = 0.21791701\n",
      "Iteration 646, loss = 0.21767704\n",
      "Iteration 647, loss = 0.21743758\n",
      "Iteration 648, loss = 0.21719859\n",
      "Iteration 649, loss = 0.21696016\n",
      "Iteration 650, loss = 0.21672218\n",
      "Iteration 651, loss = 0.21648478\n",
      "Iteration 652, loss = 0.21624785\n",
      "Iteration 653, loss = 0.21601140\n",
      "Iteration 654, loss = 0.21577549\n",
      "Iteration 655, loss = 0.21554001\n",
      "Iteration 656, loss = 0.21530506\n",
      "Iteration 657, loss = 0.21507056\n",
      "Iteration 658, loss = 0.21483657\n",
      "Iteration 659, loss = 0.21460308\n",
      "Iteration 660, loss = 0.21437007\n",
      "Iteration 661, loss = 0.21413749\n",
      "Iteration 662, loss = 0.21390547\n",
      "Iteration 663, loss = 0.21367386\n",
      "Iteration 664, loss = 0.21344281\n",
      "Iteration 665, loss = 0.21321225\n",
      "Iteration 666, loss = 0.21298210\n",
      "Iteration 667, loss = 0.21275247\n",
      "Iteration 668, loss = 0.21252331\n",
      "Iteration 669, loss = 0.21229465\n",
      "Iteration 670, loss = 0.21206642\n",
      "Iteration 671, loss = 0.21183865\n",
      "Iteration 672, loss = 0.21161142\n",
      "Iteration 673, loss = 0.21138457\n",
      "Iteration 674, loss = 0.21115825\n",
      "Iteration 675, loss = 0.21093240\n",
      "Iteration 676, loss = 0.21070702\n",
      "Iteration 677, loss = 0.21048206\n",
      "Iteration 678, loss = 0.21025762\n",
      "Iteration 679, loss = 0.21003366\n",
      "Iteration 680, loss = 0.20981014\n",
      "Iteration 681, loss = 0.20958706\n",
      "Iteration 682, loss = 0.20936449\n",
      "Iteration 683, loss = 0.20914233\n",
      "Iteration 684, loss = 0.20892067\n",
      "Iteration 685, loss = 0.20869944\n",
      "Iteration 686, loss = 0.20847863\n",
      "Iteration 687, loss = 0.20825835\n",
      "Iteration 688, loss = 0.20803843\n",
      "Iteration 689, loss = 0.20781903\n",
      "Iteration 690, loss = 0.20760008\n",
      "Iteration 691, loss = 0.20738155\n",
      "Iteration 692, loss = 0.20716345\n",
      "Iteration 693, loss = 0.20694584\n",
      "Iteration 694, loss = 0.20672865\n",
      "Iteration 695, loss = 0.20651191\n",
      "Iteration 696, loss = 0.20629565\n",
      "Iteration 697, loss = 0.20607977\n",
      "Iteration 698, loss = 0.20586435\n",
      "Iteration 699, loss = 0.20564938\n",
      "Iteration 700, loss = 0.20543483\n",
      "Iteration 701, loss = 0.20522071\n",
      "Iteration 702, loss = 0.20500703\n",
      "Iteration 703, loss = 0.20479380\n",
      "Iteration 704, loss = 0.20458098\n",
      "Iteration 705, loss = 0.20436859\n",
      "Iteration 706, loss = 0.20415666\n",
      "Iteration 707, loss = 0.20394515\n",
      "Iteration 708, loss = 0.20373408\n",
      "Iteration 709, loss = 0.20352339\n",
      "Iteration 710, loss = 0.20331319\n",
      "Iteration 711, loss = 0.20310337\n",
      "Iteration 712, loss = 0.20289406\n",
      "Iteration 713, loss = 0.20268507\n",
      "Iteration 714, loss = 0.20247665\n",
      "Iteration 715, loss = 0.20226856\n",
      "Iteration 716, loss = 0.20206096\n",
      "Iteration 717, loss = 0.20185376\n",
      "Iteration 718, loss = 0.20164704\n",
      "Iteration 719, loss = 0.20144067\n",
      "Iteration 720, loss = 0.20123477\n",
      "Iteration 721, loss = 0.20102925\n",
      "Iteration 722, loss = 0.20082420\n",
      "Iteration 723, loss = 0.20061953\n",
      "Iteration 724, loss = 0.20041530\n",
      "Iteration 725, loss = 0.20021146\n",
      "Iteration 726, loss = 0.20000806\n",
      "Iteration 727, loss = 0.19980504\n",
      "Iteration 728, loss = 0.19960246\n",
      "Iteration 729, loss = 0.19940028\n",
      "Iteration 730, loss = 0.19919853\n",
      "Iteration 731, loss = 0.19899718\n",
      "Iteration 732, loss = 0.19879622\n",
      "Iteration 733, loss = 0.19859569\n",
      "Iteration 734, loss = 0.19839558\n",
      "Iteration 735, loss = 0.19819590\n",
      "Iteration 736, loss = 0.19799659\n",
      "Iteration 737, loss = 0.19779774\n",
      "Iteration 738, loss = 0.19759922\n",
      "Iteration 739, loss = 0.19740116\n",
      "Iteration 740, loss = 0.19720347\n",
      "Iteration 741, loss = 0.19700622\n",
      "Iteration 742, loss = 0.19680933\n",
      "Iteration 743, loss = 0.19661289\n",
      "Iteration 744, loss = 0.19641687\n",
      "Iteration 745, loss = 0.19622122\n",
      "Iteration 746, loss = 0.19602598\n",
      "Iteration 747, loss = 0.19583114\n",
      "Iteration 748, loss = 0.19563671\n",
      "Iteration 749, loss = 0.19544263\n",
      "Iteration 750, loss = 0.19524899\n",
      "Iteration 751, loss = 0.19505574\n",
      "Iteration 752, loss = 0.19486283\n",
      "Iteration 753, loss = 0.19467034\n",
      "Iteration 754, loss = 0.19447823\n",
      "Iteration 755, loss = 0.19428651\n",
      "Iteration 756, loss = 0.19409518\n",
      "Iteration 757, loss = 0.19390416\n",
      "Iteration 758, loss = 0.19371353\n",
      "Iteration 759, loss = 0.19352330\n",
      "Iteration 760, loss = 0.19333342\n",
      "Iteration 761, loss = 0.19314393\n",
      "Iteration 762, loss = 0.19295481\n",
      "Iteration 763, loss = 0.19276607\n",
      "Iteration 764, loss = 0.19257771\n",
      "Iteration 765, loss = 0.19238973\n",
      "Iteration 766, loss = 0.19220210\n",
      "Iteration 767, loss = 0.19201487\n",
      "Iteration 768, loss = 0.19182799\n",
      "Iteration 769, loss = 0.19164149\n",
      "Iteration 770, loss = 0.19145539\n",
      "Iteration 771, loss = 0.19126961\n",
      "Iteration 772, loss = 0.19108423\n",
      "Iteration 773, loss = 0.19089920\n",
      "Iteration 774, loss = 0.19071454\n",
      "Iteration 775, loss = 0.19053029\n",
      "Iteration 776, loss = 0.19034634\n",
      "Iteration 777, loss = 0.19016283\n",
      "Iteration 778, loss = 0.18997964\n",
      "Iteration 779, loss = 0.18979685\n",
      "Iteration 780, loss = 0.18961441\n",
      "Iteration 781, loss = 0.18943234\n",
      "Iteration 782, loss = 0.18925063\n",
      "Iteration 783, loss = 0.18906926\n",
      "Iteration 784, loss = 0.18888829\n",
      "Iteration 785, loss = 0.18870764\n",
      "Iteration 786, loss = 0.18852736\n",
      "Iteration 787, loss = 0.18834743\n",
      "Iteration 788, loss = 0.18816786\n",
      "Iteration 789, loss = 0.18798867\n",
      "Iteration 790, loss = 0.18780981\n",
      "Iteration 791, loss = 0.18763131\n",
      "Iteration 792, loss = 0.18745318\n",
      "Iteration 793, loss = 0.18727543\n",
      "Iteration 794, loss = 0.18709803\n",
      "Iteration 795, loss = 0.18692104\n",
      "Iteration 796, loss = 0.18674435\n",
      "Iteration 797, loss = 0.18656802\n",
      "Iteration 798, loss = 0.18639204\n",
      "Iteration 799, loss = 0.18621643\n",
      "Iteration 800, loss = 0.18604115\n",
      "Iteration 801, loss = 0.18586623\n",
      "Iteration 802, loss = 0.18569164\n",
      "Iteration 803, loss = 0.18551740\n",
      "Iteration 804, loss = 0.18534350\n",
      "Iteration 805, loss = 0.18516995\n",
      "Iteration 806, loss = 0.18499672\n",
      "Iteration 807, loss = 0.18482389\n",
      "Iteration 808, loss = 0.18465135\n",
      "Iteration 809, loss = 0.18447915\n",
      "Iteration 810, loss = 0.18430731\n",
      "Iteration 811, loss = 0.18413579\n",
      "Iteration 812, loss = 0.18396464\n",
      "Iteration 813, loss = 0.18379378\n",
      "Iteration 814, loss = 0.18362329\n",
      "Iteration 815, loss = 0.18345311\n",
      "Iteration 816, loss = 0.18328330\n",
      "Iteration 817, loss = 0.18311379\n",
      "Iteration 818, loss = 0.18294464\n",
      "Iteration 819, loss = 0.18277583\n",
      "Iteration 820, loss = 0.18260735\n",
      "Iteration 821, loss = 0.18243920\n",
      "Iteration 822, loss = 0.18227137\n",
      "Iteration 823, loss = 0.18210388\n",
      "Iteration 824, loss = 0.18193674\n",
      "Iteration 825, loss = 0.18176989\n",
      "Iteration 826, loss = 0.18160339\n",
      "Iteration 827, loss = 0.18143721\n",
      "Iteration 828, loss = 0.18127136\n",
      "Iteration 829, loss = 0.18110582\n",
      "Iteration 830, loss = 0.18094062\n",
      "Iteration 831, loss = 0.18077572\n",
      "Iteration 832, loss = 0.18061116\n",
      "Iteration 833, loss = 0.18044695\n",
      "Iteration 834, loss = 0.18028300\n",
      "Iteration 835, loss = 0.18011934\n",
      "Iteration 836, loss = 0.17995599\n",
      "Iteration 837, loss = 0.17979295\n",
      "Iteration 838, loss = 0.17963024\n",
      "Iteration 839, loss = 0.17946782\n",
      "Iteration 840, loss = 0.17930570\n",
      "Iteration 841, loss = 0.17914392\n",
      "Iteration 842, loss = 0.17898243\n",
      "Iteration 843, loss = 0.17882126\n",
      "Iteration 844, loss = 0.17866038\n",
      "Iteration 845, loss = 0.17849981\n",
      "Iteration 846, loss = 0.17833953\n",
      "Iteration 847, loss = 0.17817956\n",
      "Iteration 848, loss = 0.17801993\n",
      "Iteration 849, loss = 0.17786058\n",
      "Iteration 850, loss = 0.17770155\n",
      "Iteration 851, loss = 0.17754284\n",
      "Iteration 852, loss = 0.17738444\n",
      "Iteration 853, loss = 0.17722632\n",
      "Iteration 854, loss = 0.17706851\n",
      "Iteration 855, loss = 0.17691101\n",
      "Iteration 856, loss = 0.17675381\n",
      "Iteration 857, loss = 0.17659695\n",
      "Iteration 858, loss = 0.17644036\n",
      "Iteration 859, loss = 0.17628406\n",
      "Iteration 860, loss = 0.17612809\n",
      "Iteration 861, loss = 0.17597243\n",
      "Iteration 862, loss = 0.17581705\n",
      "Iteration 863, loss = 0.17566196\n",
      "Iteration 864, loss = 0.17550717\n",
      "Iteration 865, loss = 0.17535268\n",
      "Iteration 866, loss = 0.17519850\n",
      "Iteration 867, loss = 0.17504461\n",
      "Iteration 868, loss = 0.17489100\n",
      "Iteration 869, loss = 0.17473767\n",
      "Iteration 870, loss = 0.17458465\n",
      "Iteration 871, loss = 0.17443192\n",
      "Iteration 872, loss = 0.17427948\n",
      "Iteration 873, loss = 0.17412733\n",
      "Iteration 874, loss = 0.17397548\n",
      "Iteration 875, loss = 0.17382391\n",
      "Iteration 876, loss = 0.17367263\n",
      "Iteration 877, loss = 0.17352163\n",
      "Iteration 878, loss = 0.17337094\n",
      "Iteration 879, loss = 0.17322052\n",
      "Iteration 880, loss = 0.17307041\n",
      "Iteration 881, loss = 0.17292055\n",
      "Iteration 882, loss = 0.17277100\n",
      "Iteration 883, loss = 0.17262172\n",
      "Iteration 884, loss = 0.17247273\n",
      "Iteration 885, loss = 0.17232403\n",
      "Iteration 886, loss = 0.17217560\n",
      "Iteration 887, loss = 0.17202746\n",
      "Iteration 888, loss = 0.17187961\n",
      "Iteration 889, loss = 0.17173202\n",
      "Iteration 890, loss = 0.17158474\n",
      "Iteration 891, loss = 0.17143771\n",
      "Iteration 892, loss = 0.17129098\n",
      "Iteration 893, loss = 0.17114451\n",
      "Iteration 894, loss = 0.17099833\n",
      "Iteration 895, loss = 0.17085242\n",
      "Iteration 896, loss = 0.17070678\n",
      "Iteration 897, loss = 0.17056143\n",
      "Iteration 898, loss = 0.17041635\n",
      "Iteration 899, loss = 0.17027154\n",
      "Iteration 900, loss = 0.17012700\n",
      "Iteration 901, loss = 0.16998275\n",
      "Iteration 902, loss = 0.16983875\n",
      "Iteration 903, loss = 0.16969504\n",
      "Iteration 904, loss = 0.16955159\n",
      "Iteration 905, loss = 0.16940840\n",
      "Iteration 906, loss = 0.16926542\n",
      "Iteration 907, loss = 0.16912269\n",
      "Iteration 908, loss = 0.16898024\n",
      "Iteration 909, loss = 0.16883804\n",
      "Iteration 910, loss = 0.16869611\n",
      "Iteration 911, loss = 0.16855444\n",
      "Iteration 912, loss = 0.16841304\n",
      "Iteration 913, loss = 0.16827191\n",
      "Iteration 914, loss = 0.16813115\n",
      "Iteration 915, loss = 0.16799077\n",
      "Iteration 916, loss = 0.16785064\n",
      "Iteration 917, loss = 0.16771076\n",
      "Iteration 918, loss = 0.16757114\n",
      "Iteration 919, loss = 0.16743177\n",
      "Iteration 920, loss = 0.16729266\n",
      "Iteration 921, loss = 0.16715380\n",
      "Iteration 922, loss = 0.16701519\n",
      "Iteration 923, loss = 0.16687685\n",
      "Iteration 924, loss = 0.16673876\n",
      "Iteration 925, loss = 0.16660092\n",
      "Iteration 926, loss = 0.16646333\n",
      "Iteration 927, loss = 0.16632600\n",
      "Iteration 928, loss = 0.16618893\n",
      "Iteration 929, loss = 0.16605210\n",
      "Iteration 930, loss = 0.16591547\n",
      "Iteration 931, loss = 0.16577908\n",
      "Iteration 932, loss = 0.16564295\n",
      "Iteration 933, loss = 0.16550706\n",
      "Iteration 934, loss = 0.16537143\n",
      "Iteration 935, loss = 0.16523606\n",
      "Iteration 936, loss = 0.16510096\n",
      "Iteration 937, loss = 0.16496609\n",
      "Iteration 938, loss = 0.16483147\n",
      "Iteration 939, loss = 0.16469710\n",
      "Iteration 940, loss = 0.16456297\n",
      "Iteration 941, loss = 0.16442909\n",
      "Iteration 942, loss = 0.16429545\n",
      "Iteration 943, loss = 0.16416206\n",
      "Iteration 944, loss = 0.16402891\n",
      "Iteration 945, loss = 0.16389602\n",
      "Iteration 946, loss = 0.16376338\n",
      "Iteration 947, loss = 0.16363098\n",
      "Iteration 948, loss = 0.16349885\n",
      "Iteration 949, loss = 0.16336697\n",
      "Iteration 950, loss = 0.16323527\n",
      "Iteration 951, loss = 0.16310380\n",
      "Iteration 952, loss = 0.16297261\n",
      "Iteration 953, loss = 0.16284171\n",
      "Iteration 954, loss = 0.16271105\n",
      "Iteration 955, loss = 0.16258064\n",
      "Iteration 956, loss = 0.16245046\n",
      "Iteration 957, loss = 0.16232052\n",
      "Iteration 958, loss = 0.16219082\n",
      "Iteration 959, loss = 0.16206136\n",
      "Iteration 960, loss = 0.16193215\n",
      "Iteration 961, loss = 0.16180316\n",
      "Iteration 962, loss = 0.16167445\n",
      "Iteration 963, loss = 0.16154593\n",
      "Iteration 964, loss = 0.16141766\n",
      "Iteration 965, loss = 0.16128963\n",
      "Iteration 966, loss = 0.16116184\n",
      "Iteration 967, loss = 0.16103427\n",
      "Iteration 968, loss = 0.16090695\n",
      "Iteration 969, loss = 0.16077985\n",
      "Iteration 970, loss = 0.16065299\n",
      "Iteration 971, loss = 0.16052636\n",
      "Iteration 972, loss = 0.16039991\n",
      "Iteration 973, loss = 0.16027371\n",
      "Iteration 974, loss = 0.16014772\n",
      "Iteration 975, loss = 0.16002197\n",
      "Iteration 976, loss = 0.15989644\n",
      "Iteration 977, loss = 0.15977116\n",
      "Iteration 978, loss = 0.15964613\n",
      "Iteration 979, loss = 0.15952133\n",
      "Iteration 980, loss = 0.15939675\n",
      "Iteration 981, loss = 0.15927239\n",
      "Iteration 982, loss = 0.15914827\n",
      "Iteration 983, loss = 0.15902437\n",
      "Iteration 984, loss = 0.15890069\n",
      "Iteration 985, loss = 0.15877724\n",
      "Iteration 986, loss = 0.15865402\n",
      "Iteration 987, loss = 0.15853102\n",
      "Iteration 988, loss = 0.15840823\n",
      "Iteration 989, loss = 0.15828567\n",
      "Iteration 990, loss = 0.15816334\n",
      "Iteration 991, loss = 0.15804123\n",
      "Iteration 992, loss = 0.15791933\n",
      "Iteration 993, loss = 0.15779766\n",
      "Iteration 994, loss = 0.15767621\n",
      "Iteration 995, loss = 0.15755497\n",
      "Iteration 996, loss = 0.15743401\n",
      "Iteration 997, loss = 0.15731336\n",
      "Iteration 998, loss = 0.15719293\n",
      "Iteration 999, loss = 0.15707273\n",
      "Iteration 1000, loss = 0.15695274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.49174295\n",
      "Iteration 2, loss = 1.47766840\n",
      "Iteration 3, loss = 1.45789899\n",
      "Iteration 4, loss = 1.43337458\n",
      "Iteration 5, loss = 1.40509817\n",
      "Iteration 6, loss = 1.37396278\n",
      "Iteration 7, loss = 1.34081595\n",
      "Iteration 8, loss = 1.30669020\n",
      "Iteration 9, loss = 1.27261121\n",
      "Iteration 10, loss = 1.23970099\n",
      "Iteration 11, loss = 1.20826669\n",
      "Iteration 12, loss = 1.17879780\n",
      "Iteration 13, loss = 1.15178669\n",
      "Iteration 14, loss = 1.12737491\n",
      "Iteration 15, loss = 1.10589709\n",
      "Iteration 16, loss = 1.08709143\n",
      "Iteration 17, loss = 1.07037567\n",
      "Iteration 18, loss = 1.05546375\n",
      "Iteration 19, loss = 1.04202444\n",
      "Iteration 20, loss = 1.02948197\n",
      "Iteration 21, loss = 1.01756254\n",
      "Iteration 22, loss = 1.00574447\n",
      "Iteration 23, loss = 0.99377160\n",
      "Iteration 24, loss = 0.98156624\n",
      "Iteration 25, loss = 0.96915537\n",
      "Iteration 26, loss = 0.95667874\n",
      "Iteration 27, loss = 0.94426787\n",
      "Iteration 28, loss = 0.93196854\n",
      "Iteration 29, loss = 0.91980693\n",
      "Iteration 30, loss = 0.90794477\n",
      "Iteration 31, loss = 0.89634742\n",
      "Iteration 32, loss = 0.88506706\n",
      "Iteration 33, loss = 0.87421834\n",
      "Iteration 34, loss = 0.86389466\n",
      "Iteration 35, loss = 0.85408038\n",
      "Iteration 36, loss = 0.84462527\n",
      "Iteration 37, loss = 0.83551381\n",
      "Iteration 38, loss = 0.82668806\n",
      "Iteration 39, loss = 0.81810280\n",
      "Iteration 40, loss = 0.80969354\n",
      "Iteration 41, loss = 0.80148592\n",
      "Iteration 42, loss = 0.79345166\n",
      "Iteration 43, loss = 0.78566282\n",
      "Iteration 44, loss = 0.77811779\n",
      "Iteration 45, loss = 0.77085084\n",
      "Iteration 46, loss = 0.76383040\n",
      "Iteration 47, loss = 0.75697145\n",
      "Iteration 48, loss = 0.75026431\n",
      "Iteration 49, loss = 0.74374743\n",
      "Iteration 50, loss = 0.73741269\n",
      "Iteration 51, loss = 0.73124710\n",
      "Iteration 52, loss = 0.72528505\n",
      "Iteration 53, loss = 0.71950349\n",
      "Iteration 54, loss = 0.71386369\n",
      "Iteration 55, loss = 0.70835719\n",
      "Iteration 56, loss = 0.70299171\n",
      "Iteration 57, loss = 0.69775428\n",
      "Iteration 58, loss = 0.69267672\n",
      "Iteration 59, loss = 0.68772964\n",
      "Iteration 60, loss = 0.68291348\n",
      "Iteration 61, loss = 0.67821083\n",
      "Iteration 62, loss = 0.67360310\n",
      "Iteration 63, loss = 0.66908205\n",
      "Iteration 64, loss = 0.66465562\n",
      "Iteration 65, loss = 0.66031956\n",
      "Iteration 66, loss = 0.65607929\n",
      "Iteration 67, loss = 0.65195774\n",
      "Iteration 68, loss = 0.64795262\n",
      "Iteration 69, loss = 0.64406770\n",
      "Iteration 70, loss = 0.64028155\n",
      "Iteration 71, loss = 0.63659040\n",
      "Iteration 72, loss = 0.63298840\n",
      "Iteration 73, loss = 0.62945302\n",
      "Iteration 74, loss = 0.62598297\n",
      "Iteration 75, loss = 0.62257299\n",
      "Iteration 76, loss = 0.61921957\n",
      "Iteration 77, loss = 0.61592188\n",
      "Iteration 78, loss = 0.61267877\n",
      "Iteration 79, loss = 0.60948861\n",
      "Iteration 80, loss = 0.60635168\n",
      "Iteration 81, loss = 0.60326715\n",
      "Iteration 82, loss = 0.60023759\n",
      "Iteration 83, loss = 0.59726360\n",
      "Iteration 84, loss = 0.59433974\n",
      "Iteration 85, loss = 0.59146659\n",
      "Iteration 86, loss = 0.58863930\n",
      "Iteration 87, loss = 0.58585975\n",
      "Iteration 88, loss = 0.58313278\n",
      "Iteration 89, loss = 0.58045331\n",
      "Iteration 90, loss = 0.57782290\n",
      "Iteration 91, loss = 0.57523801\n",
      "Iteration 92, loss = 0.57269820\n",
      "Iteration 93, loss = 0.57019934\n",
      "Iteration 94, loss = 0.56773925\n",
      "Iteration 95, loss = 0.56531738\n",
      "Iteration 96, loss = 0.56293285\n",
      "Iteration 97, loss = 0.56058502\n",
      "Iteration 98, loss = 0.55827343\n",
      "Iteration 99, loss = 0.55599780\n",
      "Iteration 100, loss = 0.55375847\n",
      "Iteration 101, loss = 0.55155471\n",
      "Iteration 102, loss = 0.54938273\n",
      "Iteration 103, loss = 0.54724366\n",
      "Iteration 104, loss = 0.54513628\n",
      "Iteration 105, loss = 0.54306027\n",
      "Iteration 106, loss = 0.54101601\n",
      "Iteration 107, loss = 0.53900138\n",
      "Iteration 108, loss = 0.53701595\n",
      "Iteration 109, loss = 0.53505805\n",
      "Iteration 110, loss = 0.53312718\n",
      "Iteration 111, loss = 0.53122293\n",
      "Iteration 112, loss = 0.52934452\n",
      "Iteration 113, loss = 0.52749263\n",
      "Iteration 114, loss = 0.52566513\n",
      "Iteration 115, loss = 0.52386183\n",
      "Iteration 116, loss = 0.52208185\n",
      "Iteration 117, loss = 0.52032441\n",
      "Iteration 118, loss = 0.51858821\n",
      "Iteration 119, loss = 0.51687362\n",
      "Iteration 120, loss = 0.51518074\n",
      "Iteration 121, loss = 0.51350865\n",
      "Iteration 122, loss = 0.51185676\n",
      "Iteration 123, loss = 0.51022463\n",
      "Iteration 124, loss = 0.50861303\n",
      "Iteration 125, loss = 0.50702191\n",
      "Iteration 126, loss = 0.50544990\n",
      "Iteration 127, loss = 0.50389394\n",
      "Iteration 128, loss = 0.50235445\n",
      "Iteration 129, loss = 0.50083274\n",
      "Iteration 130, loss = 0.49932832\n",
      "Iteration 131, loss = 0.49784080\n",
      "Iteration 132, loss = 0.49636813\n",
      "Iteration 133, loss = 0.49490884\n",
      "Iteration 134, loss = 0.49346179\n",
      "Iteration 135, loss = 0.49202411\n",
      "Iteration 136, loss = 0.49059263\n",
      "Iteration 137, loss = 0.48916867\n",
      "Iteration 138, loss = 0.48774238\n",
      "Iteration 139, loss = 0.48631283\n",
      "Iteration 140, loss = 0.48489317\n",
      "Iteration 141, loss = 0.48348728\n",
      "Iteration 142, loss = 0.48209389\n",
      "Iteration 143, loss = 0.48071099\n",
      "Iteration 144, loss = 0.47933245\n",
      "Iteration 145, loss = 0.47797277\n",
      "Iteration 146, loss = 0.47661889\n",
      "Iteration 147, loss = 0.47527852\n",
      "Iteration 148, loss = 0.47393405\n",
      "Iteration 149, loss = 0.47260773\n",
      "Iteration 150, loss = 0.47131175\n",
      "Iteration 151, loss = 0.47004580\n",
      "Iteration 152, loss = 0.46877876\n",
      "Iteration 153, loss = 0.46752051\n",
      "Iteration 154, loss = 0.46628424\n",
      "Iteration 155, loss = 0.46505932\n",
      "Iteration 156, loss = 0.46386758\n",
      "Iteration 157, loss = 0.46270429\n",
      "Iteration 158, loss = 0.46156234\n",
      "Iteration 159, loss = 0.46043355\n",
      "Iteration 160, loss = 0.45931122\n",
      "Iteration 161, loss = 0.45819650\n",
      "Iteration 162, loss = 0.45709348\n",
      "Iteration 163, loss = 0.45600108\n",
      "Iteration 164, loss = 0.45492184\n",
      "Iteration 165, loss = 0.45384935\n",
      "Iteration 166, loss = 0.45278493\n",
      "Iteration 167, loss = 0.45172567\n",
      "Iteration 168, loss = 0.45067261\n",
      "Iteration 169, loss = 0.44962578\n",
      "Iteration 170, loss = 0.44858553\n",
      "Iteration 171, loss = 0.44755350\n",
      "Iteration 172, loss = 0.44652908\n",
      "Iteration 173, loss = 0.44551267\n",
      "Iteration 174, loss = 0.44450410\n",
      "Iteration 175, loss = 0.44350349\n",
      "Iteration 176, loss = 0.44251352\n",
      "Iteration 177, loss = 0.44153100\n",
      "Iteration 178, loss = 0.44055595\n",
      "Iteration 179, loss = 0.43958810\n",
      "Iteration 180, loss = 0.43862751\n",
      "Iteration 181, loss = 0.43767403\n",
      "Iteration 182, loss = 0.43672776\n",
      "Iteration 183, loss = 0.43578805\n",
      "Iteration 184, loss = 0.43485501\n",
      "Iteration 185, loss = 0.43392840\n",
      "Iteration 186, loss = 0.43300863\n",
      "Iteration 187, loss = 0.43209514\n",
      "Iteration 188, loss = 0.43118814\n",
      "Iteration 189, loss = 0.43028718\n",
      "Iteration 190, loss = 0.42939221\n",
      "Iteration 191, loss = 0.42850330\n",
      "Iteration 192, loss = 0.42762010\n",
      "Iteration 193, loss = 0.42674280\n",
      "Iteration 194, loss = 0.42587180\n",
      "Iteration 195, loss = 0.42500661\n",
      "Iteration 196, loss = 0.42414689\n",
      "Iteration 197, loss = 0.42329264\n",
      "Iteration 198, loss = 0.42244396\n",
      "Iteration 199, loss = 0.42160052\n",
      "Iteration 200, loss = 0.42076244\n",
      "Iteration 201, loss = 0.41992959\n",
      "Iteration 202, loss = 0.41910187\n",
      "Iteration 203, loss = 0.41827939\n",
      "Iteration 204, loss = 0.41746190\n",
      "Iteration 205, loss = 0.41664935\n",
      "Iteration 206, loss = 0.41584167\n",
      "Iteration 207, loss = 0.41503884\n",
      "Iteration 208, loss = 0.41424075\n",
      "Iteration 209, loss = 0.41344732\n",
      "Iteration 210, loss = 0.41265848\n",
      "Iteration 211, loss = 0.41187419\n",
      "Iteration 212, loss = 0.41109438\n",
      "Iteration 213, loss = 0.41031899\n",
      "Iteration 214, loss = 0.40954797\n",
      "Iteration 215, loss = 0.40878129\n",
      "Iteration 216, loss = 0.40801875\n",
      "Iteration 217, loss = 0.40726044\n",
      "Iteration 218, loss = 0.40650623\n",
      "Iteration 219, loss = 0.40575609\n",
      "Iteration 220, loss = 0.40500995\n",
      "Iteration 221, loss = 0.40426785\n",
      "Iteration 222, loss = 0.40352995\n",
      "Iteration 223, loss = 0.40279624\n",
      "Iteration 224, loss = 0.40206634\n",
      "Iteration 225, loss = 0.40134043\n",
      "Iteration 226, loss = 0.40061844\n",
      "Iteration 227, loss = 0.39990008\n",
      "Iteration 228, loss = 0.39918539\n",
      "Iteration 229, loss = 0.39847432\n",
      "Iteration 230, loss = 0.39776764\n",
      "Iteration 231, loss = 0.39706476\n",
      "Iteration 232, loss = 0.39636539\n",
      "Iteration 233, loss = 0.39566946\n",
      "Iteration 234, loss = 0.39497657\n",
      "Iteration 235, loss = 0.39428674\n",
      "Iteration 236, loss = 0.39360039\n",
      "Iteration 237, loss = 0.39291759\n",
      "Iteration 238, loss = 0.39223805\n",
      "Iteration 239, loss = 0.39156182\n",
      "Iteration 240, loss = 0.39088898\n",
      "Iteration 241, loss = 0.39021961\n",
      "Iteration 242, loss = 0.38955333\n",
      "Iteration 243, loss = 0.38889012\n",
      "Iteration 244, loss = 0.38822993\n",
      "Iteration 245, loss = 0.38757289\n",
      "Iteration 246, loss = 0.38691841\n",
      "Iteration 247, loss = 0.38626618\n",
      "Iteration 248, loss = 0.38561692\n",
      "Iteration 249, loss = 0.38497056\n",
      "Iteration 250, loss = 0.38432710\n",
      "Iteration 251, loss = 0.38368635\n",
      "Iteration 252, loss = 0.38304838\n",
      "Iteration 253, loss = 0.38241322\n",
      "Iteration 254, loss = 0.38178079\n",
      "Iteration 255, loss = 0.38115107\n",
      "Iteration 256, loss = 0.38052399\n",
      "Iteration 257, loss = 0.37989842\n",
      "Iteration 258, loss = 0.37927526\n",
      "Iteration 259, loss = 0.37865382\n",
      "Iteration 260, loss = 0.37803455\n",
      "Iteration 261, loss = 0.37741765\n",
      "Iteration 262, loss = 0.37680318\n",
      "Iteration 263, loss = 0.37619103\n",
      "Iteration 264, loss = 0.37558125\n",
      "Iteration 265, loss = 0.37497382\n",
      "Iteration 266, loss = 0.37436879\n",
      "Iteration 267, loss = 0.37376615\n",
      "Iteration 268, loss = 0.37316584\n",
      "Iteration 269, loss = 0.37256788\n",
      "Iteration 270, loss = 0.37197120\n",
      "Iteration 271, loss = 0.37137646\n",
      "Iteration 272, loss = 0.37078293\n",
      "Iteration 273, loss = 0.37019108\n",
      "Iteration 274, loss = 0.36960001\n",
      "Iteration 275, loss = 0.36901053\n",
      "Iteration 276, loss = 0.36842309\n",
      "Iteration 277, loss = 0.36783757\n",
      "Iteration 278, loss = 0.36725404\n",
      "Iteration 279, loss = 0.36667250\n",
      "Iteration 280, loss = 0.36609293\n",
      "Iteration 281, loss = 0.36551537\n",
      "Iteration 282, loss = 0.36493981\n",
      "Iteration 283, loss = 0.36436625\n",
      "Iteration 284, loss = 0.36379466\n",
      "Iteration 285, loss = 0.36322380\n",
      "Iteration 286, loss = 0.36265484\n",
      "Iteration 287, loss = 0.36208780\n",
      "Iteration 288, loss = 0.36152430\n",
      "Iteration 289, loss = 0.36096489\n",
      "Iteration 290, loss = 0.36040514\n",
      "Iteration 291, loss = 0.35984738\n",
      "Iteration 292, loss = 0.35929204\n",
      "Iteration 293, loss = 0.35873790\n",
      "Iteration 294, loss = 0.35818399\n",
      "Iteration 295, loss = 0.35762864\n",
      "Iteration 296, loss = 0.35706807\n",
      "Iteration 297, loss = 0.35650646\n",
      "Iteration 298, loss = 0.35594453\n",
      "Iteration 299, loss = 0.35538189\n",
      "Iteration 300, loss = 0.35481820\n",
      "Iteration 301, loss = 0.35425339\n",
      "Iteration 302, loss = 0.35368663\n",
      "Iteration 303, loss = 0.35311963\n",
      "Iteration 304, loss = 0.35255320\n",
      "Iteration 305, loss = 0.35198761\n",
      "Iteration 306, loss = 0.35141962\n",
      "Iteration 307, loss = 0.35084941\n",
      "Iteration 308, loss = 0.35027977\n",
      "Iteration 309, loss = 0.34971438\n",
      "Iteration 310, loss = 0.34915305\n",
      "Iteration 311, loss = 0.34859134\n",
      "Iteration 312, loss = 0.34802860\n",
      "Iteration 313, loss = 0.34747008\n",
      "Iteration 314, loss = 0.34691784\n",
      "Iteration 315, loss = 0.34636734\n",
      "Iteration 316, loss = 0.34581751\n",
      "Iteration 317, loss = 0.34526964\n",
      "Iteration 318, loss = 0.34472201\n",
      "Iteration 319, loss = 0.34417402\n",
      "Iteration 320, loss = 0.34362482\n",
      "Iteration 321, loss = 0.34307909\n",
      "Iteration 322, loss = 0.34253484\n",
      "Iteration 323, loss = 0.34198791\n",
      "Iteration 324, loss = 0.34143886\n",
      "Iteration 325, loss = 0.34089212\n",
      "Iteration 326, loss = 0.34035022\n",
      "Iteration 327, loss = 0.33981232\n",
      "Iteration 328, loss = 0.33927198\n",
      "Iteration 329, loss = 0.33873553\n",
      "Iteration 330, loss = 0.33820685\n",
      "Iteration 331, loss = 0.33768523\n",
      "Iteration 332, loss = 0.33717286\n",
      "Iteration 333, loss = 0.33666573\n",
      "Iteration 334, loss = 0.33616339\n",
      "Iteration 335, loss = 0.33566354\n",
      "Iteration 336, loss = 0.33516773\n",
      "Iteration 337, loss = 0.33467788\n",
      "Iteration 338, loss = 0.33419513\n",
      "Iteration 339, loss = 0.33371935\n",
      "Iteration 340, loss = 0.33325191\n",
      "Iteration 341, loss = 0.33278868\n",
      "Iteration 342, loss = 0.33232761\n",
      "Iteration 343, loss = 0.33186972\n",
      "Iteration 344, loss = 0.33141371\n",
      "Iteration 345, loss = 0.33095927\n",
      "Iteration 346, loss = 0.33050645\n",
      "Iteration 347, loss = 0.33005514\n",
      "Iteration 348, loss = 0.32960562\n",
      "Iteration 349, loss = 0.32915880\n",
      "Iteration 350, loss = 0.32871370\n",
      "Iteration 351, loss = 0.32827097\n",
      "Iteration 352, loss = 0.32782966\n",
      "Iteration 353, loss = 0.32738958\n",
      "Iteration 354, loss = 0.32695071\n",
      "Iteration 355, loss = 0.32651313\n",
      "Iteration 356, loss = 0.32607680\n",
      "Iteration 357, loss = 0.32564199\n",
      "Iteration 358, loss = 0.32520835\n",
      "Iteration 359, loss = 0.32477593\n",
      "Iteration 360, loss = 0.32434454\n",
      "Iteration 361, loss = 0.32391441\n",
      "Iteration 362, loss = 0.32348546\n",
      "Iteration 363, loss = 0.32305767\n",
      "Iteration 364, loss = 0.32263112\n",
      "Iteration 365, loss = 0.32220567\n",
      "Iteration 366, loss = 0.32178139\n",
      "Iteration 367, loss = 0.32135827\n",
      "Iteration 368, loss = 0.32093632\n",
      "Iteration 369, loss = 0.32051554\n",
      "Iteration 370, loss = 0.32009597\n",
      "Iteration 371, loss = 0.31967792\n",
      "Iteration 372, loss = 0.31926107\n",
      "Iteration 373, loss = 0.31884534\n",
      "Iteration 374, loss = 0.31843042\n",
      "Iteration 375, loss = 0.31801667\n",
      "Iteration 376, loss = 0.31760401\n",
      "Iteration 377, loss = 0.31719240\n",
      "Iteration 378, loss = 0.31678187\n",
      "Iteration 379, loss = 0.31637247\n",
      "Iteration 380, loss = 0.31596415\n",
      "Iteration 381, loss = 0.31555698\n",
      "Iteration 382, loss = 0.31515098\n",
      "Iteration 383, loss = 0.31474611\n",
      "Iteration 384, loss = 0.31434217\n",
      "Iteration 385, loss = 0.31393933\n",
      "Iteration 386, loss = 0.31353755\n",
      "Iteration 387, loss = 0.31313678\n",
      "Iteration 388, loss = 0.31273708\n",
      "Iteration 389, loss = 0.31233848\n",
      "Iteration 390, loss = 0.31194085\n",
      "Iteration 391, loss = 0.31154421\n",
      "Iteration 392, loss = 0.31114858\n",
      "Iteration 393, loss = 0.31075399\n",
      "Iteration 394, loss = 0.31036042\n",
      "Iteration 395, loss = 0.30996791\n",
      "Iteration 396, loss = 0.30957635\n",
      "Iteration 397, loss = 0.30918576\n",
      "Iteration 398, loss = 0.30879621\n",
      "Iteration 399, loss = 0.30840755\n",
      "Iteration 400, loss = 0.30801985\n",
      "Iteration 401, loss = 0.30763318\n",
      "Iteration 402, loss = 0.30724741\n",
      "Iteration 403, loss = 0.30686265\n",
      "Iteration 404, loss = 0.30647878\n",
      "Iteration 405, loss = 0.30609587\n",
      "Iteration 406, loss = 0.30571391\n",
      "Iteration 407, loss = 0.30533302\n",
      "Iteration 408, loss = 0.30495308\n",
      "Iteration 409, loss = 0.30457407\n",
      "Iteration 410, loss = 0.30419614\n",
      "Iteration 411, loss = 0.30381919\n",
      "Iteration 412, loss = 0.30344338\n",
      "Iteration 413, loss = 0.30306862\n",
      "Iteration 414, loss = 0.30269487\n",
      "Iteration 415, loss = 0.30232262\n",
      "Iteration 416, loss = 0.30195123\n",
      "Iteration 417, loss = 0.30158071\n",
      "Iteration 418, loss = 0.30121104\n",
      "Iteration 419, loss = 0.30084221\n",
      "Iteration 420, loss = 0.30047427\n",
      "Iteration 421, loss = 0.30010720\n",
      "Iteration 422, loss = 0.29974103\n",
      "Iteration 423, loss = 0.29937570\n",
      "Iteration 424, loss = 0.29901127\n",
      "Iteration 425, loss = 0.29864771\n",
      "Iteration 426, loss = 0.29828573\n",
      "Iteration 427, loss = 0.29792420\n",
      "Iteration 428, loss = 0.29756338\n",
      "Iteration 429, loss = 0.29720368\n",
      "Iteration 430, loss = 0.29684487\n",
      "Iteration 431, loss = 0.29648693\n",
      "Iteration 432, loss = 0.29612986\n",
      "Iteration 433, loss = 0.29577365\n",
      "Iteration 434, loss = 0.29541830\n",
      "Iteration 435, loss = 0.29506378\n",
      "Iteration 436, loss = 0.29471011\n",
      "Iteration 437, loss = 0.29435730\n",
      "Iteration 438, loss = 0.29400529\n",
      "Iteration 439, loss = 0.29365411\n",
      "Iteration 440, loss = 0.29330378\n",
      "Iteration 441, loss = 0.29295427\n",
      "Iteration 442, loss = 0.29260556\n",
      "Iteration 443, loss = 0.29225766\n",
      "Iteration 444, loss = 0.29191056\n",
      "Iteration 445, loss = 0.29156428\n",
      "Iteration 446, loss = 0.29121879\n",
      "Iteration 447, loss = 0.29087412\n",
      "Iteration 448, loss = 0.29053024\n",
      "Iteration 449, loss = 0.29018713\n",
      "Iteration 450, loss = 0.28984482\n",
      "Iteration 451, loss = 0.28950329\n",
      "Iteration 452, loss = 0.28916256\n",
      "Iteration 453, loss = 0.28882259\n",
      "Iteration 454, loss = 0.28848340\n",
      "Iteration 455, loss = 0.28814499\n",
      "Iteration 456, loss = 0.28780736\n",
      "Iteration 457, loss = 0.28747047\n",
      "Iteration 458, loss = 0.28713435\n",
      "Iteration 459, loss = 0.28679900\n",
      "Iteration 460, loss = 0.28646439\n",
      "Iteration 461, loss = 0.28613056\n",
      "Iteration 462, loss = 0.28579745\n",
      "Iteration 463, loss = 0.28546511\n",
      "Iteration 464, loss = 0.28513351\n",
      "Iteration 465, loss = 0.28480266\n",
      "Iteration 466, loss = 0.28447255\n",
      "Iteration 467, loss = 0.28414318\n",
      "Iteration 468, loss = 0.28381453\n",
      "Iteration 469, loss = 0.28348664\n",
      "Iteration 470, loss = 0.28315946\n",
      "Iteration 471, loss = 0.28283303\n",
      "Iteration 472, loss = 0.28250734\n",
      "Iteration 473, loss = 0.28218242\n",
      "Iteration 474, loss = 0.28185821\n",
      "Iteration 475, loss = 0.28153469\n",
      "Iteration 476, loss = 0.28121188\n",
      "Iteration 477, loss = 0.28088981\n",
      "Iteration 478, loss = 0.28056851\n",
      "Iteration 479, loss = 0.28024785\n",
      "Iteration 480, loss = 0.27992793\n",
      "Iteration 481, loss = 0.27960871\n",
      "Iteration 482, loss = 0.27929022\n",
      "Iteration 483, loss = 0.27897241\n",
      "Iteration 484, loss = 0.27865532\n",
      "Iteration 485, loss = 0.27833892\n",
      "Iteration 486, loss = 0.27802324\n",
      "Iteration 487, loss = 0.27770822\n",
      "Iteration 488, loss = 0.27739395\n",
      "Iteration 489, loss = 0.27708031\n",
      "Iteration 490, loss = 0.27676739\n",
      "Iteration 491, loss = 0.27645517\n",
      "Iteration 492, loss = 0.27614363\n",
      "Iteration 493, loss = 0.27583281\n",
      "Iteration 494, loss = 0.27552262\n",
      "Iteration 495, loss = 0.27521311\n",
      "Iteration 496, loss = 0.27490430\n",
      "Iteration 497, loss = 0.27459617\n",
      "Iteration 498, loss = 0.27428874\n",
      "Iteration 499, loss = 0.27398195\n",
      "Iteration 500, loss = 0.27367586\n",
      "Iteration 501, loss = 0.27337043\n",
      "Iteration 502, loss = 0.27306572\n",
      "Iteration 503, loss = 0.27276167\n",
      "Iteration 504, loss = 0.27245832\n",
      "Iteration 505, loss = 0.27215560\n",
      "Iteration 506, loss = 0.27185357\n",
      "Iteration 507, loss = 0.27155219\n",
      "Iteration 508, loss = 0.27125147\n",
      "Iteration 509, loss = 0.27095140\n",
      "Iteration 510, loss = 0.27065199\n",
      "Iteration 511, loss = 0.27035323\n",
      "Iteration 512, loss = 0.27005513\n",
      "Iteration 513, loss = 0.26975764\n",
      "Iteration 514, loss = 0.26946083\n",
      "Iteration 515, loss = 0.26916466\n",
      "Iteration 516, loss = 0.26886910\n",
      "Iteration 517, loss = 0.26857421\n",
      "Iteration 518, loss = 0.26827996\n",
      "Iteration 519, loss = 0.26798631\n",
      "Iteration 520, loss = 0.26769332\n",
      "Iteration 521, loss = 0.26740097\n",
      "Iteration 522, loss = 0.26710923\n",
      "Iteration 523, loss = 0.26681813\n",
      "Iteration 524, loss = 0.26652786\n",
      "Iteration 525, loss = 0.26623818\n",
      "Iteration 526, loss = 0.26594914\n",
      "Iteration 527, loss = 0.26566075\n",
      "Iteration 528, loss = 0.26537303\n",
      "Iteration 529, loss = 0.26508590\n",
      "Iteration 530, loss = 0.26479945\n",
      "Iteration 531, loss = 0.26451359\n",
      "Iteration 532, loss = 0.26422863\n",
      "Iteration 533, loss = 0.26394407\n",
      "Iteration 534, loss = 0.26366025\n",
      "Iteration 535, loss = 0.26337699\n",
      "Iteration 536, loss = 0.26309443\n",
      "Iteration 537, loss = 0.26281243\n",
      "Iteration 538, loss = 0.26253107\n",
      "Iteration 539, loss = 0.26225032\n",
      "Iteration 540, loss = 0.26197013\n",
      "Iteration 541, loss = 0.26169077\n",
      "Iteration 542, loss = 0.26141191\n",
      "Iteration 543, loss = 0.26113370\n",
      "Iteration 544, loss = 0.26085608\n",
      "Iteration 545, loss = 0.26057906\n",
      "Iteration 546, loss = 0.26030266\n",
      "Iteration 547, loss = 0.26002680\n",
      "Iteration 548, loss = 0.25975163\n",
      "Iteration 549, loss = 0.25947698\n",
      "Iteration 550, loss = 0.25920320\n",
      "Iteration 551, loss = 0.25892991\n",
      "Iteration 552, loss = 0.25865721\n",
      "Iteration 553, loss = 0.25838522\n",
      "Iteration 554, loss = 0.25811367\n",
      "Iteration 555, loss = 0.25784286\n",
      "Iteration 556, loss = 0.25757256\n",
      "Iteration 557, loss = 0.25730287\n",
      "Iteration 558, loss = 0.25703399\n",
      "Iteration 559, loss = 0.25676559\n",
      "Iteration 560, loss = 0.25649798\n",
      "Iteration 561, loss = 0.25623082\n",
      "Iteration 562, loss = 0.25596423\n",
      "Iteration 563, loss = 0.25569829\n",
      "Iteration 564, loss = 0.25543283\n",
      "Iteration 565, loss = 0.25516805\n",
      "Iteration 566, loss = 0.25490378\n",
      "Iteration 567, loss = 0.25464006\n",
      "Iteration 568, loss = 0.25437702\n",
      "Iteration 569, loss = 0.25411439\n",
      "Iteration 570, loss = 0.25385243\n",
      "Iteration 571, loss = 0.25359100\n",
      "Iteration 572, loss = 0.25333010\n",
      "Iteration 573, loss = 0.25306980\n",
      "Iteration 574, loss = 0.25281004\n",
      "Iteration 575, loss = 0.25255081\n",
      "Iteration 576, loss = 0.25229218\n",
      "Iteration 577, loss = 0.25203404\n",
      "Iteration 578, loss = 0.25177647\n",
      "Iteration 579, loss = 0.25151958\n",
      "Iteration 580, loss = 0.25126309\n",
      "Iteration 581, loss = 0.25100723\n",
      "Iteration 582, loss = 0.25075194\n",
      "Iteration 583, loss = 0.25049713\n",
      "Iteration 584, loss = 0.25024288\n",
      "Iteration 585, loss = 0.24998931\n",
      "Iteration 586, loss = 0.24973614\n",
      "Iteration 587, loss = 0.24948360\n",
      "Iteration 588, loss = 0.24923160\n",
      "Iteration 589, loss = 0.24898011\n",
      "Iteration 590, loss = 0.24872911\n",
      "Iteration 591, loss = 0.24847876\n",
      "Iteration 592, loss = 0.24822883\n",
      "Iteration 593, loss = 0.24797953\n",
      "Iteration 594, loss = 0.24773082\n",
      "Iteration 595, loss = 0.24748256\n",
      "Iteration 596, loss = 0.24723482\n",
      "Iteration 597, loss = 0.24698764\n",
      "Iteration 598, loss = 0.24674098\n",
      "Iteration 599, loss = 0.24649483\n",
      "Iteration 600, loss = 0.24624926\n",
      "Iteration 601, loss = 0.24600419\n",
      "Iteration 602, loss = 0.24575958\n",
      "Iteration 603, loss = 0.24551553\n",
      "Iteration 604, loss = 0.24527206\n",
      "Iteration 605, loss = 0.24502901\n",
      "Iteration 606, loss = 0.24478647\n",
      "Iteration 607, loss = 0.24454453\n",
      "Iteration 608, loss = 0.24430297\n",
      "Iteration 609, loss = 0.24406199\n",
      "Iteration 610, loss = 0.24382169\n",
      "Iteration 611, loss = 0.24358184\n",
      "Iteration 612, loss = 0.24334249\n",
      "Iteration 613, loss = 0.24310365\n",
      "Iteration 614, loss = 0.24286546\n",
      "Iteration 615, loss = 0.24262752\n",
      "Iteration 616, loss = 0.24239032\n",
      "Iteration 617, loss = 0.24215351\n",
      "Iteration 618, loss = 0.24191721\n",
      "Iteration 619, loss = 0.24168138\n",
      "Iteration 620, loss = 0.24144602\n",
      "Iteration 621, loss = 0.24121130\n",
      "Iteration 622, loss = 0.24097689\n",
      "Iteration 623, loss = 0.24074310\n",
      "Iteration 624, loss = 0.24050979\n",
      "Iteration 625, loss = 0.24027697\n",
      "Iteration 626, loss = 0.24004458\n",
      "Iteration 627, loss = 0.23981271\n",
      "Iteration 628, loss = 0.23958138\n",
      "Iteration 629, loss = 0.23935048\n",
      "Iteration 630, loss = 0.23912006\n",
      "Iteration 631, loss = 0.23889017\n",
      "Iteration 632, loss = 0.23866074\n",
      "Iteration 633, loss = 0.23843175\n",
      "Iteration 634, loss = 0.23820326\n",
      "Iteration 635, loss = 0.23797536\n",
      "Iteration 636, loss = 0.23774774\n",
      "Iteration 637, loss = 0.23752068\n",
      "Iteration 638, loss = 0.23729413\n",
      "Iteration 639, loss = 0.23706804\n",
      "Iteration 640, loss = 0.23684240\n",
      "Iteration 641, loss = 0.23661722\n",
      "Iteration 642, loss = 0.23639248\n",
      "Iteration 643, loss = 0.23616834\n",
      "Iteration 644, loss = 0.23594449\n",
      "Iteration 645, loss = 0.23572118\n",
      "Iteration 646, loss = 0.23549836\n",
      "Iteration 647, loss = 0.23527599\n",
      "Iteration 648, loss = 0.23505411\n",
      "Iteration 649, loss = 0.23483262\n",
      "Iteration 650, loss = 0.23461160\n",
      "Iteration 651, loss = 0.23439108\n",
      "Iteration 652, loss = 0.23417101\n",
      "Iteration 653, loss = 0.23395137\n",
      "Iteration 654, loss = 0.23373221\n",
      "Iteration 655, loss = 0.23351359\n",
      "Iteration 656, loss = 0.23329541\n",
      "Iteration 657, loss = 0.23307773\n",
      "Iteration 658, loss = 0.23286049\n",
      "Iteration 659, loss = 0.23264371\n",
      "Iteration 660, loss = 0.23242745\n",
      "Iteration 661, loss = 0.23221158\n",
      "Iteration 662, loss = 0.23199621\n",
      "Iteration 663, loss = 0.23178124\n",
      "Iteration 664, loss = 0.23156680\n",
      "Iteration 665, loss = 0.23135287\n",
      "Iteration 666, loss = 0.23113951\n",
      "Iteration 667, loss = 0.23092656\n",
      "Iteration 668, loss = 0.23071411\n",
      "Iteration 669, loss = 0.23050211\n",
      "Iteration 670, loss = 0.23029051\n",
      "Iteration 671, loss = 0.23007938\n",
      "Iteration 672, loss = 0.22986863\n",
      "Iteration 673, loss = 0.22965846\n",
      "Iteration 674, loss = 0.22944859\n",
      "Iteration 675, loss = 0.22923919\n",
      "Iteration 676, loss = 0.22903023\n",
      "Iteration 677, loss = 0.22882180\n",
      "Iteration 678, loss = 0.22861367\n",
      "Iteration 679, loss = 0.22840603\n",
      "Iteration 680, loss = 0.22819878\n",
      "Iteration 681, loss = 0.22799203\n",
      "Iteration 682, loss = 0.22778567\n",
      "Iteration 683, loss = 0.22757975\n",
      "Iteration 684, loss = 0.22737426\n",
      "Iteration 685, loss = 0.22716915\n",
      "Iteration 686, loss = 0.22696452\n",
      "Iteration 687, loss = 0.22676029\n",
      "Iteration 688, loss = 0.22655650\n",
      "Iteration 689, loss = 0.22635308\n",
      "Iteration 690, loss = 0.22615011\n",
      "Iteration 691, loss = 0.22594762\n",
      "Iteration 692, loss = 0.22574546\n",
      "Iteration 693, loss = 0.22554375\n",
      "Iteration 694, loss = 0.22534246\n",
      "Iteration 695, loss = 0.22514160\n",
      "Iteration 696, loss = 0.22494126\n",
      "Iteration 697, loss = 0.22474121\n",
      "Iteration 698, loss = 0.22454163\n",
      "Iteration 699, loss = 0.22434245\n",
      "Iteration 700, loss = 0.22414365\n",
      "Iteration 701, loss = 0.22394531\n",
      "Iteration 702, loss = 0.22374734\n",
      "Iteration 703, loss = 0.22354983\n",
      "Iteration 704, loss = 0.22335267\n",
      "Iteration 705, loss = 0.22315591\n",
      "Iteration 706, loss = 0.22295966\n",
      "Iteration 707, loss = 0.22276370\n",
      "Iteration 708, loss = 0.22256818\n",
      "Iteration 709, loss = 0.22237306\n",
      "Iteration 710, loss = 0.22217833\n",
      "Iteration 711, loss = 0.22198400\n",
      "Iteration 712, loss = 0.22179012\n",
      "Iteration 713, loss = 0.22159659\n",
      "Iteration 714, loss = 0.22140349\n",
      "Iteration 715, loss = 0.22121080\n",
      "Iteration 716, loss = 0.22101845\n",
      "Iteration 717, loss = 0.22082657\n",
      "Iteration 718, loss = 0.22063506\n",
      "Iteration 719, loss = 0.22044390\n",
      "Iteration 720, loss = 0.22025317\n",
      "Iteration 721, loss = 0.22006279\n",
      "Iteration 722, loss = 0.21987281\n",
      "Iteration 723, loss = 0.21968330\n",
      "Iteration 724, loss = 0.21949409\n",
      "Iteration 725, loss = 0.21930527\n",
      "Iteration 726, loss = 0.21911686\n",
      "Iteration 727, loss = 0.21892879\n",
      "Iteration 728, loss = 0.21874110\n",
      "Iteration 729, loss = 0.21855395\n",
      "Iteration 730, loss = 0.21836696\n",
      "Iteration 731, loss = 0.21818037\n",
      "Iteration 732, loss = 0.21799411\n",
      "Iteration 733, loss = 0.21780823\n",
      "Iteration 734, loss = 0.21762278\n",
      "Iteration 735, loss = 0.21743758\n",
      "Iteration 736, loss = 0.21725284\n",
      "Iteration 737, loss = 0.21706839\n",
      "Iteration 738, loss = 0.21688434\n",
      "Iteration 739, loss = 0.21670064\n",
      "Iteration 740, loss = 0.21651730\n",
      "Iteration 741, loss = 0.21633445\n",
      "Iteration 742, loss = 0.21615178\n",
      "Iteration 743, loss = 0.21596956\n",
      "Iteration 744, loss = 0.21578768\n",
      "Iteration 745, loss = 0.21560618\n",
      "Iteration 746, loss = 0.21542504\n",
      "Iteration 747, loss = 0.21524434\n",
      "Iteration 748, loss = 0.21506387\n",
      "Iteration 749, loss = 0.21488382\n",
      "Iteration 750, loss = 0.21470413\n",
      "Iteration 751, loss = 0.21452481\n",
      "Iteration 752, loss = 0.21434600\n",
      "Iteration 753, loss = 0.21416756\n",
      "Iteration 754, loss = 0.21398960\n",
      "Iteration 755, loss = 0.21381182\n",
      "Iteration 756, loss = 0.21363449\n",
      "Iteration 757, loss = 0.21345748\n",
      "Iteration 758, loss = 0.21328082\n",
      "Iteration 759, loss = 0.21310451\n",
      "Iteration 760, loss = 0.21292856\n",
      "Iteration 761, loss = 0.21275301\n",
      "Iteration 762, loss = 0.21257778\n",
      "Iteration 763, loss = 0.21240291\n",
      "Iteration 764, loss = 0.21222836\n",
      "Iteration 765, loss = 0.21205420\n",
      "Iteration 766, loss = 0.21188035\n",
      "Iteration 767, loss = 0.21170688\n",
      "Iteration 768, loss = 0.21153378\n",
      "Iteration 769, loss = 0.21136105\n",
      "Iteration 770, loss = 0.21118867\n",
      "Iteration 771, loss = 0.21101663\n",
      "Iteration 772, loss = 0.21084492\n",
      "Iteration 773, loss = 0.21067358\n",
      "Iteration 774, loss = 0.21050256\n",
      "Iteration 775, loss = 0.21033196\n",
      "Iteration 776, loss = 0.21016163\n",
      "Iteration 777, loss = 0.20999168\n",
      "Iteration 778, loss = 0.20982206\n",
      "Iteration 779, loss = 0.20965275\n",
      "Iteration 780, loss = 0.20948374\n",
      "Iteration 781, loss = 0.20931509\n",
      "Iteration 782, loss = 0.20914677\n",
      "Iteration 783, loss = 0.20897884\n",
      "Iteration 784, loss = 0.20881121\n",
      "Iteration 785, loss = 0.20864391\n",
      "Iteration 786, loss = 0.20847693\n",
      "Iteration 787, loss = 0.20831031\n",
      "Iteration 788, loss = 0.20814406\n",
      "Iteration 789, loss = 0.20797814\n",
      "Iteration 790, loss = 0.20781258\n",
      "Iteration 791, loss = 0.20764732\n",
      "Iteration 792, loss = 0.20748240\n",
      "Iteration 793, loss = 0.20731781\n",
      "Iteration 794, loss = 0.20715355\n",
      "Iteration 795, loss = 0.20698961\n",
      "Iteration 796, loss = 0.20682600\n",
      "Iteration 797, loss = 0.20666277\n",
      "Iteration 798, loss = 0.20649981\n",
      "Iteration 799, loss = 0.20633723\n",
      "Iteration 800, loss = 0.20617495\n",
      "Iteration 801, loss = 0.20601301\n",
      "Iteration 802, loss = 0.20585138\n",
      "Iteration 803, loss = 0.20569007\n",
      "Iteration 804, loss = 0.20552910\n",
      "Iteration 805, loss = 0.20536844\n",
      "Iteration 806, loss = 0.20520810\n",
      "Iteration 807, loss = 0.20504809\n",
      "Iteration 808, loss = 0.20488838\n",
      "Iteration 809, loss = 0.20472900\n",
      "Iteration 810, loss = 0.20456993\n",
      "Iteration 811, loss = 0.20441118\n",
      "Iteration 812, loss = 0.20425275\n",
      "Iteration 813, loss = 0.20409463\n",
      "Iteration 814, loss = 0.20393682\n",
      "Iteration 815, loss = 0.20377933\n",
      "Iteration 816, loss = 0.20362215\n",
      "Iteration 817, loss = 0.20346528\n",
      "Iteration 818, loss = 0.20330872\n",
      "Iteration 819, loss = 0.20315247\n",
      "Iteration 820, loss = 0.20299654\n",
      "Iteration 821, loss = 0.20284090\n",
      "Iteration 822, loss = 0.20268558\n",
      "Iteration 823, loss = 0.20253058\n",
      "Iteration 824, loss = 0.20237587\n",
      "Iteration 825, loss = 0.20222146\n",
      "Iteration 826, loss = 0.20206738\n",
      "Iteration 827, loss = 0.20191360\n",
      "Iteration 828, loss = 0.20176012\n",
      "Iteration 829, loss = 0.20160693\n",
      "Iteration 830, loss = 0.20145405\n",
      "Iteration 831, loss = 0.20130147\n",
      "Iteration 832, loss = 0.20114920\n",
      "Iteration 833, loss = 0.20099723\n",
      "Iteration 834, loss = 0.20084555\n",
      "Iteration 835, loss = 0.20069417\n",
      "Iteration 836, loss = 0.20054310\n",
      "Iteration 837, loss = 0.20039231\n",
      "Iteration 838, loss = 0.20024183\n",
      "Iteration 839, loss = 0.20009164\n",
      "Iteration 840, loss = 0.19994175\n",
      "Iteration 841, loss = 0.19979215\n",
      "Iteration 842, loss = 0.19964285\n",
      "Iteration 843, loss = 0.19949383\n",
      "Iteration 844, loss = 0.19934512\n",
      "Iteration 845, loss = 0.19919670\n",
      "Iteration 846, loss = 0.19904856\n",
      "Iteration 847, loss = 0.19890072\n",
      "Iteration 848, loss = 0.19875324\n",
      "Iteration 849, loss = 0.19860604\n",
      "Iteration 850, loss = 0.19845913\n",
      "Iteration 851, loss = 0.19831251\n",
      "Iteration 852, loss = 0.19816619\n",
      "Iteration 853, loss = 0.19802015\n",
      "Iteration 854, loss = 0.19787440\n",
      "Iteration 855, loss = 0.19772893\n",
      "Iteration 856, loss = 0.19758376\n",
      "Iteration 857, loss = 0.19743886\n",
      "Iteration 858, loss = 0.19729426\n",
      "Iteration 859, loss = 0.19714993\n",
      "Iteration 860, loss = 0.19700589\n",
      "Iteration 861, loss = 0.19686214\n",
      "Iteration 862, loss = 0.19671866\n",
      "Iteration 863, loss = 0.19657546\n",
      "Iteration 864, loss = 0.19643254\n",
      "Iteration 865, loss = 0.19628990\n",
      "Iteration 866, loss = 0.19614755\n",
      "Iteration 867, loss = 0.19600546\n",
      "Iteration 868, loss = 0.19586366\n",
      "Iteration 869, loss = 0.19572213\n",
      "Iteration 870, loss = 0.19558088\n",
      "Iteration 871, loss = 0.19543990\n",
      "Iteration 872, loss = 0.19529920\n",
      "Iteration 873, loss = 0.19515877\n",
      "Iteration 874, loss = 0.19501861\n",
      "Iteration 875, loss = 0.19487873\n",
      "Iteration 876, loss = 0.19473912\n",
      "Iteration 877, loss = 0.19459978\n",
      "Iteration 878, loss = 0.19446071\n",
      "Iteration 879, loss = 0.19432191\n",
      "Iteration 880, loss = 0.19418338\n",
      "Iteration 881, loss = 0.19404512\n",
      "Iteration 882, loss = 0.19390713\n",
      "Iteration 883, loss = 0.19376941\n",
      "Iteration 884, loss = 0.19363195\n",
      "Iteration 885, loss = 0.19349476\n",
      "Iteration 886, loss = 0.19335783\n",
      "Iteration 887, loss = 0.19322117\n",
      "Iteration 888, loss = 0.19308478\n",
      "Iteration 889, loss = 0.19294864\n",
      "Iteration 890, loss = 0.19281277\n",
      "Iteration 891, loss = 0.19267716\n",
      "Iteration 892, loss = 0.19254182\n",
      "Iteration 893, loss = 0.19240674\n",
      "Iteration 894, loss = 0.19227192\n",
      "Iteration 895, loss = 0.19213737\n",
      "Iteration 896, loss = 0.19200308\n",
      "Iteration 897, loss = 0.19186904\n",
      "Iteration 898, loss = 0.19173526\n",
      "Iteration 899, loss = 0.19160175\n",
      "Iteration 900, loss = 0.19146836\n",
      "Iteration 901, loss = 0.19133511\n",
      "Iteration 902, loss = 0.19120208\n",
      "Iteration 903, loss = 0.19106929\n",
      "Iteration 904, loss = 0.19093673\n",
      "Iteration 905, loss = 0.19080441\n",
      "Iteration 906, loss = 0.19067226\n",
      "Iteration 907, loss = 0.19054032\n",
      "Iteration 908, loss = 0.19040860\n",
      "Iteration 909, loss = 0.19027713\n",
      "Iteration 910, loss = 0.19014589\n",
      "Iteration 911, loss = 0.19001487\n",
      "Iteration 912, loss = 0.18988410\n",
      "Iteration 913, loss = 0.18975358\n",
      "Iteration 914, loss = 0.18962384\n",
      "Iteration 915, loss = 0.18949434\n",
      "Iteration 916, loss = 0.18936510\n",
      "Iteration 917, loss = 0.18923610\n",
      "Iteration 918, loss = 0.18910736\n",
      "Iteration 919, loss = 0.18897887\n",
      "Iteration 920, loss = 0.18885065\n",
      "Iteration 921, loss = 0.18872265\n",
      "Iteration 922, loss = 0.18859491\n",
      "Iteration 923, loss = 0.18846741\n",
      "Iteration 924, loss = 0.18834016\n",
      "Iteration 925, loss = 0.18821314\n",
      "Iteration 926, loss = 0.18808637\n",
      "Iteration 927, loss = 0.18795985\n",
      "Iteration 928, loss = 0.18783356\n",
      "Iteration 929, loss = 0.18770752\n",
      "Iteration 930, loss = 0.18758172\n",
      "Iteration 931, loss = 0.18745616\n",
      "Iteration 932, loss = 0.18733085\n",
      "Iteration 933, loss = 0.18720576\n",
      "Iteration 934, loss = 0.18708092\n",
      "Iteration 935, loss = 0.18695632\n",
      "Iteration 936, loss = 0.18683194\n",
      "Iteration 937, loss = 0.18670781\n",
      "Iteration 938, loss = 0.18658391\n",
      "Iteration 939, loss = 0.18646025\n",
      "Iteration 940, loss = 0.18633682\n",
      "Iteration 941, loss = 0.18621362\n",
      "Iteration 942, loss = 0.18609066\n",
      "Iteration 943, loss = 0.18596793\n",
      "Iteration 944, loss = 0.18584543\n",
      "Iteration 945, loss = 0.18572316\n",
      "Iteration 946, loss = 0.18560112\n",
      "Iteration 947, loss = 0.18547931\n",
      "Iteration 948, loss = 0.18535773\n",
      "Iteration 949, loss = 0.18523638\n",
      "Iteration 950, loss = 0.18511526\n",
      "Iteration 951, loss = 0.18499437\n",
      "Iteration 952, loss = 0.18487370\n",
      "Iteration 953, loss = 0.18475326\n",
      "Iteration 954, loss = 0.18463304\n",
      "Iteration 955, loss = 0.18451305\n",
      "Iteration 956, loss = 0.18439329\n",
      "Iteration 957, loss = 0.18427375\n",
      "Iteration 958, loss = 0.18415443\n",
      "Iteration 959, loss = 0.18403534\n",
      "Iteration 960, loss = 0.18391648\n",
      "Iteration 961, loss = 0.18379783\n",
      "Iteration 962, loss = 0.18367940\n",
      "Iteration 963, loss = 0.18356120\n",
      "Iteration 964, loss = 0.18344322\n",
      "Iteration 965, loss = 0.18332545\n",
      "Iteration 966, loss = 0.18320791\n",
      "Iteration 967, loss = 0.18309059\n",
      "Iteration 968, loss = 0.18297350\n",
      "Iteration 969, loss = 0.18285661\n",
      "Iteration 970, loss = 0.18273995\n",
      "Iteration 971, loss = 0.18262350\n",
      "Iteration 972, loss = 0.18250727\n",
      "Iteration 973, loss = 0.18239126\n",
      "Iteration 974, loss = 0.18227546\n",
      "Iteration 975, loss = 0.18215988\n",
      "Iteration 976, loss = 0.18204451\n",
      "Iteration 977, loss = 0.18192936\n",
      "Iteration 978, loss = 0.18181442\n",
      "Iteration 979, loss = 0.18169969\n",
      "Iteration 980, loss = 0.18158518\n",
      "Iteration 981, loss = 0.18147088\n",
      "Iteration 982, loss = 0.18135679\n",
      "Iteration 983, loss = 0.18124291\n",
      "Iteration 984, loss = 0.18112924\n",
      "Iteration 985, loss = 0.18101579\n",
      "Iteration 986, loss = 0.18090254\n",
      "Iteration 987, loss = 0.18078950\n",
      "Iteration 988, loss = 0.18067667\n",
      "Iteration 989, loss = 0.18056405\n",
      "Iteration 990, loss = 0.18045164\n",
      "Iteration 991, loss = 0.18033943\n",
      "Iteration 992, loss = 0.18022744\n",
      "Iteration 993, loss = 0.18011564\n",
      "Iteration 994, loss = 0.18000405\n",
      "Iteration 995, loss = 0.17989267\n",
      "Iteration 996, loss = 0.17978150\n",
      "Iteration 997, loss = 0.17967052\n",
      "Iteration 998, loss = 0.17955976\n",
      "Iteration 999, loss = 0.17944919\n",
      "Iteration 1000, loss = 0.17933883\n",
      "Iteration 1, loss = 1.35652749\n",
      "Iteration 2, loss = 7.61823747\n",
      "Iteration 3, loss = 6.56289214\n",
      "Iteration 4, loss = 2.34739817\n",
      "Iteration 5, loss = 2.17906063\n",
      "Iteration 6, loss = 1.68909108\n",
      "Iteration 7, loss = 0.82257109\n",
      "Iteration 8, loss = 0.75929698\n",
      "Iteration 9, loss = 0.87045474\n",
      "Iteration 10, loss = 0.81510445\n",
      "Iteration 11, loss = 0.74455888\n",
      "Iteration 12, loss = 0.69390257\n",
      "Iteration 13, loss = 0.63812403\n",
      "Iteration 14, loss = 0.59012755\n",
      "Iteration 15, loss = 0.53752963\n",
      "Iteration 16, loss = 0.49826417\n",
      "Iteration 17, loss = 0.46988952\n",
      "Iteration 18, loss = 0.45238378\n",
      "Iteration 19, loss = 0.41061576\n",
      "Iteration 20, loss = 0.38847258\n",
      "Iteration 21, loss = 0.37746835\n",
      "Iteration 22, loss = 0.36182499\n",
      "Iteration 23, loss = 0.33427811\n",
      "Iteration 24, loss = 0.31688273\n",
      "Iteration 25, loss = 0.30509151\n",
      "Iteration 26, loss = 0.28746695\n",
      "Iteration 27, loss = 0.27457806\n",
      "Iteration 28, loss = 0.26184453\n",
      "Iteration 29, loss = 0.24606778\n",
      "Iteration 30, loss = 0.23658529\n",
      "Iteration 31, loss = 0.22284147\n",
      "Iteration 32, loss = 0.21338166\n",
      "Iteration 33, loss = 0.20326170\n",
      "Iteration 34, loss = 0.19352864\n",
      "Iteration 35, loss = 0.18633083\n",
      "Iteration 36, loss = 0.17746272\n",
      "Iteration 37, loss = 0.17247137\n",
      "Iteration 38, loss = 0.16506320\n",
      "Iteration 39, loss = 0.16128003\n",
      "Iteration 40, loss = 0.15559196\n",
      "Iteration 41, loss = 0.15266166\n",
      "Iteration 42, loss = 0.14831912\n",
      "Iteration 43, loss = 0.14586926\n",
      "Iteration 44, loss = 0.14266760\n",
      "Iteration 45, loss = 0.14059433\n",
      "Iteration 46, loss = 0.13818003\n",
      "Iteration 47, loss = 0.13639530\n",
      "Iteration 48, loss = 0.13457303\n",
      "Iteration 49, loss = 0.13307754\n",
      "Iteration 50, loss = 0.13163096\n",
      "Iteration 51, loss = 0.13036171\n",
      "Iteration 52, loss = 0.12915874\n",
      "Iteration 53, loss = 0.12806400\n",
      "Iteration 54, loss = 0.12701483\n",
      "Iteration 55, loss = 0.12609087\n",
      "Iteration 56, loss = 0.12512485\n",
      "Iteration 57, loss = 0.12433489\n",
      "Iteration 58, loss = 0.12341850\n",
      "Iteration 59, loss = 0.12275162\n",
      "Iteration 60, loss = 0.12187826\n",
      "Iteration 61, loss = 0.12129708\n",
      "Iteration 62, loss = 0.12046787\n",
      "Iteration 63, loss = 0.11994759\n",
      "Iteration 64, loss = 0.11918483\n",
      "Iteration 65, loss = 0.11869225\n",
      "Iteration 66, loss = 0.11801311\n",
      "Iteration 67, loss = 0.11750870\n",
      "Iteration 68, loss = 0.11692493\n",
      "Iteration 69, loss = 0.11639527\n",
      "Iteration 70, loss = 0.11590272\n",
      "Iteration 71, loss = 0.11535743\n",
      "Iteration 72, loss = 0.11492279\n",
      "Iteration 73, loss = 0.11439077\n",
      "Iteration 74, loss = 0.11396959\n",
      "Iteration 75, loss = 0.11348588\n",
      "Iteration 76, loss = 0.11304816\n",
      "Iteration 77, loss = 0.11262270\n",
      "Iteration 78, loss = 0.11216986\n",
      "Iteration 79, loss = 0.11177914\n",
      "Iteration 80, loss = 0.11134015\n",
      "Iteration 81, loss = 0.11094774\n",
      "Iteration 82, loss = 0.11054716\n",
      "Iteration 83, loss = 0.11014182\n",
      "Iteration 84, loss = 0.10977121\n",
      "Iteration 85, loss = 0.10937381\n",
      "Iteration 86, loss = 0.10900592\n",
      "Iteration 87, loss = 0.10863698\n",
      "Iteration 88, loss = 0.10826287\n",
      "Iteration 89, loss = 0.10791447\n",
      "Iteration 90, loss = 0.10755236\n",
      "Iteration 91, loss = 0.10720374\n",
      "Iteration 92, loss = 0.10686457\n",
      "Iteration 93, loss = 0.10651738\n",
      "Iteration 94, loss = 0.10618765\n",
      "Iteration 95, loss = 0.10585728\n",
      "Iteration 96, loss = 0.10552763\n",
      "Iteration 97, loss = 0.10521124\n",
      "Iteration 98, loss = 0.10489149\n",
      "Iteration 99, loss = 0.10457737\n",
      "Iteration 100, loss = 0.10427173\n",
      "Iteration 101, loss = 0.10396377\n",
      "Iteration 102, loss = 0.10366292\n",
      "Iteration 103, loss = 0.10336746\n",
      "Iteration 104, loss = 0.10307137\n",
      "Iteration 105, loss = 0.10278203\n",
      "Iteration 106, loss = 0.10249680\n",
      "Iteration 107, loss = 0.10221206\n",
      "Iteration 108, loss = 0.10193323\n",
      "Iteration 109, loss = 0.10165815\n",
      "Iteration 110, loss = 0.10138413\n",
      "Iteration 111, loss = 0.10111519\n",
      "Iteration 112, loss = 0.10084987\n",
      "Iteration 113, loss = 0.10058599\n",
      "Iteration 114, loss = 0.10032630\n",
      "Iteration 115, loss = 0.10007036\n",
      "Iteration 116, loss = 0.09981609\n",
      "Iteration 117, loss = 0.09956514\n",
      "Iteration 118, loss = 0.09931798\n",
      "Iteration 119, loss = 0.09907288\n",
      "Iteration 120, loss = 0.09883045\n",
      "Iteration 121, loss = 0.09859151\n",
      "Iteration 122, loss = 0.09835500\n",
      "Iteration 123, loss = 0.09812071\n",
      "Iteration 124, loss = 0.09788944\n",
      "Iteration 125, loss = 0.09766098\n",
      "Iteration 126, loss = 0.09743460\n",
      "Iteration 127, loss = 0.09721068\n",
      "Iteration 128, loss = 0.09698951\n",
      "Iteration 129, loss = 0.09677061\n",
      "Iteration 130, loss = 0.09655377\n",
      "Iteration 131, loss = 0.09633934\n",
      "Iteration 132, loss = 0.09612728\n",
      "Iteration 133, loss = 0.09591730\n",
      "Iteration 134, loss = 0.09570935\n",
      "Iteration 135, loss = 0.09550364\n",
      "Iteration 136, loss = 0.09530007\n",
      "Iteration 137, loss = 0.09509842\n",
      "Iteration 138, loss = 0.09489871\n",
      "Iteration 139, loss = 0.09470103\n",
      "Iteration 140, loss = 0.09450530\n",
      "Iteration 141, loss = 0.09431141\n",
      "Iteration 142, loss = 0.09411930\n",
      "Iteration 143, loss = 0.09392907\n",
      "Iteration 144, loss = 0.09374068\n",
      "Iteration 145, loss = 0.09355402\n",
      "Iteration 146, loss = 0.09336907\n",
      "Iteration 147, loss = 0.09318579\n",
      "Iteration 148, loss = 0.09300434\n",
      "Iteration 149, loss = 0.09282452\n",
      "Iteration 150, loss = 0.09264629\n",
      "Iteration 151, loss = 0.09246961\n",
      "Iteration 152, loss = 0.09229446\n",
      "Iteration 153, loss = 0.09212082\n",
      "Iteration 154, loss = 0.09194866\n",
      "Iteration 155, loss = 0.09177796\n",
      "Iteration 156, loss = 0.09160869\n",
      "Iteration 157, loss = 0.09144084\n",
      "Iteration 158, loss = 0.09127439\n",
      "Iteration 159, loss = 0.09110930\n",
      "Iteration 160, loss = 0.09094558\n",
      "Iteration 161, loss = 0.09078317\n",
      "Iteration 162, loss = 0.09062208\n",
      "Iteration 163, loss = 0.09046227\n",
      "Iteration 164, loss = 0.09030373\n",
      "Iteration 165, loss = 0.09014644\n",
      "Iteration 166, loss = 0.08999040\n",
      "Iteration 167, loss = 0.08983558\n",
      "Iteration 168, loss = 0.08968195\n",
      "Iteration 169, loss = 0.08952950\n",
      "Iteration 170, loss = 0.08937821\n",
      "Iteration 171, loss = 0.08922805\n",
      "Iteration 172, loss = 0.08907903\n",
      "Iteration 173, loss = 0.08893110\n",
      "Iteration 174, loss = 0.08878427\n",
      "Iteration 175, loss = 0.08863852\n",
      "Iteration 176, loss = 0.08849383\n",
      "Iteration 177, loss = 0.08835020\n",
      "Iteration 178, loss = 0.08820759\n",
      "Iteration 179, loss = 0.08806601\n",
      "Iteration 180, loss = 0.08792543\n",
      "Iteration 181, loss = 0.08778585\n",
      "Iteration 182, loss = 0.08764726\n",
      "Iteration 183, loss = 0.08750965\n",
      "Iteration 184, loss = 0.08737301\n",
      "Iteration 185, loss = 0.08723732\n",
      "Iteration 186, loss = 0.08710259\n",
      "Iteration 187, loss = 0.08696880\n",
      "Iteration 188, loss = 0.08683597\n",
      "Iteration 189, loss = 0.08670412\n",
      "Iteration 190, loss = 0.08657332\n",
      "Iteration 191, loss = 0.08644373\n",
      "Iteration 192, loss = 0.08631580\n",
      "Iteration 193, loss = 0.08619060\n",
      "Iteration 194, loss = 0.08607083\n",
      "Iteration 195, loss = 0.08596370\n",
      "Iteration 196, loss = 0.08588707\n",
      "Iteration 197, loss = 0.08589532\n",
      "Iteration 198, loss = 0.08610139\n",
      "Iteration 199, loss = 0.08696304\n",
      "Iteration 200, loss = 0.08904321\n",
      "Iteration 201, loss = 0.09625394\n",
      "Iteration 202, loss = 0.10394344\n",
      "Iteration 203, loss = 0.12948167\n",
      "Iteration 204, loss = 0.10196014\n",
      "Iteration 205, loss = 0.08772481\n",
      "Iteration 206, loss = 0.08596064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35233713\n",
      "Iteration 2, loss = 7.69527616\n",
      "Iteration 3, loss = 6.63738584\n",
      "Iteration 4, loss = 2.38485357\n",
      "Iteration 5, loss = 2.22182610\n",
      "Iteration 6, loss = 1.69673610\n",
      "Iteration 7, loss = 0.84644767\n",
      "Iteration 8, loss = 0.76424588\n",
      "Iteration 9, loss = 0.87397031\n",
      "Iteration 10, loss = 0.82540106\n",
      "Iteration 11, loss = 0.76061042\n",
      "Iteration 12, loss = 0.71239507\n",
      "Iteration 13, loss = 0.65390601\n",
      "Iteration 14, loss = 0.59972076\n",
      "Iteration 15, loss = 0.54786885\n",
      "Iteration 16, loss = 0.50515938\n",
      "Iteration 17, loss = 0.46655106\n",
      "Iteration 18, loss = 0.41957583\n",
      "Iteration 19, loss = 0.39396694\n",
      "Iteration 20, loss = 0.38274169\n",
      "Iteration 21, loss = 0.36962837\n",
      "Iteration 22, loss = 0.35191176\n",
      "Iteration 23, loss = 0.32454473\n",
      "Iteration 24, loss = 0.30741753\n",
      "Iteration 25, loss = 0.29374272\n",
      "Iteration 26, loss = 0.27656518\n",
      "Iteration 27, loss = 0.26117464\n",
      "Iteration 28, loss = 0.24767418\n",
      "Iteration 29, loss = 0.23246572\n",
      "Iteration 30, loss = 0.22166778\n",
      "Iteration 31, loss = 0.20940399\n",
      "Iteration 32, loss = 0.19899503\n",
      "Iteration 33, loss = 0.18794877\n",
      "Iteration 34, loss = 0.17857071\n",
      "Iteration 35, loss = 0.17518395\n",
      "Iteration 36, loss = 0.16652180\n",
      "Iteration 37, loss = 0.15681693\n",
      "Iteration 38, loss = 0.15369595\n",
      "Iteration 39, loss = 0.14641432\n",
      "Iteration 40, loss = 0.14414217\n",
      "Iteration 41, loss = 0.13861665\n",
      "Iteration 42, loss = 0.13697781\n",
      "Iteration 43, loss = 0.13262766\n",
      "Iteration 44, loss = 0.13133226\n",
      "Iteration 45, loss = 0.12836507\n",
      "Iteration 46, loss = 0.12688311\n",
      "Iteration 47, loss = 0.12455679\n",
      "Iteration 48, loss = 0.12346702\n",
      "Iteration 49, loss = 0.12188418\n",
      "Iteration 50, loss = 0.12074079\n",
      "Iteration 51, loss = 0.11975117\n",
      "Iteration 52, loss = 0.11836260\n",
      "Iteration 53, loss = 0.11780659\n",
      "Iteration 54, loss = 0.11639954\n",
      "Iteration 55, loss = 0.11580692\n",
      "Iteration 56, loss = 0.11492984\n",
      "Iteration 57, loss = 0.11391388\n",
      "Iteration 58, loss = 0.11348850\n",
      "Iteration 59, loss = 0.11261211\n",
      "Iteration 60, loss = 0.11182777\n",
      "Iteration 61, loss = 0.11144112\n",
      "Iteration 62, loss = 0.11079719\n",
      "Iteration 63, loss = 0.10998843\n",
      "Iteration 64, loss = 0.10943221\n",
      "Iteration 65, loss = 0.10909688\n",
      "Iteration 66, loss = 0.10881676\n",
      "Iteration 67, loss = 0.10859086\n",
      "Iteration 68, loss = 0.10866667\n",
      "Iteration 69, loss = 0.10849670\n",
      "Iteration 70, loss = 0.10810146\n",
      "Iteration 71, loss = 0.10669940\n",
      "Iteration 72, loss = 0.10553410\n",
      "Iteration 73, loss = 0.10503849\n",
      "Iteration 74, loss = 0.10513515\n",
      "Iteration 75, loss = 0.10584314\n",
      "Iteration 76, loss = 0.10630275\n",
      "Iteration 77, loss = 0.10674281\n",
      "Iteration 78, loss = 0.10462117\n",
      "Iteration 79, loss = 0.10283676\n",
      "Iteration 80, loss = 0.10238082\n",
      "Iteration 81, loss = 0.10311154\n",
      "Iteration 82, loss = 0.10416973\n",
      "Iteration 83, loss = 0.10344260\n",
      "Iteration 84, loss = 0.10220952\n",
      "Iteration 85, loss = 0.10069102\n",
      "Iteration 86, loss = 0.10029828\n",
      "Iteration 87, loss = 0.10085338\n",
      "Iteration 88, loss = 0.10146493\n",
      "Iteration 89, loss = 0.10226481\n",
      "Iteration 90, loss = 0.10124011\n",
      "Iteration 91, loss = 0.10012511\n",
      "Iteration 92, loss = 0.09866731\n",
      "Iteration 93, loss = 0.09805131\n",
      "Iteration 94, loss = 0.09818758\n",
      "Iteration 95, loss = 0.09872401\n",
      "Iteration 96, loss = 0.09983441\n",
      "Iteration 97, loss = 0.10011337\n",
      "Iteration 98, loss = 0.10076848\n",
      "Iteration 99, loss = 0.09891839\n",
      "Iteration 100, loss = 0.09733915\n",
      "Iteration 101, loss = 0.09604445\n",
      "Iteration 102, loss = 0.09585701\n",
      "Iteration 103, loss = 0.09650760\n",
      "Iteration 104, loss = 0.09724112\n",
      "Iteration 105, loss = 0.09825496\n",
      "Iteration 106, loss = 0.09753737\n",
      "Iteration 107, loss = 0.09685847\n",
      "Iteration 108, loss = 0.09522936\n",
      "Iteration 109, loss = 0.09422955\n",
      "Iteration 110, loss = 0.09392609\n",
      "Iteration 111, loss = 0.09420888\n",
      "Iteration 112, loss = 0.09489124\n",
      "Iteration 113, loss = 0.09537804\n",
      "Iteration 114, loss = 0.09632030\n",
      "Iteration 115, loss = 0.09586043\n",
      "Iteration 116, loss = 0.09576614\n",
      "Iteration 117, loss = 0.09420793\n",
      "Iteration 118, loss = 0.09307726\n",
      "Iteration 119, loss = 0.09218223\n",
      "Iteration 120, loss = 0.09190765\n",
      "Iteration 121, loss = 0.09212828\n",
      "Iteration 122, loss = 0.09257726\n",
      "Iteration 123, loss = 0.09339596\n",
      "Iteration 124, loss = 0.09378249\n",
      "Iteration 125, loss = 0.09484269\n",
      "Iteration 126, loss = 0.09423454\n",
      "Iteration 127, loss = 0.09414007\n",
      "Iteration 128, loss = 0.09240234\n",
      "Iteration 129, loss = 0.09116203\n",
      "Iteration 130, loss = 0.09023405\n",
      "Iteration 131, loss = 0.09001382\n",
      "Iteration 132, loss = 0.09034066\n",
      "Iteration 133, loss = 0.09085503\n",
      "Iteration 134, loss = 0.09164276\n",
      "Iteration 135, loss = 0.09176434\n",
      "Iteration 136, loss = 0.09224065\n",
      "Iteration 137, loss = 0.09141585\n",
      "Iteration 138, loss = 0.09091252\n",
      "Iteration 139, loss = 0.08974618\n",
      "Iteration 140, loss = 0.08895558\n",
      "Iteration 141, loss = 0.08844810\n",
      "Iteration 142, loss = 0.08829560\n",
      "Iteration 143, loss = 0.08841199\n",
      "Iteration 144, loss = 0.08867432\n",
      "Iteration 145, loss = 0.08917869\n",
      "Iteration 146, loss = 0.08953548\n",
      "Iteration 147, loss = 0.09049434\n",
      "Iteration 148, loss = 0.09065530\n",
      "Iteration 149, loss = 0.09174478\n",
      "Iteration 150, loss = 0.09094011\n",
      "Iteration 151, loss = 0.09088923\n",
      "Iteration 152, loss = 0.08909460\n",
      "Iteration 153, loss = 0.08789364\n",
      "Iteration 154, loss = 0.08684257\n",
      "Iteration 155, loss = 0.08647923\n",
      "Iteration 156, loss = 0.08668313\n",
      "Iteration 157, loss = 0.08714673\n",
      "Iteration 158, loss = 0.08780707\n",
      "Iteration 159, loss = 0.08789764\n",
      "Iteration 160, loss = 0.08813169\n",
      "Iteration 161, loss = 0.08745947\n",
      "Iteration 162, loss = 0.08698282\n",
      "Iteration 163, loss = 0.08619046\n",
      "Iteration 164, loss = 0.08564587\n",
      "Iteration 165, loss = 0.08527512\n",
      "Iteration 166, loss = 0.08511576\n",
      "Iteration 167, loss = 0.08511684\n",
      "Iteration 168, loss = 0.08521795\n",
      "Iteration 169, loss = 0.08544115\n",
      "Iteration 170, loss = 0.08566351\n",
      "Iteration 171, loss = 0.08619097\n",
      "Iteration 172, loss = 0.08653306\n",
      "Iteration 173, loss = 0.08762056\n",
      "Iteration 174, loss = 0.08788832\n",
      "Iteration 175, loss = 0.08944695\n",
      "Iteration 176, loss = 0.08872354\n",
      "Iteration 177, loss = 0.08916776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34884587\n",
      "Iteration 2, loss = 7.32435576\n",
      "Iteration 3, loss = 6.69208868\n",
      "Iteration 4, loss = 2.50091930\n",
      "Iteration 5, loss = 2.07430790\n",
      "Iteration 6, loss = 1.54701161\n",
      "Iteration 7, loss = 0.74416627\n",
      "Iteration 8, loss = 0.74502472\n",
      "Iteration 9, loss = 0.82895549\n",
      "Iteration 10, loss = 0.76290585\n",
      "Iteration 11, loss = 0.69034417\n",
      "Iteration 12, loss = 0.63090005\n",
      "Iteration 13, loss = 0.57886358\n",
      "Iteration 14, loss = 0.54758168\n",
      "Iteration 15, loss = 0.51211913\n",
      "Iteration 16, loss = 0.45870759\n",
      "Iteration 17, loss = 0.41493497\n",
      "Iteration 18, loss = 0.40670565\n",
      "Iteration 19, loss = 0.40397949\n",
      "Iteration 20, loss = 0.38518178\n",
      "Iteration 21, loss = 0.35717318\n",
      "Iteration 22, loss = 0.34523452\n",
      "Iteration 23, loss = 0.34614400\n",
      "Iteration 24, loss = 0.32600111\n",
      "Iteration 25, loss = 0.29380276\n",
      "Iteration 26, loss = 0.28621888\n",
      "Iteration 27, loss = 0.27985280\n",
      "Iteration 28, loss = 0.25172942\n",
      "Iteration 29, loss = 0.23894074\n",
      "Iteration 30, loss = 0.23711898\n",
      "Iteration 31, loss = 0.21289427\n",
      "Iteration 32, loss = 0.20539235\n",
      "Iteration 33, loss = 0.20121321\n",
      "Iteration 34, loss = 0.18201839\n",
      "Iteration 35, loss = 0.17971893\n",
      "Iteration 36, loss = 0.17213758\n",
      "Iteration 37, loss = 0.15919142\n",
      "Iteration 38, loss = 0.15301495\n",
      "Iteration 39, loss = 0.14066838\n",
      "Iteration 40, loss = 0.13500738\n",
      "Iteration 41, loss = 0.13285089\n",
      "Iteration 42, loss = 0.12766962\n",
      "Iteration 43, loss = 0.12495405\n",
      "Iteration 44, loss = 0.12046300\n",
      "Iteration 45, loss = 0.11915477\n",
      "Iteration 46, loss = 0.11541843\n",
      "Iteration 47, loss = 0.11407132\n",
      "Iteration 48, loss = 0.11059528\n",
      "Iteration 49, loss = 0.10997638\n",
      "Iteration 50, loss = 0.10706793\n",
      "Iteration 51, loss = 0.10649512\n",
      "Iteration 52, loss = 0.10372874\n",
      "Iteration 53, loss = 0.10357020\n",
      "Iteration 54, loss = 0.10122159\n",
      "Iteration 55, loss = 0.10113858\n",
      "Iteration 56, loss = 0.09886988\n",
      "Iteration 57, loss = 0.09894553\n",
      "Iteration 58, loss = 0.09698360\n",
      "Iteration 59, loss = 0.09700939\n",
      "Iteration 60, loss = 0.09555460\n",
      "Iteration 61, loss = 0.09447623\n",
      "Iteration 62, loss = 0.09352986\n",
      "Iteration 63, loss = 0.09302603\n",
      "Iteration 64, loss = 0.09214830\n",
      "Iteration 65, loss = 0.09115363\n",
      "Iteration 66, loss = 0.09059162\n",
      "Iteration 67, loss = 0.09111487\n",
      "Iteration 68, loss = 0.08998397\n",
      "Iteration 69, loss = 0.08980316\n",
      "Iteration 70, loss = 0.08900175\n",
      "Iteration 71, loss = 0.08858524\n",
      "Iteration 72, loss = 0.08812968\n",
      "Iteration 73, loss = 0.08748460\n",
      "Iteration 74, loss = 0.08724837\n",
      "Iteration 75, loss = 0.08649429\n",
      "Iteration 76, loss = 0.08631245\n",
      "Iteration 77, loss = 0.08565224\n",
      "Iteration 78, loss = 0.08537075\n",
      "Iteration 79, loss = 0.08491517\n",
      "Iteration 80, loss = 0.08446383\n",
      "Iteration 81, loss = 0.08419077\n",
      "Iteration 82, loss = 0.08366030\n",
      "Iteration 83, loss = 0.08342463\n",
      "Iteration 84, loss = 0.08298321\n",
      "Iteration 85, loss = 0.08265760\n",
      "Iteration 86, loss = 0.08236772\n",
      "Iteration 87, loss = 0.08196262\n",
      "Iteration 88, loss = 0.08173455\n",
      "Iteration 89, loss = 0.08137439\n",
      "Iteration 90, loss = 0.08108365\n",
      "Iteration 91, loss = 0.08083416\n",
      "Iteration 92, loss = 0.08049692\n",
      "Iteration 93, loss = 0.08026885\n",
      "Iteration 94, loss = 0.07999127\n",
      "Iteration 95, loss = 0.07970936\n",
      "Iteration 96, loss = 0.07949253\n",
      "Iteration 97, loss = 0.07921487\n",
      "Iteration 98, loss = 0.07897364\n",
      "Iteration 99, loss = 0.07875264\n",
      "Iteration 100, loss = 0.07848988\n",
      "Iteration 101, loss = 0.07826909\n",
      "Iteration 102, loss = 0.07804631\n",
      "Iteration 103, loss = 0.07779951\n",
      "Iteration 104, loss = 0.07758756\n",
      "Iteration 105, loss = 0.07736808\n",
      "Iteration 106, loss = 0.07713387\n",
      "Iteration 107, loss = 0.07692651\n",
      "Iteration 108, loss = 0.07671371\n",
      "Iteration 109, loss = 0.07648945\n",
      "Iteration 110, loss = 0.07628501\n",
      "Iteration 111, loss = 0.07607962\n",
      "Iteration 112, loss = 0.07586386\n",
      "Iteration 113, loss = 0.07566148\n",
      "Iteration 114, loss = 0.07546344\n",
      "Iteration 115, loss = 0.07525617\n",
      "Iteration 116, loss = 0.07505532\n",
      "Iteration 117, loss = 0.07486254\n",
      "Iteration 118, loss = 0.07466453\n",
      "Iteration 119, loss = 0.07446657\n",
      "Iteration 120, loss = 0.07427672\n",
      "Iteration 121, loss = 0.07408764\n",
      "Iteration 122, loss = 0.07389572\n",
      "Iteration 123, loss = 0.07370780\n",
      "Iteration 124, loss = 0.07352493\n",
      "Iteration 125, loss = 0.07334155\n",
      "Iteration 126, loss = 0.07315765\n",
      "Iteration 127, loss = 0.07297786\n",
      "Iteration 128, loss = 0.07280144\n",
      "Iteration 129, loss = 0.07262493\n",
      "Iteration 130, loss = 0.07244885\n",
      "Iteration 131, loss = 0.07227583\n",
      "Iteration 132, loss = 0.07210559\n",
      "Iteration 133, loss = 0.07193604\n",
      "Iteration 134, loss = 0.07176689\n",
      "Iteration 135, loss = 0.07159975\n",
      "Iteration 136, loss = 0.07143520\n",
      "Iteration 137, loss = 0.07127234\n",
      "Iteration 138, loss = 0.07111032\n",
      "Iteration 139, loss = 0.07094947\n",
      "Iteration 140, loss = 0.07079062\n",
      "Iteration 141, loss = 0.07063377\n",
      "Iteration 142, loss = 0.07047845\n",
      "Iteration 143, loss = 0.07032428\n",
      "Iteration 144, loss = 0.07017123\n",
      "Iteration 145, loss = 0.07001957\n",
      "Iteration 146, loss = 0.06986980\n",
      "Iteration 147, loss = 0.06972147\n",
      "Iteration 148, loss = 0.06957436\n",
      "Iteration 149, loss = 0.06942833\n",
      "Iteration 150, loss = 0.06928335\n",
      "Iteration 151, loss = 0.06913941\n",
      "Iteration 152, loss = 0.06899691\n",
      "Iteration 153, loss = 0.06885578\n",
      "Iteration 154, loss = 0.06871606\n",
      "Iteration 155, loss = 0.06857760\n",
      "Iteration 156, loss = 0.06844036\n",
      "Iteration 157, loss = 0.06830432\n",
      "Iteration 158, loss = 0.06816953\n",
      "Iteration 159, loss = 0.06803630\n",
      "Iteration 160, loss = 0.06790414\n",
      "Iteration 161, loss = 0.06777312\n",
      "Iteration 162, loss = 0.06764332\n",
      "Iteration 163, loss = 0.06751504\n",
      "Iteration 164, loss = 0.06738884\n",
      "Iteration 165, loss = 0.06726624\n",
      "Iteration 166, loss = 0.06715032\n",
      "Iteration 167, loss = 0.06705017\n",
      "Iteration 168, loss = 0.06698355\n",
      "Iteration 169, loss = 0.06701047\n",
      "Iteration 170, loss = 0.06723590\n",
      "Iteration 171, loss = 0.06809379\n",
      "Iteration 172, loss = 0.06980173\n",
      "Iteration 173, loss = 0.07426671\n",
      "Iteration 174, loss = 0.07501960\n",
      "Iteration 175, loss = 0.07306064\n",
      "Iteration 176, loss = 0.06602078\n",
      "Iteration 177, loss = 0.07074673\n",
      "Iteration 178, loss = 0.07404760\n",
      "Iteration 179, loss = 0.06583408\n",
      "Iteration 180, loss = 0.07102998\n",
      "Iteration 181, loss = 0.07268468\n",
      "Iteration 182, loss = 0.06548055\n",
      "Iteration 183, loss = 0.07297410\n",
      "Iteration 184, loss = 0.06960350\n",
      "Iteration 185, loss = 0.06708722\n",
      "Iteration 186, loss = 0.07167462\n",
      "Iteration 187, loss = 0.06521598\n",
      "Iteration 188, loss = 0.06875483\n",
      "Iteration 189, loss = 0.06607078\n",
      "Iteration 190, loss = 0.06622810\n",
      "Iteration 191, loss = 0.06711321\n",
      "Iteration 192, loss = 0.06486368\n",
      "Iteration 193, loss = 0.06723163\n",
      "Iteration 194, loss = 0.06427088\n",
      "Iteration 195, loss = 0.06615060\n",
      "Iteration 196, loss = 0.06434123\n",
      "Iteration 197, loss = 0.06520453\n",
      "Iteration 198, loss = 0.06471690\n",
      "Iteration 199, loss = 0.06433973\n",
      "Iteration 200, loss = 0.06477617\n",
      "Iteration 201, loss = 0.06366570\n",
      "Iteration 202, loss = 0.06452238\n",
      "Iteration 203, loss = 0.06340645\n",
      "Iteration 204, loss = 0.06418666\n",
      "Iteration 205, loss = 0.06335322\n",
      "Iteration 206, loss = 0.06366431\n",
      "Iteration 207, loss = 0.06332103\n",
      "Iteration 208, loss = 0.06318731\n",
      "Iteration 209, loss = 0.06327418\n",
      "Iteration 210, loss = 0.06284396\n",
      "Iteration 211, loss = 0.06312970\n",
      "Iteration 212, loss = 0.06261241\n",
      "Iteration 213, loss = 0.06285299\n",
      "Iteration 214, loss = 0.06248507\n",
      "Iteration 215, loss = 0.06254869\n",
      "Iteration 216, loss = 0.06240395\n",
      "Iteration 217, loss = 0.06225540\n",
      "Iteration 218, loss = 0.06228701\n",
      "Iteration 219, loss = 0.06201958\n",
      "Iteration 220, loss = 0.06210245\n",
      "Iteration 221, loss = 0.06186006\n",
      "Iteration 222, loss = 0.06187898\n",
      "Iteration 223, loss = 0.06174246\n",
      "Iteration 224, loss = 0.06164596\n",
      "Iteration 225, loss = 0.06161483\n",
      "Iteration 226, loss = 0.06144614\n",
      "Iteration 227, loss = 0.06144841\n",
      "Iteration 228, loss = 0.06129326\n",
      "Iteration 229, loss = 0.06125552\n",
      "Iteration 230, loss = 0.06116395\n",
      "Iteration 231, loss = 0.06106295\n",
      "Iteration 232, loss = 0.06102387\n",
      "Iteration 233, loss = 0.06089709\n",
      "Iteration 234, loss = 0.06085787\n",
      "Iteration 235, loss = 0.06075740\n",
      "Iteration 236, loss = 0.06068255\n",
      "Iteration 237, loss = 0.06062130\n",
      "Iteration 238, loss = 0.06052035\n",
      "Iteration 239, loss = 0.06047091\n",
      "Iteration 240, loss = 0.06037827\n",
      "Iteration 241, loss = 0.06030949\n",
      "Iteration 242, loss = 0.06024253\n",
      "Iteration 243, loss = 0.06015506\n",
      "Iteration 244, loss = 0.06009790\n",
      "Iteration 245, loss = 0.06001512\n",
      "Iteration 246, loss = 0.05994650\n",
      "Iteration 247, loss = 0.05988021\n",
      "Iteration 248, loss = 0.05980052\n",
      "Iteration 249, loss = 0.05973958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34975994\n",
      "Iteration 2, loss = 7.44561173\n",
      "Iteration 3, loss = 6.60302349\n",
      "Iteration 4, loss = 2.42171784\n",
      "Iteration 5, loss = 2.15833823\n",
      "Iteration 6, loss = 1.58817094\n",
      "Iteration 7, loss = 0.78183370\n",
      "Iteration 8, loss = 0.74613860\n",
      "Iteration 9, loss = 0.82670259\n",
      "Iteration 10, loss = 0.77831940\n",
      "Iteration 11, loss = 0.72501511\n",
      "Iteration 12, loss = 0.68025195\n",
      "Iteration 13, loss = 0.62077108\n",
      "Iteration 14, loss = 0.56885095\n",
      "Iteration 15, loss = 0.52534397\n",
      "Iteration 16, loss = 0.48718941\n",
      "Iteration 17, loss = 0.45650453\n",
      "Iteration 18, loss = 0.42851970\n",
      "Iteration 19, loss = 0.40594183\n",
      "Iteration 20, loss = 0.38457345\n",
      "Iteration 21, loss = 0.36222759\n",
      "Iteration 22, loss = 0.35398198\n",
      "Iteration 23, loss = 0.34643384\n",
      "Iteration 24, loss = 0.31986749\n",
      "Iteration 25, loss = 0.29605096\n",
      "Iteration 26, loss = 0.28979426\n",
      "Iteration 27, loss = 0.27281698\n",
      "Iteration 28, loss = 0.24949214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.24477881\n",
      "Iteration 30, loss = 0.23005966\n",
      "Iteration 31, loss = 0.21246217\n",
      "Iteration 32, loss = 0.20925363\n",
      "Iteration 33, loss = 0.19339232\n",
      "Iteration 34, loss = 0.18365549\n",
      "Iteration 35, loss = 0.17890184\n",
      "Iteration 36, loss = 0.16449764\n",
      "Iteration 37, loss = 0.16551704\n",
      "Iteration 38, loss = 0.15067087\n",
      "Iteration 39, loss = 0.14712583\n",
      "Iteration 40, loss = 0.14131846\n",
      "Iteration 41, loss = 0.13472149\n",
      "Iteration 42, loss = 0.13284642\n",
      "Iteration 43, loss = 0.12666326\n",
      "Iteration 44, loss = 0.12417445\n",
      "Iteration 45, loss = 0.12086453\n",
      "Iteration 46, loss = 0.11461758\n",
      "Iteration 47, loss = 0.12417812\n",
      "Iteration 48, loss = 0.10811363\n",
      "Iteration 49, loss = 0.11823947\n",
      "Iteration 50, loss = 0.10470867\n",
      "Iteration 51, loss = 0.11192348\n",
      "Iteration 52, loss = 0.10153905\n",
      "Iteration 53, loss = 0.10806830\n",
      "Iteration 54, loss = 0.09874019\n",
      "Iteration 55, loss = 0.10313502\n",
      "Iteration 56, loss = 0.09711164\n",
      "Iteration 57, loss = 0.09939423\n",
      "Iteration 58, loss = 0.09490615\n",
      "Iteration 59, loss = 0.09601952\n",
      "Iteration 60, loss = 0.09370092\n",
      "Iteration 61, loss = 0.09311818\n",
      "Iteration 62, loss = 0.09190417\n",
      "Iteration 63, loss = 0.09103265\n",
      "Iteration 64, loss = 0.09045596\n",
      "Iteration 65, loss = 0.08894791\n",
      "Iteration 66, loss = 0.08875266\n",
      "Iteration 67, loss = 0.08742329\n",
      "Iteration 68, loss = 0.08717502\n",
      "Iteration 69, loss = 0.08584117\n",
      "Iteration 70, loss = 0.08556954\n",
      "Iteration 71, loss = 0.08462047\n",
      "Iteration 72, loss = 0.08410679\n",
      "Iteration 73, loss = 0.08340282\n",
      "Iteration 74, loss = 0.08263754\n",
      "Iteration 75, loss = 0.08236136\n",
      "Iteration 76, loss = 0.08134389\n",
      "Iteration 77, loss = 0.08133536\n",
      "Iteration 78, loss = 0.08010093\n",
      "Iteration 79, loss = 0.08020250\n",
      "Iteration 80, loss = 0.07908380\n",
      "Iteration 81, loss = 0.07901254\n",
      "Iteration 82, loss = 0.07829950\n",
      "Iteration 83, loss = 0.07774167\n",
      "Iteration 84, loss = 0.07753972\n",
      "Iteration 85, loss = 0.07666407\n",
      "Iteration 86, loss = 0.07651832\n",
      "Iteration 87, loss = 0.07594501\n",
      "Iteration 88, loss = 0.07538174\n",
      "Iteration 89, loss = 0.07520377\n",
      "Iteration 90, loss = 0.07457189\n",
      "Iteration 91, loss = 0.07417130\n",
      "Iteration 92, loss = 0.07393012\n",
      "Iteration 93, loss = 0.07335508\n",
      "Iteration 94, loss = 0.07296473\n",
      "Iteration 95, loss = 0.07272505\n",
      "Iteration 96, loss = 0.07224829\n",
      "Iteration 97, loss = 0.07180135\n",
      "Iteration 98, loss = 0.07153817\n",
      "Iteration 99, loss = 0.07121560\n",
      "Iteration 100, loss = 0.07078116\n",
      "Iteration 101, loss = 0.07039052\n",
      "Iteration 102, loss = 0.07010926\n",
      "Iteration 103, loss = 0.06983624\n",
      "Iteration 104, loss = 0.06948311\n",
      "Iteration 105, loss = 0.06909982\n",
      "Iteration 106, loss = 0.06874124\n",
      "Iteration 107, loss = 0.06843386\n",
      "Iteration 108, loss = 0.06816397\n",
      "Iteration 109, loss = 0.06790878\n",
      "Iteration 110, loss = 0.06767315\n",
      "Iteration 111, loss = 0.06744803\n",
      "Iteration 112, loss = 0.06730121\n",
      "Iteration 113, loss = 0.06722196\n",
      "Iteration 114, loss = 0.06745618\n",
      "Iteration 115, loss = 0.06783631\n",
      "Iteration 116, loss = 0.06908942\n",
      "Iteration 117, loss = 0.06906127\n",
      "Iteration 118, loss = 0.06902555\n",
      "Iteration 119, loss = 0.06628364\n",
      "Iteration 120, loss = 0.06483252\n",
      "Iteration 121, loss = 0.06526907\n",
      "Iteration 122, loss = 0.06620237\n",
      "Iteration 123, loss = 0.06657109\n",
      "Iteration 124, loss = 0.06481212\n",
      "Iteration 125, loss = 0.06370737\n",
      "Iteration 126, loss = 0.06394684\n",
      "Iteration 127, loss = 0.06452157\n",
      "Iteration 128, loss = 0.06457804\n",
      "Iteration 129, loss = 0.06339487\n",
      "Iteration 130, loss = 0.06265368\n",
      "Iteration 131, loss = 0.06276700\n",
      "Iteration 132, loss = 0.06309084\n",
      "Iteration 133, loss = 0.06308630\n",
      "Iteration 134, loss = 0.06231343\n",
      "Iteration 135, loss = 0.06170509\n",
      "Iteration 136, loss = 0.06157739\n",
      "Iteration 137, loss = 0.06173672\n",
      "Iteration 138, loss = 0.06183032\n",
      "Iteration 139, loss = 0.06144753\n",
      "Iteration 140, loss = 0.06096947\n",
      "Iteration 141, loss = 0.06058101\n",
      "Iteration 142, loss = 0.06045007\n",
      "Iteration 143, loss = 0.06048283\n",
      "Iteration 144, loss = 0.06045824\n",
      "Iteration 145, loss = 0.06034938\n",
      "Iteration 146, loss = 0.06003247\n",
      "Iteration 147, loss = 0.05970771\n",
      "Iteration 148, loss = 0.05940935\n",
      "Iteration 149, loss = 0.05920590\n",
      "Iteration 150, loss = 0.05908676\n",
      "Iteration 151, loss = 0.05901493\n",
      "Iteration 152, loss = 0.05897122\n",
      "Iteration 153, loss = 0.05889653\n",
      "Iteration 154, loss = 0.05884945\n",
      "Iteration 155, loss = 0.05872624\n",
      "Iteration 156, loss = 0.05865974\n",
      "Iteration 157, loss = 0.05849621\n",
      "Iteration 158, loss = 0.05841226\n",
      "Iteration 159, loss = 0.05822281\n",
      "Iteration 160, loss = 0.05812219\n",
      "Iteration 161, loss = 0.05791705\n",
      "Iteration 162, loss = 0.05779978\n",
      "Iteration 163, loss = 0.05758843\n",
      "Iteration 164, loss = 0.05745827\n",
      "Iteration 165, loss = 0.05725094\n",
      "Iteration 166, loss = 0.05711686\n",
      "Iteration 167, loss = 0.05692372\n",
      "Iteration 168, loss = 0.05679678\n",
      "Iteration 169, loss = 0.05662442\n",
      "Iteration 170, loss = 0.05651586\n",
      "Iteration 171, loss = 0.05636784\n",
      "Iteration 172, loss = 0.05628984\n",
      "Iteration 173, loss = 0.05616983\n",
      "Iteration 174, loss = 0.05613927\n",
      "Iteration 175, loss = 0.05604881\n",
      "Iteration 176, loss = 0.05608372\n",
      "Iteration 177, loss = 0.05600683\n",
      "Iteration 178, loss = 0.05609751\n",
      "Iteration 179, loss = 0.05596705\n",
      "Iteration 180, loss = 0.05601364\n",
      "Iteration 181, loss = 0.05572215\n",
      "Iteration 182, loss = 0.05556097\n",
      "Iteration 183, loss = 0.05512678\n",
      "Iteration 184, loss = 0.05478992\n",
      "Iteration 185, loss = 0.05442502\n",
      "Iteration 186, loss = 0.05416503\n",
      "Iteration 187, loss = 0.05399578\n",
      "Iteration 188, loss = 0.05390815\n",
      "Iteration 189, loss = 0.05387793\n",
      "Iteration 190, loss = 0.05386872\n",
      "Iteration 191, loss = 0.05389184\n",
      "Iteration 192, loss = 0.05386745\n",
      "Iteration 193, loss = 0.05389169\n",
      "Iteration 194, loss = 0.05380392\n",
      "Iteration 195, loss = 0.05378487\n",
      "Iteration 196, loss = 0.05361329\n",
      "Iteration 197, loss = 0.05351103\n",
      "Iteration 198, loss = 0.05327332\n",
      "Iteration 199, loss = 0.05309273\n",
      "Iteration 200, loss = 0.05284810\n",
      "Iteration 201, loss = 0.05264910\n",
      "Iteration 202, loss = 0.05245182\n",
      "Iteration 203, loss = 0.05228924\n",
      "Iteration 204, loss = 0.05214944\n",
      "Iteration 205, loss = 0.05203189\n",
      "Iteration 206, loss = 0.05193096\n",
      "Iteration 207, loss = 0.05184249\n",
      "Iteration 208, loss = 0.05176471\n",
      "Iteration 209, loss = 0.05169551\n",
      "Iteration 210, loss = 0.05164324\n",
      "Iteration 211, loss = 0.05160167\n",
      "Iteration 212, loss = 0.05160498\n",
      "Iteration 213, loss = 0.05162448\n",
      "Iteration 214, loss = 0.05176599\n",
      "Iteration 215, loss = 0.05190769\n",
      "Iteration 216, loss = 0.05235279\n",
      "Iteration 217, loss = 0.05260646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34996548\n",
      "Iteration 2, loss = 7.97739132\n",
      "Iteration 3, loss = 6.20100727\n",
      "Iteration 4, loss = 2.24248670\n",
      "Iteration 5, loss = 2.27998341\n",
      "Iteration 6, loss = 1.57582497\n",
      "Iteration 7, loss = 0.75469039\n",
      "Iteration 8, loss = 0.75675959\n",
      "Iteration 9, loss = 0.82015300\n",
      "Iteration 10, loss = 0.76912637\n",
      "Iteration 11, loss = 0.71305526\n",
      "Iteration 12, loss = 0.66432156\n",
      "Iteration 13, loss = 0.60936543\n",
      "Iteration 14, loss = 0.57590997\n",
      "Iteration 15, loss = 0.54143403\n",
      "Iteration 16, loss = 0.51180415\n",
      "Iteration 17, loss = 0.48152578\n",
      "Iteration 18, loss = 0.46104736\n",
      "Iteration 19, loss = 0.43711812\n",
      "Iteration 20, loss = 0.39285299\n",
      "Iteration 21, loss = 0.37324390\n",
      "Iteration 22, loss = 0.36146369\n",
      "Iteration 23, loss = 0.34898609\n",
      "Iteration 24, loss = 0.33187872\n",
      "Iteration 25, loss = 0.31522134\n",
      "Iteration 26, loss = 0.30284360\n",
      "Iteration 27, loss = 0.28822433\n",
      "Iteration 28, loss = 0.27315708\n",
      "Iteration 29, loss = 0.26271600\n",
      "Iteration 30, loss = 0.24917144\n",
      "Iteration 31, loss = 0.23733552\n",
      "Iteration 32, loss = 0.22815074\n",
      "Iteration 33, loss = 0.21640652\n",
      "Iteration 34, loss = 0.20888151\n",
      "Iteration 35, loss = 0.19943386\n",
      "Iteration 36, loss = 0.19184790\n",
      "Iteration 37, loss = 0.18519724\n",
      "Iteration 38, loss = 0.17805588\n",
      "Iteration 39, loss = 0.17349336\n",
      "Iteration 40, loss = 0.16747455\n",
      "Iteration 41, loss = 0.16402053\n",
      "Iteration 42, loss = 0.15928056\n",
      "Iteration 43, loss = 0.15630196\n",
      "Iteration 44, loss = 0.15268398\n",
      "Iteration 45, loss = 0.14999027\n",
      "Iteration 46, loss = 0.14732574\n",
      "Iteration 47, loss = 0.14484117\n",
      "Iteration 48, loss = 0.14281746\n",
      "Iteration 49, loss = 0.14056067\n",
      "Iteration 50, loss = 0.13895299\n",
      "Iteration 51, loss = 0.13693949\n",
      "Iteration 52, loss = 0.13564041\n",
      "Iteration 53, loss = 0.13389977\n",
      "Iteration 54, loss = 0.13281552\n",
      "Iteration 55, loss = 0.13131359\n",
      "Iteration 56, loss = 0.13037448\n",
      "Iteration 57, loss = 0.12906799\n",
      "Iteration 58, loss = 0.12822415\n",
      "Iteration 59, loss = 0.12707715\n",
      "Iteration 60, loss = 0.12630787\n",
      "Iteration 61, loss = 0.12528561\n",
      "Iteration 62, loss = 0.12457281\n",
      "Iteration 63, loss = 0.12365303\n",
      "Iteration 64, loss = 0.12298688\n",
      "Iteration 65, loss = 0.12214846\n",
      "Iteration 66, loss = 0.12151913\n",
      "Iteration 67, loss = 0.12075255\n",
      "Iteration 68, loss = 0.12015601\n",
      "Iteration 69, loss = 0.11945284\n",
      "Iteration 70, loss = 0.11888208\n",
      "Iteration 71, loss = 0.11823704\n",
      "Iteration 72, loss = 0.11768776\n",
      "Iteration 73, loss = 0.11709542\n",
      "Iteration 74, loss = 0.11656244\n",
      "Iteration 75, loss = 0.11601785\n",
      "Iteration 76, loss = 0.11549998\n",
      "Iteration 77, loss = 0.11499785\n",
      "Iteration 78, loss = 0.11449456\n",
      "Iteration 79, loss = 0.11402646\n",
      "Iteration 80, loss = 0.11353945\n",
      "Iteration 81, loss = 0.11309748\n",
      "Iteration 82, loss = 0.11263084\n",
      "Iteration 83, loss = 0.11220735\n",
      "Iteration 84, loss = 0.11176411\n",
      "Iteration 85, loss = 0.11135259\n",
      "Iteration 86, loss = 0.11093433\n",
      "Iteration 87, loss = 0.11053131\n",
      "Iteration 88, loss = 0.11013630\n",
      "Iteration 89, loss = 0.10974212\n",
      "Iteration 90, loss = 0.10936612\n",
      "Iteration 91, loss = 0.10898355\n",
      "Iteration 92, loss = 0.10862090\n",
      "Iteration 93, loss = 0.10825319\n",
      "Iteration 94, loss = 0.10789948\n",
      "Iteration 95, loss = 0.10754796\n",
      "Iteration 96, loss = 0.10720168\n",
      "Iteration 97, loss = 0.10686480\n",
      "Iteration 98, loss = 0.10652729\n",
      "Iteration 99, loss = 0.10620276\n",
      "Iteration 100, loss = 0.10587775\n",
      "Iteration 101, loss = 0.10556122\n",
      "Iteration 102, loss = 0.10524833\n",
      "Iteration 103, loss = 0.10493883\n",
      "Iteration 104, loss = 0.10463664\n",
      "Iteration 105, loss = 0.10433529\n",
      "Iteration 106, loss = 0.10404210\n",
      "Iteration 107, loss = 0.10375195\n",
      "Iteration 108, loss = 0.10346613\n",
      "Iteration 109, loss = 0.10318529\n",
      "Iteration 110, loss = 0.10290640\n",
      "Iteration 111, loss = 0.10263311\n",
      "Iteration 112, loss = 0.10236209\n",
      "Iteration 113, loss = 0.10209495\n",
      "Iteration 114, loss = 0.10183171\n",
      "Iteration 115, loss = 0.10157073\n",
      "Iteration 116, loss = 0.10131413\n",
      "Iteration 117, loss = 0.10106011\n",
      "Iteration 118, loss = 0.10080948\n",
      "Iteration 119, loss = 0.10056241\n",
      "Iteration 120, loss = 0.10031770\n",
      "Iteration 121, loss = 0.10007659\n",
      "Iteration 122, loss = 0.09983810\n",
      "Iteration 123, loss = 0.09960225\n",
      "Iteration 124, loss = 0.09936958\n",
      "Iteration 125, loss = 0.09913923\n",
      "Iteration 126, loss = 0.09891173\n",
      "Iteration 127, loss = 0.09868677\n",
      "Iteration 128, loss = 0.09846414\n",
      "Iteration 129, loss = 0.09824431\n",
      "Iteration 130, loss = 0.09802672\n",
      "Iteration 131, loss = 0.09781158\n",
      "Iteration 132, loss = 0.09759891\n",
      "Iteration 133, loss = 0.09738826\n",
      "Iteration 134, loss = 0.09717989\n",
      "Iteration 135, loss = 0.09697368\n",
      "Iteration 136, loss = 0.09676945\n",
      "Iteration 137, loss = 0.09656744\n",
      "Iteration 138, loss = 0.09636743\n",
      "Iteration 139, loss = 0.09616936\n",
      "Iteration 140, loss = 0.09597331\n",
      "Iteration 141, loss = 0.09577932\n",
      "Iteration 142, loss = 0.09558731\n",
      "Iteration 143, loss = 0.09539737\n",
      "Iteration 144, loss = 0.09520920\n",
      "Iteration 145, loss = 0.09502280\n",
      "Iteration 146, loss = 0.09483820\n",
      "Iteration 147, loss = 0.09465528\n",
      "Iteration 148, loss = 0.09447404\n",
      "Iteration 149, loss = 0.09429447\n",
      "Iteration 150, loss = 0.09411649\n",
      "Iteration 151, loss = 0.09394010\n",
      "Iteration 152, loss = 0.09376526\n",
      "Iteration 153, loss = 0.09359193\n",
      "Iteration 154, loss = 0.09342011\n",
      "Iteration 155, loss = 0.09324978\n",
      "Iteration 156, loss = 0.09308087\n",
      "Iteration 157, loss = 0.09291339\n",
      "Iteration 158, loss = 0.09274732\n",
      "Iteration 159, loss = 0.09258261\n",
      "Iteration 160, loss = 0.09241926\n",
      "Iteration 161, loss = 0.09225724\n",
      "Iteration 162, loss = 0.09209652\n",
      "Iteration 163, loss = 0.09193709\n",
      "Iteration 164, loss = 0.09177899\n",
      "Iteration 165, loss = 0.09162213\n",
      "Iteration 166, loss = 0.09146652\n",
      "Iteration 167, loss = 0.09131212\n",
      "Iteration 168, loss = 0.09115893\n",
      "Iteration 169, loss = 0.09100691\n",
      "Iteration 170, loss = 0.09085606\n",
      "Iteration 171, loss = 0.09070636\n",
      "Iteration 172, loss = 0.09055779\n",
      "Iteration 173, loss = 0.09041034\n",
      "Iteration 174, loss = 0.09026399\n",
      "Iteration 175, loss = 0.09011873\n",
      "Iteration 176, loss = 0.08997454\n",
      "Iteration 177, loss = 0.08983142\n",
      "Iteration 178, loss = 0.08968933\n",
      "Iteration 179, loss = 0.08954828\n",
      "Iteration 180, loss = 0.08940824\n",
      "Iteration 181, loss = 0.08926922\n",
      "Iteration 182, loss = 0.08913118\n",
      "Iteration 183, loss = 0.08899414\n",
      "Iteration 184, loss = 0.08885809\n",
      "Iteration 185, loss = 0.08872299\n",
      "Iteration 186, loss = 0.08858888\n",
      "Iteration 187, loss = 0.08845577\n",
      "Iteration 188, loss = 0.08832360\n",
      "Iteration 189, loss = 0.08819235\n",
      "Iteration 190, loss = 0.08806202\n",
      "Iteration 191, loss = 0.08793259\n",
      "Iteration 192, loss = 0.08780404\n",
      "Iteration 193, loss = 0.08767638\n",
      "Iteration 194, loss = 0.08754965\n",
      "Iteration 195, loss = 0.08742382\n",
      "Iteration 196, loss = 0.08729885\n",
      "Iteration 197, loss = 0.08717473\n",
      "Iteration 198, loss = 0.08705145\n",
      "Iteration 199, loss = 0.08692901\n",
      "Iteration 200, loss = 0.08680739\n",
      "Iteration 201, loss = 0.08668659\n",
      "Iteration 202, loss = 0.08656659\n",
      "Iteration 203, loss = 0.08644738\n",
      "Iteration 204, loss = 0.08632897\n",
      "Iteration 205, loss = 0.08621133\n",
      "Iteration 206, loss = 0.08609447\n",
      "Iteration 207, loss = 0.08597839\n",
      "Iteration 208, loss = 0.08586309\n",
      "Iteration 209, loss = 0.08574856\n",
      "Iteration 210, loss = 0.08563483\n",
      "Iteration 211, loss = 0.08552186\n",
      "Iteration 212, loss = 0.08540963\n",
      "Iteration 213, loss = 0.08529813\n",
      "Iteration 214, loss = 0.08518736\n",
      "Iteration 215, loss = 0.08507730\n",
      "Iteration 216, loss = 0.08496796\n",
      "Iteration 217, loss = 0.08485932\n",
      "Iteration 218, loss = 0.08475137\n",
      "Iteration 219, loss = 0.08464412\n",
      "Iteration 220, loss = 0.08453756\n",
      "Iteration 221, loss = 0.08443167\n",
      "Iteration 222, loss = 0.08432646\n",
      "Iteration 223, loss = 0.08422192\n",
      "Iteration 224, loss = 0.08411804\n",
      "Iteration 225, loss = 0.08401482\n",
      "Iteration 226, loss = 0.08391226\n",
      "Iteration 227, loss = 0.08381033\n",
      "Iteration 228, loss = 0.08370905\n",
      "Iteration 229, loss = 0.08360842\n",
      "Iteration 230, loss = 0.08350841\n",
      "Iteration 231, loss = 0.08340903\n",
      "Iteration 232, loss = 0.08331027\n",
      "Iteration 233, loss = 0.08321213\n",
      "Iteration 234, loss = 0.08311462\n",
      "Iteration 235, loss = 0.08301773\n",
      "Iteration 236, loss = 0.08292145\n",
      "Iteration 237, loss = 0.08282577\n",
      "Iteration 238, loss = 0.08273069\n",
      "Iteration 239, loss = 0.08263619\n",
      "Iteration 240, loss = 0.08254229\n",
      "Iteration 241, loss = 0.08244897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35652749\n",
      "Iteration 2, loss = 2.76085304\n",
      "Iteration 3, loss = 3.19401424\n",
      "Iteration 4, loss = 3.00778365\n",
      "Iteration 5, loss = 0.89242969\n",
      "Iteration 6, loss = 0.72850401\n",
      "Iteration 7, loss = 0.62577121\n",
      "Iteration 8, loss = 0.54927698\n",
      "Iteration 9, loss = 0.50060616\n",
      "Iteration 10, loss = 0.46037817\n",
      "Iteration 11, loss = 0.43090335\n",
      "Iteration 12, loss = 0.40558278\n",
      "Iteration 13, loss = 0.38188335\n",
      "Iteration 14, loss = 0.35917481\n",
      "Iteration 15, loss = 0.34384353\n",
      "Iteration 16, loss = 0.41910515\n",
      "Iteration 17, loss = 1.24346525\n",
      "Iteration 18, loss = 1.73424111\n",
      "Iteration 19, loss = 0.38703239\n",
      "Iteration 20, loss = 0.50141891\n",
      "Iteration 21, loss = 0.41747281\n",
      "Iteration 22, loss = 0.41020830\n",
      "Iteration 23, loss = 0.40739404\n",
      "Iteration 24, loss = 0.40054895\n",
      "Iteration 25, loss = 0.39243526\n",
      "Iteration 26, loss = 0.38034941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35233713\n",
      "Iteration 2, loss = 2.70849357\n",
      "Iteration 3, loss = 3.03597528\n",
      "Iteration 4, loss = 3.38332993\n",
      "Iteration 5, loss = 0.88357796\n",
      "Iteration 6, loss = 0.72684871\n",
      "Iteration 7, loss = 0.62322723\n",
      "Iteration 8, loss = 0.54981391\n",
      "Iteration 9, loss = 0.49732222\n",
      "Iteration 10, loss = 0.45787654\n",
      "Iteration 11, loss = 0.42655841\n",
      "Iteration 12, loss = 0.39931911\n",
      "Iteration 13, loss = 0.37375521\n",
      "Iteration 14, loss = 0.35022768\n",
      "Iteration 15, loss = 0.34472202\n",
      "Iteration 16, loss = 0.58920668\n",
      "Iteration 17, loss = 2.13622445\n",
      "Iteration 18, loss = 0.51835401\n",
      "Iteration 19, loss = 0.65497843\n",
      "Iteration 20, loss = 0.46248870\n",
      "Iteration 21, loss = 0.42065107\n",
      "Iteration 22, loss = 0.38788997\n",
      "Iteration 23, loss = 0.34633008\n",
      "Iteration 24, loss = 0.32018732\n",
      "Iteration 25, loss = 0.30138156\n",
      "Iteration 26, loss = 0.28219480\n",
      "Iteration 27, loss = 0.26349605\n",
      "Iteration 28, loss = 0.24920538\n",
      "Iteration 29, loss = 0.25984173\n",
      "Iteration 30, loss = 0.52574736\n",
      "Iteration 31, loss = 2.26687888\n",
      "Iteration 32, loss = 0.32741197\n",
      "Iteration 33, loss = 0.27288396\n",
      "Iteration 34, loss = 0.29265658\n",
      "Iteration 35, loss = 0.24361152\n",
      "Iteration 36, loss = 0.26516441\n",
      "Iteration 37, loss = 0.32450264\n",
      "Iteration 38, loss = 0.48447150\n",
      "Iteration 39, loss = 0.96214106\n",
      "Iteration 40, loss = 0.44418844\n",
      "Iteration 41, loss = 0.33973327\n",
      "Iteration 42, loss = 0.30231707\n",
      "Iteration 43, loss = 0.28028769\n",
      "Iteration 44, loss = 0.26628438\n",
      "Iteration 45, loss = 0.25412146\n",
      "Iteration 46, loss = 0.24242009\n",
      "Iteration 47, loss = 0.23005287\n",
      "Iteration 48, loss = 0.21750571\n",
      "Iteration 49, loss = 0.20531656\n",
      "Iteration 50, loss = 0.19419386\n",
      "Iteration 51, loss = 0.18419163\n",
      "Iteration 52, loss = 0.17594518\n",
      "Iteration 53, loss = 0.16968450\n",
      "Iteration 54, loss = 0.16940610\n",
      "Iteration 55, loss = 0.19279955\n",
      "Iteration 56, loss = 0.33473794\n",
      "Iteration 57, loss = 1.44055379\n",
      "Iteration 58, loss = 2.56099209\n",
      "Iteration 59, loss = 0.48892473\n",
      "Iteration 60, loss = 0.34098686\n",
      "Iteration 61, loss = 0.31931374\n",
      "Iteration 62, loss = 0.31809047\n",
      "Iteration 63, loss = 0.31416732\n",
      "Iteration 64, loss = 0.30777787\n",
      "Iteration 65, loss = 0.29963183\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34884587\n",
      "Iteration 2, loss = 2.69065295\n",
      "Iteration 3, loss = 3.03817580\n",
      "Iteration 4, loss = 3.12169338\n",
      "Iteration 5, loss = 0.88887158\n",
      "Iteration 6, loss = 0.70264947\n",
      "Iteration 7, loss = 0.60028508\n",
      "Iteration 8, loss = 0.53042574\n",
      "Iteration 9, loss = 0.48119820\n",
      "Iteration 10, loss = 0.44162021\n",
      "Iteration 11, loss = 0.41013785\n",
      "Iteration 12, loss = 0.38212856\n",
      "Iteration 13, loss = 0.35616378\n",
      "Iteration 14, loss = 0.33745132\n",
      "Iteration 15, loss = 0.40197384\n",
      "Iteration 16, loss = 1.20001565\n",
      "Iteration 17, loss = 1.46203657\n",
      "Iteration 18, loss = 0.81417745\n",
      "Iteration 19, loss = 0.40605503\n",
      "Iteration 20, loss = 0.44175659\n",
      "Iteration 21, loss = 0.42392569\n",
      "Iteration 22, loss = 0.38425748\n",
      "Iteration 23, loss = 0.36677618\n",
      "Iteration 24, loss = 0.35227700\n",
      "Iteration 25, loss = 0.33560046\n",
      "Iteration 26, loss = 0.31651348\n",
      "Iteration 27, loss = 0.29566065\n",
      "Iteration 28, loss = 0.27408746\n",
      "Iteration 29, loss = 0.25313645\n",
      "Iteration 30, loss = 0.23350813\n",
      "Iteration 31, loss = 0.21627324\n",
      "Iteration 32, loss = 0.20384906\n",
      "Iteration 33, loss = 0.23086248\n",
      "Iteration 34, loss = 0.74474167\n",
      "Iteration 35, loss = 3.16224669\n",
      "Iteration 36, loss = 0.55160002\n",
      "Iteration 37, loss = 0.34112307\n",
      "Iteration 38, loss = 0.37196535\n",
      "Iteration 39, loss = 0.36280072\n",
      "Iteration 40, loss = 0.35797595\n",
      "Iteration 41, loss = 0.35009751\n",
      "Iteration 42, loss = 0.34372464\n",
      "Iteration 43, loss = 0.33550143\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34975994\n",
      "Iteration 2, loss = 2.72268738\n",
      "Iteration 3, loss = 3.06907461\n",
      "Iteration 4, loss = 3.31933919\n",
      "Iteration 5, loss = 0.88486873\n",
      "Iteration 6, loss = 0.72252447\n",
      "Iteration 7, loss = 0.61590794\n",
      "Iteration 8, loss = 0.54360482\n",
      "Iteration 9, loss = 0.49359772\n",
      "Iteration 10, loss = 0.45209434\n",
      "Iteration 11, loss = 0.41942604\n",
      "Iteration 12, loss = 0.39042411\n",
      "Iteration 13, loss = 0.36350847\n",
      "Iteration 14, loss = 0.34338805\n",
      "Iteration 15, loss = 0.40380069\n",
      "Iteration 16, loss = 1.19807560\n",
      "Iteration 17, loss = 1.60358494\n",
      "Iteration 18, loss = 0.80325025\n",
      "Iteration 19, loss = 0.41720401\n",
      "Iteration 20, loss = 0.43406634\n",
      "Iteration 21, loss = 0.43089704\n",
      "Iteration 22, loss = 0.39921417\n",
      "Iteration 23, loss = 0.37895133\n",
      "Iteration 24, loss = 0.36482929\n",
      "Iteration 25, loss = 0.34683023\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34996548\n",
      "Iteration 2, loss = 2.76103851\n",
      "Iteration 3, loss = 3.12852427\n",
      "Iteration 4, loss = 3.35596419\n",
      "Iteration 5, loss = 0.88785387\n",
      "Iteration 6, loss = 0.71096946\n",
      "Iteration 7, loss = 0.60840827\n",
      "Iteration 8, loss = 0.53892371\n",
      "Iteration 9, loss = 0.48862591\n",
      "Iteration 10, loss = 0.45076680\n",
      "Iteration 11, loss = 0.42023281\n",
      "Iteration 12, loss = 0.39315939\n",
      "Iteration 13, loss = 0.36743301\n",
      "Iteration 14, loss = 0.34341966\n",
      "Iteration 15, loss = 0.33436467\n",
      "Iteration 16, loss = 0.55331141\n",
      "Iteration 17, loss = 2.18626637\n",
      "Iteration 18, loss = 0.62600714\n",
      "Iteration 19, loss = 0.81549213\n",
      "Iteration 20, loss = 0.46531167\n",
      "Iteration 21, loss = 0.46263951\n",
      "Iteration 22, loss = 0.44231605\n",
      "Iteration 23, loss = 0.40346441\n",
      "Iteration 24, loss = 0.37863922\n",
      "Iteration 25, loss = 0.36379977\n",
      "Iteration 26, loss = 0.34709040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35652749\n",
      "Iteration 2, loss = 1.05372679\n",
      "Iteration 3, loss = 0.87031905\n",
      "Iteration 4, loss = 0.84480330\n",
      "Iteration 5, loss = 0.82648473\n",
      "Iteration 6, loss = 0.74203285\n",
      "Iteration 7, loss = 0.64689349\n",
      "Iteration 8, loss = 0.58709581\n",
      "Iteration 9, loss = 0.57098602\n",
      "Iteration 10, loss = 0.55534502\n",
      "Iteration 11, loss = 0.52471833\n",
      "Iteration 12, loss = 0.49889731\n",
      "Iteration 13, loss = 0.48128055\n",
      "Iteration 14, loss = 0.45569041\n",
      "Iteration 15, loss = 0.43095494\n",
      "Iteration 16, loss = 0.41964573\n",
      "Iteration 17, loss = 0.41059976\n",
      "Iteration 18, loss = 0.39566578\n",
      "Iteration 19, loss = 0.38263848\n",
      "Iteration 20, loss = 0.37287298\n",
      "Iteration 21, loss = 0.35914589\n",
      "Iteration 22, loss = 0.34494750\n",
      "Iteration 23, loss = 0.33738802\n",
      "Iteration 24, loss = 0.32966440\n",
      "Iteration 25, loss = 0.31779004\n",
      "Iteration 26, loss = 0.30911717\n",
      "Iteration 27, loss = 0.30217460\n",
      "Iteration 28, loss = 0.29186776\n",
      "Iteration 29, loss = 0.28340356\n",
      "Iteration 30, loss = 0.27606975\n",
      "Iteration 31, loss = 0.26604653\n",
      "Iteration 32, loss = 0.25787712\n",
      "Iteration 33, loss = 0.24966312\n",
      "Iteration 34, loss = 0.24027725\n",
      "Iteration 35, loss = 0.23283163\n",
      "Iteration 36, loss = 0.22377305\n",
      "Iteration 37, loss = 0.21602623\n",
      "Iteration 38, loss = 0.20812938\n",
      "Iteration 39, loss = 0.20012756\n",
      "Iteration 40, loss = 0.19311504\n",
      "Iteration 41, loss = 0.18535100\n",
      "Iteration 42, loss = 0.17890596\n",
      "Iteration 43, loss = 0.17165991\n",
      "Iteration 44, loss = 0.16570984\n",
      "Iteration 45, loss = 0.15905901\n",
      "Iteration 46, loss = 0.15356377\n",
      "Iteration 47, loss = 0.14764530\n",
      "Iteration 48, loss = 0.14257607\n",
      "Iteration 49, loss = 0.13759809\n",
      "Iteration 50, loss = 0.13273431\n",
      "Iteration 51, loss = 0.12871722\n",
      "Iteration 52, loss = 0.12444265\n",
      "Iteration 53, loss = 0.12075650\n",
      "Iteration 54, loss = 0.11750275\n",
      "Iteration 55, loss = 0.11407515\n",
      "Iteration 56, loss = 0.11113229\n",
      "Iteration 57, loss = 0.10863706\n",
      "Iteration 58, loss = 0.10604640\n",
      "Iteration 59, loss = 0.10359067\n",
      "Iteration 60, loss = 0.10156664\n",
      "Iteration 61, loss = 0.09978412\n",
      "Iteration 62, loss = 0.09801592\n",
      "Iteration 63, loss = 0.09624097\n",
      "Iteration 64, loss = 0.09469623\n",
      "Iteration 65, loss = 0.09342461\n",
      "Iteration 66, loss = 0.09231195\n",
      "Iteration 67, loss = 0.09126296\n",
      "Iteration 68, loss = 0.09015008\n",
      "Iteration 69, loss = 0.08905308\n",
      "Iteration 70, loss = 0.08808610\n",
      "Iteration 71, loss = 0.08731285\n",
      "Iteration 72, loss = 0.08668447\n",
      "Iteration 73, loss = 0.08612913\n",
      "Iteration 74, loss = 0.08559578\n",
      "Iteration 75, loss = 0.08494563\n",
      "Iteration 76, loss = 0.08425072\n",
      "Iteration 77, loss = 0.08362216\n",
      "Iteration 78, loss = 0.08316906\n",
      "Iteration 79, loss = 0.08285447\n",
      "Iteration 80, loss = 0.08256340\n",
      "Iteration 81, loss = 0.08223583\n",
      "Iteration 82, loss = 0.08177869\n",
      "Iteration 83, loss = 0.08130384\n",
      "Iteration 84, loss = 0.08091398\n",
      "Iteration 85, loss = 0.08065695\n",
      "Iteration 86, loss = 0.08048266\n",
      "Iteration 87, loss = 0.08028282\n",
      "Iteration 88, loss = 0.08004698\n",
      "Iteration 89, loss = 0.07972128\n",
      "Iteration 90, loss = 0.07938852\n",
      "Iteration 91, loss = 0.07910877\n",
      "Iteration 92, loss = 0.07891001\n",
      "Iteration 93, loss = 0.07876827\n",
      "Iteration 94, loss = 0.07863841\n",
      "Iteration 95, loss = 0.07849487\n",
      "Iteration 96, loss = 0.07829639\n",
      "Iteration 97, loss = 0.07807306\n",
      "Iteration 98, loss = 0.07783132\n",
      "Iteration 99, loss = 0.07761535\n",
      "Iteration 100, loss = 0.07743994\n",
      "Iteration 101, loss = 0.07730187\n",
      "Iteration 102, loss = 0.07718966\n",
      "Iteration 103, loss = 0.07708932\n",
      "Iteration 104, loss = 0.07699854\n",
      "Iteration 105, loss = 0.07689382\n",
      "Iteration 106, loss = 0.07678347\n",
      "Iteration 107, loss = 0.07662682\n",
      "Iteration 108, loss = 0.07645605\n",
      "Iteration 109, loss = 0.07626344\n",
      "Iteration 110, loss = 0.07608696\n",
      "Iteration 111, loss = 0.07594007\n",
      "Iteration 112, loss = 0.07582441\n",
      "Iteration 113, loss = 0.07573137\n",
      "Iteration 114, loss = 0.07565534\n",
      "Iteration 115, loss = 0.07561088\n",
      "Iteration 116, loss = 0.07558553\n",
      "Iteration 117, loss = 0.07560707\n",
      "Iteration 118, loss = 0.07559700\n",
      "Iteration 119, loss = 0.07557226\n",
      "Iteration 120, loss = 0.07534494\n",
      "Iteration 121, loss = 0.07504999\n",
      "Iteration 122, loss = 0.07480575\n",
      "Iteration 123, loss = 0.07472448\n",
      "Iteration 124, loss = 0.07476002\n",
      "Iteration 125, loss = 0.07477519\n",
      "Iteration 126, loss = 0.07469822\n",
      "Iteration 127, loss = 0.07449096\n",
      "Iteration 128, loss = 0.07430784\n",
      "Iteration 129, loss = 0.07421178\n",
      "Iteration 130, loss = 0.07419800\n",
      "Iteration 131, loss = 0.07420144\n",
      "Iteration 132, loss = 0.07414763\n",
      "Iteration 133, loss = 0.07403527\n",
      "Iteration 134, loss = 0.07388387\n",
      "Iteration 135, loss = 0.07375839\n",
      "Iteration 136, loss = 0.07369109\n",
      "Iteration 137, loss = 0.07366336\n",
      "Iteration 138, loss = 0.07364175\n",
      "Iteration 139, loss = 0.07359900\n",
      "Iteration 140, loss = 0.07354017\n",
      "Iteration 141, loss = 0.07346341\n",
      "Iteration 142, loss = 0.07338351\n",
      "Iteration 143, loss = 0.07328990\n",
      "Iteration 144, loss = 0.07320309\n",
      "Iteration 145, loss = 0.07311604\n",
      "Iteration 146, loss = 0.07303377\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35233713\n",
      "Iteration 2, loss = 1.05869727\n",
      "Iteration 3, loss = 0.87399416\n",
      "Iteration 4, loss = 0.84882175\n",
      "Iteration 5, loss = 0.83083761\n",
      "Iteration 6, loss = 0.74569409\n",
      "Iteration 7, loss = 0.64954930\n",
      "Iteration 8, loss = 0.59058945\n",
      "Iteration 9, loss = 0.57576136\n",
      "Iteration 10, loss = 0.55838285\n",
      "Iteration 11, loss = 0.52583160\n",
      "Iteration 12, loss = 0.49973865\n",
      "Iteration 13, loss = 0.48214355\n",
      "Iteration 14, loss = 0.45463624\n",
      "Iteration 15, loss = 0.42765176\n",
      "Iteration 16, loss = 0.41460327\n",
      "Iteration 17, loss = 0.40423309\n",
      "Iteration 18, loss = 0.38857005\n",
      "Iteration 19, loss = 0.37628401\n",
      "Iteration 20, loss = 0.36945126\n",
      "Iteration 21, loss = 0.35829875\n",
      "Iteration 22, loss = 0.34400048\n",
      "Iteration 23, loss = 0.33522126\n",
      "Iteration 24, loss = 0.32727237\n",
      "Iteration 25, loss = 0.31472195\n",
      "Iteration 26, loss = 0.30455261\n",
      "Iteration 27, loss = 0.29722943\n",
      "Iteration 28, loss = 0.28672077\n",
      "Iteration 29, loss = 0.27722563\n",
      "Iteration 30, loss = 0.26973033\n",
      "Iteration 31, loss = 0.25956563\n",
      "Iteration 32, loss = 0.25074264\n",
      "Iteration 33, loss = 0.24272757\n",
      "Iteration 34, loss = 0.23298377\n",
      "Iteration 35, loss = 0.22531430\n",
      "Iteration 36, loss = 0.21676279\n",
      "Iteration 37, loss = 0.20863310\n",
      "Iteration 38, loss = 0.20146314\n",
      "Iteration 39, loss = 0.19320739\n",
      "Iteration 40, loss = 0.18640157\n",
      "Iteration 41, loss = 0.17860129\n",
      "Iteration 42, loss = 0.17191511\n",
      "Iteration 43, loss = 0.16490486\n",
      "Iteration 44, loss = 0.15858314\n",
      "Iteration 45, loss = 0.15237338\n",
      "Iteration 46, loss = 0.14660722\n",
      "Iteration 47, loss = 0.14105172\n",
      "Iteration 48, loss = 0.13598872\n",
      "Iteration 49, loss = 0.13091108\n",
      "Iteration 50, loss = 0.12653285\n",
      "Iteration 51, loss = 0.12194908\n",
      "Iteration 52, loss = 0.11817643\n",
      "Iteration 53, loss = 0.11426585\n",
      "Iteration 54, loss = 0.11081285\n",
      "Iteration 55, loss = 0.10774653\n",
      "Iteration 56, loss = 0.10455488\n",
      "Iteration 57, loss = 0.10198300\n",
      "Iteration 58, loss = 0.09953323\n",
      "Iteration 59, loss = 0.09706633\n",
      "Iteration 60, loss = 0.09509494\n",
      "Iteration 61, loss = 0.09327270\n",
      "Iteration 62, loss = 0.09139601\n",
      "Iteration 63, loss = 0.08983799\n",
      "Iteration 64, loss = 0.08853737\n",
      "Iteration 65, loss = 0.08718820\n",
      "Iteration 66, loss = 0.08590553\n",
      "Iteration 67, loss = 0.08487073\n",
      "Iteration 68, loss = 0.08397926\n",
      "Iteration 69, loss = 0.08307917\n",
      "Iteration 70, loss = 0.08218217\n",
      "Iteration 71, loss = 0.08142637\n",
      "Iteration 72, loss = 0.08081666\n",
      "Iteration 73, loss = 0.08025902\n",
      "Iteration 74, loss = 0.07969475\n",
      "Iteration 75, loss = 0.07912302\n",
      "Iteration 76, loss = 0.07861366\n",
      "Iteration 77, loss = 0.07819286\n",
      "Iteration 78, loss = 0.07784109\n",
      "Iteration 79, loss = 0.07752633\n",
      "Iteration 80, loss = 0.07721833\n",
      "Iteration 81, loss = 0.07690684\n",
      "Iteration 82, loss = 0.07658307\n",
      "Iteration 83, loss = 0.07627558\n",
      "Iteration 84, loss = 0.07600329\n",
      "Iteration 85, loss = 0.07577345\n",
      "Iteration 86, loss = 0.07557867\n",
      "Iteration 87, loss = 0.07541141\n",
      "Iteration 88, loss = 0.07527423\n",
      "Iteration 89, loss = 0.07515521\n",
      "Iteration 90, loss = 0.07505657\n",
      "Iteration 91, loss = 0.07490178\n",
      "Iteration 92, loss = 0.07469593\n",
      "Iteration 93, loss = 0.07443854\n",
      "Iteration 94, loss = 0.07423720\n",
      "Iteration 95, loss = 0.07413026\n",
      "Iteration 96, loss = 0.07407725\n",
      "Iteration 97, loss = 0.07401694\n",
      "Iteration 98, loss = 0.07388863\n",
      "Iteration 99, loss = 0.07371646\n",
      "Iteration 100, loss = 0.07354507\n",
      "Iteration 101, loss = 0.07342785\n",
      "Iteration 102, loss = 0.07336170\n",
      "Iteration 103, loss = 0.07330893\n",
      "Iteration 104, loss = 0.07323776\n",
      "Iteration 105, loss = 0.07312455\n",
      "Iteration 106, loss = 0.07299318\n",
      "Iteration 107, loss = 0.07286758\n",
      "Iteration 108, loss = 0.07277056\n",
      "Iteration 109, loss = 0.07270098\n",
      "Iteration 110, loss = 0.07264408\n",
      "Iteration 111, loss = 0.07258778\n",
      "Iteration 112, loss = 0.07251661\n",
      "Iteration 113, loss = 0.07243382\n",
      "Iteration 114, loss = 0.07233594\n",
      "Iteration 115, loss = 0.07223681\n",
      "Iteration 116, loss = 0.07214168\n",
      "Iteration 117, loss = 0.07205513\n",
      "Iteration 118, loss = 0.07197592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34884587\n",
      "Iteration 2, loss = 1.05517284\n",
      "Iteration 3, loss = 0.86604109\n",
      "Iteration 4, loss = 0.83525191\n",
      "Iteration 5, loss = 0.81810118\n",
      "Iteration 6, loss = 0.73589478\n",
      "Iteration 7, loss = 0.63971791\n",
      "Iteration 8, loss = 0.57791441\n",
      "Iteration 9, loss = 0.56097129\n",
      "Iteration 10, loss = 0.54542034\n",
      "Iteration 11, loss = 0.51394833\n",
      "Iteration 12, loss = 0.48628164\n",
      "Iteration 13, loss = 0.46873894\n",
      "Iteration 14, loss = 0.44356733\n",
      "Iteration 15, loss = 0.41594276\n",
      "Iteration 16, loss = 0.40074118\n",
      "Iteration 17, loss = 0.39091835\n",
      "Iteration 18, loss = 0.37594826\n",
      "Iteration 19, loss = 0.36181398\n",
      "Iteration 20, loss = 0.35376878\n",
      "Iteration 21, loss = 0.34368021\n",
      "Iteration 22, loss = 0.32923097\n",
      "Iteration 23, loss = 0.31898694\n",
      "Iteration 24, loss = 0.31098925\n",
      "Iteration 25, loss = 0.29881152\n",
      "Iteration 26, loss = 0.28736681\n",
      "Iteration 27, loss = 0.27936071\n",
      "Iteration 28, loss = 0.26907003\n",
      "Iteration 29, loss = 0.25850523\n",
      "Iteration 30, loss = 0.25043508\n",
      "Iteration 31, loss = 0.24038834\n",
      "Iteration 32, loss = 0.23049035\n",
      "Iteration 33, loss = 0.22214264\n",
      "Iteration 34, loss = 0.21208110\n",
      "Iteration 35, loss = 0.20369512\n",
      "Iteration 36, loss = 0.19476671\n",
      "Iteration 37, loss = 0.18594656\n",
      "Iteration 38, loss = 0.17812661\n",
      "Iteration 39, loss = 0.16941123\n",
      "Iteration 40, loss = 0.16200971\n",
      "Iteration 41, loss = 0.15383159\n",
      "Iteration 42, loss = 0.14673224\n",
      "Iteration 43, loss = 0.13933566\n",
      "Iteration 44, loss = 0.13288533\n",
      "Iteration 45, loss = 0.12632500\n",
      "Iteration 46, loss = 0.12061137\n",
      "Iteration 47, loss = 0.11479874\n",
      "Iteration 48, loss = 0.10988850\n",
      "Iteration 49, loss = 0.10477257\n",
      "Iteration 50, loss = 0.10049777\n",
      "Iteration 51, loss = 0.09623435\n",
      "Iteration 52, loss = 0.09239592\n",
      "Iteration 53, loss = 0.08901409\n",
      "Iteration 54, loss = 0.08562828\n",
      "Iteration 55, loss = 0.08283754\n",
      "Iteration 56, loss = 0.08015591\n",
      "Iteration 57, loss = 0.07764267\n",
      "Iteration 58, loss = 0.07557685\n",
      "Iteration 59, loss = 0.07352829\n",
      "Iteration 60, loss = 0.07168162\n",
      "Iteration 61, loss = 0.07015484\n",
      "Iteration 62, loss = 0.06865043\n",
      "Iteration 63, loss = 0.06727572\n",
      "Iteration 64, loss = 0.06614899\n",
      "Iteration 65, loss = 0.06508382\n",
      "Iteration 66, loss = 0.06406329\n",
      "Iteration 67, loss = 0.06318634\n",
      "Iteration 68, loss = 0.06243747\n",
      "Iteration 69, loss = 0.06172524\n",
      "Iteration 70, loss = 0.06105099\n",
      "Iteration 71, loss = 0.06047272\n",
      "Iteration 72, loss = 0.05997541\n",
      "Iteration 73, loss = 0.05950682\n",
      "Iteration 74, loss = 0.05906036\n",
      "Iteration 75, loss = 0.05865142\n",
      "Iteration 76, loss = 0.05830031\n",
      "Iteration 77, loss = 0.05799619\n",
      "Iteration 78, loss = 0.05771472\n",
      "Iteration 79, loss = 0.05744419\n",
      "Iteration 80, loss = 0.05718869\n",
      "Iteration 81, loss = 0.05695649\n",
      "Iteration 82, loss = 0.05675264\n",
      "Iteration 83, loss = 0.05657390\n",
      "Iteration 84, loss = 0.05641220\n",
      "Iteration 85, loss = 0.05626261\n",
      "Iteration 86, loss = 0.05611714\n",
      "Iteration 87, loss = 0.05597391\n",
      "Iteration 88, loss = 0.05583335\n",
      "Iteration 89, loss = 0.05570125\n",
      "Iteration 90, loss = 0.05558219\n",
      "Iteration 91, loss = 0.05547616\n",
      "Iteration 92, loss = 0.05538057\n",
      "Iteration 93, loss = 0.05529100\n",
      "Iteration 94, loss = 0.05520333\n",
      "Iteration 95, loss = 0.05511565\n",
      "Iteration 96, loss = 0.05502334\n",
      "Iteration 97, loss = 0.05492846\n",
      "Iteration 98, loss = 0.05483247\n",
      "Iteration 99, loss = 0.05474110\n",
      "Iteration 100, loss = 0.05465673\n",
      "Iteration 101, loss = 0.05457909\n",
      "Iteration 102, loss = 0.05450625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34975994\n",
      "Iteration 2, loss = 1.05079277\n",
      "Iteration 3, loss = 0.86503767\n",
      "Iteration 4, loss = 0.83824751\n",
      "Iteration 5, loss = 0.81894764\n",
      "Iteration 6, loss = 0.73494737\n",
      "Iteration 7, loss = 0.63905805\n",
      "Iteration 8, loss = 0.57888922\n",
      "Iteration 9, loss = 0.56313018\n",
      "Iteration 10, loss = 0.54690158\n",
      "Iteration 11, loss = 0.51474619\n",
      "Iteration 12, loss = 0.48762701\n",
      "Iteration 13, loss = 0.46982539\n",
      "Iteration 14, loss = 0.44275450\n",
      "Iteration 15, loss = 0.41490641\n",
      "Iteration 16, loss = 0.40115915\n",
      "Iteration 17, loss = 0.39158627\n",
      "Iteration 18, loss = 0.37648929\n",
      "Iteration 19, loss = 0.36312075\n",
      "Iteration 20, loss = 0.35576427\n",
      "Iteration 21, loss = 0.34481089\n",
      "Iteration 22, loss = 0.32976221\n",
      "Iteration 23, loss = 0.31967298\n",
      "Iteration 24, loss = 0.31166934\n",
      "Iteration 25, loss = 0.29907362\n",
      "Iteration 26, loss = 0.28783582\n",
      "Iteration 27, loss = 0.28011518\n",
      "Iteration 28, loss = 0.26985925\n",
      "Iteration 29, loss = 0.25955992\n",
      "Iteration 30, loss = 0.25189998\n",
      "Iteration 31, loss = 0.24204462\n",
      "Iteration 32, loss = 0.23236539\n",
      "Iteration 33, loss = 0.22450684\n",
      "Iteration 34, loss = 0.21482144\n",
      "Iteration 35, loss = 0.20678732\n",
      "Iteration 36, loss = 0.19829886\n",
      "Iteration 37, loss = 0.18946663\n",
      "Iteration 38, loss = 0.18194266\n",
      "Iteration 39, loss = 0.17323420\n",
      "Iteration 40, loss = 0.16591522\n",
      "Iteration 41, loss = 0.15804032\n",
      "Iteration 42, loss = 0.15092947\n",
      "Iteration 43, loss = 0.14381461\n",
      "Iteration 44, loss = 0.13716169\n",
      "Iteration 45, loss = 0.13074645\n",
      "Iteration 46, loss = 0.12460321\n",
      "Iteration 47, loss = 0.11877466\n",
      "Iteration 48, loss = 0.11340883\n",
      "Iteration 49, loss = 0.10803702\n",
      "Iteration 50, loss = 0.10338949\n",
      "Iteration 51, loss = 0.09861511\n",
      "Iteration 52, loss = 0.09451905\n",
      "Iteration 53, loss = 0.09054043\n",
      "Iteration 54, loss = 0.08678564\n",
      "Iteration 55, loss = 0.08358365\n",
      "Iteration 56, loss = 0.08035554\n",
      "Iteration 57, loss = 0.07744398\n",
      "Iteration 58, loss = 0.07491654\n",
      "Iteration 59, loss = 0.07240187\n",
      "Iteration 60, loss = 0.07009178\n",
      "Iteration 61, loss = 0.06811850\n",
      "Iteration 62, loss = 0.06625530\n",
      "Iteration 63, loss = 0.06443952\n",
      "Iteration 64, loss = 0.06279962\n",
      "Iteration 65, loss = 0.06138510\n",
      "Iteration 66, loss = 0.06012595\n",
      "Iteration 67, loss = 0.05890369\n",
      "Iteration 68, loss = 0.05770019\n",
      "Iteration 69, loss = 0.05660203\n",
      "Iteration 70, loss = 0.05567749\n",
      "Iteration 71, loss = 0.05487834\n",
      "Iteration 72, loss = 0.05412250\n",
      "Iteration 73, loss = 0.05337317\n",
      "Iteration 74, loss = 0.05261577\n",
      "Iteration 75, loss = 0.05192353\n",
      "Iteration 76, loss = 0.05133123\n",
      "Iteration 77, loss = 0.05082844\n",
      "Iteration 78, loss = 0.05038950\n",
      "Iteration 79, loss = 0.04997835\n",
      "Iteration 80, loss = 0.04957684\n",
      "Iteration 81, loss = 0.04912665\n",
      "Iteration 82, loss = 0.04866695\n",
      "Iteration 83, loss = 0.04824522\n",
      "Iteration 84, loss = 0.04790695\n",
      "Iteration 85, loss = 0.04763883\n",
      "Iteration 86, loss = 0.04740468\n",
      "Iteration 87, loss = 0.04717698\n",
      "Iteration 88, loss = 0.04690714\n",
      "Iteration 89, loss = 0.04660795\n",
      "Iteration 90, loss = 0.04629562\n",
      "Iteration 91, loss = 0.04602747\n",
      "Iteration 92, loss = 0.04581815\n",
      "Iteration 93, loss = 0.04565143\n",
      "Iteration 94, loss = 0.04550549\n",
      "Iteration 95, loss = 0.04535270\n",
      "Iteration 96, loss = 0.04518689\n",
      "Iteration 97, loss = 0.04497840\n",
      "Iteration 98, loss = 0.04475552\n",
      "Iteration 99, loss = 0.04453979\n",
      "Iteration 100, loss = 0.04436208\n",
      "Iteration 101, loss = 0.04422336\n",
      "Iteration 102, loss = 0.04411061\n",
      "Iteration 103, loss = 0.04401246\n",
      "Iteration 104, loss = 0.04391105\n",
      "Iteration 105, loss = 0.04380735\n",
      "Iteration 106, loss = 0.04366740\n",
      "Iteration 107, loss = 0.04350884\n",
      "Iteration 108, loss = 0.04332922\n",
      "Iteration 109, loss = 0.04316690\n",
      "Iteration 110, loss = 0.04303855\n",
      "Iteration 111, loss = 0.04293901\n",
      "Iteration 112, loss = 0.04285949\n",
      "Iteration 113, loss = 0.04278950\n",
      "Iteration 114, loss = 0.04272625\n",
      "Iteration 115, loss = 0.04265469\n",
      "Iteration 116, loss = 0.04258187\n",
      "Iteration 117, loss = 0.04247769\n",
      "Iteration 118, loss = 0.04236756\n",
      "Iteration 119, loss = 0.04223232\n",
      "Iteration 120, loss = 0.04210120\n",
      "Iteration 121, loss = 0.04198239\n",
      "Iteration 122, loss = 0.04188514\n",
      "Iteration 123, loss = 0.04181092\n",
      "Iteration 124, loss = 0.04175258\n",
      "Iteration 125, loss = 0.04171052\n",
      "Iteration 126, loss = 0.04169156\n",
      "Iteration 127, loss = 0.04170998\n",
      "Iteration 128, loss = 0.04173193\n",
      "Iteration 129, loss = 0.04177816\n",
      "Iteration 130, loss = 0.04166597\n",
      "Iteration 131, loss = 0.04146898\n",
      "Iteration 132, loss = 0.04121545\n",
      "Iteration 133, loss = 0.04109901\n",
      "Iteration 134, loss = 0.04112658\n",
      "Iteration 135, loss = 0.04117108\n",
      "Iteration 136, loss = 0.04113661\n",
      "Iteration 137, loss = 0.04097100\n",
      "Iteration 138, loss = 0.04081634\n",
      "Iteration 139, loss = 0.04075705\n",
      "Iteration 140, loss = 0.04077442\n",
      "Iteration 141, loss = 0.04078132\n",
      "Iteration 142, loss = 0.04070068\n",
      "Iteration 143, loss = 0.04058044\n",
      "Iteration 144, loss = 0.04048237\n",
      "Iteration 145, loss = 0.04044619\n",
      "Iteration 146, loss = 0.04044367\n",
      "Iteration 147, loss = 0.04041991\n",
      "Iteration 148, loss = 0.04035905\n",
      "Iteration 149, loss = 0.04026861\n",
      "Iteration 150, loss = 0.04019072\n",
      "Iteration 151, loss = 0.04014255\n",
      "Iteration 152, loss = 0.04011765\n",
      "Iteration 153, loss = 0.04009743\n",
      "Iteration 154, loss = 0.04006235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34996548\n",
      "Iteration 2, loss = 1.03285657\n",
      "Iteration 3, loss = 0.85880740\n",
      "Iteration 4, loss = 0.83845101\n",
      "Iteration 5, loss = 0.81071523\n",
      "Iteration 6, loss = 0.72122095\n",
      "Iteration 7, loss = 0.62847839\n",
      "Iteration 8, loss = 0.57406577\n",
      "Iteration 9, loss = 0.55981003\n",
      "Iteration 10, loss = 0.54096305\n",
      "Iteration 11, loss = 0.50984703\n",
      "Iteration 12, loss = 0.48668091\n",
      "Iteration 13, loss = 0.46822171\n",
      "Iteration 14, loss = 0.43990996\n",
      "Iteration 15, loss = 0.41603423\n",
      "Iteration 16, loss = 0.40488225\n",
      "Iteration 17, loss = 0.39297945\n",
      "Iteration 18, loss = 0.37804573\n",
      "Iteration 19, loss = 0.36909027\n",
      "Iteration 20, loss = 0.36190164\n",
      "Iteration 21, loss = 0.34884386\n",
      "Iteration 22, loss = 0.33724840\n",
      "Iteration 23, loss = 0.32988736\n",
      "Iteration 24, loss = 0.31977424\n",
      "Iteration 25, loss = 0.30809648\n",
      "Iteration 26, loss = 0.30045155\n",
      "Iteration 27, loss = 0.29177561\n",
      "Iteration 28, loss = 0.28144296\n",
      "Iteration 29, loss = 0.27413206\n",
      "Iteration 30, loss = 0.26576533\n",
      "Iteration 31, loss = 0.25613664\n",
      "Iteration 32, loss = 0.24894293\n",
      "Iteration 33, loss = 0.24000710\n",
      "Iteration 34, loss = 0.23184797\n",
      "Iteration 35, loss = 0.22428433\n",
      "Iteration 36, loss = 0.21587276\n",
      "Iteration 37, loss = 0.20911729\n",
      "Iteration 38, loss = 0.20130061\n",
      "Iteration 39, loss = 0.19447112\n",
      "Iteration 40, loss = 0.18739992\n",
      "Iteration 41, loss = 0.18051773\n",
      "Iteration 42, loss = 0.17411008\n",
      "Iteration 43, loss = 0.16750400\n",
      "Iteration 44, loss = 0.16170100\n",
      "Iteration 45, loss = 0.15563265\n",
      "Iteration 46, loss = 0.15030125\n",
      "Iteration 47, loss = 0.14499103\n",
      "Iteration 48, loss = 0.14009559\n",
      "Iteration 49, loss = 0.13554531\n",
      "Iteration 50, loss = 0.13097840\n",
      "Iteration 51, loss = 0.12707998\n",
      "Iteration 52, loss = 0.12312209\n",
      "Iteration 53, loss = 0.11948094\n",
      "Iteration 54, loss = 0.11632675\n",
      "Iteration 55, loss = 0.11312790\n",
      "Iteration 56, loss = 0.11018145\n",
      "Iteration 57, loss = 0.10765622\n",
      "Iteration 58, loss = 0.10523527\n",
      "Iteration 59, loss = 0.10286533\n",
      "Iteration 60, loss = 0.10072714\n",
      "Iteration 61, loss = 0.09887523\n",
      "Iteration 62, loss = 0.09724097\n",
      "Iteration 63, loss = 0.09574424\n",
      "Iteration 64, loss = 0.09430411\n",
      "Iteration 65, loss = 0.09284576\n",
      "Iteration 66, loss = 0.09150006\n",
      "Iteration 67, loss = 0.09036796\n",
      "Iteration 68, loss = 0.08943455\n",
      "Iteration 69, loss = 0.08865601\n",
      "Iteration 70, loss = 0.08794827\n",
      "Iteration 71, loss = 0.08717298\n",
      "Iteration 72, loss = 0.08621945\n",
      "Iteration 73, loss = 0.08536918\n",
      "Iteration 74, loss = 0.08481600\n",
      "Iteration 75, loss = 0.08445169\n",
      "Iteration 76, loss = 0.08405413\n",
      "Iteration 77, loss = 0.08344361\n",
      "Iteration 78, loss = 0.08279646\n",
      "Iteration 79, loss = 0.08234359\n",
      "Iteration 80, loss = 0.08209170\n",
      "Iteration 81, loss = 0.08186667\n",
      "Iteration 82, loss = 0.08149240\n",
      "Iteration 83, loss = 0.08102918\n",
      "Iteration 84, loss = 0.08063400\n",
      "Iteration 85, loss = 0.08039859\n",
      "Iteration 86, loss = 0.08024793\n",
      "Iteration 87, loss = 0.08005318\n",
      "Iteration 88, loss = 0.07977449\n",
      "Iteration 89, loss = 0.07943880\n",
      "Iteration 90, loss = 0.07915977\n",
      "Iteration 91, loss = 0.07897599\n",
      "Iteration 92, loss = 0.07885115\n",
      "Iteration 93, loss = 0.07873025\n",
      "Iteration 94, loss = 0.07856158\n",
      "Iteration 95, loss = 0.07834890\n",
      "Iteration 96, loss = 0.07810997\n",
      "Iteration 97, loss = 0.07790028\n",
      "Iteration 98, loss = 0.07773975\n",
      "Iteration 99, loss = 0.07762036\n",
      "Iteration 100, loss = 0.07752352\n",
      "Iteration 101, loss = 0.07742837\n",
      "Iteration 102, loss = 0.07732828\n",
      "Iteration 103, loss = 0.07719860\n",
      "Iteration 104, loss = 0.07705217\n",
      "Iteration 105, loss = 0.07688107\n",
      "Iteration 106, loss = 0.07671407\n",
      "Iteration 107, loss = 0.07656280\n",
      "Iteration 108, loss = 0.07643601\n",
      "Iteration 109, loss = 0.07633030\n",
      "Iteration 110, loss = 0.07624083\n",
      "Iteration 111, loss = 0.07616486\n",
      "Iteration 112, loss = 0.07610250\n",
      "Iteration 113, loss = 0.07606823\n",
      "Iteration 114, loss = 0.07605430\n",
      "Iteration 115, loss = 0.07608862\n",
      "Iteration 116, loss = 0.07606829\n",
      "Iteration 117, loss = 0.07598864\n",
      "Iteration 118, loss = 0.07570984\n",
      "Iteration 119, loss = 0.07542385\n",
      "Iteration 120, loss = 0.07526509\n",
      "Iteration 121, loss = 0.07526992\n",
      "Iteration 122, loss = 0.07532476\n",
      "Iteration 123, loss = 0.07526982\n",
      "Iteration 124, loss = 0.07510690\n",
      "Iteration 125, loss = 0.07490548\n",
      "Iteration 126, loss = 0.07479641\n",
      "Iteration 127, loss = 0.07478633\n",
      "Iteration 128, loss = 0.07478885\n",
      "Iteration 129, loss = 0.07473774\n",
      "Iteration 130, loss = 0.07460483\n",
      "Iteration 131, loss = 0.07446241\n",
      "Iteration 132, loss = 0.07436249\n",
      "Iteration 133, loss = 0.07431799\n",
      "Iteration 134, loss = 0.07430093\n",
      "Iteration 135, loss = 0.07426974\n",
      "Iteration 136, loss = 0.07421279\n",
      "Iteration 137, loss = 0.07411844\n",
      "Iteration 138, loss = 0.07401602\n",
      "Iteration 139, loss = 0.07391663\n",
      "Iteration 140, loss = 0.07383496\n",
      "Iteration 141, loss = 0.07377128\n",
      "Iteration 142, loss = 0.07372214\n",
      "Iteration 143, loss = 0.07368587\n",
      "Iteration 144, loss = 0.07366150\n",
      "Iteration 145, loss = 0.07365990\n",
      "Iteration 146, loss = 0.07367610\n",
      "Iteration 147, loss = 0.07373763\n",
      "Iteration 148, loss = 0.07376782\n",
      "Iteration 149, loss = 0.07377911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35652749\n",
      "Iteration 2, loss = 1.15359881\n",
      "Iteration 3, loss = 0.96938958\n",
      "Iteration 4, loss = 0.86592362\n",
      "Iteration 5, loss = 0.85570323\n",
      "Iteration 6, loss = 0.82596799\n",
      "Iteration 7, loss = 0.75964938\n",
      "Iteration 8, loss = 0.69899234\n",
      "Iteration 9, loss = 0.65633874\n",
      "Iteration 10, loss = 0.62428740\n",
      "Iteration 11, loss = 0.59781428\n",
      "Iteration 12, loss = 0.57570120\n",
      "Iteration 13, loss = 0.55677272\n",
      "Iteration 14, loss = 0.53832107\n",
      "Iteration 15, loss = 0.52084449\n",
      "Iteration 16, loss = 0.50446117\n",
      "Iteration 17, loss = 0.48905808\n",
      "Iteration 18, loss = 0.47472471\n",
      "Iteration 19, loss = 0.46240436\n",
      "Iteration 20, loss = 0.45174934\n",
      "Iteration 21, loss = 0.44199774\n",
      "Iteration 22, loss = 0.43284743\n",
      "Iteration 23, loss = 0.42417531\n",
      "Iteration 24, loss = 0.41582237\n",
      "Iteration 25, loss = 0.40777768\n",
      "Iteration 26, loss = 0.39989025\n",
      "Iteration 27, loss = 0.39231911\n",
      "Iteration 28, loss = 0.38539149\n",
      "Iteration 29, loss = 0.37912740\n",
      "Iteration 30, loss = 0.37333388\n",
      "Iteration 31, loss = 0.36786652\n",
      "Iteration 32, loss = 0.36263762\n",
      "Iteration 33, loss = 0.35763580\n",
      "Iteration 34, loss = 0.35282792\n",
      "Iteration 35, loss = 0.34817471\n",
      "Iteration 36, loss = 0.34364061\n",
      "Iteration 37, loss = 0.33921850\n",
      "Iteration 38, loss = 0.33491231\n",
      "Iteration 39, loss = 0.33071418\n",
      "Iteration 40, loss = 0.32661427\n",
      "Iteration 41, loss = 0.32261624\n",
      "Iteration 42, loss = 0.31871896\n",
      "Iteration 43, loss = 0.31491756\n",
      "Iteration 44, loss = 0.31119490\n",
      "Iteration 45, loss = 0.30754981\n",
      "Iteration 46, loss = 0.30398102\n",
      "Iteration 47, loss = 0.30048504\n",
      "Iteration 48, loss = 0.29705936\n",
      "Iteration 49, loss = 0.29370226\n",
      "Iteration 50, loss = 0.29041182\n",
      "Iteration 51, loss = 0.28718932\n",
      "Iteration 52, loss = 0.28403176\n",
      "Iteration 53, loss = 0.28093760\n",
      "Iteration 54, loss = 0.27790620\n",
      "Iteration 55, loss = 0.27493609\n",
      "Iteration 56, loss = 0.27203231\n",
      "Iteration 57, loss = 0.26918464\n",
      "Iteration 58, loss = 0.26639418\n",
      "Iteration 59, loss = 0.26365821\n",
      "Iteration 60, loss = 0.26097613\n",
      "Iteration 61, loss = 0.25834671\n",
      "Iteration 62, loss = 0.25576916\n",
      "Iteration 63, loss = 0.25324203\n",
      "Iteration 64, loss = 0.25076816\n",
      "Iteration 65, loss = 0.24834308\n",
      "Iteration 66, loss = 0.24596629\n",
      "Iteration 67, loss = 0.24363636\n",
      "Iteration 68, loss = 0.24135302\n",
      "Iteration 69, loss = 0.23911568\n",
      "Iteration 70, loss = 0.23692364\n",
      "Iteration 71, loss = 0.23477564\n",
      "Iteration 72, loss = 0.23266979\n",
      "Iteration 73, loss = 0.23060675\n",
      "Iteration 74, loss = 0.22858471\n",
      "Iteration 75, loss = 0.22660281\n",
      "Iteration 76, loss = 0.22466104\n",
      "Iteration 77, loss = 0.22275837\n",
      "Iteration 78, loss = 0.22089333\n",
      "Iteration 79, loss = 0.21906523\n",
      "Iteration 80, loss = 0.21727339\n",
      "Iteration 81, loss = 0.21551759\n",
      "Iteration 82, loss = 0.21379546\n",
      "Iteration 83, loss = 0.21210749\n",
      "Iteration 84, loss = 0.21045313\n",
      "Iteration 85, loss = 0.20883101\n",
      "Iteration 86, loss = 0.20723999\n",
      "Iteration 87, loss = 0.20568119\n",
      "Iteration 88, loss = 0.20415341\n",
      "Iteration 89, loss = 0.20265487\n",
      "Iteration 90, loss = 0.20118509\n",
      "Iteration 91, loss = 0.19974443\n",
      "Iteration 92, loss = 0.19833064\n",
      "Iteration 93, loss = 0.19694441\n",
      "Iteration 94, loss = 0.19558499\n",
      "Iteration 95, loss = 0.19425139\n",
      "Iteration 96, loss = 0.19294431\n",
      "Iteration 97, loss = 0.19166089\n",
      "Iteration 98, loss = 0.19040270\n",
      "Iteration 99, loss = 0.18916705\n",
      "Iteration 100, loss = 0.18795541\n",
      "Iteration 101, loss = 0.18676655\n",
      "Iteration 102, loss = 0.18560055\n",
      "Iteration 103, loss = 0.18445711\n",
      "Iteration 104, loss = 0.18333407\n",
      "Iteration 105, loss = 0.18223208\n",
      "Iteration 106, loss = 0.18115051\n",
      "Iteration 107, loss = 0.18008852\n",
      "Iteration 108, loss = 0.17904587\n",
      "Iteration 109, loss = 0.17802278\n",
      "Iteration 110, loss = 0.17701721\n",
      "Iteration 111, loss = 0.17602975\n",
      "Iteration 112, loss = 0.17506095\n",
      "Iteration 113, loss = 0.17410799\n",
      "Iteration 114, loss = 0.17317230\n",
      "Iteration 115, loss = 0.17225407\n",
      "Iteration 116, loss = 0.17135082\n",
      "Iteration 117, loss = 0.17046341\n",
      "Iteration 118, loss = 0.16959221\n",
      "Iteration 119, loss = 0.16873540\n",
      "Iteration 120, loss = 0.16789335\n",
      "Iteration 121, loss = 0.16706605\n",
      "Iteration 122, loss = 0.16625314\n",
      "Iteration 123, loss = 0.16545353\n",
      "Iteration 124, loss = 0.16466762\n",
      "Iteration 125, loss = 0.16389531\n",
      "Iteration 126, loss = 0.16313535\n",
      "Iteration 127, loss = 0.16238831\n",
      "Iteration 128, loss = 0.16165345\n",
      "Iteration 129, loss = 0.16093028\n",
      "Iteration 130, loss = 0.16021984\n",
      "Iteration 131, loss = 0.15951961\n",
      "Iteration 132, loss = 0.15883165\n",
      "Iteration 133, loss = 0.15815442\n",
      "Iteration 134, loss = 0.15748765\n",
      "Iteration 135, loss = 0.15683186\n",
      "Iteration 136, loss = 0.15618635\n",
      "Iteration 137, loss = 0.15555121\n",
      "Iteration 138, loss = 0.15492579\n",
      "Iteration 139, loss = 0.15430993\n",
      "Iteration 140, loss = 0.15370354\n",
      "Iteration 141, loss = 0.15310639\n",
      "Iteration 142, loss = 0.15251825\n",
      "Iteration 143, loss = 0.15193928\n",
      "Iteration 144, loss = 0.15136885\n",
      "Iteration 145, loss = 0.15080698\n",
      "Iteration 146, loss = 0.15025342\n",
      "Iteration 147, loss = 0.14970803\n",
      "Iteration 148, loss = 0.14917100\n",
      "Iteration 149, loss = 0.14864143\n",
      "Iteration 150, loss = 0.14811952\n",
      "Iteration 151, loss = 0.14760531\n",
      "Iteration 152, loss = 0.14709853\n",
      "Iteration 153, loss = 0.14659915\n",
      "Iteration 154, loss = 0.14610693\n",
      "Iteration 155, loss = 0.14562146\n",
      "Iteration 156, loss = 0.14514284\n",
      "Iteration 157, loss = 0.14467091\n",
      "Iteration 158, loss = 0.14420570\n",
      "Iteration 159, loss = 0.14374706\n",
      "Iteration 160, loss = 0.14329470\n",
      "Iteration 161, loss = 0.14284852\n",
      "Iteration 162, loss = 0.14240838\n",
      "Iteration 163, loss = 0.14197422\n",
      "Iteration 164, loss = 0.14154585\n",
      "Iteration 165, loss = 0.14112328\n",
      "Iteration 166, loss = 0.14070629\n",
      "Iteration 167, loss = 0.14029482\n",
      "Iteration 168, loss = 0.13988876\n",
      "Iteration 169, loss = 0.13948861\n",
      "Iteration 170, loss = 0.13909277\n",
      "Iteration 171, loss = 0.13870250\n",
      "Iteration 172, loss = 0.13831741\n",
      "Iteration 173, loss = 0.13793724\n",
      "Iteration 174, loss = 0.13756185\n",
      "Iteration 175, loss = 0.13719120\n",
      "Iteration 176, loss = 0.13682527\n",
      "Iteration 177, loss = 0.13646421\n",
      "Iteration 178, loss = 0.13610749\n",
      "Iteration 179, loss = 0.13575522\n",
      "Iteration 180, loss = 0.13540734\n",
      "Iteration 181, loss = 0.13506381\n",
      "Iteration 182, loss = 0.13472448\n",
      "Iteration 183, loss = 0.13438929\n",
      "Iteration 184, loss = 0.13405815\n",
      "Iteration 185, loss = 0.13373100\n",
      "Iteration 186, loss = 0.13340778\n",
      "Iteration 187, loss = 0.13308840\n",
      "Iteration 188, loss = 0.13277280\n",
      "Iteration 189, loss = 0.13246100\n",
      "Iteration 190, loss = 0.13215276\n",
      "Iteration 191, loss = 0.13184823\n",
      "Iteration 192, loss = 0.13154725\n",
      "Iteration 193, loss = 0.13124976\n",
      "Iteration 194, loss = 0.13095572\n",
      "Iteration 195, loss = 0.13066502\n",
      "Iteration 196, loss = 0.13037763\n",
      "Iteration 197, loss = 0.13009351\n",
      "Iteration 198, loss = 0.12981259\n",
      "Iteration 199, loss = 0.12953488\n",
      "Iteration 200, loss = 0.12926022\n",
      "Iteration 201, loss = 0.12898863\n",
      "Iteration 202, loss = 0.12872006\n",
      "Iteration 203, loss = 0.12845445\n",
      "Iteration 204, loss = 0.12819175\n",
      "Iteration 205, loss = 0.12793192\n",
      "Iteration 206, loss = 0.12767489\n",
      "Iteration 207, loss = 0.12742064\n",
      "Iteration 208, loss = 0.12716916\n",
      "Iteration 209, loss = 0.12692031\n",
      "Iteration 210, loss = 0.12667414\n",
      "Iteration 211, loss = 0.12643059\n",
      "Iteration 212, loss = 0.12618958\n",
      "Iteration 213, loss = 0.12595112\n",
      "Iteration 214, loss = 0.12571517\n",
      "Iteration 215, loss = 0.12548170\n",
      "Iteration 216, loss = 0.12525057\n",
      "Iteration 217, loss = 0.12502185\n",
      "Iteration 218, loss = 0.12479546\n",
      "Iteration 219, loss = 0.12457142\n",
      "Iteration 220, loss = 0.12434962\n",
      "Iteration 221, loss = 0.12413006\n",
      "Iteration 222, loss = 0.12391274\n",
      "Iteration 223, loss = 0.12369752\n",
      "Iteration 224, loss = 0.12348448\n",
      "Iteration 225, loss = 0.12327352\n",
      "Iteration 226, loss = 0.12306467\n",
      "Iteration 227, loss = 0.12285786\n",
      "Iteration 228, loss = 0.12265308\n",
      "Iteration 229, loss = 0.12245027\n",
      "Iteration 230, loss = 0.12224948\n",
      "Iteration 231, loss = 0.12205056\n",
      "Iteration 232, loss = 0.12185361\n",
      "Iteration 233, loss = 0.12165851\n",
      "Iteration 234, loss = 0.12146525\n",
      "Iteration 235, loss = 0.12127387\n",
      "Iteration 236, loss = 0.12108426\n",
      "Iteration 237, loss = 0.12089643\n",
      "Iteration 238, loss = 0.12071037\n",
      "Iteration 239, loss = 0.12052602\n",
      "Iteration 240, loss = 0.12034339\n",
      "Iteration 241, loss = 0.12016240\n",
      "Iteration 242, loss = 0.11998304\n",
      "Iteration 243, loss = 0.11980529\n",
      "Iteration 244, loss = 0.11962920\n",
      "Iteration 245, loss = 0.11945465\n",
      "Iteration 246, loss = 0.11928169\n",
      "Iteration 247, loss = 0.11911026\n",
      "Iteration 248, loss = 0.11894037\n",
      "Iteration 249, loss = 0.11877198\n",
      "Iteration 250, loss = 0.11860506\n",
      "Iteration 251, loss = 0.11843945\n",
      "Iteration 252, loss = 0.11827534\n",
      "Iteration 253, loss = 0.11811258\n",
      "Iteration 254, loss = 0.11795120\n",
      "Iteration 255, loss = 0.11779123\n",
      "Iteration 256, loss = 0.11763260\n",
      "Iteration 257, loss = 0.11747529\n",
      "Iteration 258, loss = 0.11731932\n",
      "Iteration 259, loss = 0.11716464\n",
      "Iteration 260, loss = 0.11701126\n",
      "Iteration 261, loss = 0.11685916\n",
      "Iteration 262, loss = 0.11670838\n",
      "Iteration 263, loss = 0.11655879\n",
      "Iteration 264, loss = 0.11641037\n",
      "Iteration 265, loss = 0.11626315\n",
      "Iteration 266, loss = 0.11611715\n",
      "Iteration 267, loss = 0.11597228\n",
      "Iteration 268, loss = 0.11582857\n",
      "Iteration 269, loss = 0.11568603\n",
      "Iteration 270, loss = 0.11554464\n",
      "Iteration 271, loss = 0.11540431\n",
      "Iteration 272, loss = 0.11526509\n",
      "Iteration 273, loss = 0.11512694\n",
      "Iteration 274, loss = 0.11498987\n",
      "Iteration 275, loss = 0.11485392\n",
      "Iteration 276, loss = 0.11471899\n",
      "Iteration 277, loss = 0.11458511\n",
      "Iteration 278, loss = 0.11445234\n",
      "Iteration 279, loss = 0.11432055\n",
      "Iteration 280, loss = 0.11418983\n",
      "Iteration 281, loss = 0.11406013\n",
      "Iteration 282, loss = 0.11393140\n",
      "Iteration 283, loss = 0.11380369\n",
      "Iteration 284, loss = 0.11367696\n",
      "Iteration 285, loss = 0.11355119\n",
      "Iteration 286, loss = 0.11342640\n",
      "Iteration 287, loss = 0.11330254\n",
      "Iteration 288, loss = 0.11317966\n",
      "Iteration 289, loss = 0.11305763\n",
      "Iteration 290, loss = 0.11293657\n",
      "Iteration 291, loss = 0.11281633\n",
      "Iteration 292, loss = 0.11269702\n",
      "Iteration 293, loss = 0.11257857\n",
      "Iteration 294, loss = 0.11246097\n",
      "Iteration 295, loss = 0.11234425\n",
      "Iteration 296, loss = 0.11222837\n",
      "Iteration 297, loss = 0.11211336\n",
      "Iteration 298, loss = 0.11199914\n",
      "Iteration 299, loss = 0.11188573\n",
      "Iteration 300, loss = 0.11177307\n",
      "Iteration 301, loss = 0.11166123\n",
      "Iteration 302, loss = 0.11155014\n",
      "Iteration 303, loss = 0.11143986\n",
      "Iteration 304, loss = 0.11133031\n",
      "Iteration 305, loss = 0.11122156\n",
      "Iteration 306, loss = 0.11111356\n",
      "Iteration 307, loss = 0.11100635\n",
      "Iteration 308, loss = 0.11089986\n",
      "Iteration 309, loss = 0.11079415\n",
      "Iteration 310, loss = 0.11068915\n",
      "Iteration 311, loss = 0.11058491\n",
      "Iteration 312, loss = 0.11048134\n",
      "Iteration 313, loss = 0.11037850\n",
      "Iteration 314, loss = 0.11027636\n",
      "Iteration 315, loss = 0.11017493\n",
      "Iteration 316, loss = 0.11007419\n",
      "Iteration 317, loss = 0.10997413\n",
      "Iteration 318, loss = 0.10987478\n",
      "Iteration 319, loss = 0.10977605\n",
      "Iteration 320, loss = 0.10967795\n",
      "Iteration 321, loss = 0.10958037\n",
      "Iteration 322, loss = 0.10948347\n",
      "Iteration 323, loss = 0.10938718\n",
      "Iteration 324, loss = 0.10929154\n",
      "Iteration 325, loss = 0.10919653\n",
      "Iteration 326, loss = 0.10910214\n",
      "Iteration 327, loss = 0.10900839\n",
      "Iteration 328, loss = 0.10891522\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35233713\n",
      "Iteration 2, loss = 1.15655690\n",
      "Iteration 3, loss = 0.97627098\n",
      "Iteration 4, loss = 0.87219722\n",
      "Iteration 5, loss = 0.86143075\n",
      "Iteration 6, loss = 0.83223910\n",
      "Iteration 7, loss = 0.76711648\n",
      "Iteration 8, loss = 0.70532645\n",
      "Iteration 9, loss = 0.66197002\n",
      "Iteration 10, loss = 0.63010781\n",
      "Iteration 11, loss = 0.60382781\n",
      "Iteration 12, loss = 0.57918631\n",
      "Iteration 13, loss = 0.55757352\n",
      "Iteration 14, loss = 0.53929630\n",
      "Iteration 15, loss = 0.52173765\n",
      "Iteration 16, loss = 0.50515604\n",
      "Iteration 17, loss = 0.48932977\n",
      "Iteration 18, loss = 0.47423858\n",
      "Iteration 19, loss = 0.46096905\n",
      "Iteration 20, loss = 0.44956616\n",
      "Iteration 21, loss = 0.43937722\n",
      "Iteration 22, loss = 0.42979352\n",
      "Iteration 23, loss = 0.42075869\n",
      "Iteration 24, loss = 0.41218760\n",
      "Iteration 25, loss = 0.40392347\n",
      "Iteration 26, loss = 0.39586624\n",
      "Iteration 27, loss = 0.38792707\n",
      "Iteration 28, loss = 0.38035228\n",
      "Iteration 29, loss = 0.37339438\n",
      "Iteration 30, loss = 0.36703894\n",
      "Iteration 31, loss = 0.36126376\n",
      "Iteration 32, loss = 0.35582006\n",
      "Iteration 33, loss = 0.35055664\n",
      "Iteration 34, loss = 0.34542601\n",
      "Iteration 35, loss = 0.34045278\n",
      "Iteration 36, loss = 0.33564406\n",
      "Iteration 37, loss = 0.33096959\n",
      "Iteration 38, loss = 0.32643153\n",
      "Iteration 39, loss = 0.32200101\n",
      "Iteration 40, loss = 0.31767735\n",
      "Iteration 41, loss = 0.31345358\n",
      "Iteration 42, loss = 0.30932655\n",
      "Iteration 43, loss = 0.30530025\n",
      "Iteration 44, loss = 0.30137176\n",
      "Iteration 45, loss = 0.29753197\n",
      "Iteration 46, loss = 0.29377729\n",
      "Iteration 47, loss = 0.29010444\n",
      "Iteration 48, loss = 0.28650970\n",
      "Iteration 49, loss = 0.28299226\n",
      "Iteration 50, loss = 0.27955163\n",
      "Iteration 51, loss = 0.27618448\n",
      "Iteration 52, loss = 0.27288891\n",
      "Iteration 53, loss = 0.26966324\n",
      "Iteration 54, loss = 0.26650737\n",
      "Iteration 55, loss = 0.26342006\n",
      "Iteration 56, loss = 0.26039891\n",
      "Iteration 57, loss = 0.25744252\n",
      "Iteration 58, loss = 0.25454960\n",
      "Iteration 59, loss = 0.25171919\n",
      "Iteration 60, loss = 0.24894894\n",
      "Iteration 61, loss = 0.24623876\n",
      "Iteration 62, loss = 0.24358801\n",
      "Iteration 63, loss = 0.24099399\n",
      "Iteration 64, loss = 0.23845528\n",
      "Iteration 65, loss = 0.23597112\n",
      "Iteration 66, loss = 0.23354047\n",
      "Iteration 67, loss = 0.23116183\n",
      "Iteration 68, loss = 0.22883649\n",
      "Iteration 69, loss = 0.22656105\n",
      "Iteration 70, loss = 0.22433451\n",
      "Iteration 71, loss = 0.22215580\n",
      "Iteration 72, loss = 0.22002403\n",
      "Iteration 73, loss = 0.21793811\n",
      "Iteration 74, loss = 0.21589752\n",
      "Iteration 75, loss = 0.21390085\n",
      "Iteration 76, loss = 0.21194632\n",
      "Iteration 77, loss = 0.21003359\n",
      "Iteration 78, loss = 0.20816169\n",
      "Iteration 79, loss = 0.20632984\n",
      "Iteration 80, loss = 0.20453704\n",
      "Iteration 81, loss = 0.20278232\n",
      "Iteration 82, loss = 0.20106467\n",
      "Iteration 83, loss = 0.19938318\n",
      "Iteration 84, loss = 0.19773725\n",
      "Iteration 85, loss = 0.19612628\n",
      "Iteration 86, loss = 0.19454912\n",
      "Iteration 87, loss = 0.19300496\n",
      "Iteration 88, loss = 0.19149259\n",
      "Iteration 89, loss = 0.19001149\n",
      "Iteration 90, loss = 0.18856149\n",
      "Iteration 91, loss = 0.18714187\n",
      "Iteration 92, loss = 0.18575174\n",
      "Iteration 93, loss = 0.18439041\n",
      "Iteration 94, loss = 0.18305624\n",
      "Iteration 95, loss = 0.18175036\n",
      "Iteration 96, loss = 0.18046921\n",
      "Iteration 97, loss = 0.17921537\n",
      "Iteration 98, loss = 0.17798572\n",
      "Iteration 99, loss = 0.17678051\n",
      "Iteration 100, loss = 0.17560015\n",
      "Iteration 101, loss = 0.17444284\n",
      "Iteration 102, loss = 0.17330845\n",
      "Iteration 103, loss = 0.17219642\n",
      "Iteration 104, loss = 0.17110606\n",
      "Iteration 105, loss = 0.17003633\n",
      "Iteration 106, loss = 0.16898751\n",
      "Iteration 107, loss = 0.16795885\n",
      "Iteration 108, loss = 0.16694934\n",
      "Iteration 109, loss = 0.16595916\n",
      "Iteration 110, loss = 0.16498818\n",
      "Iteration 111, loss = 0.16403512\n",
      "Iteration 112, loss = 0.16309979\n",
      "Iteration 113, loss = 0.16218273\n",
      "Iteration 114, loss = 0.16128169\n",
      "Iteration 115, loss = 0.16039699\n",
      "Iteration 116, loss = 0.15952971\n",
      "Iteration 117, loss = 0.15867666\n",
      "Iteration 118, loss = 0.15784016\n",
      "Iteration 119, loss = 0.15701861\n",
      "Iteration 120, loss = 0.15621157\n",
      "Iteration 121, loss = 0.15541874\n",
      "Iteration 122, loss = 0.15464140\n",
      "Iteration 123, loss = 0.15387569\n",
      "Iteration 124, loss = 0.15312500\n",
      "Iteration 125, loss = 0.15238633\n",
      "Iteration 126, loss = 0.15166059\n",
      "Iteration 127, loss = 0.15094774\n",
      "Iteration 128, loss = 0.15024673\n",
      "Iteration 129, loss = 0.14955781\n",
      "Iteration 130, loss = 0.14888044\n",
      "Iteration 131, loss = 0.14821456\n",
      "Iteration 132, loss = 0.14755940\n",
      "Iteration 133, loss = 0.14691560\n",
      "Iteration 134, loss = 0.14628168\n",
      "Iteration 135, loss = 0.14565934\n",
      "Iteration 136, loss = 0.14504601\n",
      "Iteration 137, loss = 0.14444340\n",
      "Iteration 138, loss = 0.14385029\n",
      "Iteration 139, loss = 0.14326640\n",
      "Iteration 140, loss = 0.14269234\n",
      "Iteration 141, loss = 0.14212696\n",
      "Iteration 142, loss = 0.14157029\n",
      "Iteration 143, loss = 0.14102317\n",
      "Iteration 144, loss = 0.14048356\n",
      "Iteration 145, loss = 0.13995271\n",
      "Iteration 146, loss = 0.13943023\n",
      "Iteration 147, loss = 0.13891526\n",
      "Iteration 148, loss = 0.13840819\n",
      "Iteration 149, loss = 0.13790923\n",
      "Iteration 150, loss = 0.13741723\n",
      "Iteration 151, loss = 0.13693310\n",
      "Iteration 152, loss = 0.13645587\n",
      "Iteration 153, loss = 0.13598574\n",
      "Iteration 154, loss = 0.13552234\n",
      "Iteration 155, loss = 0.13506574\n",
      "Iteration 156, loss = 0.13461637\n",
      "Iteration 157, loss = 0.13417256\n",
      "Iteration 158, loss = 0.13373549\n",
      "Iteration 159, loss = 0.13330465\n",
      "Iteration 160, loss = 0.13288037\n",
      "Iteration 161, loss = 0.13246170\n",
      "Iteration 162, loss = 0.13204887\n",
      "Iteration 163, loss = 0.13164174\n",
      "Iteration 164, loss = 0.13124038\n",
      "Iteration 165, loss = 0.13084474\n",
      "Iteration 166, loss = 0.13045428\n",
      "Iteration 167, loss = 0.13006906\n",
      "Iteration 168, loss = 0.12968935\n",
      "Iteration 169, loss = 0.12931472\n",
      "Iteration 170, loss = 0.12894500\n",
      "Iteration 171, loss = 0.12858012\n",
      "Iteration 172, loss = 0.12822026\n",
      "Iteration 173, loss = 0.12786547\n",
      "Iteration 174, loss = 0.12751486\n",
      "Iteration 175, loss = 0.12716901\n",
      "Iteration 176, loss = 0.12682763\n",
      "Iteration 177, loss = 0.12649067\n",
      "Iteration 178, loss = 0.12615802\n",
      "Iteration 179, loss = 0.12582966\n",
      "Iteration 180, loss = 0.12550597\n",
      "Iteration 181, loss = 0.12518565\n",
      "Iteration 182, loss = 0.12486962\n",
      "Iteration 183, loss = 0.12455751\n",
      "Iteration 184, loss = 0.12424930\n",
      "Iteration 185, loss = 0.12394515\n",
      "Iteration 186, loss = 0.12364445\n",
      "Iteration 187, loss = 0.12334758\n",
      "Iteration 188, loss = 0.12305425\n",
      "Iteration 189, loss = 0.12276447\n",
      "Iteration 190, loss = 0.12247814\n",
      "Iteration 191, loss = 0.12219567\n",
      "Iteration 192, loss = 0.12191609\n",
      "Iteration 193, loss = 0.12163996\n",
      "Iteration 194, loss = 0.12136707\n",
      "Iteration 195, loss = 0.12109735\n",
      "Iteration 196, loss = 0.12083127\n",
      "Iteration 197, loss = 0.12056764\n",
      "Iteration 198, loss = 0.12030725\n",
      "Iteration 199, loss = 0.12004986\n",
      "Iteration 200, loss = 0.11979554\n",
      "Iteration 201, loss = 0.11954418\n",
      "Iteration 202, loss = 0.11929549\n",
      "Iteration 203, loss = 0.11904962\n",
      "Iteration 204, loss = 0.11880647\n",
      "Iteration 205, loss = 0.11856602\n",
      "Iteration 206, loss = 0.11832876\n",
      "Iteration 207, loss = 0.11809338\n",
      "Iteration 208, loss = 0.11786082\n",
      "Iteration 209, loss = 0.11763104\n",
      "Iteration 210, loss = 0.11740363\n",
      "Iteration 211, loss = 0.11717867\n",
      "Iteration 212, loss = 0.11695609\n",
      "Iteration 213, loss = 0.11673588\n",
      "Iteration 214, loss = 0.11651809\n",
      "Iteration 215, loss = 0.11630303\n",
      "Iteration 216, loss = 0.11608969\n",
      "Iteration 217, loss = 0.11587875\n",
      "Iteration 218, loss = 0.11567005\n",
      "Iteration 219, loss = 0.11546354\n",
      "Iteration 220, loss = 0.11525916\n",
      "Iteration 221, loss = 0.11505690\n",
      "Iteration 222, loss = 0.11485703\n",
      "Iteration 223, loss = 0.11465885\n",
      "Iteration 224, loss = 0.11446277\n",
      "Iteration 225, loss = 0.11426871\n",
      "Iteration 226, loss = 0.11407659\n",
      "Iteration 227, loss = 0.11388648\n",
      "Iteration 228, loss = 0.11369830\n",
      "Iteration 229, loss = 0.11351201\n",
      "Iteration 230, loss = 0.11332755\n",
      "Iteration 231, loss = 0.11314494\n",
      "Iteration 232, loss = 0.11296410\n",
      "Iteration 233, loss = 0.11278507\n",
      "Iteration 234, loss = 0.11260780\n",
      "Iteration 235, loss = 0.11243221\n",
      "Iteration 236, loss = 0.11225834\n",
      "Iteration 237, loss = 0.11208615\n",
      "Iteration 238, loss = 0.11191558\n",
      "Iteration 239, loss = 0.11174669\n",
      "Iteration 240, loss = 0.11157936\n",
      "Iteration 241, loss = 0.11141363\n",
      "Iteration 242, loss = 0.11124948\n",
      "Iteration 243, loss = 0.11108684\n",
      "Iteration 244, loss = 0.11092576\n",
      "Iteration 245, loss = 0.11076614\n",
      "Iteration 246, loss = 0.11060802\n",
      "Iteration 247, loss = 0.11045135\n",
      "Iteration 248, loss = 0.11029608\n",
      "Iteration 249, loss = 0.11014223\n",
      "Iteration 250, loss = 0.10998975\n",
      "Iteration 251, loss = 0.10983866\n",
      "Iteration 252, loss = 0.10968893\n",
      "Iteration 253, loss = 0.10954057\n",
      "Iteration 254, loss = 0.10939350\n",
      "Iteration 255, loss = 0.10924774\n",
      "Iteration 256, loss = 0.10910329\n",
      "Iteration 257, loss = 0.10896011\n",
      "Iteration 258, loss = 0.10881819\n",
      "Iteration 259, loss = 0.10867750\n",
      "Iteration 260, loss = 0.10853804\n",
      "Iteration 261, loss = 0.10839979\n",
      "Iteration 262, loss = 0.10826274\n",
      "Iteration 263, loss = 0.10812686\n",
      "Iteration 264, loss = 0.10799215\n",
      "Iteration 265, loss = 0.10785860\n",
      "Iteration 266, loss = 0.10772618\n",
      "Iteration 267, loss = 0.10759487\n",
      "Iteration 268, loss = 0.10746468\n",
      "Iteration 269, loss = 0.10733558\n",
      "Iteration 270, loss = 0.10720756\n",
      "Iteration 271, loss = 0.10708061\n",
      "Iteration 272, loss = 0.10695468\n",
      "Iteration 273, loss = 0.10682976\n",
      "Iteration 274, loss = 0.10670587\n",
      "Iteration 275, loss = 0.10658299\n",
      "Iteration 276, loss = 0.10646111\n",
      "Iteration 277, loss = 0.10634022\n",
      "Iteration 278, loss = 0.10622030\n",
      "Iteration 279, loss = 0.10610135\n",
      "Iteration 280, loss = 0.10598336\n",
      "Iteration 281, loss = 0.10586632\n",
      "Iteration 282, loss = 0.10575023\n",
      "Iteration 283, loss = 0.10563504\n",
      "Iteration 284, loss = 0.10552077\n",
      "Iteration 285, loss = 0.10540737\n",
      "Iteration 286, loss = 0.10529487\n",
      "Iteration 287, loss = 0.10518323\n",
      "Iteration 288, loss = 0.10507246\n",
      "Iteration 289, loss = 0.10496242\n",
      "Iteration 290, loss = 0.10485321\n",
      "Iteration 291, loss = 0.10474481\n",
      "Iteration 292, loss = 0.10463722\n",
      "Iteration 293, loss = 0.10453045\n",
      "Iteration 294, loss = 0.10442448\n",
      "Iteration 295, loss = 0.10431931\n",
      "Iteration 296, loss = 0.10421492\n",
      "Iteration 297, loss = 0.10411131\n",
      "Iteration 298, loss = 0.10400843\n",
      "Iteration 299, loss = 0.10390629\n",
      "Iteration 300, loss = 0.10380486\n",
      "Iteration 301, loss = 0.10370416\n",
      "Iteration 302, loss = 0.10360420\n",
      "Iteration 303, loss = 0.10350495\n",
      "Iteration 304, loss = 0.10340643\n",
      "Iteration 305, loss = 0.10330861\n",
      "Iteration 306, loss = 0.10321149\n",
      "Iteration 307, loss = 0.10311506\n",
      "Iteration 308, loss = 0.10301928\n",
      "Iteration 309, loss = 0.10292417\n",
      "Iteration 310, loss = 0.10282975\n",
      "Iteration 311, loss = 0.10273598\n",
      "Iteration 312, loss = 0.10264287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34884587\n",
      "Iteration 2, loss = 1.14783885\n",
      "Iteration 3, loss = 0.96469443\n",
      "Iteration 4, loss = 0.85994437\n",
      "Iteration 5, loss = 0.84849121\n",
      "Iteration 6, loss = 0.81825647\n",
      "Iteration 7, loss = 0.75227772\n",
      "Iteration 8, loss = 0.68897845\n",
      "Iteration 9, loss = 0.64467770\n",
      "Iteration 10, loss = 0.61293099\n",
      "Iteration 11, loss = 0.58707044\n",
      "Iteration 12, loss = 0.56544887\n",
      "Iteration 13, loss = 0.54605341\n",
      "Iteration 14, loss = 0.52698733\n",
      "Iteration 15, loss = 0.50874141\n",
      "Iteration 16, loss = 0.49163611\n",
      "Iteration 17, loss = 0.47561660\n",
      "Iteration 18, loss = 0.46098126\n",
      "Iteration 19, loss = 0.44834183\n",
      "Iteration 20, loss = 0.43732302\n",
      "Iteration 21, loss = 0.42711458\n",
      "Iteration 22, loss = 0.41754050\n",
      "Iteration 23, loss = 0.40846450\n",
      "Iteration 24, loss = 0.39980112\n",
      "Iteration 25, loss = 0.39143594\n",
      "Iteration 26, loss = 0.38330919\n",
      "Iteration 27, loss = 0.37529332\n",
      "Iteration 28, loss = 0.36752709\n",
      "Iteration 29, loss = 0.36029053\n",
      "Iteration 30, loss = 0.35373075\n",
      "Iteration 31, loss = 0.34770291\n",
      "Iteration 32, loss = 0.34201565\n",
      "Iteration 33, loss = 0.33656185\n",
      "Iteration 34, loss = 0.33125724\n",
      "Iteration 35, loss = 0.32609074\n",
      "Iteration 36, loss = 0.32106224\n",
      "Iteration 37, loss = 0.31620302\n",
      "Iteration 38, loss = 0.31146713\n",
      "Iteration 39, loss = 0.30684665\n",
      "Iteration 40, loss = 0.30233540\n",
      "Iteration 41, loss = 0.29793256\n",
      "Iteration 42, loss = 0.29363247\n",
      "Iteration 43, loss = 0.28942634\n",
      "Iteration 44, loss = 0.28531830\n",
      "Iteration 45, loss = 0.28130291\n",
      "Iteration 46, loss = 0.27737617\n",
      "Iteration 47, loss = 0.27353770\n",
      "Iteration 48, loss = 0.26978165\n",
      "Iteration 49, loss = 0.26610675\n",
      "Iteration 50, loss = 0.26251193\n",
      "Iteration 51, loss = 0.25899435\n",
      "Iteration 52, loss = 0.25555307\n",
      "Iteration 53, loss = 0.25218719\n",
      "Iteration 54, loss = 0.24889695\n",
      "Iteration 55, loss = 0.24567950\n",
      "Iteration 56, loss = 0.24253274\n",
      "Iteration 57, loss = 0.23945556\n",
      "Iteration 58, loss = 0.23644617\n",
      "Iteration 59, loss = 0.23350341\n",
      "Iteration 60, loss = 0.23062629\n",
      "Iteration 61, loss = 0.22781504\n",
      "Iteration 62, loss = 0.22506440\n",
      "Iteration 63, loss = 0.22237705\n",
      "Iteration 64, loss = 0.21974941\n",
      "Iteration 65, loss = 0.21718159\n",
      "Iteration 66, loss = 0.21467171\n",
      "Iteration 67, loss = 0.21221840\n",
      "Iteration 68, loss = 0.20981921\n",
      "Iteration 69, loss = 0.20747549\n",
      "Iteration 70, loss = 0.20518271\n",
      "Iteration 71, loss = 0.20294172\n",
      "Iteration 72, loss = 0.20075103\n",
      "Iteration 73, loss = 0.19860972\n",
      "Iteration 74, loss = 0.19651531\n",
      "Iteration 75, loss = 0.19446851\n",
      "Iteration 76, loss = 0.19246658\n",
      "Iteration 77, loss = 0.19050975\n",
      "Iteration 78, loss = 0.18859600\n",
      "Iteration 79, loss = 0.18672441\n",
      "Iteration 80, loss = 0.18489499\n",
      "Iteration 81, loss = 0.18310477\n",
      "Iteration 82, loss = 0.18135463\n",
      "Iteration 83, loss = 0.17964319\n",
      "Iteration 84, loss = 0.17796847\n",
      "Iteration 85, loss = 0.17633014\n",
      "Iteration 86, loss = 0.17472784\n",
      "Iteration 87, loss = 0.17315974\n",
      "Iteration 88, loss = 0.17162515\n",
      "Iteration 89, loss = 0.17012498\n",
      "Iteration 90, loss = 0.16865621\n",
      "Iteration 91, loss = 0.16721842\n",
      "Iteration 92, loss = 0.16581208\n",
      "Iteration 93, loss = 0.16443409\n",
      "Iteration 94, loss = 0.16308577\n",
      "Iteration 95, loss = 0.16176558\n",
      "Iteration 96, loss = 0.16047317\n",
      "Iteration 97, loss = 0.15920777\n",
      "Iteration 98, loss = 0.15796858\n",
      "Iteration 99, loss = 0.15675536\n",
      "Iteration 100, loss = 0.15556658\n",
      "Iteration 101, loss = 0.15440238\n",
      "Iteration 102, loss = 0.15326100\n",
      "Iteration 103, loss = 0.15214292\n",
      "Iteration 104, loss = 0.15104656\n",
      "Iteration 105, loss = 0.14997281\n",
      "Iteration 106, loss = 0.14891972\n",
      "Iteration 107, loss = 0.14788768\n",
      "Iteration 108, loss = 0.14687506\n",
      "Iteration 109, loss = 0.14588264\n",
      "Iteration 110, loss = 0.14490939\n",
      "Iteration 111, loss = 0.14395452\n",
      "Iteration 112, loss = 0.14301834\n",
      "Iteration 113, loss = 0.14209918\n",
      "Iteration 114, loss = 0.14119838\n",
      "Iteration 115, loss = 0.14031326\n",
      "Iteration 116, loss = 0.13944568\n",
      "Iteration 117, loss = 0.13859327\n",
      "Iteration 118, loss = 0.13775704\n",
      "Iteration 119, loss = 0.13693583\n",
      "Iteration 120, loss = 0.13613000\n",
      "Iteration 121, loss = 0.13533888\n",
      "Iteration 122, loss = 0.13456220\n",
      "Iteration 123, loss = 0.13379936\n",
      "Iteration 124, loss = 0.13305042\n",
      "Iteration 125, loss = 0.13231447\n",
      "Iteration 126, loss = 0.13159143\n",
      "Iteration 127, loss = 0.13088126\n",
      "Iteration 128, loss = 0.13018340\n",
      "Iteration 129, loss = 0.12949758\n",
      "Iteration 130, loss = 0.12882330\n",
      "Iteration 131, loss = 0.12816060\n",
      "Iteration 132, loss = 0.12750913\n",
      "Iteration 133, loss = 0.12686920\n",
      "Iteration 134, loss = 0.12623940\n",
      "Iteration 135, loss = 0.12562034\n",
      "Iteration 136, loss = 0.12501159\n",
      "Iteration 137, loss = 0.12441274\n",
      "Iteration 138, loss = 0.12382380\n",
      "Iteration 139, loss = 0.12324433\n",
      "Iteration 140, loss = 0.12267420\n",
      "Iteration 141, loss = 0.12211316\n",
      "Iteration 142, loss = 0.12156117\n",
      "Iteration 143, loss = 0.12101779\n",
      "Iteration 144, loss = 0.12048297\n",
      "Iteration 145, loss = 0.11995661\n",
      "Iteration 146, loss = 0.11943831\n",
      "Iteration 147, loss = 0.11892817\n",
      "Iteration 148, loss = 0.11842579\n",
      "Iteration 149, loss = 0.11793112\n",
      "Iteration 150, loss = 0.11744393\n",
      "Iteration 151, loss = 0.11696416\n",
      "Iteration 152, loss = 0.11649158\n",
      "Iteration 153, loss = 0.11602604\n",
      "Iteration 154, loss = 0.11556749\n",
      "Iteration 155, loss = 0.11511565\n",
      "Iteration 156, loss = 0.11467040\n",
      "Iteration 157, loss = 0.11423162\n",
      "Iteration 158, loss = 0.11379919\n",
      "Iteration 159, loss = 0.11337302\n",
      "Iteration 160, loss = 0.11295288\n",
      "Iteration 161, loss = 0.11253890\n",
      "Iteration 162, loss = 0.11213062\n",
      "Iteration 163, loss = 0.11172808\n",
      "Iteration 164, loss = 0.11133119\n",
      "Iteration 165, loss = 0.11093977\n",
      "Iteration 166, loss = 0.11055379\n",
      "Iteration 167, loss = 0.11017310\n",
      "Iteration 168, loss = 0.10979755\n",
      "Iteration 169, loss = 0.10942712\n",
      "Iteration 170, loss = 0.10906165\n",
      "Iteration 171, loss = 0.10870109\n",
      "Iteration 172, loss = 0.10834530\n",
      "Iteration 173, loss = 0.10799423\n",
      "Iteration 174, loss = 0.10764784\n",
      "Iteration 175, loss = 0.10730608\n",
      "Iteration 176, loss = 0.10696863\n",
      "Iteration 177, loss = 0.10663556\n",
      "Iteration 178, loss = 0.10630680\n",
      "Iteration 179, loss = 0.10598222\n",
      "Iteration 180, loss = 0.10566177\n",
      "Iteration 181, loss = 0.10534564\n",
      "Iteration 182, loss = 0.10503316\n",
      "Iteration 183, loss = 0.10472468\n",
      "Iteration 184, loss = 0.10442005\n",
      "Iteration 185, loss = 0.10411916\n",
      "Iteration 186, loss = 0.10382202\n",
      "Iteration 187, loss = 0.10352867\n",
      "Iteration 188, loss = 0.10323876\n",
      "Iteration 189, loss = 0.10295236\n",
      "Iteration 190, loss = 0.10266937\n",
      "Iteration 191, loss = 0.10238978\n",
      "Iteration 192, loss = 0.10211352\n",
      "Iteration 193, loss = 0.10184073\n",
      "Iteration 194, loss = 0.10157100\n",
      "Iteration 195, loss = 0.10130442\n",
      "Iteration 196, loss = 0.10104093\n",
      "Iteration 197, loss = 0.10078075\n",
      "Iteration 198, loss = 0.10052328\n",
      "Iteration 199, loss = 0.10026887\n",
      "Iteration 200, loss = 0.10001734\n",
      "Iteration 201, loss = 0.09976889\n",
      "Iteration 202, loss = 0.09952302\n",
      "Iteration 203, loss = 0.09927993\n",
      "Iteration 204, loss = 0.09903960\n",
      "Iteration 205, loss = 0.09880204\n",
      "Iteration 206, loss = 0.09856706\n",
      "Iteration 207, loss = 0.09833461\n",
      "Iteration 208, loss = 0.09810470\n",
      "Iteration 209, loss = 0.09787741\n",
      "Iteration 210, loss = 0.09765258\n",
      "Iteration 211, loss = 0.09743009\n",
      "Iteration 212, loss = 0.09720998\n",
      "Iteration 213, loss = 0.09699237\n",
      "Iteration 214, loss = 0.09677696\n",
      "Iteration 215, loss = 0.09656382\n",
      "Iteration 216, loss = 0.09635291\n",
      "Iteration 217, loss = 0.09614445\n",
      "Iteration 218, loss = 0.09593798\n",
      "Iteration 219, loss = 0.09573356\n",
      "Iteration 220, loss = 0.09553133\n",
      "Iteration 221, loss = 0.09533125\n",
      "Iteration 222, loss = 0.09513313\n",
      "Iteration 223, loss = 0.09493687\n",
      "Iteration 224, loss = 0.09474288\n",
      "Iteration 225, loss = 0.09455052\n",
      "Iteration 226, loss = 0.09436016\n",
      "Iteration 227, loss = 0.09417195\n",
      "Iteration 228, loss = 0.09398534\n",
      "Iteration 229, loss = 0.09380063\n",
      "Iteration 230, loss = 0.09361770\n",
      "Iteration 231, loss = 0.09343672\n",
      "Iteration 232, loss = 0.09325734\n",
      "Iteration 233, loss = 0.09307970\n",
      "Iteration 234, loss = 0.09290397\n",
      "Iteration 235, loss = 0.09272979\n",
      "Iteration 236, loss = 0.09255720\n",
      "Iteration 237, loss = 0.09238662\n",
      "Iteration 238, loss = 0.09221717\n",
      "Iteration 239, loss = 0.09204968\n",
      "Iteration 240, loss = 0.09188361\n",
      "Iteration 241, loss = 0.09171909\n",
      "Iteration 242, loss = 0.09155623\n",
      "Iteration 243, loss = 0.09139466\n",
      "Iteration 244, loss = 0.09123462\n",
      "Iteration 245, loss = 0.09107625\n",
      "Iteration 246, loss = 0.09091910\n",
      "Iteration 247, loss = 0.09076341\n",
      "Iteration 248, loss = 0.09060935\n",
      "Iteration 249, loss = 0.09045640\n",
      "Iteration 250, loss = 0.09030497\n",
      "Iteration 251, loss = 0.09015497\n",
      "Iteration 252, loss = 0.09000616\n",
      "Iteration 253, loss = 0.08985849\n",
      "Iteration 254, loss = 0.08971232\n",
      "Iteration 255, loss = 0.08956724\n",
      "Iteration 256, loss = 0.08942347\n",
      "Iteration 257, loss = 0.08928109\n",
      "Iteration 258, loss = 0.08913975\n",
      "Iteration 259, loss = 0.08899959\n",
      "Iteration 260, loss = 0.08886067\n",
      "Iteration 261, loss = 0.08872295\n",
      "Iteration 262, loss = 0.08858638\n",
      "Iteration 263, loss = 0.08845095\n",
      "Iteration 264, loss = 0.08831668\n",
      "Iteration 265, loss = 0.08818350\n",
      "Iteration 266, loss = 0.08805136\n",
      "Iteration 267, loss = 0.08792029\n",
      "Iteration 268, loss = 0.08779024\n",
      "Iteration 269, loss = 0.08766122\n",
      "Iteration 270, loss = 0.08753325\n",
      "Iteration 271, loss = 0.08740625\n",
      "Iteration 272, loss = 0.08728026\n",
      "Iteration 273, loss = 0.08715530\n",
      "Iteration 274, loss = 0.08703134\n",
      "Iteration 275, loss = 0.08690833\n",
      "Iteration 276, loss = 0.08678631\n",
      "Iteration 277, loss = 0.08666526\n",
      "Iteration 278, loss = 0.08654514\n",
      "Iteration 279, loss = 0.08642597\n",
      "Iteration 280, loss = 0.08630773\n",
      "Iteration 281, loss = 0.08619044\n",
      "Iteration 282, loss = 0.08607405\n",
      "Iteration 283, loss = 0.08595857\n",
      "Iteration 284, loss = 0.08584401\n",
      "Iteration 285, loss = 0.08573031\n",
      "Iteration 286, loss = 0.08561751\n",
      "Iteration 287, loss = 0.08550558\n",
      "Iteration 288, loss = 0.08539450\n",
      "Iteration 289, loss = 0.08528428\n",
      "Iteration 290, loss = 0.08517489\n",
      "Iteration 291, loss = 0.08506633\n",
      "Iteration 292, loss = 0.08495859\n",
      "Iteration 293, loss = 0.08485167\n",
      "Iteration 294, loss = 0.08474556\n",
      "Iteration 295, loss = 0.08464023\n",
      "Iteration 296, loss = 0.08453570\n",
      "Iteration 297, loss = 0.08443195\n",
      "Iteration 298, loss = 0.08432898\n",
      "Iteration 299, loss = 0.08422675\n",
      "Iteration 300, loss = 0.08412526\n",
      "Iteration 301, loss = 0.08402452\n",
      "Iteration 302, loss = 0.08392452\n",
      "Iteration 303, loss = 0.08382524\n",
      "Iteration 304, loss = 0.08372667\n",
      "Iteration 305, loss = 0.08362878\n",
      "Iteration 306, loss = 0.08353160\n",
      "Iteration 307, loss = 0.08343513\n",
      "Iteration 308, loss = 0.08333934\n",
      "Iteration 309, loss = 0.08324425\n",
      "Iteration 310, loss = 0.08314983\n",
      "Iteration 311, loss = 0.08305606\n",
      "Iteration 312, loss = 0.08296293\n",
      "Iteration 313, loss = 0.08287045\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34975994\n",
      "Iteration 2, loss = 1.14979834\n",
      "Iteration 3, loss = 0.96590973\n",
      "Iteration 4, loss = 0.86239058\n",
      "Iteration 5, loss = 0.85184405\n",
      "Iteration 6, loss = 0.82149425\n",
      "Iteration 7, loss = 0.75554220\n",
      "Iteration 8, loss = 0.69299591\n",
      "Iteration 9, loss = 0.64921708\n",
      "Iteration 10, loss = 0.61726045\n",
      "Iteration 11, loss = 0.59112404\n",
      "Iteration 12, loss = 0.56817051\n",
      "Iteration 13, loss = 0.54856232\n",
      "Iteration 14, loss = 0.52952543\n",
      "Iteration 15, loss = 0.51121000\n",
      "Iteration 16, loss = 0.49383799\n",
      "Iteration 17, loss = 0.47754655\n",
      "Iteration 18, loss = 0.46331670\n",
      "Iteration 19, loss = 0.45082347\n",
      "Iteration 20, loss = 0.43955188\n",
      "Iteration 21, loss = 0.42909054\n",
      "Iteration 22, loss = 0.41926267\n",
      "Iteration 23, loss = 0.40998814\n",
      "Iteration 24, loss = 0.40114931\n",
      "Iteration 25, loss = 0.39263774\n",
      "Iteration 26, loss = 0.38439759\n",
      "Iteration 27, loss = 0.37627557\n",
      "Iteration 28, loss = 0.36840914\n",
      "Iteration 29, loss = 0.36109710\n",
      "Iteration 30, loss = 0.35448541\n",
      "Iteration 31, loss = 0.34839151\n",
      "Iteration 32, loss = 0.34261117\n",
      "Iteration 33, loss = 0.33705652\n",
      "Iteration 34, loss = 0.33166403\n",
      "Iteration 35, loss = 0.32643127\n",
      "Iteration 36, loss = 0.32135274\n",
      "Iteration 37, loss = 0.31642184\n",
      "Iteration 38, loss = 0.31162752\n",
      "Iteration 39, loss = 0.30696137\n",
      "Iteration 40, loss = 0.30239744\n",
      "Iteration 41, loss = 0.29793696\n",
      "Iteration 42, loss = 0.29358367\n",
      "Iteration 43, loss = 0.28932920\n",
      "Iteration 44, loss = 0.28517347\n",
      "Iteration 45, loss = 0.28111297\n",
      "Iteration 46, loss = 0.27714275\n",
      "Iteration 47, loss = 0.27326305\n",
      "Iteration 48, loss = 0.26946867\n",
      "Iteration 49, loss = 0.26575781\n",
      "Iteration 50, loss = 0.26212992\n",
      "Iteration 51, loss = 0.25858091\n",
      "Iteration 52, loss = 0.25511124\n",
      "Iteration 53, loss = 0.25171861\n",
      "Iteration 54, loss = 0.24839901\n",
      "Iteration 55, loss = 0.24515283\n",
      "Iteration 56, loss = 0.24197840\n",
      "Iteration 57, loss = 0.23887319\n",
      "Iteration 58, loss = 0.23583695\n",
      "Iteration 59, loss = 0.23286684\n",
      "Iteration 60, loss = 0.22996229\n",
      "Iteration 61, loss = 0.22712201\n",
      "Iteration 62, loss = 0.22434586\n",
      "Iteration 63, loss = 0.22162906\n",
      "Iteration 64, loss = 0.21897344\n",
      "Iteration 65, loss = 0.21637601\n",
      "Iteration 66, loss = 0.21383629\n",
      "Iteration 67, loss = 0.21135294\n",
      "Iteration 68, loss = 0.20892478\n",
      "Iteration 69, loss = 0.20654961\n",
      "Iteration 70, loss = 0.20422615\n",
      "Iteration 71, loss = 0.20195339\n",
      "Iteration 72, loss = 0.19973033\n",
      "Iteration 73, loss = 0.19755645\n",
      "Iteration 74, loss = 0.19542951\n",
      "Iteration 75, loss = 0.19334828\n",
      "Iteration 76, loss = 0.19131202\n",
      "Iteration 77, loss = 0.18931976\n",
      "Iteration 78, loss = 0.18737016\n",
      "Iteration 79, loss = 0.18546302\n",
      "Iteration 80, loss = 0.18359650\n",
      "Iteration 81, loss = 0.18177041\n",
      "Iteration 82, loss = 0.17998317\n",
      "Iteration 83, loss = 0.17823387\n",
      "Iteration 84, loss = 0.17652176\n",
      "Iteration 85, loss = 0.17484536\n",
      "Iteration 86, loss = 0.17320396\n",
      "Iteration 87, loss = 0.17159709\n",
      "Iteration 88, loss = 0.17002442\n",
      "Iteration 89, loss = 0.16848419\n",
      "Iteration 90, loss = 0.16697554\n",
      "Iteration 91, loss = 0.16549769\n",
      "Iteration 92, loss = 0.16404973\n",
      "Iteration 93, loss = 0.16263088\n",
      "Iteration 94, loss = 0.16124052\n",
      "Iteration 95, loss = 0.15987793\n",
      "Iteration 96, loss = 0.15854269\n",
      "Iteration 97, loss = 0.15723414\n",
      "Iteration 98, loss = 0.15595146\n",
      "Iteration 99, loss = 0.15469408\n",
      "Iteration 100, loss = 0.15346150\n",
      "Iteration 101, loss = 0.15225262\n",
      "Iteration 102, loss = 0.15106695\n",
      "Iteration 103, loss = 0.14990391\n",
      "Iteration 104, loss = 0.14876298\n",
      "Iteration 105, loss = 0.14764375\n",
      "Iteration 106, loss = 0.14654556\n",
      "Iteration 107, loss = 0.14546844\n",
      "Iteration 108, loss = 0.14441163\n",
      "Iteration 109, loss = 0.14337462\n",
      "Iteration 110, loss = 0.14235677\n",
      "Iteration 111, loss = 0.14135721\n",
      "Iteration 112, loss = 0.14037575\n",
      "Iteration 113, loss = 0.13941226\n",
      "Iteration 114, loss = 0.13846609\n",
      "Iteration 115, loss = 0.13753712\n",
      "Iteration 116, loss = 0.13662471\n",
      "Iteration 117, loss = 0.13572842\n",
      "Iteration 118, loss = 0.13484784\n",
      "Iteration 119, loss = 0.13398273\n",
      "Iteration 120, loss = 0.13313269\n",
      "Iteration 121, loss = 0.13229737\n",
      "Iteration 122, loss = 0.13147634\n",
      "Iteration 123, loss = 0.13066926\n",
      "Iteration 124, loss = 0.12987582\n",
      "Iteration 125, loss = 0.12909571\n",
      "Iteration 126, loss = 0.12832868\n",
      "Iteration 127, loss = 0.12757429\n",
      "Iteration 128, loss = 0.12683239\n",
      "Iteration 129, loss = 0.12610265\n",
      "Iteration 130, loss = 0.12538487\n",
      "Iteration 131, loss = 0.12467867\n",
      "Iteration 132, loss = 0.12398384\n",
      "Iteration 133, loss = 0.12330008\n",
      "Iteration 134, loss = 0.12262716\n",
      "Iteration 135, loss = 0.12196485\n",
      "Iteration 136, loss = 0.12131289\n",
      "Iteration 137, loss = 0.12067107\n",
      "Iteration 138, loss = 0.12003927\n",
      "Iteration 139, loss = 0.11941719\n",
      "Iteration 140, loss = 0.11880474\n",
      "Iteration 141, loss = 0.11820153\n",
      "Iteration 142, loss = 0.11760746\n",
      "Iteration 143, loss = 0.11702225\n",
      "Iteration 144, loss = 0.11644578\n",
      "Iteration 145, loss = 0.11587788\n",
      "Iteration 146, loss = 0.11531833\n",
      "Iteration 147, loss = 0.11476690\n",
      "Iteration 148, loss = 0.11422358\n",
      "Iteration 149, loss = 0.11368805\n",
      "Iteration 150, loss = 0.11316021\n",
      "Iteration 151, loss = 0.11263991\n",
      "Iteration 152, loss = 0.11212696\n",
      "Iteration 153, loss = 0.11162125\n",
      "Iteration 154, loss = 0.11112263\n",
      "Iteration 155, loss = 0.11063089\n",
      "Iteration 156, loss = 0.11014604\n",
      "Iteration 157, loss = 0.10966795\n",
      "Iteration 158, loss = 0.10919638\n",
      "Iteration 159, loss = 0.10873120\n",
      "Iteration 160, loss = 0.10827235\n",
      "Iteration 161, loss = 0.10781962\n",
      "Iteration 162, loss = 0.10737295\n",
      "Iteration 163, loss = 0.10693220\n",
      "Iteration 164, loss = 0.10649727\n",
      "Iteration 165, loss = 0.10606805\n",
      "Iteration 166, loss = 0.10564444\n",
      "Iteration 167, loss = 0.10522632\n",
      "Iteration 168, loss = 0.10481359\n",
      "Iteration 169, loss = 0.10440613\n",
      "Iteration 170, loss = 0.10400386\n",
      "Iteration 171, loss = 0.10360669\n",
      "Iteration 172, loss = 0.10321453\n",
      "Iteration 173, loss = 0.10282728\n",
      "Iteration 174, loss = 0.10244496\n",
      "Iteration 175, loss = 0.10206741\n",
      "Iteration 176, loss = 0.10169445\n",
      "Iteration 177, loss = 0.10132604\n",
      "Iteration 178, loss = 0.10096213\n",
      "Iteration 179, loss = 0.10060264\n",
      "Iteration 180, loss = 0.10024752\n",
      "Iteration 181, loss = 0.09989662\n",
      "Iteration 182, loss = 0.09954994\n",
      "Iteration 183, loss = 0.09920735\n",
      "Iteration 184, loss = 0.09886883\n",
      "Iteration 185, loss = 0.09853426\n",
      "Iteration 186, loss = 0.09820360\n",
      "Iteration 187, loss = 0.09787681\n",
      "Iteration 188, loss = 0.09755377\n",
      "Iteration 189, loss = 0.09723448\n",
      "Iteration 190, loss = 0.09691893\n",
      "Iteration 191, loss = 0.09660702\n",
      "Iteration 192, loss = 0.09629879\n",
      "Iteration 193, loss = 0.09599401\n",
      "Iteration 194, loss = 0.09569263\n",
      "Iteration 195, loss = 0.09539461\n",
      "Iteration 196, loss = 0.09509982\n",
      "Iteration 197, loss = 0.09480827\n",
      "Iteration 198, loss = 0.09451991\n",
      "Iteration 199, loss = 0.09423469\n",
      "Iteration 200, loss = 0.09395256\n",
      "Iteration 201, loss = 0.09367347\n",
      "Iteration 202, loss = 0.09339738\n",
      "Iteration 203, loss = 0.09312425\n",
      "Iteration 204, loss = 0.09285403\n",
      "Iteration 205, loss = 0.09258670\n",
      "Iteration 206, loss = 0.09232217\n",
      "Iteration 207, loss = 0.09206040\n",
      "Iteration 208, loss = 0.09180137\n",
      "Iteration 209, loss = 0.09154506\n",
      "Iteration 210, loss = 0.09129139\n",
      "Iteration 211, loss = 0.09104033\n",
      "Iteration 212, loss = 0.09079183\n",
      "Iteration 213, loss = 0.09054586\n",
      "Iteration 214, loss = 0.09030230\n",
      "Iteration 215, loss = 0.09006114\n",
      "Iteration 216, loss = 0.08982219\n",
      "Iteration 217, loss = 0.08958517\n",
      "Iteration 218, loss = 0.08935030\n",
      "Iteration 219, loss = 0.08911755\n",
      "Iteration 220, loss = 0.08888694\n",
      "Iteration 221, loss = 0.08865842\n",
      "Iteration 222, loss = 0.08843195\n",
      "Iteration 223, loss = 0.08820738\n",
      "Iteration 224, loss = 0.08798473\n",
      "Iteration 225, loss = 0.08776408\n",
      "Iteration 226, loss = 0.08754598\n",
      "Iteration 227, loss = 0.08733086\n",
      "Iteration 228, loss = 0.08711774\n",
      "Iteration 229, loss = 0.08690662\n",
      "Iteration 230, loss = 0.08669748\n",
      "Iteration 231, loss = 0.08649083\n",
      "Iteration 232, loss = 0.08628610\n",
      "Iteration 233, loss = 0.08608329\n",
      "Iteration 234, loss = 0.08588208\n",
      "Iteration 235, loss = 0.08568247\n",
      "Iteration 236, loss = 0.08548450\n",
      "Iteration 237, loss = 0.08528828\n",
      "Iteration 238, loss = 0.08509368\n",
      "Iteration 239, loss = 0.08490075\n",
      "Iteration 240, loss = 0.08470946\n",
      "Iteration 241, loss = 0.08451982\n",
      "Iteration 242, loss = 0.08433177\n",
      "Iteration 243, loss = 0.08414528\n",
      "Iteration 244, loss = 0.08396059\n",
      "Iteration 245, loss = 0.08377764\n",
      "Iteration 246, loss = 0.08359624\n",
      "Iteration 247, loss = 0.08341641\n",
      "Iteration 248, loss = 0.08323815\n",
      "Iteration 249, loss = 0.08306135\n",
      "Iteration 250, loss = 0.08288604\n",
      "Iteration 251, loss = 0.08271228\n",
      "Iteration 252, loss = 0.08254000\n",
      "Iteration 253, loss = 0.08236916\n",
      "Iteration 254, loss = 0.08219976\n",
      "Iteration 255, loss = 0.08203179\n",
      "Iteration 256, loss = 0.08186523\n",
      "Iteration 257, loss = 0.08170002\n",
      "Iteration 258, loss = 0.08153621\n",
      "Iteration 259, loss = 0.08137374\n",
      "Iteration 260, loss = 0.08121260\n",
      "Iteration 261, loss = 0.08105280\n",
      "Iteration 262, loss = 0.08089429\n",
      "Iteration 263, loss = 0.08073706\n",
      "Iteration 264, loss = 0.08058106\n",
      "Iteration 265, loss = 0.08042626\n",
      "Iteration 266, loss = 0.08027266\n",
      "Iteration 267, loss = 0.08012024\n",
      "Iteration 268, loss = 0.07996903\n",
      "Iteration 269, loss = 0.07981903\n",
      "Iteration 270, loss = 0.07967025\n",
      "Iteration 271, loss = 0.07952261\n",
      "Iteration 272, loss = 0.07937613\n",
      "Iteration 273, loss = 0.07923080\n",
      "Iteration 274, loss = 0.07908659\n",
      "Iteration 275, loss = 0.07894349\n",
      "Iteration 276, loss = 0.07880141\n",
      "Iteration 277, loss = 0.07866042\n",
      "Iteration 278, loss = 0.07852057\n",
      "Iteration 279, loss = 0.07838179\n",
      "Iteration 280, loss = 0.07824404\n",
      "Iteration 281, loss = 0.07810739\n",
      "Iteration 282, loss = 0.07797170\n",
      "Iteration 283, loss = 0.07783707\n",
      "Iteration 284, loss = 0.07770342\n",
      "Iteration 285, loss = 0.07757077\n",
      "Iteration 286, loss = 0.07743915\n",
      "Iteration 287, loss = 0.07730848\n",
      "Iteration 288, loss = 0.07717880\n",
      "Iteration 289, loss = 0.07705006\n",
      "Iteration 290, loss = 0.07692210\n",
      "Iteration 291, loss = 0.07679506\n",
      "Iteration 292, loss = 0.07666898\n",
      "Iteration 293, loss = 0.07654378\n",
      "Iteration 294, loss = 0.07641954\n",
      "Iteration 295, loss = 0.07629614\n",
      "Iteration 296, loss = 0.07617366\n",
      "Iteration 297, loss = 0.07605204\n",
      "Iteration 298, loss = 0.07593136\n",
      "Iteration 299, loss = 0.07581160\n",
      "Iteration 300, loss = 0.07569270\n",
      "Iteration 301, loss = 0.07557466\n",
      "Iteration 302, loss = 0.07545746\n",
      "Iteration 303, loss = 0.07534111\n",
      "Iteration 304, loss = 0.07522556\n",
      "Iteration 305, loss = 0.07511084\n",
      "Iteration 306, loss = 0.07499693\n",
      "Iteration 307, loss = 0.07488382\n",
      "Iteration 308, loss = 0.07477148\n",
      "Iteration 309, loss = 0.07465995\n",
      "Iteration 310, loss = 0.07454917\n",
      "Iteration 311, loss = 0.07443930\n",
      "Iteration 312, loss = 0.07433017\n",
      "Iteration 313, loss = 0.07422180\n",
      "Iteration 314, loss = 0.07411418\n",
      "Iteration 315, loss = 0.07400728\n",
      "Iteration 316, loss = 0.07390118\n",
      "Iteration 317, loss = 0.07379581\n",
      "Iteration 318, loss = 0.07369118\n",
      "Iteration 319, loss = 0.07358724\n",
      "Iteration 320, loss = 0.07348401\n",
      "Iteration 321, loss = 0.07338153\n",
      "Iteration 322, loss = 0.07327968\n",
      "Iteration 323, loss = 0.07317853\n",
      "Iteration 324, loss = 0.07307807\n",
      "Iteration 325, loss = 0.07297824\n",
      "Iteration 326, loss = 0.07287907\n",
      "Iteration 327, loss = 0.07278053\n",
      "Iteration 328, loss = 0.07268267\n",
      "Iteration 329, loss = 0.07258543\n",
      "Iteration 330, loss = 0.07248884\n",
      "Iteration 331, loss = 0.07239291\n",
      "Iteration 332, loss = 0.07229761\n",
      "Iteration 333, loss = 0.07220297\n",
      "Iteration 334, loss = 0.07210919\n",
      "Iteration 335, loss = 0.07201600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34996548\n",
      "Iteration 2, loss = 1.14784789\n",
      "Iteration 3, loss = 0.96133715\n",
      "Iteration 4, loss = 0.85692983\n",
      "Iteration 5, loss = 0.84686874\n",
      "Iteration 6, loss = 0.81547565\n",
      "Iteration 7, loss = 0.74781547\n",
      "Iteration 8, loss = 0.68582014\n",
      "Iteration 9, loss = 0.64322893\n",
      "Iteration 10, loss = 0.61236716\n",
      "Iteration 11, loss = 0.58664077\n",
      "Iteration 12, loss = 0.56437157\n",
      "Iteration 13, loss = 0.54526318\n",
      "Iteration 14, loss = 0.52676800\n",
      "Iteration 15, loss = 0.50913678\n",
      "Iteration 16, loss = 0.49267735\n",
      "Iteration 17, loss = 0.47703043\n",
      "Iteration 18, loss = 0.46302808\n",
      "Iteration 19, loss = 0.45078720\n",
      "Iteration 20, loss = 0.44012280\n",
      "Iteration 21, loss = 0.43041020\n",
      "Iteration 22, loss = 0.42130710\n",
      "Iteration 23, loss = 0.41275601\n",
      "Iteration 24, loss = 0.40462355\n",
      "Iteration 25, loss = 0.39676398\n",
      "Iteration 26, loss = 0.38915847\n",
      "Iteration 27, loss = 0.38164108\n",
      "Iteration 28, loss = 0.37435490\n",
      "Iteration 29, loss = 0.36755400\n",
      "Iteration 30, loss = 0.36136637\n",
      "Iteration 31, loss = 0.35567233\n",
      "Iteration 32, loss = 0.35038118\n",
      "Iteration 33, loss = 0.34531872\n",
      "Iteration 34, loss = 0.34041378\n",
      "Iteration 35, loss = 0.33563795\n",
      "Iteration 36, loss = 0.33099751\n",
      "Iteration 37, loss = 0.32648310\n",
      "Iteration 38, loss = 0.32210381\n",
      "Iteration 39, loss = 0.31783844\n",
      "Iteration 40, loss = 0.31369318\n",
      "Iteration 41, loss = 0.30965425\n",
      "Iteration 42, loss = 0.30571208\n",
      "Iteration 43, loss = 0.30186529\n",
      "Iteration 44, loss = 0.29810544\n",
      "Iteration 45, loss = 0.29442979\n",
      "Iteration 46, loss = 0.29083485\n",
      "Iteration 47, loss = 0.28732203\n",
      "Iteration 48, loss = 0.28388522\n",
      "Iteration 49, loss = 0.28052286\n",
      "Iteration 50, loss = 0.27723369\n",
      "Iteration 51, loss = 0.27401866\n",
      "Iteration 52, loss = 0.27087461\n",
      "Iteration 53, loss = 0.26779801\n",
      "Iteration 54, loss = 0.26478855\n",
      "Iteration 55, loss = 0.26184489\n",
      "Iteration 56, loss = 0.25896614\n",
      "Iteration 57, loss = 0.25615124\n",
      "Iteration 58, loss = 0.25340209\n",
      "Iteration 59, loss = 0.25071208\n",
      "Iteration 60, loss = 0.24808069\n",
      "Iteration 61, loss = 0.24550679\n",
      "Iteration 62, loss = 0.24298955\n",
      "Iteration 63, loss = 0.24052671\n",
      "Iteration 64, loss = 0.23811982\n",
      "Iteration 65, loss = 0.23576428\n",
      "Iteration 66, loss = 0.23346006\n",
      "Iteration 67, loss = 0.23120511\n",
      "Iteration 68, loss = 0.22900092\n",
      "Iteration 69, loss = 0.22684496\n",
      "Iteration 70, loss = 0.22473637\n",
      "Iteration 71, loss = 0.22267418\n",
      "Iteration 72, loss = 0.22065594\n",
      "Iteration 73, loss = 0.21868236\n",
      "Iteration 74, loss = 0.21675045\n",
      "Iteration 75, loss = 0.21486099\n",
      "Iteration 76, loss = 0.21301231\n",
      "Iteration 77, loss = 0.21120315\n",
      "Iteration 78, loss = 0.20943350\n",
      "Iteration 79, loss = 0.20770228\n",
      "Iteration 80, loss = 0.20600800\n",
      "Iteration 81, loss = 0.20434939\n",
      "Iteration 82, loss = 0.20272580\n",
      "Iteration 83, loss = 0.20113630\n",
      "Iteration 84, loss = 0.19958083\n",
      "Iteration 85, loss = 0.19805746\n",
      "Iteration 86, loss = 0.19656562\n",
      "Iteration 87, loss = 0.19510628\n",
      "Iteration 88, loss = 0.19367573\n",
      "Iteration 89, loss = 0.19227634\n",
      "Iteration 90, loss = 0.19090523\n",
      "Iteration 91, loss = 0.18956187\n",
      "Iteration 92, loss = 0.18824669\n",
      "Iteration 93, loss = 0.18695900\n",
      "Iteration 94, loss = 0.18569733\n",
      "Iteration 95, loss = 0.18446209\n",
      "Iteration 96, loss = 0.18325043\n",
      "Iteration 97, loss = 0.18206429\n",
      "Iteration 98, loss = 0.18090110\n",
      "Iteration 99, loss = 0.17976069\n",
      "Iteration 100, loss = 0.17864411\n",
      "Iteration 101, loss = 0.17754763\n",
      "Iteration 102, loss = 0.17647427\n",
      "Iteration 103, loss = 0.17542083\n",
      "Iteration 104, loss = 0.17438776\n",
      "Iteration 105, loss = 0.17337534\n",
      "Iteration 106, loss = 0.17238155\n",
      "Iteration 107, loss = 0.17140796\n",
      "Iteration 108, loss = 0.17045208\n",
      "Iteration 109, loss = 0.16951511\n",
      "Iteration 110, loss = 0.16859540\n",
      "Iteration 111, loss = 0.16769331\n",
      "Iteration 112, loss = 0.16680768\n",
      "Iteration 113, loss = 0.16593863\n",
      "Iteration 114, loss = 0.16508544\n",
      "Iteration 115, loss = 0.16424797\n",
      "Iteration 116, loss = 0.16342555\n",
      "Iteration 117, loss = 0.16261790\n",
      "Iteration 118, loss = 0.16182496\n",
      "Iteration 119, loss = 0.16104583\n",
      "Iteration 120, loss = 0.16028102\n",
      "Iteration 121, loss = 0.15952906\n",
      "Iteration 122, loss = 0.15879098\n",
      "Iteration 123, loss = 0.15806496\n",
      "Iteration 124, loss = 0.15735231\n",
      "Iteration 125, loss = 0.15665131\n",
      "Iteration 126, loss = 0.15596282\n",
      "Iteration 127, loss = 0.15528546\n",
      "Iteration 128, loss = 0.15462005\n",
      "Iteration 129, loss = 0.15396551\n",
      "Iteration 130, loss = 0.15332192\n",
      "Iteration 131, loss = 0.15268936\n",
      "Iteration 132, loss = 0.15206673\n",
      "Iteration 133, loss = 0.15145567\n",
      "Iteration 134, loss = 0.15085386\n",
      "Iteration 135, loss = 0.15026233\n",
      "Iteration 136, loss = 0.14967987\n",
      "Iteration 137, loss = 0.14910688\n",
      "Iteration 138, loss = 0.14854342\n",
      "Iteration 139, loss = 0.14798848\n",
      "Iteration 140, loss = 0.14744287\n",
      "Iteration 141, loss = 0.14690515\n",
      "Iteration 142, loss = 0.14637667\n",
      "Iteration 143, loss = 0.14585576\n",
      "Iteration 144, loss = 0.14534347\n",
      "Iteration 145, loss = 0.14483860\n",
      "Iteration 146, loss = 0.14434215\n",
      "Iteration 147, loss = 0.14385248\n",
      "Iteration 148, loss = 0.14337083\n",
      "Iteration 149, loss = 0.14289590\n",
      "Iteration 150, loss = 0.14242824\n",
      "Iteration 151, loss = 0.14196755\n",
      "Iteration 152, loss = 0.14151366\n",
      "Iteration 153, loss = 0.14106661\n",
      "Iteration 154, loss = 0.14062581\n",
      "Iteration 155, loss = 0.14019201\n",
      "Iteration 156, loss = 0.13976378\n",
      "Iteration 157, loss = 0.13934212\n",
      "Iteration 158, loss = 0.13892644\n",
      "Iteration 159, loss = 0.13851644\n",
      "Iteration 160, loss = 0.13811235\n",
      "Iteration 161, loss = 0.13771402\n",
      "Iteration 162, loss = 0.13732101\n",
      "Iteration 163, loss = 0.13693367\n",
      "Iteration 164, loss = 0.13655149\n",
      "Iteration 165, loss = 0.13617464\n",
      "Iteration 166, loss = 0.13580320\n",
      "Iteration 167, loss = 0.13543648\n",
      "Iteration 168, loss = 0.13507472\n",
      "Iteration 169, loss = 0.13471787\n",
      "Iteration 170, loss = 0.13436599\n",
      "Iteration 171, loss = 0.13401856\n",
      "Iteration 172, loss = 0.13367569\n",
      "Iteration 173, loss = 0.13333730\n",
      "Iteration 174, loss = 0.13300344\n",
      "Iteration 175, loss = 0.13267395\n",
      "Iteration 176, loss = 0.13234860\n",
      "Iteration 177, loss = 0.13202742\n",
      "Iteration 178, loss = 0.13171028\n",
      "Iteration 179, loss = 0.13139770\n",
      "Iteration 180, loss = 0.13108832\n",
      "Iteration 181, loss = 0.13078312\n",
      "Iteration 182, loss = 0.13048180\n",
      "Iteration 183, loss = 0.13018419\n",
      "Iteration 184, loss = 0.12989026\n",
      "Iteration 185, loss = 0.12960014\n",
      "Iteration 186, loss = 0.12931330\n",
      "Iteration 187, loss = 0.12902998\n",
      "Iteration 188, loss = 0.12875009\n",
      "Iteration 189, loss = 0.12847357\n",
      "Iteration 190, loss = 0.12820033\n",
      "Iteration 191, loss = 0.12793042\n",
      "Iteration 192, loss = 0.12766366\n",
      "Iteration 193, loss = 0.12740008\n",
      "Iteration 194, loss = 0.12713964\n",
      "Iteration 195, loss = 0.12688214\n",
      "Iteration 196, loss = 0.12662766\n",
      "Iteration 197, loss = 0.12637607\n",
      "Iteration 198, loss = 0.12612740\n",
      "Iteration 199, loss = 0.12588153\n",
      "Iteration 200, loss = 0.12563847\n",
      "Iteration 201, loss = 0.12539811\n",
      "Iteration 202, loss = 0.12516051\n",
      "Iteration 203, loss = 0.12492553\n",
      "Iteration 204, loss = 0.12469317\n",
      "Iteration 205, loss = 0.12446338\n",
      "Iteration 206, loss = 0.12423609\n",
      "Iteration 207, loss = 0.12401130\n",
      "Iteration 208, loss = 0.12378896\n",
      "Iteration 209, loss = 0.12356911\n",
      "Iteration 210, loss = 0.12335153\n",
      "Iteration 211, loss = 0.12313631\n",
      "Iteration 212, loss = 0.12292336\n",
      "Iteration 213, loss = 0.12271266\n",
      "Iteration 214, loss = 0.12250426\n",
      "Iteration 215, loss = 0.12229813\n",
      "Iteration 216, loss = 0.12209405\n",
      "Iteration 217, loss = 0.12189207\n",
      "Iteration 218, loss = 0.12169218\n",
      "Iteration 219, loss = 0.12149434\n",
      "Iteration 220, loss = 0.12129853\n",
      "Iteration 221, loss = 0.12110481\n",
      "Iteration 222, loss = 0.12091302\n",
      "Iteration 223, loss = 0.12072314\n",
      "Iteration 224, loss = 0.12053517\n",
      "Iteration 225, loss = 0.12034907\n",
      "Iteration 226, loss = 0.12016480\n",
      "Iteration 227, loss = 0.11998256\n",
      "Iteration 228, loss = 0.11980189\n",
      "Iteration 229, loss = 0.11962304\n",
      "Iteration 230, loss = 0.11944594\n",
      "Iteration 231, loss = 0.11927059\n",
      "Iteration 232, loss = 0.11909701\n",
      "Iteration 233, loss = 0.11892501\n",
      "Iteration 234, loss = 0.11875465\n",
      "Iteration 235, loss = 0.11858590\n",
      "Iteration 236, loss = 0.11841873\n",
      "Iteration 237, loss = 0.11825333\n",
      "Iteration 238, loss = 0.11808925\n",
      "Iteration 239, loss = 0.11792676\n",
      "Iteration 240, loss = 0.11776582\n",
      "Iteration 241, loss = 0.11760644\n",
      "Iteration 242, loss = 0.11744845\n",
      "Iteration 243, loss = 0.11729190\n",
      "Iteration 244, loss = 0.11713679\n",
      "Iteration 245, loss = 0.11698311\n",
      "Iteration 246, loss = 0.11683092\n",
      "Iteration 247, loss = 0.11667998\n",
      "Iteration 248, loss = 0.11653037\n",
      "Iteration 249, loss = 0.11638228\n",
      "Iteration 250, loss = 0.11623533\n",
      "Iteration 251, loss = 0.11608970\n",
      "Iteration 252, loss = 0.11594545\n",
      "Iteration 253, loss = 0.11580245\n",
      "Iteration 254, loss = 0.11566065\n",
      "Iteration 255, loss = 0.11552005\n",
      "Iteration 256, loss = 0.11538072\n",
      "Iteration 257, loss = 0.11524265\n",
      "Iteration 258, loss = 0.11510567\n",
      "Iteration 259, loss = 0.11496984\n",
      "Iteration 260, loss = 0.11483536\n",
      "Iteration 261, loss = 0.11470178\n",
      "Iteration 262, loss = 0.11456938\n",
      "Iteration 263, loss = 0.11443828\n",
      "Iteration 264, loss = 0.11430806\n",
      "Iteration 265, loss = 0.11417894\n",
      "Iteration 266, loss = 0.11405108\n",
      "Iteration 267, loss = 0.11392413\n",
      "Iteration 268, loss = 0.11379824\n",
      "Iteration 269, loss = 0.11367337\n",
      "Iteration 270, loss = 0.11354965\n",
      "Iteration 271, loss = 0.11342682\n",
      "Iteration 272, loss = 0.11330498\n",
      "Iteration 273, loss = 0.11318412\n",
      "Iteration 274, loss = 0.11306429\n",
      "Iteration 275, loss = 0.11294541\n",
      "Iteration 276, loss = 0.11282744\n",
      "Iteration 277, loss = 0.11271040\n",
      "Iteration 278, loss = 0.11259426\n",
      "Iteration 279, loss = 0.11247923\n",
      "Iteration 280, loss = 0.11236489\n",
      "Iteration 281, loss = 0.11225149\n",
      "Iteration 282, loss = 0.11213908\n",
      "Iteration 283, loss = 0.11202744\n",
      "Iteration 284, loss = 0.11191666\n",
      "Iteration 285, loss = 0.11180672\n",
      "Iteration 286, loss = 0.11169769\n",
      "Iteration 287, loss = 0.11158944\n",
      "Iteration 288, loss = 0.11148199\n",
      "Iteration 289, loss = 0.11137534\n",
      "Iteration 290, loss = 0.11126963\n",
      "Iteration 291, loss = 0.11116454\n",
      "Iteration 292, loss = 0.11106028\n",
      "Iteration 293, loss = 0.11095679\n",
      "Iteration 294, loss = 0.11085412\n",
      "Iteration 295, loss = 0.11075219\n",
      "Iteration 296, loss = 0.11065100\n",
      "Iteration 297, loss = 0.11055054\n",
      "Iteration 298, loss = 0.11045080\n",
      "Iteration 299, loss = 0.11035177\n",
      "Iteration 300, loss = 0.11025343\n",
      "Iteration 301, loss = 0.11015591\n",
      "Iteration 302, loss = 0.11005891\n",
      "Iteration 303, loss = 0.10996266\n",
      "Iteration 304, loss = 0.10986708\n",
      "Iteration 305, loss = 0.10977227\n",
      "Iteration 306, loss = 0.10967804\n",
      "Iteration 307, loss = 0.10958448\n",
      "Iteration 308, loss = 0.10949156\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35652749\n",
      "Iteration 2, loss = 1.31012045\n",
      "Iteration 3, loss = 1.26643198\n",
      "Iteration 4, loss = 1.22553533\n",
      "Iteration 5, loss = 1.18712214\n",
      "Iteration 6, loss = 1.15097675\n",
      "Iteration 7, loss = 1.11679489\n",
      "Iteration 8, loss = 1.08433352\n",
      "Iteration 9, loss = 1.05344777\n",
      "Iteration 10, loss = 1.02427548\n",
      "Iteration 11, loss = 0.99681347\n",
      "Iteration 12, loss = 0.97117390\n",
      "Iteration 13, loss = 0.94751401\n",
      "Iteration 14, loss = 0.92586927\n",
      "Iteration 15, loss = 0.90623024\n",
      "Iteration 16, loss = 0.88885227\n",
      "Iteration 17, loss = 0.87349786\n",
      "Iteration 18, loss = 0.86004620\n",
      "Iteration 19, loss = 0.84839160\n",
      "Iteration 20, loss = 0.83822994\n",
      "Iteration 21, loss = 0.82905945\n",
      "Iteration 22, loss = 0.82057335\n",
      "Iteration 23, loss = 0.81245782\n",
      "Iteration 24, loss = 0.80446602\n",
      "Iteration 25, loss = 0.79638196\n",
      "Iteration 26, loss = 0.78806801\n",
      "Iteration 27, loss = 0.77941877\n",
      "Iteration 28, loss = 0.77041333\n",
      "Iteration 29, loss = 0.76109467\n",
      "Iteration 30, loss = 0.75160524\n",
      "Iteration 31, loss = 0.74202020\n",
      "Iteration 32, loss = 0.73244855\n",
      "Iteration 33, loss = 0.72292116\n",
      "Iteration 34, loss = 0.71350466\n",
      "Iteration 35, loss = 0.70421640\n",
      "Iteration 36, loss = 0.69513376\n",
      "Iteration 37, loss = 0.68641513\n",
      "Iteration 38, loss = 0.67806956\n",
      "Iteration 39, loss = 0.67014411\n",
      "Iteration 40, loss = 0.66265601\n",
      "Iteration 41, loss = 0.65557571\n",
      "Iteration 42, loss = 0.64883384\n",
      "Iteration 43, loss = 0.64243012\n",
      "Iteration 44, loss = 0.63627714\n",
      "Iteration 45, loss = 0.63038459\n",
      "Iteration 46, loss = 0.62469962\n",
      "Iteration 47, loss = 0.61925227\n",
      "Iteration 48, loss = 0.61395477\n",
      "Iteration 49, loss = 0.60888300\n",
      "Iteration 50, loss = 0.60397344\n",
      "Iteration 51, loss = 0.59919672\n",
      "Iteration 52, loss = 0.59452269\n",
      "Iteration 53, loss = 0.58993470\n",
      "Iteration 54, loss = 0.58542794\n",
      "Iteration 55, loss = 0.58100203\n",
      "Iteration 56, loss = 0.57662405\n",
      "Iteration 57, loss = 0.57228637\n",
      "Iteration 58, loss = 0.56799256\n",
      "Iteration 59, loss = 0.56374494\n",
      "Iteration 60, loss = 0.55954763\n",
      "Iteration 61, loss = 0.55540745\n",
      "Iteration 62, loss = 0.55132692\n",
      "Iteration 63, loss = 0.54731366\n",
      "Iteration 64, loss = 0.54338542\n",
      "Iteration 65, loss = 0.53951319\n",
      "Iteration 66, loss = 0.53569350\n",
      "Iteration 67, loss = 0.53193592\n",
      "Iteration 68, loss = 0.52822215\n",
      "Iteration 69, loss = 0.52452936\n",
      "Iteration 70, loss = 0.52087693\n",
      "Iteration 71, loss = 0.51726644\n",
      "Iteration 72, loss = 0.51369911\n",
      "Iteration 73, loss = 0.51018173\n",
      "Iteration 74, loss = 0.50670789\n",
      "Iteration 75, loss = 0.50327645\n",
      "Iteration 76, loss = 0.49989361\n",
      "Iteration 77, loss = 0.49658436\n",
      "Iteration 78, loss = 0.49336358\n",
      "Iteration 79, loss = 0.49022221\n",
      "Iteration 80, loss = 0.48716928\n",
      "Iteration 81, loss = 0.48415943\n",
      "Iteration 82, loss = 0.48118915\n",
      "Iteration 83, loss = 0.47825081\n",
      "Iteration 84, loss = 0.47534530\n",
      "Iteration 85, loss = 0.47247786\n",
      "Iteration 86, loss = 0.46964727\n",
      "Iteration 87, loss = 0.46685094\n",
      "Iteration 88, loss = 0.46408853\n",
      "Iteration 89, loss = 0.46136141\n",
      "Iteration 90, loss = 0.45866843\n",
      "Iteration 91, loss = 0.45600899\n",
      "Iteration 92, loss = 0.45338581\n",
      "Iteration 93, loss = 0.45079675\n",
      "Iteration 94, loss = 0.44824126\n",
      "Iteration 95, loss = 0.44571693\n",
      "Iteration 96, loss = 0.44322236\n",
      "Iteration 97, loss = 0.44075719\n",
      "Iteration 98, loss = 0.43832067\n",
      "Iteration 99, loss = 0.43591061\n",
      "Iteration 100, loss = 0.43352615\n",
      "Iteration 101, loss = 0.43116729\n",
      "Iteration 102, loss = 0.42883270\n",
      "Iteration 103, loss = 0.42652205\n",
      "Iteration 104, loss = 0.42423499\n",
      "Iteration 105, loss = 0.42197431\n",
      "Iteration 106, loss = 0.41973690\n",
      "Iteration 107, loss = 0.41752205\n",
      "Iteration 108, loss = 0.41532929\n",
      "Iteration 109, loss = 0.41315813\n",
      "Iteration 110, loss = 0.41100804\n",
      "Iteration 111, loss = 0.40887905\n",
      "Iteration 112, loss = 0.40676944\n",
      "Iteration 113, loss = 0.40467829\n",
      "Iteration 114, loss = 0.40260541\n",
      "Iteration 115, loss = 0.40055197\n",
      "Iteration 116, loss = 0.39851634\n",
      "Iteration 117, loss = 0.39649726\n",
      "Iteration 118, loss = 0.39449460\n",
      "Iteration 119, loss = 0.39250759\n",
      "Iteration 120, loss = 0.39053582\n",
      "Iteration 121, loss = 0.38857903\n",
      "Iteration 122, loss = 0.38663709\n",
      "Iteration 123, loss = 0.38470979\n",
      "Iteration 124, loss = 0.38279691\n",
      "Iteration 125, loss = 0.38089812\n",
      "Iteration 126, loss = 0.37901222\n",
      "Iteration 127, loss = 0.37713895\n",
      "Iteration 128, loss = 0.37527824\n",
      "Iteration 129, loss = 0.37342987\n",
      "Iteration 130, loss = 0.37159342\n",
      "Iteration 131, loss = 0.36976847\n",
      "Iteration 132, loss = 0.36795449\n",
      "Iteration 133, loss = 0.36615113\n",
      "Iteration 134, loss = 0.36435847\n",
      "Iteration 135, loss = 0.36257726\n",
      "Iteration 136, loss = 0.36080707\n",
      "Iteration 137, loss = 0.35904694\n",
      "Iteration 138, loss = 0.35729721\n",
      "Iteration 139, loss = 0.35555713\n",
      "Iteration 140, loss = 0.35382598\n",
      "Iteration 141, loss = 0.35210433\n",
      "Iteration 142, loss = 0.35039215\n",
      "Iteration 143, loss = 0.34868935\n",
      "Iteration 144, loss = 0.34699583\n",
      "Iteration 145, loss = 0.34531101\n",
      "Iteration 146, loss = 0.34363504\n",
      "Iteration 147, loss = 0.34196779\n",
      "Iteration 148, loss = 0.34030940\n",
      "Iteration 149, loss = 0.33865966\n",
      "Iteration 150, loss = 0.33701817\n",
      "Iteration 151, loss = 0.33538488\n",
      "Iteration 152, loss = 0.33375978\n",
      "Iteration 153, loss = 0.33214327\n",
      "Iteration 154, loss = 0.33053529\n",
      "Iteration 155, loss = 0.32893498\n",
      "Iteration 156, loss = 0.32734225\n",
      "Iteration 157, loss = 0.32575758\n",
      "Iteration 158, loss = 0.32418093\n",
      "Iteration 159, loss = 0.32261198\n",
      "Iteration 160, loss = 0.32105064\n",
      "Iteration 161, loss = 0.31949710\n",
      "Iteration 162, loss = 0.31795158\n",
      "Iteration 163, loss = 0.31641353\n",
      "Iteration 164, loss = 0.31488305\n",
      "Iteration 165, loss = 0.31336030\n",
      "Iteration 166, loss = 0.31184520\n",
      "Iteration 167, loss = 0.31033775\n",
      "Iteration 168, loss = 0.30883779\n",
      "Iteration 169, loss = 0.30734540\n",
      "Iteration 170, loss = 0.30586036\n",
      "Iteration 171, loss = 0.30438263\n",
      "Iteration 172, loss = 0.30291228\n",
      "Iteration 173, loss = 0.30144957\n",
      "Iteration 174, loss = 0.29999443\n",
      "Iteration 175, loss = 0.29854757\n",
      "Iteration 176, loss = 0.29710807\n",
      "Iteration 177, loss = 0.29567626\n",
      "Iteration 178, loss = 0.29425182\n",
      "Iteration 179, loss = 0.29283585\n",
      "Iteration 180, loss = 0.29142919\n",
      "Iteration 181, loss = 0.29002989\n",
      "Iteration 182, loss = 0.28863796\n",
      "Iteration 183, loss = 0.28725351\n",
      "Iteration 184, loss = 0.28587623\n",
      "Iteration 185, loss = 0.28450487\n",
      "Iteration 186, loss = 0.28313630\n",
      "Iteration 187, loss = 0.28176789\n",
      "Iteration 188, loss = 0.28039735\n",
      "Iteration 189, loss = 0.27902586\n",
      "Iteration 190, loss = 0.27765478\n",
      "Iteration 191, loss = 0.27630673\n",
      "Iteration 192, loss = 0.27496945\n",
      "Iteration 193, loss = 0.27363989\n",
      "Iteration 194, loss = 0.27231240\n",
      "Iteration 195, loss = 0.27098613\n",
      "Iteration 196, loss = 0.26966186\n",
      "Iteration 197, loss = 0.26833961\n",
      "Iteration 198, loss = 0.26701874\n",
      "Iteration 199, loss = 0.26569863\n",
      "Iteration 200, loss = 0.26437901\n",
      "Iteration 201, loss = 0.26305988\n",
      "Iteration 202, loss = 0.26174165\n",
      "Iteration 203, loss = 0.26042428\n",
      "Iteration 204, loss = 0.25910816\n",
      "Iteration 205, loss = 0.25779349\n",
      "Iteration 206, loss = 0.25647941\n",
      "Iteration 207, loss = 0.25516785\n",
      "Iteration 208, loss = 0.25385757\n",
      "Iteration 209, loss = 0.25254954\n",
      "Iteration 210, loss = 0.25124559\n",
      "Iteration 211, loss = 0.24994423\n",
      "Iteration 212, loss = 0.24864556\n",
      "Iteration 213, loss = 0.24734991\n",
      "Iteration 214, loss = 0.24605743\n",
      "Iteration 215, loss = 0.24476829\n",
      "Iteration 216, loss = 0.24348293\n",
      "Iteration 217, loss = 0.24220111\n",
      "Iteration 218, loss = 0.24092380\n",
      "Iteration 219, loss = 0.23965072\n",
      "Iteration 220, loss = 0.23838194\n",
      "Iteration 221, loss = 0.23711785\n",
      "Iteration 222, loss = 0.23585880\n",
      "Iteration 223, loss = 0.23460546\n",
      "Iteration 224, loss = 0.23335797\n",
      "Iteration 225, loss = 0.23211743\n",
      "Iteration 226, loss = 0.23088326\n",
      "Iteration 227, loss = 0.22965525\n",
      "Iteration 228, loss = 0.22843358\n",
      "Iteration 229, loss = 0.22721860\n",
      "Iteration 230, loss = 0.22601049\n",
      "Iteration 231, loss = 0.22480931\n",
      "Iteration 232, loss = 0.22361516\n",
      "Iteration 233, loss = 0.22242885\n",
      "Iteration 234, loss = 0.22124930\n",
      "Iteration 235, loss = 0.22007758\n",
      "Iteration 236, loss = 0.21891346\n",
      "Iteration 237, loss = 0.21775708\n",
      "Iteration 238, loss = 0.21660898\n",
      "Iteration 239, loss = 0.21546889\n",
      "Iteration 240, loss = 0.21433705\n",
      "Iteration 241, loss = 0.21321359\n",
      "Iteration 242, loss = 0.21209846\n",
      "Iteration 243, loss = 0.21099189\n",
      "Iteration 244, loss = 0.20989445\n",
      "Iteration 245, loss = 0.20880558\n",
      "Iteration 246, loss = 0.20772519\n",
      "Iteration 247, loss = 0.20665338\n",
      "Iteration 248, loss = 0.20559088\n",
      "Iteration 249, loss = 0.20453657\n",
      "Iteration 250, loss = 0.20349060\n",
      "Iteration 251, loss = 0.20245394\n",
      "Iteration 252, loss = 0.20142596\n",
      "Iteration 253, loss = 0.20040670\n",
      "Iteration 254, loss = 0.19939637\n",
      "Iteration 255, loss = 0.19839496\n",
      "Iteration 256, loss = 0.19740249\n",
      "Iteration 257, loss = 0.19641887\n",
      "Iteration 258, loss = 0.19544403\n",
      "Iteration 259, loss = 0.19447831\n",
      "Iteration 260, loss = 0.19352179\n",
      "Iteration 261, loss = 0.19257437\n",
      "Iteration 262, loss = 0.19163569\n",
      "Iteration 263, loss = 0.19070587\n",
      "Iteration 264, loss = 0.18978455\n",
      "Iteration 265, loss = 0.18887172\n",
      "Iteration 266, loss = 0.18796747\n",
      "Iteration 267, loss = 0.18707193\n",
      "Iteration 268, loss = 0.18618497\n",
      "Iteration 269, loss = 0.18530647\n",
      "Iteration 270, loss = 0.18443645\n",
      "Iteration 271, loss = 0.18357472\n",
      "Iteration 272, loss = 0.18272120\n",
      "Iteration 273, loss = 0.18187615\n",
      "Iteration 274, loss = 0.18103882\n",
      "Iteration 275, loss = 0.18021049\n",
      "Iteration 276, loss = 0.17938992\n",
      "Iteration 277, loss = 0.17857846\n",
      "Iteration 278, loss = 0.17777525\n",
      "Iteration 279, loss = 0.17697980\n",
      "Iteration 280, loss = 0.17619207\n",
      "Iteration 281, loss = 0.17541218\n",
      "Iteration 282, loss = 0.17464050\n",
      "Iteration 283, loss = 0.17387595\n",
      "Iteration 284, loss = 0.17311894\n",
      "Iteration 285, loss = 0.17236997\n",
      "Iteration 286, loss = 0.17162850\n",
      "Iteration 287, loss = 0.17089480\n",
      "Iteration 288, loss = 0.17016843\n",
      "Iteration 289, loss = 0.16944917\n",
      "Iteration 290, loss = 0.16873704\n",
      "Iteration 291, loss = 0.16803204\n",
      "Iteration 292, loss = 0.16733403\n",
      "Iteration 293, loss = 0.16664296\n",
      "Iteration 294, loss = 0.16595878\n",
      "Iteration 295, loss = 0.16528155\n",
      "Iteration 296, loss = 0.16461111\n",
      "Iteration 297, loss = 0.16394729\n",
      "Iteration 298, loss = 0.16328996\n",
      "Iteration 299, loss = 0.16263945\n",
      "Iteration 300, loss = 0.16199558\n",
      "Iteration 301, loss = 0.16135815\n",
      "Iteration 302, loss = 0.16072709\n",
      "Iteration 303, loss = 0.16010229\n",
      "Iteration 304, loss = 0.15948374\n",
      "Iteration 305, loss = 0.15887130\n",
      "Iteration 306, loss = 0.15826489\n",
      "Iteration 307, loss = 0.15766431\n",
      "Iteration 308, loss = 0.15706978\n",
      "Iteration 309, loss = 0.15648121\n",
      "Iteration 310, loss = 0.15589871\n",
      "Iteration 311, loss = 0.15532188\n",
      "Iteration 312, loss = 0.15475071\n",
      "Iteration 313, loss = 0.15418513\n",
      "Iteration 314, loss = 0.15362476\n",
      "Iteration 315, loss = 0.15306990\n",
      "Iteration 316, loss = 0.15252050\n",
      "Iteration 317, loss = 0.15197591\n",
      "Iteration 318, loss = 0.15143663\n",
      "Iteration 319, loss = 0.15090126\n",
      "Iteration 320, loss = 0.15037118\n",
      "Iteration 321, loss = 0.14984519\n",
      "Iteration 322, loss = 0.14932330\n",
      "Iteration 323, loss = 0.14880521\n",
      "Iteration 324, loss = 0.14829045\n",
      "Iteration 325, loss = 0.14777882\n",
      "Iteration 326, loss = 0.14726982\n",
      "Iteration 327, loss = 0.14676310\n",
      "Iteration 328, loss = 0.14625886\n",
      "Iteration 329, loss = 0.14575670\n",
      "Iteration 330, loss = 0.14525649\n",
      "Iteration 331, loss = 0.14475828\n",
      "Iteration 332, loss = 0.14426177\n",
      "Iteration 333, loss = 0.14376689\n",
      "Iteration 334, loss = 0.14327313\n",
      "Iteration 335, loss = 0.14277975\n",
      "Iteration 336, loss = 0.14229242\n",
      "Iteration 337, loss = 0.14179939\n",
      "Iteration 338, loss = 0.14131344\n",
      "Iteration 339, loss = 0.14082784\n",
      "Iteration 340, loss = 0.14034342\n",
      "Iteration 341, loss = 0.13986002\n",
      "Iteration 342, loss = 0.13937690\n",
      "Iteration 343, loss = 0.13889487\n",
      "Iteration 344, loss = 0.13841384\n",
      "Iteration 345, loss = 0.13793270\n",
      "Iteration 346, loss = 0.13745015\n",
      "Iteration 347, loss = 0.13696659\n",
      "Iteration 348, loss = 0.13648550\n",
      "Iteration 349, loss = 0.13601715\n",
      "Iteration 350, loss = 0.13553371\n",
      "Iteration 351, loss = 0.13505937\n",
      "Iteration 352, loss = 0.13459005\n",
      "Iteration 353, loss = 0.13411869\n",
      "Iteration 354, loss = 0.13364808\n",
      "Iteration 355, loss = 0.13317489\n",
      "Iteration 356, loss = 0.13270198\n",
      "Iteration 357, loss = 0.13223030\n",
      "Iteration 358, loss = 0.13176493\n",
      "Iteration 359, loss = 0.13129707\n",
      "Iteration 360, loss = 0.13082483\n",
      "Iteration 361, loss = 0.13035895\n",
      "Iteration 362, loss = 0.12989159\n",
      "Iteration 363, loss = 0.12942440\n",
      "Iteration 364, loss = 0.12895503\n",
      "Iteration 365, loss = 0.12848521\n",
      "Iteration 366, loss = 0.12801591\n",
      "Iteration 367, loss = 0.12754771\n",
      "Iteration 368, loss = 0.12708084\n",
      "Iteration 369, loss = 0.12661016\n",
      "Iteration 370, loss = 0.12614117\n",
      "Iteration 371, loss = 0.12567336\n",
      "Iteration 372, loss = 0.12520695\n",
      "Iteration 373, loss = 0.12474012\n",
      "Iteration 374, loss = 0.12427376\n",
      "Iteration 375, loss = 0.12380895\n",
      "Iteration 376, loss = 0.12334569\n",
      "Iteration 377, loss = 0.12288472\n",
      "Iteration 378, loss = 0.12242480\n",
      "Iteration 379, loss = 0.12196747\n",
      "Iteration 380, loss = 0.12151172\n",
      "Iteration 381, loss = 0.12105879\n",
      "Iteration 382, loss = 0.12060824\n",
      "Iteration 383, loss = 0.12016048\n",
      "Iteration 384, loss = 0.11971594\n",
      "Iteration 385, loss = 0.11927410\n",
      "Iteration 386, loss = 0.11883540\n",
      "Iteration 387, loss = 0.11839980\n",
      "Iteration 388, loss = 0.11796752\n",
      "Iteration 389, loss = 0.11753900\n",
      "Iteration 390, loss = 0.11711381\n",
      "Iteration 391, loss = 0.11669217\n",
      "Iteration 392, loss = 0.11627395\n",
      "Iteration 393, loss = 0.11585968\n",
      "Iteration 394, loss = 0.11544888\n",
      "Iteration 395, loss = 0.11504171\n",
      "Iteration 396, loss = 0.11463815\n",
      "Iteration 397, loss = 0.11423831\n",
      "Iteration 398, loss = 0.11384210\n",
      "Iteration 399, loss = 0.11344969\n",
      "Iteration 400, loss = 0.11306103\n",
      "Iteration 401, loss = 0.11267600\n",
      "Iteration 402, loss = 0.11229465\n",
      "Iteration 403, loss = 0.11191708\n",
      "Iteration 404, loss = 0.11154324\n",
      "Iteration 405, loss = 0.11117312\n",
      "Iteration 406, loss = 0.11080658\n",
      "Iteration 407, loss = 0.11044384\n",
      "Iteration 408, loss = 0.11008487\n",
      "Iteration 409, loss = 0.10972970\n",
      "Iteration 410, loss = 0.10937849\n",
      "Iteration 411, loss = 0.10903101\n",
      "Iteration 412, loss = 0.10868734\n",
      "Iteration 413, loss = 0.10834755\n",
      "Iteration 414, loss = 0.10801156\n",
      "Iteration 415, loss = 0.10767955\n",
      "Iteration 416, loss = 0.10735140\n",
      "Iteration 417, loss = 0.10702706\n",
      "Iteration 418, loss = 0.10670658\n",
      "Iteration 419, loss = 0.10639009\n",
      "Iteration 420, loss = 0.10607766\n",
      "Iteration 421, loss = 0.10576895\n",
      "Iteration 422, loss = 0.10546398\n",
      "Iteration 423, loss = 0.10516294\n",
      "Iteration 424, loss = 0.10486578\n",
      "Iteration 425, loss = 0.10457243\n",
      "Iteration 426, loss = 0.10428275\n",
      "Iteration 427, loss = 0.10399690\n",
      "Iteration 428, loss = 0.10371483\n",
      "Iteration 429, loss = 0.10343644\n",
      "Iteration 430, loss = 0.10316165\n",
      "Iteration 431, loss = 0.10289064\n",
      "Iteration 432, loss = 0.10262321\n",
      "Iteration 433, loss = 0.10235929\n",
      "Iteration 434, loss = 0.10209888\n",
      "Iteration 435, loss = 0.10184218\n",
      "Iteration 436, loss = 0.10158863\n",
      "Iteration 437, loss = 0.10133865\n",
      "Iteration 438, loss = 0.10109191\n",
      "Iteration 439, loss = 0.10084857\n",
      "Iteration 440, loss = 0.10060863\n",
      "Iteration 441, loss = 0.10037192\n",
      "Iteration 442, loss = 0.10013872\n",
      "Iteration 443, loss = 0.09990869\n",
      "Iteration 444, loss = 0.09968154\n",
      "Iteration 445, loss = 0.09945778\n",
      "Iteration 446, loss = 0.09923701\n",
      "Iteration 447, loss = 0.09901928\n",
      "Iteration 448, loss = 0.09880451\n",
      "Iteration 449, loss = 0.09859274\n",
      "Iteration 450, loss = 0.09838393\n",
      "Iteration 451, loss = 0.09817812\n",
      "Iteration 452, loss = 0.09797521\n",
      "Iteration 453, loss = 0.09777512\n",
      "Iteration 454, loss = 0.09757765\n",
      "Iteration 455, loss = 0.09738285\n",
      "Iteration 456, loss = 0.09719068\n",
      "Iteration 457, loss = 0.09700106\n",
      "Iteration 458, loss = 0.09681396\n",
      "Iteration 459, loss = 0.09662947\n",
      "Iteration 460, loss = 0.09644752\n",
      "Iteration 461, loss = 0.09626804\n",
      "Iteration 462, loss = 0.09609083\n",
      "Iteration 463, loss = 0.09591591\n",
      "Iteration 464, loss = 0.09574324\n",
      "Iteration 465, loss = 0.09557281\n",
      "Iteration 466, loss = 0.09540450\n",
      "Iteration 467, loss = 0.09523838\n",
      "Iteration 468, loss = 0.09507441\n",
      "Iteration 469, loss = 0.09491242\n",
      "Iteration 470, loss = 0.09475236\n",
      "Iteration 471, loss = 0.09459423\n",
      "Iteration 472, loss = 0.09443801\n",
      "Iteration 473, loss = 0.09428373\n",
      "Iteration 474, loss = 0.09413116\n",
      "Iteration 475, loss = 0.09398031\n",
      "Iteration 476, loss = 0.09383122\n",
      "Iteration 477, loss = 0.09368375\n",
      "Iteration 478, loss = 0.09353786\n",
      "Iteration 479, loss = 0.09339349\n",
      "Iteration 480, loss = 0.09325064\n",
      "Iteration 481, loss = 0.09310926\n",
      "Iteration 482, loss = 0.09296936\n",
      "Iteration 483, loss = 0.09283091\n",
      "Iteration 484, loss = 0.09269390\n",
      "Iteration 485, loss = 0.09255825\n",
      "Iteration 486, loss = 0.09242388\n",
      "Iteration 487, loss = 0.09229083\n",
      "Iteration 488, loss = 0.09215906\n",
      "Iteration 489, loss = 0.09202852\n",
      "Iteration 490, loss = 0.09189919\n",
      "Iteration 491, loss = 0.09177109\n",
      "Iteration 492, loss = 0.09164420\n",
      "Iteration 493, loss = 0.09151847\n",
      "Iteration 494, loss = 0.09139390\n",
      "Iteration 495, loss = 0.09127048\n",
      "Iteration 496, loss = 0.09114824\n",
      "Iteration 497, loss = 0.09102716\n",
      "Iteration 498, loss = 0.09090723\n",
      "Iteration 499, loss = 0.09078846\n",
      "Iteration 500, loss = 0.09067084\n",
      "Iteration 501, loss = 0.09055428\n",
      "Iteration 502, loss = 0.09043884\n",
      "Iteration 503, loss = 0.09032452\n",
      "Iteration 504, loss = 0.09021128\n",
      "Iteration 505, loss = 0.09009914\n",
      "Iteration 506, loss = 0.08998806\n",
      "Iteration 507, loss = 0.08987801\n",
      "Iteration 508, loss = 0.08976899\n",
      "Iteration 509, loss = 0.08966103\n",
      "Iteration 510, loss = 0.08955406\n",
      "Iteration 511, loss = 0.08944810\n",
      "Iteration 512, loss = 0.08934313\n",
      "Iteration 513, loss = 0.08923915\n",
      "Iteration 514, loss = 0.08913615\n",
      "Iteration 515, loss = 0.08903412\n",
      "Iteration 516, loss = 0.08893305\n",
      "Iteration 517, loss = 0.08883296\n",
      "Iteration 518, loss = 0.08873386\n",
      "Iteration 519, loss = 0.08863577\n",
      "Iteration 520, loss = 0.08853865\n",
      "Iteration 521, loss = 0.08844248\n",
      "Iteration 522, loss = 0.08834725\n",
      "Iteration 523, loss = 0.08825301\n",
      "Iteration 524, loss = 0.08815982\n",
      "Iteration 525, loss = 0.08806763\n",
      "Iteration 526, loss = 0.08797646\n",
      "Iteration 527, loss = 0.08788636\n",
      "Iteration 528, loss = 0.08779756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35233713\n",
      "Iteration 2, loss = 1.30676601\n",
      "Iteration 3, loss = 1.26399856\n",
      "Iteration 4, loss = 1.22395589\n",
      "Iteration 5, loss = 1.18632980\n",
      "Iteration 6, loss = 1.15085494\n",
      "Iteration 7, loss = 1.11725067\n",
      "Iteration 8, loss = 1.08524952\n",
      "Iteration 9, loss = 1.05477689\n",
      "Iteration 10, loss = 1.02595185\n",
      "Iteration 11, loss = 0.99872904\n",
      "Iteration 12, loss = 0.97326594\n",
      "Iteration 13, loss = 0.94975436\n",
      "Iteration 14, loss = 0.92826947\n",
      "Iteration 15, loss = 0.90888230\n",
      "Iteration 16, loss = 0.89171768\n",
      "Iteration 17, loss = 0.87656530\n",
      "Iteration 18, loss = 0.86334519\n",
      "Iteration 19, loss = 0.85196471\n",
      "Iteration 20, loss = 0.84206148\n",
      "Iteration 21, loss = 0.83308979\n",
      "Iteration 22, loss = 0.82477083\n",
      "Iteration 23, loss = 0.81675776\n",
      "Iteration 24, loss = 0.80881921\n",
      "Iteration 25, loss = 0.80075368\n",
      "Iteration 26, loss = 0.79241318\n",
      "Iteration 27, loss = 0.78370689\n",
      "Iteration 28, loss = 0.77463711\n",
      "Iteration 29, loss = 0.76522848\n",
      "Iteration 30, loss = 0.75563552\n",
      "Iteration 31, loss = 0.74593607\n",
      "Iteration 32, loss = 0.73617616\n",
      "Iteration 33, loss = 0.72645563\n",
      "Iteration 34, loss = 0.71695382\n",
      "Iteration 35, loss = 0.70756304\n",
      "Iteration 36, loss = 0.69842649\n",
      "Iteration 37, loss = 0.68955280\n",
      "Iteration 38, loss = 0.68105715\n",
      "Iteration 39, loss = 0.67296879\n",
      "Iteration 40, loss = 0.66536285\n",
      "Iteration 41, loss = 0.65820073\n",
      "Iteration 42, loss = 0.65136997\n",
      "Iteration 43, loss = 0.64482230\n",
      "Iteration 44, loss = 0.63856122\n",
      "Iteration 45, loss = 0.63253910\n",
      "Iteration 46, loss = 0.62679878\n",
      "Iteration 47, loss = 0.62126967\n",
      "Iteration 48, loss = 0.61601839\n",
      "Iteration 49, loss = 0.61096702\n",
      "Iteration 50, loss = 0.60606956\n",
      "Iteration 51, loss = 0.60127616\n",
      "Iteration 52, loss = 0.59657961\n",
      "Iteration 53, loss = 0.59196587\n",
      "Iteration 54, loss = 0.58741122\n",
      "Iteration 55, loss = 0.58288890\n",
      "Iteration 56, loss = 0.57840709\n",
      "Iteration 57, loss = 0.57396881\n",
      "Iteration 58, loss = 0.56957022\n",
      "Iteration 59, loss = 0.56521817\n",
      "Iteration 60, loss = 0.56091620\n",
      "Iteration 61, loss = 0.55666491\n",
      "Iteration 62, loss = 0.55248106\n",
      "Iteration 63, loss = 0.54836976\n",
      "Iteration 64, loss = 0.54432886\n",
      "Iteration 65, loss = 0.54036091\n",
      "Iteration 66, loss = 0.53644710\n",
      "Iteration 67, loss = 0.53258763\n",
      "Iteration 68, loss = 0.52878246\n",
      "Iteration 69, loss = 0.52501206\n",
      "Iteration 70, loss = 0.52128254\n",
      "Iteration 71, loss = 0.51759234\n",
      "Iteration 72, loss = 0.51394366\n",
      "Iteration 73, loss = 0.51034316\n",
      "Iteration 74, loss = 0.50678142\n",
      "Iteration 75, loss = 0.50326217\n",
      "Iteration 76, loss = 0.49978700\n",
      "Iteration 77, loss = 0.49639258\n",
      "Iteration 78, loss = 0.49307915\n",
      "Iteration 79, loss = 0.48983905\n",
      "Iteration 80, loss = 0.48667851\n",
      "Iteration 81, loss = 0.48356608\n",
      "Iteration 82, loss = 0.48049941\n",
      "Iteration 83, loss = 0.47747275\n",
      "Iteration 84, loss = 0.47448074\n",
      "Iteration 85, loss = 0.47152705\n",
      "Iteration 86, loss = 0.46861290\n",
      "Iteration 87, loss = 0.46573541\n",
      "Iteration 88, loss = 0.46289379\n",
      "Iteration 89, loss = 0.46008964\n",
      "Iteration 90, loss = 0.45732225\n",
      "Iteration 91, loss = 0.45458981\n",
      "Iteration 92, loss = 0.45189075\n",
      "Iteration 93, loss = 0.44922209\n",
      "Iteration 94, loss = 0.44658671\n",
      "Iteration 95, loss = 0.44398465\n",
      "Iteration 96, loss = 0.44141166\n",
      "Iteration 97, loss = 0.43886792\n",
      "Iteration 98, loss = 0.43635266\n",
      "Iteration 99, loss = 0.43386406\n",
      "Iteration 100, loss = 0.43140195\n",
      "Iteration 101, loss = 0.42896785\n",
      "Iteration 102, loss = 0.42655935\n",
      "Iteration 103, loss = 0.42417533\n",
      "Iteration 104, loss = 0.42181556\n",
      "Iteration 105, loss = 0.41948085\n",
      "Iteration 106, loss = 0.41716959\n",
      "Iteration 107, loss = 0.41488191\n",
      "Iteration 108, loss = 0.41261570\n",
      "Iteration 109, loss = 0.41037075\n",
      "Iteration 110, loss = 0.40814591\n",
      "Iteration 111, loss = 0.40594112\n",
      "Iteration 112, loss = 0.40375554\n",
      "Iteration 113, loss = 0.40158900\n",
      "Iteration 114, loss = 0.39944090\n",
      "Iteration 115, loss = 0.39731114\n",
      "Iteration 116, loss = 0.39519916\n",
      "Iteration 117, loss = 0.39310499\n",
      "Iteration 118, loss = 0.39102899\n",
      "Iteration 119, loss = 0.38896946\n",
      "Iteration 120, loss = 0.38692638\n",
      "Iteration 121, loss = 0.38489872\n",
      "Iteration 122, loss = 0.38288558\n",
      "Iteration 123, loss = 0.38088734\n",
      "Iteration 124, loss = 0.37890350\n",
      "Iteration 125, loss = 0.37693375\n",
      "Iteration 126, loss = 0.37497778\n",
      "Iteration 127, loss = 0.37303398\n",
      "Iteration 128, loss = 0.37110315\n",
      "Iteration 129, loss = 0.36918491\n",
      "Iteration 130, loss = 0.36727929\n",
      "Iteration 131, loss = 0.36538555\n",
      "Iteration 132, loss = 0.36350356\n",
      "Iteration 133, loss = 0.36163312\n",
      "Iteration 134, loss = 0.35977396\n",
      "Iteration 135, loss = 0.35792606\n",
      "Iteration 136, loss = 0.35608886\n",
      "Iteration 137, loss = 0.35426199\n",
      "Iteration 138, loss = 0.35244544\n",
      "Iteration 139, loss = 0.35063901\n",
      "Iteration 140, loss = 0.34884240\n",
      "Iteration 141, loss = 0.34705582\n",
      "Iteration 142, loss = 0.34527869\n",
      "Iteration 143, loss = 0.34351100\n",
      "Iteration 144, loss = 0.34175309\n",
      "Iteration 145, loss = 0.34000463\n",
      "Iteration 146, loss = 0.33826516\n",
      "Iteration 147, loss = 0.33653427\n",
      "Iteration 148, loss = 0.33481184\n",
      "Iteration 149, loss = 0.33309849\n",
      "Iteration 150, loss = 0.33139374\n",
      "Iteration 151, loss = 0.32969711\n",
      "Iteration 152, loss = 0.32800910\n",
      "Iteration 153, loss = 0.32632932\n",
      "Iteration 154, loss = 0.32465787\n",
      "Iteration 155, loss = 0.32299719\n",
      "Iteration 156, loss = 0.32134644\n",
      "Iteration 157, loss = 0.31970400\n",
      "Iteration 158, loss = 0.31807061\n",
      "Iteration 159, loss = 0.31644495\n",
      "Iteration 160, loss = 0.31482747\n",
      "Iteration 161, loss = 0.31321877\n",
      "Iteration 162, loss = 0.31161807\n",
      "Iteration 163, loss = 0.31002572\n",
      "Iteration 164, loss = 0.30844110\n",
      "Iteration 165, loss = 0.30686439\n",
      "Iteration 166, loss = 0.30529656\n",
      "Iteration 167, loss = 0.30373681\n",
      "Iteration 168, loss = 0.30218504\n",
      "Iteration 169, loss = 0.30064107\n",
      "Iteration 170, loss = 0.29910531\n",
      "Iteration 171, loss = 0.29757737\n",
      "Iteration 172, loss = 0.29605739\n",
      "Iteration 173, loss = 0.29454503\n",
      "Iteration 174, loss = 0.29304079\n",
      "Iteration 175, loss = 0.29154440\n",
      "Iteration 176, loss = 0.29005601\n",
      "Iteration 177, loss = 0.28857592\n",
      "Iteration 178, loss = 0.28710621\n",
      "Iteration 179, loss = 0.28564571\n",
      "Iteration 180, loss = 0.28419359\n",
      "Iteration 181, loss = 0.28274959\n",
      "Iteration 182, loss = 0.28131314\n",
      "Iteration 183, loss = 0.27988505\n",
      "Iteration 184, loss = 0.27846823\n",
      "Iteration 185, loss = 0.27705854\n",
      "Iteration 186, loss = 0.27565183\n",
      "Iteration 187, loss = 0.27424659\n",
      "Iteration 188, loss = 0.27284227\n",
      "Iteration 189, loss = 0.27143682\n",
      "Iteration 190, loss = 0.27003290\n",
      "Iteration 191, loss = 0.26865057\n",
      "Iteration 192, loss = 0.26728324\n",
      "Iteration 193, loss = 0.26592002\n",
      "Iteration 194, loss = 0.26456291\n",
      "Iteration 195, loss = 0.26320906\n",
      "Iteration 196, loss = 0.26185738\n",
      "Iteration 197, loss = 0.26050751\n",
      "Iteration 198, loss = 0.25916134\n",
      "Iteration 199, loss = 0.25781587\n",
      "Iteration 200, loss = 0.25647175\n",
      "Iteration 201, loss = 0.25512939\n",
      "Iteration 202, loss = 0.25378779\n",
      "Iteration 203, loss = 0.25244692\n",
      "Iteration 204, loss = 0.25110715\n",
      "Iteration 205, loss = 0.24976955\n",
      "Iteration 206, loss = 0.24843489\n",
      "Iteration 207, loss = 0.24710212\n",
      "Iteration 208, loss = 0.24577151\n",
      "Iteration 209, loss = 0.24444358\n",
      "Iteration 210, loss = 0.24311989\n",
      "Iteration 211, loss = 0.24179894\n",
      "Iteration 212, loss = 0.24048110\n",
      "Iteration 213, loss = 0.23916605\n",
      "Iteration 214, loss = 0.23785426\n",
      "Iteration 215, loss = 0.23654630\n",
      "Iteration 216, loss = 0.23524205\n",
      "Iteration 217, loss = 0.23394170\n",
      "Iteration 218, loss = 0.23264580\n",
      "Iteration 219, loss = 0.23135447\n",
      "Iteration 220, loss = 0.23006796\n",
      "Iteration 221, loss = 0.22878645\n",
      "Iteration 222, loss = 0.22751014\n",
      "Iteration 223, loss = 0.22623925\n",
      "Iteration 224, loss = 0.22497403\n",
      "Iteration 225, loss = 0.22371470\n",
      "Iteration 226, loss = 0.22246176\n",
      "Iteration 227, loss = 0.22121561\n",
      "Iteration 228, loss = 0.21997605\n",
      "Iteration 229, loss = 0.21874313\n",
      "Iteration 230, loss = 0.21751717\n",
      "Iteration 231, loss = 0.21629837\n",
      "Iteration 232, loss = 0.21508695\n",
      "Iteration 233, loss = 0.21388329\n",
      "Iteration 234, loss = 0.21268722\n",
      "Iteration 235, loss = 0.21149885\n",
      "Iteration 236, loss = 0.21031951\n",
      "Iteration 237, loss = 0.20914762\n",
      "Iteration 238, loss = 0.20798342\n",
      "Iteration 239, loss = 0.20682814\n",
      "Iteration 240, loss = 0.20568144\n",
      "Iteration 241, loss = 0.20454330\n",
      "Iteration 242, loss = 0.20341377\n",
      "Iteration 243, loss = 0.20229306\n",
      "Iteration 244, loss = 0.20118129\n",
      "Iteration 245, loss = 0.20007822\n",
      "Iteration 246, loss = 0.19898394\n",
      "Iteration 247, loss = 0.19789859\n",
      "Iteration 248, loss = 0.19682214\n",
      "Iteration 249, loss = 0.19575462\n",
      "Iteration 250, loss = 0.19469715\n",
      "Iteration 251, loss = 0.19364939\n",
      "Iteration 252, loss = 0.19261075\n",
      "Iteration 253, loss = 0.19158123\n",
      "Iteration 254, loss = 0.19056092\n",
      "Iteration 255, loss = 0.18955001\n",
      "Iteration 256, loss = 0.18854829\n",
      "Iteration 257, loss = 0.18755569\n",
      "Iteration 258, loss = 0.18657245\n",
      "Iteration 259, loss = 0.18559907\n",
      "Iteration 260, loss = 0.18463485\n",
      "Iteration 261, loss = 0.18367975\n",
      "Iteration 262, loss = 0.18273356\n",
      "Iteration 263, loss = 0.18179629\n",
      "Iteration 264, loss = 0.18086797\n",
      "Iteration 265, loss = 0.17994859\n",
      "Iteration 266, loss = 0.17903837\n",
      "Iteration 267, loss = 0.17813696\n",
      "Iteration 268, loss = 0.17724456\n",
      "Iteration 269, loss = 0.17636123\n",
      "Iteration 270, loss = 0.17548690\n",
      "Iteration 271, loss = 0.17462122\n",
      "Iteration 272, loss = 0.17376412\n",
      "Iteration 273, loss = 0.17291550\n",
      "Iteration 274, loss = 0.17207535\n",
      "Iteration 275, loss = 0.17124364\n",
      "Iteration 276, loss = 0.17042076\n",
      "Iteration 277, loss = 0.16960640\n",
      "Iteration 278, loss = 0.16880028\n",
      "Iteration 279, loss = 0.16800221\n",
      "Iteration 280, loss = 0.16721215\n",
      "Iteration 281, loss = 0.16643018\n",
      "Iteration 282, loss = 0.16565614\n",
      "Iteration 283, loss = 0.16489003\n",
      "Iteration 284, loss = 0.16413165\n",
      "Iteration 285, loss = 0.16338097\n",
      "Iteration 286, loss = 0.16263793\n",
      "Iteration 287, loss = 0.16190325\n",
      "Iteration 288, loss = 0.16117578\n",
      "Iteration 289, loss = 0.16045562\n",
      "Iteration 290, loss = 0.15974290\n",
      "Iteration 291, loss = 0.15903783\n",
      "Iteration 292, loss = 0.15833985\n",
      "Iteration 293, loss = 0.15764906\n",
      "Iteration 294, loss = 0.15696543\n",
      "Iteration 295, loss = 0.15628893\n",
      "Iteration 296, loss = 0.15561959\n",
      "Iteration 297, loss = 0.15495712\n",
      "Iteration 298, loss = 0.15430140\n",
      "Iteration 299, loss = 0.15365227\n",
      "Iteration 300, loss = 0.15300989\n",
      "Iteration 301, loss = 0.15237426\n",
      "Iteration 302, loss = 0.15174493\n",
      "Iteration 303, loss = 0.15112219\n",
      "Iteration 304, loss = 0.15050580\n",
      "Iteration 305, loss = 0.14989580\n",
      "Iteration 306, loss = 0.14929227\n",
      "Iteration 307, loss = 0.14869494\n",
      "Iteration 308, loss = 0.14810342\n",
      "Iteration 309, loss = 0.14751825\n",
      "Iteration 310, loss = 0.14693901\n",
      "Iteration 311, loss = 0.14636615\n",
      "Iteration 312, loss = 0.14579830\n",
      "Iteration 313, loss = 0.14523671\n",
      "Iteration 314, loss = 0.14468062\n",
      "Iteration 315, loss = 0.14412999\n",
      "Iteration 316, loss = 0.14358446\n",
      "Iteration 317, loss = 0.14304458\n",
      "Iteration 318, loss = 0.14250889\n",
      "Iteration 319, loss = 0.14197816\n",
      "Iteration 320, loss = 0.14145266\n",
      "Iteration 321, loss = 0.14093050\n",
      "Iteration 322, loss = 0.14041213\n",
      "Iteration 323, loss = 0.13989756\n",
      "Iteration 324, loss = 0.13938657\n",
      "Iteration 325, loss = 0.13887843\n",
      "Iteration 326, loss = 0.13837330\n",
      "Iteration 327, loss = 0.13787047\n",
      "Iteration 328, loss = 0.13736980\n",
      "Iteration 329, loss = 0.13687197\n",
      "Iteration 330, loss = 0.13637585\n",
      "Iteration 331, loss = 0.13588162\n",
      "Iteration 332, loss = 0.13538924\n",
      "Iteration 333, loss = 0.13489927\n",
      "Iteration 334, loss = 0.13441102\n",
      "Iteration 335, loss = 0.13392436\n",
      "Iteration 336, loss = 0.13343930\n",
      "Iteration 337, loss = 0.13295455\n",
      "Iteration 338, loss = 0.13247796\n",
      "Iteration 339, loss = 0.13199428\n",
      "Iteration 340, loss = 0.13151906\n",
      "Iteration 341, loss = 0.13104430\n",
      "Iteration 342, loss = 0.13057042\n",
      "Iteration 343, loss = 0.13009688\n",
      "Iteration 344, loss = 0.12962414\n",
      "Iteration 345, loss = 0.12915985\n",
      "Iteration 346, loss = 0.12868962\n",
      "Iteration 347, loss = 0.12822615\n",
      "Iteration 348, loss = 0.12776613\n",
      "Iteration 349, loss = 0.12730615\n",
      "Iteration 350, loss = 0.12684701\n",
      "Iteration 351, loss = 0.12638875\n",
      "Iteration 352, loss = 0.12593277\n",
      "Iteration 353, loss = 0.12548151\n",
      "Iteration 354, loss = 0.12502727\n",
      "Iteration 355, loss = 0.12457684\n",
      "Iteration 356, loss = 0.12412800\n",
      "Iteration 357, loss = 0.12367885\n",
      "Iteration 358, loss = 0.12322928\n",
      "Iteration 359, loss = 0.12278342\n",
      "Iteration 360, loss = 0.12233549\n",
      "Iteration 361, loss = 0.12189081\n",
      "Iteration 362, loss = 0.12144516\n",
      "Iteration 363, loss = 0.12100055\n",
      "Iteration 364, loss = 0.12055617\n",
      "Iteration 365, loss = 0.12011296\n",
      "Iteration 366, loss = 0.11966834\n",
      "Iteration 367, loss = 0.11922561\n",
      "Iteration 368, loss = 0.11878364\n",
      "Iteration 369, loss = 0.11834227\n",
      "Iteration 370, loss = 0.11790037\n",
      "Iteration 371, loss = 0.11746033\n",
      "Iteration 372, loss = 0.11702095\n",
      "Iteration 373, loss = 0.11658405\n",
      "Iteration 374, loss = 0.11614784\n",
      "Iteration 375, loss = 0.11571383\n",
      "Iteration 376, loss = 0.11528036\n",
      "Iteration 377, loss = 0.11484967\n",
      "Iteration 378, loss = 0.11442156\n",
      "Iteration 379, loss = 0.11399508\n",
      "Iteration 380, loss = 0.11357124\n",
      "Iteration 381, loss = 0.11315054\n",
      "Iteration 382, loss = 0.11273270\n",
      "Iteration 383, loss = 0.11231714\n",
      "Iteration 384, loss = 0.11190420\n",
      "Iteration 385, loss = 0.11149470\n",
      "Iteration 386, loss = 0.11108862\n",
      "Iteration 387, loss = 0.11068542\n",
      "Iteration 388, loss = 0.11028580\n",
      "Iteration 389, loss = 0.10989005\n",
      "Iteration 390, loss = 0.10949769\n",
      "Iteration 391, loss = 0.10910921\n",
      "Iteration 392, loss = 0.10872456\n",
      "Iteration 393, loss = 0.10834400\n",
      "Iteration 394, loss = 0.10796771\n",
      "Iteration 395, loss = 0.10759524\n",
      "Iteration 396, loss = 0.10722702\n",
      "Iteration 397, loss = 0.10686297\n",
      "Iteration 398, loss = 0.10650309\n",
      "Iteration 399, loss = 0.10614798\n",
      "Iteration 400, loss = 0.10579710\n",
      "Iteration 401, loss = 0.10545063\n",
      "Iteration 402, loss = 0.10510866\n",
      "Iteration 403, loss = 0.10477130\n",
      "Iteration 404, loss = 0.10443818\n",
      "Iteration 405, loss = 0.10410968\n",
      "Iteration 406, loss = 0.10378590\n",
      "Iteration 407, loss = 0.10346661\n",
      "Iteration 408, loss = 0.10315166\n",
      "Iteration 409, loss = 0.10284154\n",
      "Iteration 410, loss = 0.10253582\n",
      "Iteration 411, loss = 0.10223448\n",
      "Iteration 412, loss = 0.10193729\n",
      "Iteration 413, loss = 0.10164450\n",
      "Iteration 414, loss = 0.10135598\n",
      "Iteration 415, loss = 0.10107153\n",
      "Iteration 416, loss = 0.10079114\n",
      "Iteration 417, loss = 0.10051480\n",
      "Iteration 418, loss = 0.10024230\n",
      "Iteration 419, loss = 0.09997356\n",
      "Iteration 420, loss = 0.09970846\n",
      "Iteration 421, loss = 0.09944691\n",
      "Iteration 422, loss = 0.09918904\n",
      "Iteration 423, loss = 0.09893439\n",
      "Iteration 424, loss = 0.09868293\n",
      "Iteration 425, loss = 0.09843458\n",
      "Iteration 426, loss = 0.09818944\n",
      "Iteration 427, loss = 0.09794727\n",
      "Iteration 428, loss = 0.09770805\n",
      "Iteration 429, loss = 0.09747163\n",
      "Iteration 430, loss = 0.09723794\n",
      "Iteration 431, loss = 0.09700699\n",
      "Iteration 432, loss = 0.09677867\n",
      "Iteration 433, loss = 0.09655299\n",
      "Iteration 434, loss = 0.09632985\n",
      "Iteration 435, loss = 0.09610931\n",
      "Iteration 436, loss = 0.09589127\n",
      "Iteration 437, loss = 0.09567569\n",
      "Iteration 438, loss = 0.09546252\n",
      "Iteration 439, loss = 0.09525175\n",
      "Iteration 440, loss = 0.09504332\n",
      "Iteration 441, loss = 0.09483729\n",
      "Iteration 442, loss = 0.09463367\n",
      "Iteration 443, loss = 0.09443226\n",
      "Iteration 444, loss = 0.09423322\n",
      "Iteration 445, loss = 0.09403641\n",
      "Iteration 446, loss = 0.09384182\n",
      "Iteration 447, loss = 0.09364951\n",
      "Iteration 448, loss = 0.09345942\n",
      "Iteration 449, loss = 0.09327170\n",
      "Iteration 450, loss = 0.09308627\n",
      "Iteration 451, loss = 0.09290295\n",
      "Iteration 452, loss = 0.09272184\n",
      "Iteration 453, loss = 0.09254270\n",
      "Iteration 454, loss = 0.09236573\n",
      "Iteration 455, loss = 0.09219090\n",
      "Iteration 456, loss = 0.09201808\n",
      "Iteration 457, loss = 0.09184727\n",
      "Iteration 458, loss = 0.09167845\n",
      "Iteration 459, loss = 0.09151161\n",
      "Iteration 460, loss = 0.09134677\n",
      "Iteration 461, loss = 0.09118391\n",
      "Iteration 462, loss = 0.09102297\n",
      "Iteration 463, loss = 0.09086392\n",
      "Iteration 464, loss = 0.09070676\n",
      "Iteration 465, loss = 0.09055148\n",
      "Iteration 466, loss = 0.09039816\n",
      "Iteration 467, loss = 0.09024673\n",
      "Iteration 468, loss = 0.09009714\n",
      "Iteration 469, loss = 0.08994923\n",
      "Iteration 470, loss = 0.08980305\n",
      "Iteration 471, loss = 0.08965860\n",
      "Iteration 472, loss = 0.08951587\n",
      "Iteration 473, loss = 0.08937479\n",
      "Iteration 474, loss = 0.08923535\n",
      "Iteration 475, loss = 0.08909747\n",
      "Iteration 476, loss = 0.08896115\n",
      "Iteration 477, loss = 0.08882646\n",
      "Iteration 478, loss = 0.08869337\n",
      "Iteration 479, loss = 0.08856168\n",
      "Iteration 480, loss = 0.08843145\n",
      "Iteration 481, loss = 0.08830263\n",
      "Iteration 482, loss = 0.08817515\n",
      "Iteration 483, loss = 0.08804897\n",
      "Iteration 484, loss = 0.08792408\n",
      "Iteration 485, loss = 0.08780043\n",
      "Iteration 486, loss = 0.08767798\n",
      "Iteration 487, loss = 0.08755670\n",
      "Iteration 488, loss = 0.08743656\n",
      "Iteration 489, loss = 0.08731751\n",
      "Iteration 490, loss = 0.08719955\n",
      "Iteration 491, loss = 0.08708262\n",
      "Iteration 492, loss = 0.08696675\n",
      "Iteration 493, loss = 0.08685190\n",
      "Iteration 494, loss = 0.08673792\n",
      "Iteration 495, loss = 0.08662496\n",
      "Iteration 496, loss = 0.08651296\n",
      "Iteration 497, loss = 0.08640189\n",
      "Iteration 498, loss = 0.08629174\n",
      "Iteration 499, loss = 0.08618252\n",
      "Iteration 500, loss = 0.08607419\n",
      "Iteration 501, loss = 0.08596677\n",
      "Iteration 502, loss = 0.08586024\n",
      "Iteration 503, loss = 0.08575459\n",
      "Iteration 504, loss = 0.08564983\n",
      "Iteration 505, loss = 0.08554594\n",
      "Iteration 506, loss = 0.08544299\n",
      "Iteration 507, loss = 0.08534101\n",
      "Iteration 508, loss = 0.08523991\n",
      "Iteration 509, loss = 0.08513972\n",
      "Iteration 510, loss = 0.08504051\n",
      "Iteration 511, loss = 0.08494231\n",
      "Iteration 512, loss = 0.08484494\n",
      "Iteration 513, loss = 0.08474840\n",
      "Iteration 514, loss = 0.08465265\n",
      "Iteration 515, loss = 0.08455773\n",
      "Iteration 516, loss = 0.08446359\n",
      "Iteration 517, loss = 0.08437036\n",
      "Iteration 518, loss = 0.08427798\n",
      "Iteration 519, loss = 0.08418651\n",
      "Iteration 520, loss = 0.08409593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34884587\n",
      "Iteration 2, loss = 1.30252841\n",
      "Iteration 3, loss = 1.25913051\n",
      "Iteration 4, loss = 1.21849216\n",
      "Iteration 5, loss = 1.18031501\n",
      "Iteration 6, loss = 1.14435427\n",
      "Iteration 7, loss = 1.11032530\n",
      "Iteration 8, loss = 1.07801223\n",
      "Iteration 9, loss = 1.04723159\n",
      "Iteration 10, loss = 1.01806192\n",
      "Iteration 11, loss = 0.99059355\n",
      "Iteration 12, loss = 0.96492362\n",
      "Iteration 13, loss = 0.94122341\n",
      "Iteration 14, loss = 0.91954831\n",
      "Iteration 15, loss = 0.89992001\n",
      "Iteration 16, loss = 0.88248265\n",
      "Iteration 17, loss = 0.86703508\n",
      "Iteration 18, loss = 0.85353556\n",
      "Iteration 19, loss = 0.84184803\n",
      "Iteration 20, loss = 0.83165042\n",
      "Iteration 21, loss = 0.82243328\n",
      "Iteration 22, loss = 0.81388723\n",
      "Iteration 23, loss = 0.80571307\n",
      "Iteration 24, loss = 0.79765053\n",
      "Iteration 25, loss = 0.78949063\n",
      "Iteration 26, loss = 0.78110523\n",
      "Iteration 27, loss = 0.77237712\n",
      "Iteration 28, loss = 0.76332802\n",
      "Iteration 29, loss = 0.75398122\n",
      "Iteration 30, loss = 0.74441825\n",
      "Iteration 31, loss = 0.73468724\n",
      "Iteration 32, loss = 0.72489665\n",
      "Iteration 33, loss = 0.71515208\n",
      "Iteration 34, loss = 0.70556293\n",
      "Iteration 35, loss = 0.69610783\n",
      "Iteration 36, loss = 0.68690381\n",
      "Iteration 37, loss = 0.67802132\n",
      "Iteration 38, loss = 0.66956176\n",
      "Iteration 39, loss = 0.66154306\n",
      "Iteration 40, loss = 0.65392145\n",
      "Iteration 41, loss = 0.64666920\n",
      "Iteration 42, loss = 0.63972107\n",
      "Iteration 43, loss = 0.63312086\n",
      "Iteration 44, loss = 0.62679133\n",
      "Iteration 45, loss = 0.62075550\n",
      "Iteration 46, loss = 0.61495047\n",
      "Iteration 47, loss = 0.60936132\n",
      "Iteration 48, loss = 0.60398673\n",
      "Iteration 49, loss = 0.59883500\n",
      "Iteration 50, loss = 0.59383436\n",
      "Iteration 51, loss = 0.58900345\n",
      "Iteration 52, loss = 0.58427357\n",
      "Iteration 53, loss = 0.57964998\n",
      "Iteration 54, loss = 0.57510713\n",
      "Iteration 55, loss = 0.57061977\n",
      "Iteration 56, loss = 0.56617616\n",
      "Iteration 57, loss = 0.56176999\n",
      "Iteration 58, loss = 0.55740204\n",
      "Iteration 59, loss = 0.55307635\n",
      "Iteration 60, loss = 0.54879832\n",
      "Iteration 61, loss = 0.54457189\n",
      "Iteration 62, loss = 0.54040482\n",
      "Iteration 63, loss = 0.53629528\n",
      "Iteration 64, loss = 0.53225133\n",
      "Iteration 65, loss = 0.52829029\n",
      "Iteration 66, loss = 0.52440361\n",
      "Iteration 67, loss = 0.52059331\n",
      "Iteration 68, loss = 0.51685643\n",
      "Iteration 69, loss = 0.51318801\n",
      "Iteration 70, loss = 0.50957961\n",
      "Iteration 71, loss = 0.50603632\n",
      "Iteration 72, loss = 0.50255533\n",
      "Iteration 73, loss = 0.49912982\n",
      "Iteration 74, loss = 0.49575617\n",
      "Iteration 75, loss = 0.49242851\n",
      "Iteration 76, loss = 0.48914468\n",
      "Iteration 77, loss = 0.48589536\n",
      "Iteration 78, loss = 0.48267061\n",
      "Iteration 79, loss = 0.47947105\n",
      "Iteration 80, loss = 0.47627711\n",
      "Iteration 81, loss = 0.47310022\n",
      "Iteration 82, loss = 0.46994526\n",
      "Iteration 83, loss = 0.46681353\n",
      "Iteration 84, loss = 0.46370586\n",
      "Iteration 85, loss = 0.46062365\n",
      "Iteration 86, loss = 0.45757837\n",
      "Iteration 87, loss = 0.45460017\n",
      "Iteration 88, loss = 0.45170182\n",
      "Iteration 89, loss = 0.44887341\n",
      "Iteration 90, loss = 0.44609812\n",
      "Iteration 91, loss = 0.44334870\n",
      "Iteration 92, loss = 0.44063105\n",
      "Iteration 93, loss = 0.43794098\n",
      "Iteration 94, loss = 0.43527349\n",
      "Iteration 95, loss = 0.43263065\n",
      "Iteration 96, loss = 0.43001171\n",
      "Iteration 97, loss = 0.42741664\n",
      "Iteration 98, loss = 0.42485047\n",
      "Iteration 99, loss = 0.42230896\n",
      "Iteration 100, loss = 0.41979173\n",
      "Iteration 101, loss = 0.41729727\n",
      "Iteration 102, loss = 0.41482637\n",
      "Iteration 103, loss = 0.41238066\n",
      "Iteration 104, loss = 0.40995868\n",
      "Iteration 105, loss = 0.40755939\n",
      "Iteration 106, loss = 0.40518175\n",
      "Iteration 107, loss = 0.40282545\n",
      "Iteration 108, loss = 0.40049103\n",
      "Iteration 109, loss = 0.39817713\n",
      "Iteration 110, loss = 0.39588176\n",
      "Iteration 111, loss = 0.39360544\n",
      "Iteration 112, loss = 0.39134692\n",
      "Iteration 113, loss = 0.38910580\n",
      "Iteration 114, loss = 0.38688196\n",
      "Iteration 115, loss = 0.38467496\n",
      "Iteration 116, loss = 0.38248465\n",
      "Iteration 117, loss = 0.38031044\n",
      "Iteration 118, loss = 0.37815262\n",
      "Iteration 119, loss = 0.37601107\n",
      "Iteration 120, loss = 0.37388594\n",
      "Iteration 121, loss = 0.37177607\n",
      "Iteration 122, loss = 0.36967998\n",
      "Iteration 123, loss = 0.36759844\n",
      "Iteration 124, loss = 0.36553001\n",
      "Iteration 125, loss = 0.36347516\n",
      "Iteration 126, loss = 0.36143259\n",
      "Iteration 127, loss = 0.35940326\n",
      "Iteration 128, loss = 0.35738640\n",
      "Iteration 129, loss = 0.35538156\n",
      "Iteration 130, loss = 0.35338853\n",
      "Iteration 131, loss = 0.35140744\n",
      "Iteration 132, loss = 0.34943795\n",
      "Iteration 133, loss = 0.34747928\n",
      "Iteration 134, loss = 0.34553125\n",
      "Iteration 135, loss = 0.34359363\n",
      "Iteration 136, loss = 0.34166656\n",
      "Iteration 137, loss = 0.33974971\n",
      "Iteration 138, loss = 0.33784356\n",
      "Iteration 139, loss = 0.33594754\n",
      "Iteration 140, loss = 0.33406313\n",
      "Iteration 141, loss = 0.33219095\n",
      "Iteration 142, loss = 0.33032846\n",
      "Iteration 143, loss = 0.32847575\n",
      "Iteration 144, loss = 0.32663312\n",
      "Iteration 145, loss = 0.32480020\n",
      "Iteration 146, loss = 0.32297667\n",
      "Iteration 147, loss = 0.32116271\n",
      "Iteration 148, loss = 0.31935832\n",
      "Iteration 149, loss = 0.31756344\n",
      "Iteration 150, loss = 0.31577749\n",
      "Iteration 151, loss = 0.31400165\n",
      "Iteration 152, loss = 0.31223911\n",
      "Iteration 153, loss = 0.31048573\n",
      "Iteration 154, loss = 0.30874161\n",
      "Iteration 155, loss = 0.30700683\n",
      "Iteration 156, loss = 0.30528162\n",
      "Iteration 157, loss = 0.30356546\n",
      "Iteration 158, loss = 0.30185855\n",
      "Iteration 159, loss = 0.30016084\n",
      "Iteration 160, loss = 0.29847255\n",
      "Iteration 161, loss = 0.29679332\n",
      "Iteration 162, loss = 0.29512302\n",
      "Iteration 163, loss = 0.29346259\n",
      "Iteration 164, loss = 0.29181097\n",
      "Iteration 165, loss = 0.29016836\n",
      "Iteration 166, loss = 0.28853437\n",
      "Iteration 167, loss = 0.28690946\n",
      "Iteration 168, loss = 0.28529412\n",
      "Iteration 169, loss = 0.28368786\n",
      "Iteration 170, loss = 0.28209069\n",
      "Iteration 171, loss = 0.28050269\n",
      "Iteration 172, loss = 0.27892365\n",
      "Iteration 173, loss = 0.27735322\n",
      "Iteration 174, loss = 0.27579282\n",
      "Iteration 175, loss = 0.27424365\n",
      "Iteration 176, loss = 0.27270345\n",
      "Iteration 177, loss = 0.27117219\n",
      "Iteration 178, loss = 0.26964996\n",
      "Iteration 179, loss = 0.26813711\n",
      "Iteration 180, loss = 0.26663238\n",
      "Iteration 181, loss = 0.26513651\n",
      "Iteration 182, loss = 0.26364975\n",
      "Iteration 183, loss = 0.26217433\n",
      "Iteration 184, loss = 0.26070846\n",
      "Iteration 185, loss = 0.25924860\n",
      "Iteration 186, loss = 0.25779458\n",
      "Iteration 187, loss = 0.25634546\n",
      "Iteration 188, loss = 0.25489375\n",
      "Iteration 189, loss = 0.25344302\n",
      "Iteration 190, loss = 0.25199674\n",
      "Iteration 191, loss = 0.25057364\n",
      "Iteration 192, loss = 0.24915920\n",
      "Iteration 193, loss = 0.24774816\n",
      "Iteration 194, loss = 0.24633992\n",
      "Iteration 195, loss = 0.24493393\n",
      "Iteration 196, loss = 0.24353166\n",
      "Iteration 197, loss = 0.24213149\n",
      "Iteration 198, loss = 0.24073421\n",
      "Iteration 199, loss = 0.23934002\n",
      "Iteration 200, loss = 0.23794765\n",
      "Iteration 201, loss = 0.23655708\n",
      "Iteration 202, loss = 0.23516835\n",
      "Iteration 203, loss = 0.23378150\n",
      "Iteration 204, loss = 0.23239671\n",
      "Iteration 205, loss = 0.23101430\n",
      "Iteration 206, loss = 0.22963413\n",
      "Iteration 207, loss = 0.22825630\n",
      "Iteration 208, loss = 0.22688115\n",
      "Iteration 209, loss = 0.22550905\n",
      "Iteration 210, loss = 0.22414004\n",
      "Iteration 211, loss = 0.22277441\n",
      "Iteration 212, loss = 0.22141391\n",
      "Iteration 213, loss = 0.22005833\n",
      "Iteration 214, loss = 0.21870730\n",
      "Iteration 215, loss = 0.21736072\n",
      "Iteration 216, loss = 0.21601883\n",
      "Iteration 217, loss = 0.21468227\n",
      "Iteration 218, loss = 0.21335103\n",
      "Iteration 219, loss = 0.21202535\n",
      "Iteration 220, loss = 0.21070544\n",
      "Iteration 221, loss = 0.20939167\n",
      "Iteration 222, loss = 0.20808416\n",
      "Iteration 223, loss = 0.20678324\n",
      "Iteration 224, loss = 0.20548912\n",
      "Iteration 225, loss = 0.20420199\n",
      "Iteration 226, loss = 0.20292223\n",
      "Iteration 227, loss = 0.20164983\n",
      "Iteration 228, loss = 0.20038499\n",
      "Iteration 229, loss = 0.19912852\n",
      "Iteration 230, loss = 0.19788081\n",
      "Iteration 231, loss = 0.19664044\n",
      "Iteration 232, loss = 0.19540779\n",
      "Iteration 233, loss = 0.19418417\n",
      "Iteration 234, loss = 0.19296914\n",
      "Iteration 235, loss = 0.19176286\n",
      "Iteration 236, loss = 0.19056571\n",
      "Iteration 237, loss = 0.18937745\n",
      "Iteration 238, loss = 0.18819817\n",
      "Iteration 239, loss = 0.18702894\n",
      "Iteration 240, loss = 0.18586917\n",
      "Iteration 241, loss = 0.18471883\n",
      "Iteration 242, loss = 0.18357792\n",
      "Iteration 243, loss = 0.18244635\n",
      "Iteration 244, loss = 0.18132436\n",
      "Iteration 245, loss = 0.18021212\n",
      "Iteration 246, loss = 0.17910949\n",
      "Iteration 247, loss = 0.17801632\n",
      "Iteration 248, loss = 0.17693305\n",
      "Iteration 249, loss = 0.17585984\n",
      "Iteration 250, loss = 0.17479725\n",
      "Iteration 251, loss = 0.17374386\n",
      "Iteration 252, loss = 0.17269983\n",
      "Iteration 253, loss = 0.17166538\n",
      "Iteration 254, loss = 0.17064092\n",
      "Iteration 255, loss = 0.16962591\n",
      "Iteration 256, loss = 0.16862093\n",
      "Iteration 257, loss = 0.16762552\n",
      "Iteration 258, loss = 0.16663950\n",
      "Iteration 259, loss = 0.16566322\n",
      "Iteration 260, loss = 0.16469638\n",
      "Iteration 261, loss = 0.16373903\n",
      "Iteration 262, loss = 0.16279094\n",
      "Iteration 263, loss = 0.16185273\n",
      "Iteration 264, loss = 0.16092440\n",
      "Iteration 265, loss = 0.16000496\n",
      "Iteration 266, loss = 0.15909454\n",
      "Iteration 267, loss = 0.15819373\n",
      "Iteration 268, loss = 0.15730195\n",
      "Iteration 269, loss = 0.15641913\n",
      "Iteration 270, loss = 0.15554537\n",
      "Iteration 271, loss = 0.15468070\n",
      "Iteration 272, loss = 0.15382567\n",
      "Iteration 273, loss = 0.15297909\n",
      "Iteration 274, loss = 0.15214107\n",
      "Iteration 275, loss = 0.15131134\n",
      "Iteration 276, loss = 0.15049081\n",
      "Iteration 277, loss = 0.14967869\n",
      "Iteration 278, loss = 0.14887468\n",
      "Iteration 279, loss = 0.14807879\n",
      "Iteration 280, loss = 0.14729152\n",
      "Iteration 281, loss = 0.14651222\n",
      "Iteration 282, loss = 0.14574085\n",
      "Iteration 283, loss = 0.14497753\n",
      "Iteration 284, loss = 0.14422225\n",
      "Iteration 285, loss = 0.14347493\n",
      "Iteration 286, loss = 0.14273570\n",
      "Iteration 287, loss = 0.14200424\n",
      "Iteration 288, loss = 0.14128050\n",
      "Iteration 289, loss = 0.14056452\n",
      "Iteration 290, loss = 0.13985651\n",
      "Iteration 291, loss = 0.13915546\n",
      "Iteration 292, loss = 0.13846230\n",
      "Iteration 293, loss = 0.13777641\n",
      "Iteration 294, loss = 0.13709761\n",
      "Iteration 295, loss = 0.13642578\n",
      "Iteration 296, loss = 0.13576115\n",
      "Iteration 297, loss = 0.13510382\n",
      "Iteration 298, loss = 0.13445326\n",
      "Iteration 299, loss = 0.13380948\n",
      "Iteration 300, loss = 0.13317234\n",
      "Iteration 301, loss = 0.13254190\n",
      "Iteration 302, loss = 0.13191814\n",
      "Iteration 303, loss = 0.13130092\n",
      "Iteration 304, loss = 0.13069003\n",
      "Iteration 305, loss = 0.13008567\n",
      "Iteration 306, loss = 0.12948763\n",
      "Iteration 307, loss = 0.12889568\n",
      "Iteration 308, loss = 0.12830983\n",
      "Iteration 309, loss = 0.12773015\n",
      "Iteration 310, loss = 0.12715673\n",
      "Iteration 311, loss = 0.12658913\n",
      "Iteration 312, loss = 0.12602744\n",
      "Iteration 313, loss = 0.12547157\n",
      "Iteration 314, loss = 0.12492103\n",
      "Iteration 315, loss = 0.12437569\n",
      "Iteration 316, loss = 0.12383635\n",
      "Iteration 317, loss = 0.12330239\n",
      "Iteration 318, loss = 0.12277400\n",
      "Iteration 319, loss = 0.12224995\n",
      "Iteration 320, loss = 0.12173006\n",
      "Iteration 321, loss = 0.12121436\n",
      "Iteration 322, loss = 0.12070414\n",
      "Iteration 323, loss = 0.12019688\n",
      "Iteration 324, loss = 0.11969253\n",
      "Iteration 325, loss = 0.11919202\n",
      "Iteration 326, loss = 0.11869466\n",
      "Iteration 327, loss = 0.11819960\n",
      "Iteration 328, loss = 0.11770709\n",
      "Iteration 329, loss = 0.11721678\n",
      "Iteration 330, loss = 0.11672943\n",
      "Iteration 331, loss = 0.11624374\n",
      "Iteration 332, loss = 0.11575986\n",
      "Iteration 333, loss = 0.11527816\n",
      "Iteration 334, loss = 0.11479808\n",
      "Iteration 335, loss = 0.11431910\n",
      "Iteration 336, loss = 0.11384189\n",
      "Iteration 337, loss = 0.11336620\n",
      "Iteration 338, loss = 0.11289152\n",
      "Iteration 339, loss = 0.11241819\n",
      "Iteration 340, loss = 0.11194692\n",
      "Iteration 341, loss = 0.11147687\n",
      "Iteration 342, loss = 0.11100811\n",
      "Iteration 343, loss = 0.11054031\n",
      "Iteration 344, loss = 0.11007377\n",
      "Iteration 345, loss = 0.10960909\n",
      "Iteration 346, loss = 0.10914466\n",
      "Iteration 347, loss = 0.10868256\n",
      "Iteration 348, loss = 0.10822173\n",
      "Iteration 349, loss = 0.10776147\n",
      "Iteration 350, loss = 0.10730231\n",
      "Iteration 351, loss = 0.10684617\n",
      "Iteration 352, loss = 0.10638913\n",
      "Iteration 353, loss = 0.10593509\n",
      "Iteration 354, loss = 0.10548113\n",
      "Iteration 355, loss = 0.10502735\n",
      "Iteration 356, loss = 0.10457424\n",
      "Iteration 357, loss = 0.10412352\n",
      "Iteration 358, loss = 0.10367141\n",
      "Iteration 359, loss = 0.10322118\n",
      "Iteration 360, loss = 0.10277163\n",
      "Iteration 361, loss = 0.10232157\n",
      "Iteration 362, loss = 0.10187163\n",
      "Iteration 363, loss = 0.10142316\n",
      "Iteration 364, loss = 0.10097552\n",
      "Iteration 365, loss = 0.10052756\n",
      "Iteration 366, loss = 0.10008039\n",
      "Iteration 367, loss = 0.09963480\n",
      "Iteration 368, loss = 0.09919041\n",
      "Iteration 369, loss = 0.09874593\n",
      "Iteration 370, loss = 0.09830310\n",
      "Iteration 371, loss = 0.09786287\n",
      "Iteration 372, loss = 0.09742336\n",
      "Iteration 373, loss = 0.09698499\n",
      "Iteration 374, loss = 0.09654934\n",
      "Iteration 375, loss = 0.09611613\n",
      "Iteration 376, loss = 0.09568463\n",
      "Iteration 377, loss = 0.09525559\n",
      "Iteration 378, loss = 0.09482997\n",
      "Iteration 379, loss = 0.09440676\n",
      "Iteration 380, loss = 0.09398677\n",
      "Iteration 381, loss = 0.09357058\n",
      "Iteration 382, loss = 0.09315731\n",
      "Iteration 383, loss = 0.09274738\n",
      "Iteration 384, loss = 0.09234158\n",
      "Iteration 385, loss = 0.09193930\n",
      "Iteration 386, loss = 0.09154086\n",
      "Iteration 387, loss = 0.09114654\n",
      "Iteration 388, loss = 0.09075601\n",
      "Iteration 389, loss = 0.09036978\n",
      "Iteration 390, loss = 0.08998777\n",
      "Iteration 391, loss = 0.08960963\n",
      "Iteration 392, loss = 0.08923585\n",
      "Iteration 393, loss = 0.08886616\n",
      "Iteration 394, loss = 0.08850048\n",
      "Iteration 395, loss = 0.08813940\n",
      "Iteration 396, loss = 0.08778259\n",
      "Iteration 397, loss = 0.08742977\n",
      "Iteration 398, loss = 0.08708095\n",
      "Iteration 399, loss = 0.08673621\n",
      "Iteration 400, loss = 0.08639577\n",
      "Iteration 401, loss = 0.08605892\n",
      "Iteration 402, loss = 0.08572574\n",
      "Iteration 403, loss = 0.08539663\n",
      "Iteration 404, loss = 0.08507144\n",
      "Iteration 405, loss = 0.08475007\n",
      "Iteration 406, loss = 0.08443220\n",
      "Iteration 407, loss = 0.08411783\n",
      "Iteration 408, loss = 0.08380684\n",
      "Iteration 409, loss = 0.08349967\n",
      "Iteration 410, loss = 0.08319609\n",
      "Iteration 411, loss = 0.08289600\n",
      "Iteration 412, loss = 0.08259938\n",
      "Iteration 413, loss = 0.08230628\n",
      "Iteration 414, loss = 0.08201653\n",
      "Iteration 415, loss = 0.08173042\n",
      "Iteration 416, loss = 0.08144779\n",
      "Iteration 417, loss = 0.08116821\n",
      "Iteration 418, loss = 0.08089210\n",
      "Iteration 419, loss = 0.08061933\n",
      "Iteration 420, loss = 0.08035000\n",
      "Iteration 421, loss = 0.08008387\n",
      "Iteration 422, loss = 0.07982080\n",
      "Iteration 423, loss = 0.07956093\n",
      "Iteration 424, loss = 0.07930439\n",
      "Iteration 425, loss = 0.07905094\n",
      "Iteration 426, loss = 0.07880056\n",
      "Iteration 427, loss = 0.07855342\n",
      "Iteration 428, loss = 0.07830946\n",
      "Iteration 429, loss = 0.07806849\n",
      "Iteration 430, loss = 0.07783056\n",
      "Iteration 431, loss = 0.07759571\n",
      "Iteration 432, loss = 0.07736379\n",
      "Iteration 433, loss = 0.07713491\n",
      "Iteration 434, loss = 0.07690896\n",
      "Iteration 435, loss = 0.07668578\n",
      "Iteration 436, loss = 0.07646549\n",
      "Iteration 437, loss = 0.07624826\n",
      "Iteration 438, loss = 0.07603373\n",
      "Iteration 439, loss = 0.07582201\n",
      "Iteration 440, loss = 0.07561301\n",
      "Iteration 441, loss = 0.07540674\n",
      "Iteration 442, loss = 0.07520315\n",
      "Iteration 443, loss = 0.07500220\n",
      "Iteration 444, loss = 0.07480390\n",
      "Iteration 445, loss = 0.07460815\n",
      "Iteration 446, loss = 0.07441499\n",
      "Iteration 447, loss = 0.07422426\n",
      "Iteration 448, loss = 0.07403603\n",
      "Iteration 449, loss = 0.07385027\n",
      "Iteration 450, loss = 0.07366691\n",
      "Iteration 451, loss = 0.07348599\n",
      "Iteration 452, loss = 0.07330750\n",
      "Iteration 453, loss = 0.07313126\n",
      "Iteration 454, loss = 0.07295731\n",
      "Iteration 455, loss = 0.07278576\n",
      "Iteration 456, loss = 0.07261647\n",
      "Iteration 457, loss = 0.07244939\n",
      "Iteration 458, loss = 0.07228457\n",
      "Iteration 459, loss = 0.07212181\n",
      "Iteration 460, loss = 0.07196115\n",
      "Iteration 461, loss = 0.07180257\n",
      "Iteration 462, loss = 0.07164631\n",
      "Iteration 463, loss = 0.07149174\n",
      "Iteration 464, loss = 0.07133936\n",
      "Iteration 465, loss = 0.07118884\n",
      "Iteration 466, loss = 0.07104014\n",
      "Iteration 467, loss = 0.07089331\n",
      "Iteration 468, loss = 0.07074826\n",
      "Iteration 469, loss = 0.07060496\n",
      "Iteration 470, loss = 0.07046335\n",
      "Iteration 471, loss = 0.07032334\n",
      "Iteration 472, loss = 0.07018492\n",
      "Iteration 473, loss = 0.07004808\n",
      "Iteration 474, loss = 0.06991272\n",
      "Iteration 475, loss = 0.06977882\n",
      "Iteration 476, loss = 0.06964631\n",
      "Iteration 477, loss = 0.06951518\n",
      "Iteration 478, loss = 0.06938536\n",
      "Iteration 479, loss = 0.06925685\n",
      "Iteration 480, loss = 0.06912960\n",
      "Iteration 481, loss = 0.06900356\n",
      "Iteration 482, loss = 0.06887872\n",
      "Iteration 483, loss = 0.06875505\n",
      "Iteration 484, loss = 0.06863248\n",
      "Iteration 485, loss = 0.06851107\n",
      "Iteration 486, loss = 0.06839093\n",
      "Iteration 487, loss = 0.06827154\n",
      "Iteration 488, loss = 0.06815334\n",
      "Iteration 489, loss = 0.06803614\n",
      "Iteration 490, loss = 0.06791996\n",
      "Iteration 491, loss = 0.06780489\n",
      "Iteration 492, loss = 0.06769088\n",
      "Iteration 493, loss = 0.06757789\n",
      "Iteration 494, loss = 0.06746587\n",
      "Iteration 495, loss = 0.06735485\n",
      "Iteration 496, loss = 0.06724480\n",
      "Iteration 497, loss = 0.06713576\n",
      "Iteration 498, loss = 0.06702770\n",
      "Iteration 499, loss = 0.06692061\n",
      "Iteration 500, loss = 0.06681454\n",
      "Iteration 501, loss = 0.06670943\n",
      "Iteration 502, loss = 0.06660534\n",
      "Iteration 503, loss = 0.06650227\n",
      "Iteration 504, loss = 0.06640018\n",
      "Iteration 505, loss = 0.06629909\n",
      "Iteration 506, loss = 0.06619910\n",
      "Iteration 507, loss = 0.06610009\n",
      "Iteration 508, loss = 0.06600213\n",
      "Iteration 509, loss = 0.06590519\n",
      "Iteration 510, loss = 0.06580928\n",
      "Iteration 511, loss = 0.06571446\n",
      "Iteration 512, loss = 0.06562061\n",
      "Iteration 513, loss = 0.06552805\n",
      "Iteration 514, loss = 0.06543639\n",
      "Iteration 515, loss = 0.06534568\n",
      "Iteration 516, loss = 0.06525578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34975994\n",
      "Iteration 2, loss = 1.30362454\n",
      "Iteration 3, loss = 1.26024416\n",
      "Iteration 4, loss = 1.21956087\n",
      "Iteration 5, loss = 1.18133663\n",
      "Iteration 6, loss = 1.14531821\n",
      "Iteration 7, loss = 1.11117818\n",
      "Iteration 8, loss = 1.07874601\n",
      "Iteration 9, loss = 1.04783577\n",
      "Iteration 10, loss = 1.01862248\n",
      "Iteration 11, loss = 0.99111446\n",
      "Iteration 12, loss = 0.96546483\n",
      "Iteration 13, loss = 0.94185821\n",
      "Iteration 14, loss = 0.92036025\n",
      "Iteration 15, loss = 0.90093926\n",
      "Iteration 16, loss = 0.88367761\n",
      "Iteration 17, loss = 0.86839511\n",
      "Iteration 18, loss = 0.85501158\n",
      "Iteration 19, loss = 0.84337657\n",
      "Iteration 20, loss = 0.83328179\n",
      "Iteration 21, loss = 0.82415601\n",
      "Iteration 22, loss = 0.81569687\n",
      "Iteration 23, loss = 0.80759494\n",
      "Iteration 24, loss = 0.79958222\n",
      "Iteration 25, loss = 0.79143979\n",
      "Iteration 26, loss = 0.78303963\n",
      "Iteration 27, loss = 0.77429213\n",
      "Iteration 28, loss = 0.76521756\n",
      "Iteration 29, loss = 0.75583815\n",
      "Iteration 30, loss = 0.74624378\n",
      "Iteration 31, loss = 0.73648564\n",
      "Iteration 32, loss = 0.72665516\n",
      "Iteration 33, loss = 0.71687157\n",
      "Iteration 34, loss = 0.70731085\n",
      "Iteration 35, loss = 0.69786938\n",
      "Iteration 36, loss = 0.68867564\n",
      "Iteration 37, loss = 0.67980943\n",
      "Iteration 38, loss = 0.67129718\n",
      "Iteration 39, loss = 0.66319341\n",
      "Iteration 40, loss = 0.65543463\n",
      "Iteration 41, loss = 0.64811099\n",
      "Iteration 42, loss = 0.64120238\n",
      "Iteration 43, loss = 0.63466728\n",
      "Iteration 44, loss = 0.62842672\n",
      "Iteration 45, loss = 0.62243855\n",
      "Iteration 46, loss = 0.61665539\n",
      "Iteration 47, loss = 0.61109131\n",
      "Iteration 48, loss = 0.60573991\n",
      "Iteration 49, loss = 0.60063339\n",
      "Iteration 50, loss = 0.59565589\n",
      "Iteration 51, loss = 0.59080526\n",
      "Iteration 52, loss = 0.58607017\n",
      "Iteration 53, loss = 0.58143073\n",
      "Iteration 54, loss = 0.57686755\n",
      "Iteration 55, loss = 0.57235996\n",
      "Iteration 56, loss = 0.56789459\n",
      "Iteration 57, loss = 0.56346947\n",
      "Iteration 58, loss = 0.55908212\n",
      "Iteration 59, loss = 0.55473530\n",
      "Iteration 60, loss = 0.55043234\n",
      "Iteration 61, loss = 0.54617859\n",
      "Iteration 62, loss = 0.54199785\n",
      "Iteration 63, loss = 0.53788128\n",
      "Iteration 64, loss = 0.53382489\n",
      "Iteration 65, loss = 0.52984307\n",
      "Iteration 66, loss = 0.52591670\n",
      "Iteration 67, loss = 0.52204305\n",
      "Iteration 68, loss = 0.51822083\n",
      "Iteration 69, loss = 0.51443783\n",
      "Iteration 70, loss = 0.51069695\n",
      "Iteration 71, loss = 0.50699171\n",
      "Iteration 72, loss = 0.50333733\n",
      "Iteration 73, loss = 0.49972427\n",
      "Iteration 74, loss = 0.49615164\n",
      "Iteration 75, loss = 0.49261639\n",
      "Iteration 76, loss = 0.48912036\n",
      "Iteration 77, loss = 0.48567986\n",
      "Iteration 78, loss = 0.48233799\n",
      "Iteration 79, loss = 0.47907888\n",
      "Iteration 80, loss = 0.47589365\n",
      "Iteration 81, loss = 0.47275267\n",
      "Iteration 82, loss = 0.46966040\n",
      "Iteration 83, loss = 0.46660796\n",
      "Iteration 84, loss = 0.46358982\n",
      "Iteration 85, loss = 0.46060892\n",
      "Iteration 86, loss = 0.45766161\n",
      "Iteration 87, loss = 0.45474860\n",
      "Iteration 88, loss = 0.45187188\n",
      "Iteration 89, loss = 0.44903142\n",
      "Iteration 90, loss = 0.44622401\n",
      "Iteration 91, loss = 0.44345148\n",
      "Iteration 92, loss = 0.44071065\n",
      "Iteration 93, loss = 0.43800007\n",
      "Iteration 94, loss = 0.43532211\n",
      "Iteration 95, loss = 0.43267399\n",
      "Iteration 96, loss = 0.43005459\n",
      "Iteration 97, loss = 0.42746294\n",
      "Iteration 98, loss = 0.42489810\n",
      "Iteration 99, loss = 0.42235967\n",
      "Iteration 100, loss = 0.41984698\n",
      "Iteration 101, loss = 0.41735910\n",
      "Iteration 102, loss = 0.41489669\n",
      "Iteration 103, loss = 0.41246037\n",
      "Iteration 104, loss = 0.41004802\n",
      "Iteration 105, loss = 0.40765895\n",
      "Iteration 106, loss = 0.40529191\n",
      "Iteration 107, loss = 0.40294674\n",
      "Iteration 108, loss = 0.40062303\n",
      "Iteration 109, loss = 0.39832030\n",
      "Iteration 110, loss = 0.39603785\n",
      "Iteration 111, loss = 0.39377492\n",
      "Iteration 112, loss = 0.39153094\n",
      "Iteration 113, loss = 0.38930591\n",
      "Iteration 114, loss = 0.38709956\n",
      "Iteration 115, loss = 0.38491100\n",
      "Iteration 116, loss = 0.38273977\n",
      "Iteration 117, loss = 0.38058605\n",
      "Iteration 118, loss = 0.37844972\n",
      "Iteration 119, loss = 0.37632937\n",
      "Iteration 120, loss = 0.37422491\n",
      "Iteration 121, loss = 0.37213609\n",
      "Iteration 122, loss = 0.37006225\n",
      "Iteration 123, loss = 0.36800335\n",
      "Iteration 124, loss = 0.36595897\n",
      "Iteration 125, loss = 0.36392825\n",
      "Iteration 126, loss = 0.36191151\n",
      "Iteration 127, loss = 0.35990785\n",
      "Iteration 128, loss = 0.35791726\n",
      "Iteration 129, loss = 0.35593943\n",
      "Iteration 130, loss = 0.35397415\n",
      "Iteration 131, loss = 0.35202107\n",
      "Iteration 132, loss = 0.35007987\n",
      "Iteration 133, loss = 0.34815083\n",
      "Iteration 134, loss = 0.34623262\n",
      "Iteration 135, loss = 0.34432549\n",
      "Iteration 136, loss = 0.34243024\n",
      "Iteration 137, loss = 0.34054561\n",
      "Iteration 138, loss = 0.33867142\n",
      "Iteration 139, loss = 0.33680816\n",
      "Iteration 140, loss = 0.33495501\n",
      "Iteration 141, loss = 0.33311210\n",
      "Iteration 142, loss = 0.33127948\n",
      "Iteration 143, loss = 0.32945638\n",
      "Iteration 144, loss = 0.32764348\n",
      "Iteration 145, loss = 0.32584456\n",
      "Iteration 146, loss = 0.32405524\n",
      "Iteration 147, loss = 0.32227549\n",
      "Iteration 148, loss = 0.32050505\n",
      "Iteration 149, loss = 0.31874437\n",
      "Iteration 150, loss = 0.31699304\n",
      "Iteration 151, loss = 0.31525109\n",
      "Iteration 152, loss = 0.31351877\n",
      "Iteration 153, loss = 0.31179548\n",
      "Iteration 154, loss = 0.31008119\n",
      "Iteration 155, loss = 0.30837600\n",
      "Iteration 156, loss = 0.30668130\n",
      "Iteration 157, loss = 0.30499914\n",
      "Iteration 158, loss = 0.30332611\n",
      "Iteration 159, loss = 0.30166216\n",
      "Iteration 160, loss = 0.30000720\n",
      "Iteration 161, loss = 0.29836138\n",
      "Iteration 162, loss = 0.29672452\n",
      "Iteration 163, loss = 0.29509666\n",
      "Iteration 164, loss = 0.29347749\n",
      "Iteration 165, loss = 0.29186718\n",
      "Iteration 166, loss = 0.29026597\n",
      "Iteration 167, loss = 0.28867308\n",
      "Iteration 168, loss = 0.28708848\n",
      "Iteration 169, loss = 0.28551289\n",
      "Iteration 170, loss = 0.28394582\n",
      "Iteration 171, loss = 0.28238999\n",
      "Iteration 172, loss = 0.28084272\n",
      "Iteration 173, loss = 0.27930384\n",
      "Iteration 174, loss = 0.27777441\n",
      "Iteration 175, loss = 0.27625335\n",
      "Iteration 176, loss = 0.27474065\n",
      "Iteration 177, loss = 0.27323707\n",
      "Iteration 178, loss = 0.27174204\n",
      "Iteration 179, loss = 0.27025528\n",
      "Iteration 180, loss = 0.26877811\n",
      "Iteration 181, loss = 0.26730995\n",
      "Iteration 182, loss = 0.26585021\n",
      "Iteration 183, loss = 0.26439903\n",
      "Iteration 184, loss = 0.26295589\n",
      "Iteration 185, loss = 0.26151861\n",
      "Iteration 186, loss = 0.26008571\n",
      "Iteration 187, loss = 0.25865501\n",
      "Iteration 188, loss = 0.25722553\n",
      "Iteration 189, loss = 0.25579698\n",
      "Iteration 190, loss = 0.25437208\n",
      "Iteration 191, loss = 0.25297144\n",
      "Iteration 192, loss = 0.25157794\n",
      "Iteration 193, loss = 0.25018765\n",
      "Iteration 194, loss = 0.24880017\n",
      "Iteration 195, loss = 0.24741471\n",
      "Iteration 196, loss = 0.24603259\n",
      "Iteration 197, loss = 0.24465384\n",
      "Iteration 198, loss = 0.24327638\n",
      "Iteration 199, loss = 0.24190023\n",
      "Iteration 200, loss = 0.24052535\n",
      "Iteration 201, loss = 0.23915182\n",
      "Iteration 202, loss = 0.23777996\n",
      "Iteration 203, loss = 0.23641002\n",
      "Iteration 204, loss = 0.23504082\n",
      "Iteration 205, loss = 0.23367451\n",
      "Iteration 206, loss = 0.23231066\n",
      "Iteration 207, loss = 0.23094870\n",
      "Iteration 208, loss = 0.22958867\n",
      "Iteration 209, loss = 0.22823087\n",
      "Iteration 210, loss = 0.22687551\n",
      "Iteration 211, loss = 0.22552285\n",
      "Iteration 212, loss = 0.22417312\n",
      "Iteration 213, loss = 0.22282711\n",
      "Iteration 214, loss = 0.22148460\n",
      "Iteration 215, loss = 0.22014589\n",
      "Iteration 216, loss = 0.21881122\n",
      "Iteration 217, loss = 0.21748084\n",
      "Iteration 218, loss = 0.21615510\n",
      "Iteration 219, loss = 0.21483591\n",
      "Iteration 220, loss = 0.21352191\n",
      "Iteration 221, loss = 0.21221322\n",
      "Iteration 222, loss = 0.21091011\n",
      "Iteration 223, loss = 0.20961284\n",
      "Iteration 224, loss = 0.20832183\n",
      "Iteration 225, loss = 0.20703704\n",
      "Iteration 226, loss = 0.20575879\n",
      "Iteration 227, loss = 0.20448725\n",
      "Iteration 228, loss = 0.20322261\n",
      "Iteration 229, loss = 0.20196503\n",
      "Iteration 230, loss = 0.20071472\n",
      "Iteration 231, loss = 0.19947195\n",
      "Iteration 232, loss = 0.19823686\n",
      "Iteration 233, loss = 0.19700956\n",
      "Iteration 234, loss = 0.19579005\n",
      "Iteration 235, loss = 0.19457862\n",
      "Iteration 236, loss = 0.19337533\n",
      "Iteration 237, loss = 0.19218027\n",
      "Iteration 238, loss = 0.19099352\n",
      "Iteration 239, loss = 0.18981519\n",
      "Iteration 240, loss = 0.18864543\n",
      "Iteration 241, loss = 0.18748434\n",
      "Iteration 242, loss = 0.18633190\n",
      "Iteration 243, loss = 0.18518820\n",
      "Iteration 244, loss = 0.18405356\n",
      "Iteration 245, loss = 0.18292791\n",
      "Iteration 246, loss = 0.18181123\n",
      "Iteration 247, loss = 0.18070366\n",
      "Iteration 248, loss = 0.17960549\n",
      "Iteration 249, loss = 0.17851641\n",
      "Iteration 250, loss = 0.17743640\n",
      "Iteration 251, loss = 0.17636547\n",
      "Iteration 252, loss = 0.17530362\n",
      "Iteration 253, loss = 0.17425108\n",
      "Iteration 254, loss = 0.17320779\n",
      "Iteration 255, loss = 0.17217355\n",
      "Iteration 256, loss = 0.17114842\n",
      "Iteration 257, loss = 0.17013239\n",
      "Iteration 258, loss = 0.16912543\n",
      "Iteration 259, loss = 0.16812778\n",
      "Iteration 260, loss = 0.16713987\n",
      "Iteration 261, loss = 0.16616108\n",
      "Iteration 262, loss = 0.16519149\n",
      "Iteration 263, loss = 0.16423109\n",
      "Iteration 264, loss = 0.16327971\n",
      "Iteration 265, loss = 0.16233735\n",
      "Iteration 266, loss = 0.16140385\n",
      "Iteration 267, loss = 0.16047924\n",
      "Iteration 268, loss = 0.15956344\n",
      "Iteration 269, loss = 0.15865631\n",
      "Iteration 270, loss = 0.15775813\n",
      "Iteration 271, loss = 0.15686852\n",
      "Iteration 272, loss = 0.15598745\n",
      "Iteration 273, loss = 0.15511483\n",
      "Iteration 274, loss = 0.15425067\n",
      "Iteration 275, loss = 0.15339511\n",
      "Iteration 276, loss = 0.15254800\n",
      "Iteration 277, loss = 0.15170914\n",
      "Iteration 278, loss = 0.15087880\n",
      "Iteration 279, loss = 0.15005672\n",
      "Iteration 280, loss = 0.14924272\n",
      "Iteration 281, loss = 0.14843683\n",
      "Iteration 282, loss = 0.14763888\n",
      "Iteration 283, loss = 0.14684886\n",
      "Iteration 284, loss = 0.14606661\n",
      "Iteration 285, loss = 0.14529208\n",
      "Iteration 286, loss = 0.14452536\n",
      "Iteration 287, loss = 0.14376634\n",
      "Iteration 288, loss = 0.14301476\n",
      "Iteration 289, loss = 0.14227060\n",
      "Iteration 290, loss = 0.14153383\n",
      "Iteration 291, loss = 0.14080434\n",
      "Iteration 292, loss = 0.14008204\n",
      "Iteration 293, loss = 0.13936707\n",
      "Iteration 294, loss = 0.13865920\n",
      "Iteration 295, loss = 0.13795842\n",
      "Iteration 296, loss = 0.13726457\n",
      "Iteration 297, loss = 0.13657766\n",
      "Iteration 298, loss = 0.13589756\n",
      "Iteration 299, loss = 0.13522428\n",
      "Iteration 300, loss = 0.13455765\n",
      "Iteration 301, loss = 0.13389764\n",
      "Iteration 302, loss = 0.13324414\n",
      "Iteration 303, loss = 0.13259715\n",
      "Iteration 304, loss = 0.13195660\n",
      "Iteration 305, loss = 0.13132244\n",
      "Iteration 306, loss = 0.13069456\n",
      "Iteration 307, loss = 0.13007267\n",
      "Iteration 308, loss = 0.12945682\n",
      "Iteration 309, loss = 0.12884708\n",
      "Iteration 310, loss = 0.12824362\n",
      "Iteration 311, loss = 0.12764593\n",
      "Iteration 312, loss = 0.12705407\n",
      "Iteration 313, loss = 0.12646763\n",
      "Iteration 314, loss = 0.12588684\n",
      "Iteration 315, loss = 0.12531143\n",
      "Iteration 316, loss = 0.12474213\n",
      "Iteration 317, loss = 0.12417760\n",
      "Iteration 318, loss = 0.12361773\n",
      "Iteration 319, loss = 0.12306293\n",
      "Iteration 320, loss = 0.12251255\n",
      "Iteration 321, loss = 0.12196647\n",
      "Iteration 322, loss = 0.12142413\n",
      "Iteration 323, loss = 0.12088521\n",
      "Iteration 324, loss = 0.12034958\n",
      "Iteration 325, loss = 0.11981655\n",
      "Iteration 326, loss = 0.11928593\n",
      "Iteration 327, loss = 0.11875787\n",
      "Iteration 328, loss = 0.11823196\n",
      "Iteration 329, loss = 0.11770825\n",
      "Iteration 330, loss = 0.11718648\n",
      "Iteration 331, loss = 0.11666654\n",
      "Iteration 332, loss = 0.11614829\n",
      "Iteration 333, loss = 0.11563154\n",
      "Iteration 334, loss = 0.11511584\n",
      "Iteration 335, loss = 0.11460018\n",
      "Iteration 336, loss = 0.11408752\n",
      "Iteration 337, loss = 0.11357505\n",
      "Iteration 338, loss = 0.11306572\n",
      "Iteration 339, loss = 0.11255642\n",
      "Iteration 340, loss = 0.11204456\n",
      "Iteration 341, loss = 0.11153935\n",
      "Iteration 342, loss = 0.11103150\n",
      "Iteration 343, loss = 0.11052596\n",
      "Iteration 344, loss = 0.11001891\n",
      "Iteration 345, loss = 0.10951273\n",
      "Iteration 346, loss = 0.10901068\n",
      "Iteration 347, loss = 0.10850153\n",
      "Iteration 348, loss = 0.10799834\n",
      "Iteration 349, loss = 0.10749615\n",
      "Iteration 350, loss = 0.10699111\n",
      "Iteration 351, loss = 0.10648629\n",
      "Iteration 352, loss = 0.10598292\n",
      "Iteration 353, loss = 0.10547956\n",
      "Iteration 354, loss = 0.10497714\n",
      "Iteration 355, loss = 0.10447403\n",
      "Iteration 356, loss = 0.10397181\n",
      "Iteration 357, loss = 0.10346963\n",
      "Iteration 358, loss = 0.10296877\n",
      "Iteration 359, loss = 0.10246499\n",
      "Iteration 360, loss = 0.10196278\n",
      "Iteration 361, loss = 0.10146203\n",
      "Iteration 362, loss = 0.10095934\n",
      "Iteration 363, loss = 0.10045596\n",
      "Iteration 364, loss = 0.09995356\n",
      "Iteration 365, loss = 0.09945225\n",
      "Iteration 366, loss = 0.09894862\n",
      "Iteration 367, loss = 0.09844664\n",
      "Iteration 368, loss = 0.09794548\n",
      "Iteration 369, loss = 0.09744449\n",
      "Iteration 370, loss = 0.09694438\n",
      "Iteration 371, loss = 0.09644447\n",
      "Iteration 372, loss = 0.09594643\n",
      "Iteration 373, loss = 0.09544963\n",
      "Iteration 374, loss = 0.09495505\n",
      "Iteration 375, loss = 0.09446147\n",
      "Iteration 376, loss = 0.09396952\n",
      "Iteration 377, loss = 0.09347963\n",
      "Iteration 378, loss = 0.09299221\n",
      "Iteration 379, loss = 0.09250754\n",
      "Iteration 380, loss = 0.09202483\n",
      "Iteration 381, loss = 0.09154470\n",
      "Iteration 382, loss = 0.09106799\n",
      "Iteration 383, loss = 0.09059372\n",
      "Iteration 384, loss = 0.09012277\n",
      "Iteration 385, loss = 0.08965505\n",
      "Iteration 386, loss = 0.08919023\n",
      "Iteration 387, loss = 0.08872881\n",
      "Iteration 388, loss = 0.08827101\n",
      "Iteration 389, loss = 0.08781709\n",
      "Iteration 390, loss = 0.08736667\n",
      "Iteration 391, loss = 0.08692011\n",
      "Iteration 392, loss = 0.08647771\n",
      "Iteration 393, loss = 0.08603961\n",
      "Iteration 394, loss = 0.08560574\n",
      "Iteration 395, loss = 0.08517616\n",
      "Iteration 396, loss = 0.08475094\n",
      "Iteration 397, loss = 0.08433023\n",
      "Iteration 398, loss = 0.08391416\n",
      "Iteration 399, loss = 0.08350274\n",
      "Iteration 400, loss = 0.08309624\n",
      "Iteration 401, loss = 0.08269447\n",
      "Iteration 402, loss = 0.08229765\n",
      "Iteration 403, loss = 0.08190577\n",
      "Iteration 404, loss = 0.08151869\n",
      "Iteration 405, loss = 0.08113628\n",
      "Iteration 406, loss = 0.08075873\n",
      "Iteration 407, loss = 0.08038590\n",
      "Iteration 408, loss = 0.08001769\n",
      "Iteration 409, loss = 0.07965400\n",
      "Iteration 410, loss = 0.07929481\n",
      "Iteration 411, loss = 0.07894020\n",
      "Iteration 412, loss = 0.07858977\n",
      "Iteration 413, loss = 0.07824356\n",
      "Iteration 414, loss = 0.07790163\n",
      "Iteration 415, loss = 0.07756372\n",
      "Iteration 416, loss = 0.07722967\n",
      "Iteration 417, loss = 0.07689950\n",
      "Iteration 418, loss = 0.07657312\n",
      "Iteration 419, loss = 0.07625048\n",
      "Iteration 420, loss = 0.07593156\n",
      "Iteration 421, loss = 0.07561626\n",
      "Iteration 422, loss = 0.07530463\n",
      "Iteration 423, loss = 0.07499649\n",
      "Iteration 424, loss = 0.07469198\n",
      "Iteration 425, loss = 0.07439079\n",
      "Iteration 426, loss = 0.07409306\n",
      "Iteration 427, loss = 0.07379869\n",
      "Iteration 428, loss = 0.07350809\n",
      "Iteration 429, loss = 0.07322068\n",
      "Iteration 430, loss = 0.07293642\n",
      "Iteration 431, loss = 0.07265551\n",
      "Iteration 432, loss = 0.07237763\n",
      "Iteration 433, loss = 0.07210268\n",
      "Iteration 434, loss = 0.07183068\n",
      "Iteration 435, loss = 0.07156168\n",
      "Iteration 436, loss = 0.07129586\n",
      "Iteration 437, loss = 0.07103290\n",
      "Iteration 438, loss = 0.07077279\n",
      "Iteration 439, loss = 0.07051573\n",
      "Iteration 440, loss = 0.07026156\n",
      "Iteration 441, loss = 0.07001031\n",
      "Iteration 442, loss = 0.06976181\n",
      "Iteration 443, loss = 0.06951634\n",
      "Iteration 444, loss = 0.06927364\n",
      "Iteration 445, loss = 0.06903373\n",
      "Iteration 446, loss = 0.06879661\n",
      "Iteration 447, loss = 0.06856210\n",
      "Iteration 448, loss = 0.06833026\n",
      "Iteration 449, loss = 0.06810107\n",
      "Iteration 450, loss = 0.06787440\n",
      "Iteration 451, loss = 0.06765003\n",
      "Iteration 452, loss = 0.06742799\n",
      "Iteration 453, loss = 0.06720905\n",
      "Iteration 454, loss = 0.06699263\n",
      "Iteration 455, loss = 0.06677860\n",
      "Iteration 456, loss = 0.06656702\n",
      "Iteration 457, loss = 0.06635792\n",
      "Iteration 458, loss = 0.06615121\n",
      "Iteration 459, loss = 0.06594686\n",
      "Iteration 460, loss = 0.06574481\n",
      "Iteration 461, loss = 0.06554498\n",
      "Iteration 462, loss = 0.06534738\n",
      "Iteration 463, loss = 0.06515192\n",
      "Iteration 464, loss = 0.06495857\n",
      "Iteration 465, loss = 0.06476737\n",
      "Iteration 466, loss = 0.06457820\n",
      "Iteration 467, loss = 0.06439106\n",
      "Iteration 468, loss = 0.06420594\n",
      "Iteration 469, loss = 0.06402276\n",
      "Iteration 470, loss = 0.06384144\n",
      "Iteration 471, loss = 0.06366192\n",
      "Iteration 472, loss = 0.06348415\n",
      "Iteration 473, loss = 0.06330808\n",
      "Iteration 474, loss = 0.06313364\n",
      "Iteration 475, loss = 0.06296079\n",
      "Iteration 476, loss = 0.06278953\n",
      "Iteration 477, loss = 0.06261977\n",
      "Iteration 478, loss = 0.06245149\n",
      "Iteration 479, loss = 0.06228458\n",
      "Iteration 480, loss = 0.06211894\n",
      "Iteration 481, loss = 0.06195457\n",
      "Iteration 482, loss = 0.06179146\n",
      "Iteration 483, loss = 0.06162956\n",
      "Iteration 484, loss = 0.06146880\n",
      "Iteration 485, loss = 0.06130915\n",
      "Iteration 486, loss = 0.06115058\n",
      "Iteration 487, loss = 0.06099304\n",
      "Iteration 488, loss = 0.06083651\n",
      "Iteration 489, loss = 0.06068097\n",
      "Iteration 490, loss = 0.06052638\n",
      "Iteration 491, loss = 0.06037273\n",
      "Iteration 492, loss = 0.06022005\n",
      "Iteration 493, loss = 0.06006829\n",
      "Iteration 494, loss = 0.05991745\n",
      "Iteration 495, loss = 0.05976751\n",
      "Iteration 496, loss = 0.05961847\n",
      "Iteration 497, loss = 0.05947033\n",
      "Iteration 498, loss = 0.05932310\n",
      "Iteration 499, loss = 0.05917680\n",
      "Iteration 500, loss = 0.05903144\n",
      "Iteration 501, loss = 0.05888702\n",
      "Iteration 502, loss = 0.05874349\n",
      "Iteration 503, loss = 0.05860086\n",
      "Iteration 504, loss = 0.05845916\n",
      "Iteration 505, loss = 0.05831842\n",
      "Iteration 506, loss = 0.05817864\n",
      "Iteration 507, loss = 0.05803984\n",
      "Iteration 508, loss = 0.05790196\n",
      "Iteration 509, loss = 0.05776505\n",
      "Iteration 510, loss = 0.05762913\n",
      "Iteration 511, loss = 0.05749422\n",
      "Iteration 512, loss = 0.05736030\n",
      "Iteration 513, loss = 0.05722735\n",
      "Iteration 514, loss = 0.05709540\n",
      "Iteration 515, loss = 0.05696446\n",
      "Iteration 516, loss = 0.05683453\n",
      "Iteration 517, loss = 0.05670564\n",
      "Iteration 518, loss = 0.05657777\n",
      "Iteration 519, loss = 0.05645090\n",
      "Iteration 520, loss = 0.05632509\n",
      "Iteration 521, loss = 0.05620033\n",
      "Iteration 522, loss = 0.05607659\n",
      "Iteration 523, loss = 0.05595391\n",
      "Iteration 524, loss = 0.05583227\n",
      "Iteration 525, loss = 0.05571170\n",
      "Iteration 526, loss = 0.05559217\n",
      "Iteration 527, loss = 0.05547367\n",
      "Iteration 528, loss = 0.05535621\n",
      "Iteration 529, loss = 0.05523977\n",
      "Iteration 530, loss = 0.05512439\n",
      "Iteration 531, loss = 0.05501004\n",
      "Iteration 532, loss = 0.05489671\n",
      "Iteration 533, loss = 0.05478442\n",
      "Iteration 534, loss = 0.05467317\n",
      "Iteration 535, loss = 0.05456295\n",
      "Iteration 536, loss = 0.05445368\n",
      "Iteration 537, loss = 0.05434544\n",
      "Iteration 538, loss = 0.05423817\n",
      "Iteration 539, loss = 0.05413190\n",
      "Iteration 540, loss = 0.05402667\n",
      "Iteration 541, loss = 0.05392245\n",
      "Iteration 542, loss = 0.05381922\n",
      "Iteration 543, loss = 0.05371693\n",
      "Iteration 544, loss = 0.05361558\n",
      "Iteration 545, loss = 0.05351514\n",
      "Iteration 546, loss = 0.05341558\n",
      "Iteration 547, loss = 0.05331688\n",
      "Iteration 548, loss = 0.05321902\n",
      "Iteration 549, loss = 0.05312197\n",
      "Iteration 550, loss = 0.05302573\n",
      "Iteration 551, loss = 0.05293025\n",
      "Iteration 552, loss = 0.05283545\n",
      "Iteration 553, loss = 0.05274140\n",
      "Iteration 554, loss = 0.05264852\n",
      "Iteration 555, loss = 0.05255574\n",
      "Iteration 556, loss = 0.05246408\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34996548\n",
      "Iteration 2, loss = 1.30351331\n",
      "Iteration 3, loss = 1.25974469\n",
      "Iteration 4, loss = 1.21872902\n",
      "Iteration 5, loss = 1.18029929\n",
      "Iteration 6, loss = 1.14417356\n",
      "Iteration 7, loss = 1.10997009\n",
      "Iteration 8, loss = 1.07742790\n",
      "Iteration 9, loss = 1.04643571\n",
      "Iteration 10, loss = 1.01715683\n",
      "Iteration 11, loss = 0.98957305\n",
      "Iteration 12, loss = 0.96375724\n",
      "Iteration 13, loss = 0.93990434\n",
      "Iteration 14, loss = 0.91811204\n",
      "Iteration 15, loss = 0.89845926\n",
      "Iteration 16, loss = 0.88104781\n",
      "Iteration 17, loss = 0.86566010\n",
      "Iteration 18, loss = 0.85218362\n",
      "Iteration 19, loss = 0.84051543\n",
      "Iteration 20, loss = 0.83030192\n",
      "Iteration 21, loss = 0.82109547\n",
      "Iteration 22, loss = 0.81258452\n",
      "Iteration 23, loss = 0.80445866\n",
      "Iteration 24, loss = 0.79645832\n",
      "Iteration 25, loss = 0.78836223\n",
      "Iteration 26, loss = 0.78004419\n",
      "Iteration 27, loss = 0.77140420\n",
      "Iteration 28, loss = 0.76242207\n",
      "Iteration 29, loss = 0.75312600\n",
      "Iteration 30, loss = 0.74360316\n",
      "Iteration 31, loss = 0.73397409\n",
      "Iteration 32, loss = 0.72432988\n",
      "Iteration 33, loss = 0.71476455\n",
      "Iteration 34, loss = 0.70535860\n",
      "Iteration 35, loss = 0.69611458\n",
      "Iteration 36, loss = 0.68706214\n",
      "Iteration 37, loss = 0.67833615\n",
      "Iteration 38, loss = 0.66992846\n",
      "Iteration 39, loss = 0.66191721\n",
      "Iteration 40, loss = 0.65429278\n",
      "Iteration 41, loss = 0.64707810\n",
      "Iteration 42, loss = 0.64019601\n",
      "Iteration 43, loss = 0.63366756\n",
      "Iteration 44, loss = 0.62739320\n",
      "Iteration 45, loss = 0.62137055\n",
      "Iteration 46, loss = 0.61558822\n",
      "Iteration 47, loss = 0.61005743\n",
      "Iteration 48, loss = 0.60477139\n",
      "Iteration 49, loss = 0.59973955\n",
      "Iteration 50, loss = 0.59486606\n",
      "Iteration 51, loss = 0.59011902\n",
      "Iteration 52, loss = 0.58545832\n",
      "Iteration 53, loss = 0.58089762\n",
      "Iteration 54, loss = 0.57640442\n",
      "Iteration 55, loss = 0.57199529\n",
      "Iteration 56, loss = 0.56764427\n",
      "Iteration 57, loss = 0.56333887\n",
      "Iteration 58, loss = 0.55907991\n",
      "Iteration 59, loss = 0.55486698\n",
      "Iteration 60, loss = 0.55070588\n",
      "Iteration 61, loss = 0.54659901\n",
      "Iteration 62, loss = 0.54254604\n",
      "Iteration 63, loss = 0.53855493\n",
      "Iteration 64, loss = 0.53465384\n",
      "Iteration 65, loss = 0.53081363\n",
      "Iteration 66, loss = 0.52703237\n",
      "Iteration 67, loss = 0.52330317\n",
      "Iteration 68, loss = 0.51961198\n",
      "Iteration 69, loss = 0.51594340\n",
      "Iteration 70, loss = 0.51232203\n",
      "Iteration 71, loss = 0.50872961\n",
      "Iteration 72, loss = 0.50518119\n",
      "Iteration 73, loss = 0.50167825\n",
      "Iteration 74, loss = 0.49822020\n",
      "Iteration 75, loss = 0.49481041\n",
      "Iteration 76, loss = 0.49144763\n",
      "Iteration 77, loss = 0.48816364\n",
      "Iteration 78, loss = 0.48494862\n",
      "Iteration 79, loss = 0.48181050\n",
      "Iteration 80, loss = 0.47876484\n",
      "Iteration 81, loss = 0.47576463\n",
      "Iteration 82, loss = 0.47281523\n",
      "Iteration 83, loss = 0.46990375\n",
      "Iteration 84, loss = 0.46702691\n",
      "Iteration 85, loss = 0.46418693\n",
      "Iteration 86, loss = 0.46138100\n",
      "Iteration 87, loss = 0.45860949\n",
      "Iteration 88, loss = 0.45587090\n",
      "Iteration 89, loss = 0.45316502\n",
      "Iteration 90, loss = 0.45049099\n",
      "Iteration 91, loss = 0.44784986\n",
      "Iteration 92, loss = 0.44524011\n",
      "Iteration 93, loss = 0.44266088\n",
      "Iteration 94, loss = 0.44011441\n",
      "Iteration 95, loss = 0.43760051\n",
      "Iteration 96, loss = 0.43511569\n",
      "Iteration 97, loss = 0.43265996\n",
      "Iteration 98, loss = 0.43023382\n",
      "Iteration 99, loss = 0.42783552\n",
      "Iteration 100, loss = 0.42546244\n",
      "Iteration 101, loss = 0.42311358\n",
      "Iteration 102, loss = 0.42078829\n",
      "Iteration 103, loss = 0.41848657\n",
      "Iteration 104, loss = 0.41620747\n",
      "Iteration 105, loss = 0.41395022\n",
      "Iteration 106, loss = 0.41171399\n",
      "Iteration 107, loss = 0.40949902\n",
      "Iteration 108, loss = 0.40730640\n",
      "Iteration 109, loss = 0.40513870\n",
      "Iteration 110, loss = 0.40299080\n",
      "Iteration 111, loss = 0.40086186\n",
      "Iteration 112, loss = 0.39875172\n",
      "Iteration 113, loss = 0.39665981\n",
      "Iteration 114, loss = 0.39458592\n",
      "Iteration 115, loss = 0.39252886\n",
      "Iteration 116, loss = 0.39048915\n",
      "Iteration 117, loss = 0.38846601\n",
      "Iteration 118, loss = 0.38645893\n",
      "Iteration 119, loss = 0.38447013\n",
      "Iteration 120, loss = 0.38249675\n",
      "Iteration 121, loss = 0.38053818\n",
      "Iteration 122, loss = 0.37859406\n",
      "Iteration 123, loss = 0.37666461\n",
      "Iteration 124, loss = 0.37474895\n",
      "Iteration 125, loss = 0.37284738\n",
      "Iteration 126, loss = 0.37095939\n",
      "Iteration 127, loss = 0.36908357\n",
      "Iteration 128, loss = 0.36722052\n",
      "Iteration 129, loss = 0.36537045\n",
      "Iteration 130, loss = 0.36353200\n",
      "Iteration 131, loss = 0.36170551\n",
      "Iteration 132, loss = 0.35989092\n",
      "Iteration 133, loss = 0.35808796\n",
      "Iteration 134, loss = 0.35629614\n",
      "Iteration 135, loss = 0.35451537\n",
      "Iteration 136, loss = 0.35274539\n",
      "Iteration 137, loss = 0.35098595\n",
      "Iteration 138, loss = 0.34923693\n",
      "Iteration 139, loss = 0.34749817\n",
      "Iteration 140, loss = 0.34576951\n",
      "Iteration 141, loss = 0.34405079\n",
      "Iteration 142, loss = 0.34234186\n",
      "Iteration 143, loss = 0.34064306\n",
      "Iteration 144, loss = 0.33895385\n",
      "Iteration 145, loss = 0.33727390\n",
      "Iteration 146, loss = 0.33560317\n",
      "Iteration 147, loss = 0.33394164\n",
      "Iteration 148, loss = 0.33228936\n",
      "Iteration 149, loss = 0.33064575\n",
      "Iteration 150, loss = 0.32901100\n",
      "Iteration 151, loss = 0.32738587\n",
      "Iteration 152, loss = 0.32576907\n",
      "Iteration 153, loss = 0.32416573\n",
      "Iteration 154, loss = 0.32257097\n",
      "Iteration 155, loss = 0.32098457\n",
      "Iteration 156, loss = 0.31940647\n",
      "Iteration 157, loss = 0.31783686\n",
      "Iteration 158, loss = 0.31627530\n",
      "Iteration 159, loss = 0.31472251\n",
      "Iteration 160, loss = 0.31317792\n",
      "Iteration 161, loss = 0.31164154\n",
      "Iteration 162, loss = 0.31011309\n",
      "Iteration 163, loss = 0.30859240\n",
      "Iteration 164, loss = 0.30708152\n",
      "Iteration 165, loss = 0.30558108\n",
      "Iteration 166, loss = 0.30408892\n",
      "Iteration 167, loss = 0.30260518\n",
      "Iteration 168, loss = 0.30112948\n",
      "Iteration 169, loss = 0.29966142\n",
      "Iteration 170, loss = 0.29820238\n",
      "Iteration 171, loss = 0.29675334\n",
      "Iteration 172, loss = 0.29531289\n",
      "Iteration 173, loss = 0.29388015\n",
      "Iteration 174, loss = 0.29245518\n",
      "Iteration 175, loss = 0.29103804\n",
      "Iteration 176, loss = 0.28962899\n",
      "Iteration 177, loss = 0.28822778\n",
      "Iteration 178, loss = 0.28683415\n",
      "Iteration 179, loss = 0.28544866\n",
      "Iteration 180, loss = 0.28407268\n",
      "Iteration 181, loss = 0.28270510\n",
      "Iteration 182, loss = 0.28134482\n",
      "Iteration 183, loss = 0.27999190\n",
      "Iteration 184, loss = 0.27864642\n",
      "Iteration 185, loss = 0.27730860\n",
      "Iteration 186, loss = 0.27597884\n",
      "Iteration 187, loss = 0.27465671\n",
      "Iteration 188, loss = 0.27334226\n",
      "Iteration 189, loss = 0.27203569\n",
      "Iteration 190, loss = 0.27073672\n",
      "Iteration 191, loss = 0.26944625\n",
      "Iteration 192, loss = 0.26816269\n",
      "Iteration 193, loss = 0.26688608\n",
      "Iteration 194, loss = 0.26561828\n",
      "Iteration 195, loss = 0.26435735\n",
      "Iteration 196, loss = 0.26310340\n",
      "Iteration 197, loss = 0.26185752\n",
      "Iteration 198, loss = 0.26061693\n",
      "Iteration 199, loss = 0.25938040\n",
      "Iteration 200, loss = 0.25814793\n",
      "Iteration 201, loss = 0.25691948\n",
      "Iteration 202, loss = 0.25569479\n",
      "Iteration 203, loss = 0.25447633\n",
      "Iteration 204, loss = 0.25326190\n",
      "Iteration 205, loss = 0.25205228\n",
      "Iteration 206, loss = 0.25084640\n",
      "Iteration 207, loss = 0.24964487\n",
      "Iteration 208, loss = 0.24844666\n",
      "Iteration 209, loss = 0.24725145\n",
      "Iteration 210, loss = 0.24606067\n",
      "Iteration 211, loss = 0.24487341\n",
      "Iteration 212, loss = 0.24369163\n",
      "Iteration 213, loss = 0.24251197\n",
      "Iteration 214, loss = 0.24133264\n",
      "Iteration 215, loss = 0.24015064\n",
      "Iteration 216, loss = 0.23896745\n",
      "Iteration 217, loss = 0.23778233\n",
      "Iteration 218, loss = 0.23661136\n",
      "Iteration 219, loss = 0.23544479\n",
      "Iteration 220, loss = 0.23427904\n",
      "Iteration 221, loss = 0.23311463\n",
      "Iteration 222, loss = 0.23195162\n",
      "Iteration 223, loss = 0.23079143\n",
      "Iteration 224, loss = 0.22963243\n",
      "Iteration 225, loss = 0.22847518\n",
      "Iteration 226, loss = 0.22732006\n",
      "Iteration 227, loss = 0.22616864\n",
      "Iteration 228, loss = 0.22502033\n",
      "Iteration 229, loss = 0.22387433\n",
      "Iteration 230, loss = 0.22273137\n",
      "Iteration 231, loss = 0.22159227\n",
      "Iteration 232, loss = 0.22045681\n",
      "Iteration 233, loss = 0.21932520\n",
      "Iteration 234, loss = 0.21819807\n",
      "Iteration 235, loss = 0.21707538\n",
      "Iteration 236, loss = 0.21595794\n",
      "Iteration 237, loss = 0.21484690\n",
      "Iteration 238, loss = 0.21374061\n",
      "Iteration 239, loss = 0.21263915\n",
      "Iteration 240, loss = 0.21154308\n",
      "Iteration 241, loss = 0.21045301\n",
      "Iteration 242, loss = 0.20936861\n",
      "Iteration 243, loss = 0.20829044\n",
      "Iteration 244, loss = 0.20721841\n",
      "Iteration 245, loss = 0.20615286\n",
      "Iteration 246, loss = 0.20509381\n",
      "Iteration 247, loss = 0.20404201\n",
      "Iteration 248, loss = 0.20299648\n",
      "Iteration 249, loss = 0.20195781\n",
      "Iteration 250, loss = 0.20092668\n",
      "Iteration 251, loss = 0.19990243\n",
      "Iteration 252, loss = 0.19888521\n",
      "Iteration 253, loss = 0.19787572\n",
      "Iteration 254, loss = 0.19687315\n",
      "Iteration 255, loss = 0.19587780\n",
      "Iteration 256, loss = 0.19489042\n",
      "Iteration 257, loss = 0.19391064\n",
      "Iteration 258, loss = 0.19293854\n",
      "Iteration 259, loss = 0.19197430\n",
      "Iteration 260, loss = 0.19101792\n",
      "Iteration 261, loss = 0.19006966\n",
      "Iteration 262, loss = 0.18912927\n",
      "Iteration 263, loss = 0.18819683\n",
      "Iteration 264, loss = 0.18727246\n",
      "Iteration 265, loss = 0.18635595\n",
      "Iteration 266, loss = 0.18544766\n",
      "Iteration 267, loss = 0.18454725\n",
      "Iteration 268, loss = 0.18365460\n",
      "Iteration 269, loss = 0.18276987\n",
      "Iteration 270, loss = 0.18189379\n",
      "Iteration 271, loss = 0.18102571\n",
      "Iteration 272, loss = 0.18016554\n",
      "Iteration 273, loss = 0.17931306\n",
      "Iteration 274, loss = 0.17846885\n",
      "Iteration 275, loss = 0.17763252\n",
      "Iteration 276, loss = 0.17680429\n",
      "Iteration 277, loss = 0.17598373\n",
      "Iteration 278, loss = 0.17517141\n",
      "Iteration 279, loss = 0.17436706\n",
      "Iteration 280, loss = 0.17357049\n",
      "Iteration 281, loss = 0.17278171\n",
      "Iteration 282, loss = 0.17200082\n",
      "Iteration 283, loss = 0.17122752\n",
      "Iteration 284, loss = 0.17046193\n",
      "Iteration 285, loss = 0.16970370\n",
      "Iteration 286, loss = 0.16895283\n",
      "Iteration 287, loss = 0.16820965\n",
      "Iteration 288, loss = 0.16747406\n",
      "Iteration 289, loss = 0.16674564\n",
      "Iteration 290, loss = 0.16602433\n",
      "Iteration 291, loss = 0.16531026\n",
      "Iteration 292, loss = 0.16460351\n",
      "Iteration 293, loss = 0.16390359\n",
      "Iteration 294, loss = 0.16321046\n",
      "Iteration 295, loss = 0.16252447\n",
      "Iteration 296, loss = 0.16184550\n",
      "Iteration 297, loss = 0.16117346\n",
      "Iteration 298, loss = 0.16050827\n",
      "Iteration 299, loss = 0.15984995\n",
      "Iteration 300, loss = 0.15919810\n",
      "Iteration 301, loss = 0.15855339\n",
      "Iteration 302, loss = 0.15791515\n",
      "Iteration 303, loss = 0.15728334\n",
      "Iteration 304, loss = 0.15665793\n",
      "Iteration 305, loss = 0.15603886\n",
      "Iteration 306, loss = 0.15542614\n",
      "Iteration 307, loss = 0.15481966\n",
      "Iteration 308, loss = 0.15421939\n",
      "Iteration 309, loss = 0.15362531\n",
      "Iteration 310, loss = 0.15303726\n",
      "Iteration 311, loss = 0.15245518\n",
      "Iteration 312, loss = 0.15187906\n",
      "Iteration 313, loss = 0.15130879\n",
      "Iteration 314, loss = 0.15074416\n",
      "Iteration 315, loss = 0.15018481\n",
      "Iteration 316, loss = 0.14963148\n",
      "Iteration 317, loss = 0.14908360\n",
      "Iteration 318, loss = 0.14854097\n",
      "Iteration 319, loss = 0.14800353\n",
      "Iteration 320, loss = 0.14747117\n",
      "Iteration 321, loss = 0.14694355\n",
      "Iteration 322, loss = 0.14642037\n",
      "Iteration 323, loss = 0.14590116\n",
      "Iteration 324, loss = 0.14538580\n",
      "Iteration 325, loss = 0.14487415\n",
      "Iteration 326, loss = 0.14436614\n",
      "Iteration 327, loss = 0.14386155\n",
      "Iteration 328, loss = 0.14336039\n",
      "Iteration 329, loss = 0.14286220\n",
      "Iteration 330, loss = 0.14236732\n",
      "Iteration 331, loss = 0.14187547\n",
      "Iteration 332, loss = 0.14138666\n",
      "Iteration 333, loss = 0.14090093\n",
      "Iteration 334, loss = 0.14041797\n",
      "Iteration 335, loss = 0.13993660\n",
      "Iteration 336, loss = 0.13946768\n",
      "Iteration 337, loss = 0.13898580\n",
      "Iteration 338, loss = 0.13851712\n",
      "Iteration 339, loss = 0.13805018\n",
      "Iteration 340, loss = 0.13758603\n",
      "Iteration 341, loss = 0.13712490\n",
      "Iteration 342, loss = 0.13666672\n",
      "Iteration 343, loss = 0.13621159\n",
      "Iteration 344, loss = 0.13575950\n",
      "Iteration 345, loss = 0.13531042\n",
      "Iteration 346, loss = 0.13486417\n",
      "Iteration 347, loss = 0.13442116\n",
      "Iteration 348, loss = 0.13398178\n",
      "Iteration 349, loss = 0.13354488\n",
      "Iteration 350, loss = 0.13311093\n",
      "Iteration 351, loss = 0.13267988\n",
      "Iteration 352, loss = 0.13225270\n",
      "Iteration 353, loss = 0.13182630\n",
      "Iteration 354, loss = 0.13140412\n",
      "Iteration 355, loss = 0.13098412\n",
      "Iteration 356, loss = 0.13056585\n",
      "Iteration 357, loss = 0.13014899\n",
      "Iteration 358, loss = 0.12973599\n",
      "Iteration 359, loss = 0.12932326\n",
      "Iteration 360, loss = 0.12891367\n",
      "Iteration 361, loss = 0.12850575\n",
      "Iteration 362, loss = 0.12809835\n",
      "Iteration 363, loss = 0.12769344\n",
      "Iteration 364, loss = 0.12728980\n",
      "Iteration 365, loss = 0.12688699\n",
      "Iteration 366, loss = 0.12648665\n",
      "Iteration 367, loss = 0.12608746\n",
      "Iteration 368, loss = 0.12568879\n",
      "Iteration 369, loss = 0.12529123\n",
      "Iteration 370, loss = 0.12489485\n",
      "Iteration 371, loss = 0.12450078\n",
      "Iteration 372, loss = 0.12410644\n",
      "Iteration 373, loss = 0.12371346\n",
      "Iteration 374, loss = 0.12332158\n",
      "Iteration 375, loss = 0.12293040\n",
      "Iteration 376, loss = 0.12253992\n",
      "Iteration 377, loss = 0.12215052\n",
      "Iteration 378, loss = 0.12176120\n",
      "Iteration 379, loss = 0.12137368\n",
      "Iteration 380, loss = 0.12098679\n",
      "Iteration 381, loss = 0.12060104\n",
      "Iteration 382, loss = 0.12021538\n",
      "Iteration 383, loss = 0.11983004\n",
      "Iteration 384, loss = 0.11944611\n",
      "Iteration 385, loss = 0.11906490\n",
      "Iteration 386, loss = 0.11868281\n",
      "Iteration 387, loss = 0.11830372\n",
      "Iteration 388, loss = 0.11792579\n",
      "Iteration 389, loss = 0.11754893\n",
      "Iteration 390, loss = 0.11717380\n",
      "Iteration 391, loss = 0.11680089\n",
      "Iteration 392, loss = 0.11642897\n",
      "Iteration 393, loss = 0.11605980\n",
      "Iteration 394, loss = 0.11569226\n",
      "Iteration 395, loss = 0.11532519\n",
      "Iteration 396, loss = 0.11495889\n",
      "Iteration 397, loss = 0.11459508\n",
      "Iteration 398, loss = 0.11423296\n",
      "Iteration 399, loss = 0.11387222\n",
      "Iteration 400, loss = 0.11351411\n",
      "Iteration 401, loss = 0.11315791\n",
      "Iteration 402, loss = 0.11280318\n",
      "Iteration 403, loss = 0.11245092\n",
      "Iteration 404, loss = 0.11210055\n",
      "Iteration 405, loss = 0.11175201\n",
      "Iteration 406, loss = 0.11140563\n",
      "Iteration 407, loss = 0.11106097\n",
      "Iteration 408, loss = 0.11071806\n",
      "Iteration 409, loss = 0.11037738\n",
      "Iteration 410, loss = 0.11003870\n",
      "Iteration 411, loss = 0.10970210\n",
      "Iteration 412, loss = 0.10936765\n",
      "Iteration 413, loss = 0.10903522\n",
      "Iteration 414, loss = 0.10870500\n",
      "Iteration 415, loss = 0.10837715\n",
      "Iteration 416, loss = 0.10805165\n",
      "Iteration 417, loss = 0.10772839\n",
      "Iteration 418, loss = 0.10740752\n",
      "Iteration 419, loss = 0.10708927\n",
      "Iteration 420, loss = 0.10677353\n",
      "Iteration 421, loss = 0.10646032\n",
      "Iteration 422, loss = 0.10614969\n",
      "Iteration 423, loss = 0.10584178\n",
      "Iteration 424, loss = 0.10553676\n",
      "Iteration 425, loss = 0.10523445\n",
      "Iteration 426, loss = 0.10493491\n",
      "Iteration 427, loss = 0.10463809\n",
      "Iteration 428, loss = 0.10434452\n",
      "Iteration 429, loss = 0.10405389\n",
      "Iteration 430, loss = 0.10376614\n",
      "Iteration 431, loss = 0.10348130\n",
      "Iteration 432, loss = 0.10319941\n",
      "Iteration 433, loss = 0.10292083\n",
      "Iteration 434, loss = 0.10264535\n",
      "Iteration 435, loss = 0.10237259\n",
      "Iteration 436, loss = 0.10210287\n",
      "Iteration 437, loss = 0.10183626\n",
      "Iteration 438, loss = 0.10157261\n",
      "Iteration 439, loss = 0.10131161\n",
      "Iteration 440, loss = 0.10105191\n",
      "Iteration 441, loss = 0.10079572\n",
      "Iteration 442, loss = 0.10054266\n",
      "Iteration 443, loss = 0.10029222\n",
      "Iteration 444, loss = 0.10004423\n",
      "Iteration 445, loss = 0.09979875\n",
      "Iteration 446, loss = 0.09955576\n",
      "Iteration 447, loss = 0.09931526\n",
      "Iteration 448, loss = 0.09907718\n",
      "Iteration 449, loss = 0.09884165\n",
      "Iteration 450, loss = 0.09860845\n",
      "Iteration 451, loss = 0.09837760\n",
      "Iteration 452, loss = 0.09814918\n",
      "Iteration 453, loss = 0.09792338\n",
      "Iteration 454, loss = 0.09770074\n",
      "Iteration 455, loss = 0.09747998\n",
      "Iteration 456, loss = 0.09726112\n",
      "Iteration 457, loss = 0.09704500\n",
      "Iteration 458, loss = 0.09683153\n",
      "Iteration 459, loss = 0.09662026\n",
      "Iteration 460, loss = 0.09641137\n",
      "Iteration 461, loss = 0.09620471\n",
      "Iteration 462, loss = 0.09600020\n",
      "Iteration 463, loss = 0.09579806\n",
      "Iteration 464, loss = 0.09559814\n",
      "Iteration 465, loss = 0.09540035\n",
      "Iteration 466, loss = 0.09520465\n",
      "Iteration 467, loss = 0.09501141\n",
      "Iteration 468, loss = 0.09482007\n",
      "Iteration 469, loss = 0.09463086\n",
      "Iteration 470, loss = 0.09444381\n",
      "Iteration 471, loss = 0.09425871\n",
      "Iteration 472, loss = 0.09407560\n",
      "Iteration 473, loss = 0.09389462\n",
      "Iteration 474, loss = 0.09371580\n",
      "Iteration 475, loss = 0.09353882\n",
      "Iteration 476, loss = 0.09336379\n",
      "Iteration 477, loss = 0.09319066\n",
      "Iteration 478, loss = 0.09301958\n",
      "Iteration 479, loss = 0.09285034\n",
      "Iteration 480, loss = 0.09268294\n",
      "Iteration 481, loss = 0.09251721\n",
      "Iteration 482, loss = 0.09235328\n",
      "Iteration 483, loss = 0.09219114\n",
      "Iteration 484, loss = 0.09203088\n",
      "Iteration 485, loss = 0.09187232\n",
      "Iteration 486, loss = 0.09171546\n",
      "Iteration 487, loss = 0.09156031\n",
      "Iteration 488, loss = 0.09140682\n",
      "Iteration 489, loss = 0.09125505\n",
      "Iteration 490, loss = 0.09110498\n",
      "Iteration 491, loss = 0.09095653\n",
      "Iteration 492, loss = 0.09080972\n",
      "Iteration 493, loss = 0.09066470\n",
      "Iteration 494, loss = 0.09052126\n",
      "Iteration 495, loss = 0.09037933\n",
      "Iteration 496, loss = 0.09023898\n",
      "Iteration 497, loss = 0.09010023\n",
      "Iteration 498, loss = 0.08996306\n",
      "Iteration 499, loss = 0.08982741\n",
      "Iteration 500, loss = 0.08969334\n",
      "Iteration 501, loss = 0.08956085\n",
      "Iteration 502, loss = 0.08942987\n",
      "Iteration 503, loss = 0.08930044\n",
      "Iteration 504, loss = 0.08917256\n",
      "Iteration 505, loss = 0.08904613\n",
      "Iteration 506, loss = 0.08892118\n",
      "Iteration 507, loss = 0.08879779\n",
      "Iteration 508, loss = 0.08867589\n",
      "Iteration 509, loss = 0.08855547\n",
      "Iteration 510, loss = 0.08843651\n",
      "Iteration 511, loss = 0.08831895\n",
      "Iteration 512, loss = 0.08820281\n",
      "Iteration 513, loss = 0.08808804\n",
      "Iteration 514, loss = 0.08797470\n",
      "Iteration 515, loss = 0.08786271\n",
      "Iteration 516, loss = 0.08775206\n",
      "Iteration 517, loss = 0.08764280\n",
      "Iteration 518, loss = 0.08753489\n",
      "Iteration 519, loss = 0.08742832\n",
      "Iteration 520, loss = 0.08732305\n",
      "Iteration 521, loss = 0.08721902\n",
      "Iteration 522, loss = 0.08711620\n",
      "Iteration 523, loss = 0.08701463\n",
      "Iteration 524, loss = 0.08691435\n",
      "Iteration 525, loss = 0.08681529\n",
      "Iteration 526, loss = 0.08671747\n",
      "Iteration 527, loss = 0.08662081\n",
      "Iteration 528, loss = 0.08652532\n",
      "Iteration 529, loss = 0.08643099\n",
      "Iteration 530, loss = 0.08633778\n",
      "Iteration 531, loss = 0.08624570\n",
      "Iteration 532, loss = 0.08615482\n",
      "Iteration 533, loss = 0.08606523\n",
      "Iteration 534, loss = 0.08597723\n",
      "Iteration 535, loss = 0.08589056\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35652749\n",
      "Iteration 2, loss = 1.33251874\n",
      "Iteration 3, loss = 1.30028251\n",
      "Iteration 4, loss = 1.26220532\n",
      "Iteration 5, loss = 1.22036152\n",
      "Iteration 6, loss = 1.17648882\n",
      "Iteration 7, loss = 1.13187949\n",
      "Iteration 8, loss = 1.08757483\n",
      "Iteration 9, loss = 1.04467697\n",
      "Iteration 10, loss = 1.00420713\n",
      "Iteration 11, loss = 0.96728796\n",
      "Iteration 12, loss = 0.93495560\n",
      "Iteration 13, loss = 0.90779413\n",
      "Iteration 14, loss = 0.88610606\n",
      "Iteration 15, loss = 0.86953071\n",
      "Iteration 16, loss = 0.85768043\n",
      "Iteration 17, loss = 0.84939917\n",
      "Iteration 18, loss = 0.84341836\n",
      "Iteration 19, loss = 0.83831328\n",
      "Iteration 20, loss = 0.83279345\n",
      "Iteration 21, loss = 0.82610169\n",
      "Iteration 22, loss = 0.81779481\n",
      "Iteration 23, loss = 0.80814864\n",
      "Iteration 24, loss = 0.79761207\n",
      "Iteration 25, loss = 0.78691860\n",
      "Iteration 26, loss = 0.77596961\n",
      "Iteration 27, loss = 0.76486954\n",
      "Iteration 28, loss = 0.75384094\n",
      "Iteration 29, loss = 0.74322311\n",
      "Iteration 30, loss = 0.73341551\n",
      "Iteration 31, loss = 0.72463527\n",
      "Iteration 32, loss = 0.71671672\n",
      "Iteration 33, loss = 0.70955530\n",
      "Iteration 34, loss = 0.70314690\n",
      "Iteration 35, loss = 0.69738192\n",
      "Iteration 36, loss = 0.69196005\n",
      "Iteration 37, loss = 0.68678635\n",
      "Iteration 38, loss = 0.68182348\n",
      "Iteration 39, loss = 0.67697079\n",
      "Iteration 40, loss = 0.67218381\n",
      "Iteration 41, loss = 0.66744734\n",
      "Iteration 42, loss = 0.66276911\n",
      "Iteration 43, loss = 0.65814624\n",
      "Iteration 44, loss = 0.65356903\n",
      "Iteration 45, loss = 0.64905466\n",
      "Iteration 46, loss = 0.64464812\n",
      "Iteration 47, loss = 0.64032725\n",
      "Iteration 48, loss = 0.63608246\n",
      "Iteration 49, loss = 0.63196474\n",
      "Iteration 50, loss = 0.62801540\n",
      "Iteration 51, loss = 0.62423111\n",
      "Iteration 52, loss = 0.62057959\n",
      "Iteration 53, loss = 0.61709488\n",
      "Iteration 54, loss = 0.61381177\n",
      "Iteration 55, loss = 0.61069376\n",
      "Iteration 56, loss = 0.60767678\n",
      "Iteration 57, loss = 0.60475461\n",
      "Iteration 58, loss = 0.60189720\n",
      "Iteration 59, loss = 0.59908276\n",
      "Iteration 60, loss = 0.59630261\n",
      "Iteration 61, loss = 0.59354767\n",
      "Iteration 62, loss = 0.59081674\n",
      "Iteration 63, loss = 0.58811160\n",
      "Iteration 64, loss = 0.58542954\n",
      "Iteration 65, loss = 0.58277465\n",
      "Iteration 66, loss = 0.58015433\n",
      "Iteration 67, loss = 0.57757430\n",
      "Iteration 68, loss = 0.57503822\n",
      "Iteration 69, loss = 0.57254309\n",
      "Iteration 70, loss = 0.57008648\n",
      "Iteration 71, loss = 0.56767845\n",
      "Iteration 72, loss = 0.56530894\n",
      "Iteration 73, loss = 0.56298990\n",
      "Iteration 74, loss = 0.56071790\n",
      "Iteration 75, loss = 0.55848628\n",
      "Iteration 76, loss = 0.55629236\n",
      "Iteration 77, loss = 0.55413646\n",
      "Iteration 78, loss = 0.55201681\n",
      "Iteration 79, loss = 0.54993498\n",
      "Iteration 80, loss = 0.54788600\n",
      "Iteration 81, loss = 0.54586577\n",
      "Iteration 82, loss = 0.54387624\n",
      "Iteration 83, loss = 0.54191464\n",
      "Iteration 84, loss = 0.53997983\n",
      "Iteration 85, loss = 0.53807149\n",
      "Iteration 86, loss = 0.53618815\n",
      "Iteration 87, loss = 0.53432990\n",
      "Iteration 88, loss = 0.53249662\n",
      "Iteration 89, loss = 0.53068844\n",
      "Iteration 90, loss = 0.52890418\n",
      "Iteration 91, loss = 0.52714357\n",
      "Iteration 92, loss = 0.52540607\n",
      "Iteration 93, loss = 0.52369258\n",
      "Iteration 94, loss = 0.52200188\n",
      "Iteration 95, loss = 0.52033342\n",
      "Iteration 96, loss = 0.51868635\n",
      "Iteration 97, loss = 0.51706030\n",
      "Iteration 98, loss = 0.51545553\n",
      "Iteration 99, loss = 0.51387111\n",
      "Iteration 100, loss = 0.51230558\n",
      "Iteration 101, loss = 0.51075917\n",
      "Iteration 102, loss = 0.50923154\n",
      "Iteration 103, loss = 0.50772157\n",
      "Iteration 104, loss = 0.50622938\n",
      "Iteration 105, loss = 0.50475433\n",
      "Iteration 106, loss = 0.50329699\n",
      "Iteration 107, loss = 0.50185644\n",
      "Iteration 108, loss = 0.50043261\n",
      "Iteration 109, loss = 0.49902488\n",
      "Iteration 110, loss = 0.49763300\n",
      "Iteration 111, loss = 0.49625661\n",
      "Iteration 112, loss = 0.49489591\n",
      "Iteration 113, loss = 0.49355031\n",
      "Iteration 114, loss = 0.49221945\n",
      "Iteration 115, loss = 0.49090300\n",
      "Iteration 116, loss = 0.48960079\n",
      "Iteration 117, loss = 0.48831237\n",
      "Iteration 118, loss = 0.48703705\n",
      "Iteration 119, loss = 0.48577466\n",
      "Iteration 120, loss = 0.48452554\n",
      "Iteration 121, loss = 0.48328965\n",
      "Iteration 122, loss = 0.48206618\n",
      "Iteration 123, loss = 0.48085548\n",
      "Iteration 124, loss = 0.47965719\n",
      "Iteration 125, loss = 0.47847023\n",
      "Iteration 126, loss = 0.47729383\n",
      "Iteration 127, loss = 0.47612856\n",
      "Iteration 128, loss = 0.47497417\n",
      "Iteration 129, loss = 0.47383101\n",
      "Iteration 130, loss = 0.47269906\n",
      "Iteration 131, loss = 0.47157707\n",
      "Iteration 132, loss = 0.47046479\n",
      "Iteration 133, loss = 0.46936306\n",
      "Iteration 134, loss = 0.46826934\n",
      "Iteration 135, loss = 0.46718379\n",
      "Iteration 136, loss = 0.46610740\n",
      "Iteration 137, loss = 0.46503947\n",
      "Iteration 138, loss = 0.46397944\n",
      "Iteration 139, loss = 0.46292695\n",
      "Iteration 140, loss = 0.46188201\n",
      "Iteration 141, loss = 0.46084318\n",
      "Iteration 142, loss = 0.45981034\n",
      "Iteration 143, loss = 0.45878307\n",
      "Iteration 144, loss = 0.45776370\n",
      "Iteration 145, loss = 0.45675174\n",
      "Iteration 146, loss = 0.45574734\n",
      "Iteration 147, loss = 0.45474428\n",
      "Iteration 148, loss = 0.45374183\n",
      "Iteration 149, loss = 0.45274017\n",
      "Iteration 150, loss = 0.45174255\n",
      "Iteration 151, loss = 0.45074611\n",
      "Iteration 152, loss = 0.44974898\n",
      "Iteration 153, loss = 0.44875306\n",
      "Iteration 154, loss = 0.44775528\n",
      "Iteration 155, loss = 0.44676278\n",
      "Iteration 156, loss = 0.44577573\n",
      "Iteration 157, loss = 0.44479286\n",
      "Iteration 158, loss = 0.44380904\n",
      "Iteration 159, loss = 0.44282929\n",
      "Iteration 160, loss = 0.44184805\n",
      "Iteration 161, loss = 0.44084732\n",
      "Iteration 162, loss = 0.43984588\n",
      "Iteration 163, loss = 0.43883661\n",
      "Iteration 164, loss = 0.43782582\n",
      "Iteration 165, loss = 0.43681631\n",
      "Iteration 166, loss = 0.43580727\n",
      "Iteration 167, loss = 0.43480368\n",
      "Iteration 168, loss = 0.43379616\n",
      "Iteration 169, loss = 0.43279441\n",
      "Iteration 170, loss = 0.43178934\n",
      "Iteration 171, loss = 0.43080375\n",
      "Iteration 172, loss = 0.42984062\n",
      "Iteration 173, loss = 0.42888936\n",
      "Iteration 174, loss = 0.42795604\n",
      "Iteration 175, loss = 0.42703714\n",
      "Iteration 176, loss = 0.42613423\n",
      "Iteration 177, loss = 0.42524737\n",
      "Iteration 178, loss = 0.42439121\n",
      "Iteration 179, loss = 0.42355046\n",
      "Iteration 180, loss = 0.42272655\n",
      "Iteration 181, loss = 0.42191387\n",
      "Iteration 182, loss = 0.42111670\n",
      "Iteration 183, loss = 0.42032678\n",
      "Iteration 184, loss = 0.41954537\n",
      "Iteration 185, loss = 0.41877586\n",
      "Iteration 186, loss = 0.41801720\n",
      "Iteration 187, loss = 0.41726853\n",
      "Iteration 188, loss = 0.41652612\n",
      "Iteration 189, loss = 0.41578891\n",
      "Iteration 190, loss = 0.41505945\n",
      "Iteration 191, loss = 0.41433915\n",
      "Iteration 192, loss = 0.41362564\n",
      "Iteration 193, loss = 0.41291725\n",
      "Iteration 194, loss = 0.41221385\n",
      "Iteration 195, loss = 0.41151574\n",
      "Iteration 196, loss = 0.41082167\n",
      "Iteration 197, loss = 0.41013171\n",
      "Iteration 198, loss = 0.40944559\n",
      "Iteration 199, loss = 0.40876463\n",
      "Iteration 200, loss = 0.40808746\n",
      "Iteration 201, loss = 0.40741431\n",
      "Iteration 202, loss = 0.40674540\n",
      "Iteration 203, loss = 0.40608014\n",
      "Iteration 204, loss = 0.40541839\n",
      "Iteration 205, loss = 0.40476034\n",
      "Iteration 206, loss = 0.40410549\n",
      "Iteration 207, loss = 0.40345403\n",
      "Iteration 208, loss = 0.40280593\n",
      "Iteration 209, loss = 0.40216097\n",
      "Iteration 210, loss = 0.40151920\n",
      "Iteration 211, loss = 0.40088049\n",
      "Iteration 212, loss = 0.40024569\n",
      "Iteration 213, loss = 0.39961369\n",
      "Iteration 214, loss = 0.39898481\n",
      "Iteration 215, loss = 0.39835866\n",
      "Iteration 216, loss = 0.39773537\n",
      "Iteration 217, loss = 0.39711503\n",
      "Iteration 218, loss = 0.39649743\n",
      "Iteration 219, loss = 0.39588261\n",
      "Iteration 220, loss = 0.39527050\n",
      "Iteration 221, loss = 0.39466101\n",
      "Iteration 222, loss = 0.39405424\n",
      "Iteration 223, loss = 0.39345034\n",
      "Iteration 224, loss = 0.39284913\n",
      "Iteration 225, loss = 0.39225057\n",
      "Iteration 226, loss = 0.39165460\n",
      "Iteration 227, loss = 0.39106106\n",
      "Iteration 228, loss = 0.39047066\n",
      "Iteration 229, loss = 0.38988292\n",
      "Iteration 230, loss = 0.38929755\n",
      "Iteration 231, loss = 0.38871446\n",
      "Iteration 232, loss = 0.38813442\n",
      "Iteration 233, loss = 0.38755678\n",
      "Iteration 234, loss = 0.38698140\n",
      "Iteration 235, loss = 0.38640851\n",
      "Iteration 236, loss = 0.38583813\n",
      "Iteration 237, loss = 0.38526992\n",
      "Iteration 238, loss = 0.38470392\n",
      "Iteration 239, loss = 0.38414032\n",
      "Iteration 240, loss = 0.38357912\n",
      "Iteration 241, loss = 0.38301963\n",
      "Iteration 242, loss = 0.38246249\n",
      "Iteration 243, loss = 0.38190752\n",
      "Iteration 244, loss = 0.38135461\n",
      "Iteration 245, loss = 0.38080384\n",
      "Iteration 246, loss = 0.38025510\n",
      "Iteration 247, loss = 0.37970830\n",
      "Iteration 248, loss = 0.37916355\n",
      "Iteration 249, loss = 0.37862089\n",
      "Iteration 250, loss = 0.37808023\n",
      "Iteration 251, loss = 0.37754152\n",
      "Iteration 252, loss = 0.37700483\n",
      "Iteration 253, loss = 0.37647005\n",
      "Iteration 254, loss = 0.37593720\n",
      "Iteration 255, loss = 0.37540621\n",
      "Iteration 256, loss = 0.37487711\n",
      "Iteration 257, loss = 0.37434989\n",
      "Iteration 258, loss = 0.37382451\n",
      "Iteration 259, loss = 0.37330094\n",
      "Iteration 260, loss = 0.37277922\n",
      "Iteration 261, loss = 0.37225926\n",
      "Iteration 262, loss = 0.37174099\n",
      "Iteration 263, loss = 0.37122444\n",
      "Iteration 264, loss = 0.37070962\n",
      "Iteration 265, loss = 0.37019647\n",
      "Iteration 266, loss = 0.36968505\n",
      "Iteration 267, loss = 0.36917534\n",
      "Iteration 268, loss = 0.36866730\n",
      "Iteration 269, loss = 0.36816107\n",
      "Iteration 270, loss = 0.36765647\n",
      "Iteration 271, loss = 0.36715352\n",
      "Iteration 272, loss = 0.36665219\n",
      "Iteration 273, loss = 0.36615243\n",
      "Iteration 274, loss = 0.36565431\n",
      "Iteration 275, loss = 0.36515775\n",
      "Iteration 276, loss = 0.36466282\n",
      "Iteration 277, loss = 0.36416928\n",
      "Iteration 278, loss = 0.36367725\n",
      "Iteration 279, loss = 0.36318677\n",
      "Iteration 280, loss = 0.36269776\n",
      "Iteration 281, loss = 0.36221024\n",
      "Iteration 282, loss = 0.36172429\n",
      "Iteration 283, loss = 0.36123977\n",
      "Iteration 284, loss = 0.36075678\n",
      "Iteration 285, loss = 0.36027521\n",
      "Iteration 286, loss = 0.35979508\n",
      "Iteration 287, loss = 0.35931641\n",
      "Iteration 288, loss = 0.35883907\n",
      "Iteration 289, loss = 0.35836317\n",
      "Iteration 290, loss = 0.35788865\n",
      "Iteration 291, loss = 0.35741551\n",
      "Iteration 292, loss = 0.35694393\n",
      "Iteration 293, loss = 0.35647369\n",
      "Iteration 294, loss = 0.35600474\n",
      "Iteration 295, loss = 0.35553713\n",
      "Iteration 296, loss = 0.35507080\n",
      "Iteration 297, loss = 0.35460589\n",
      "Iteration 298, loss = 0.35414232\n",
      "Iteration 299, loss = 0.35368006\n",
      "Iteration 300, loss = 0.35321909\n",
      "Iteration 301, loss = 0.35275942\n",
      "Iteration 302, loss = 0.35230114\n",
      "Iteration 303, loss = 0.35184409\n",
      "Iteration 304, loss = 0.35138825\n",
      "Iteration 305, loss = 0.35093373\n",
      "Iteration 306, loss = 0.35048052\n",
      "Iteration 307, loss = 0.35002850\n",
      "Iteration 308, loss = 0.34957774\n",
      "Iteration 309, loss = 0.34912834\n",
      "Iteration 310, loss = 0.34867999\n",
      "Iteration 311, loss = 0.34823299\n",
      "Iteration 312, loss = 0.34778719\n",
      "Iteration 313, loss = 0.34734259\n",
      "Iteration 314, loss = 0.34689921\n",
      "Iteration 315, loss = 0.34645698\n",
      "Iteration 316, loss = 0.34601598\n",
      "Iteration 317, loss = 0.34557610\n",
      "Iteration 318, loss = 0.34513746\n",
      "Iteration 319, loss = 0.34469991\n",
      "Iteration 320, loss = 0.34426354\n",
      "Iteration 321, loss = 0.34382831\n",
      "Iteration 322, loss = 0.34339425\n",
      "Iteration 323, loss = 0.34296128\n",
      "Iteration 324, loss = 0.34252946\n",
      "Iteration 325, loss = 0.34209873\n",
      "Iteration 326, loss = 0.34166913\n",
      "Iteration 327, loss = 0.34124062\n",
      "Iteration 328, loss = 0.34081327\n",
      "Iteration 329, loss = 0.34038697\n",
      "Iteration 330, loss = 0.33996209\n",
      "Iteration 331, loss = 0.33953837\n",
      "Iteration 332, loss = 0.33911572\n",
      "Iteration 333, loss = 0.33869411\n",
      "Iteration 334, loss = 0.33827357\n",
      "Iteration 335, loss = 0.33785404\n",
      "Iteration 336, loss = 0.33743557\n",
      "Iteration 337, loss = 0.33701811\n",
      "Iteration 338, loss = 0.33660168\n",
      "Iteration 339, loss = 0.33618630\n",
      "Iteration 340, loss = 0.33577190\n",
      "Iteration 341, loss = 0.33535860\n",
      "Iteration 342, loss = 0.33494628\n",
      "Iteration 343, loss = 0.33453500\n",
      "Iteration 344, loss = 0.33412474\n",
      "Iteration 345, loss = 0.33371544\n",
      "Iteration 346, loss = 0.33330717\n",
      "Iteration 347, loss = 0.33289983\n",
      "Iteration 348, loss = 0.33249345\n",
      "Iteration 349, loss = 0.33208801\n",
      "Iteration 350, loss = 0.33168357\n",
      "Iteration 351, loss = 0.33128009\n",
      "Iteration 352, loss = 0.33087759\n",
      "Iteration 353, loss = 0.33047612\n",
      "Iteration 354, loss = 0.33007561\n",
      "Iteration 355, loss = 0.32967597\n",
      "Iteration 356, loss = 0.32927730\n",
      "Iteration 357, loss = 0.32887957\n",
      "Iteration 358, loss = 0.32848283\n",
      "Iteration 359, loss = 0.32808696\n",
      "Iteration 360, loss = 0.32769204\n",
      "Iteration 361, loss = 0.32729804\n",
      "Iteration 362, loss = 0.32690494\n",
      "Iteration 363, loss = 0.32651274\n",
      "Iteration 364, loss = 0.32612146\n",
      "Iteration 365, loss = 0.32573110\n",
      "Iteration 366, loss = 0.32534159\n",
      "Iteration 367, loss = 0.32495298\n",
      "Iteration 368, loss = 0.32456520\n",
      "Iteration 369, loss = 0.32417836\n",
      "Iteration 370, loss = 0.32379262\n",
      "Iteration 371, loss = 0.32340769\n",
      "Iteration 372, loss = 0.32302355\n",
      "Iteration 373, loss = 0.32264047\n",
      "Iteration 374, loss = 0.32225815\n",
      "Iteration 375, loss = 0.32187673\n",
      "Iteration 376, loss = 0.32149613\n",
      "Iteration 377, loss = 0.32111648\n",
      "Iteration 378, loss = 0.32073766\n",
      "Iteration 379, loss = 0.32035961\n",
      "Iteration 380, loss = 0.31998254\n",
      "Iteration 381, loss = 0.31960624\n",
      "Iteration 382, loss = 0.31923078\n",
      "Iteration 383, loss = 0.31885624\n",
      "Iteration 384, loss = 0.31848241\n",
      "Iteration 385, loss = 0.31810945\n",
      "Iteration 386, loss = 0.31773738\n",
      "Iteration 387, loss = 0.31736607\n",
      "Iteration 388, loss = 0.31699558\n",
      "Iteration 389, loss = 0.31662590\n",
      "Iteration 390, loss = 0.31625716\n",
      "Iteration 391, loss = 0.31588907\n",
      "Iteration 392, loss = 0.31552186\n",
      "Iteration 393, loss = 0.31515566\n",
      "Iteration 394, loss = 0.31479049\n",
      "Iteration 395, loss = 0.31442610\n",
      "Iteration 396, loss = 0.31406250\n",
      "Iteration 397, loss = 0.31369988\n",
      "Iteration 398, loss = 0.31333788\n",
      "Iteration 399, loss = 0.31297676\n",
      "Iteration 400, loss = 0.31261653\n",
      "Iteration 401, loss = 0.31225701\n",
      "Iteration 402, loss = 0.31189824\n",
      "Iteration 403, loss = 0.31154024\n",
      "Iteration 404, loss = 0.31118319\n",
      "Iteration 405, loss = 0.31082669\n",
      "Iteration 406, loss = 0.31047106\n",
      "Iteration 407, loss = 0.31011631\n",
      "Iteration 408, loss = 0.30976229\n",
      "Iteration 409, loss = 0.30940902\n",
      "Iteration 410, loss = 0.30905642\n",
      "Iteration 411, loss = 0.30870477\n",
      "Iteration 412, loss = 0.30835374\n",
      "Iteration 413, loss = 0.30800349\n",
      "Iteration 414, loss = 0.30765409\n",
      "Iteration 415, loss = 0.30730535\n",
      "Iteration 416, loss = 0.30695733\n",
      "Iteration 417, loss = 0.30661006\n",
      "Iteration 418, loss = 0.30626367\n",
      "Iteration 419, loss = 0.30591787\n",
      "Iteration 420, loss = 0.30557286\n",
      "Iteration 421, loss = 0.30522861\n",
      "Iteration 422, loss = 0.30488515\n",
      "Iteration 423, loss = 0.30454231\n",
      "Iteration 424, loss = 0.30420020\n",
      "Iteration 425, loss = 0.30385886\n",
      "Iteration 426, loss = 0.30351830\n",
      "Iteration 427, loss = 0.30317838\n",
      "Iteration 428, loss = 0.30283914\n",
      "Iteration 429, loss = 0.30250080\n",
      "Iteration 430, loss = 0.30216295\n",
      "Iteration 431, loss = 0.30182588\n",
      "Iteration 432, loss = 0.30148958\n",
      "Iteration 433, loss = 0.30115390\n",
      "Iteration 434, loss = 0.30081894\n",
      "Iteration 435, loss = 0.30048465\n",
      "Iteration 436, loss = 0.30015114\n",
      "Iteration 437, loss = 0.29981825\n",
      "Iteration 438, loss = 0.29948600\n",
      "Iteration 439, loss = 0.29915449\n",
      "Iteration 440, loss = 0.29882374\n",
      "Iteration 441, loss = 0.29849362\n",
      "Iteration 442, loss = 0.29816411\n",
      "Iteration 443, loss = 0.29783528\n",
      "Iteration 444, loss = 0.29750732\n",
      "Iteration 445, loss = 0.29717974\n",
      "Iteration 446, loss = 0.29685300\n",
      "Iteration 447, loss = 0.29652696\n",
      "Iteration 448, loss = 0.29620154\n",
      "Iteration 449, loss = 0.29587675\n",
      "Iteration 450, loss = 0.29555260\n",
      "Iteration 451, loss = 0.29522924\n",
      "Iteration 452, loss = 0.29490642\n",
      "Iteration 453, loss = 0.29458425\n",
      "Iteration 454, loss = 0.29426292\n",
      "Iteration 455, loss = 0.29394204\n",
      "Iteration 456, loss = 0.29362186\n",
      "Iteration 457, loss = 0.29330235\n",
      "Iteration 458, loss = 0.29298356\n",
      "Iteration 459, loss = 0.29266535\n",
      "Iteration 460, loss = 0.29234775\n",
      "Iteration 461, loss = 0.29203080\n",
      "Iteration 462, loss = 0.29171464\n",
      "Iteration 463, loss = 0.29139890\n",
      "Iteration 464, loss = 0.29108392\n",
      "Iteration 465, loss = 0.29076961\n",
      "Iteration 466, loss = 0.29045585\n",
      "Iteration 467, loss = 0.29014277\n",
      "Iteration 468, loss = 0.28983025\n",
      "Iteration 469, loss = 0.28951851\n",
      "Iteration 470, loss = 0.28920724\n",
      "Iteration 471, loss = 0.28889664\n",
      "Iteration 472, loss = 0.28858679\n",
      "Iteration 473, loss = 0.28827742\n",
      "Iteration 474, loss = 0.28796873\n",
      "Iteration 475, loss = 0.28766065\n",
      "Iteration 476, loss = 0.28735330\n",
      "Iteration 477, loss = 0.28704646\n",
      "Iteration 478, loss = 0.28674024\n",
      "Iteration 479, loss = 0.28643459\n",
      "Iteration 480, loss = 0.28612974\n",
      "Iteration 481, loss = 0.28582527\n",
      "Iteration 482, loss = 0.28552154\n",
      "Iteration 483, loss = 0.28521842\n",
      "Iteration 484, loss = 0.28491588\n",
      "Iteration 485, loss = 0.28461393\n",
      "Iteration 486, loss = 0.28431256\n",
      "Iteration 487, loss = 0.28401187\n",
      "Iteration 488, loss = 0.28371169\n",
      "Iteration 489, loss = 0.28341214\n",
      "Iteration 490, loss = 0.28311325\n",
      "Iteration 491, loss = 0.28281486\n",
      "Iteration 492, loss = 0.28251712\n",
      "Iteration 493, loss = 0.28221991\n",
      "Iteration 494, loss = 0.28192346\n",
      "Iteration 495, loss = 0.28162739\n",
      "Iteration 496, loss = 0.28133196\n",
      "Iteration 497, loss = 0.28103723\n",
      "Iteration 498, loss = 0.28074300\n",
      "Iteration 499, loss = 0.28044934\n",
      "Iteration 500, loss = 0.28015625\n",
      "Iteration 501, loss = 0.27986387\n",
      "Iteration 502, loss = 0.27957192\n",
      "Iteration 503, loss = 0.27928059\n",
      "Iteration 504, loss = 0.27898987\n",
      "Iteration 505, loss = 0.27869976\n",
      "Iteration 506, loss = 0.27841018\n",
      "Iteration 507, loss = 0.27812111\n",
      "Iteration 508, loss = 0.27783268\n",
      "Iteration 509, loss = 0.27754487\n",
      "Iteration 510, loss = 0.27725753\n",
      "Iteration 511, loss = 0.27697081\n",
      "Iteration 512, loss = 0.27668479\n",
      "Iteration 513, loss = 0.27639917\n",
      "Iteration 514, loss = 0.27611413\n",
      "Iteration 515, loss = 0.27582976\n",
      "Iteration 516, loss = 0.27554591\n",
      "Iteration 517, loss = 0.27526256\n",
      "Iteration 518, loss = 0.27497979\n",
      "Iteration 519, loss = 0.27469769\n",
      "Iteration 520, loss = 0.27441600\n",
      "Iteration 521, loss = 0.27413489\n",
      "Iteration 522, loss = 0.27385440\n",
      "Iteration 523, loss = 0.27357446\n",
      "Iteration 524, loss = 0.27329501\n",
      "Iteration 525, loss = 0.27301616\n",
      "Iteration 526, loss = 0.27273781\n",
      "Iteration 527, loss = 0.27246009\n",
      "Iteration 528, loss = 0.27218284\n",
      "Iteration 529, loss = 0.27190614\n",
      "Iteration 530, loss = 0.27163006\n",
      "Iteration 531, loss = 0.27135443\n",
      "Iteration 532, loss = 0.27107937\n",
      "Iteration 533, loss = 0.27080489\n",
      "Iteration 534, loss = 0.27053091\n",
      "Iteration 535, loss = 0.27025751\n",
      "Iteration 536, loss = 0.26998455\n",
      "Iteration 537, loss = 0.26971223\n",
      "Iteration 538, loss = 0.26944041\n",
      "Iteration 539, loss = 0.26916909\n",
      "Iteration 540, loss = 0.26889830\n",
      "Iteration 541, loss = 0.26862815\n",
      "Iteration 542, loss = 0.26835843\n",
      "Iteration 543, loss = 0.26808923\n",
      "Iteration 544, loss = 0.26782054\n",
      "Iteration 545, loss = 0.26755250\n",
      "Iteration 546, loss = 0.26728490\n",
      "Iteration 547, loss = 0.26701778\n",
      "Iteration 548, loss = 0.26675133\n",
      "Iteration 549, loss = 0.26648523\n",
      "Iteration 550, loss = 0.26621974\n",
      "Iteration 551, loss = 0.26595478\n",
      "Iteration 552, loss = 0.26569034\n",
      "Iteration 553, loss = 0.26542636\n",
      "Iteration 554, loss = 0.26516292\n",
      "Iteration 555, loss = 0.26490005\n",
      "Iteration 556, loss = 0.26463761\n",
      "Iteration 557, loss = 0.26437574\n",
      "Iteration 558, loss = 0.26411431\n",
      "Iteration 559, loss = 0.26385353\n",
      "Iteration 560, loss = 0.26359319\n",
      "Iteration 561, loss = 0.26333332\n",
      "Iteration 562, loss = 0.26307396\n",
      "Iteration 563, loss = 0.26281535\n",
      "Iteration 564, loss = 0.26255689\n",
      "Iteration 565, loss = 0.26229921\n",
      "Iteration 566, loss = 0.26204193\n",
      "Iteration 567, loss = 0.26178516\n",
      "Iteration 568, loss = 0.26152888\n",
      "Iteration 569, loss = 0.26127308\n",
      "Iteration 570, loss = 0.26101793\n",
      "Iteration 571, loss = 0.26076309\n",
      "Iteration 572, loss = 0.26050879\n",
      "Iteration 573, loss = 0.26025512\n",
      "Iteration 574, loss = 0.26000184\n",
      "Iteration 575, loss = 0.25974903\n",
      "Iteration 576, loss = 0.25949673\n",
      "Iteration 577, loss = 0.25924494\n",
      "Iteration 578, loss = 0.25899363\n",
      "Iteration 579, loss = 0.25874281\n",
      "Iteration 580, loss = 0.25849249\n",
      "Iteration 581, loss = 0.25824270\n",
      "Iteration 582, loss = 0.25799333\n",
      "Iteration 583, loss = 0.25774443\n",
      "Iteration 584, loss = 0.25749608\n",
      "Iteration 585, loss = 0.25724819\n",
      "Iteration 586, loss = 0.25700074\n",
      "Iteration 587, loss = 0.25675379\n",
      "Iteration 588, loss = 0.25650741\n",
      "Iteration 589, loss = 0.25626142\n",
      "Iteration 590, loss = 0.25601593\n",
      "Iteration 591, loss = 0.25577084\n",
      "Iteration 592, loss = 0.25552646\n",
      "Iteration 593, loss = 0.25528226\n",
      "Iteration 594, loss = 0.25503869\n",
      "Iteration 595, loss = 0.25479563\n",
      "Iteration 596, loss = 0.25455294\n",
      "Iteration 597, loss = 0.25431077\n",
      "Iteration 598, loss = 0.25406901\n",
      "Iteration 599, loss = 0.25382790\n",
      "Iteration 600, loss = 0.25358705\n",
      "Iteration 601, loss = 0.25334673\n",
      "Iteration 602, loss = 0.25310697\n",
      "Iteration 603, loss = 0.25286760\n",
      "Iteration 604, loss = 0.25262867\n",
      "Iteration 605, loss = 0.25239022\n",
      "Iteration 606, loss = 0.25215227\n",
      "Iteration 607, loss = 0.25191475\n",
      "Iteration 608, loss = 0.25167767\n",
      "Iteration 609, loss = 0.25144109\n",
      "Iteration 610, loss = 0.25120502\n",
      "Iteration 611, loss = 0.25096930\n",
      "Iteration 612, loss = 0.25073407\n",
      "Iteration 613, loss = 0.25049939\n",
      "Iteration 614, loss = 0.25026503\n",
      "Iteration 615, loss = 0.25003121\n",
      "Iteration 616, loss = 0.24979780\n",
      "Iteration 617, loss = 0.24956492\n",
      "Iteration 618, loss = 0.24933241\n",
      "Iteration 619, loss = 0.24910034\n",
      "Iteration 620, loss = 0.24886879\n",
      "Iteration 621, loss = 0.24863766\n",
      "Iteration 622, loss = 0.24840694\n",
      "Iteration 623, loss = 0.24817669\n",
      "Iteration 624, loss = 0.24794697\n",
      "Iteration 625, loss = 0.24771757\n",
      "Iteration 626, loss = 0.24748862\n",
      "Iteration 627, loss = 0.24726025\n",
      "Iteration 628, loss = 0.24703218\n",
      "Iteration 629, loss = 0.24680462\n",
      "Iteration 630, loss = 0.24657744\n",
      "Iteration 631, loss = 0.24635082\n",
      "Iteration 632, loss = 0.24612453\n",
      "Iteration 633, loss = 0.24589868\n",
      "Iteration 634, loss = 0.24567332\n",
      "Iteration 635, loss = 0.24544840\n",
      "Iteration 636, loss = 0.24522387\n",
      "Iteration 637, loss = 0.24499979\n",
      "Iteration 638, loss = 0.24477614\n",
      "Iteration 639, loss = 0.24455298\n",
      "Iteration 640, loss = 0.24433019\n",
      "Iteration 641, loss = 0.24410780\n",
      "Iteration 642, loss = 0.24388599\n",
      "Iteration 643, loss = 0.24366443\n",
      "Iteration 644, loss = 0.24344336\n",
      "Iteration 645, loss = 0.24322280\n",
      "Iteration 646, loss = 0.24300262\n",
      "Iteration 647, loss = 0.24278283\n",
      "Iteration 648, loss = 0.24256349\n",
      "Iteration 649, loss = 0.24234467\n",
      "Iteration 650, loss = 0.24212614\n",
      "Iteration 651, loss = 0.24190805\n",
      "Iteration 652, loss = 0.24169043\n",
      "Iteration 653, loss = 0.24147325\n",
      "Iteration 654, loss = 0.24125641\n",
      "Iteration 655, loss = 0.24104006\n",
      "Iteration 656, loss = 0.24082409\n",
      "Iteration 657, loss = 0.24060857\n",
      "Iteration 658, loss = 0.24039341\n",
      "Iteration 659, loss = 0.24017870\n",
      "Iteration 660, loss = 0.23996446\n",
      "Iteration 661, loss = 0.23975054\n",
      "Iteration 662, loss = 0.23953705\n",
      "Iteration 663, loss = 0.23932403\n",
      "Iteration 664, loss = 0.23911140\n",
      "Iteration 665, loss = 0.23889915\n",
      "Iteration 666, loss = 0.23868727\n",
      "Iteration 667, loss = 0.23847593\n",
      "Iteration 668, loss = 0.23826489\n",
      "Iteration 669, loss = 0.23805426\n",
      "Iteration 670, loss = 0.23784408\n",
      "Iteration 671, loss = 0.23763432\n",
      "Iteration 672, loss = 0.23742494\n",
      "Iteration 673, loss = 0.23721593\n",
      "Iteration 674, loss = 0.23700732\n",
      "Iteration 675, loss = 0.23679919\n",
      "Iteration 676, loss = 0.23659140\n",
      "Iteration 677, loss = 0.23638399\n",
      "Iteration 678, loss = 0.23617707\n",
      "Iteration 679, loss = 0.23597047\n",
      "Iteration 680, loss = 0.23576428\n",
      "Iteration 681, loss = 0.23555856\n",
      "Iteration 682, loss = 0.23535317\n",
      "Iteration 683, loss = 0.23514822\n",
      "Iteration 684, loss = 0.23494359\n",
      "Iteration 685, loss = 0.23473946\n",
      "Iteration 686, loss = 0.23453577\n",
      "Iteration 687, loss = 0.23433239\n",
      "Iteration 688, loss = 0.23412946\n",
      "Iteration 689, loss = 0.23392706\n",
      "Iteration 690, loss = 0.23372490\n",
      "Iteration 691, loss = 0.23352319\n",
      "Iteration 692, loss = 0.23332182\n",
      "Iteration 693, loss = 0.23312102\n",
      "Iteration 694, loss = 0.23292036\n",
      "Iteration 695, loss = 0.23272023\n",
      "Iteration 696, loss = 0.23252053\n",
      "Iteration 697, loss = 0.23232114\n",
      "Iteration 698, loss = 0.23212213\n",
      "Iteration 699, loss = 0.23192348\n",
      "Iteration 700, loss = 0.23172532\n",
      "Iteration 701, loss = 0.23152742\n",
      "Iteration 702, loss = 0.23132995\n",
      "Iteration 703, loss = 0.23113294\n",
      "Iteration 704, loss = 0.23093622\n",
      "Iteration 705, loss = 0.23073985\n",
      "Iteration 706, loss = 0.23054393\n",
      "Iteration 707, loss = 0.23034837\n",
      "Iteration 708, loss = 0.23015316\n",
      "Iteration 709, loss = 0.22995835\n",
      "Iteration 710, loss = 0.22976396\n",
      "Iteration 711, loss = 0.22956992\n",
      "Iteration 712, loss = 0.22937622\n",
      "Iteration 713, loss = 0.22918305\n",
      "Iteration 714, loss = 0.22899003\n",
      "Iteration 715, loss = 0.22879752\n",
      "Iteration 716, loss = 0.22860538\n",
      "Iteration 717, loss = 0.22841359\n",
      "Iteration 718, loss = 0.22822211\n",
      "Iteration 719, loss = 0.22803108\n",
      "Iteration 720, loss = 0.22784038\n",
      "Iteration 721, loss = 0.22765002\n",
      "Iteration 722, loss = 0.22746007\n",
      "Iteration 723, loss = 0.22727050\n",
      "Iteration 724, loss = 0.22708123\n",
      "Iteration 725, loss = 0.22689237\n",
      "Iteration 726, loss = 0.22670390\n",
      "Iteration 727, loss = 0.22651573\n",
      "Iteration 728, loss = 0.22632789\n",
      "Iteration 729, loss = 0.22614059\n",
      "Iteration 730, loss = 0.22595340\n",
      "Iteration 731, loss = 0.22576675\n",
      "Iteration 732, loss = 0.22558039\n",
      "Iteration 733, loss = 0.22539438\n",
      "Iteration 734, loss = 0.22520868\n",
      "Iteration 735, loss = 0.22502349\n",
      "Iteration 736, loss = 0.22483847\n",
      "Iteration 737, loss = 0.22465388\n",
      "Iteration 738, loss = 0.22446972\n",
      "Iteration 739, loss = 0.22428585\n",
      "Iteration 740, loss = 0.22410228\n",
      "Iteration 741, loss = 0.22391908\n",
      "Iteration 742, loss = 0.22373630\n",
      "Iteration 743, loss = 0.22355378\n",
      "Iteration 744, loss = 0.22337166\n",
      "Iteration 745, loss = 0.22318989\n",
      "Iteration 746, loss = 0.22300845\n",
      "Iteration 747, loss = 0.22282734\n",
      "Iteration 748, loss = 0.22264665\n",
      "Iteration 749, loss = 0.22246621\n",
      "Iteration 750, loss = 0.22228615\n",
      "Iteration 751, loss = 0.22210648\n",
      "Iteration 752, loss = 0.22192712\n",
      "Iteration 753, loss = 0.22174805\n",
      "Iteration 754, loss = 0.22156946\n",
      "Iteration 755, loss = 0.22139102\n",
      "Iteration 756, loss = 0.22121309\n",
      "Iteration 757, loss = 0.22103546\n",
      "Iteration 758, loss = 0.22085808\n",
      "Iteration 759, loss = 0.22068106\n",
      "Iteration 760, loss = 0.22050442\n",
      "Iteration 761, loss = 0.22032808\n",
      "Iteration 762, loss = 0.22015208\n",
      "Iteration 763, loss = 0.21997648\n",
      "Iteration 764, loss = 0.21980114\n",
      "Iteration 765, loss = 0.21962615\n",
      "Iteration 766, loss = 0.21945158\n",
      "Iteration 767, loss = 0.21927719\n",
      "Iteration 768, loss = 0.21910326\n",
      "Iteration 769, loss = 0.21892960\n",
      "Iteration 770, loss = 0.21875628\n",
      "Iteration 771, loss = 0.21858326\n",
      "Iteration 772, loss = 0.21841068\n",
      "Iteration 773, loss = 0.21823830\n",
      "Iteration 774, loss = 0.21806627\n",
      "Iteration 775, loss = 0.21789469\n",
      "Iteration 776, loss = 0.21772324\n",
      "Iteration 777, loss = 0.21755229\n",
      "Iteration 778, loss = 0.21738158\n",
      "Iteration 779, loss = 0.21721118\n",
      "Iteration 780, loss = 0.21704107\n",
      "Iteration 781, loss = 0.21687144\n",
      "Iteration 782, loss = 0.21670192\n",
      "Iteration 783, loss = 0.21653289\n",
      "Iteration 784, loss = 0.21636412\n",
      "Iteration 785, loss = 0.21619562\n",
      "Iteration 786, loss = 0.21602748\n",
      "Iteration 787, loss = 0.21585969\n",
      "Iteration 788, loss = 0.21569217\n",
      "Iteration 789, loss = 0.21552499\n",
      "Iteration 790, loss = 0.21535812\n",
      "Iteration 791, loss = 0.21519153\n",
      "Iteration 792, loss = 0.21502537\n",
      "Iteration 793, loss = 0.21485941\n",
      "Iteration 794, loss = 0.21469379\n",
      "Iteration 795, loss = 0.21452856\n",
      "Iteration 796, loss = 0.21436355\n",
      "Iteration 797, loss = 0.21419886\n",
      "Iteration 798, loss = 0.21403455\n",
      "Iteration 799, loss = 0.21387044\n",
      "Iteration 800, loss = 0.21370674\n",
      "Iteration 801, loss = 0.21354331\n",
      "Iteration 802, loss = 0.21338018\n",
      "Iteration 803, loss = 0.21321734\n",
      "Iteration 804, loss = 0.21305490\n",
      "Iteration 805, loss = 0.21289268\n",
      "Iteration 806, loss = 0.21273078\n",
      "Iteration 807, loss = 0.21256926\n",
      "Iteration 808, loss = 0.21240793\n",
      "Iteration 809, loss = 0.21224703\n",
      "Iteration 810, loss = 0.21208633\n",
      "Iteration 811, loss = 0.21192592\n",
      "Iteration 812, loss = 0.21176599\n",
      "Iteration 813, loss = 0.21160612\n",
      "Iteration 814, loss = 0.21144672\n",
      "Iteration 815, loss = 0.21128756\n",
      "Iteration 816, loss = 0.21112867\n",
      "Iteration 817, loss = 0.21097010\n",
      "Iteration 818, loss = 0.21081189\n",
      "Iteration 819, loss = 0.21065391\n",
      "Iteration 820, loss = 0.21049619\n",
      "Iteration 821, loss = 0.21033889\n",
      "Iteration 822, loss = 0.21018177\n",
      "Iteration 823, loss = 0.21002500\n",
      "Iteration 824, loss = 0.20986853\n",
      "Iteration 825, loss = 0.20971229\n",
      "Iteration 826, loss = 0.20955647\n",
      "Iteration 827, loss = 0.20940079\n",
      "Iteration 828, loss = 0.20924551\n",
      "Iteration 829, loss = 0.20909051\n",
      "Iteration 830, loss = 0.20893572\n",
      "Iteration 831, loss = 0.20878132\n",
      "Iteration 832, loss = 0.20862718\n",
      "Iteration 833, loss = 0.20847328\n",
      "Iteration 834, loss = 0.20831974\n",
      "Iteration 835, loss = 0.20816644\n",
      "Iteration 836, loss = 0.20801342\n",
      "Iteration 837, loss = 0.20786082\n",
      "Iteration 838, loss = 0.20770830\n",
      "Iteration 839, loss = 0.20755625\n",
      "Iteration 840, loss = 0.20740439\n",
      "Iteration 841, loss = 0.20725279\n",
      "Iteration 842, loss = 0.20710154\n",
      "Iteration 843, loss = 0.20695054\n",
      "Iteration 844, loss = 0.20679981\n",
      "Iteration 845, loss = 0.20664943\n",
      "Iteration 846, loss = 0.20649924\n",
      "Iteration 847, loss = 0.20634938\n",
      "Iteration 848, loss = 0.20619982\n",
      "Iteration 849, loss = 0.20605050\n",
      "Iteration 850, loss = 0.20590143\n",
      "Iteration 851, loss = 0.20575273\n",
      "Iteration 852, loss = 0.20560423\n",
      "Iteration 853, loss = 0.20545603\n",
      "Iteration 854, loss = 0.20530812\n",
      "Iteration 855, loss = 0.20516043\n",
      "Iteration 856, loss = 0.20501313\n",
      "Iteration 857, loss = 0.20486602\n",
      "Iteration 858, loss = 0.20471915\n",
      "Iteration 859, loss = 0.20457270\n",
      "Iteration 860, loss = 0.20442640\n",
      "Iteration 861, loss = 0.20428043\n",
      "Iteration 862, loss = 0.20413472\n",
      "Iteration 863, loss = 0.20398922\n",
      "Iteration 864, loss = 0.20384412\n",
      "Iteration 865, loss = 0.20369916\n",
      "Iteration 866, loss = 0.20355456\n",
      "Iteration 867, loss = 0.20341019\n",
      "Iteration 868, loss = 0.20326605\n",
      "Iteration 869, loss = 0.20312230\n",
      "Iteration 870, loss = 0.20297869\n",
      "Iteration 871, loss = 0.20283543\n",
      "Iteration 872, loss = 0.20269239\n",
      "Iteration 873, loss = 0.20254962\n",
      "Iteration 874, loss = 0.20240712\n",
      "Iteration 875, loss = 0.20226490\n",
      "Iteration 876, loss = 0.20212289\n",
      "Iteration 877, loss = 0.20198127\n",
      "Iteration 878, loss = 0.20183973\n",
      "Iteration 879, loss = 0.20169865\n",
      "Iteration 880, loss = 0.20155771\n",
      "Iteration 881, loss = 0.20141702\n",
      "Iteration 882, loss = 0.20127657\n",
      "Iteration 883, loss = 0.20113647\n",
      "Iteration 884, loss = 0.20099654\n",
      "Iteration 885, loss = 0.20085701\n",
      "Iteration 886, loss = 0.20071754\n",
      "Iteration 887, loss = 0.20057852\n",
      "Iteration 888, loss = 0.20043963\n",
      "Iteration 889, loss = 0.20030099\n",
      "Iteration 890, loss = 0.20016269\n",
      "Iteration 891, loss = 0.20002454\n",
      "Iteration 892, loss = 0.19988676\n",
      "Iteration 893, loss = 0.19974914\n",
      "Iteration 894, loss = 0.19961179\n",
      "Iteration 895, loss = 0.19947481\n",
      "Iteration 896, loss = 0.19933797\n",
      "Iteration 897, loss = 0.19920134\n",
      "Iteration 898, loss = 0.19906514\n",
      "Iteration 899, loss = 0.19892898\n",
      "Iteration 900, loss = 0.19879321\n",
      "Iteration 901, loss = 0.19865762\n",
      "Iteration 902, loss = 0.19852226\n",
      "Iteration 903, loss = 0.19838726\n",
      "Iteration 904, loss = 0.19825242\n",
      "Iteration 905, loss = 0.19811783\n",
      "Iteration 906, loss = 0.19798351\n",
      "Iteration 907, loss = 0.19784941\n",
      "Iteration 908, loss = 0.19771562\n",
      "Iteration 909, loss = 0.19758201\n",
      "Iteration 910, loss = 0.19744867\n",
      "Iteration 911, loss = 0.19731554\n",
      "Iteration 912, loss = 0.19718269\n",
      "Iteration 913, loss = 0.19705012\n",
      "Iteration 914, loss = 0.19691770\n",
      "Iteration 915, loss = 0.19678558\n",
      "Iteration 916, loss = 0.19665369\n",
      "Iteration 917, loss = 0.19652200\n",
      "Iteration 918, loss = 0.19639068\n",
      "Iteration 919, loss = 0.19625947\n",
      "Iteration 920, loss = 0.19612852\n",
      "Iteration 921, loss = 0.19599786\n",
      "Iteration 922, loss = 0.19586735\n",
      "Iteration 923, loss = 0.19573720\n",
      "Iteration 924, loss = 0.19560721\n",
      "Iteration 925, loss = 0.19547747\n",
      "Iteration 926, loss = 0.19534795\n",
      "Iteration 927, loss = 0.19521887\n",
      "Iteration 928, loss = 0.19509016\n",
      "Iteration 929, loss = 0.19496163\n",
      "Iteration 930, loss = 0.19483341\n",
      "Iteration 931, loss = 0.19470538\n",
      "Iteration 932, loss = 0.19457763\n",
      "Iteration 933, loss = 0.19445015\n",
      "Iteration 934, loss = 0.19432282\n",
      "Iteration 935, loss = 0.19419588\n",
      "Iteration 936, loss = 0.19406902\n",
      "Iteration 937, loss = 0.19394255\n",
      "Iteration 938, loss = 0.19381621\n",
      "Iteration 939, loss = 0.19369007\n",
      "Iteration 940, loss = 0.19356430\n",
      "Iteration 941, loss = 0.19343862\n",
      "Iteration 942, loss = 0.19331327\n",
      "Iteration 943, loss = 0.19318809\n",
      "Iteration 944, loss = 0.19306316\n",
      "Iteration 945, loss = 0.19293846\n",
      "Iteration 946, loss = 0.19281398\n",
      "Iteration 947, loss = 0.19268976\n",
      "Iteration 948, loss = 0.19256568\n",
      "Iteration 949, loss = 0.19244194\n",
      "Iteration 950, loss = 0.19231829\n",
      "Iteration 951, loss = 0.19219500\n",
      "Iteration 952, loss = 0.19207180\n",
      "Iteration 953, loss = 0.19194895\n",
      "Iteration 954, loss = 0.19182625\n",
      "Iteration 955, loss = 0.19170378\n",
      "Iteration 956, loss = 0.19158161\n",
      "Iteration 957, loss = 0.19145952\n",
      "Iteration 958, loss = 0.19133779\n",
      "Iteration 959, loss = 0.19121615\n",
      "Iteration 960, loss = 0.19109483\n",
      "Iteration 961, loss = 0.19097371\n",
      "Iteration 962, loss = 0.19085276\n",
      "Iteration 963, loss = 0.19073215\n",
      "Iteration 964, loss = 0.19061159\n",
      "Iteration 965, loss = 0.19049144\n",
      "Iteration 966, loss = 0.19037140\n",
      "Iteration 967, loss = 0.19025154\n",
      "Iteration 968, loss = 0.19013213\n",
      "Iteration 969, loss = 0.19001272\n",
      "Iteration 970, loss = 0.18989365\n",
      "Iteration 971, loss = 0.18977485\n",
      "Iteration 972, loss = 0.18965622\n",
      "Iteration 973, loss = 0.18953780\n",
      "Iteration 974, loss = 0.18941959\n",
      "Iteration 975, loss = 0.18930158\n",
      "Iteration 976, loss = 0.18918378\n",
      "Iteration 977, loss = 0.18906619\n",
      "Iteration 978, loss = 0.18894891\n",
      "Iteration 979, loss = 0.18883169\n",
      "Iteration 980, loss = 0.18871476\n",
      "Iteration 981, loss = 0.18859804\n",
      "Iteration 982, loss = 0.18848155\n",
      "Iteration 983, loss = 0.18836524\n",
      "Iteration 984, loss = 0.18824915\n",
      "Iteration 985, loss = 0.18813324\n",
      "Iteration 986, loss = 0.18801753\n",
      "Iteration 987, loss = 0.18790205\n",
      "Iteration 988, loss = 0.18778686\n",
      "Iteration 989, loss = 0.18767169\n",
      "Iteration 990, loss = 0.18755684\n",
      "Iteration 991, loss = 0.18744218\n",
      "Iteration 992, loss = 0.18732775\n",
      "Iteration 993, loss = 0.18721349\n",
      "Iteration 994, loss = 0.18709945\n",
      "Iteration 995, loss = 0.18698559\n",
      "Iteration 996, loss = 0.18687192\n",
      "Iteration 997, loss = 0.18675845\n",
      "Iteration 998, loss = 0.18664524\n",
      "Iteration 999, loss = 0.18653218\n",
      "Iteration 1000, loss = 0.18641932\n",
      "Iteration 1, loss = 1.35233713\n",
      "Iteration 2, loss = 1.32914641\n",
      "Iteration 3, loss = 1.29803045\n",
      "Iteration 4, loss = 1.26127742\n",
      "Iteration 5, loss = 1.22088319\n",
      "Iteration 6, loss = 1.17847925\n",
      "Iteration 7, loss = 1.13524232\n",
      "Iteration 8, loss = 1.09218546\n",
      "Iteration 9, loss = 1.05030394\n",
      "Iteration 10, loss = 1.01061173\n",
      "Iteration 11, loss = 0.97433069\n",
      "Iteration 12, loss = 0.94227541\n",
      "Iteration 13, loss = 0.91517137\n",
      "Iteration 14, loss = 0.89331221\n",
      "Iteration 15, loss = 0.87672411\n",
      "Iteration 16, loss = 0.86475889\n",
      "Iteration 17, loss = 0.85638016\n",
      "Iteration 18, loss = 0.85039374\n",
      "Iteration 19, loss = 0.84533458\n",
      "Iteration 20, loss = 0.84000697\n",
      "Iteration 21, loss = 0.83359304\n",
      "Iteration 22, loss = 0.82558508\n",
      "Iteration 23, loss = 0.81606064\n",
      "Iteration 24, loss = 0.80542630\n",
      "Iteration 25, loss = 0.79451850\n",
      "Iteration 26, loss = 0.78348353\n",
      "Iteration 27, loss = 0.77244750\n",
      "Iteration 28, loss = 0.76152425\n",
      "Iteration 29, loss = 0.75092757\n",
      "Iteration 30, loss = 0.74097324\n",
      "Iteration 31, loss = 0.73185174\n",
      "Iteration 32, loss = 0.72361954\n",
      "Iteration 33, loss = 0.71623018\n",
      "Iteration 34, loss = 0.70952809\n",
      "Iteration 35, loss = 0.70344600\n",
      "Iteration 36, loss = 0.69782464\n",
      "Iteration 37, loss = 0.69252918\n",
      "Iteration 38, loss = 0.68747814\n",
      "Iteration 39, loss = 0.68258918\n",
      "Iteration 40, loss = 0.67783339\n",
      "Iteration 41, loss = 0.67316289\n",
      "Iteration 42, loss = 0.66853474\n",
      "Iteration 43, loss = 0.66395034\n",
      "Iteration 44, loss = 0.65942748\n",
      "Iteration 45, loss = 0.65497510\n",
      "Iteration 46, loss = 0.65058549\n",
      "Iteration 47, loss = 0.64625485\n",
      "Iteration 48, loss = 0.64198909\n",
      "Iteration 49, loss = 0.63779874\n",
      "Iteration 50, loss = 0.63369580\n",
      "Iteration 51, loss = 0.62968261\n",
      "Iteration 52, loss = 0.62575691\n",
      "Iteration 53, loss = 0.62193464\n",
      "Iteration 54, loss = 0.61822363\n",
      "Iteration 55, loss = 0.61462222\n",
      "Iteration 56, loss = 0.61113438\n",
      "Iteration 57, loss = 0.60782734\n",
      "Iteration 58, loss = 0.60470550\n",
      "Iteration 59, loss = 0.60173750\n",
      "Iteration 60, loss = 0.59885551\n",
      "Iteration 61, loss = 0.59607587\n",
      "Iteration 62, loss = 0.59333847\n",
      "Iteration 63, loss = 0.59062721\n",
      "Iteration 64, loss = 0.58794047\n",
      "Iteration 65, loss = 0.58526826\n",
      "Iteration 66, loss = 0.58261339\n",
      "Iteration 67, loss = 0.57997770\n",
      "Iteration 68, loss = 0.57736285\n",
      "Iteration 69, loss = 0.57477415\n",
      "Iteration 70, loss = 0.57221913\n",
      "Iteration 71, loss = 0.56969506\n",
      "Iteration 72, loss = 0.56721550\n",
      "Iteration 73, loss = 0.56477325\n",
      "Iteration 74, loss = 0.56238787\n",
      "Iteration 75, loss = 0.56006437\n",
      "Iteration 76, loss = 0.55777732\n",
      "Iteration 77, loss = 0.55552563\n",
      "Iteration 78, loss = 0.55331963\n",
      "Iteration 79, loss = 0.55115518\n",
      "Iteration 80, loss = 0.54902496\n",
      "Iteration 81, loss = 0.54692453\n",
      "Iteration 82, loss = 0.54485463\n",
      "Iteration 83, loss = 0.54281281\n",
      "Iteration 84, loss = 0.54080046\n",
      "Iteration 85, loss = 0.53881264\n",
      "Iteration 86, loss = 0.53684943\n",
      "Iteration 87, loss = 0.53491072\n",
      "Iteration 88, loss = 0.53299607\n",
      "Iteration 89, loss = 0.53110545\n",
      "Iteration 90, loss = 0.52923947\n",
      "Iteration 91, loss = 0.52739916\n",
      "Iteration 92, loss = 0.52558283\n",
      "Iteration 93, loss = 0.52378987\n",
      "Iteration 94, loss = 0.52202135\n",
      "Iteration 95, loss = 0.52027667\n",
      "Iteration 96, loss = 0.51855581\n",
      "Iteration 97, loss = 0.51685670\n",
      "Iteration 98, loss = 0.51517889\n",
      "Iteration 99, loss = 0.51352287\n",
      "Iteration 100, loss = 0.51188731\n",
      "Iteration 101, loss = 0.51027293\n",
      "Iteration 102, loss = 0.50867855\n",
      "Iteration 103, loss = 0.50710272\n",
      "Iteration 104, loss = 0.50554523\n",
      "Iteration 105, loss = 0.50400547\n",
      "Iteration 106, loss = 0.50248279\n",
      "Iteration 107, loss = 0.50097772\n",
      "Iteration 108, loss = 0.49948945\n",
      "Iteration 109, loss = 0.49801831\n",
      "Iteration 110, loss = 0.49656444\n",
      "Iteration 111, loss = 0.49512832\n",
      "Iteration 112, loss = 0.49370813\n",
      "Iteration 113, loss = 0.49230397\n",
      "Iteration 114, loss = 0.49091571\n",
      "Iteration 115, loss = 0.48954248\n",
      "Iteration 116, loss = 0.48818393\n",
      "Iteration 117, loss = 0.48684050\n",
      "Iteration 118, loss = 0.48551120\n",
      "Iteration 119, loss = 0.48419619\n",
      "Iteration 120, loss = 0.48289473\n",
      "Iteration 121, loss = 0.48160643\n",
      "Iteration 122, loss = 0.48033114\n",
      "Iteration 123, loss = 0.47906910\n",
      "Iteration 124, loss = 0.47781921\n",
      "Iteration 125, loss = 0.47658144\n",
      "Iteration 126, loss = 0.47535554\n",
      "Iteration 127, loss = 0.47414156\n",
      "Iteration 128, loss = 0.47293895\n",
      "Iteration 129, loss = 0.47174795\n",
      "Iteration 130, loss = 0.47056847\n",
      "Iteration 131, loss = 0.46939977\n",
      "Iteration 132, loss = 0.46824183\n",
      "Iteration 133, loss = 0.46709454\n",
      "Iteration 134, loss = 0.46595789\n",
      "Iteration 135, loss = 0.46482987\n",
      "Iteration 136, loss = 0.46371191\n",
      "Iteration 137, loss = 0.46260266\n",
      "Iteration 138, loss = 0.46150130\n",
      "Iteration 139, loss = 0.46040726\n",
      "Iteration 140, loss = 0.45932195\n",
      "Iteration 141, loss = 0.45824473\n",
      "Iteration 142, loss = 0.45717570\n",
      "Iteration 143, loss = 0.45611287\n",
      "Iteration 144, loss = 0.45505755\n",
      "Iteration 145, loss = 0.45400872\n",
      "Iteration 146, loss = 0.45296764\n",
      "Iteration 147, loss = 0.45193099\n",
      "Iteration 148, loss = 0.45089998\n",
      "Iteration 149, loss = 0.44987492\n",
      "Iteration 150, loss = 0.44885492\n",
      "Iteration 151, loss = 0.44783867\n",
      "Iteration 152, loss = 0.44682407\n",
      "Iteration 153, loss = 0.44581451\n",
      "Iteration 154, loss = 0.44480812\n",
      "Iteration 155, loss = 0.44380256\n",
      "Iteration 156, loss = 0.44279931\n",
      "Iteration 157, loss = 0.44179676\n",
      "Iteration 158, loss = 0.44079325\n",
      "Iteration 159, loss = 0.43978929\n",
      "Iteration 160, loss = 0.43878677\n",
      "Iteration 161, loss = 0.43778549\n",
      "Iteration 162, loss = 0.43678630\n",
      "Iteration 163, loss = 0.43578828\n",
      "Iteration 164, loss = 0.43479388\n",
      "Iteration 165, loss = 0.43378917\n",
      "Iteration 166, loss = 0.43277305\n",
      "Iteration 167, loss = 0.43175456\n",
      "Iteration 168, loss = 0.43073106\n",
      "Iteration 169, loss = 0.42970438\n",
      "Iteration 170, loss = 0.42867134\n",
      "Iteration 171, loss = 0.42764384\n",
      "Iteration 172, loss = 0.42661927\n",
      "Iteration 173, loss = 0.42559376\n",
      "Iteration 174, loss = 0.42456943\n",
      "Iteration 175, loss = 0.42354952\n",
      "Iteration 176, loss = 0.42255812\n",
      "Iteration 177, loss = 0.42157879\n",
      "Iteration 178, loss = 0.42061625\n",
      "Iteration 179, loss = 0.41967038\n",
      "Iteration 180, loss = 0.41873777\n",
      "Iteration 181, loss = 0.41781403\n",
      "Iteration 182, loss = 0.41690748\n",
      "Iteration 183, loss = 0.41602093\n",
      "Iteration 184, loss = 0.41515414\n",
      "Iteration 185, loss = 0.41430527\n",
      "Iteration 186, loss = 0.41346806\n",
      "Iteration 187, loss = 0.41264231\n",
      "Iteration 188, loss = 0.41182685\n",
      "Iteration 189, loss = 0.41102353\n",
      "Iteration 190, loss = 0.41023456\n",
      "Iteration 191, loss = 0.40945696\n",
      "Iteration 192, loss = 0.40868628\n",
      "Iteration 193, loss = 0.40792327\n",
      "Iteration 194, loss = 0.40717123\n",
      "Iteration 195, loss = 0.40643030\n",
      "Iteration 196, loss = 0.40569603\n",
      "Iteration 197, loss = 0.40496818\n",
      "Iteration 198, loss = 0.40424657\n",
      "Iteration 199, loss = 0.40352927\n",
      "Iteration 200, loss = 0.40281640\n",
      "Iteration 201, loss = 0.40210840\n",
      "Iteration 202, loss = 0.40140441\n",
      "Iteration 203, loss = 0.40070409\n",
      "Iteration 204, loss = 0.40000732\n",
      "Iteration 205, loss = 0.39931411\n",
      "Iteration 206, loss = 0.39862489\n",
      "Iteration 207, loss = 0.39793934\n",
      "Iteration 208, loss = 0.39725725\n",
      "Iteration 209, loss = 0.39657910\n",
      "Iteration 210, loss = 0.39590428\n",
      "Iteration 211, loss = 0.39523281\n",
      "Iteration 212, loss = 0.39456438\n",
      "Iteration 213, loss = 0.39389902\n",
      "Iteration 214, loss = 0.39323674\n",
      "Iteration 215, loss = 0.39257769\n",
      "Iteration 216, loss = 0.39192149\n",
      "Iteration 217, loss = 0.39126849\n",
      "Iteration 218, loss = 0.39061824\n",
      "Iteration 219, loss = 0.38997110\n",
      "Iteration 220, loss = 0.38932675\n",
      "Iteration 221, loss = 0.38868521\n",
      "Iteration 222, loss = 0.38804667\n",
      "Iteration 223, loss = 0.38741063\n",
      "Iteration 224, loss = 0.38677760\n",
      "Iteration 225, loss = 0.38614703\n",
      "Iteration 226, loss = 0.38551919\n",
      "Iteration 227, loss = 0.38489425\n",
      "Iteration 228, loss = 0.38427166\n",
      "Iteration 229, loss = 0.38365177\n",
      "Iteration 230, loss = 0.38303432\n",
      "Iteration 231, loss = 0.38241954\n",
      "Iteration 232, loss = 0.38180711\n",
      "Iteration 233, loss = 0.38119736\n",
      "Iteration 234, loss = 0.38058993\n",
      "Iteration 235, loss = 0.37998524\n",
      "Iteration 236, loss = 0.37938316\n",
      "Iteration 237, loss = 0.37878343\n",
      "Iteration 238, loss = 0.37818609\n",
      "Iteration 239, loss = 0.37759096\n",
      "Iteration 240, loss = 0.37699836\n",
      "Iteration 241, loss = 0.37640791\n",
      "Iteration 242, loss = 0.37581977\n",
      "Iteration 243, loss = 0.37523400\n",
      "Iteration 244, loss = 0.37465044\n",
      "Iteration 245, loss = 0.37406907\n",
      "Iteration 246, loss = 0.37348984\n",
      "Iteration 247, loss = 0.37291282\n",
      "Iteration 248, loss = 0.37233800\n",
      "Iteration 249, loss = 0.37176527\n",
      "Iteration 250, loss = 0.37119467\n",
      "Iteration 251, loss = 0.37062615\n",
      "Iteration 252, loss = 0.37005968\n",
      "Iteration 253, loss = 0.36949529\n",
      "Iteration 254, loss = 0.36893283\n",
      "Iteration 255, loss = 0.36837244\n",
      "Iteration 256, loss = 0.36781402\n",
      "Iteration 257, loss = 0.36725755\n",
      "Iteration 258, loss = 0.36670302\n",
      "Iteration 259, loss = 0.36615048\n",
      "Iteration 260, loss = 0.36559999\n",
      "Iteration 261, loss = 0.36505140\n",
      "Iteration 262, loss = 0.36450468\n",
      "Iteration 263, loss = 0.36395983\n",
      "Iteration 264, loss = 0.36341679\n",
      "Iteration 265, loss = 0.36287558\n",
      "Iteration 266, loss = 0.36233624\n",
      "Iteration 267, loss = 0.36179868\n",
      "Iteration 268, loss = 0.36126294\n",
      "Iteration 269, loss = 0.36072898\n",
      "Iteration 270, loss = 0.36019692\n",
      "Iteration 271, loss = 0.35966653\n",
      "Iteration 272, loss = 0.35913786\n",
      "Iteration 273, loss = 0.35861089\n",
      "Iteration 274, loss = 0.35808565\n",
      "Iteration 275, loss = 0.35756212\n",
      "Iteration 276, loss = 0.35704028\n",
      "Iteration 277, loss = 0.35652015\n",
      "Iteration 278, loss = 0.35600162\n",
      "Iteration 279, loss = 0.35548473\n",
      "Iteration 280, loss = 0.35497023\n",
      "Iteration 281, loss = 0.35445773\n",
      "Iteration 282, loss = 0.35394691\n",
      "Iteration 283, loss = 0.35343781\n",
      "Iteration 284, loss = 0.35293026\n",
      "Iteration 285, loss = 0.35242433\n",
      "Iteration 286, loss = 0.35191997\n",
      "Iteration 287, loss = 0.35141707\n",
      "Iteration 288, loss = 0.35091569\n",
      "Iteration 289, loss = 0.35041583\n",
      "Iteration 290, loss = 0.34991746\n",
      "Iteration 291, loss = 0.34942060\n",
      "Iteration 292, loss = 0.34892522\n",
      "Iteration 293, loss = 0.34843130\n",
      "Iteration 294, loss = 0.34793883\n",
      "Iteration 295, loss = 0.34744784\n",
      "Iteration 296, loss = 0.34695827\n",
      "Iteration 297, loss = 0.34647021\n",
      "Iteration 298, loss = 0.34598354\n",
      "Iteration 299, loss = 0.34549829\n",
      "Iteration 300, loss = 0.34501445\n",
      "Iteration 301, loss = 0.34453204\n",
      "Iteration 302, loss = 0.34405103\n",
      "Iteration 303, loss = 0.34357133\n",
      "Iteration 304, loss = 0.34309299\n",
      "Iteration 305, loss = 0.34261602\n",
      "Iteration 306, loss = 0.34214040\n",
      "Iteration 307, loss = 0.34166619\n",
      "Iteration 308, loss = 0.34119353\n",
      "Iteration 309, loss = 0.34072217\n",
      "Iteration 310, loss = 0.34025216\n",
      "Iteration 311, loss = 0.33978339\n",
      "Iteration 312, loss = 0.33931600\n",
      "Iteration 313, loss = 0.33884979\n",
      "Iteration 314, loss = 0.33838498\n",
      "Iteration 315, loss = 0.33792146\n",
      "Iteration 316, loss = 0.33745916\n",
      "Iteration 317, loss = 0.33699814\n",
      "Iteration 318, loss = 0.33653840\n",
      "Iteration 319, loss = 0.33607991\n",
      "Iteration 320, loss = 0.33562283\n",
      "Iteration 321, loss = 0.33516701\n",
      "Iteration 322, loss = 0.33471241\n",
      "Iteration 323, loss = 0.33425907\n",
      "Iteration 324, loss = 0.33380690\n",
      "Iteration 325, loss = 0.33335589\n",
      "Iteration 326, loss = 0.33290608\n",
      "Iteration 327, loss = 0.33245750\n",
      "Iteration 328, loss = 0.33201006\n",
      "Iteration 329, loss = 0.33156381\n",
      "Iteration 330, loss = 0.33111883\n",
      "Iteration 331, loss = 0.33067502\n",
      "Iteration 332, loss = 0.33023234\n",
      "Iteration 333, loss = 0.32979083\n",
      "Iteration 334, loss = 0.32935048\n",
      "Iteration 335, loss = 0.32891127\n",
      "Iteration 336, loss = 0.32847337\n",
      "Iteration 337, loss = 0.32803653\n",
      "Iteration 338, loss = 0.32760087\n",
      "Iteration 339, loss = 0.32716638\n",
      "Iteration 340, loss = 0.32673292\n",
      "Iteration 341, loss = 0.32630063\n",
      "Iteration 342, loss = 0.32586943\n",
      "Iteration 343, loss = 0.32543925\n",
      "Iteration 344, loss = 0.32501023\n",
      "Iteration 345, loss = 0.32458230\n",
      "Iteration 346, loss = 0.32415540\n",
      "Iteration 347, loss = 0.32372961\n",
      "Iteration 348, loss = 0.32330486\n",
      "Iteration 349, loss = 0.32288115\n",
      "Iteration 350, loss = 0.32245860\n",
      "Iteration 351, loss = 0.32203693\n",
      "Iteration 352, loss = 0.32161645\n",
      "Iteration 353, loss = 0.32119693\n",
      "Iteration 354, loss = 0.32077843\n",
      "Iteration 355, loss = 0.32036101\n",
      "Iteration 356, loss = 0.31994459\n",
      "Iteration 357, loss = 0.31952919\n",
      "Iteration 358, loss = 0.31911480\n",
      "Iteration 359, loss = 0.31870144\n",
      "Iteration 360, loss = 0.31828906\n",
      "Iteration 361, loss = 0.31787767\n",
      "Iteration 362, loss = 0.31746730\n",
      "Iteration 363, loss = 0.31705795\n",
      "Iteration 364, loss = 0.31664952\n",
      "Iteration 365, loss = 0.31624215\n",
      "Iteration 366, loss = 0.31583574\n",
      "Iteration 367, loss = 0.31543028\n",
      "Iteration 368, loss = 0.31502579\n",
      "Iteration 369, loss = 0.31462224\n",
      "Iteration 370, loss = 0.31421974\n",
      "Iteration 371, loss = 0.31381811\n",
      "Iteration 372, loss = 0.31341744\n",
      "Iteration 373, loss = 0.31301774\n",
      "Iteration 374, loss = 0.31261896\n",
      "Iteration 375, loss = 0.31222116\n",
      "Iteration 376, loss = 0.31182424\n",
      "Iteration 377, loss = 0.31142831\n",
      "Iteration 378, loss = 0.31103323\n",
      "Iteration 379, loss = 0.31063916\n",
      "Iteration 380, loss = 0.31024596\n",
      "Iteration 381, loss = 0.30985366\n",
      "Iteration 382, loss = 0.30946230\n",
      "Iteration 383, loss = 0.30907183\n",
      "Iteration 384, loss = 0.30868231\n",
      "Iteration 385, loss = 0.30829370\n",
      "Iteration 386, loss = 0.30790597\n",
      "Iteration 387, loss = 0.30751913\n",
      "Iteration 388, loss = 0.30713321\n",
      "Iteration 389, loss = 0.30674815\n",
      "Iteration 390, loss = 0.30636399\n",
      "Iteration 391, loss = 0.30598073\n",
      "Iteration 392, loss = 0.30559834\n",
      "Iteration 393, loss = 0.30521683\n",
      "Iteration 394, loss = 0.30483621\n",
      "Iteration 395, loss = 0.30445645\n",
      "Iteration 396, loss = 0.30407754\n",
      "Iteration 397, loss = 0.30369953\n",
      "Iteration 398, loss = 0.30332236\n",
      "Iteration 399, loss = 0.30294603\n",
      "Iteration 400, loss = 0.30257055\n",
      "Iteration 401, loss = 0.30219594\n",
      "Iteration 402, loss = 0.30182213\n",
      "Iteration 403, loss = 0.30144925\n",
      "Iteration 404, loss = 0.30107710\n",
      "Iteration 405, loss = 0.30070587\n",
      "Iteration 406, loss = 0.30033542\n",
      "Iteration 407, loss = 0.29996583\n",
      "Iteration 408, loss = 0.29959706\n",
      "Iteration 409, loss = 0.29922910\n",
      "Iteration 410, loss = 0.29886198\n",
      "Iteration 411, loss = 0.29849565\n",
      "Iteration 412, loss = 0.29813017\n",
      "Iteration 413, loss = 0.29776547\n",
      "Iteration 414, loss = 0.29740161\n",
      "Iteration 415, loss = 0.29703851\n",
      "Iteration 416, loss = 0.29667629\n",
      "Iteration 417, loss = 0.29631479\n",
      "Iteration 418, loss = 0.29595415\n",
      "Iteration 419, loss = 0.29559426\n",
      "Iteration 420, loss = 0.29523521\n",
      "Iteration 421, loss = 0.29487691\n",
      "Iteration 422, loss = 0.29451941\n",
      "Iteration 423, loss = 0.29416269\n",
      "Iteration 424, loss = 0.29380677\n",
      "Iteration 425, loss = 0.29345161\n",
      "Iteration 426, loss = 0.29309725\n",
      "Iteration 427, loss = 0.29274369\n",
      "Iteration 428, loss = 0.29239093\n",
      "Iteration 429, loss = 0.29203893\n",
      "Iteration 430, loss = 0.29168771\n",
      "Iteration 431, loss = 0.29133726\n",
      "Iteration 432, loss = 0.29098758\n",
      "Iteration 433, loss = 0.29063865\n",
      "Iteration 434, loss = 0.29029049\n",
      "Iteration 435, loss = 0.28994311\n",
      "Iteration 436, loss = 0.28959644\n",
      "Iteration 437, loss = 0.28925058\n",
      "Iteration 438, loss = 0.28890543\n",
      "Iteration 439, loss = 0.28856104\n",
      "Iteration 440, loss = 0.28821739\n",
      "Iteration 441, loss = 0.28787455\n",
      "Iteration 442, loss = 0.28753241\n",
      "Iteration 443, loss = 0.28719103\n",
      "Iteration 444, loss = 0.28685038\n",
      "Iteration 445, loss = 0.28651049\n",
      "Iteration 446, loss = 0.28617130\n",
      "Iteration 447, loss = 0.28583286\n",
      "Iteration 448, loss = 0.28549515\n",
      "Iteration 449, loss = 0.28515817\n",
      "Iteration 450, loss = 0.28482190\n",
      "Iteration 451, loss = 0.28448638\n",
      "Iteration 452, loss = 0.28415156\n",
      "Iteration 453, loss = 0.28381747\n",
      "Iteration 454, loss = 0.28348408\n",
      "Iteration 455, loss = 0.28315144\n",
      "Iteration 456, loss = 0.28281948\n",
      "Iteration 457, loss = 0.28248824\n",
      "Iteration 458, loss = 0.28215772\n",
      "Iteration 459, loss = 0.28182790\n",
      "Iteration 460, loss = 0.28149880\n",
      "Iteration 461, loss = 0.28117037\n",
      "Iteration 462, loss = 0.28084266\n",
      "Iteration 463, loss = 0.28051565\n",
      "Iteration 464, loss = 0.28018935\n",
      "Iteration 465, loss = 0.27986373\n",
      "Iteration 466, loss = 0.27953880\n",
      "Iteration 467, loss = 0.27921457\n",
      "Iteration 468, loss = 0.27889101\n",
      "Iteration 469, loss = 0.27856816\n",
      "Iteration 470, loss = 0.27824598\n",
      "Iteration 471, loss = 0.27792451\n",
      "Iteration 472, loss = 0.27760370\n",
      "Iteration 473, loss = 0.27728360\n",
      "Iteration 474, loss = 0.27696412\n",
      "Iteration 475, loss = 0.27664537\n",
      "Iteration 476, loss = 0.27632729\n",
      "Iteration 477, loss = 0.27600988\n",
      "Iteration 478, loss = 0.27569312\n",
      "Iteration 479, loss = 0.27537705\n",
      "Iteration 480, loss = 0.27506165\n",
      "Iteration 481, loss = 0.27474696\n",
      "Iteration 482, loss = 0.27443288\n",
      "Iteration 483, loss = 0.27411947\n",
      "Iteration 484, loss = 0.27380680\n",
      "Iteration 485, loss = 0.27349470\n",
      "Iteration 486, loss = 0.27318329\n",
      "Iteration 487, loss = 0.27287255\n",
      "Iteration 488, loss = 0.27256244\n",
      "Iteration 489, loss = 0.27225303\n",
      "Iteration 490, loss = 0.27194419\n",
      "Iteration 491, loss = 0.27163606\n",
      "Iteration 492, loss = 0.27132858\n",
      "Iteration 493, loss = 0.27102175\n",
      "Iteration 494, loss = 0.27071552\n",
      "Iteration 495, loss = 0.27041002\n",
      "Iteration 496, loss = 0.27010506\n",
      "Iteration 497, loss = 0.26980084\n",
      "Iteration 498, loss = 0.26949719\n",
      "Iteration 499, loss = 0.26919422\n",
      "Iteration 500, loss = 0.26889188\n",
      "Iteration 501, loss = 0.26859019\n",
      "Iteration 502, loss = 0.26828910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 503, loss = 0.26798867\n",
      "Iteration 504, loss = 0.26768890\n",
      "Iteration 505, loss = 0.26738971\n",
      "Iteration 506, loss = 0.26709119\n",
      "Iteration 507, loss = 0.26679328\n",
      "Iteration 508, loss = 0.26649601\n",
      "Iteration 509, loss = 0.26619934\n",
      "Iteration 510, loss = 0.26590330\n",
      "Iteration 511, loss = 0.26560791\n",
      "Iteration 512, loss = 0.26531310\n",
      "Iteration 513, loss = 0.26501893\n",
      "Iteration 514, loss = 0.26472538\n",
      "Iteration 515, loss = 0.26443242\n",
      "Iteration 516, loss = 0.26414009\n",
      "Iteration 517, loss = 0.26384837\n",
      "Iteration 518, loss = 0.26355725\n",
      "Iteration 519, loss = 0.26326676\n",
      "Iteration 520, loss = 0.26297687\n",
      "Iteration 521, loss = 0.26268759\n",
      "Iteration 522, loss = 0.26239893\n",
      "Iteration 523, loss = 0.26211086\n",
      "Iteration 524, loss = 0.26182340\n",
      "Iteration 525, loss = 0.26153655\n",
      "Iteration 526, loss = 0.26125027\n",
      "Iteration 527, loss = 0.26096462\n",
      "Iteration 528, loss = 0.26067956\n",
      "Iteration 529, loss = 0.26039508\n",
      "Iteration 530, loss = 0.26011119\n",
      "Iteration 531, loss = 0.25982791\n",
      "Iteration 532, loss = 0.25954520\n",
      "Iteration 533, loss = 0.25926311\n",
      "Iteration 534, loss = 0.25898158\n",
      "Iteration 535, loss = 0.25870064\n",
      "Iteration 536, loss = 0.25842031\n",
      "Iteration 537, loss = 0.25814053\n",
      "Iteration 538, loss = 0.25786134\n",
      "Iteration 539, loss = 0.25758277\n",
      "Iteration 540, loss = 0.25730479\n",
      "Iteration 541, loss = 0.25702736\n",
      "Iteration 542, loss = 0.25675053\n",
      "Iteration 543, loss = 0.25647427\n",
      "Iteration 544, loss = 0.25619859\n",
      "Iteration 545, loss = 0.25592351\n",
      "Iteration 546, loss = 0.25564896\n",
      "Iteration 547, loss = 0.25537500\n",
      "Iteration 548, loss = 0.25510162\n",
      "Iteration 549, loss = 0.25482879\n",
      "Iteration 550, loss = 0.25455653\n",
      "Iteration 551, loss = 0.25428484\n",
      "Iteration 552, loss = 0.25401373\n",
      "Iteration 553, loss = 0.25374317\n",
      "Iteration 554, loss = 0.25347317\n",
      "Iteration 555, loss = 0.25320374\n",
      "Iteration 556, loss = 0.25293486\n",
      "Iteration 557, loss = 0.25266654\n",
      "Iteration 558, loss = 0.25239881\n",
      "Iteration 559, loss = 0.25213158\n",
      "Iteration 560, loss = 0.25186494\n",
      "Iteration 561, loss = 0.25159893\n",
      "Iteration 562, loss = 0.25133346\n",
      "Iteration 563, loss = 0.25106853\n",
      "Iteration 564, loss = 0.25080414\n",
      "Iteration 565, loss = 0.25054032\n",
      "Iteration 566, loss = 0.25027701\n",
      "Iteration 567, loss = 0.25001424\n",
      "Iteration 568, loss = 0.24975203\n",
      "Iteration 569, loss = 0.24949034\n",
      "Iteration 570, loss = 0.24922920\n",
      "Iteration 571, loss = 0.24896860\n",
      "Iteration 572, loss = 0.24870859\n",
      "Iteration 573, loss = 0.24844909\n",
      "Iteration 574, loss = 0.24819015\n",
      "Iteration 575, loss = 0.24793173\n",
      "Iteration 576, loss = 0.24767383\n",
      "Iteration 577, loss = 0.24741646\n",
      "Iteration 578, loss = 0.24715963\n",
      "Iteration 579, loss = 0.24690336\n",
      "Iteration 580, loss = 0.24664760\n",
      "Iteration 581, loss = 0.24639237\n",
      "Iteration 582, loss = 0.24613767\n",
      "Iteration 583, loss = 0.24588352\n",
      "Iteration 584, loss = 0.24562986\n",
      "Iteration 585, loss = 0.24537674\n",
      "Iteration 586, loss = 0.24512417\n",
      "Iteration 587, loss = 0.24487211\n",
      "Iteration 588, loss = 0.24462057\n",
      "Iteration 589, loss = 0.24436954\n",
      "Iteration 590, loss = 0.24411904\n",
      "Iteration 591, loss = 0.24386906\n",
      "Iteration 592, loss = 0.24361962\n",
      "Iteration 593, loss = 0.24337065\n",
      "Iteration 594, loss = 0.24312222\n",
      "Iteration 595, loss = 0.24287431\n",
      "Iteration 596, loss = 0.24262690\n",
      "Iteration 597, loss = 0.24238000\n",
      "Iteration 598, loss = 0.24213367\n",
      "Iteration 599, loss = 0.24188780\n",
      "Iteration 600, loss = 0.24164244\n",
      "Iteration 601, loss = 0.24139758\n",
      "Iteration 602, loss = 0.24115325\n",
      "Iteration 603, loss = 0.24090942\n",
      "Iteration 604, loss = 0.24066607\n",
      "Iteration 605, loss = 0.24042329\n",
      "Iteration 606, loss = 0.24018093\n",
      "Iteration 607, loss = 0.23993912\n",
      "Iteration 608, loss = 0.23969781\n",
      "Iteration 609, loss = 0.23945700\n",
      "Iteration 610, loss = 0.23921667\n",
      "Iteration 611, loss = 0.23897685\n",
      "Iteration 612, loss = 0.23873753\n",
      "Iteration 613, loss = 0.23849869\n",
      "Iteration 614, loss = 0.23826034\n",
      "Iteration 615, loss = 0.23802250\n",
      "Iteration 616, loss = 0.23778515\n",
      "Iteration 617, loss = 0.23754832\n",
      "Iteration 618, loss = 0.23731200\n",
      "Iteration 619, loss = 0.23707615\n",
      "Iteration 620, loss = 0.23684080\n",
      "Iteration 621, loss = 0.23660591\n",
      "Iteration 622, loss = 0.23637156\n",
      "Iteration 623, loss = 0.23613764\n",
      "Iteration 624, loss = 0.23590422\n",
      "Iteration 625, loss = 0.23567129\n",
      "Iteration 626, loss = 0.23543881\n",
      "Iteration 627, loss = 0.23520689\n",
      "Iteration 628, loss = 0.23497537\n",
      "Iteration 629, loss = 0.23474436\n",
      "Iteration 630, loss = 0.23451381\n",
      "Iteration 631, loss = 0.23428373\n",
      "Iteration 632, loss = 0.23405416\n",
      "Iteration 633, loss = 0.23382502\n",
      "Iteration 634, loss = 0.23359639\n",
      "Iteration 635, loss = 0.23336820\n",
      "Iteration 636, loss = 0.23314049\n",
      "Iteration 637, loss = 0.23291327\n",
      "Iteration 638, loss = 0.23268649\n",
      "Iteration 639, loss = 0.23246017\n",
      "Iteration 640, loss = 0.23223437\n",
      "Iteration 641, loss = 0.23200909\n",
      "Iteration 642, loss = 0.23178430\n",
      "Iteration 643, loss = 0.23155996\n",
      "Iteration 644, loss = 0.23133608\n",
      "Iteration 645, loss = 0.23111269\n",
      "Iteration 646, loss = 0.23088973\n",
      "Iteration 647, loss = 0.23066724\n",
      "Iteration 648, loss = 0.23044523\n",
      "Iteration 649, loss = 0.23022363\n",
      "Iteration 650, loss = 0.23000253\n",
      "Iteration 651, loss = 0.22978187\n",
      "Iteration 652, loss = 0.22956166\n",
      "Iteration 653, loss = 0.22934191\n",
      "Iteration 654, loss = 0.22912260\n",
      "Iteration 655, loss = 0.22890374\n",
      "Iteration 656, loss = 0.22868536\n",
      "Iteration 657, loss = 0.22846739\n",
      "Iteration 658, loss = 0.22824989\n",
      "Iteration 659, loss = 0.22803283\n",
      "Iteration 660, loss = 0.22781622\n",
      "Iteration 661, loss = 0.22760005\n",
      "Iteration 662, loss = 0.22738435\n",
      "Iteration 663, loss = 0.22716905\n",
      "Iteration 664, loss = 0.22695422\n",
      "Iteration 665, loss = 0.22673984\n",
      "Iteration 666, loss = 0.22652588\n",
      "Iteration 667, loss = 0.22631236\n",
      "Iteration 668, loss = 0.22609930\n",
      "Iteration 669, loss = 0.22588664\n",
      "Iteration 670, loss = 0.22567444\n",
      "Iteration 671, loss = 0.22546268\n",
      "Iteration 672, loss = 0.22525133\n",
      "Iteration 673, loss = 0.22504042\n",
      "Iteration 674, loss = 0.22482998\n",
      "Iteration 675, loss = 0.22461992\n",
      "Iteration 676, loss = 0.22441030\n",
      "Iteration 677, loss = 0.22420114\n",
      "Iteration 678, loss = 0.22399238\n",
      "Iteration 679, loss = 0.22378405\n",
      "Iteration 680, loss = 0.22357617\n",
      "Iteration 681, loss = 0.22336869\n",
      "Iteration 682, loss = 0.22316164\n",
      "Iteration 683, loss = 0.22295503\n",
      "Iteration 684, loss = 0.22274882\n",
      "Iteration 685, loss = 0.22254306\n",
      "Iteration 686, loss = 0.22233770\n",
      "Iteration 687, loss = 0.22213275\n",
      "Iteration 688, loss = 0.22192822\n",
      "Iteration 689, loss = 0.22172416\n",
      "Iteration 690, loss = 0.22152044\n",
      "Iteration 691, loss = 0.22131719\n",
      "Iteration 692, loss = 0.22111435\n",
      "Iteration 693, loss = 0.22091191\n",
      "Iteration 694, loss = 0.22070987\n",
      "Iteration 695, loss = 0.22050830\n",
      "Iteration 696, loss = 0.22030706\n",
      "Iteration 697, loss = 0.22010629\n",
      "Iteration 698, loss = 0.21990592\n",
      "Iteration 699, loss = 0.21970596\n",
      "Iteration 700, loss = 0.21950640\n",
      "Iteration 701, loss = 0.21930722\n",
      "Iteration 702, loss = 0.21910851\n",
      "Iteration 703, loss = 0.21891015\n",
      "Iteration 704, loss = 0.21871220\n",
      "Iteration 705, loss = 0.21851469\n",
      "Iteration 706, loss = 0.21831752\n",
      "Iteration 707, loss = 0.21812082\n",
      "Iteration 708, loss = 0.21792448\n",
      "Iteration 709, loss = 0.21772855\n",
      "Iteration 710, loss = 0.21753301\n",
      "Iteration 711, loss = 0.21733788\n",
      "Iteration 712, loss = 0.21714315\n",
      "Iteration 713, loss = 0.21694879\n",
      "Iteration 714, loss = 0.21675486\n",
      "Iteration 715, loss = 0.21656130\n",
      "Iteration 716, loss = 0.21636814\n",
      "Iteration 717, loss = 0.21617537\n",
      "Iteration 718, loss = 0.21598301\n",
      "Iteration 719, loss = 0.21579101\n",
      "Iteration 720, loss = 0.21559942\n",
      "Iteration 721, loss = 0.21540824\n",
      "Iteration 722, loss = 0.21521741\n",
      "Iteration 723, loss = 0.21502697\n",
      "Iteration 724, loss = 0.21483693\n",
      "Iteration 725, loss = 0.21464730\n",
      "Iteration 726, loss = 0.21445801\n",
      "Iteration 727, loss = 0.21426913\n",
      "Iteration 728, loss = 0.21408061\n",
      "Iteration 729, loss = 0.21389251\n",
      "Iteration 730, loss = 0.21370476\n",
      "Iteration 731, loss = 0.21351739\n",
      "Iteration 732, loss = 0.21333042\n",
      "Iteration 733, loss = 0.21314382\n",
      "Iteration 734, loss = 0.21295759\n",
      "Iteration 735, loss = 0.21277175\n",
      "Iteration 736, loss = 0.21258627\n",
      "Iteration 737, loss = 0.21240119\n",
      "Iteration 738, loss = 0.21221644\n",
      "Iteration 739, loss = 0.21203212\n",
      "Iteration 740, loss = 0.21184814\n",
      "Iteration 741, loss = 0.21166453\n",
      "Iteration 742, loss = 0.21148131\n",
      "Iteration 743, loss = 0.21129845\n",
      "Iteration 744, loss = 0.21111595\n",
      "Iteration 745, loss = 0.21093385\n",
      "Iteration 746, loss = 0.21075212\n",
      "Iteration 747, loss = 0.21057078\n",
      "Iteration 748, loss = 0.21038978\n",
      "Iteration 749, loss = 0.21020915\n",
      "Iteration 750, loss = 0.21002894\n",
      "Iteration 751, loss = 0.20984904\n",
      "Iteration 752, loss = 0.20966950\n",
      "Iteration 753, loss = 0.20949036\n",
      "Iteration 754, loss = 0.20931155\n",
      "Iteration 755, loss = 0.20913309\n",
      "Iteration 756, loss = 0.20895501\n",
      "Iteration 757, loss = 0.20877728\n",
      "Iteration 758, loss = 0.20859992\n",
      "Iteration 759, loss = 0.20842289\n",
      "Iteration 760, loss = 0.20824625\n",
      "Iteration 761, loss = 0.20806998\n",
      "Iteration 762, loss = 0.20789402\n",
      "Iteration 763, loss = 0.20771844\n",
      "Iteration 764, loss = 0.20754323\n",
      "Iteration 765, loss = 0.20736834\n",
      "Iteration 766, loss = 0.20719381\n",
      "Iteration 767, loss = 0.20701963\n",
      "Iteration 768, loss = 0.20684581\n",
      "Iteration 769, loss = 0.20667234\n",
      "Iteration 770, loss = 0.20649928\n",
      "Iteration 771, loss = 0.20632650\n",
      "Iteration 772, loss = 0.20615410\n",
      "Iteration 773, loss = 0.20598204\n",
      "Iteration 774, loss = 0.20581031\n",
      "Iteration 775, loss = 0.20563894\n",
      "Iteration 776, loss = 0.20546792\n",
      "Iteration 777, loss = 0.20529723\n",
      "Iteration 778, loss = 0.20512688\n",
      "Iteration 779, loss = 0.20495687\n",
      "Iteration 780, loss = 0.20478724\n",
      "Iteration 781, loss = 0.20461793\n",
      "Iteration 782, loss = 0.20444894\n",
      "Iteration 783, loss = 0.20428031\n",
      "Iteration 784, loss = 0.20411201\n",
      "Iteration 785, loss = 0.20394404\n",
      "Iteration 786, loss = 0.20377640\n",
      "Iteration 787, loss = 0.20360908\n",
      "Iteration 788, loss = 0.20344214\n",
      "Iteration 789, loss = 0.20327554\n",
      "Iteration 790, loss = 0.20310921\n",
      "Iteration 791, loss = 0.20294328\n",
      "Iteration 792, loss = 0.20277765\n",
      "Iteration 793, loss = 0.20261235\n",
      "Iteration 794, loss = 0.20244738\n",
      "Iteration 795, loss = 0.20228274\n",
      "Iteration 796, loss = 0.20211845\n",
      "Iteration 797, loss = 0.20195445\n",
      "Iteration 798, loss = 0.20179086\n",
      "Iteration 799, loss = 0.20162751\n",
      "Iteration 800, loss = 0.20146454\n",
      "Iteration 801, loss = 0.20130187\n",
      "Iteration 802, loss = 0.20113953\n",
      "Iteration 803, loss = 0.20097749\n",
      "Iteration 804, loss = 0.20081583\n",
      "Iteration 805, loss = 0.20065446\n",
      "Iteration 806, loss = 0.20049340\n",
      "Iteration 807, loss = 0.20033273\n",
      "Iteration 808, loss = 0.20017227\n",
      "Iteration 809, loss = 0.20001221\n",
      "Iteration 810, loss = 0.19985245\n",
      "Iteration 811, loss = 0.19969298\n",
      "Iteration 812, loss = 0.19953385\n",
      "Iteration 813, loss = 0.19937505\n",
      "Iteration 814, loss = 0.19921653\n",
      "Iteration 815, loss = 0.19905832\n",
      "Iteration 816, loss = 0.19890051\n",
      "Iteration 817, loss = 0.19874293\n",
      "Iteration 818, loss = 0.19858568\n",
      "Iteration 819, loss = 0.19842876\n",
      "Iteration 820, loss = 0.19827214\n",
      "Iteration 821, loss = 0.19811584\n",
      "Iteration 822, loss = 0.19795988\n",
      "Iteration 823, loss = 0.19780418\n",
      "Iteration 824, loss = 0.19764881\n",
      "Iteration 825, loss = 0.19749384\n",
      "Iteration 826, loss = 0.19733904\n",
      "Iteration 827, loss = 0.19718462\n",
      "Iteration 828, loss = 0.19703049\n",
      "Iteration 829, loss = 0.19687667\n",
      "Iteration 830, loss = 0.19672315\n",
      "Iteration 831, loss = 0.19656996\n",
      "Iteration 832, loss = 0.19641703\n",
      "Iteration 833, loss = 0.19626443\n",
      "Iteration 834, loss = 0.19611215\n",
      "Iteration 835, loss = 0.19596017\n",
      "Iteration 836, loss = 0.19580846\n",
      "Iteration 837, loss = 0.19565708\n",
      "Iteration 838, loss = 0.19550596\n",
      "Iteration 839, loss = 0.19535515\n",
      "Iteration 840, loss = 0.19520464\n",
      "Iteration 841, loss = 0.19505442\n",
      "Iteration 842, loss = 0.19490450\n",
      "Iteration 843, loss = 0.19475491\n",
      "Iteration 844, loss = 0.19460559\n",
      "Iteration 845, loss = 0.19445657\n",
      "Iteration 846, loss = 0.19430784\n",
      "Iteration 847, loss = 0.19415938\n",
      "Iteration 848, loss = 0.19401123\n",
      "Iteration 849, loss = 0.19386335\n",
      "Iteration 850, loss = 0.19371578\n",
      "Iteration 851, loss = 0.19356853\n",
      "Iteration 852, loss = 0.19342153\n",
      "Iteration 853, loss = 0.19327482\n",
      "Iteration 854, loss = 0.19312844\n",
      "Iteration 855, loss = 0.19298231\n",
      "Iteration 856, loss = 0.19283647\n",
      "Iteration 857, loss = 0.19269092\n",
      "Iteration 858, loss = 0.19254566\n",
      "Iteration 859, loss = 0.19240069\n",
      "Iteration 860, loss = 0.19225604\n",
      "Iteration 861, loss = 0.19211161\n",
      "Iteration 862, loss = 0.19196751\n",
      "Iteration 863, loss = 0.19182366\n",
      "Iteration 864, loss = 0.19168010\n",
      "Iteration 865, loss = 0.19153683\n",
      "Iteration 866, loss = 0.19139382\n",
      "Iteration 867, loss = 0.19125111\n",
      "Iteration 868, loss = 0.19110873\n",
      "Iteration 869, loss = 0.19096655\n",
      "Iteration 870, loss = 0.19082468\n",
      "Iteration 871, loss = 0.19068308\n",
      "Iteration 872, loss = 0.19054176\n",
      "Iteration 873, loss = 0.19040075\n",
      "Iteration 874, loss = 0.19026000\n",
      "Iteration 875, loss = 0.19011975\n",
      "Iteration 876, loss = 0.18997954\n",
      "Iteration 877, loss = 0.18983970\n",
      "Iteration 878, loss = 0.18970010\n",
      "Iteration 879, loss = 0.18956079\n",
      "Iteration 880, loss = 0.18942181\n",
      "Iteration 881, loss = 0.18928304\n",
      "Iteration 882, loss = 0.18914455\n",
      "Iteration 883, loss = 0.18900637\n",
      "Iteration 884, loss = 0.18886841\n",
      "Iteration 885, loss = 0.18873078\n",
      "Iteration 886, loss = 0.18859334\n",
      "Iteration 887, loss = 0.18845621\n",
      "Iteration 888, loss = 0.18831938\n",
      "Iteration 889, loss = 0.18818278\n",
      "Iteration 890, loss = 0.18804639\n",
      "Iteration 891, loss = 0.18791040\n",
      "Iteration 892, loss = 0.18777457\n",
      "Iteration 893, loss = 0.18763902\n",
      "Iteration 894, loss = 0.18750378\n",
      "Iteration 895, loss = 0.18736873\n",
      "Iteration 896, loss = 0.18723405\n",
      "Iteration 897, loss = 0.18709949\n",
      "Iteration 898, loss = 0.18696531\n",
      "Iteration 899, loss = 0.18683134\n",
      "Iteration 900, loss = 0.18669772\n",
      "Iteration 901, loss = 0.18656455\n",
      "Iteration 902, loss = 0.18643166\n",
      "Iteration 903, loss = 0.18629899\n",
      "Iteration 904, loss = 0.18616668\n",
      "Iteration 905, loss = 0.18603455\n",
      "Iteration 906, loss = 0.18590269\n",
      "Iteration 907, loss = 0.18577118\n",
      "Iteration 908, loss = 0.18563985\n",
      "Iteration 909, loss = 0.18550877\n",
      "Iteration 910, loss = 0.18537805\n",
      "Iteration 911, loss = 0.18524747\n",
      "Iteration 912, loss = 0.18511724\n",
      "Iteration 913, loss = 0.18498722\n",
      "Iteration 914, loss = 0.18485746\n",
      "Iteration 915, loss = 0.18472796\n",
      "Iteration 916, loss = 0.18459873\n",
      "Iteration 917, loss = 0.18446971\n",
      "Iteration 918, loss = 0.18434101\n",
      "Iteration 919, loss = 0.18421252\n",
      "Iteration 920, loss = 0.18408426\n",
      "Iteration 921, loss = 0.18395632\n",
      "Iteration 922, loss = 0.18382854\n",
      "Iteration 923, loss = 0.18370102\n",
      "Iteration 924, loss = 0.18357385\n",
      "Iteration 925, loss = 0.18344683\n",
      "Iteration 926, loss = 0.18332003\n",
      "Iteration 927, loss = 0.18319357\n",
      "Iteration 928, loss = 0.18306724\n",
      "Iteration 929, loss = 0.18294123\n",
      "Iteration 930, loss = 0.18281544\n",
      "Iteration 931, loss = 0.18268987\n",
      "Iteration 932, loss = 0.18256455\n",
      "Iteration 933, loss = 0.18243950\n",
      "Iteration 934, loss = 0.18231463\n",
      "Iteration 935, loss = 0.18219006\n",
      "Iteration 936, loss = 0.18206568\n",
      "Iteration 937, loss = 0.18194153\n",
      "Iteration 938, loss = 0.18181771\n",
      "Iteration 939, loss = 0.18169399\n",
      "Iteration 940, loss = 0.18157057\n",
      "Iteration 941, loss = 0.18144744\n",
      "Iteration 942, loss = 0.18132445\n",
      "Iteration 943, loss = 0.18120169\n",
      "Iteration 944, loss = 0.18107929\n",
      "Iteration 945, loss = 0.18095693\n",
      "Iteration 946, loss = 0.18083497\n",
      "Iteration 947, loss = 0.18071314\n",
      "Iteration 948, loss = 0.18059155\n",
      "Iteration 949, loss = 0.18047027\n",
      "Iteration 950, loss = 0.18034912\n",
      "Iteration 951, loss = 0.18022822\n",
      "Iteration 952, loss = 0.18010763\n",
      "Iteration 953, loss = 0.17998714\n",
      "Iteration 954, loss = 0.17986694\n",
      "Iteration 955, loss = 0.17974702\n",
      "Iteration 956, loss = 0.17962725\n",
      "Iteration 957, loss = 0.17950770\n",
      "Iteration 958, loss = 0.17938845\n",
      "Iteration 959, loss = 0.17926931\n",
      "Iteration 960, loss = 0.17915049\n",
      "Iteration 961, loss = 0.17903186\n",
      "Iteration 962, loss = 0.17891343\n",
      "Iteration 963, loss = 0.17879528\n",
      "Iteration 964, loss = 0.17867732\n",
      "Iteration 965, loss = 0.17855956\n",
      "Iteration 966, loss = 0.17844210\n",
      "Iteration 967, loss = 0.17832478\n",
      "Iteration 968, loss = 0.17820769\n",
      "Iteration 969, loss = 0.17809092\n",
      "Iteration 970, loss = 0.17797426\n",
      "Iteration 971, loss = 0.17785781\n",
      "Iteration 972, loss = 0.17774165\n",
      "Iteration 973, loss = 0.17762563\n",
      "Iteration 974, loss = 0.17750989\n",
      "Iteration 975, loss = 0.17739438\n",
      "Iteration 976, loss = 0.17727903\n",
      "Iteration 977, loss = 0.17716394\n",
      "Iteration 978, loss = 0.17704907\n",
      "Iteration 979, loss = 0.17693437\n",
      "Iteration 980, loss = 0.17681994\n",
      "Iteration 981, loss = 0.17670568\n",
      "Iteration 982, loss = 0.17659163\n",
      "Iteration 983, loss = 0.17647788\n",
      "Iteration 984, loss = 0.17636422\n",
      "Iteration 985, loss = 0.17625083\n",
      "Iteration 986, loss = 0.17613767\n",
      "Iteration 987, loss = 0.17602468\n",
      "Iteration 988, loss = 0.17591187\n",
      "Iteration 989, loss = 0.17579938\n",
      "Iteration 990, loss = 0.17568701\n",
      "Iteration 991, loss = 0.17557483\n",
      "Iteration 992, loss = 0.17546295\n",
      "Iteration 993, loss = 0.17535120\n",
      "Iteration 994, loss = 0.17523968\n",
      "Iteration 995, loss = 0.17512838\n",
      "Iteration 996, loss = 0.17501724\n",
      "Iteration 997, loss = 0.17490641\n",
      "Iteration 998, loss = 0.17479565\n",
      "Iteration 999, loss = 0.17468517\n",
      "Iteration 1000, loss = 0.17457490\n",
      "Iteration 1, loss = 1.34884587\n",
      "Iteration 2, loss = 1.32501816\n",
      "Iteration 3, loss = 1.29301249\n",
      "Iteration 4, loss = 1.25523626\n",
      "Iteration 5, loss = 1.21379618\n",
      "Iteration 6, loss = 1.17034833\n",
      "Iteration 7, loss = 1.12612768\n",
      "Iteration 8, loss = 1.08217530\n",
      "Iteration 9, loss = 1.03956335\n",
      "Iteration 10, loss = 0.99933244\n",
      "Iteration 11, loss = 0.96255168\n",
      "Iteration 12, loss = 0.93020547\n",
      "Iteration 13, loss = 0.90288408\n",
      "Iteration 14, loss = 0.88096243\n",
      "Iteration 15, loss = 0.86421312\n",
      "Iteration 16, loss = 0.85208220\n",
      "Iteration 17, loss = 0.84344670\n",
      "Iteration 18, loss = 0.83723294\n",
      "Iteration 19, loss = 0.83193255\n",
      "Iteration 20, loss = 0.82629072\n",
      "Iteration 21, loss = 0.81957850\n",
      "Iteration 22, loss = 0.81131771\n",
      "Iteration 23, loss = 0.80167984\n",
      "Iteration 24, loss = 0.79097473\n",
      "Iteration 25, loss = 0.77995292\n",
      "Iteration 26, loss = 0.76856715\n",
      "Iteration 27, loss = 0.75719039\n",
      "Iteration 28, loss = 0.74600040\n",
      "Iteration 29, loss = 0.73515884\n",
      "Iteration 30, loss = 0.72499660\n",
      "Iteration 31, loss = 0.71575819\n",
      "Iteration 32, loss = 0.70732857\n",
      "Iteration 33, loss = 0.69974200\n",
      "Iteration 34, loss = 0.69287924\n",
      "Iteration 35, loss = 0.68675955\n",
      "Iteration 36, loss = 0.68115547\n",
      "Iteration 37, loss = 0.67586654\n",
      "Iteration 38, loss = 0.67082517\n",
      "Iteration 39, loss = 0.66599572\n",
      "Iteration 40, loss = 0.66127821\n",
      "Iteration 41, loss = 0.65663419\n",
      "Iteration 42, loss = 0.65207606\n",
      "Iteration 43, loss = 0.64758959\n",
      "Iteration 44, loss = 0.64315950\n",
      "Iteration 45, loss = 0.63877207\n",
      "Iteration 46, loss = 0.63444367\n",
      "Iteration 47, loss = 0.63017294\n",
      "Iteration 48, loss = 0.62599856\n",
      "Iteration 49, loss = 0.62193289\n",
      "Iteration 50, loss = 0.61796922\n",
      "Iteration 51, loss = 0.61416207\n",
      "Iteration 52, loss = 0.61050320\n",
      "Iteration 53, loss = 0.60703121\n",
      "Iteration 54, loss = 0.60374302\n",
      "Iteration 55, loss = 0.60059477\n",
      "Iteration 56, loss = 0.59754162\n",
      "Iteration 57, loss = 0.59455044\n",
      "Iteration 58, loss = 0.59160510\n",
      "Iteration 59, loss = 0.58869383\n",
      "Iteration 60, loss = 0.58581250\n",
      "Iteration 61, loss = 0.58295645\n",
      "Iteration 62, loss = 0.58012571\n",
      "Iteration 63, loss = 0.57732303\n",
      "Iteration 64, loss = 0.57455543\n",
      "Iteration 65, loss = 0.57182714\n",
      "Iteration 66, loss = 0.56912989\n",
      "Iteration 67, loss = 0.56647898\n",
      "Iteration 68, loss = 0.56387303\n",
      "Iteration 69, loss = 0.56132875\n",
      "Iteration 70, loss = 0.55885377\n",
      "Iteration 71, loss = 0.55642281\n",
      "Iteration 72, loss = 0.55403204\n",
      "Iteration 73, loss = 0.55168911\n",
      "Iteration 74, loss = 0.54939394\n",
      "Iteration 75, loss = 0.54713806\n",
      "Iteration 76, loss = 0.54491269\n",
      "Iteration 77, loss = 0.54271815\n",
      "Iteration 78, loss = 0.54055298\n",
      "Iteration 79, loss = 0.53841588\n",
      "Iteration 80, loss = 0.53630643\n",
      "Iteration 81, loss = 0.53422450\n",
      "Iteration 82, loss = 0.53217294\n",
      "Iteration 83, loss = 0.53014992\n",
      "Iteration 84, loss = 0.52815418\n",
      "Iteration 85, loss = 0.52618511\n",
      "Iteration 86, loss = 0.52424229\n",
      "Iteration 87, loss = 0.52232913\n",
      "Iteration 88, loss = 0.52044366\n",
      "Iteration 89, loss = 0.51858646\n",
      "Iteration 90, loss = 0.51675546\n",
      "Iteration 91, loss = 0.51494843\n",
      "Iteration 92, loss = 0.51316421\n",
      "Iteration 93, loss = 0.51140213\n",
      "Iteration 94, loss = 0.50966171\n",
      "Iteration 95, loss = 0.50794252\n",
      "Iteration 96, loss = 0.50624435\n",
      "Iteration 97, loss = 0.50456650\n",
      "Iteration 98, loss = 0.50290930\n",
      "Iteration 99, loss = 0.50127273\n",
      "Iteration 100, loss = 0.49965615\n",
      "Iteration 101, loss = 0.49805892\n",
      "Iteration 102, loss = 0.49648043\n",
      "Iteration 103, loss = 0.49491946\n",
      "Iteration 104, loss = 0.49337857\n",
      "Iteration 105, loss = 0.49185625\n",
      "Iteration 106, loss = 0.49035035\n",
      "Iteration 107, loss = 0.48886119\n",
      "Iteration 108, loss = 0.48738897\n",
      "Iteration 109, loss = 0.48593295\n",
      "Iteration 110, loss = 0.48449344\n",
      "Iteration 111, loss = 0.48307019\n",
      "Iteration 112, loss = 0.48166271\n",
      "Iteration 113, loss = 0.48027081\n",
      "Iteration 114, loss = 0.47889468\n",
      "Iteration 115, loss = 0.47753298\n",
      "Iteration 116, loss = 0.47618577\n",
      "Iteration 117, loss = 0.47485238\n",
      "Iteration 118, loss = 0.47353229\n",
      "Iteration 119, loss = 0.47222595\n",
      "Iteration 120, loss = 0.47093315\n",
      "Iteration 121, loss = 0.46965327\n",
      "Iteration 122, loss = 0.46838533\n",
      "Iteration 123, loss = 0.46712924\n",
      "Iteration 124, loss = 0.46588534\n",
      "Iteration 125, loss = 0.46465314\n",
      "Iteration 126, loss = 0.46343240\n",
      "Iteration 127, loss = 0.46222303\n",
      "Iteration 128, loss = 0.46102491\n",
      "Iteration 129, loss = 0.45983756\n",
      "Iteration 130, loss = 0.45865999\n",
      "Iteration 131, loss = 0.45749206\n",
      "Iteration 132, loss = 0.45633456\n",
      "Iteration 133, loss = 0.45518672\n",
      "Iteration 134, loss = 0.45404790\n",
      "Iteration 135, loss = 0.45291763\n",
      "Iteration 136, loss = 0.45179667\n",
      "Iteration 137, loss = 0.45068481\n",
      "Iteration 138, loss = 0.44958215\n",
      "Iteration 139, loss = 0.44848826\n",
      "Iteration 140, loss = 0.44740341\n",
      "Iteration 141, loss = 0.44632702\n",
      "Iteration 142, loss = 0.44525920\n",
      "Iteration 143, loss = 0.44419919\n",
      "Iteration 144, loss = 0.44314638\n",
      "Iteration 145, loss = 0.44210010\n",
      "Iteration 146, loss = 0.44106060\n",
      "Iteration 147, loss = 0.44002788\n",
      "Iteration 148, loss = 0.43900062\n",
      "Iteration 149, loss = 0.43797918\n",
      "Iteration 150, loss = 0.43696211\n",
      "Iteration 151, loss = 0.43595032\n",
      "Iteration 152, loss = 0.43494497\n",
      "Iteration 153, loss = 0.43394418\n",
      "Iteration 154, loss = 0.43294903\n",
      "Iteration 155, loss = 0.43195286\n",
      "Iteration 156, loss = 0.43095850\n",
      "Iteration 157, loss = 0.42996572\n",
      "Iteration 158, loss = 0.42897163\n",
      "Iteration 159, loss = 0.42797834\n",
      "Iteration 160, loss = 0.42698210\n",
      "Iteration 161, loss = 0.42598761\n",
      "Iteration 162, loss = 0.42499380\n",
      "Iteration 163, loss = 0.42400063\n",
      "Iteration 164, loss = 0.42300705\n",
      "Iteration 165, loss = 0.42201582\n",
      "Iteration 166, loss = 0.42102295\n",
      "Iteration 167, loss = 0.42003342\n",
      "Iteration 168, loss = 0.41904174\n",
      "Iteration 169, loss = 0.41803974\n",
      "Iteration 170, loss = 0.41703466\n",
      "Iteration 171, loss = 0.41602572\n",
      "Iteration 172, loss = 0.41501851\n",
      "Iteration 173, loss = 0.41400406\n",
      "Iteration 174, loss = 0.41298751\n",
      "Iteration 175, loss = 0.41197716\n",
      "Iteration 176, loss = 0.41096343\n",
      "Iteration 177, loss = 0.40995297\n",
      "Iteration 178, loss = 0.40894935\n",
      "Iteration 179, loss = 0.40794869\n",
      "Iteration 180, loss = 0.40696469\n",
      "Iteration 181, loss = 0.40599165\n",
      "Iteration 182, loss = 0.40502415\n",
      "Iteration 183, loss = 0.40406834\n",
      "Iteration 184, loss = 0.40312699\n",
      "Iteration 185, loss = 0.40219932\n",
      "Iteration 186, loss = 0.40129853\n",
      "Iteration 187, loss = 0.40041230\n",
      "Iteration 188, loss = 0.39954409\n",
      "Iteration 189, loss = 0.39869163\n",
      "Iteration 190, loss = 0.39785564\n",
      "Iteration 191, loss = 0.39702934\n",
      "Iteration 192, loss = 0.39621540\n",
      "Iteration 193, loss = 0.39541480\n",
      "Iteration 194, loss = 0.39462969\n",
      "Iteration 195, loss = 0.39385456\n",
      "Iteration 196, loss = 0.39308473\n",
      "Iteration 197, loss = 0.39232186\n",
      "Iteration 198, loss = 0.39156688\n",
      "Iteration 199, loss = 0.39082058\n",
      "Iteration 200, loss = 0.39007996\n",
      "Iteration 201, loss = 0.38934560\n",
      "Iteration 202, loss = 0.38861676\n",
      "Iteration 203, loss = 0.38789227\n",
      "Iteration 204, loss = 0.38717313\n",
      "Iteration 205, loss = 0.38645781\n",
      "Iteration 206, loss = 0.38574621\n",
      "Iteration 207, loss = 0.38503844\n",
      "Iteration 208, loss = 0.38433420\n",
      "Iteration 209, loss = 0.38363346\n",
      "Iteration 210, loss = 0.38293657\n",
      "Iteration 211, loss = 0.38224394\n",
      "Iteration 212, loss = 0.38155559\n",
      "Iteration 213, loss = 0.38087061\n",
      "Iteration 214, loss = 0.38018880\n",
      "Iteration 215, loss = 0.37951008\n",
      "Iteration 216, loss = 0.37883438\n",
      "Iteration 217, loss = 0.37816184\n",
      "Iteration 218, loss = 0.37749272\n",
      "Iteration 219, loss = 0.37682654\n",
      "Iteration 220, loss = 0.37616321\n",
      "Iteration 221, loss = 0.37550281\n",
      "Iteration 222, loss = 0.37484520\n",
      "Iteration 223, loss = 0.37419033\n",
      "Iteration 224, loss = 0.37353832\n",
      "Iteration 225, loss = 0.37288895\n",
      "Iteration 226, loss = 0.37224225\n",
      "Iteration 227, loss = 0.37159822\n",
      "Iteration 228, loss = 0.37095684\n",
      "Iteration 229, loss = 0.37031800\n",
      "Iteration 230, loss = 0.36968175\n",
      "Iteration 231, loss = 0.36904806\n",
      "Iteration 232, loss = 0.36841697\n",
      "Iteration 233, loss = 0.36778824\n",
      "Iteration 234, loss = 0.36716192\n",
      "Iteration 235, loss = 0.36653788\n",
      "Iteration 236, loss = 0.36591622\n",
      "Iteration 237, loss = 0.36529688\n",
      "Iteration 238, loss = 0.36467992\n",
      "Iteration 239, loss = 0.36406523\n",
      "Iteration 240, loss = 0.36345283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 241, loss = 0.36284274\n",
      "Iteration 242, loss = 0.36223505\n",
      "Iteration 243, loss = 0.36162958\n",
      "Iteration 244, loss = 0.36102642\n",
      "Iteration 245, loss = 0.36042540\n",
      "Iteration 246, loss = 0.35982659\n",
      "Iteration 247, loss = 0.35922996\n",
      "Iteration 248, loss = 0.35863543\n",
      "Iteration 249, loss = 0.35804290\n",
      "Iteration 250, loss = 0.35745251\n",
      "Iteration 251, loss = 0.35686414\n",
      "Iteration 252, loss = 0.35627777\n",
      "Iteration 253, loss = 0.35569345\n",
      "Iteration 254, loss = 0.35511114\n",
      "Iteration 255, loss = 0.35453083\n",
      "Iteration 256, loss = 0.35395250\n",
      "Iteration 257, loss = 0.35337612\n",
      "Iteration 258, loss = 0.35280170\n",
      "Iteration 259, loss = 0.35222922\n",
      "Iteration 260, loss = 0.35165864\n",
      "Iteration 261, loss = 0.35108998\n",
      "Iteration 262, loss = 0.35052315\n",
      "Iteration 263, loss = 0.34995828\n",
      "Iteration 264, loss = 0.34939516\n",
      "Iteration 265, loss = 0.34883390\n",
      "Iteration 266, loss = 0.34827449\n",
      "Iteration 267, loss = 0.34771678\n",
      "Iteration 268, loss = 0.34716085\n",
      "Iteration 269, loss = 0.34660683\n",
      "Iteration 270, loss = 0.34605463\n",
      "Iteration 271, loss = 0.34550407\n",
      "Iteration 272, loss = 0.34495536\n",
      "Iteration 273, loss = 0.34440830\n",
      "Iteration 274, loss = 0.34386294\n",
      "Iteration 275, loss = 0.34331931\n",
      "Iteration 276, loss = 0.34277737\n",
      "Iteration 277, loss = 0.34223708\n",
      "Iteration 278, loss = 0.34169846\n",
      "Iteration 279, loss = 0.34116148\n",
      "Iteration 280, loss = 0.34062617\n",
      "Iteration 281, loss = 0.34009242\n",
      "Iteration 282, loss = 0.33956035\n",
      "Iteration 283, loss = 0.33902983\n",
      "Iteration 284, loss = 0.33850090\n",
      "Iteration 285, loss = 0.33797361\n",
      "Iteration 286, loss = 0.33744778\n",
      "Iteration 287, loss = 0.33692357\n",
      "Iteration 288, loss = 0.33640087\n",
      "Iteration 289, loss = 0.33587972\n",
      "Iteration 290, loss = 0.33536024\n",
      "Iteration 291, loss = 0.33484215\n",
      "Iteration 292, loss = 0.33432553\n",
      "Iteration 293, loss = 0.33381039\n",
      "Iteration 294, loss = 0.33329681\n",
      "Iteration 295, loss = 0.33278464\n",
      "Iteration 296, loss = 0.33227395\n",
      "Iteration 297, loss = 0.33176474\n",
      "Iteration 298, loss = 0.33125699\n",
      "Iteration 299, loss = 0.33075058\n",
      "Iteration 300, loss = 0.33024552\n",
      "Iteration 301, loss = 0.32974191\n",
      "Iteration 302, loss = 0.32923967\n",
      "Iteration 303, loss = 0.32873885\n",
      "Iteration 304, loss = 0.32823936\n",
      "Iteration 305, loss = 0.32774131\n",
      "Iteration 306, loss = 0.32724458\n",
      "Iteration 307, loss = 0.32674922\n",
      "Iteration 308, loss = 0.32625522\n",
      "Iteration 309, loss = 0.32576258\n",
      "Iteration 310, loss = 0.32527122\n",
      "Iteration 311, loss = 0.32478114\n",
      "Iteration 312, loss = 0.32429234\n",
      "Iteration 313, loss = 0.32380489\n",
      "Iteration 314, loss = 0.32331872\n",
      "Iteration 315, loss = 0.32283393\n",
      "Iteration 316, loss = 0.32235028\n",
      "Iteration 317, loss = 0.32186800\n",
      "Iteration 318, loss = 0.32138704\n",
      "Iteration 319, loss = 0.32090734\n",
      "Iteration 320, loss = 0.32042886\n",
      "Iteration 321, loss = 0.31995166\n",
      "Iteration 322, loss = 0.31947571\n",
      "Iteration 323, loss = 0.31900100\n",
      "Iteration 324, loss = 0.31852750\n",
      "Iteration 325, loss = 0.31805525\n",
      "Iteration 326, loss = 0.31758424\n",
      "Iteration 327, loss = 0.31711486\n",
      "Iteration 328, loss = 0.31664682\n",
      "Iteration 329, loss = 0.31617997\n",
      "Iteration 330, loss = 0.31571431\n",
      "Iteration 331, loss = 0.31524980\n",
      "Iteration 332, loss = 0.31478650\n",
      "Iteration 333, loss = 0.31432435\n",
      "Iteration 334, loss = 0.31386338\n",
      "Iteration 335, loss = 0.31340356\n",
      "Iteration 336, loss = 0.31294490\n",
      "Iteration 337, loss = 0.31248740\n",
      "Iteration 338, loss = 0.31203109\n",
      "Iteration 339, loss = 0.31157592\n",
      "Iteration 340, loss = 0.31112185\n",
      "Iteration 341, loss = 0.31066894\n",
      "Iteration 342, loss = 0.31021716\n",
      "Iteration 343, loss = 0.30976649\n",
      "Iteration 344, loss = 0.30931695\n",
      "Iteration 345, loss = 0.30886858\n",
      "Iteration 346, loss = 0.30842180\n",
      "Iteration 347, loss = 0.30797612\n",
      "Iteration 348, loss = 0.30753146\n",
      "Iteration 349, loss = 0.30708796\n",
      "Iteration 350, loss = 0.30664544\n",
      "Iteration 351, loss = 0.30620407\n",
      "Iteration 352, loss = 0.30576383\n",
      "Iteration 353, loss = 0.30532467\n",
      "Iteration 354, loss = 0.30488658\n",
      "Iteration 355, loss = 0.30444959\n",
      "Iteration 356, loss = 0.30401371\n",
      "Iteration 357, loss = 0.30357880\n",
      "Iteration 358, loss = 0.30314501\n",
      "Iteration 359, loss = 0.30271223\n",
      "Iteration 360, loss = 0.30228049\n",
      "Iteration 361, loss = 0.30184980\n",
      "Iteration 362, loss = 0.30142012\n",
      "Iteration 363, loss = 0.30099151\n",
      "Iteration 364, loss = 0.30056389\n",
      "Iteration 365, loss = 0.30013743\n",
      "Iteration 366, loss = 0.29971185\n",
      "Iteration 367, loss = 0.29928734\n",
      "Iteration 368, loss = 0.29886397\n",
      "Iteration 369, loss = 0.29844160\n",
      "Iteration 370, loss = 0.29802018\n",
      "Iteration 371, loss = 0.29759974\n",
      "Iteration 372, loss = 0.29718031\n",
      "Iteration 373, loss = 0.29676189\n",
      "Iteration 374, loss = 0.29634446\n",
      "Iteration 375, loss = 0.29592799\n",
      "Iteration 376, loss = 0.29551247\n",
      "Iteration 377, loss = 0.29509805\n",
      "Iteration 378, loss = 0.29468449\n",
      "Iteration 379, loss = 0.29427191\n",
      "Iteration 380, loss = 0.29386031\n",
      "Iteration 381, loss = 0.29344970\n",
      "Iteration 382, loss = 0.29304002\n",
      "Iteration 383, loss = 0.29263132\n",
      "Iteration 384, loss = 0.29222355\n",
      "Iteration 385, loss = 0.29181672\n",
      "Iteration 386, loss = 0.29141084\n",
      "Iteration 387, loss = 0.29100593\n",
      "Iteration 388, loss = 0.29060192\n",
      "Iteration 389, loss = 0.29019896\n",
      "Iteration 390, loss = 0.28979684\n",
      "Iteration 391, loss = 0.28939572\n",
      "Iteration 392, loss = 0.28899549\n",
      "Iteration 393, loss = 0.28859623\n",
      "Iteration 394, loss = 0.28819791\n",
      "Iteration 395, loss = 0.28780057\n",
      "Iteration 396, loss = 0.28740407\n",
      "Iteration 397, loss = 0.28700858\n",
      "Iteration 398, loss = 0.28661404\n",
      "Iteration 399, loss = 0.28622041\n",
      "Iteration 400, loss = 0.28582771\n",
      "Iteration 401, loss = 0.28543590\n",
      "Iteration 402, loss = 0.28504498\n",
      "Iteration 403, loss = 0.28465503\n",
      "Iteration 404, loss = 0.28426591\n",
      "Iteration 405, loss = 0.28387770\n",
      "Iteration 406, loss = 0.28349041\n",
      "Iteration 407, loss = 0.28310395\n",
      "Iteration 408, loss = 0.28271842\n",
      "Iteration 409, loss = 0.28233373\n",
      "Iteration 410, loss = 0.28194994\n",
      "Iteration 411, loss = 0.28156696\n",
      "Iteration 412, loss = 0.28118496\n",
      "Iteration 413, loss = 0.28080375\n",
      "Iteration 414, loss = 0.28042345\n",
      "Iteration 415, loss = 0.28004400\n",
      "Iteration 416, loss = 0.27966539\n",
      "Iteration 417, loss = 0.27928766\n",
      "Iteration 418, loss = 0.27891080\n",
      "Iteration 419, loss = 0.27853474\n",
      "Iteration 420, loss = 0.27815954\n",
      "Iteration 421, loss = 0.27778519\n",
      "Iteration 422, loss = 0.27741167\n",
      "Iteration 423, loss = 0.27703902\n",
      "Iteration 424, loss = 0.27666715\n",
      "Iteration 425, loss = 0.27629618\n",
      "Iteration 426, loss = 0.27592599\n",
      "Iteration 427, loss = 0.27555665\n",
      "Iteration 428, loss = 0.27518814\n",
      "Iteration 429, loss = 0.27482045\n",
      "Iteration 430, loss = 0.27445356\n",
      "Iteration 431, loss = 0.27408753\n",
      "Iteration 432, loss = 0.27372227\n",
      "Iteration 433, loss = 0.27335784\n",
      "Iteration 434, loss = 0.27299423\n",
      "Iteration 435, loss = 0.27263141\n",
      "Iteration 436, loss = 0.27226944\n",
      "Iteration 437, loss = 0.27190824\n",
      "Iteration 438, loss = 0.27154782\n",
      "Iteration 439, loss = 0.27118825\n",
      "Iteration 440, loss = 0.27082943\n",
      "Iteration 441, loss = 0.27047144\n",
      "Iteration 442, loss = 0.27011425\n",
      "Iteration 443, loss = 0.26975787\n",
      "Iteration 444, loss = 0.26940234\n",
      "Iteration 445, loss = 0.26904752\n",
      "Iteration 446, loss = 0.26869353\n",
      "Iteration 447, loss = 0.26834033\n",
      "Iteration 448, loss = 0.26798788\n",
      "Iteration 449, loss = 0.26763626\n",
      "Iteration 450, loss = 0.26728536\n",
      "Iteration 451, loss = 0.26693526\n",
      "Iteration 452, loss = 0.26658596\n",
      "Iteration 453, loss = 0.26623738\n",
      "Iteration 454, loss = 0.26588959\n",
      "Iteration 455, loss = 0.26554260\n",
      "Iteration 456, loss = 0.26519635\n",
      "Iteration 457, loss = 0.26485092\n",
      "Iteration 458, loss = 0.26450620\n",
      "Iteration 459, loss = 0.26416224\n",
      "Iteration 460, loss = 0.26381906\n",
      "Iteration 461, loss = 0.26347661\n",
      "Iteration 462, loss = 0.26313494\n",
      "Iteration 463, loss = 0.26279402\n",
      "Iteration 464, loss = 0.26245382\n",
      "Iteration 465, loss = 0.26211442\n",
      "Iteration 466, loss = 0.26177571\n",
      "Iteration 467, loss = 0.26143778\n",
      "Iteration 468, loss = 0.26110054\n",
      "Iteration 469, loss = 0.26076411\n",
      "Iteration 470, loss = 0.26042836\n",
      "Iteration 471, loss = 0.26009335\n",
      "Iteration 472, loss = 0.25975909\n",
      "Iteration 473, loss = 0.25942552\n",
      "Iteration 474, loss = 0.25909272\n",
      "Iteration 475, loss = 0.25876063\n",
      "Iteration 476, loss = 0.25842925\n",
      "Iteration 477, loss = 0.25809864\n",
      "Iteration 478, loss = 0.25776871\n",
      "Iteration 479, loss = 0.25743951\n",
      "Iteration 480, loss = 0.25711105\n",
      "Iteration 481, loss = 0.25678326\n",
      "Iteration 482, loss = 0.25645624\n",
      "Iteration 483, loss = 0.25612988\n",
      "Iteration 484, loss = 0.25580427\n",
      "Iteration 485, loss = 0.25547934\n",
      "Iteration 486, loss = 0.25515513\n",
      "Iteration 487, loss = 0.25483167\n",
      "Iteration 488, loss = 0.25450883\n",
      "Iteration 489, loss = 0.25418678\n",
      "Iteration 490, loss = 0.25386539\n",
      "Iteration 491, loss = 0.25354468\n",
      "Iteration 492, loss = 0.25322471\n",
      "Iteration 493, loss = 0.25290539\n",
      "Iteration 494, loss = 0.25258682\n",
      "Iteration 495, loss = 0.25226891\n",
      "Iteration 496, loss = 0.25195169\n",
      "Iteration 497, loss = 0.25163517\n",
      "Iteration 498, loss = 0.25131933\n",
      "Iteration 499, loss = 0.25100418\n",
      "Iteration 500, loss = 0.25068971\n",
      "Iteration 501, loss = 0.25037594\n",
      "Iteration 502, loss = 0.25006284\n",
      "Iteration 503, loss = 0.24975040\n",
      "Iteration 504, loss = 0.24943869\n",
      "Iteration 505, loss = 0.24912760\n",
      "Iteration 506, loss = 0.24881723\n",
      "Iteration 507, loss = 0.24850752\n",
      "Iteration 508, loss = 0.24819844\n",
      "Iteration 509, loss = 0.24789009\n",
      "Iteration 510, loss = 0.24758237\n",
      "Iteration 511, loss = 0.24727535\n",
      "Iteration 512, loss = 0.24696896\n",
      "Iteration 513, loss = 0.24666327\n",
      "Iteration 514, loss = 0.24635823\n",
      "Iteration 515, loss = 0.24605385\n",
      "Iteration 516, loss = 0.24575014\n",
      "Iteration 517, loss = 0.24544706\n",
      "Iteration 518, loss = 0.24514468\n",
      "Iteration 519, loss = 0.24484290\n",
      "Iteration 520, loss = 0.24454183\n",
      "Iteration 521, loss = 0.24424141\n",
      "Iteration 522, loss = 0.24394157\n",
      "Iteration 523, loss = 0.24364245\n",
      "Iteration 524, loss = 0.24334393\n",
      "Iteration 525, loss = 0.24304611\n",
      "Iteration 526, loss = 0.24274887\n",
      "Iteration 527, loss = 0.24245230\n",
      "Iteration 528, loss = 0.24215639\n",
      "Iteration 529, loss = 0.24186110\n",
      "Iteration 530, loss = 0.24156645\n",
      "Iteration 531, loss = 0.24127243\n",
      "Iteration 532, loss = 0.24097907\n",
      "Iteration 533, loss = 0.24068632\n",
      "Iteration 534, loss = 0.24039422\n",
      "Iteration 535, loss = 0.24010273\n",
      "Iteration 536, loss = 0.23981190\n",
      "Iteration 537, loss = 0.23952167\n",
      "Iteration 538, loss = 0.23923209\n",
      "Iteration 539, loss = 0.23894312\n",
      "Iteration 540, loss = 0.23865477\n",
      "Iteration 541, loss = 0.23836706\n",
      "Iteration 542, loss = 0.23807994\n",
      "Iteration 543, loss = 0.23779348\n",
      "Iteration 544, loss = 0.23750759\n",
      "Iteration 545, loss = 0.23722237\n",
      "Iteration 546, loss = 0.23693771\n",
      "Iteration 547, loss = 0.23665373\n",
      "Iteration 548, loss = 0.23637033\n",
      "Iteration 549, loss = 0.23608751\n",
      "Iteration 550, loss = 0.23580534\n",
      "Iteration 551, loss = 0.23552376\n",
      "Iteration 552, loss = 0.23524283\n",
      "Iteration 553, loss = 0.23496245\n",
      "Iteration 554, loss = 0.23468269\n",
      "Iteration 555, loss = 0.23440353\n",
      "Iteration 556, loss = 0.23412500\n",
      "Iteration 557, loss = 0.23384703\n",
      "Iteration 558, loss = 0.23356969\n",
      "Iteration 559, loss = 0.23329292\n",
      "Iteration 560, loss = 0.23301678\n",
      "Iteration 561, loss = 0.23274122\n",
      "Iteration 562, loss = 0.23246627\n",
      "Iteration 563, loss = 0.23219194\n",
      "Iteration 564, loss = 0.23191818\n",
      "Iteration 565, loss = 0.23164507\n",
      "Iteration 566, loss = 0.23137248\n",
      "Iteration 567, loss = 0.23110047\n",
      "Iteration 568, loss = 0.23082907\n",
      "Iteration 569, loss = 0.23055824\n",
      "Iteration 570, loss = 0.23028801\n",
      "Iteration 571, loss = 0.23001851\n",
      "Iteration 572, loss = 0.22974950\n",
      "Iteration 573, loss = 0.22948109\n",
      "Iteration 574, loss = 0.22921324\n",
      "Iteration 575, loss = 0.22894594\n",
      "Iteration 576, loss = 0.22867931\n",
      "Iteration 577, loss = 0.22841321\n",
      "Iteration 578, loss = 0.22814767\n",
      "Iteration 579, loss = 0.22788270\n",
      "Iteration 580, loss = 0.22761831\n",
      "Iteration 581, loss = 0.22735452\n",
      "Iteration 582, loss = 0.22709125\n",
      "Iteration 583, loss = 0.22682859\n",
      "Iteration 584, loss = 0.22656646\n",
      "Iteration 585, loss = 0.22630493\n",
      "Iteration 586, loss = 0.22604393\n",
      "Iteration 587, loss = 0.22578351\n",
      "Iteration 588, loss = 0.22552363\n",
      "Iteration 589, loss = 0.22526435\n",
      "Iteration 590, loss = 0.22500558\n",
      "Iteration 591, loss = 0.22474740\n",
      "Iteration 592, loss = 0.22448977\n",
      "Iteration 593, loss = 0.22423268\n",
      "Iteration 594, loss = 0.22397615\n",
      "Iteration 595, loss = 0.22372020\n",
      "Iteration 596, loss = 0.22346477\n",
      "Iteration 597, loss = 0.22320992\n",
      "Iteration 598, loss = 0.22295558\n",
      "Iteration 599, loss = 0.22270177\n",
      "Iteration 600, loss = 0.22244856\n",
      "Iteration 601, loss = 0.22219584\n",
      "Iteration 602, loss = 0.22194372\n",
      "Iteration 603, loss = 0.22169210\n",
      "Iteration 604, loss = 0.22144104\n",
      "Iteration 605, loss = 0.22119050\n",
      "Iteration 606, loss = 0.22094054\n",
      "Iteration 607, loss = 0.22069107\n",
      "Iteration 608, loss = 0.22044217\n",
      "Iteration 609, loss = 0.22019378\n",
      "Iteration 610, loss = 0.21994598\n",
      "Iteration 611, loss = 0.21969866\n",
      "Iteration 612, loss = 0.21945187\n",
      "Iteration 613, loss = 0.21920563\n",
      "Iteration 614, loss = 0.21895992\n",
      "Iteration 615, loss = 0.21871471\n",
      "Iteration 616, loss = 0.21847009\n",
      "Iteration 617, loss = 0.21822593\n",
      "Iteration 618, loss = 0.21798237\n",
      "Iteration 619, loss = 0.21773927\n",
      "Iteration 620, loss = 0.21749673\n",
      "Iteration 621, loss = 0.21725470\n",
      "Iteration 622, loss = 0.21701318\n",
      "Iteration 623, loss = 0.21677236\n",
      "Iteration 624, loss = 0.21653184\n",
      "Iteration 625, loss = 0.21629196\n",
      "Iteration 626, loss = 0.21605269\n",
      "Iteration 627, loss = 0.21581373\n",
      "Iteration 628, loss = 0.21557544\n",
      "Iteration 629, loss = 0.21533764\n",
      "Iteration 630, loss = 0.21510034\n",
      "Iteration 631, loss = 0.21486351\n",
      "Iteration 632, loss = 0.21462730\n",
      "Iteration 633, loss = 0.21439150\n",
      "Iteration 634, loss = 0.21415623\n",
      "Iteration 635, loss = 0.21392156\n",
      "Iteration 636, loss = 0.21368724\n",
      "Iteration 637, loss = 0.21345348\n",
      "Iteration 638, loss = 0.21322032\n",
      "Iteration 639, loss = 0.21298756\n",
      "Iteration 640, loss = 0.21275530\n",
      "Iteration 641, loss = 0.21252350\n",
      "Iteration 642, loss = 0.21229233\n",
      "Iteration 643, loss = 0.21206155\n",
      "Iteration 644, loss = 0.21183125\n",
      "Iteration 645, loss = 0.21160156\n",
      "Iteration 646, loss = 0.21137224\n",
      "Iteration 647, loss = 0.21114344\n",
      "Iteration 648, loss = 0.21091526\n",
      "Iteration 649, loss = 0.21068731\n",
      "Iteration 650, loss = 0.21046012\n",
      "Iteration 651, loss = 0.21023327\n",
      "Iteration 652, loss = 0.21000689\n",
      "Iteration 653, loss = 0.20978097\n",
      "Iteration 654, loss = 0.20955560\n",
      "Iteration 655, loss = 0.20933068\n",
      "Iteration 656, loss = 0.20910624\n",
      "Iteration 657, loss = 0.20888240\n",
      "Iteration 658, loss = 0.20865884\n",
      "Iteration 659, loss = 0.20843588\n",
      "Iteration 660, loss = 0.20821340\n",
      "Iteration 661, loss = 0.20799137\n",
      "Iteration 662, loss = 0.20776975\n",
      "Iteration 663, loss = 0.20754870\n",
      "Iteration 664, loss = 0.20732807\n",
      "Iteration 665, loss = 0.20710792\n",
      "Iteration 666, loss = 0.20688832\n",
      "Iteration 667, loss = 0.20666908\n",
      "Iteration 668, loss = 0.20645035\n",
      "Iteration 669, loss = 0.20623214\n",
      "Iteration 670, loss = 0.20601433\n",
      "Iteration 671, loss = 0.20579698\n",
      "Iteration 672, loss = 0.20558017\n",
      "Iteration 673, loss = 0.20536376\n",
      "Iteration 674, loss = 0.20514779\n",
      "Iteration 675, loss = 0.20493235\n",
      "Iteration 676, loss = 0.20471733\n",
      "Iteration 677, loss = 0.20450275\n",
      "Iteration 678, loss = 0.20428863\n",
      "Iteration 679, loss = 0.20407502\n",
      "Iteration 680, loss = 0.20386181\n",
      "Iteration 681, loss = 0.20364902\n",
      "Iteration 682, loss = 0.20343685\n",
      "Iteration 683, loss = 0.20322488\n",
      "Iteration 684, loss = 0.20301355\n",
      "Iteration 685, loss = 0.20280261\n",
      "Iteration 686, loss = 0.20259209\n",
      "Iteration 687, loss = 0.20238200\n",
      "Iteration 688, loss = 0.20217251\n",
      "Iteration 689, loss = 0.20196324\n",
      "Iteration 690, loss = 0.20175458\n",
      "Iteration 691, loss = 0.20154630\n",
      "Iteration 692, loss = 0.20133848\n",
      "Iteration 693, loss = 0.20113104\n",
      "Iteration 694, loss = 0.20092413\n",
      "Iteration 695, loss = 0.20071759\n",
      "Iteration 696, loss = 0.20051147\n",
      "Iteration 697, loss = 0.20030593\n",
      "Iteration 698, loss = 0.20010064\n",
      "Iteration 699, loss = 0.19989585\n",
      "Iteration 700, loss = 0.19969160\n",
      "Iteration 701, loss = 0.19948767\n",
      "Iteration 702, loss = 0.19928417\n",
      "Iteration 703, loss = 0.19908108\n",
      "Iteration 704, loss = 0.19887850\n",
      "Iteration 705, loss = 0.19867630\n",
      "Iteration 706, loss = 0.19847452\n",
      "Iteration 707, loss = 0.19827321\n",
      "Iteration 708, loss = 0.19807227\n",
      "Iteration 709, loss = 0.19787177\n",
      "Iteration 710, loss = 0.19767174\n",
      "Iteration 711, loss = 0.19747206\n",
      "Iteration 712, loss = 0.19727284\n",
      "Iteration 713, loss = 0.19707407\n",
      "Iteration 714, loss = 0.19687565\n",
      "Iteration 715, loss = 0.19667766\n",
      "Iteration 716, loss = 0.19648017\n",
      "Iteration 717, loss = 0.19628302\n",
      "Iteration 718, loss = 0.19608625\n",
      "Iteration 719, loss = 0.19589004\n",
      "Iteration 720, loss = 0.19569406\n",
      "Iteration 721, loss = 0.19549864\n",
      "Iteration 722, loss = 0.19530353\n",
      "Iteration 723, loss = 0.19510889\n",
      "Iteration 724, loss = 0.19491460\n",
      "Iteration 725, loss = 0.19472083\n",
      "Iteration 726, loss = 0.19452738\n",
      "Iteration 727, loss = 0.19433430\n",
      "Iteration 728, loss = 0.19414175\n",
      "Iteration 729, loss = 0.19394946\n",
      "Iteration 730, loss = 0.19375767\n",
      "Iteration 731, loss = 0.19356629\n",
      "Iteration 732, loss = 0.19337526\n",
      "Iteration 733, loss = 0.19318464\n",
      "Iteration 734, loss = 0.19299446\n",
      "Iteration 735, loss = 0.19280461\n",
      "Iteration 736, loss = 0.19261519\n",
      "Iteration 737, loss = 0.19242624\n",
      "Iteration 738, loss = 0.19223763\n",
      "Iteration 739, loss = 0.19204938\n",
      "Iteration 740, loss = 0.19186155\n",
      "Iteration 741, loss = 0.19167413\n",
      "Iteration 742, loss = 0.19148706\n",
      "Iteration 743, loss = 0.19130046\n",
      "Iteration 744, loss = 0.19111419\n",
      "Iteration 745, loss = 0.19092829\n",
      "Iteration 746, loss = 0.19074289\n",
      "Iteration 747, loss = 0.19055776\n",
      "Iteration 748, loss = 0.19037303\n",
      "Iteration 749, loss = 0.19018880\n",
      "Iteration 750, loss = 0.19000482\n",
      "Iteration 751, loss = 0.18982126\n",
      "Iteration 752, loss = 0.18963821\n",
      "Iteration 753, loss = 0.18945541\n",
      "Iteration 754, loss = 0.18927300\n",
      "Iteration 755, loss = 0.18909108\n",
      "Iteration 756, loss = 0.18890972\n",
      "Iteration 757, loss = 0.18872889\n",
      "Iteration 758, loss = 0.18854852\n",
      "Iteration 759, loss = 0.18836851\n",
      "Iteration 760, loss = 0.18818887\n",
      "Iteration 761, loss = 0.18800962\n",
      "Iteration 762, loss = 0.18783080\n",
      "Iteration 763, loss = 0.18765236\n",
      "Iteration 764, loss = 0.18747424\n",
      "Iteration 765, loss = 0.18729663\n",
      "Iteration 766, loss = 0.18711927\n",
      "Iteration 767, loss = 0.18694233\n",
      "Iteration 768, loss = 0.18676581\n",
      "Iteration 769, loss = 0.18658964\n",
      "Iteration 770, loss = 0.18641380\n",
      "Iteration 771, loss = 0.18623834\n",
      "Iteration 772, loss = 0.18606332\n",
      "Iteration 773, loss = 0.18588859\n",
      "Iteration 774, loss = 0.18571422\n",
      "Iteration 775, loss = 0.18554034\n",
      "Iteration 776, loss = 0.18536672\n",
      "Iteration 777, loss = 0.18519344\n",
      "Iteration 778, loss = 0.18502064\n",
      "Iteration 779, loss = 0.18484807\n",
      "Iteration 780, loss = 0.18467590\n",
      "Iteration 781, loss = 0.18450419\n",
      "Iteration 782, loss = 0.18433274\n",
      "Iteration 783, loss = 0.18416163\n",
      "Iteration 784, loss = 0.18399092\n",
      "Iteration 785, loss = 0.18382058\n",
      "Iteration 786, loss = 0.18365054\n",
      "Iteration 787, loss = 0.18348087\n",
      "Iteration 788, loss = 0.18331160\n",
      "Iteration 789, loss = 0.18314262\n",
      "Iteration 790, loss = 0.18297399\n",
      "Iteration 791, loss = 0.18280583\n",
      "Iteration 792, loss = 0.18263781\n",
      "Iteration 793, loss = 0.18247030\n",
      "Iteration 794, loss = 0.18230307\n",
      "Iteration 795, loss = 0.18213619\n",
      "Iteration 796, loss = 0.18196964\n",
      "Iteration 797, loss = 0.18180350\n",
      "Iteration 798, loss = 0.18163762\n",
      "Iteration 799, loss = 0.18147209\n",
      "Iteration 800, loss = 0.18130704\n",
      "Iteration 801, loss = 0.18114221\n",
      "Iteration 802, loss = 0.18097772\n",
      "Iteration 803, loss = 0.18081361\n",
      "Iteration 804, loss = 0.18064979\n",
      "Iteration 805, loss = 0.18048636\n",
      "Iteration 806, loss = 0.18032321\n",
      "Iteration 807, loss = 0.18016052\n",
      "Iteration 808, loss = 0.17999800\n",
      "Iteration 809, loss = 0.17983592\n",
      "Iteration 810, loss = 0.17967417\n",
      "Iteration 811, loss = 0.17951271\n",
      "Iteration 812, loss = 0.17935161\n",
      "Iteration 813, loss = 0.17919080\n",
      "Iteration 814, loss = 0.17903039\n",
      "Iteration 815, loss = 0.17887027\n",
      "Iteration 816, loss = 0.17871047\n",
      "Iteration 817, loss = 0.17855105\n",
      "Iteration 818, loss = 0.17839190\n",
      "Iteration 819, loss = 0.17823309\n",
      "Iteration 820, loss = 0.17807469\n",
      "Iteration 821, loss = 0.17791646\n",
      "Iteration 822, loss = 0.17775865\n",
      "Iteration 823, loss = 0.17760117\n",
      "Iteration 824, loss = 0.17744397\n",
      "Iteration 825, loss = 0.17728710\n",
      "Iteration 826, loss = 0.17713056\n",
      "Iteration 827, loss = 0.17697435\n",
      "Iteration 828, loss = 0.17681842\n",
      "Iteration 829, loss = 0.17666284\n",
      "Iteration 830, loss = 0.17650758\n",
      "Iteration 831, loss = 0.17635262\n",
      "Iteration 832, loss = 0.17619796\n",
      "Iteration 833, loss = 0.17604372\n",
      "Iteration 834, loss = 0.17588962\n",
      "Iteration 835, loss = 0.17573597\n",
      "Iteration 836, loss = 0.17558259\n",
      "Iteration 837, loss = 0.17542951\n",
      "Iteration 838, loss = 0.17527675\n",
      "Iteration 839, loss = 0.17512431\n",
      "Iteration 840, loss = 0.17497219\n",
      "Iteration 841, loss = 0.17482035\n",
      "Iteration 842, loss = 0.17466885\n",
      "Iteration 843, loss = 0.17451768\n",
      "Iteration 844, loss = 0.17436676\n",
      "Iteration 845, loss = 0.17421615\n",
      "Iteration 846, loss = 0.17406597\n",
      "Iteration 847, loss = 0.17391592\n",
      "Iteration 848, loss = 0.17376637\n",
      "Iteration 849, loss = 0.17361707\n",
      "Iteration 850, loss = 0.17346815\n",
      "Iteration 851, loss = 0.17331943\n",
      "Iteration 852, loss = 0.17317117\n",
      "Iteration 853, loss = 0.17302309\n",
      "Iteration 854, loss = 0.17287530\n",
      "Iteration 855, loss = 0.17272795\n",
      "Iteration 856, loss = 0.17258077\n",
      "Iteration 857, loss = 0.17243391\n",
      "Iteration 858, loss = 0.17228733\n",
      "Iteration 859, loss = 0.17214118\n",
      "Iteration 860, loss = 0.17199515\n",
      "Iteration 861, loss = 0.17184958\n",
      "Iteration 862, loss = 0.17170421\n",
      "Iteration 863, loss = 0.17155910\n",
      "Iteration 864, loss = 0.17141439\n",
      "Iteration 865, loss = 0.17126985\n",
      "Iteration 866, loss = 0.17112571\n",
      "Iteration 867, loss = 0.17098178\n",
      "Iteration 868, loss = 0.17083819\n",
      "Iteration 869, loss = 0.17069487\n",
      "Iteration 870, loss = 0.17055186\n",
      "Iteration 871, loss = 0.17040912\n",
      "Iteration 872, loss = 0.17026664\n",
      "Iteration 873, loss = 0.17012455\n",
      "Iteration 874, loss = 0.16998263\n",
      "Iteration 875, loss = 0.16984106\n",
      "Iteration 876, loss = 0.16969972\n",
      "Iteration 877, loss = 0.16955877\n",
      "Iteration 878, loss = 0.16941797\n",
      "Iteration 879, loss = 0.16927756\n",
      "Iteration 880, loss = 0.16913738\n",
      "Iteration 881, loss = 0.16899747\n",
      "Iteration 882, loss = 0.16885791\n",
      "Iteration 883, loss = 0.16871853\n",
      "Iteration 884, loss = 0.16857952\n",
      "Iteration 885, loss = 0.16844070\n",
      "Iteration 886, loss = 0.16830225\n",
      "Iteration 887, loss = 0.16816398\n",
      "Iteration 888, loss = 0.16802608\n",
      "Iteration 889, loss = 0.16788838\n",
      "Iteration 890, loss = 0.16775096\n",
      "Iteration 891, loss = 0.16761391\n",
      "Iteration 892, loss = 0.16747702\n",
      "Iteration 893, loss = 0.16734044\n",
      "Iteration 894, loss = 0.16720411\n",
      "Iteration 895, loss = 0.16706812\n",
      "Iteration 896, loss = 0.16693232\n",
      "Iteration 897, loss = 0.16679684\n",
      "Iteration 898, loss = 0.16666165\n",
      "Iteration 899, loss = 0.16652667\n",
      "Iteration 900, loss = 0.16639207\n",
      "Iteration 901, loss = 0.16625761\n",
      "Iteration 902, loss = 0.16612351\n",
      "Iteration 903, loss = 0.16598960\n",
      "Iteration 904, loss = 0.16585598\n",
      "Iteration 905, loss = 0.16572266\n",
      "Iteration 906, loss = 0.16558957\n",
      "Iteration 907, loss = 0.16545681\n",
      "Iteration 908, loss = 0.16532421\n",
      "Iteration 909, loss = 0.16519201\n",
      "Iteration 910, loss = 0.16505995\n",
      "Iteration 911, loss = 0.16492823\n",
      "Iteration 912, loss = 0.16479670\n",
      "Iteration 913, loss = 0.16466550\n",
      "Iteration 914, loss = 0.16453451\n",
      "Iteration 915, loss = 0.16440380\n",
      "Iteration 916, loss = 0.16427335\n",
      "Iteration 917, loss = 0.16414314\n",
      "Iteration 918, loss = 0.16401325\n",
      "Iteration 919, loss = 0.16388353\n",
      "Iteration 920, loss = 0.16375412\n",
      "Iteration 921, loss = 0.16362491\n",
      "Iteration 922, loss = 0.16349604\n",
      "Iteration 923, loss = 0.16336731\n",
      "Iteration 924, loss = 0.16323895\n",
      "Iteration 925, loss = 0.16311076\n",
      "Iteration 926, loss = 0.16298287\n",
      "Iteration 927, loss = 0.16285520\n",
      "Iteration 928, loss = 0.16272774\n",
      "Iteration 929, loss = 0.16260067\n",
      "Iteration 930, loss = 0.16247370\n",
      "Iteration 931, loss = 0.16234705\n",
      "Iteration 932, loss = 0.16222061\n",
      "Iteration 933, loss = 0.16209448\n",
      "Iteration 934, loss = 0.16196857\n",
      "Iteration 935, loss = 0.16184304\n",
      "Iteration 936, loss = 0.16171776\n",
      "Iteration 937, loss = 0.16159273\n",
      "Iteration 938, loss = 0.16146801\n",
      "Iteration 939, loss = 0.16134347\n",
      "Iteration 940, loss = 0.16121955\n",
      "Iteration 941, loss = 0.16109577\n",
      "Iteration 942, loss = 0.16097237\n",
      "Iteration 943, loss = 0.16084915\n",
      "Iteration 944, loss = 0.16072616\n",
      "Iteration 945, loss = 0.16060348\n",
      "Iteration 946, loss = 0.16048104\n",
      "Iteration 947, loss = 0.16035881\n",
      "Iteration 948, loss = 0.16023686\n",
      "Iteration 949, loss = 0.16011517\n",
      "Iteration 950, loss = 0.15999368\n",
      "Iteration 951, loss = 0.15987247\n",
      "Iteration 952, loss = 0.15975145\n",
      "Iteration 953, loss = 0.15963073\n",
      "Iteration 954, loss = 0.15951020\n",
      "Iteration 955, loss = 0.15938995\n",
      "Iteration 956, loss = 0.15926988\n",
      "Iteration 957, loss = 0.15915015\n",
      "Iteration 958, loss = 0.15903054\n",
      "Iteration 959, loss = 0.15891118\n",
      "Iteration 960, loss = 0.15879206\n",
      "Iteration 961, loss = 0.15867324\n",
      "Iteration 962, loss = 0.15855456\n",
      "Iteration 963, loss = 0.15843617\n",
      "Iteration 964, loss = 0.15831794\n",
      "Iteration 965, loss = 0.15820006\n",
      "Iteration 966, loss = 0.15808230\n",
      "Iteration 967, loss = 0.15796480\n",
      "Iteration 968, loss = 0.15784752\n",
      "Iteration 969, loss = 0.15773051\n",
      "Iteration 970, loss = 0.15761367\n",
      "Iteration 971, loss = 0.15749713\n",
      "Iteration 972, loss = 0.15738071\n",
      "Iteration 973, loss = 0.15726464\n",
      "Iteration 974, loss = 0.15714867\n",
      "Iteration 975, loss = 0.15703303\n",
      "Iteration 976, loss = 0.15691753\n",
      "Iteration 977, loss = 0.15680229\n",
      "Iteration 978, loss = 0.15668727\n",
      "Iteration 979, loss = 0.15657244\n",
      "Iteration 980, loss = 0.15645788\n",
      "Iteration 981, loss = 0.15634348\n",
      "Iteration 982, loss = 0.15622940\n",
      "Iteration 983, loss = 0.15611543\n",
      "Iteration 984, loss = 0.15600178\n",
      "Iteration 985, loss = 0.15588826\n",
      "Iteration 986, loss = 0.15577500\n",
      "Iteration 987, loss = 0.15566190\n",
      "Iteration 988, loss = 0.15554914\n",
      "Iteration 989, loss = 0.15543648\n",
      "Iteration 990, loss = 0.15532408\n",
      "Iteration 991, loss = 0.15521186\n",
      "Iteration 992, loss = 0.15509994\n",
      "Iteration 993, loss = 0.15498812\n",
      "Iteration 994, loss = 0.15487662\n",
      "Iteration 995, loss = 0.15476523\n",
      "Iteration 996, loss = 0.15465412\n",
      "Iteration 997, loss = 0.15454316\n",
      "Iteration 998, loss = 0.15443251\n",
      "Iteration 999, loss = 0.15432195\n",
      "Iteration 1000, loss = 0.15421173\n",
      "Iteration 1, loss = 1.34975994\n",
      "Iteration 2, loss = 1.32617022\n",
      "Iteration 3, loss = 1.29444823\n",
      "Iteration 4, loss = 1.25692045\n",
      "Iteration 5, loss = 1.21557162\n",
      "Iteration 6, loss = 1.17216352\n",
      "Iteration 7, loss = 1.12785756\n",
      "Iteration 8, loss = 1.08378052\n",
      "Iteration 9, loss = 1.04099714\n",
      "Iteration 10, loss = 1.00059309\n",
      "Iteration 11, loss = 0.96384740\n",
      "Iteration 12, loss = 0.93167760\n",
      "Iteration 13, loss = 0.90456750\n",
      "Iteration 14, loss = 0.88294024\n",
      "Iteration 15, loss = 0.86650962\n",
      "Iteration 16, loss = 0.85461995\n",
      "Iteration 17, loss = 0.84619075\n",
      "Iteration 18, loss = 0.84018062\n",
      "Iteration 19, loss = 0.83506939\n",
      "Iteration 20, loss = 0.82955357\n",
      "Iteration 21, loss = 0.82293840\n",
      "Iteration 22, loss = 0.81475258\n",
      "Iteration 23, loss = 0.80504016\n",
      "Iteration 24, loss = 0.79417991\n",
      "Iteration 25, loss = 0.78303622\n",
      "Iteration 26, loss = 0.77191343\n",
      "Iteration 27, loss = 0.76081623\n",
      "Iteration 28, loss = 0.74986613\n",
      "Iteration 29, loss = 0.73916963\n",
      "Iteration 30, loss = 0.72906942\n",
      "Iteration 31, loss = 0.71971934\n",
      "Iteration 32, loss = 0.71128565\n",
      "Iteration 33, loss = 0.70376162\n",
      "Iteration 34, loss = 0.69701422\n",
      "Iteration 35, loss = 0.69098953\n",
      "Iteration 36, loss = 0.68537301\n",
      "Iteration 37, loss = 0.68007128\n",
      "Iteration 38, loss = 0.67501539\n",
      "Iteration 39, loss = 0.67012521\n",
      "Iteration 40, loss = 0.66535922\n",
      "Iteration 41, loss = 0.66069840\n",
      "Iteration 42, loss = 0.65608235\n",
      "Iteration 43, loss = 0.65151083\n",
      "Iteration 44, loss = 0.64700411\n",
      "Iteration 45, loss = 0.64257711\n",
      "Iteration 46, loss = 0.63823051\n",
      "Iteration 47, loss = 0.63394611\n",
      "Iteration 48, loss = 0.62973184\n",
      "Iteration 49, loss = 0.62559703\n",
      "Iteration 50, loss = 0.62158940\n",
      "Iteration 51, loss = 0.61771054\n",
      "Iteration 52, loss = 0.61396856\n",
      "Iteration 53, loss = 0.61037740\n",
      "Iteration 54, loss = 0.60696316\n",
      "Iteration 55, loss = 0.60370493\n",
      "Iteration 56, loss = 0.60057334\n",
      "Iteration 57, loss = 0.59757731\n",
      "Iteration 58, loss = 0.59464705\n",
      "Iteration 59, loss = 0.59176149\n",
      "Iteration 60, loss = 0.58891423\n",
      "Iteration 61, loss = 0.58609621\n",
      "Iteration 62, loss = 0.58330361\n",
      "Iteration 63, loss = 0.58053381\n",
      "Iteration 64, loss = 0.57779045\n",
      "Iteration 65, loss = 0.57507068\n",
      "Iteration 66, loss = 0.57237960\n",
      "Iteration 67, loss = 0.56972607\n",
      "Iteration 68, loss = 0.56710891\n",
      "Iteration 69, loss = 0.56452771\n",
      "Iteration 70, loss = 0.56198875\n",
      "Iteration 71, loss = 0.55948809\n",
      "Iteration 72, loss = 0.55705717\n",
      "Iteration 73, loss = 0.55467899\n",
      "Iteration 74, loss = 0.55234484\n",
      "Iteration 75, loss = 0.55004536\n",
      "Iteration 76, loss = 0.54778379\n",
      "Iteration 77, loss = 0.54556426\n",
      "Iteration 78, loss = 0.54338085\n",
      "Iteration 79, loss = 0.54122814\n",
      "Iteration 80, loss = 0.53910669\n",
      "Iteration 81, loss = 0.53701367\n",
      "Iteration 82, loss = 0.53494934\n",
      "Iteration 83, loss = 0.53291129\n",
      "Iteration 84, loss = 0.53089937\n",
      "Iteration 85, loss = 0.52891338\n",
      "Iteration 86, loss = 0.52695277\n",
      "Iteration 87, loss = 0.52501757\n",
      "Iteration 88, loss = 0.52310756\n",
      "Iteration 89, loss = 0.52122188\n",
      "Iteration 90, loss = 0.51936020\n",
      "Iteration 91, loss = 0.51752233\n",
      "Iteration 92, loss = 0.51570751\n",
      "Iteration 93, loss = 0.51391564\n",
      "Iteration 94, loss = 0.51214796\n",
      "Iteration 95, loss = 0.51040296\n",
      "Iteration 96, loss = 0.50868317\n",
      "Iteration 97, loss = 0.50698635\n",
      "Iteration 98, loss = 0.50531018\n",
      "Iteration 99, loss = 0.50365540\n",
      "Iteration 100, loss = 0.50201890\n",
      "Iteration 101, loss = 0.50040164\n",
      "Iteration 102, loss = 0.49880408\n",
      "Iteration 103, loss = 0.49722548\n",
      "Iteration 104, loss = 0.49566429\n",
      "Iteration 105, loss = 0.49412111\n",
      "Iteration 106, loss = 0.49259530\n",
      "Iteration 107, loss = 0.49108668\n",
      "Iteration 108, loss = 0.48959517\n",
      "Iteration 109, loss = 0.48812044\n",
      "Iteration 110, loss = 0.48666181\n",
      "Iteration 111, loss = 0.48521898\n",
      "Iteration 112, loss = 0.48379182\n",
      "Iteration 113, loss = 0.48237962\n",
      "Iteration 114, loss = 0.48098268\n",
      "Iteration 115, loss = 0.47960056\n",
      "Iteration 116, loss = 0.47823455\n",
      "Iteration 117, loss = 0.47688187\n",
      "Iteration 118, loss = 0.47554245\n",
      "Iteration 119, loss = 0.47421631\n",
      "Iteration 120, loss = 0.47290355\n",
      "Iteration 121, loss = 0.47160370\n",
      "Iteration 122, loss = 0.47031648\n",
      "Iteration 123, loss = 0.46904127\n",
      "Iteration 124, loss = 0.46777806\n",
      "Iteration 125, loss = 0.46652657\n",
      "Iteration 126, loss = 0.46528649\n",
      "Iteration 127, loss = 0.46405806\n",
      "Iteration 128, loss = 0.46284058\n",
      "Iteration 129, loss = 0.46163391\n",
      "Iteration 130, loss = 0.46043782\n",
      "Iteration 131, loss = 0.45925080\n",
      "Iteration 132, loss = 0.45807460\n",
      "Iteration 133, loss = 0.45690817\n",
      "Iteration 134, loss = 0.45575142\n",
      "Iteration 135, loss = 0.45460296\n",
      "Iteration 136, loss = 0.45346387\n",
      "Iteration 137, loss = 0.45233319\n",
      "Iteration 138, loss = 0.45121166\n",
      "Iteration 139, loss = 0.45009854\n",
      "Iteration 140, loss = 0.44899276\n",
      "Iteration 141, loss = 0.44789379\n",
      "Iteration 142, loss = 0.44680302\n",
      "Iteration 143, loss = 0.44572032\n",
      "Iteration 144, loss = 0.44464468\n",
      "Iteration 145, loss = 0.44357501\n",
      "Iteration 146, loss = 0.44251316\n",
      "Iteration 147, loss = 0.44145733\n",
      "Iteration 148, loss = 0.44040724\n",
      "Iteration 149, loss = 0.43936101\n",
      "Iteration 150, loss = 0.43832156\n",
      "Iteration 151, loss = 0.43728595\n",
      "Iteration 152, loss = 0.43625636\n",
      "Iteration 153, loss = 0.43522776\n",
      "Iteration 154, loss = 0.43420190\n",
      "Iteration 155, loss = 0.43317939\n",
      "Iteration 156, loss = 0.43215781\n",
      "Iteration 157, loss = 0.43113927\n",
      "Iteration 158, loss = 0.43012385\n",
      "Iteration 159, loss = 0.42910901\n",
      "Iteration 160, loss = 0.42809300\n",
      "Iteration 161, loss = 0.42707842\n",
      "Iteration 162, loss = 0.42606359\n",
      "Iteration 163, loss = 0.42505190\n",
      "Iteration 164, loss = 0.42404065\n",
      "Iteration 165, loss = 0.42303321\n",
      "Iteration 166, loss = 0.42202476\n",
      "Iteration 167, loss = 0.42100648\n",
      "Iteration 168, loss = 0.41998453\n",
      "Iteration 169, loss = 0.41895525\n",
      "Iteration 170, loss = 0.41792484\n",
      "Iteration 171, loss = 0.41688542\n",
      "Iteration 172, loss = 0.41584495\n",
      "Iteration 173, loss = 0.41480095\n",
      "Iteration 174, loss = 0.41375098\n",
      "Iteration 175, loss = 0.41270509\n",
      "Iteration 176, loss = 0.41166258\n",
      "Iteration 177, loss = 0.41063203\n",
      "Iteration 178, loss = 0.40961035\n",
      "Iteration 179, loss = 0.40859363\n",
      "Iteration 180, loss = 0.40759452\n",
      "Iteration 181, loss = 0.40660895\n",
      "Iteration 182, loss = 0.40563892\n",
      "Iteration 183, loss = 0.40468641\n",
      "Iteration 184, loss = 0.40375227\n",
      "Iteration 185, loss = 0.40284825\n",
      "Iteration 186, loss = 0.40196363\n",
      "Iteration 187, loss = 0.40109507\n",
      "Iteration 188, loss = 0.40024054\n",
      "Iteration 189, loss = 0.39939776\n",
      "Iteration 190, loss = 0.39856913\n",
      "Iteration 191, loss = 0.39775639\n",
      "Iteration 192, loss = 0.39695267\n",
      "Iteration 193, loss = 0.39615502\n",
      "Iteration 194, loss = 0.39536669\n",
      "Iteration 195, loss = 0.39458915\n",
      "Iteration 196, loss = 0.39382080\n",
      "Iteration 197, loss = 0.39305975\n",
      "Iteration 198, loss = 0.39230628\n",
      "Iteration 199, loss = 0.39155750\n",
      "Iteration 200, loss = 0.39081282\n",
      "Iteration 201, loss = 0.39007245\n",
      "Iteration 202, loss = 0.38933638\n",
      "Iteration 203, loss = 0.38860417\n",
      "Iteration 204, loss = 0.38787583\n",
      "Iteration 205, loss = 0.38715145\n",
      "Iteration 206, loss = 0.38643179\n",
      "Iteration 207, loss = 0.38571678\n",
      "Iteration 208, loss = 0.38500592\n",
      "Iteration 209, loss = 0.38429817\n",
      "Iteration 210, loss = 0.38359381\n",
      "Iteration 211, loss = 0.38289299\n",
      "Iteration 212, loss = 0.38219566\n",
      "Iteration 213, loss = 0.38150208\n",
      "Iteration 214, loss = 0.38081178\n",
      "Iteration 215, loss = 0.38012449\n",
      "Iteration 216, loss = 0.37944013\n",
      "Iteration 217, loss = 0.37875887\n",
      "Iteration 218, loss = 0.37808060\n",
      "Iteration 219, loss = 0.37740542\n",
      "Iteration 220, loss = 0.37673308\n",
      "Iteration 221, loss = 0.37606366\n",
      "Iteration 222, loss = 0.37539705\n",
      "Iteration 223, loss = 0.37473340\n",
      "Iteration 224, loss = 0.37407258\n",
      "Iteration 225, loss = 0.37341437\n",
      "Iteration 226, loss = 0.37275903\n",
      "Iteration 227, loss = 0.37210645\n",
      "Iteration 228, loss = 0.37145647\n",
      "Iteration 229, loss = 0.37080920\n",
      "Iteration 230, loss = 0.37016459\n",
      "Iteration 231, loss = 0.36952243\n",
      "Iteration 232, loss = 0.36888294\n",
      "Iteration 233, loss = 0.36824613\n",
      "Iteration 234, loss = 0.36761180\n",
      "Iteration 235, loss = 0.36697991\n",
      "Iteration 236, loss = 0.36635061\n",
      "Iteration 237, loss = 0.36572346\n",
      "Iteration 238, loss = 0.36509881\n",
      "Iteration 239, loss = 0.36447646\n",
      "Iteration 240, loss = 0.36385651\n",
      "Iteration 241, loss = 0.36323883\n",
      "Iteration 242, loss = 0.36262344\n",
      "Iteration 243, loss = 0.36201041\n",
      "Iteration 244, loss = 0.36139959\n",
      "Iteration 245, loss = 0.36079111\n",
      "Iteration 246, loss = 0.36018476\n",
      "Iteration 247, loss = 0.35958064\n",
      "Iteration 248, loss = 0.35897867\n",
      "Iteration 249, loss = 0.35837883\n",
      "Iteration 250, loss = 0.35778114\n",
      "Iteration 251, loss = 0.35718555\n",
      "Iteration 252, loss = 0.35659249\n",
      "Iteration 253, loss = 0.35600120\n",
      "Iteration 254, loss = 0.35541199\n",
      "Iteration 255, loss = 0.35482479\n",
      "Iteration 256, loss = 0.35423964\n",
      "Iteration 257, loss = 0.35365646\n",
      "Iteration 258, loss = 0.35307524\n",
      "Iteration 259, loss = 0.35249616\n",
      "Iteration 260, loss = 0.35191895\n",
      "Iteration 261, loss = 0.35134374\n",
      "Iteration 262, loss = 0.35077040\n",
      "Iteration 263, loss = 0.35019896\n",
      "Iteration 264, loss = 0.34962946\n",
      "Iteration 265, loss = 0.34906178\n",
      "Iteration 266, loss = 0.34849600\n",
      "Iteration 267, loss = 0.34793205\n",
      "Iteration 268, loss = 0.34737000\n",
      "Iteration 269, loss = 0.34680966\n",
      "Iteration 270, loss = 0.34625119\n",
      "Iteration 271, loss = 0.34569445\n",
      "Iteration 272, loss = 0.34513957\n",
      "Iteration 273, loss = 0.34458640\n",
      "Iteration 274, loss = 0.34403486\n",
      "Iteration 275, loss = 0.34348512\n",
      "Iteration 276, loss = 0.34293704\n",
      "Iteration 277, loss = 0.34239070\n",
      "Iteration 278, loss = 0.34184618\n",
      "Iteration 279, loss = 0.34130325\n",
      "Iteration 280, loss = 0.34076196\n",
      "Iteration 281, loss = 0.34022238\n",
      "Iteration 282, loss = 0.33968442\n",
      "Iteration 283, loss = 0.33914810\n",
      "Iteration 284, loss = 0.33861335\n",
      "Iteration 285, loss = 0.33808028\n",
      "Iteration 286, loss = 0.33754875\n",
      "Iteration 287, loss = 0.33701874\n",
      "Iteration 288, loss = 0.33649031\n",
      "Iteration 289, loss = 0.33596338\n",
      "Iteration 290, loss = 0.33543803\n",
      "Iteration 291, loss = 0.33491416\n",
      "Iteration 292, loss = 0.33439187\n",
      "Iteration 293, loss = 0.33387108\n",
      "Iteration 294, loss = 0.33335180\n",
      "Iteration 295, loss = 0.33283411\n",
      "Iteration 296, loss = 0.33231784\n",
      "Iteration 297, loss = 0.33180305\n",
      "Iteration 298, loss = 0.33128973\n",
      "Iteration 299, loss = 0.33077790\n",
      "Iteration 300, loss = 0.33026748\n",
      "Iteration 301, loss = 0.32975853\n",
      "Iteration 302, loss = 0.32925099\n",
      "Iteration 303, loss = 0.32874485\n",
      "Iteration 304, loss = 0.32824018\n",
      "Iteration 305, loss = 0.32773689\n",
      "Iteration 306, loss = 0.32723503\n",
      "Iteration 307, loss = 0.32673460\n",
      "Iteration 308, loss = 0.32623552\n",
      "Iteration 309, loss = 0.32573781\n",
      "Iteration 310, loss = 0.32524152\n",
      "Iteration 311, loss = 0.32474659\n",
      "Iteration 312, loss = 0.32425302\n",
      "Iteration 313, loss = 0.32376077\n",
      "Iteration 314, loss = 0.32326986\n",
      "Iteration 315, loss = 0.32278029\n",
      "Iteration 316, loss = 0.32229207\n",
      "Iteration 317, loss = 0.32180516\n",
      "Iteration 318, loss = 0.32131953\n",
      "Iteration 319, loss = 0.32083519\n",
      "Iteration 320, loss = 0.32035222\n",
      "Iteration 321, loss = 0.31987042\n",
      "Iteration 322, loss = 0.31938995\n",
      "Iteration 323, loss = 0.31891078\n",
      "Iteration 324, loss = 0.31843298\n",
      "Iteration 325, loss = 0.31795691\n",
      "Iteration 326, loss = 0.31748206\n",
      "Iteration 327, loss = 0.31700852\n",
      "Iteration 328, loss = 0.31653623\n",
      "Iteration 329, loss = 0.31606524\n",
      "Iteration 330, loss = 0.31559547\n",
      "Iteration 331, loss = 0.31512691\n",
      "Iteration 332, loss = 0.31465964\n",
      "Iteration 333, loss = 0.31419352\n",
      "Iteration 334, loss = 0.31372872\n",
      "Iteration 335, loss = 0.31326505\n",
      "Iteration 336, loss = 0.31280262\n",
      "Iteration 337, loss = 0.31234143\n",
      "Iteration 338, loss = 0.31188137\n",
      "Iteration 339, loss = 0.31142251\n",
      "Iteration 340, loss = 0.31096482\n",
      "Iteration 341, loss = 0.31050831\n",
      "Iteration 342, loss = 0.31005294\n",
      "Iteration 343, loss = 0.30959869\n",
      "Iteration 344, loss = 0.30914561\n",
      "Iteration 345, loss = 0.30869369\n",
      "Iteration 346, loss = 0.30824285\n",
      "Iteration 347, loss = 0.30779318\n",
      "Iteration 348, loss = 0.30734461\n",
      "Iteration 349, loss = 0.30689718\n",
      "Iteration 350, loss = 0.30645086\n",
      "Iteration 351, loss = 0.30600561\n",
      "Iteration 352, loss = 0.30556147\n",
      "Iteration 353, loss = 0.30511844\n",
      "Iteration 354, loss = 0.30467647\n",
      "Iteration 355, loss = 0.30423562\n",
      "Iteration 356, loss = 0.30379580\n",
      "Iteration 357, loss = 0.30335713\n",
      "Iteration 358, loss = 0.30291945\n",
      "Iteration 359, loss = 0.30248287\n",
      "Iteration 360, loss = 0.30204732\n",
      "Iteration 361, loss = 0.30161290\n",
      "Iteration 362, loss = 0.30117945\n",
      "Iteration 363, loss = 0.30074708\n",
      "Iteration 364, loss = 0.30031574\n",
      "Iteration 365, loss = 0.29988554\n",
      "Iteration 366, loss = 0.29945631\n",
      "Iteration 367, loss = 0.29902816\n",
      "Iteration 368, loss = 0.29860099\n",
      "Iteration 369, loss = 0.29817493\n",
      "Iteration 370, loss = 0.29774982\n",
      "Iteration 371, loss = 0.29732577\n",
      "Iteration 372, loss = 0.29690268\n",
      "Iteration 373, loss = 0.29648069\n",
      "Iteration 374, loss = 0.29605963\n",
      "Iteration 375, loss = 0.29563967\n",
      "Iteration 376, loss = 0.29522068\n",
      "Iteration 377, loss = 0.29480277\n",
      "Iteration 378, loss = 0.29438578\n",
      "Iteration 379, loss = 0.29396979\n",
      "Iteration 380, loss = 0.29355476\n",
      "Iteration 381, loss = 0.29314073\n",
      "Iteration 382, loss = 0.29272765\n",
      "Iteration 383, loss = 0.29231559\n",
      "Iteration 384, loss = 0.29190442\n",
      "Iteration 385, loss = 0.29149430\n",
      "Iteration 386, loss = 0.29108506\n",
      "Iteration 387, loss = 0.29067677\n",
      "Iteration 388, loss = 0.29026947\n",
      "Iteration 389, loss = 0.28986316\n",
      "Iteration 390, loss = 0.28945801\n",
      "Iteration 391, loss = 0.28905381\n",
      "Iteration 392, loss = 0.28865053\n",
      "Iteration 393, loss = 0.28824826\n",
      "Iteration 394, loss = 0.28784686\n",
      "Iteration 395, loss = 0.28744638\n",
      "Iteration 396, loss = 0.28704686\n",
      "Iteration 397, loss = 0.28664825\n",
      "Iteration 398, loss = 0.28625056\n",
      "Iteration 399, loss = 0.28585380\n",
      "Iteration 400, loss = 0.28545793\n",
      "Iteration 401, loss = 0.28506300\n",
      "Iteration 402, loss = 0.28466892\n",
      "Iteration 403, loss = 0.28427579\n",
      "Iteration 404, loss = 0.28388358\n",
      "Iteration 405, loss = 0.28349219\n",
      "Iteration 406, loss = 0.28310175\n",
      "Iteration 407, loss = 0.28271219\n",
      "Iteration 408, loss = 0.28232353\n",
      "Iteration 409, loss = 0.28193572\n",
      "Iteration 410, loss = 0.28154882\n",
      "Iteration 411, loss = 0.28116279\n",
      "Iteration 412, loss = 0.28077762\n",
      "Iteration 413, loss = 0.28039335\n",
      "Iteration 414, loss = 0.28000993\n",
      "Iteration 415, loss = 0.27962739\n",
      "Iteration 416, loss = 0.27924571\n",
      "Iteration 417, loss = 0.27886487\n",
      "Iteration 418, loss = 0.27848495\n",
      "Iteration 419, loss = 0.27810582\n",
      "Iteration 420, loss = 0.27772759\n",
      "Iteration 421, loss = 0.27735022\n",
      "Iteration 422, loss = 0.27697374\n",
      "Iteration 423, loss = 0.27659806\n",
      "Iteration 424, loss = 0.27622324\n",
      "Iteration 425, loss = 0.27584928\n",
      "Iteration 426, loss = 0.27547613\n",
      "Iteration 427, loss = 0.27510382\n",
      "Iteration 428, loss = 0.27473237\n",
      "Iteration 429, loss = 0.27436172\n",
      "Iteration 430, loss = 0.27399193\n",
      "Iteration 431, loss = 0.27362297\n",
      "Iteration 432, loss = 0.27325483\n",
      "Iteration 433, loss = 0.27288749\n",
      "Iteration 434, loss = 0.27252103\n",
      "Iteration 435, loss = 0.27215535\n",
      "Iteration 436, loss = 0.27179047\n",
      "Iteration 437, loss = 0.27142640\n",
      "Iteration 438, loss = 0.27106319\n",
      "Iteration 439, loss = 0.27070073\n",
      "Iteration 440, loss = 0.27033910\n",
      "Iteration 441, loss = 0.26997826\n",
      "Iteration 442, loss = 0.26961824\n",
      "Iteration 443, loss = 0.26925902\n",
      "Iteration 444, loss = 0.26890063\n",
      "Iteration 445, loss = 0.26854300\n",
      "Iteration 446, loss = 0.26818619\n",
      "Iteration 447, loss = 0.26783016\n",
      "Iteration 448, loss = 0.26747490\n",
      "Iteration 449, loss = 0.26712045\n",
      "Iteration 450, loss = 0.26676678\n",
      "Iteration 451, loss = 0.26641389\n",
      "Iteration 452, loss = 0.26606176\n",
      "Iteration 453, loss = 0.26571042\n",
      "Iteration 454, loss = 0.26535986\n",
      "Iteration 455, loss = 0.26501006\n",
      "Iteration 456, loss = 0.26466102\n",
      "Iteration 457, loss = 0.26431289\n",
      "Iteration 458, loss = 0.26396542\n",
      "Iteration 459, loss = 0.26361871\n",
      "Iteration 460, loss = 0.26327277\n",
      "Iteration 461, loss = 0.26292759\n",
      "Iteration 462, loss = 0.26258319\n",
      "Iteration 463, loss = 0.26223957\n",
      "Iteration 464, loss = 0.26189667\n",
      "Iteration 465, loss = 0.26155457\n",
      "Iteration 466, loss = 0.26121317\n",
      "Iteration 467, loss = 0.26087255\n",
      "Iteration 468, loss = 0.26053266\n",
      "Iteration 469, loss = 0.26019352\n",
      "Iteration 470, loss = 0.25985517\n",
      "Iteration 471, loss = 0.25951748\n",
      "Iteration 472, loss = 0.25918060\n",
      "Iteration 473, loss = 0.25884440\n",
      "Iteration 474, loss = 0.25850899\n",
      "Iteration 475, loss = 0.25817427\n",
      "Iteration 476, loss = 0.25784027\n",
      "Iteration 477, loss = 0.25750703\n",
      "Iteration 478, loss = 0.25717452\n",
      "Iteration 479, loss = 0.25684273\n",
      "Iteration 480, loss = 0.25651165\n",
      "Iteration 481, loss = 0.25618129\n",
      "Iteration 482, loss = 0.25585168\n",
      "Iteration 483, loss = 0.25552273\n",
      "Iteration 484, loss = 0.25519455\n",
      "Iteration 485, loss = 0.25486705\n",
      "Iteration 486, loss = 0.25454029\n",
      "Iteration 487, loss = 0.25421419\n",
      "Iteration 488, loss = 0.25388886\n",
      "Iteration 489, loss = 0.25356415\n",
      "Iteration 490, loss = 0.25324023\n",
      "Iteration 491, loss = 0.25291698\n",
      "Iteration 492, loss = 0.25259440\n",
      "Iteration 493, loss = 0.25227256\n",
      "Iteration 494, loss = 0.25195139\n",
      "Iteration 495, loss = 0.25163099\n",
      "Iteration 496, loss = 0.25131129\n",
      "Iteration 497, loss = 0.25099225\n",
      "Iteration 498, loss = 0.25067401\n",
      "Iteration 499, loss = 0.25035638\n",
      "Iteration 500, loss = 0.25003940\n",
      "Iteration 501, loss = 0.24972323\n",
      "Iteration 502, loss = 0.24940763\n",
      "Iteration 503, loss = 0.24909283\n",
      "Iteration 504, loss = 0.24877860\n",
      "Iteration 505, loss = 0.24846518\n",
      "Iteration 506, loss = 0.24815233\n",
      "Iteration 507, loss = 0.24784012\n",
      "Iteration 508, loss = 0.24752869\n",
      "Iteration 509, loss = 0.24721786\n",
      "Iteration 510, loss = 0.24690774\n",
      "Iteration 511, loss = 0.24659831\n",
      "Iteration 512, loss = 0.24628944\n",
      "Iteration 513, loss = 0.24598138\n",
      "Iteration 514, loss = 0.24567383\n",
      "Iteration 515, loss = 0.24536709\n",
      "Iteration 516, loss = 0.24506088\n",
      "Iteration 517, loss = 0.24475547\n",
      "Iteration 518, loss = 0.24445061\n",
      "Iteration 519, loss = 0.24414654\n",
      "Iteration 520, loss = 0.24384302\n",
      "Iteration 521, loss = 0.24354026\n",
      "Iteration 522, loss = 0.24323804\n",
      "Iteration 523, loss = 0.24293657\n",
      "Iteration 524, loss = 0.24263570\n",
      "Iteration 525, loss = 0.24233546\n",
      "Iteration 526, loss = 0.24203594\n",
      "Iteration 527, loss = 0.24173694\n",
      "Iteration 528, loss = 0.24143874\n",
      "Iteration 529, loss = 0.24114105\n",
      "Iteration 530, loss = 0.24084409\n",
      "Iteration 531, loss = 0.24054766\n",
      "Iteration 532, loss = 0.24025197\n",
      "Iteration 533, loss = 0.23995682\n",
      "Iteration 534, loss = 0.23966239\n",
      "Iteration 535, loss = 0.23936849\n",
      "Iteration 536, loss = 0.23907534\n",
      "Iteration 537, loss = 0.23878270\n",
      "Iteration 538, loss = 0.23849083\n",
      "Iteration 539, loss = 0.23819945\n",
      "Iteration 540, loss = 0.23790868\n",
      "Iteration 541, loss = 0.23761868\n",
      "Iteration 542, loss = 0.23732917\n",
      "Iteration 543, loss = 0.23704026\n",
      "Iteration 544, loss = 0.23675204\n",
      "Iteration 545, loss = 0.23646442\n",
      "Iteration 546, loss = 0.23617739\n",
      "Iteration 547, loss = 0.23589104\n",
      "Iteration 548, loss = 0.23560518\n",
      "Iteration 549, loss = 0.23532008\n",
      "Iteration 550, loss = 0.23503548\n",
      "Iteration 551, loss = 0.23475148\n",
      "Iteration 552, loss = 0.23446815\n",
      "Iteration 553, loss = 0.23418534\n",
      "Iteration 554, loss = 0.23390327\n",
      "Iteration 555, loss = 0.23362164\n",
      "Iteration 556, loss = 0.23334075\n",
      "Iteration 557, loss = 0.23306035\n",
      "Iteration 558, loss = 0.23278063\n",
      "Iteration 559, loss = 0.23250143\n",
      "Iteration 560, loss = 0.23222286\n",
      "Iteration 561, loss = 0.23194486\n",
      "Iteration 562, loss = 0.23166748\n",
      "Iteration 563, loss = 0.23139069\n",
      "Iteration 564, loss = 0.23111444\n",
      "Iteration 565, loss = 0.23083887\n",
      "Iteration 566, loss = 0.23056380\n",
      "Iteration 567, loss = 0.23028937\n",
      "Iteration 568, loss = 0.23001544\n",
      "Iteration 569, loss = 0.22974218\n",
      "Iteration 570, loss = 0.22946945\n",
      "Iteration 571, loss = 0.22919732\n",
      "Iteration 572, loss = 0.22892578\n",
      "Iteration 573, loss = 0.22865475\n",
      "Iteration 574, loss = 0.22838441\n",
      "Iteration 575, loss = 0.22811451\n",
      "Iteration 576, loss = 0.22784551\n",
      "Iteration 577, loss = 0.22757672\n",
      "Iteration 578, loss = 0.22730870\n",
      "Iteration 579, loss = 0.22704124\n",
      "Iteration 580, loss = 0.22677436\n",
      "Iteration 581, loss = 0.22650807\n",
      "Iteration 582, loss = 0.22624232\n",
      "Iteration 583, loss = 0.22597715\n",
      "Iteration 584, loss = 0.22571254\n",
      "Iteration 585, loss = 0.22544849\n",
      "Iteration 586, loss = 0.22518501\n",
      "Iteration 587, loss = 0.22492208\n",
      "Iteration 588, loss = 0.22465972\n",
      "Iteration 589, loss = 0.22439790\n",
      "Iteration 590, loss = 0.22413664\n",
      "Iteration 591, loss = 0.22387593\n",
      "Iteration 592, loss = 0.22361579\n",
      "Iteration 593, loss = 0.22335618\n",
      "Iteration 594, loss = 0.22309712\n",
      "Iteration 595, loss = 0.22283863\n",
      "Iteration 596, loss = 0.22258066\n",
      "Iteration 597, loss = 0.22232324\n",
      "Iteration 598, loss = 0.22206637\n",
      "Iteration 599, loss = 0.22181004\n",
      "Iteration 600, loss = 0.22155425\n",
      "Iteration 601, loss = 0.22129899\n",
      "Iteration 602, loss = 0.22104428\n",
      "Iteration 603, loss = 0.22079011\n",
      "Iteration 604, loss = 0.22053647\n",
      "Iteration 605, loss = 0.22028337\n",
      "Iteration 606, loss = 0.22003080\n",
      "Iteration 607, loss = 0.21977877\n",
      "Iteration 608, loss = 0.21952727\n",
      "Iteration 609, loss = 0.21927629\n",
      "Iteration 610, loss = 0.21902584\n",
      "Iteration 611, loss = 0.21877592\n",
      "Iteration 612, loss = 0.21852651\n",
      "Iteration 613, loss = 0.21827764\n",
      "Iteration 614, loss = 0.21802929\n",
      "Iteration 615, loss = 0.21778146\n",
      "Iteration 616, loss = 0.21753416\n",
      "Iteration 617, loss = 0.21728749\n",
      "Iteration 618, loss = 0.21704133\n",
      "Iteration 619, loss = 0.21679569\n",
      "Iteration 620, loss = 0.21655055\n",
      "Iteration 621, loss = 0.21630593\n",
      "Iteration 622, loss = 0.21606182\n",
      "Iteration 623, loss = 0.21581822\n",
      "Iteration 624, loss = 0.21557514\n",
      "Iteration 625, loss = 0.21533256\n",
      "Iteration 626, loss = 0.21509049\n",
      "Iteration 627, loss = 0.21484893\n",
      "Iteration 628, loss = 0.21460788\n",
      "Iteration 629, loss = 0.21436733\n",
      "Iteration 630, loss = 0.21412728\n",
      "Iteration 631, loss = 0.21388774\n",
      "Iteration 632, loss = 0.21364870\n",
      "Iteration 633, loss = 0.21341016\n",
      "Iteration 634, loss = 0.21317213\n",
      "Iteration 635, loss = 0.21293460\n",
      "Iteration 636, loss = 0.21269757\n",
      "Iteration 637, loss = 0.21246106\n",
      "Iteration 638, loss = 0.21222501\n",
      "Iteration 639, loss = 0.21198948\n",
      "Iteration 640, loss = 0.21175444\n",
      "Iteration 641, loss = 0.21151989\n",
      "Iteration 642, loss = 0.21128582\n",
      "Iteration 643, loss = 0.21105225\n",
      "Iteration 644, loss = 0.21081918\n",
      "Iteration 645, loss = 0.21058663\n",
      "Iteration 646, loss = 0.21035456\n",
      "Iteration 647, loss = 0.21012299\n",
      "Iteration 648, loss = 0.20989188\n",
      "Iteration 649, loss = 0.20966127\n",
      "Iteration 650, loss = 0.20943114\n",
      "Iteration 651, loss = 0.20920148\n",
      "Iteration 652, loss = 0.20897231\n",
      "Iteration 653, loss = 0.20874361\n",
      "Iteration 654, loss = 0.20851539\n",
      "Iteration 655, loss = 0.20828766\n",
      "Iteration 656, loss = 0.20806038\n",
      "Iteration 657, loss = 0.20783359\n",
      "Iteration 658, loss = 0.20760734\n",
      "Iteration 659, loss = 0.20738159\n",
      "Iteration 660, loss = 0.20715629\n",
      "Iteration 661, loss = 0.20693146\n",
      "Iteration 662, loss = 0.20670714\n",
      "Iteration 663, loss = 0.20648326\n",
      "Iteration 664, loss = 0.20625983\n",
      "Iteration 665, loss = 0.20603687\n",
      "Iteration 666, loss = 0.20581433\n",
      "Iteration 667, loss = 0.20559231\n",
      "Iteration 668, loss = 0.20537077\n",
      "Iteration 669, loss = 0.20514968\n",
      "Iteration 670, loss = 0.20492907\n",
      "Iteration 671, loss = 0.20470891\n",
      "Iteration 672, loss = 0.20448921\n",
      "Iteration 673, loss = 0.20427000\n",
      "Iteration 674, loss = 0.20405121\n",
      "Iteration 675, loss = 0.20383289\n",
      "Iteration 676, loss = 0.20361502\n",
      "Iteration 677, loss = 0.20339763\n",
      "Iteration 678, loss = 0.20318067\n",
      "Iteration 679, loss = 0.20296416\n",
      "Iteration 680, loss = 0.20274810\n",
      "Iteration 681, loss = 0.20253250\n",
      "Iteration 682, loss = 0.20231734\n",
      "Iteration 683, loss = 0.20210262\n",
      "Iteration 684, loss = 0.20188835\n",
      "Iteration 685, loss = 0.20167453\n",
      "Iteration 686, loss = 0.20146115\n",
      "Iteration 687, loss = 0.20124821\n",
      "Iteration 688, loss = 0.20103571\n",
      "Iteration 689, loss = 0.20082365\n",
      "Iteration 690, loss = 0.20061205\n",
      "Iteration 691, loss = 0.20040088\n",
      "Iteration 692, loss = 0.20019015\n",
      "Iteration 693, loss = 0.19997984\n",
      "Iteration 694, loss = 0.19976998\n",
      "Iteration 695, loss = 0.19956056\n",
      "Iteration 696, loss = 0.19935157\n",
      "Iteration 697, loss = 0.19914300\n",
      "Iteration 698, loss = 0.19893487\n",
      "Iteration 699, loss = 0.19872716\n",
      "Iteration 700, loss = 0.19851989\n",
      "Iteration 701, loss = 0.19831306\n",
      "Iteration 702, loss = 0.19810664\n",
      "Iteration 703, loss = 0.19790065\n",
      "Iteration 704, loss = 0.19769511\n",
      "Iteration 705, loss = 0.19748999\n",
      "Iteration 706, loss = 0.19728529\n",
      "Iteration 707, loss = 0.19708103\n",
      "Iteration 708, loss = 0.19687717\n",
      "Iteration 709, loss = 0.19667373\n",
      "Iteration 710, loss = 0.19647072\n",
      "Iteration 711, loss = 0.19626812\n",
      "Iteration 712, loss = 0.19606593\n",
      "Iteration 713, loss = 0.19586416\n",
      "Iteration 714, loss = 0.19566281\n",
      "Iteration 715, loss = 0.19546187\n",
      "Iteration 716, loss = 0.19526134\n",
      "Iteration 717, loss = 0.19506122\n",
      "Iteration 718, loss = 0.19486151\n",
      "Iteration 719, loss = 0.19466221\n",
      "Iteration 720, loss = 0.19446331\n",
      "Iteration 721, loss = 0.19426482\n",
      "Iteration 722, loss = 0.19406674\n",
      "Iteration 723, loss = 0.19386906\n",
      "Iteration 724, loss = 0.19367180\n",
      "Iteration 725, loss = 0.19347495\n",
      "Iteration 726, loss = 0.19327852\n",
      "Iteration 727, loss = 0.19308249\n",
      "Iteration 728, loss = 0.19288685\n",
      "Iteration 729, loss = 0.19269160\n",
      "Iteration 730, loss = 0.19249674\n",
      "Iteration 731, loss = 0.19230228\n",
      "Iteration 732, loss = 0.19210821\n",
      "Iteration 733, loss = 0.19191456\n",
      "Iteration 734, loss = 0.19172131\n",
      "Iteration 735, loss = 0.19152845\n",
      "Iteration 736, loss = 0.19133597\n",
      "Iteration 737, loss = 0.19114388\n",
      "Iteration 738, loss = 0.19095218\n",
      "Iteration 739, loss = 0.19076088\n",
      "Iteration 740, loss = 0.19056997\n",
      "Iteration 741, loss = 0.19037944\n",
      "Iteration 742, loss = 0.19018932\n",
      "Iteration 743, loss = 0.18999957\n",
      "Iteration 744, loss = 0.18981022\n",
      "Iteration 745, loss = 0.18962125\n",
      "Iteration 746, loss = 0.18943267\n",
      "Iteration 747, loss = 0.18924446\n",
      "Iteration 748, loss = 0.18905664\n",
      "Iteration 749, loss = 0.18886920\n",
      "Iteration 750, loss = 0.18868214\n",
      "Iteration 751, loss = 0.18849545\n",
      "Iteration 752, loss = 0.18830916\n",
      "Iteration 753, loss = 0.18812323\n",
      "Iteration 754, loss = 0.18793769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 755, loss = 0.18775252\n",
      "Iteration 756, loss = 0.18756774\n",
      "Iteration 757, loss = 0.18738332\n",
      "Iteration 758, loss = 0.18719929\n",
      "Iteration 759, loss = 0.18701563\n",
      "Iteration 760, loss = 0.18683235\n",
      "Iteration 761, loss = 0.18664944\n",
      "Iteration 762, loss = 0.18646689\n",
      "Iteration 763, loss = 0.18628473\n",
      "Iteration 764, loss = 0.18610292\n",
      "Iteration 765, loss = 0.18592148\n",
      "Iteration 766, loss = 0.18574041\n",
      "Iteration 767, loss = 0.18555970\n",
      "Iteration 768, loss = 0.18537936\n",
      "Iteration 769, loss = 0.18519937\n",
      "Iteration 770, loss = 0.18501976\n",
      "Iteration 771, loss = 0.18484050\n",
      "Iteration 772, loss = 0.18466160\n",
      "Iteration 773, loss = 0.18448307\n",
      "Iteration 774, loss = 0.18430489\n",
      "Iteration 775, loss = 0.18412707\n",
      "Iteration 776, loss = 0.18394961\n",
      "Iteration 777, loss = 0.18377250\n",
      "Iteration 778, loss = 0.18359574\n",
      "Iteration 779, loss = 0.18341935\n",
      "Iteration 780, loss = 0.18324331\n",
      "Iteration 781, loss = 0.18306762\n",
      "Iteration 782, loss = 0.18289229\n",
      "Iteration 783, loss = 0.18271731\n",
      "Iteration 784, loss = 0.18254287\n",
      "Iteration 785, loss = 0.18236899\n",
      "Iteration 786, loss = 0.18219548\n",
      "Iteration 787, loss = 0.18202233\n",
      "Iteration 788, loss = 0.18184956\n",
      "Iteration 789, loss = 0.18167714\n",
      "Iteration 790, loss = 0.18150507\n",
      "Iteration 791, loss = 0.18133337\n",
      "Iteration 792, loss = 0.18116204\n",
      "Iteration 793, loss = 0.18099105\n",
      "Iteration 794, loss = 0.18082042\n",
      "Iteration 795, loss = 0.18065012\n",
      "Iteration 796, loss = 0.18048017\n",
      "Iteration 797, loss = 0.18031057\n",
      "Iteration 798, loss = 0.18014132\n",
      "Iteration 799, loss = 0.17997240\n",
      "Iteration 800, loss = 0.17980382\n",
      "Iteration 801, loss = 0.17963558\n",
      "Iteration 802, loss = 0.17946769\n",
      "Iteration 803, loss = 0.17930015\n",
      "Iteration 804, loss = 0.17913296\n",
      "Iteration 805, loss = 0.17896610\n",
      "Iteration 806, loss = 0.17879956\n",
      "Iteration 807, loss = 0.17863334\n",
      "Iteration 808, loss = 0.17846746\n",
      "Iteration 809, loss = 0.17830190\n",
      "Iteration 810, loss = 0.17813669\n",
      "Iteration 811, loss = 0.17797182\n",
      "Iteration 812, loss = 0.17780727\n",
      "Iteration 813, loss = 0.17764304\n",
      "Iteration 814, loss = 0.17747914\n",
      "Iteration 815, loss = 0.17731556\n",
      "Iteration 816, loss = 0.17715231\n",
      "Iteration 817, loss = 0.17698938\n",
      "Iteration 818, loss = 0.17682677\n",
      "Iteration 819, loss = 0.17666448\n",
      "Iteration 820, loss = 0.17650252\n",
      "Iteration 821, loss = 0.17634089\n",
      "Iteration 822, loss = 0.17617957\n",
      "Iteration 823, loss = 0.17601857\n",
      "Iteration 824, loss = 0.17585789\n",
      "Iteration 825, loss = 0.17569753\n",
      "Iteration 826, loss = 0.17553747\n",
      "Iteration 827, loss = 0.17537775\n",
      "Iteration 828, loss = 0.17521832\n",
      "Iteration 829, loss = 0.17505922\n",
      "Iteration 830, loss = 0.17490042\n",
      "Iteration 831, loss = 0.17474194\n",
      "Iteration 832, loss = 0.17458376\n",
      "Iteration 833, loss = 0.17442590\n",
      "Iteration 834, loss = 0.17426834\n",
      "Iteration 835, loss = 0.17411111\n",
      "Iteration 836, loss = 0.17395416\n",
      "Iteration 837, loss = 0.17379754\n",
      "Iteration 838, loss = 0.17364121\n",
      "Iteration 839, loss = 0.17348519\n",
      "Iteration 840, loss = 0.17332947\n",
      "Iteration 841, loss = 0.17317406\n",
      "Iteration 842, loss = 0.17301896\n",
      "Iteration 843, loss = 0.17286416\n",
      "Iteration 844, loss = 0.17270966\n",
      "Iteration 845, loss = 0.17255547\n",
      "Iteration 846, loss = 0.17240158\n",
      "Iteration 847, loss = 0.17224800\n",
      "Iteration 848, loss = 0.17209471\n",
      "Iteration 849, loss = 0.17194172\n",
      "Iteration 850, loss = 0.17178902\n",
      "Iteration 851, loss = 0.17163663\n",
      "Iteration 852, loss = 0.17148453\n",
      "Iteration 853, loss = 0.17133273\n",
      "Iteration 854, loss = 0.17118122\n",
      "Iteration 855, loss = 0.17103000\n",
      "Iteration 856, loss = 0.17087908\n",
      "Iteration 857, loss = 0.17072846\n",
      "Iteration 858, loss = 0.17057813\n",
      "Iteration 859, loss = 0.17042810\n",
      "Iteration 860, loss = 0.17027835\n",
      "Iteration 861, loss = 0.17012890\n",
      "Iteration 862, loss = 0.16997973\n",
      "Iteration 863, loss = 0.16983085\n",
      "Iteration 864, loss = 0.16968226\n",
      "Iteration 865, loss = 0.16953396\n",
      "Iteration 866, loss = 0.16938594\n",
      "Iteration 867, loss = 0.16923821\n",
      "Iteration 868, loss = 0.16909076\n",
      "Iteration 869, loss = 0.16894360\n",
      "Iteration 870, loss = 0.16879672\n",
      "Iteration 871, loss = 0.16865012\n",
      "Iteration 872, loss = 0.16850386\n",
      "Iteration 873, loss = 0.16835811\n",
      "Iteration 874, loss = 0.16821266\n",
      "Iteration 875, loss = 0.16806749\n",
      "Iteration 876, loss = 0.16792261\n",
      "Iteration 877, loss = 0.16777802\n",
      "Iteration 878, loss = 0.16763371\n",
      "Iteration 879, loss = 0.16748969\n",
      "Iteration 880, loss = 0.16734595\n",
      "Iteration 881, loss = 0.16720248\n",
      "Iteration 882, loss = 0.16705931\n",
      "Iteration 883, loss = 0.16691640\n",
      "Iteration 884, loss = 0.16677377\n",
      "Iteration 885, loss = 0.16663145\n",
      "Iteration 886, loss = 0.16648937\n",
      "Iteration 887, loss = 0.16634758\n",
      "Iteration 888, loss = 0.16620605\n",
      "Iteration 889, loss = 0.16606481\n",
      "Iteration 890, loss = 0.16592384\n",
      "Iteration 891, loss = 0.16578314\n",
      "Iteration 892, loss = 0.16564270\n",
      "Iteration 893, loss = 0.16550253\n",
      "Iteration 894, loss = 0.16536264\n",
      "Iteration 895, loss = 0.16522301\n",
      "Iteration 896, loss = 0.16508365\n",
      "Iteration 897, loss = 0.16494456\n",
      "Iteration 898, loss = 0.16480572\n",
      "Iteration 899, loss = 0.16466716\n",
      "Iteration 900, loss = 0.16452886\n",
      "Iteration 901, loss = 0.16439081\n",
      "Iteration 902, loss = 0.16425303\n",
      "Iteration 903, loss = 0.16411551\n",
      "Iteration 904, loss = 0.16397826\n",
      "Iteration 905, loss = 0.16384127\n",
      "Iteration 906, loss = 0.16370455\n",
      "Iteration 907, loss = 0.16356809\n",
      "Iteration 908, loss = 0.16343187\n",
      "Iteration 909, loss = 0.16329592\n",
      "Iteration 910, loss = 0.16316024\n",
      "Iteration 911, loss = 0.16302481\n",
      "Iteration 912, loss = 0.16288964\n",
      "Iteration 913, loss = 0.16275473\n",
      "Iteration 914, loss = 0.16262012\n",
      "Iteration 915, loss = 0.16248579\n",
      "Iteration 916, loss = 0.16235172\n",
      "Iteration 917, loss = 0.16221792\n",
      "Iteration 918, loss = 0.16208435\n",
      "Iteration 919, loss = 0.16195103\n",
      "Iteration 920, loss = 0.16181797\n",
      "Iteration 921, loss = 0.16168516\n",
      "Iteration 922, loss = 0.16155259\n",
      "Iteration 923, loss = 0.16142028\n",
      "Iteration 924, loss = 0.16128821\n",
      "Iteration 925, loss = 0.16115639\n",
      "Iteration 926, loss = 0.16102482\n",
      "Iteration 927, loss = 0.16089349\n",
      "Iteration 928, loss = 0.16076240\n",
      "Iteration 929, loss = 0.16063158\n",
      "Iteration 930, loss = 0.16050098\n",
      "Iteration 931, loss = 0.16037063\n",
      "Iteration 932, loss = 0.16024053\n",
      "Iteration 933, loss = 0.16011067\n",
      "Iteration 934, loss = 0.15998105\n",
      "Iteration 935, loss = 0.15985168\n",
      "Iteration 936, loss = 0.15972254\n",
      "Iteration 937, loss = 0.15959365\n",
      "Iteration 938, loss = 0.15946500\n",
      "Iteration 939, loss = 0.15933658\n",
      "Iteration 940, loss = 0.15920841\n",
      "Iteration 941, loss = 0.15908047\n",
      "Iteration 942, loss = 0.15895277\n",
      "Iteration 943, loss = 0.15882532\n",
      "Iteration 944, loss = 0.15869809\n",
      "Iteration 945, loss = 0.15857111\n",
      "Iteration 946, loss = 0.15844436\n",
      "Iteration 947, loss = 0.15831784\n",
      "Iteration 948, loss = 0.15819156\n",
      "Iteration 949, loss = 0.15806552\n",
      "Iteration 950, loss = 0.15793981\n",
      "Iteration 951, loss = 0.15781437\n",
      "Iteration 952, loss = 0.15768917\n",
      "Iteration 953, loss = 0.15756421\n",
      "Iteration 954, loss = 0.15743950\n",
      "Iteration 955, loss = 0.15731502\n",
      "Iteration 956, loss = 0.15719078\n",
      "Iteration 957, loss = 0.15706677\n",
      "Iteration 958, loss = 0.15694300\n",
      "Iteration 959, loss = 0.15681946\n",
      "Iteration 960, loss = 0.15669615\n",
      "Iteration 961, loss = 0.15657308\n",
      "Iteration 962, loss = 0.15645023\n",
      "Iteration 963, loss = 0.15632762\n",
      "Iteration 964, loss = 0.15620523\n",
      "Iteration 965, loss = 0.15608307\n",
      "Iteration 966, loss = 0.15596113\n",
      "Iteration 967, loss = 0.15583943\n",
      "Iteration 968, loss = 0.15571795\n",
      "Iteration 969, loss = 0.15559670\n",
      "Iteration 970, loss = 0.15547572\n",
      "Iteration 971, loss = 0.15535497\n",
      "Iteration 972, loss = 0.15523443\n",
      "Iteration 973, loss = 0.15511411\n",
      "Iteration 974, loss = 0.15499401\n",
      "Iteration 975, loss = 0.15487411\n",
      "Iteration 976, loss = 0.15475444\n",
      "Iteration 977, loss = 0.15463498\n",
      "Iteration 978, loss = 0.15451574\n",
      "Iteration 979, loss = 0.15439675\n",
      "Iteration 980, loss = 0.15427797\n",
      "Iteration 981, loss = 0.15415940\n",
      "Iteration 982, loss = 0.15404105\n",
      "Iteration 983, loss = 0.15392291\n",
      "Iteration 984, loss = 0.15380499\n",
      "Iteration 985, loss = 0.15368728\n",
      "Iteration 986, loss = 0.15356980\n",
      "Iteration 987, loss = 0.15345253\n",
      "Iteration 988, loss = 0.15333547\n",
      "Iteration 989, loss = 0.15321861\n",
      "Iteration 990, loss = 0.15310199\n",
      "Iteration 991, loss = 0.15298557\n",
      "Iteration 992, loss = 0.15286936\n",
      "Iteration 993, loss = 0.15275338\n",
      "Iteration 994, loss = 0.15263777\n",
      "Iteration 995, loss = 0.15252242\n",
      "Iteration 996, loss = 0.15240729\n",
      "Iteration 997, loss = 0.15229238\n",
      "Iteration 998, loss = 0.15217767\n",
      "Iteration 999, loss = 0.15206319\n",
      "Iteration 1000, loss = 0.15194892\n",
      "Iteration 1, loss = 1.34996548\n",
      "Iteration 2, loss = 1.32612428\n",
      "Iteration 3, loss = 1.29405243\n",
      "Iteration 4, loss = 1.25608480\n",
      "Iteration 5, loss = 1.21428653\n",
      "Iteration 6, loss = 1.17034915\n",
      "Iteration 7, loss = 1.12552698\n",
      "Iteration 8, loss = 1.08090013\n",
      "Iteration 9, loss = 1.03752665\n",
      "Iteration 10, loss = 0.99659600\n",
      "Iteration 11, loss = 0.95937619\n",
      "Iteration 12, loss = 0.92677733\n",
      "Iteration 13, loss = 0.89929481\n",
      "Iteration 14, loss = 0.87740447\n",
      "Iteration 15, loss = 0.86077411\n",
      "Iteration 16, loss = 0.84886351\n",
      "Iteration 17, loss = 0.84062711\n",
      "Iteration 18, loss = 0.83463813\n",
      "Iteration 19, loss = 0.82947762\n",
      "Iteration 20, loss = 0.82393638\n",
      "Iteration 21, loss = 0.81730766\n",
      "Iteration 22, loss = 0.80904078\n",
      "Iteration 23, loss = 0.79928767\n",
      "Iteration 24, loss = 0.78833964\n",
      "Iteration 25, loss = 0.77710158\n",
      "Iteration 26, loss = 0.76564441\n",
      "Iteration 27, loss = 0.75415239\n",
      "Iteration 28, loss = 0.74294333\n",
      "Iteration 29, loss = 0.73218560\n",
      "Iteration 30, loss = 0.72207844\n",
      "Iteration 31, loss = 0.71294576\n",
      "Iteration 32, loss = 0.70475321\n",
      "Iteration 33, loss = 0.69743311\n",
      "Iteration 34, loss = 0.69085190\n",
      "Iteration 35, loss = 0.68495912\n",
      "Iteration 36, loss = 0.67953014\n",
      "Iteration 37, loss = 0.67441594\n",
      "Iteration 38, loss = 0.66953110\n",
      "Iteration 39, loss = 0.66479665\n",
      "Iteration 40, loss = 0.66016689\n",
      "Iteration 41, loss = 0.65560758\n",
      "Iteration 42, loss = 0.65110709\n",
      "Iteration 43, loss = 0.64664781\n",
      "Iteration 44, loss = 0.64223647\n",
      "Iteration 45, loss = 0.63787459\n",
      "Iteration 46, loss = 0.63356575\n",
      "Iteration 47, loss = 0.62931804\n",
      "Iteration 48, loss = 0.62514051\n",
      "Iteration 49, loss = 0.62104451\n",
      "Iteration 50, loss = 0.61707215\n",
      "Iteration 51, loss = 0.61322626\n",
      "Iteration 52, loss = 0.60952934\n",
      "Iteration 53, loss = 0.60602885\n",
      "Iteration 54, loss = 0.60270710\n",
      "Iteration 55, loss = 0.59956014\n",
      "Iteration 56, loss = 0.59652573\n",
      "Iteration 57, loss = 0.59358191\n",
      "Iteration 58, loss = 0.59069772\n",
      "Iteration 59, loss = 0.58786319\n",
      "Iteration 60, loss = 0.58506999\n",
      "Iteration 61, loss = 0.58232262\n",
      "Iteration 62, loss = 0.57961068\n",
      "Iteration 63, loss = 0.57693138\n",
      "Iteration 64, loss = 0.57428461\n",
      "Iteration 65, loss = 0.57167242\n",
      "Iteration 66, loss = 0.56909143\n",
      "Iteration 67, loss = 0.56654199\n",
      "Iteration 68, loss = 0.56402917\n",
      "Iteration 69, loss = 0.56155161\n",
      "Iteration 70, loss = 0.55910791\n",
      "Iteration 71, loss = 0.55669801\n",
      "Iteration 72, loss = 0.55433110\n",
      "Iteration 73, loss = 0.55199841\n",
      "Iteration 74, loss = 0.54970869\n",
      "Iteration 75, loss = 0.54746486\n",
      "Iteration 76, loss = 0.54526019\n",
      "Iteration 77, loss = 0.54309171\n",
      "Iteration 78, loss = 0.54096707\n",
      "Iteration 79, loss = 0.53888682\n",
      "Iteration 80, loss = 0.53683982\n",
      "Iteration 81, loss = 0.53482314\n",
      "Iteration 82, loss = 0.53283558\n",
      "Iteration 83, loss = 0.53087429\n",
      "Iteration 84, loss = 0.52893934\n",
      "Iteration 85, loss = 0.52703059\n",
      "Iteration 86, loss = 0.52514662\n",
      "Iteration 87, loss = 0.52328802\n",
      "Iteration 88, loss = 0.52145288\n",
      "Iteration 89, loss = 0.51964154\n",
      "Iteration 90, loss = 0.51785544\n",
      "Iteration 91, loss = 0.51609222\n",
      "Iteration 92, loss = 0.51435426\n",
      "Iteration 93, loss = 0.51263929\n",
      "Iteration 94, loss = 0.51094644\n",
      "Iteration 95, loss = 0.50927513\n",
      "Iteration 96, loss = 0.50762488\n",
      "Iteration 97, loss = 0.50599538\n",
      "Iteration 98, loss = 0.50438754\n",
      "Iteration 99, loss = 0.50279992\n",
      "Iteration 100, loss = 0.50123163\n",
      "Iteration 101, loss = 0.49968330\n",
      "Iteration 102, loss = 0.49815435\n",
      "Iteration 103, loss = 0.49664434\n",
      "Iteration 104, loss = 0.49515214\n",
      "Iteration 105, loss = 0.49367794\n",
      "Iteration 106, loss = 0.49222105\n",
      "Iteration 107, loss = 0.49078019\n",
      "Iteration 108, loss = 0.48935541\n",
      "Iteration 109, loss = 0.48794717\n",
      "Iteration 110, loss = 0.48655596\n",
      "Iteration 111, loss = 0.48518037\n",
      "Iteration 112, loss = 0.48381984\n",
      "Iteration 113, loss = 0.48247407\n",
      "Iteration 114, loss = 0.48114274\n",
      "Iteration 115, loss = 0.47982559\n",
      "Iteration 116, loss = 0.47852231\n",
      "Iteration 117, loss = 0.47723265\n",
      "Iteration 118, loss = 0.47595628\n",
      "Iteration 119, loss = 0.47469370\n",
      "Iteration 120, loss = 0.47344420\n",
      "Iteration 121, loss = 0.47220767\n",
      "Iteration 122, loss = 0.47098437\n",
      "Iteration 123, loss = 0.46977287\n",
      "Iteration 124, loss = 0.46857211\n",
      "Iteration 125, loss = 0.46738279\n",
      "Iteration 126, loss = 0.46620479\n",
      "Iteration 127, loss = 0.46503860\n",
      "Iteration 128, loss = 0.46388401\n",
      "Iteration 129, loss = 0.46274118\n",
      "Iteration 130, loss = 0.46160902\n",
      "Iteration 131, loss = 0.46048666\n",
      "Iteration 132, loss = 0.45937363\n",
      "Iteration 133, loss = 0.45827094\n",
      "Iteration 134, loss = 0.45717834\n",
      "Iteration 135, loss = 0.45609541\n",
      "Iteration 136, loss = 0.45502077\n",
      "Iteration 137, loss = 0.45395536\n",
      "Iteration 138, loss = 0.45289829\n",
      "Iteration 139, loss = 0.45184945\n",
      "Iteration 140, loss = 0.45080926\n",
      "Iteration 141, loss = 0.44977565\n",
      "Iteration 142, loss = 0.44874919\n",
      "Iteration 143, loss = 0.44772992\n",
      "Iteration 144, loss = 0.44671854\n",
      "Iteration 145, loss = 0.44571350\n",
      "Iteration 146, loss = 0.44471508\n",
      "Iteration 147, loss = 0.44372382\n",
      "Iteration 148, loss = 0.44273821\n",
      "Iteration 149, loss = 0.44175736\n",
      "Iteration 150, loss = 0.44078323\n",
      "Iteration 151, loss = 0.43981361\n",
      "Iteration 152, loss = 0.43884986\n",
      "Iteration 153, loss = 0.43789008\n",
      "Iteration 154, loss = 0.43693563\n",
      "Iteration 155, loss = 0.43598326\n",
      "Iteration 156, loss = 0.43503338\n",
      "Iteration 157, loss = 0.43408587\n",
      "Iteration 158, loss = 0.43313717\n",
      "Iteration 159, loss = 0.43218996\n",
      "Iteration 160, loss = 0.43124455\n",
      "Iteration 161, loss = 0.43030198\n",
      "Iteration 162, loss = 0.42935876\n",
      "Iteration 163, loss = 0.42841749\n",
      "Iteration 164, loss = 0.42747590\n",
      "Iteration 165, loss = 0.42653786\n",
      "Iteration 166, loss = 0.42560382\n",
      "Iteration 167, loss = 0.42467426\n",
      "Iteration 168, loss = 0.42374572\n",
      "Iteration 169, loss = 0.42280409\n",
      "Iteration 170, loss = 0.42185869\n",
      "Iteration 171, loss = 0.42090683\n",
      "Iteration 172, loss = 0.41995141\n",
      "Iteration 173, loss = 0.41899091\n",
      "Iteration 174, loss = 0.41802667\n",
      "Iteration 175, loss = 0.41706224\n",
      "Iteration 176, loss = 0.41610226\n",
      "Iteration 177, loss = 0.41514429\n",
      "Iteration 178, loss = 0.41418301\n",
      "Iteration 179, loss = 0.41322177\n",
      "Iteration 180, loss = 0.41228831\n",
      "Iteration 181, loss = 0.41136517\n",
      "Iteration 182, loss = 0.41045850\n",
      "Iteration 183, loss = 0.40956276\n",
      "Iteration 184, loss = 0.40867235\n",
      "Iteration 185, loss = 0.40779360\n",
      "Iteration 186, loss = 0.40693810\n",
      "Iteration 187, loss = 0.40610465\n",
      "Iteration 188, loss = 0.40529019\n",
      "Iteration 189, loss = 0.40448875\n",
      "Iteration 190, loss = 0.40369725\n",
      "Iteration 191, loss = 0.40291730\n",
      "Iteration 192, loss = 0.40214537\n",
      "Iteration 193, loss = 0.40137914\n",
      "Iteration 194, loss = 0.40062294\n",
      "Iteration 195, loss = 0.39987875\n",
      "Iteration 196, loss = 0.39914344\n",
      "Iteration 197, loss = 0.39841348\n",
      "Iteration 198, loss = 0.39769481\n",
      "Iteration 199, loss = 0.39698471\n",
      "Iteration 200, loss = 0.39628197\n",
      "Iteration 201, loss = 0.39558572\n",
      "Iteration 202, loss = 0.39489612\n",
      "Iteration 203, loss = 0.39421238\n",
      "Iteration 204, loss = 0.39353331\n",
      "Iteration 205, loss = 0.39285948\n",
      "Iteration 206, loss = 0.39218920\n",
      "Iteration 207, loss = 0.39152273\n",
      "Iteration 208, loss = 0.39085972\n",
      "Iteration 209, loss = 0.39020014\n",
      "Iteration 210, loss = 0.38954399\n",
      "Iteration 211, loss = 0.38889218\n",
      "Iteration 212, loss = 0.38824453\n",
      "Iteration 213, loss = 0.38760057\n",
      "Iteration 214, loss = 0.38695976\n",
      "Iteration 215, loss = 0.38632207\n",
      "Iteration 216, loss = 0.38568728\n",
      "Iteration 217, loss = 0.38505568\n",
      "Iteration 218, loss = 0.38442767\n",
      "Iteration 219, loss = 0.38380255\n",
      "Iteration 220, loss = 0.38318043\n",
      "Iteration 221, loss = 0.38256122\n",
      "Iteration 222, loss = 0.38194475\n",
      "Iteration 223, loss = 0.38133111\n",
      "Iteration 224, loss = 0.38072011\n",
      "Iteration 225, loss = 0.38011175\n",
      "Iteration 226, loss = 0.37950608\n",
      "Iteration 227, loss = 0.37890301\n",
      "Iteration 228, loss = 0.37830251\n",
      "Iteration 229, loss = 0.37770468\n",
      "Iteration 230, loss = 0.37710930\n",
      "Iteration 231, loss = 0.37651626\n",
      "Iteration 232, loss = 0.37592559\n",
      "Iteration 233, loss = 0.37533724\n",
      "Iteration 234, loss = 0.37475128\n",
      "Iteration 235, loss = 0.37416769\n",
      "Iteration 236, loss = 0.37358639\n",
      "Iteration 237, loss = 0.37300750\n",
      "Iteration 238, loss = 0.37243093\n",
      "Iteration 239, loss = 0.37185661\n",
      "Iteration 240, loss = 0.37128465\n",
      "Iteration 241, loss = 0.37071483\n",
      "Iteration 242, loss = 0.37014722\n",
      "Iteration 243, loss = 0.36958179\n",
      "Iteration 244, loss = 0.36901856\n",
      "Iteration 245, loss = 0.36845746\n",
      "Iteration 246, loss = 0.36789847\n",
      "Iteration 247, loss = 0.36734157\n",
      "Iteration 248, loss = 0.36678674\n",
      "Iteration 249, loss = 0.36623395\n",
      "Iteration 250, loss = 0.36568320\n",
      "Iteration 251, loss = 0.36513443\n",
      "Iteration 252, loss = 0.36458761\n",
      "Iteration 253, loss = 0.36404278\n",
      "Iteration 254, loss = 0.36349990\n",
      "Iteration 255, loss = 0.36295894\n",
      "Iteration 256, loss = 0.36241989\n",
      "Iteration 257, loss = 0.36188268\n",
      "Iteration 258, loss = 0.36134736\n",
      "Iteration 259, loss = 0.36081389\n",
      "Iteration 260, loss = 0.36028226\n",
      "Iteration 261, loss = 0.35975247\n",
      "Iteration 262, loss = 0.35922452\n",
      "Iteration 263, loss = 0.35869840\n",
      "Iteration 264, loss = 0.35817407\n",
      "Iteration 265, loss = 0.35765148\n",
      "Iteration 266, loss = 0.35713050\n",
      "Iteration 267, loss = 0.35661125\n",
      "Iteration 268, loss = 0.35609370\n",
      "Iteration 269, loss = 0.35557803\n",
      "Iteration 270, loss = 0.35506395\n",
      "Iteration 271, loss = 0.35455148\n",
      "Iteration 272, loss = 0.35404065\n",
      "Iteration 273, loss = 0.35353144\n",
      "Iteration 274, loss = 0.35302386\n",
      "Iteration 275, loss = 0.35251787\n",
      "Iteration 276, loss = 0.35201367\n",
      "Iteration 277, loss = 0.35151189\n",
      "Iteration 278, loss = 0.35101169\n",
      "Iteration 279, loss = 0.35051306\n",
      "Iteration 280, loss = 0.35001601\n",
      "Iteration 281, loss = 0.34952053\n",
      "Iteration 282, loss = 0.34902663\n",
      "Iteration 283, loss = 0.34853422\n",
      "Iteration 284, loss = 0.34804335\n",
      "Iteration 285, loss = 0.34755390\n",
      "Iteration 286, loss = 0.34706593\n",
      "Iteration 287, loss = 0.34657948\n",
      "Iteration 288, loss = 0.34609452\n",
      "Iteration 289, loss = 0.34561107\n",
      "Iteration 290, loss = 0.34512905\n",
      "Iteration 291, loss = 0.34464847\n",
      "Iteration 292, loss = 0.34416937\n",
      "Iteration 293, loss = 0.34369165\n",
      "Iteration 294, loss = 0.34321543\n",
      "Iteration 295, loss = 0.34274063\n",
      "Iteration 296, loss = 0.34226725\n",
      "Iteration 297, loss = 0.34179527\n",
      "Iteration 298, loss = 0.34132469\n",
      "Iteration 299, loss = 0.34085546\n",
      "Iteration 300, loss = 0.34038762\n",
      "Iteration 301, loss = 0.33992123\n",
      "Iteration 302, loss = 0.33945630\n",
      "Iteration 303, loss = 0.33899269\n",
      "Iteration 304, loss = 0.33853092\n",
      "Iteration 305, loss = 0.33807051\n",
      "Iteration 306, loss = 0.33761136\n",
      "Iteration 307, loss = 0.33715360\n",
      "Iteration 308, loss = 0.33669714\n",
      "Iteration 309, loss = 0.33624198\n",
      "Iteration 310, loss = 0.33578809\n",
      "Iteration 311, loss = 0.33533554\n",
      "Iteration 312, loss = 0.33488427\n",
      "Iteration 313, loss = 0.33443446\n",
      "Iteration 314, loss = 0.33398580\n",
      "Iteration 315, loss = 0.33353844\n",
      "Iteration 316, loss = 0.33309232\n",
      "Iteration 317, loss = 0.33264735\n",
      "Iteration 318, loss = 0.33220369\n",
      "Iteration 319, loss = 0.33176114\n",
      "Iteration 320, loss = 0.33131990\n",
      "Iteration 321, loss = 0.33087976\n",
      "Iteration 322, loss = 0.33044088\n",
      "Iteration 323, loss = 0.33000316\n",
      "Iteration 324, loss = 0.32956658\n",
      "Iteration 325, loss = 0.32913127\n",
      "Iteration 326, loss = 0.32869701\n",
      "Iteration 327, loss = 0.32826400\n",
      "Iteration 328, loss = 0.32783207\n",
      "Iteration 329, loss = 0.32740130\n",
      "Iteration 330, loss = 0.32697171\n",
      "Iteration 331, loss = 0.32654329\n",
      "Iteration 332, loss = 0.32611597\n",
      "Iteration 333, loss = 0.32568978\n",
      "Iteration 334, loss = 0.32526480\n",
      "Iteration 335, loss = 0.32484093\n",
      "Iteration 336, loss = 0.32441825\n",
      "Iteration 337, loss = 0.32399660\n",
      "Iteration 338, loss = 0.32357606\n",
      "Iteration 339, loss = 0.32315665\n",
      "Iteration 340, loss = 0.32273832\n",
      "Iteration 341, loss = 0.32232106\n",
      "Iteration 342, loss = 0.32190493\n",
      "Iteration 343, loss = 0.32148982\n",
      "Iteration 344, loss = 0.32107585\n",
      "Iteration 345, loss = 0.32066286\n",
      "Iteration 346, loss = 0.32025108\n",
      "Iteration 347, loss = 0.31984026\n",
      "Iteration 348, loss = 0.31943052\n",
      "Iteration 349, loss = 0.31902190\n",
      "Iteration 350, loss = 0.31861438\n",
      "Iteration 351, loss = 0.31820787\n",
      "Iteration 352, loss = 0.31780241\n",
      "Iteration 353, loss = 0.31739798\n",
      "Iteration 354, loss = 0.31699453\n",
      "Iteration 355, loss = 0.31659209\n",
      "Iteration 356, loss = 0.31619074\n",
      "Iteration 357, loss = 0.31579029\n",
      "Iteration 358, loss = 0.31539090\n",
      "Iteration 359, loss = 0.31499248\n",
      "Iteration 360, loss = 0.31459507\n",
      "Iteration 361, loss = 0.31419861\n",
      "Iteration 362, loss = 0.31380320\n",
      "Iteration 363, loss = 0.31340878\n",
      "Iteration 364, loss = 0.31301556\n",
      "Iteration 365, loss = 0.31262326\n",
      "Iteration 366, loss = 0.31223181\n",
      "Iteration 367, loss = 0.31184119\n",
      "Iteration 368, loss = 0.31145172\n",
      "Iteration 369, loss = 0.31106325\n",
      "Iteration 370, loss = 0.31067577\n",
      "Iteration 371, loss = 0.31028916\n",
      "Iteration 372, loss = 0.30990345\n",
      "Iteration 373, loss = 0.30951869\n",
      "Iteration 374, loss = 0.30913497\n",
      "Iteration 375, loss = 0.30875202\n",
      "Iteration 376, loss = 0.30837004\n",
      "Iteration 377, loss = 0.30798904\n",
      "Iteration 378, loss = 0.30760888\n",
      "Iteration 379, loss = 0.30722968\n",
      "Iteration 380, loss = 0.30685129\n",
      "Iteration 381, loss = 0.30647398\n",
      "Iteration 382, loss = 0.30609740\n",
      "Iteration 383, loss = 0.30572176\n",
      "Iteration 384, loss = 0.30534697\n",
      "Iteration 385, loss = 0.30497322\n",
      "Iteration 386, loss = 0.30460021\n",
      "Iteration 387, loss = 0.30422818\n",
      "Iteration 388, loss = 0.30385699\n",
      "Iteration 389, loss = 0.30348673\n",
      "Iteration 390, loss = 0.30311736\n",
      "Iteration 391, loss = 0.30274883\n",
      "Iteration 392, loss = 0.30238120\n",
      "Iteration 393, loss = 0.30201445\n",
      "Iteration 394, loss = 0.30164861\n",
      "Iteration 395, loss = 0.30128356\n",
      "Iteration 396, loss = 0.30091936\n",
      "Iteration 397, loss = 0.30055603\n",
      "Iteration 398, loss = 0.30019361\n",
      "Iteration 399, loss = 0.29983193\n",
      "Iteration 400, loss = 0.29947114\n",
      "Iteration 401, loss = 0.29911116\n",
      "Iteration 402, loss = 0.29875208\n",
      "Iteration 403, loss = 0.29839377\n",
      "Iteration 404, loss = 0.29803625\n",
      "Iteration 405, loss = 0.29767962\n",
      "Iteration 406, loss = 0.29732384\n",
      "Iteration 407, loss = 0.29696884\n",
      "Iteration 408, loss = 0.29661460\n",
      "Iteration 409, loss = 0.29626120\n",
      "Iteration 410, loss = 0.29590866\n",
      "Iteration 411, loss = 0.29555695\n",
      "Iteration 412, loss = 0.29520592\n",
      "Iteration 413, loss = 0.29485577\n",
      "Iteration 414, loss = 0.29450645\n",
      "Iteration 415, loss = 0.29415788\n",
      "Iteration 416, loss = 0.29381014\n",
      "Iteration 417, loss = 0.29346314\n",
      "Iteration 418, loss = 0.29311703\n",
      "Iteration 419, loss = 0.29277160\n",
      "Iteration 420, loss = 0.29242701\n",
      "Iteration 421, loss = 0.29208318\n",
      "Iteration 422, loss = 0.29174015\n",
      "Iteration 423, loss = 0.29139784\n",
      "Iteration 424, loss = 0.29105637\n",
      "Iteration 425, loss = 0.29071561\n",
      "Iteration 426, loss = 0.29037568\n",
      "Iteration 427, loss = 0.29003645\n",
      "Iteration 428, loss = 0.28969800\n",
      "Iteration 429, loss = 0.28936034\n",
      "Iteration 430, loss = 0.28902343\n",
      "Iteration 431, loss = 0.28868722\n",
      "Iteration 432, loss = 0.28835181\n",
      "Iteration 433, loss = 0.28801718\n",
      "Iteration 434, loss = 0.28768324\n",
      "Iteration 435, loss = 0.28735006\n",
      "Iteration 436, loss = 0.28701758\n",
      "Iteration 437, loss = 0.28668602\n",
      "Iteration 438, loss = 0.28635497\n",
      "Iteration 439, loss = 0.28602477\n",
      "Iteration 440, loss = 0.28569525\n",
      "Iteration 441, loss = 0.28536656\n",
      "Iteration 442, loss = 0.28503855\n",
      "Iteration 443, loss = 0.28471129\n",
      "Iteration 444, loss = 0.28438468\n",
      "Iteration 445, loss = 0.28405890\n",
      "Iteration 446, loss = 0.28373375\n",
      "Iteration 447, loss = 0.28340938\n",
      "Iteration 448, loss = 0.28308570\n",
      "Iteration 449, loss = 0.28276283\n",
      "Iteration 450, loss = 0.28244056\n",
      "Iteration 451, loss = 0.28211902\n",
      "Iteration 452, loss = 0.28179822\n",
      "Iteration 453, loss = 0.28147826\n",
      "Iteration 454, loss = 0.28115878\n",
      "Iteration 455, loss = 0.28084009\n",
      "Iteration 456, loss = 0.28052217\n",
      "Iteration 457, loss = 0.28020483\n",
      "Iteration 458, loss = 0.27988826\n",
      "Iteration 459, loss = 0.27957234\n",
      "Iteration 460, loss = 0.27925717\n",
      "Iteration 461, loss = 0.27894272\n",
      "Iteration 462, loss = 0.27862887\n",
      "Iteration 463, loss = 0.27831573\n",
      "Iteration 464, loss = 0.27800333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 465, loss = 0.27769158\n",
      "Iteration 466, loss = 0.27738048\n",
      "Iteration 467, loss = 0.27707009\n",
      "Iteration 468, loss = 0.27676042\n",
      "Iteration 469, loss = 0.27645137\n",
      "Iteration 470, loss = 0.27614296\n",
      "Iteration 471, loss = 0.27583528\n",
      "Iteration 472, loss = 0.27552830\n",
      "Iteration 473, loss = 0.27522196\n",
      "Iteration 474, loss = 0.27491623\n",
      "Iteration 475, loss = 0.27461121\n",
      "Iteration 476, loss = 0.27430695\n",
      "Iteration 477, loss = 0.27400315\n",
      "Iteration 478, loss = 0.27370011\n",
      "Iteration 479, loss = 0.27339780\n",
      "Iteration 480, loss = 0.27309604\n",
      "Iteration 481, loss = 0.27279496\n",
      "Iteration 482, loss = 0.27249457\n",
      "Iteration 483, loss = 0.27219490\n",
      "Iteration 484, loss = 0.27189576\n",
      "Iteration 485, loss = 0.27159733\n",
      "Iteration 486, loss = 0.27129949\n",
      "Iteration 487, loss = 0.27100242\n",
      "Iteration 488, loss = 0.27070590\n",
      "Iteration 489, loss = 0.27041006\n",
      "Iteration 490, loss = 0.27011478\n",
      "Iteration 491, loss = 0.26982028\n",
      "Iteration 492, loss = 0.26952628\n",
      "Iteration 493, loss = 0.26923295\n",
      "Iteration 494, loss = 0.26894031\n",
      "Iteration 495, loss = 0.26864827\n",
      "Iteration 496, loss = 0.26835682\n",
      "Iteration 497, loss = 0.26806602\n",
      "Iteration 498, loss = 0.26777592\n",
      "Iteration 499, loss = 0.26748638\n",
      "Iteration 500, loss = 0.26719745\n",
      "Iteration 501, loss = 0.26690915\n",
      "Iteration 502, loss = 0.26662160\n",
      "Iteration 503, loss = 0.26633447\n",
      "Iteration 504, loss = 0.26604806\n",
      "Iteration 505, loss = 0.26576227\n",
      "Iteration 506, loss = 0.26547711\n",
      "Iteration 507, loss = 0.26519252\n",
      "Iteration 508, loss = 0.26490853\n",
      "Iteration 509, loss = 0.26462517\n",
      "Iteration 510, loss = 0.26434252\n",
      "Iteration 511, loss = 0.26406039\n",
      "Iteration 512, loss = 0.26377886\n",
      "Iteration 513, loss = 0.26349804\n",
      "Iteration 514, loss = 0.26321772\n",
      "Iteration 515, loss = 0.26293803\n",
      "Iteration 516, loss = 0.26265895\n",
      "Iteration 517, loss = 0.26238053\n",
      "Iteration 518, loss = 0.26210263\n",
      "Iteration 519, loss = 0.26182535\n",
      "Iteration 520, loss = 0.26154861\n",
      "Iteration 521, loss = 0.26127261\n",
      "Iteration 522, loss = 0.26099704\n",
      "Iteration 523, loss = 0.26072211\n",
      "Iteration 524, loss = 0.26044784\n",
      "Iteration 525, loss = 0.26017412\n",
      "Iteration 526, loss = 0.25990094\n",
      "Iteration 527, loss = 0.25962837\n",
      "Iteration 528, loss = 0.25935636\n",
      "Iteration 529, loss = 0.25908498\n",
      "Iteration 530, loss = 0.25881413\n",
      "Iteration 531, loss = 0.25854386\n",
      "Iteration 532, loss = 0.25827430\n",
      "Iteration 533, loss = 0.25800508\n",
      "Iteration 534, loss = 0.25773658\n",
      "Iteration 535, loss = 0.25746869\n",
      "Iteration 536, loss = 0.25720133\n",
      "Iteration 537, loss = 0.25693450\n",
      "Iteration 538, loss = 0.25666825\n",
      "Iteration 539, loss = 0.25640252\n",
      "Iteration 540, loss = 0.25613746\n",
      "Iteration 541, loss = 0.25587289\n",
      "Iteration 542, loss = 0.25560890\n",
      "Iteration 543, loss = 0.25534555\n",
      "Iteration 544, loss = 0.25508264\n",
      "Iteration 545, loss = 0.25482030\n",
      "Iteration 546, loss = 0.25455862\n",
      "Iteration 547, loss = 0.25429742\n",
      "Iteration 548, loss = 0.25403676\n",
      "Iteration 549, loss = 0.25377666\n",
      "Iteration 550, loss = 0.25351721\n",
      "Iteration 551, loss = 0.25325822\n",
      "Iteration 552, loss = 0.25299978\n",
      "Iteration 553, loss = 0.25274189\n",
      "Iteration 554, loss = 0.25248461\n",
      "Iteration 555, loss = 0.25222784\n",
      "Iteration 556, loss = 0.25197158\n",
      "Iteration 557, loss = 0.25171584\n",
      "Iteration 558, loss = 0.25146078\n",
      "Iteration 559, loss = 0.25120614\n",
      "Iteration 560, loss = 0.25095205\n",
      "Iteration 561, loss = 0.25069857\n",
      "Iteration 562, loss = 0.25044555\n",
      "Iteration 563, loss = 0.25019306\n",
      "Iteration 564, loss = 0.24994116\n",
      "Iteration 565, loss = 0.24968979\n",
      "Iteration 566, loss = 0.24943896\n",
      "Iteration 567, loss = 0.24918860\n",
      "Iteration 568, loss = 0.24893879\n",
      "Iteration 569, loss = 0.24868955\n",
      "Iteration 570, loss = 0.24844081\n",
      "Iteration 571, loss = 0.24819256\n",
      "Iteration 572, loss = 0.24794493\n",
      "Iteration 573, loss = 0.24769773\n",
      "Iteration 574, loss = 0.24745106\n",
      "Iteration 575, loss = 0.24720497\n",
      "Iteration 576, loss = 0.24695938\n",
      "Iteration 577, loss = 0.24671429\n",
      "Iteration 578, loss = 0.24646969\n",
      "Iteration 579, loss = 0.24622572\n",
      "Iteration 580, loss = 0.24598213\n",
      "Iteration 581, loss = 0.24573911\n",
      "Iteration 582, loss = 0.24549659\n",
      "Iteration 583, loss = 0.24525465\n",
      "Iteration 584, loss = 0.24501316\n",
      "Iteration 585, loss = 0.24477218\n",
      "Iteration 586, loss = 0.24453169\n",
      "Iteration 587, loss = 0.24429179\n",
      "Iteration 588, loss = 0.24405232\n",
      "Iteration 589, loss = 0.24381337\n",
      "Iteration 590, loss = 0.24357499\n",
      "Iteration 591, loss = 0.24333700\n",
      "Iteration 592, loss = 0.24309955\n",
      "Iteration 593, loss = 0.24286266\n",
      "Iteration 594, loss = 0.24262623\n",
      "Iteration 595, loss = 0.24239031\n",
      "Iteration 596, loss = 0.24215489\n",
      "Iteration 597, loss = 0.24192001\n",
      "Iteration 598, loss = 0.24168563\n",
      "Iteration 599, loss = 0.24145176\n",
      "Iteration 600, loss = 0.24121840\n",
      "Iteration 601, loss = 0.24098552\n",
      "Iteration 602, loss = 0.24075310\n",
      "Iteration 603, loss = 0.24052115\n",
      "Iteration 604, loss = 0.24028979\n",
      "Iteration 605, loss = 0.24005888\n",
      "Iteration 606, loss = 0.23982839\n",
      "Iteration 607, loss = 0.23959842\n",
      "Iteration 608, loss = 0.23936905\n",
      "Iteration 609, loss = 0.23913997\n",
      "Iteration 610, loss = 0.23891144\n",
      "Iteration 611, loss = 0.23868349\n",
      "Iteration 612, loss = 0.23845592\n",
      "Iteration 613, loss = 0.23822883\n",
      "Iteration 614, loss = 0.23800221\n",
      "Iteration 615, loss = 0.23777617\n",
      "Iteration 616, loss = 0.23755053\n",
      "Iteration 617, loss = 0.23732534\n",
      "Iteration 618, loss = 0.23710062\n",
      "Iteration 619, loss = 0.23687653\n",
      "Iteration 620, loss = 0.23665267\n",
      "Iteration 621, loss = 0.23642945\n",
      "Iteration 622, loss = 0.23620665\n",
      "Iteration 623, loss = 0.23598429\n",
      "Iteration 624, loss = 0.23576243\n",
      "Iteration 625, loss = 0.23554100\n",
      "Iteration 626, loss = 0.23532011\n",
      "Iteration 627, loss = 0.23509961\n",
      "Iteration 628, loss = 0.23487956\n",
      "Iteration 629, loss = 0.23466007\n",
      "Iteration 630, loss = 0.23444093\n",
      "Iteration 631, loss = 0.23422228\n",
      "Iteration 632, loss = 0.23400412\n",
      "Iteration 633, loss = 0.23378639\n",
      "Iteration 634, loss = 0.23356913\n",
      "Iteration 635, loss = 0.23335226\n",
      "Iteration 636, loss = 0.23313598\n",
      "Iteration 637, loss = 0.23292000\n",
      "Iteration 638, loss = 0.23270455\n",
      "Iteration 639, loss = 0.23248957\n",
      "Iteration 640, loss = 0.23227499\n",
      "Iteration 641, loss = 0.23206088\n",
      "Iteration 642, loss = 0.23184718\n",
      "Iteration 643, loss = 0.23163406\n",
      "Iteration 644, loss = 0.23142121\n",
      "Iteration 645, loss = 0.23120888\n",
      "Iteration 646, loss = 0.23099702\n",
      "Iteration 647, loss = 0.23078557\n",
      "Iteration 648, loss = 0.23057455\n",
      "Iteration 649, loss = 0.23036395\n",
      "Iteration 650, loss = 0.23015392\n",
      "Iteration 651, loss = 0.22994414\n",
      "Iteration 652, loss = 0.22973490\n",
      "Iteration 653, loss = 0.22952613\n",
      "Iteration 654, loss = 0.22931770\n",
      "Iteration 655, loss = 0.22910972\n",
      "Iteration 656, loss = 0.22890223\n",
      "Iteration 657, loss = 0.22869518\n",
      "Iteration 658, loss = 0.22848851\n",
      "Iteration 659, loss = 0.22828226\n",
      "Iteration 660, loss = 0.22807643\n",
      "Iteration 661, loss = 0.22787111\n",
      "Iteration 662, loss = 0.22766614\n",
      "Iteration 663, loss = 0.22746163\n",
      "Iteration 664, loss = 0.22725760\n",
      "Iteration 665, loss = 0.22705391\n",
      "Iteration 666, loss = 0.22685065\n",
      "Iteration 667, loss = 0.22664792\n",
      "Iteration 668, loss = 0.22644551\n",
      "Iteration 669, loss = 0.22624355\n",
      "Iteration 670, loss = 0.22604194\n",
      "Iteration 671, loss = 0.22584088\n",
      "Iteration 672, loss = 0.22564013\n",
      "Iteration 673, loss = 0.22543982\n",
      "Iteration 674, loss = 0.22524003\n",
      "Iteration 675, loss = 0.22504054\n",
      "Iteration 676, loss = 0.22484149\n",
      "Iteration 677, loss = 0.22464289\n",
      "Iteration 678, loss = 0.22444470\n",
      "Iteration 679, loss = 0.22424688\n",
      "Iteration 680, loss = 0.22404949\n",
      "Iteration 681, loss = 0.22385252\n",
      "Iteration 682, loss = 0.22365597\n",
      "Iteration 683, loss = 0.22345977\n",
      "Iteration 684, loss = 0.22326400\n",
      "Iteration 685, loss = 0.22306868\n",
      "Iteration 686, loss = 0.22287372\n",
      "Iteration 687, loss = 0.22267914\n",
      "Iteration 688, loss = 0.22248505\n",
      "Iteration 689, loss = 0.22229126\n",
      "Iteration 690, loss = 0.22209791\n",
      "Iteration 691, loss = 0.22190498\n",
      "Iteration 692, loss = 0.22171247\n",
      "Iteration 693, loss = 0.22152029\n",
      "Iteration 694, loss = 0.22132855\n",
      "Iteration 695, loss = 0.22113725\n",
      "Iteration 696, loss = 0.22094626\n",
      "Iteration 697, loss = 0.22075569\n",
      "Iteration 698, loss = 0.22056562\n",
      "Iteration 699, loss = 0.22037582\n",
      "Iteration 700, loss = 0.22018643\n",
      "Iteration 701, loss = 0.21999742\n",
      "Iteration 702, loss = 0.21980906\n",
      "Iteration 703, loss = 0.21962079\n",
      "Iteration 704, loss = 0.21943316\n",
      "Iteration 705, loss = 0.21924582\n",
      "Iteration 706, loss = 0.21905887\n",
      "Iteration 707, loss = 0.21887233\n",
      "Iteration 708, loss = 0.21868618\n",
      "Iteration 709, loss = 0.21850049\n",
      "Iteration 710, loss = 0.21831504\n",
      "Iteration 711, loss = 0.21813008\n",
      "Iteration 712, loss = 0.21794544\n",
      "Iteration 713, loss = 0.21776120\n",
      "Iteration 714, loss = 0.21757735\n",
      "Iteration 715, loss = 0.21739395\n",
      "Iteration 716, loss = 0.21721082\n",
      "Iteration 717, loss = 0.21702808\n",
      "Iteration 718, loss = 0.21684583\n",
      "Iteration 719, loss = 0.21666385\n",
      "Iteration 720, loss = 0.21648226\n",
      "Iteration 721, loss = 0.21630099\n",
      "Iteration 722, loss = 0.21612029\n",
      "Iteration 723, loss = 0.21593970\n",
      "Iteration 724, loss = 0.21575969\n",
      "Iteration 725, loss = 0.21558000\n",
      "Iteration 726, loss = 0.21540063\n",
      "Iteration 727, loss = 0.21522168\n",
      "Iteration 728, loss = 0.21504305\n",
      "Iteration 729, loss = 0.21486487\n",
      "Iteration 730, loss = 0.21468698\n",
      "Iteration 731, loss = 0.21450955\n",
      "Iteration 732, loss = 0.21433240\n",
      "Iteration 733, loss = 0.21415562\n",
      "Iteration 734, loss = 0.21397923\n",
      "Iteration 735, loss = 0.21380321\n",
      "Iteration 736, loss = 0.21362751\n",
      "Iteration 737, loss = 0.21345218\n",
      "Iteration 738, loss = 0.21327725\n",
      "Iteration 739, loss = 0.21310265\n",
      "Iteration 740, loss = 0.21292834\n",
      "Iteration 741, loss = 0.21275455\n",
      "Iteration 742, loss = 0.21258092\n",
      "Iteration 743, loss = 0.21240776\n",
      "Iteration 744, loss = 0.21223495\n",
      "Iteration 745, loss = 0.21206245\n",
      "Iteration 746, loss = 0.21189032\n",
      "Iteration 747, loss = 0.21171855\n",
      "Iteration 748, loss = 0.21154711\n",
      "Iteration 749, loss = 0.21137597\n",
      "Iteration 750, loss = 0.21120536\n",
      "Iteration 751, loss = 0.21103490\n",
      "Iteration 752, loss = 0.21086482\n",
      "Iteration 753, loss = 0.21069520\n",
      "Iteration 754, loss = 0.21052579\n",
      "Iteration 755, loss = 0.21035680\n",
      "Iteration 756, loss = 0.21018815\n",
      "Iteration 757, loss = 0.21001981\n",
      "Iteration 758, loss = 0.20985179\n",
      "Iteration 759, loss = 0.20968419\n",
      "Iteration 760, loss = 0.20951689\n",
      "Iteration 761, loss = 0.20934989\n",
      "Iteration 762, loss = 0.20918325\n",
      "Iteration 763, loss = 0.20901700\n",
      "Iteration 764, loss = 0.20885107\n",
      "Iteration 765, loss = 0.20868540\n",
      "Iteration 766, loss = 0.20852020\n",
      "Iteration 767, loss = 0.20835514\n",
      "Iteration 768, loss = 0.20819056\n",
      "Iteration 769, loss = 0.20802635\n",
      "Iteration 770, loss = 0.20786233\n",
      "Iteration 771, loss = 0.20769868\n",
      "Iteration 772, loss = 0.20753545\n",
      "Iteration 773, loss = 0.20737244\n",
      "Iteration 774, loss = 0.20720980\n",
      "Iteration 775, loss = 0.20704756\n",
      "Iteration 776, loss = 0.20688555\n",
      "Iteration 777, loss = 0.20672388\n",
      "Iteration 778, loss = 0.20656257\n",
      "Iteration 779, loss = 0.20640155\n",
      "Iteration 780, loss = 0.20624086\n",
      "Iteration 781, loss = 0.20608058\n",
      "Iteration 782, loss = 0.20592046\n",
      "Iteration 783, loss = 0.20576076\n",
      "Iteration 784, loss = 0.20560143\n",
      "Iteration 785, loss = 0.20544234\n",
      "Iteration 786, loss = 0.20528357\n",
      "Iteration 787, loss = 0.20512518\n",
      "Iteration 788, loss = 0.20496708\n",
      "Iteration 789, loss = 0.20480931\n",
      "Iteration 790, loss = 0.20465187\n",
      "Iteration 791, loss = 0.20449471\n",
      "Iteration 792, loss = 0.20433783\n",
      "Iteration 793, loss = 0.20418139\n",
      "Iteration 794, loss = 0.20402513\n",
      "Iteration 795, loss = 0.20386921\n",
      "Iteration 796, loss = 0.20371372\n",
      "Iteration 797, loss = 0.20355845\n",
      "Iteration 798, loss = 0.20340346\n",
      "Iteration 799, loss = 0.20324879\n",
      "Iteration 800, loss = 0.20309449\n",
      "Iteration 801, loss = 0.20294047\n",
      "Iteration 802, loss = 0.20278677\n",
      "Iteration 803, loss = 0.20263337\n",
      "Iteration 804, loss = 0.20248023\n",
      "Iteration 805, loss = 0.20232753\n",
      "Iteration 806, loss = 0.20217497\n",
      "Iteration 807, loss = 0.20202278\n",
      "Iteration 808, loss = 0.20187098\n",
      "Iteration 809, loss = 0.20171941\n",
      "Iteration 810, loss = 0.20156808\n",
      "Iteration 811, loss = 0.20141714\n",
      "Iteration 812, loss = 0.20126646\n",
      "Iteration 813, loss = 0.20111609\n",
      "Iteration 814, loss = 0.20096607\n",
      "Iteration 815, loss = 0.20081625\n",
      "Iteration 816, loss = 0.20066680\n",
      "Iteration 817, loss = 0.20051764\n",
      "Iteration 818, loss = 0.20036879\n",
      "Iteration 819, loss = 0.20022015\n",
      "Iteration 820, loss = 0.20007195\n",
      "Iteration 821, loss = 0.19992390\n",
      "Iteration 822, loss = 0.19977622\n",
      "Iteration 823, loss = 0.19962889\n",
      "Iteration 824, loss = 0.19948174\n",
      "Iteration 825, loss = 0.19933493\n",
      "Iteration 826, loss = 0.19918849\n",
      "Iteration 827, loss = 0.19904218\n",
      "Iteration 828, loss = 0.19889634\n",
      "Iteration 829, loss = 0.19875068\n",
      "Iteration 830, loss = 0.19860531\n",
      "Iteration 831, loss = 0.19846019\n",
      "Iteration 832, loss = 0.19831548\n",
      "Iteration 833, loss = 0.19817092\n",
      "Iteration 834, loss = 0.19802676\n",
      "Iteration 835, loss = 0.19788282\n",
      "Iteration 836, loss = 0.19773915\n",
      "Iteration 837, loss = 0.19759586\n",
      "Iteration 838, loss = 0.19745275\n",
      "Iteration 839, loss = 0.19730994\n",
      "Iteration 840, loss = 0.19716751\n",
      "Iteration 841, loss = 0.19702528\n",
      "Iteration 842, loss = 0.19688331\n",
      "Iteration 843, loss = 0.19674165\n",
      "Iteration 844, loss = 0.19660025\n",
      "Iteration 845, loss = 0.19645913\n",
      "Iteration 846, loss = 0.19631839\n",
      "Iteration 847, loss = 0.19617776\n",
      "Iteration 848, loss = 0.19603756\n",
      "Iteration 849, loss = 0.19589753\n",
      "Iteration 850, loss = 0.19575782\n",
      "Iteration 851, loss = 0.19561839\n",
      "Iteration 852, loss = 0.19547923\n",
      "Iteration 853, loss = 0.19534033\n",
      "Iteration 854, loss = 0.19520171\n",
      "Iteration 855, loss = 0.19506338\n",
      "Iteration 856, loss = 0.19492530\n",
      "Iteration 857, loss = 0.19478750\n",
      "Iteration 858, loss = 0.19464999\n",
      "Iteration 859, loss = 0.19451268\n",
      "Iteration 860, loss = 0.19437576\n",
      "Iteration 861, loss = 0.19423900\n",
      "Iteration 862, loss = 0.19410255\n",
      "Iteration 863, loss = 0.19396636\n",
      "Iteration 864, loss = 0.19383046\n",
      "Iteration 865, loss = 0.19369476\n",
      "Iteration 866, loss = 0.19355943\n",
      "Iteration 867, loss = 0.19342430\n",
      "Iteration 868, loss = 0.19328939\n",
      "Iteration 869, loss = 0.19315485\n",
      "Iteration 870, loss = 0.19302049\n",
      "Iteration 871, loss = 0.19288640\n",
      "Iteration 872, loss = 0.19275261\n",
      "Iteration 873, loss = 0.19261903\n",
      "Iteration 874, loss = 0.19248580\n",
      "Iteration 875, loss = 0.19235272\n",
      "Iteration 876, loss = 0.19221996\n",
      "Iteration 877, loss = 0.19208749\n",
      "Iteration 878, loss = 0.19195521\n",
      "Iteration 879, loss = 0.19182320\n",
      "Iteration 880, loss = 0.19169148\n",
      "Iteration 881, loss = 0.19156000\n",
      "Iteration 882, loss = 0.19142876\n",
      "Iteration 883, loss = 0.19129781\n",
      "Iteration 884, loss = 0.19116706\n",
      "Iteration 885, loss = 0.19103662\n",
      "Iteration 886, loss = 0.19090640\n",
      "Iteration 887, loss = 0.19077643\n",
      "Iteration 888, loss = 0.19064706\n",
      "Iteration 889, loss = 0.19051791\n",
      "Iteration 890, loss = 0.19038900\n",
      "Iteration 891, loss = 0.19026043\n",
      "Iteration 892, loss = 0.19013205\n",
      "Iteration 893, loss = 0.19000395\n",
      "Iteration 894, loss = 0.18987613\n",
      "Iteration 895, loss = 0.18974853\n",
      "Iteration 896, loss = 0.18962129\n",
      "Iteration 897, loss = 0.18949414\n",
      "Iteration 898, loss = 0.18936741\n",
      "Iteration 899, loss = 0.18924084\n",
      "Iteration 900, loss = 0.18911448\n",
      "Iteration 901, loss = 0.18898840\n",
      "Iteration 902, loss = 0.18886260\n",
      "Iteration 903, loss = 0.18873699\n",
      "Iteration 904, loss = 0.18861174\n",
      "Iteration 905, loss = 0.18848660\n",
      "Iteration 906, loss = 0.18836176\n",
      "Iteration 907, loss = 0.18823718\n",
      "Iteration 908, loss = 0.18811278\n",
      "Iteration 909, loss = 0.18798872\n",
      "Iteration 910, loss = 0.18786479\n",
      "Iteration 911, loss = 0.18774122\n",
      "Iteration 912, loss = 0.18761781\n",
      "Iteration 913, loss = 0.18749462\n",
      "Iteration 914, loss = 0.18737167\n",
      "Iteration 915, loss = 0.18724899\n",
      "Iteration 916, loss = 0.18712651\n",
      "Iteration 917, loss = 0.18700437\n",
      "Iteration 918, loss = 0.18688231\n",
      "Iteration 919, loss = 0.18676062\n",
      "Iteration 920, loss = 0.18663908\n",
      "Iteration 921, loss = 0.18651775\n",
      "Iteration 922, loss = 0.18639672\n",
      "Iteration 923, loss = 0.18627588\n",
      "Iteration 924, loss = 0.18615532\n",
      "Iteration 925, loss = 0.18603496\n",
      "Iteration 926, loss = 0.18591480\n",
      "Iteration 927, loss = 0.18579496\n",
      "Iteration 928, loss = 0.18567523\n",
      "Iteration 929, loss = 0.18555582\n",
      "Iteration 930, loss = 0.18543660\n",
      "Iteration 931, loss = 0.18531758\n",
      "Iteration 932, loss = 0.18519888\n",
      "Iteration 933, loss = 0.18508031\n",
      "Iteration 934, loss = 0.18496200\n",
      "Iteration 935, loss = 0.18484396\n",
      "Iteration 936, loss = 0.18472607\n",
      "Iteration 937, loss = 0.18460844\n",
      "Iteration 938, loss = 0.18449106\n",
      "Iteration 939, loss = 0.18437383\n",
      "Iteration 940, loss = 0.18425693\n",
      "Iteration 941, loss = 0.18414014\n",
      "Iteration 942, loss = 0.18402370\n",
      "Iteration 943, loss = 0.18390739\n",
      "Iteration 944, loss = 0.18379127\n",
      "Iteration 945, loss = 0.18367541\n",
      "Iteration 946, loss = 0.18355976\n",
      "Iteration 947, loss = 0.18344433\n",
      "Iteration 948, loss = 0.18332916\n",
      "Iteration 949, loss = 0.18321414\n",
      "Iteration 950, loss = 0.18309939\n",
      "Iteration 951, loss = 0.18298481\n",
      "Iteration 952, loss = 0.18287049\n",
      "Iteration 953, loss = 0.18275637\n",
      "Iteration 954, loss = 0.18264245\n",
      "Iteration 955, loss = 0.18252878\n",
      "Iteration 956, loss = 0.18241528\n",
      "Iteration 957, loss = 0.18230203\n",
      "Iteration 958, loss = 0.18218902\n",
      "Iteration 959, loss = 0.18207617\n",
      "Iteration 960, loss = 0.18196355\n",
      "Iteration 961, loss = 0.18185114\n",
      "Iteration 962, loss = 0.18173892\n",
      "Iteration 963, loss = 0.18162699\n",
      "Iteration 964, loss = 0.18151519\n",
      "Iteration 965, loss = 0.18140362\n",
      "Iteration 966, loss = 0.18129227\n",
      "Iteration 967, loss = 0.18118108\n",
      "Iteration 968, loss = 0.18107023\n",
      "Iteration 969, loss = 0.18095941\n",
      "Iteration 970, loss = 0.18084895\n",
      "Iteration 971, loss = 0.18073862\n",
      "Iteration 972, loss = 0.18062846\n",
      "Iteration 973, loss = 0.18051859\n",
      "Iteration 974, loss = 0.18040882\n",
      "Iteration 975, loss = 0.18029943\n",
      "Iteration 976, loss = 0.18019034\n",
      "Iteration 977, loss = 0.18008152\n",
      "Iteration 978, loss = 0.17997295\n",
      "Iteration 979, loss = 0.17986454\n",
      "Iteration 980, loss = 0.17975635\n",
      "Iteration 981, loss = 0.17964837\n",
      "Iteration 982, loss = 0.17954059\n",
      "Iteration 983, loss = 0.17943307\n",
      "Iteration 984, loss = 0.17932569\n",
      "Iteration 985, loss = 0.17921857\n",
      "Iteration 986, loss = 0.17911159\n",
      "Iteration 987, loss = 0.17900487\n",
      "Iteration 988, loss = 0.17889832\n",
      "Iteration 989, loss = 0.17879196\n",
      "Iteration 990, loss = 0.17868586\n",
      "Iteration 991, loss = 0.17857989\n",
      "Iteration 992, loss = 0.17847416\n",
      "Iteration 993, loss = 0.17836859\n",
      "Iteration 994, loss = 0.17826326\n",
      "Iteration 995, loss = 0.17815811\n",
      "Iteration 996, loss = 0.17805312\n",
      "Iteration 997, loss = 0.17794845\n",
      "Iteration 998, loss = 0.17784384\n",
      "Iteration 999, loss = 0.17773953\n",
      "Iteration 1000, loss = 0.17763535\n",
      "Iteration 1, loss = 1.66212118\n",
      "Iteration 2, loss = 8.48599119\n",
      "Iteration 3, loss = 2.60225949\n",
      "Iteration 4, loss = 1.16398347\n",
      "Iteration 5, loss = 0.75822460\n",
      "Iteration 6, loss = 0.82416403\n",
      "Iteration 7, loss = 0.60039815\n",
      "Iteration 8, loss = 0.69049817\n",
      "Iteration 9, loss = 0.56169170\n",
      "Iteration 10, loss = 0.58872034\n",
      "Iteration 11, loss = 0.51183010\n",
      "Iteration 12, loss = 0.51362385\n",
      "Iteration 13, loss = 0.48888984\n",
      "Iteration 14, loss = 0.42390159\n",
      "Iteration 15, loss = 0.42824143\n",
      "Iteration 16, loss = 0.38775651\n",
      "Iteration 17, loss = 0.36001153\n",
      "Iteration 18, loss = 0.35942426\n",
      "Iteration 19, loss = 0.30903459\n",
      "Iteration 20, loss = 0.30931126\n",
      "Iteration 21, loss = 0.27665442\n",
      "Iteration 22, loss = 0.26147529\n",
      "Iteration 23, loss = 0.24863740\n",
      "Iteration 24, loss = 0.22517660\n",
      "Iteration 25, loss = 0.22364138\n",
      "Iteration 26, loss = 0.19774956\n",
      "Iteration 27, loss = 0.20196634\n",
      "Iteration 28, loss = 0.17895617\n",
      "Iteration 29, loss = 0.18339945\n",
      "Iteration 30, loss = 0.16619985\n",
      "Iteration 31, loss = 0.17042974\n",
      "Iteration 32, loss = 0.15656713\n",
      "Iteration 33, loss = 0.15944901\n",
      "Iteration 34, loss = 0.14988716\n",
      "Iteration 35, loss = 0.15154873\n",
      "Iteration 36, loss = 0.14383710\n",
      "Iteration 37, loss = 0.14507373\n",
      "Iteration 38, loss = 0.13961261\n",
      "Iteration 39, loss = 0.14027385\n",
      "Iteration 40, loss = 0.13546905\n",
      "Iteration 41, loss = 0.13626577\n",
      "Iteration 42, loss = 0.13238324\n",
      "Iteration 43, loss = 0.13317833\n",
      "Iteration 44, loss = 0.12932586\n",
      "Iteration 45, loss = 0.13026492\n",
      "Iteration 46, loss = 0.12694571\n",
      "Iteration 47, loss = 0.12797637\n",
      "Iteration 48, loss = 0.12485779\n",
      "Iteration 49, loss = 0.12549407\n",
      "Iteration 50, loss = 0.12312616\n",
      "Iteration 51, loss = 0.12332498\n",
      "Iteration 52, loss = 0.12184290\n",
      "Iteration 53, loss = 0.12115026\n",
      "Iteration 54, loss = 0.12061923\n",
      "Iteration 55, loss = 0.11918583\n",
      "Iteration 56, loss = 0.11925148\n",
      "Iteration 57, loss = 0.11769836\n",
      "Iteration 58, loss = 0.11772655\n",
      "Iteration 59, loss = 0.11661507\n",
      "Iteration 60, loss = 0.11599111\n",
      "Iteration 61, loss = 0.11561178\n",
      "Iteration 62, loss = 0.11448139\n",
      "Iteration 63, loss = 0.11432994\n",
      "Iteration 64, loss = 0.11343006\n",
      "Iteration 65, loss = 0.11288849\n",
      "Iteration 66, loss = 0.11252024\n",
      "Iteration 67, loss = 0.11165918\n",
      "Iteration 68, loss = 0.11138761\n",
      "Iteration 69, loss = 0.11078027\n",
      "Iteration 70, loss = 0.11012998\n",
      "Iteration 71, loss = 0.10985469\n",
      "Iteration 72, loss = 0.10918891\n",
      "Iteration 73, loss = 0.10868555\n",
      "Iteration 74, loss = 0.10835557\n",
      "Iteration 75, loss = 0.10773006\n",
      "Iteration 76, loss = 0.10728678\n",
      "Iteration 77, loss = 0.10693094\n",
      "Iteration 78, loss = 0.10636643\n",
      "Iteration 79, loss = 0.10593856\n",
      "Iteration 80, loss = 0.10558797\n",
      "Iteration 81, loss = 0.10508077\n",
      "Iteration 82, loss = 0.10464458\n",
      "Iteration 83, loss = 0.10430363\n",
      "Iteration 84, loss = 0.10385597\n",
      "Iteration 85, loss = 0.10340956\n",
      "Iteration 86, loss = 0.10306775\n",
      "Iteration 87, loss = 0.10268135\n",
      "Iteration 88, loss = 0.10224157\n",
      "Iteration 89, loss = 0.10187633\n",
      "Iteration 90, loss = 0.10153491\n",
      "Iteration 91, loss = 0.10113418\n",
      "Iteration 92, loss = 0.10074361\n",
      "Iteration 93, loss = 0.10040625\n",
      "Iteration 94, loss = 0.10006269\n",
      "Iteration 95, loss = 0.09968719\n",
      "Iteration 96, loss = 0.09932483\n",
      "Iteration 97, loss = 0.09899748\n",
      "Iteration 98, loss = 0.09866941\n",
      "Iteration 99, loss = 0.09831963\n",
      "Iteration 100, loss = 0.09797545\n",
      "Iteration 101, loss = 0.09765341\n",
      "Iteration 102, loss = 0.09734374\n",
      "Iteration 103, loss = 0.09702570\n",
      "Iteration 104, loss = 0.09669968\n",
      "Iteration 105, loss = 0.09638080\n",
      "Iteration 106, loss = 0.09607553\n",
      "Iteration 107, loss = 0.09577662\n",
      "Iteration 108, loss = 0.09547594\n",
      "Iteration 109, loss = 0.09517321\n",
      "Iteration 110, loss = 0.09487191\n",
      "Iteration 111, loss = 0.09457628\n",
      "Iteration 112, loss = 0.09430761\n",
      "Iteration 113, loss = 0.09400660\n",
      "Iteration 114, loss = 0.09372870\n",
      "Iteration 115, loss = 0.09345411\n",
      "Iteration 116, loss = 0.09318227\n",
      "Iteration 117, loss = 0.09291275\n",
      "Iteration 118, loss = 0.09264500\n",
      "Iteration 119, loss = 0.09237879\n",
      "Iteration 120, loss = 0.09211457\n",
      "Iteration 121, loss = 0.09185255\n",
      "Iteration 122, loss = 0.09159312\n",
      "Iteration 123, loss = 0.09133663\n",
      "Iteration 124, loss = 0.09108326\n",
      "Iteration 125, loss = 0.09083312\n",
      "Iteration 126, loss = 0.09058606\n",
      "Iteration 127, loss = 0.09034214\n",
      "Iteration 128, loss = 0.09010703\n",
      "Iteration 129, loss = 0.08986878\n",
      "Iteration 130, loss = 0.08963399\n",
      "Iteration 131, loss = 0.08940532\n",
      "Iteration 132, loss = 0.08919007\n",
      "Iteration 133, loss = 0.08901475\n",
      "Iteration 134, loss = 0.08897207\n",
      "Iteration 135, loss = 0.08933124\n",
      "Iteration 136, loss = 0.09138010\n",
      "Iteration 137, loss = 0.09697011\n",
      "Iteration 138, loss = 0.11936056\n",
      "Iteration 139, loss = 0.12880286\n",
      "Iteration 140, loss = 0.17711953\n",
      "Iteration 141, loss = 0.09751805\n",
      "Iteration 142, loss = 0.10190675\n",
      "Iteration 143, loss = 0.16120187\n",
      "Iteration 144, loss = 0.09288593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65392470\n",
      "Iteration 2, loss = 8.63164286\n",
      "Iteration 3, loss = 2.48761382\n",
      "Iteration 4, loss = 1.58268781\n",
      "Iteration 5, loss = 0.81047525\n",
      "Iteration 6, loss = 0.80881919\n",
      "Iteration 7, loss = 0.69271222\n",
      "Iteration 8, loss = 0.63907295\n",
      "Iteration 9, loss = 0.57748550\n",
      "Iteration 10, loss = 0.55928681\n",
      "Iteration 11, loss = 0.53236987\n",
      "Iteration 12, loss = 0.51357496\n",
      "Iteration 13, loss = 0.47087308\n",
      "Iteration 14, loss = 0.44663580\n",
      "Iteration 15, loss = 0.40914971\n",
      "Iteration 16, loss = 0.38864373\n",
      "Iteration 17, loss = 0.35007806\n",
      "Iteration 18, loss = 0.32776539\n",
      "Iteration 19, loss = 0.29537032\n",
      "Iteration 20, loss = 0.27698835\n",
      "Iteration 21, loss = 0.24789305\n",
      "Iteration 22, loss = 0.22521872\n",
      "Iteration 23, loss = 0.21479536\n",
      "Iteration 24, loss = 0.19211188\n",
      "Iteration 25, loss = 0.17604237\n",
      "Iteration 26, loss = 0.17152221\n",
      "Iteration 27, loss = 0.15849514\n",
      "Iteration 28, loss = 0.14815137\n",
      "Iteration 29, loss = 0.14963677\n",
      "Iteration 30, loss = 0.14358896\n",
      "Iteration 31, loss = 0.13483052\n",
      "Iteration 32, loss = 0.13797167\n",
      "Iteration 33, loss = 0.13384985\n",
      "Iteration 34, loss = 0.12602436\n",
      "Iteration 35, loss = 0.12895776\n",
      "Iteration 36, loss = 0.12844137\n",
      "Iteration 37, loss = 0.12147697\n",
      "Iteration 38, loss = 0.12541062\n",
      "Iteration 39, loss = 0.12568379\n",
      "Iteration 40, loss = 0.11867301\n",
      "Iteration 41, loss = 0.12394076\n",
      "Iteration 42, loss = 0.12364618\n",
      "Iteration 43, loss = 0.11670488\n",
      "Iteration 44, loss = 0.12407439\n",
      "Iteration 45, loss = 0.12268005\n",
      "Iteration 46, loss = 0.11560056\n",
      "Iteration 47, loss = 0.12539277\n",
      "Iteration 48, loss = 0.12151306\n",
      "Iteration 49, loss = 0.11550048\n",
      "Iteration 50, loss = 0.12613053\n",
      "Iteration 51, loss = 0.11871288\n",
      "Iteration 52, loss = 0.11577352\n",
      "Iteration 53, loss = 0.12349809\n",
      "Iteration 54, loss = 0.11401848\n",
      "Iteration 55, loss = 0.11510398\n",
      "Iteration 56, loss = 0.11733401\n",
      "Iteration 57, loss = 0.11034903\n",
      "Iteration 58, loss = 0.11331906\n",
      "Iteration 59, loss = 0.11147447\n",
      "Iteration 60, loss = 0.10908850\n",
      "Iteration 61, loss = 0.11124241\n",
      "Iteration 62, loss = 0.10827977\n",
      "Iteration 63, loss = 0.10889597\n",
      "Iteration 64, loss = 0.10930192\n",
      "Iteration 65, loss = 0.10675350\n",
      "Iteration 66, loss = 0.10839227\n",
      "Iteration 67, loss = 0.10759963\n",
      "Iteration 68, loss = 0.10562424\n",
      "Iteration 69, loss = 0.10737872\n",
      "Iteration 70, loss = 0.10618501\n",
      "Iteration 71, loss = 0.10443433\n",
      "Iteration 72, loss = 0.10612759\n",
      "Iteration 73, loss = 0.10507966\n",
      "Iteration 74, loss = 0.10317289\n",
      "Iteration 75, loss = 0.10481714\n",
      "Iteration 76, loss = 0.10427076\n",
      "Iteration 77, loss = 0.10194626\n",
      "Iteration 78, loss = 0.10344426\n",
      "Iteration 79, loss = 0.10366125\n",
      "Iteration 80, loss = 0.10088209\n",
      "Iteration 81, loss = 0.10188880\n",
      "Iteration 82, loss = 0.10292528\n",
      "Iteration 83, loss = 0.10008014\n",
      "Iteration 84, loss = 0.10010142\n",
      "Iteration 85, loss = 0.10153639\n",
      "Iteration 86, loss = 0.09952449\n",
      "Iteration 87, loss = 0.09840564\n",
      "Iteration 88, loss = 0.09927335\n",
      "Iteration 89, loss = 0.09888983\n",
      "Iteration 90, loss = 0.09761073\n",
      "Iteration 91, loss = 0.09711141\n",
      "Iteration 92, loss = 0.09751671\n",
      "Iteration 93, loss = 0.09744961\n",
      "Iteration 94, loss = 0.09633797\n",
      "Iteration 95, loss = 0.09574905\n",
      "Iteration 96, loss = 0.09590834\n",
      "Iteration 97, loss = 0.09587643\n",
      "Iteration 98, loss = 0.09539693\n",
      "Iteration 99, loss = 0.09462472\n",
      "Iteration 100, loss = 0.09421502\n",
      "Iteration 101, loss = 0.09418384\n",
      "Iteration 102, loss = 0.09416338\n",
      "Iteration 103, loss = 0.09401385\n",
      "Iteration 104, loss = 0.09352682\n",
      "Iteration 105, loss = 0.09302918\n",
      "Iteration 106, loss = 0.09255154\n",
      "Iteration 107, loss = 0.09219068\n",
      "Iteration 108, loss = 0.09193218\n",
      "Iteration 109, loss = 0.09175170\n",
      "Iteration 110, loss = 0.09165892\n",
      "Iteration 111, loss = 0.09166511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 112, loss = 0.09202912\n",
      "Iteration 113, loss = 0.09275548\n",
      "Iteration 114, loss = 0.09541400\n",
      "Iteration 115, loss = 0.09832263\n",
      "Iteration 116, loss = 0.10833073\n",
      "Iteration 117, loss = 0.10390841\n",
      "Iteration 118, loss = 0.10405902\n",
      "Iteration 119, loss = 0.09173487\n",
      "Iteration 120, loss = 0.08959358\n",
      "Iteration 121, loss = 0.09561233\n",
      "Iteration 122, loss = 0.09594287\n",
      "Iteration 123, loss = 0.09360941\n",
      "Iteration 124, loss = 0.08840321\n",
      "Iteration 125, loss = 0.09009093\n",
      "Iteration 126, loss = 0.09462835\n",
      "Iteration 127, loss = 0.09083149\n",
      "Iteration 128, loss = 0.08758720\n",
      "Iteration 129, loss = 0.08824841\n",
      "Iteration 130, loss = 0.09003457\n",
      "Iteration 131, loss = 0.08996377\n",
      "Iteration 132, loss = 0.08707092\n",
      "Iteration 133, loss = 0.08688411\n",
      "Iteration 134, loss = 0.08864547\n",
      "Iteration 135, loss = 0.08804762\n",
      "Iteration 136, loss = 0.08649488\n",
      "Iteration 137, loss = 0.08569052\n",
      "Iteration 138, loss = 0.08640760\n",
      "Iteration 139, loss = 0.08714701\n",
      "Iteration 140, loss = 0.08611643\n",
      "Iteration 141, loss = 0.08504258\n",
      "Iteration 142, loss = 0.08481076\n",
      "Iteration 143, loss = 0.08525514\n",
      "Iteration 144, loss = 0.08558613\n",
      "Iteration 145, loss = 0.08497941\n",
      "Iteration 146, loss = 0.08424405\n",
      "Iteration 147, loss = 0.08376687\n",
      "Iteration 148, loss = 0.08375647\n",
      "Iteration 149, loss = 0.08399213\n",
      "Iteration 150, loss = 0.08404670\n",
      "Iteration 151, loss = 0.08398882\n",
      "Iteration 152, loss = 0.08356393\n",
      "Iteration 153, loss = 0.08316559\n",
      "Iteration 154, loss = 0.08274183\n",
      "Iteration 155, loss = 0.08242440\n",
      "Iteration 156, loss = 0.08218492\n",
      "Iteration 157, loss = 0.08200991\n",
      "Iteration 158, loss = 0.08188364\n",
      "Iteration 159, loss = 0.08180656\n",
      "Iteration 160, loss = 0.08182541\n",
      "Iteration 161, loss = 0.08200545\n",
      "Iteration 162, loss = 0.08273735\n",
      "Iteration 163, loss = 0.08422981\n",
      "Iteration 164, loss = 0.08937164\n",
      "Iteration 165, loss = 0.09539576\n",
      "Iteration 166, loss = 0.11836887\n",
      "Iteration 167, loss = 0.10585499\n",
      "Iteration 168, loss = 0.10688523\n",
      "Iteration 169, loss = 0.08336976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66585275\n",
      "Iteration 2, loss = 8.47521900\n",
      "Iteration 3, loss = 2.56026408\n",
      "Iteration 4, loss = 1.22072997\n",
      "Iteration 5, loss = 0.77801084\n",
      "Iteration 6, loss = 0.84203086\n",
      "Iteration 7, loss = 0.58301111\n",
      "Iteration 8, loss = 0.77113592\n",
      "Iteration 9, loss = 0.50291924\n",
      "Iteration 10, loss = 0.60023651\n",
      "Iteration 11, loss = 0.48567269\n",
      "Iteration 12, loss = 0.49430307\n",
      "Iteration 13, loss = 0.48213436\n",
      "Iteration 14, loss = 0.39997072\n",
      "Iteration 15, loss = 0.43416585\n",
      "Iteration 16, loss = 0.36584402\n",
      "Iteration 17, loss = 0.34221338\n",
      "Iteration 18, loss = 0.34535994\n",
      "Iteration 19, loss = 0.27719165\n",
      "Iteration 20, loss = 0.29768882\n",
      "Iteration 21, loss = 0.24251833\n",
      "Iteration 22, loss = 0.23984834\n",
      "Iteration 23, loss = 0.21677464\n",
      "Iteration 24, loss = 0.18831707\n",
      "Iteration 25, loss = 0.19475038\n",
      "Iteration 26, loss = 0.16016617\n",
      "Iteration 27, loss = 0.17088742\n",
      "Iteration 28, loss = 0.14830492\n",
      "Iteration 29, loss = 0.14507885\n",
      "Iteration 30, loss = 0.14412265\n",
      "Iteration 31, loss = 0.12747701\n",
      "Iteration 32, loss = 0.13564185\n",
      "Iteration 33, loss = 0.12238709\n",
      "Iteration 34, loss = 0.12289834\n",
      "Iteration 35, loss = 0.12274681\n",
      "Iteration 36, loss = 0.11384253\n",
      "Iteration 37, loss = 0.11835804\n",
      "Iteration 38, loss = 0.11076721\n",
      "Iteration 39, loss = 0.11230201\n",
      "Iteration 40, loss = 0.10980500\n",
      "Iteration 41, loss = 0.10645638\n",
      "Iteration 42, loss = 0.10849118\n",
      "Iteration 43, loss = 0.10350725\n",
      "Iteration 44, loss = 0.10531966\n",
      "Iteration 45, loss = 0.10298079\n",
      "Iteration 46, loss = 0.10150641\n",
      "Iteration 47, loss = 0.10233544\n",
      "Iteration 48, loss = 0.09935972\n",
      "Iteration 49, loss = 0.10031704\n",
      "Iteration 50, loss = 0.09867003\n",
      "Iteration 51, loss = 0.09773087\n",
      "Iteration 52, loss = 0.09789806\n",
      "Iteration 53, loss = 0.09593472\n",
      "Iteration 54, loss = 0.09637106\n",
      "Iteration 55, loss = 0.09488232\n",
      "Iteration 56, loss = 0.09450480\n",
      "Iteration 57, loss = 0.09395358\n",
      "Iteration 58, loss = 0.09287130\n",
      "Iteration 59, loss = 0.09283909\n",
      "Iteration 60, loss = 0.09152048\n",
      "Iteration 61, loss = 0.09155400\n",
      "Iteration 62, loss = 0.09036574\n",
      "Iteration 63, loss = 0.09025228\n",
      "Iteration 64, loss = 0.08930549\n",
      "Iteration 65, loss = 0.08900043\n",
      "Iteration 66, loss = 0.08817964\n",
      "Iteration 67, loss = 0.08783227\n",
      "Iteration 68, loss = 0.08709075\n",
      "Iteration 69, loss = 0.08673948\n",
      "Iteration 70, loss = 0.08597020\n",
      "Iteration 71, loss = 0.08568449\n",
      "Iteration 72, loss = 0.08491489\n",
      "Iteration 73, loss = 0.08466175\n",
      "Iteration 74, loss = 0.08398141\n",
      "Iteration 75, loss = 0.08353843\n",
      "Iteration 76, loss = 0.08315523\n",
      "Iteration 77, loss = 0.08249293\n",
      "Iteration 78, loss = 0.08220161\n",
      "Iteration 79, loss = 0.08175095\n",
      "Iteration 80, loss = 0.08115361\n",
      "Iteration 81, loss = 0.08085379\n",
      "Iteration 82, loss = 0.08052702\n",
      "Iteration 83, loss = 0.07998001\n",
      "Iteration 84, loss = 0.07953018\n",
      "Iteration 85, loss = 0.07923555\n",
      "Iteration 86, loss = 0.07894832\n",
      "Iteration 87, loss = 0.07858376\n",
      "Iteration 88, loss = 0.07816462\n",
      "Iteration 89, loss = 0.07775691\n",
      "Iteration 90, loss = 0.07737244\n",
      "Iteration 91, loss = 0.07701981\n",
      "Iteration 92, loss = 0.07670026\n",
      "Iteration 93, loss = 0.07641120\n",
      "Iteration 94, loss = 0.07622115\n",
      "Iteration 95, loss = 0.07632636\n",
      "Iteration 96, loss = 0.07739741\n",
      "Iteration 97, loss = 0.08146500\n",
      "Iteration 98, loss = 0.08129623\n",
      "Iteration 99, loss = 0.07653503\n",
      "Iteration 100, loss = 0.07542323\n",
      "Iteration 101, loss = 0.07838404\n",
      "Iteration 102, loss = 0.07503549\n",
      "Iteration 103, loss = 0.07497014\n",
      "Iteration 104, loss = 0.07615631\n",
      "Iteration 105, loss = 0.07329561\n",
      "Iteration 106, loss = 0.07470964\n",
      "Iteration 107, loss = 0.07352764\n",
      "Iteration 108, loss = 0.07297757\n",
      "Iteration 109, loss = 0.07361275\n",
      "Iteration 110, loss = 0.07206490\n",
      "Iteration 111, loss = 0.07298335\n",
      "Iteration 112, loss = 0.07196016\n",
      "Iteration 113, loss = 0.07176558\n",
      "Iteration 114, loss = 0.07189041\n",
      "Iteration 115, loss = 0.07085922\n",
      "Iteration 116, loss = 0.07129152\n",
      "Iteration 117, loss = 0.07052990\n",
      "Iteration 118, loss = 0.07043809\n",
      "Iteration 119, loss = 0.07037554\n",
      "Iteration 120, loss = 0.06972694\n",
      "Iteration 121, loss = 0.06994466\n",
      "Iteration 122, loss = 0.06941892\n",
      "Iteration 123, loss = 0.06920795\n",
      "Iteration 124, loss = 0.06919450\n",
      "Iteration 125, loss = 0.06865952\n",
      "Iteration 126, loss = 0.06867537\n",
      "Iteration 127, loss = 0.06844006\n",
      "Iteration 128, loss = 0.06804606\n",
      "Iteration 129, loss = 0.06806002\n",
      "Iteration 130, loss = 0.06778531\n",
      "Iteration 131, loss = 0.06744509\n",
      "Iteration 132, loss = 0.06740637\n",
      "Iteration 133, loss = 0.06719190\n",
      "Iteration 134, loss = 0.06685848\n",
      "Iteration 135, loss = 0.06673770\n",
      "Iteration 136, loss = 0.06659280\n",
      "Iteration 137, loss = 0.06630482\n",
      "Iteration 138, loss = 0.06611822\n",
      "Iteration 139, loss = 0.06600198\n",
      "Iteration 140, loss = 0.06577787\n",
      "Iteration 141, loss = 0.06554728\n",
      "Iteration 142, loss = 0.06541698\n",
      "Iteration 143, loss = 0.06525834\n",
      "Iteration 144, loss = 0.06503011\n",
      "Iteration 145, loss = 0.06485025\n",
      "Iteration 146, loss = 0.06471738\n",
      "Iteration 147, loss = 0.06454529\n",
      "Iteration 148, loss = 0.06434395\n",
      "Iteration 149, loss = 0.06417458\n",
      "Iteration 150, loss = 0.06403545\n",
      "Iteration 151, loss = 0.06387902\n",
      "Iteration 152, loss = 0.06369856\n",
      "Iteration 153, loss = 0.06352484\n",
      "Iteration 154, loss = 0.06337515\n",
      "Iteration 155, loss = 0.06323455\n",
      "Iteration 156, loss = 0.06308393\n",
      "Iteration 157, loss = 0.06292087\n",
      "Iteration 158, loss = 0.06275781\n",
      "Iteration 159, loss = 0.06260472\n",
      "Iteration 160, loss = 0.06246217\n",
      "Iteration 161, loss = 0.06232429\n",
      "Iteration 162, loss = 0.06218517\n",
      "Iteration 163, loss = 0.06204644\n",
      "Iteration 164, loss = 0.06190358\n",
      "Iteration 165, loss = 0.06176107\n",
      "Iteration 166, loss = 0.06162018\n",
      "Iteration 167, loss = 0.06148267\n",
      "Iteration 168, loss = 0.06134769\n",
      "Iteration 169, loss = 0.06121840\n",
      "Iteration 170, loss = 0.06109533\n",
      "Iteration 171, loss = 0.06098661\n",
      "Iteration 172, loss = 0.06090105\n",
      "Iteration 173, loss = 0.06087502\n",
      "Iteration 174, loss = 0.06095874\n",
      "Iteration 175, loss = 0.06137672\n",
      "Iteration 176, loss = 0.06234510\n",
      "Iteration 177, loss = 0.06516265\n",
      "Iteration 178, loss = 0.06886207\n",
      "Iteration 179, loss = 0.07721965\n",
      "Iteration 180, loss = 0.07471281\n",
      "Iteration 181, loss = 0.06651034\n",
      "Iteration 182, loss = 0.06035318\n",
      "Iteration 183, loss = 0.06949405\n",
      "Iteration 184, loss = 0.06794121\n",
      "Iteration 185, loss = 0.05976432\n",
      "Iteration 186, loss = 0.06918807\n",
      "Iteration 187, loss = 0.06526539\n",
      "Iteration 188, loss = 0.06106093\n",
      "Iteration 189, loss = 0.06793015\n",
      "Iteration 190, loss = 0.06032245\n",
      "Iteration 191, loss = 0.06317213\n",
      "Iteration 192, loss = 0.06222558\n",
      "Iteration 193, loss = 0.05941772\n",
      "Iteration 194, loss = 0.06250919\n",
      "Iteration 195, loss = 0.05860564\n",
      "Iteration 196, loss = 0.06198504\n",
      "Iteration 197, loss = 0.05924824\n",
      "Iteration 198, loss = 0.06006931\n",
      "Iteration 199, loss = 0.05973663\n",
      "Iteration 200, loss = 0.05856089\n",
      "Iteration 201, loss = 0.05981594\n",
      "Iteration 202, loss = 0.05798712\n",
      "Iteration 203, loss = 0.05952350\n",
      "Iteration 204, loss = 0.05796803\n",
      "Iteration 205, loss = 0.05861067\n",
      "Iteration 206, loss = 0.05810869\n",
      "Iteration 207, loss = 0.05774949\n",
      "Iteration 208, loss = 0.05816876\n",
      "Iteration 209, loss = 0.05725704\n",
      "Iteration 210, loss = 0.05796241\n",
      "Iteration 211, loss = 0.05711890\n",
      "Iteration 212, loss = 0.05738963\n",
      "Iteration 213, loss = 0.05714833\n",
      "Iteration 214, loss = 0.05682790\n",
      "Iteration 215, loss = 0.05708081\n",
      "Iteration 216, loss = 0.05652409\n",
      "Iteration 217, loss = 0.05679825\n",
      "Iteration 218, loss = 0.05646934\n",
      "Iteration 219, loss = 0.05635757\n",
      "Iteration 220, loss = 0.05642986\n",
      "Iteration 221, loss = 0.05605165\n",
      "Iteration 222, loss = 0.05618166\n",
      "Iteration 223, loss = 0.05596093\n",
      "Iteration 224, loss = 0.05584333\n",
      "Iteration 225, loss = 0.05588238\n",
      "Iteration 226, loss = 0.05562535\n",
      "Iteration 227, loss = 0.05564989\n",
      "Iteration 228, loss = 0.05555111\n",
      "Iteration 229, loss = 0.05538076\n",
      "Iteration 230, loss = 0.05541014\n",
      "Iteration 231, loss = 0.05525271\n",
      "Iteration 232, loss = 0.05515858\n",
      "Iteration 233, loss = 0.05514921\n",
      "Iteration 234, loss = 0.05499410\n",
      "Iteration 235, loss = 0.05493576\n",
      "Iteration 236, loss = 0.05489533\n",
      "Iteration 237, loss = 0.05476312\n",
      "Iteration 238, loss = 0.05471321\n",
      "Iteration 239, loss = 0.05465846\n",
      "Iteration 240, loss = 0.05454436\n",
      "Iteration 241, loss = 0.05449445\n",
      "Iteration 242, loss = 0.05443598\n",
      "Iteration 243, loss = 0.05433576\n",
      "Iteration 244, loss = 0.05428313\n",
      "Iteration 245, loss = 0.05422131\n",
      "Iteration 246, loss = 0.05413079\n",
      "Iteration 247, loss = 0.05407698\n",
      "Iteration 248, loss = 0.05402320\n",
      "Iteration 249, loss = 0.05394014\n",
      "Iteration 250, loss = 0.05387459\n",
      "Iteration 251, loss = 0.05382256\n",
      "Iteration 252, loss = 0.05375205\n",
      "Iteration 253, loss = 0.05368635\n",
      "Iteration 254, loss = 0.05363027\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65830114\n",
      "Iteration 2, loss = 8.62092997\n",
      "Iteration 3, loss = 2.58485209\n",
      "Iteration 4, loss = 1.23685213\n",
      "Iteration 5, loss = 0.83495112\n",
      "Iteration 6, loss = 0.88844412\n",
      "Iteration 7, loss = 0.63806050\n",
      "Iteration 8, loss = 0.64404551\n",
      "Iteration 9, loss = 0.58482290\n",
      "Iteration 10, loss = 0.52086290\n",
      "Iteration 11, loss = 0.52245671\n",
      "Iteration 12, loss = 0.48038665\n",
      "Iteration 13, loss = 0.46025534\n",
      "Iteration 14, loss = 0.39907421\n",
      "Iteration 15, loss = 0.38879286\n",
      "Iteration 16, loss = 0.34028840\n",
      "Iteration 17, loss = 0.33101977\n",
      "Iteration 18, loss = 0.28788143\n",
      "Iteration 19, loss = 0.27682016\n",
      "Iteration 20, loss = 0.23973128\n",
      "Iteration 21, loss = 0.23237050\n",
      "Iteration 22, loss = 0.20098895\n",
      "Iteration 23, loss = 0.19455671\n",
      "Iteration 24, loss = 0.17136979\n",
      "Iteration 25, loss = 0.16764760\n",
      "Iteration 26, loss = 0.14931881\n",
      "Iteration 27, loss = 0.14636731\n",
      "Iteration 28, loss = 0.13421720\n",
      "Iteration 29, loss = 0.13228312\n",
      "Iteration 30, loss = 0.12414957\n",
      "Iteration 31, loss = 0.12185580\n",
      "Iteration 32, loss = 0.11678426\n",
      "Iteration 33, loss = 0.11350000\n",
      "Iteration 34, loss = 0.11087387\n",
      "Iteration 35, loss = 0.10744882\n",
      "Iteration 36, loss = 0.10658257\n",
      "Iteration 37, loss = 0.10292745\n",
      "Iteration 38, loss = 0.10302945\n",
      "Iteration 39, loss = 0.09965758\n",
      "Iteration 40, loss = 0.09984436\n",
      "Iteration 41, loss = 0.09703526\n",
      "Iteration 42, loss = 0.09713987\n",
      "Iteration 43, loss = 0.09481654\n",
      "Iteration 44, loss = 0.09469033\n",
      "Iteration 45, loss = 0.09292471\n",
      "Iteration 46, loss = 0.09258119\n",
      "Iteration 47, loss = 0.09112943\n",
      "Iteration 48, loss = 0.09068659\n",
      "Iteration 49, loss = 0.08950100\n",
      "Iteration 50, loss = 0.08898823\n",
      "Iteration 51, loss = 0.08792046\n",
      "Iteration 52, loss = 0.08743496\n",
      "Iteration 53, loss = 0.08646584\n",
      "Iteration 54, loss = 0.08600860\n",
      "Iteration 55, loss = 0.08507029\n",
      "Iteration 56, loss = 0.08466965\n",
      "Iteration 57, loss = 0.08377674\n",
      "Iteration 58, loss = 0.08341912\n",
      "Iteration 59, loss = 0.08255181\n",
      "Iteration 60, loss = 0.08220660\n",
      "Iteration 61, loss = 0.08140853\n",
      "Iteration 62, loss = 0.08104909\n",
      "Iteration 63, loss = 0.08033606\n",
      "Iteration 64, loss = 0.07991372\n",
      "Iteration 65, loss = 0.07931669\n",
      "Iteration 66, loss = 0.07882172\n",
      "Iteration 67, loss = 0.07834012\n",
      "Iteration 68, loss = 0.07778206\n",
      "Iteration 69, loss = 0.07737967\n",
      "Iteration 70, loss = 0.07680557\n",
      "Iteration 71, loss = 0.07641768\n",
      "Iteration 72, loss = 0.07589191\n",
      "Iteration 73, loss = 0.07546780\n",
      "Iteration 74, loss = 0.07502430\n",
      "Iteration 75, loss = 0.07454638\n",
      "Iteration 76, loss = 0.07416033\n",
      "Iteration 77, loss = 0.07368170\n",
      "Iteration 78, loss = 0.07328923\n",
      "Iteration 79, loss = 0.07286091\n",
      "Iteration 80, loss = 0.07243308\n",
      "Iteration 81, loss = 0.07205206\n",
      "Iteration 82, loss = 0.07161910\n",
      "Iteration 83, loss = 0.07123934\n",
      "Iteration 84, loss = 0.07084413\n",
      "Iteration 85, loss = 0.07044106\n",
      "Iteration 86, loss = 0.07007740\n",
      "Iteration 87, loss = 0.06968373\n",
      "Iteration 88, loss = 0.06931006\n",
      "Iteration 89, loss = 0.06895043\n",
      "Iteration 90, loss = 0.06857104\n",
      "Iteration 91, loss = 0.06821662\n",
      "Iteration 92, loss = 0.06786357\n",
      "Iteration 93, loss = 0.06750156\n",
      "Iteration 94, loss = 0.06715965\n",
      "Iteration 95, loss = 0.06681623\n",
      "Iteration 96, loss = 0.06646926\n",
      "Iteration 97, loss = 0.06613772\n",
      "Iteration 98, loss = 0.06580553\n",
      "Iteration 99, loss = 0.06547205\n",
      "Iteration 100, loss = 0.06515090\n",
      "Iteration 101, loss = 0.06483171\n",
      "Iteration 102, loss = 0.06451123\n",
      "Iteration 103, loss = 0.06420014\n",
      "Iteration 104, loss = 0.06389331\n",
      "Iteration 105, loss = 0.06358558\n",
      "Iteration 106, loss = 0.06328402\n",
      "Iteration 107, loss = 0.06298856\n",
      "Iteration 108, loss = 0.06269337\n",
      "Iteration 109, loss = 0.06240138\n",
      "Iteration 110, loss = 0.06211579\n",
      "Iteration 111, loss = 0.06183279\n",
      "Iteration 112, loss = 0.06155108\n",
      "Iteration 113, loss = 0.06127471\n",
      "Iteration 114, loss = 0.06100314\n",
      "Iteration 115, loss = 0.06073404\n",
      "Iteration 116, loss = 0.06046739\n",
      "Iteration 117, loss = 0.06020497\n",
      "Iteration 118, loss = 0.05994636\n",
      "Iteration 119, loss = 0.05969017\n",
      "Iteration 120, loss = 0.05943669\n",
      "Iteration 121, loss = 0.05918662\n",
      "Iteration 122, loss = 0.05893976\n",
      "Iteration 123, loss = 0.05869501\n",
      "Iteration 124, loss = 0.05845220\n",
      "Iteration 125, loss = 0.05821224\n",
      "Iteration 126, loss = 0.05797543\n",
      "Iteration 127, loss = 0.05774144\n",
      "Iteration 128, loss = 0.05750997\n",
      "Iteration 129, loss = 0.05728104\n",
      "Iteration 130, loss = 0.05705482\n",
      "Iteration 131, loss = 0.05683139\n",
      "Iteration 132, loss = 0.05661057\n",
      "Iteration 133, loss = 0.05639208\n",
      "Iteration 134, loss = 0.05617582\n",
      "Iteration 135, loss = 0.05596185\n",
      "Iteration 136, loss = 0.05575024\n",
      "Iteration 137, loss = 0.05554100\n",
      "Iteration 138, loss = 0.05533416\n",
      "Iteration 139, loss = 0.05512973\n",
      "Iteration 140, loss = 0.05492748\n",
      "Iteration 141, loss = 0.05472733\n",
      "Iteration 142, loss = 0.05452931\n",
      "Iteration 143, loss = 0.05433344\n",
      "Iteration 144, loss = 0.05413965\n",
      "Iteration 145, loss = 0.05394791\n",
      "Iteration 146, loss = 0.05375819\n",
      "Iteration 147, loss = 0.05357045\n",
      "Iteration 148, loss = 0.05338466\n",
      "Iteration 149, loss = 0.05320078\n",
      "Iteration 150, loss = 0.05301884\n",
      "Iteration 151, loss = 0.05283878\n",
      "Iteration 152, loss = 0.05266062\n",
      "Iteration 153, loss = 0.05248439\n",
      "Iteration 154, loss = 0.05231024\n",
      "Iteration 155, loss = 0.05213852\n",
      "Iteration 156, loss = 0.05196993\n",
      "Iteration 157, loss = 0.05180608\n",
      "Iteration 158, loss = 0.05165095\n",
      "Iteration 159, loss = 0.05151348\n",
      "Iteration 160, loss = 0.05141956\n",
      "Iteration 161, loss = 0.05141626\n",
      "Iteration 162, loss = 0.05169226\n",
      "Iteration 163, loss = 0.05244698\n",
      "Iteration 164, loss = 0.05516096\n",
      "Iteration 165, loss = 0.05889475\n",
      "Iteration 166, loss = 0.07192338\n",
      "Iteration 167, loss = 0.06707075\n",
      "Iteration 168, loss = 0.06840417\n",
      "Iteration 169, loss = 0.05236976\n",
      "Iteration 170, loss = 0.05152766\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66075849\n",
      "Iteration 2, loss = 8.80107288\n",
      "Iteration 3, loss = 2.55850546\n",
      "Iteration 4, loss = 1.56288407\n",
      "Iteration 5, loss = 0.89429446\n",
      "Iteration 6, loss = 0.77944632\n",
      "Iteration 7, loss = 0.70546707\n",
      "Iteration 8, loss = 0.62035377\n",
      "Iteration 9, loss = 0.60915667\n",
      "Iteration 10, loss = 0.56287074\n",
      "Iteration 11, loss = 0.55608995\n",
      "Iteration 12, loss = 0.51334410\n",
      "Iteration 13, loss = 0.49604037\n",
      "Iteration 14, loss = 0.45114785\n",
      "Iteration 15, loss = 0.44431816\n",
      "Iteration 16, loss = 0.39959402\n",
      "Iteration 17, loss = 0.38250633\n",
      "Iteration 18, loss = 0.34593313\n",
      "Iteration 19, loss = 0.33248652\n",
      "Iteration 20, loss = 0.29697744\n",
      "Iteration 21, loss = 0.28616999\n",
      "Iteration 22, loss = 0.25261681\n",
      "Iteration 23, loss = 0.23889302\n",
      "Iteration 24, loss = 0.22040619\n",
      "Iteration 25, loss = 0.19946076\n",
      "Iteration 26, loss = 0.19429093\n",
      "Iteration 27, loss = 0.17817064\n",
      "Iteration 28, loss = 0.17025651\n",
      "Iteration 29, loss = 0.16635321\n",
      "Iteration 30, loss = 0.15430969\n",
      "Iteration 31, loss = 0.15196847\n",
      "Iteration 32, loss = 0.14954862\n",
      "Iteration 33, loss = 0.14119736\n",
      "Iteration 34, loss = 0.14180709\n",
      "Iteration 35, loss = 0.14304149\n",
      "Iteration 36, loss = 0.13657316\n",
      "Iteration 37, loss = 0.13215634\n",
      "Iteration 38, loss = 0.13371446\n",
      "Iteration 39, loss = 0.13083282\n",
      "Iteration 40, loss = 0.12802714\n",
      "Iteration 41, loss = 0.12886275\n",
      "Iteration 42, loss = 0.12707426\n",
      "Iteration 43, loss = 0.12483660\n",
      "Iteration 44, loss = 0.12496059\n",
      "Iteration 45, loss = 0.12432457\n",
      "Iteration 46, loss = 0.12254268\n",
      "Iteration 47, loss = 0.12174634\n",
      "Iteration 48, loss = 0.12178087\n",
      "Iteration 49, loss = 0.12100421\n",
      "Iteration 50, loss = 0.11954888\n",
      "Iteration 51, loss = 0.11903424\n",
      "Iteration 52, loss = 0.11900761\n",
      "Iteration 53, loss = 0.11819047\n",
      "Iteration 54, loss = 0.11707312\n",
      "Iteration 55, loss = 0.11637346\n",
      "Iteration 56, loss = 0.11613919\n",
      "Iteration 57, loss = 0.11582657\n",
      "Iteration 58, loss = 0.11501845\n",
      "Iteration 59, loss = 0.11416042\n",
      "Iteration 60, loss = 0.11355728\n",
      "Iteration 61, loss = 0.11322163\n",
      "Iteration 62, loss = 0.11292232\n",
      "Iteration 63, loss = 0.11240379\n",
      "Iteration 64, loss = 0.11176831\n",
      "Iteration 65, loss = 0.11108777\n",
      "Iteration 66, loss = 0.11051293\n",
      "Iteration 67, loss = 0.11006334\n",
      "Iteration 68, loss = 0.10969073\n",
      "Iteration 69, loss = 0.10934470\n",
      "Iteration 70, loss = 0.10895543\n",
      "Iteration 71, loss = 0.10856550\n",
      "Iteration 72, loss = 0.10810588\n",
      "Iteration 73, loss = 0.10767292\n",
      "Iteration 74, loss = 0.10720686\n",
      "Iteration 75, loss = 0.10679910\n",
      "Iteration 76, loss = 0.10639011\n",
      "Iteration 77, loss = 0.10608233\n",
      "Iteration 78, loss = 0.10580897\n",
      "Iteration 79, loss = 0.10579034\n",
      "Iteration 80, loss = 0.10588457\n",
      "Iteration 81, loss = 0.10679958\n",
      "Iteration 82, loss = 0.10774262\n",
      "Iteration 83, loss = 0.11106692\n",
      "Iteration 84, loss = 0.11151795\n",
      "Iteration 85, loss = 0.11474304\n",
      "Iteration 86, loss = 0.10881283\n",
      "Iteration 87, loss = 0.10462863\n",
      "Iteration 88, loss = 0.10142263\n",
      "Iteration 89, loss = 0.10197773\n",
      "Iteration 90, loss = 0.10487063\n",
      "Iteration 91, loss = 0.10536271\n",
      "Iteration 92, loss = 0.10495938\n",
      "Iteration 93, loss = 0.10126098\n",
      "Iteration 94, loss = 0.09931814\n",
      "Iteration 95, loss = 0.09967444\n",
      "Iteration 96, loss = 0.10097840\n",
      "Iteration 97, loss = 0.10207806\n",
      "Iteration 98, loss = 0.10046530\n",
      "Iteration 99, loss = 0.09872789\n",
      "Iteration 100, loss = 0.09745238\n",
      "Iteration 101, loss = 0.09745565\n",
      "Iteration 102, loss = 0.09821662\n",
      "Iteration 103, loss = 0.09845369\n",
      "Iteration 104, loss = 0.09828612\n",
      "Iteration 105, loss = 0.09705629\n",
      "Iteration 106, loss = 0.09601024\n",
      "Iteration 107, loss = 0.09538841\n",
      "Iteration 108, loss = 0.09531299\n",
      "Iteration 109, loss = 0.09555761\n",
      "Iteration 110, loss = 0.09568303\n",
      "Iteration 111, loss = 0.09573924\n",
      "Iteration 112, loss = 0.09525884\n",
      "Iteration 113, loss = 0.09477315\n",
      "Iteration 114, loss = 0.09408247\n",
      "Iteration 115, loss = 0.09352336\n",
      "Iteration 116, loss = 0.09306526\n",
      "Iteration 117, loss = 0.09274247\n",
      "Iteration 118, loss = 0.09252457\n",
      "Iteration 119, loss = 0.09238388\n",
      "Iteration 120, loss = 0.09232050\n",
      "Iteration 121, loss = 0.09230519\n",
      "Iteration 122, loss = 0.09246429\n",
      "Iteration 123, loss = 0.09268011\n",
      "Iteration 124, loss = 0.09346147\n",
      "Iteration 125, loss = 0.09421921\n",
      "Iteration 126, loss = 0.09664847\n",
      "Iteration 127, loss = 0.09788452\n",
      "Iteration 128, loss = 0.10268424\n",
      "Iteration 129, loss = 0.10097817\n",
      "Iteration 130, loss = 0.10247403\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66212118\n",
      "Iteration 2, loss = 4.39083993\n",
      "Iteration 3, loss = 1.89489041\n",
      "Iteration 4, loss = 1.82186881\n",
      "Iteration 5, loss = 0.88409022\n",
      "Iteration 6, loss = 0.78858701\n",
      "Iteration 7, loss = 0.68333428\n",
      "Iteration 8, loss = 0.58800165\n",
      "Iteration 9, loss = 0.51936083\n",
      "Iteration 10, loss = 0.52051493\n",
      "Iteration 11, loss = 0.99336514\n",
      "Iteration 12, loss = 1.74142114\n",
      "Iteration 13, loss = 0.88760903\n",
      "Iteration 14, loss = 0.57485548\n",
      "Iteration 15, loss = 0.55686279\n",
      "Iteration 16, loss = 0.46392314\n",
      "Iteration 17, loss = 0.44489883\n",
      "Iteration 18, loss = 0.42734409\n",
      "Iteration 19, loss = 0.40488538\n",
      "Iteration 20, loss = 0.37876511\n",
      "Iteration 21, loss = 0.35025195\n",
      "Iteration 22, loss = 0.32083972\n",
      "Iteration 23, loss = 0.29251211\n",
      "Iteration 24, loss = 0.26643520\n",
      "Iteration 25, loss = 0.24344732\n",
      "Iteration 26, loss = 0.22377981\n",
      "Iteration 27, loss = 0.20772218\n",
      "Iteration 28, loss = 0.19603849\n",
      "Iteration 29, loss = 0.20926262\n",
      "Iteration 30, loss = 0.54823162\n",
      "Iteration 31, loss = 3.16387486\n",
      "Iteration 32, loss = 3.22660028\n",
      "Iteration 33, loss = 0.40606235\n",
      "Iteration 34, loss = 0.56629922\n",
      "Iteration 35, loss = 0.57114698\n",
      "Iteration 36, loss = 0.48632282\n",
      "Iteration 37, loss = 0.39506840\n",
      "Iteration 38, loss = 0.40862521\n",
      "Iteration 39, loss = 0.38678243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65392470\n",
      "Iteration 2, loss = 4.41618898\n",
      "Iteration 3, loss = 1.74759010\n",
      "Iteration 4, loss = 1.74195402\n",
      "Iteration 5, loss = 0.85961758\n",
      "Iteration 6, loss = 0.76691075\n",
      "Iteration 7, loss = 0.65340863\n",
      "Iteration 8, loss = 0.55549976\n",
      "Iteration 9, loss = 0.51263934\n",
      "Iteration 10, loss = 0.60408880\n",
      "Iteration 11, loss = 1.29894739\n",
      "Iteration 12, loss = 0.79676351\n",
      "Iteration 13, loss = 0.48908505\n",
      "Iteration 14, loss = 0.42298642\n",
      "Iteration 15, loss = 0.40790814\n",
      "Iteration 16, loss = 0.38551556\n",
      "Iteration 17, loss = 0.36252527\n",
      "Iteration 18, loss = 0.33664178\n",
      "Iteration 19, loss = 0.30910434\n",
      "Iteration 20, loss = 0.28173115\n",
      "Iteration 21, loss = 0.25664149\n",
      "Iteration 22, loss = 0.23951047\n",
      "Iteration 23, loss = 0.27179716\n",
      "Iteration 24, loss = 0.74533713\n",
      "Iteration 25, loss = 1.81108554\n",
      "Iteration 26, loss = 0.42158479\n",
      "Iteration 27, loss = 0.64896617\n",
      "Iteration 28, loss = 0.43008388\n",
      "Iteration 29, loss = 0.41800047\n",
      "Iteration 30, loss = 0.40271076\n",
      "Iteration 31, loss = 0.38843313\n",
      "Iteration 32, loss = 0.37445631\n",
      "Iteration 33, loss = 0.36018337\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66585275\n",
      "Iteration 2, loss = 4.36051732\n",
      "Iteration 3, loss = 1.87250406\n",
      "Iteration 4, loss = 1.81400858\n",
      "Iteration 5, loss = 0.88420955\n",
      "Iteration 6, loss = 0.78635344\n",
      "Iteration 7, loss = 0.67022016\n",
      "Iteration 8, loss = 0.57518139\n",
      "Iteration 9, loss = 0.47842070\n",
      "Iteration 10, loss = 0.44442426\n",
      "Iteration 11, loss = 0.70921992\n",
      "Iteration 12, loss = 1.83780504\n",
      "Iteration 13, loss = 0.69784760\n",
      "Iteration 14, loss = 0.46194162\n",
      "Iteration 15, loss = 0.40921263\n",
      "Iteration 16, loss = 0.38854977\n",
      "Iteration 17, loss = 0.37226674\n",
      "Iteration 18, loss = 0.35047936\n",
      "Iteration 19, loss = 0.32485444\n",
      "Iteration 20, loss = 0.29596539\n",
      "Iteration 21, loss = 0.26657287\n",
      "Iteration 22, loss = 0.23922073\n",
      "Iteration 23, loss = 0.21501982\n",
      "Iteration 24, loss = 0.19442407\n",
      "Iteration 25, loss = 0.17841589\n",
      "Iteration 26, loss = 0.17211130\n",
      "Iteration 27, loss = 0.23582623\n",
      "Iteration 28, loss = 1.08241684\n",
      "Iteration 29, loss = 3.81575699\n",
      "Iteration 30, loss = 2.10380653\n",
      "Iteration 31, loss = 0.48808555\n",
      "Iteration 32, loss = 0.55487240\n",
      "Iteration 33, loss = 0.41225628\n",
      "Iteration 34, loss = 0.43225483\n",
      "Iteration 35, loss = 0.43029372\n",
      "Iteration 36, loss = 0.42320334\n",
      "Iteration 37, loss = 0.41134300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65830114\n",
      "Iteration 2, loss = 4.37516371\n",
      "Iteration 3, loss = 1.81350084\n",
      "Iteration 4, loss = 1.80657328\n",
      "Iteration 5, loss = 0.88251817\n",
      "Iteration 6, loss = 0.79720470\n",
      "Iteration 7, loss = 0.68077535\n",
      "Iteration 8, loss = 0.58062375\n",
      "Iteration 9, loss = 0.48762086\n",
      "Iteration 10, loss = 0.41881796\n",
      "Iteration 11, loss = 0.37335275\n",
      "Iteration 12, loss = 0.37390306\n",
      "Iteration 13, loss = 1.07047519\n",
      "Iteration 14, loss = 2.60845198\n",
      "Iteration 15, loss = 0.71479540\n",
      "Iteration 16, loss = 0.56698548\n",
      "Iteration 17, loss = 0.43879507\n",
      "Iteration 18, loss = 0.40572174\n",
      "Iteration 19, loss = 0.39478713\n",
      "Iteration 20, loss = 0.37291577\n",
      "Iteration 21, loss = 0.34300742\n",
      "Iteration 22, loss = 0.31520765\n",
      "Iteration 23, loss = 0.28719506\n",
      "Iteration 24, loss = 0.26059840\n",
      "Iteration 25, loss = 0.23705954\n",
      "Iteration 26, loss = 0.21688680\n",
      "Iteration 27, loss = 0.20787415\n",
      "Iteration 28, loss = 0.28815120\n",
      "Iteration 29, loss = 1.27741583\n",
      "Iteration 30, loss = 3.23924369\n",
      "Iteration 31, loss = 0.37230799\n",
      "Iteration 32, loss = 0.79189646\n",
      "Iteration 33, loss = 0.42824892\n",
      "Iteration 34, loss = 0.41390954\n",
      "Iteration 35, loss = 0.40721742\n",
      "Iteration 36, loss = 0.40004486\n",
      "Iteration 37, loss = 0.39185218\n",
      "Iteration 38, loss = 0.38268359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66075849\n",
      "Iteration 2, loss = 4.47111157\n",
      "Iteration 3, loss = 1.79900008\n",
      "Iteration 4, loss = 1.76188637\n",
      "Iteration 5, loss = 0.86230030\n",
      "Iteration 6, loss = 0.78205127\n",
      "Iteration 7, loss = 0.66464241\n",
      "Iteration 8, loss = 0.57070453\n",
      "Iteration 9, loss = 0.48485765\n",
      "Iteration 10, loss = 0.43306198\n",
      "Iteration 11, loss = 0.45182582\n",
      "Iteration 12, loss = 1.26448691\n",
      "Iteration 13, loss = 1.72435996\n",
      "Iteration 14, loss = 0.57999866\n",
      "Iteration 15, loss = 0.49091486\n",
      "Iteration 16, loss = 0.45940213\n",
      "Iteration 17, loss = 0.42228627\n",
      "Iteration 18, loss = 0.38974609\n",
      "Iteration 19, loss = 0.36062385\n",
      "Iteration 20, loss = 0.33275881\n",
      "Iteration 21, loss = 0.30544828\n",
      "Iteration 22, loss = 0.27915797\n",
      "Iteration 23, loss = 0.25514931\n",
      "Iteration 24, loss = 0.23395640\n",
      "Iteration 25, loss = 0.21598206\n",
      "Iteration 26, loss = 0.20119654\n",
      "Iteration 27, loss = 0.19650503\n",
      "Iteration 28, loss = 0.28970331\n",
      "Iteration 29, loss = 1.58388562\n",
      "Iteration 30, loss = 4.11715260\n",
      "Iteration 31, loss = 0.43958814\n",
      "Iteration 32, loss = 1.11873898\n",
      "Iteration 33, loss = 0.67672991\n",
      "Iteration 34, loss = 0.48240117\n",
      "Iteration 35, loss = 0.47242234\n",
      "Iteration 36, loss = 0.45939748\n",
      "Iteration 37, loss = 0.44490255\n",
      "Iteration 38, loss = 0.42585180\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66212118\n",
      "Iteration 2, loss = 1.22620921\n",
      "Iteration 3, loss = 1.22826617\n",
      "Iteration 4, loss = 1.09457918\n",
      "Iteration 5, loss = 0.90222519\n",
      "Iteration 6, loss = 0.80778746\n",
      "Iteration 7, loss = 0.78755742\n",
      "Iteration 8, loss = 0.75013240\n",
      "Iteration 9, loss = 0.68179250\n",
      "Iteration 10, loss = 0.61887536\n",
      "Iteration 11, loss = 0.58738415\n",
      "Iteration 12, loss = 0.57441758\n",
      "Iteration 13, loss = 0.55410832\n",
      "Iteration 14, loss = 0.52455796\n",
      "Iteration 15, loss = 0.50265363\n",
      "Iteration 16, loss = 0.49359897\n",
      "Iteration 17, loss = 0.47954609\n",
      "Iteration 18, loss = 0.45592868\n",
      "Iteration 19, loss = 0.43778253\n",
      "Iteration 20, loss = 0.42985611\n",
      "Iteration 21, loss = 0.42143300\n",
      "Iteration 22, loss = 0.40705102\n",
      "Iteration 23, loss = 0.39355801\n",
      "Iteration 24, loss = 0.38501423\n",
      "Iteration 25, loss = 0.37323462\n",
      "Iteration 26, loss = 0.35799682\n",
      "Iteration 27, loss = 0.34773484\n",
      "Iteration 28, loss = 0.33769501\n",
      "Iteration 29, loss = 0.32347130\n",
      "Iteration 30, loss = 0.31257947\n",
      "Iteration 31, loss = 0.30367421\n",
      "Iteration 32, loss = 0.29098365\n",
      "Iteration 33, loss = 0.28010486\n",
      "Iteration 34, loss = 0.27060760\n",
      "Iteration 35, loss = 0.25818859\n",
      "Iteration 36, loss = 0.24824913\n",
      "Iteration 37, loss = 0.23840543\n",
      "Iteration 38, loss = 0.22734101\n",
      "Iteration 39, loss = 0.21911291\n",
      "Iteration 40, loss = 0.20899325\n",
      "Iteration 41, loss = 0.20093635\n",
      "Iteration 42, loss = 0.19249101\n",
      "Iteration 43, loss = 0.18420831\n",
      "Iteration 44, loss = 0.17731656\n",
      "Iteration 45, loss = 0.16957140\n",
      "Iteration 46, loss = 0.16368701\n",
      "Iteration 47, loss = 0.15677165\n",
      "Iteration 48, loss = 0.15164611\n",
      "Iteration 49, loss = 0.14556939\n",
      "Iteration 50, loss = 0.14109296\n",
      "Iteration 51, loss = 0.13584926\n",
      "Iteration 52, loss = 0.13212408\n",
      "Iteration 53, loss = 0.12753642\n",
      "Iteration 54, loss = 0.12419379\n",
      "Iteration 55, loss = 0.12040522\n",
      "Iteration 56, loss = 0.11748761\n",
      "Iteration 57, loss = 0.11439747\n",
      "Iteration 58, loss = 0.11169860\n",
      "Iteration 59, loss = 0.10927176\n",
      "Iteration 60, loss = 0.10678826\n",
      "Iteration 61, loss = 0.10488589\n",
      "Iteration 62, loss = 0.10270988\n",
      "Iteration 63, loss = 0.10106052\n",
      "Iteration 64, loss = 0.09935640\n",
      "Iteration 65, loss = 0.09773889\n",
      "Iteration 66, loss = 0.09647038\n",
      "Iteration 67, loss = 0.09502527\n",
      "Iteration 68, loss = 0.09383880\n",
      "Iteration 69, loss = 0.09281679\n",
      "Iteration 70, loss = 0.09165870\n",
      "Iteration 71, loss = 0.09070409\n",
      "Iteration 72, loss = 0.08991321\n",
      "Iteration 73, loss = 0.08901916\n",
      "Iteration 74, loss = 0.08817531\n",
      "Iteration 75, loss = 0.08751573\n",
      "Iteration 76, loss = 0.08690015\n",
      "Iteration 77, loss = 0.08623022\n",
      "Iteration 78, loss = 0.08557723\n",
      "Iteration 79, loss = 0.08504108\n",
      "Iteration 80, loss = 0.08458923\n",
      "Iteration 81, loss = 0.08414003\n",
      "Iteration 82, loss = 0.08366276\n",
      "Iteration 83, loss = 0.08317475\n",
      "Iteration 84, loss = 0.08272914\n",
      "Iteration 85, loss = 0.08234999\n",
      "Iteration 86, loss = 0.08202639\n",
      "Iteration 87, loss = 0.08174469\n",
      "Iteration 88, loss = 0.08148214\n",
      "Iteration 89, loss = 0.08122729\n",
      "Iteration 90, loss = 0.08091264\n",
      "Iteration 91, loss = 0.08056572\n",
      "Iteration 92, loss = 0.08021969\n",
      "Iteration 93, loss = 0.07994086\n",
      "Iteration 94, loss = 0.07973249\n",
      "Iteration 95, loss = 0.07956688\n",
      "Iteration 96, loss = 0.07941955\n",
      "Iteration 97, loss = 0.07924410\n",
      "Iteration 98, loss = 0.07903472\n",
      "Iteration 99, loss = 0.07876408\n",
      "Iteration 100, loss = 0.07849939\n",
      "Iteration 101, loss = 0.07828142\n",
      "Iteration 102, loss = 0.07812502\n",
      "Iteration 103, loss = 0.07800993\n",
      "Iteration 104, loss = 0.07790724\n",
      "Iteration 105, loss = 0.07780196\n",
      "Iteration 106, loss = 0.07765861\n",
      "Iteration 107, loss = 0.07749019\n",
      "Iteration 108, loss = 0.07728301\n",
      "Iteration 109, loss = 0.07708259\n",
      "Iteration 110, loss = 0.07690746\n",
      "Iteration 111, loss = 0.07676585\n",
      "Iteration 112, loss = 0.07665301\n",
      "Iteration 113, loss = 0.07657578\n",
      "Iteration 114, loss = 0.07655986\n",
      "Iteration 115, loss = 0.07664045\n",
      "Iteration 116, loss = 0.07692217\n",
      "Iteration 117, loss = 0.07698199\n",
      "Iteration 118, loss = 0.07668588\n",
      "Iteration 119, loss = 0.07595778\n",
      "Iteration 120, loss = 0.07581757\n",
      "Iteration 121, loss = 0.07615713\n",
      "Iteration 122, loss = 0.07605244\n",
      "Iteration 123, loss = 0.07558778\n",
      "Iteration 124, loss = 0.07541038\n",
      "Iteration 125, loss = 0.07560287\n",
      "Iteration 126, loss = 0.07557242\n",
      "Iteration 127, loss = 0.07519397\n",
      "Iteration 128, loss = 0.07508947\n",
      "Iteration 129, loss = 0.07522318\n",
      "Iteration 130, loss = 0.07510005\n",
      "Iteration 131, loss = 0.07483872\n",
      "Iteration 132, loss = 0.07477701\n",
      "Iteration 133, loss = 0.07483368\n",
      "Iteration 134, loss = 0.07473911\n",
      "Iteration 135, loss = 0.07453474\n",
      "Iteration 136, loss = 0.07447205\n",
      "Iteration 137, loss = 0.07449822\n",
      "Iteration 138, loss = 0.07441311\n",
      "Iteration 139, loss = 0.07425788\n",
      "Iteration 140, loss = 0.07417338\n",
      "Iteration 141, loss = 0.07416654\n",
      "Iteration 142, loss = 0.07412676\n",
      "Iteration 143, loss = 0.07400960\n",
      "Iteration 144, loss = 0.07390137\n",
      "Iteration 145, loss = 0.07385316\n",
      "Iteration 146, loss = 0.07382811\n",
      "Iteration 147, loss = 0.07377242\n",
      "Iteration 148, loss = 0.07367688\n",
      "Iteration 149, loss = 0.07358589\n",
      "Iteration 150, loss = 0.07352561\n",
      "Iteration 151, loss = 0.07348719\n",
      "Iteration 152, loss = 0.07344642\n",
      "Iteration 153, loss = 0.07338511\n",
      "Iteration 154, loss = 0.07331001\n",
      "Iteration 155, loss = 0.07323189\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65392470\n",
      "Iteration 2, loss = 1.21265948\n",
      "Iteration 3, loss = 1.21364439\n",
      "Iteration 4, loss = 1.08280137\n",
      "Iteration 5, loss = 0.89682994\n",
      "Iteration 6, loss = 0.80906724\n",
      "Iteration 7, loss = 0.78975307\n",
      "Iteration 8, loss = 0.75041579\n",
      "Iteration 9, loss = 0.68692382\n",
      "Iteration 10, loss = 0.63059185\n",
      "Iteration 11, loss = 0.59961480\n",
      "Iteration 12, loss = 0.58125877\n",
      "Iteration 13, loss = 0.55631603\n",
      "Iteration 14, loss = 0.53080627\n",
      "Iteration 15, loss = 0.51501941\n",
      "Iteration 16, loss = 0.50439885\n",
      "Iteration 17, loss = 0.48307697\n",
      "Iteration 18, loss = 0.45885494\n",
      "Iteration 19, loss = 0.44318968\n",
      "Iteration 20, loss = 0.43430300\n",
      "Iteration 21, loss = 0.42273370\n",
      "Iteration 22, loss = 0.40823403\n",
      "Iteration 23, loss = 0.39718881\n",
      "Iteration 24, loss = 0.38785812\n",
      "Iteration 25, loss = 0.37364336\n",
      "Iteration 26, loss = 0.35888852\n",
      "Iteration 27, loss = 0.34845614\n",
      "Iteration 28, loss = 0.33621750\n",
      "Iteration 29, loss = 0.32267552\n",
      "Iteration 30, loss = 0.31237975\n",
      "Iteration 31, loss = 0.30148190\n",
      "Iteration 32, loss = 0.28864770\n",
      "Iteration 33, loss = 0.27836736\n",
      "Iteration 34, loss = 0.26719400\n",
      "Iteration 35, loss = 0.25482684\n",
      "Iteration 36, loss = 0.24493174\n",
      "Iteration 37, loss = 0.23379091\n",
      "Iteration 38, loss = 0.22341271\n",
      "Iteration 39, loss = 0.21398371\n",
      "Iteration 40, loss = 0.20377636\n",
      "Iteration 41, loss = 0.19542959\n",
      "Iteration 42, loss = 0.18612448\n",
      "Iteration 43, loss = 0.17841420\n",
      "Iteration 44, loss = 0.17031183\n",
      "Iteration 45, loss = 0.16312537\n",
      "Iteration 46, loss = 0.15615410\n",
      "Iteration 47, loss = 0.14975298\n",
      "Iteration 48, loss = 0.14376866\n",
      "Iteration 49, loss = 0.13821856\n",
      "Iteration 50, loss = 0.13295518\n",
      "Iteration 51, loss = 0.12827447\n",
      "Iteration 52, loss = 0.12370284\n",
      "Iteration 53, loss = 0.11977408\n",
      "Iteration 54, loss = 0.11575880\n",
      "Iteration 55, loss = 0.11246453\n",
      "Iteration 56, loss = 0.10903887\n",
      "Iteration 57, loss = 0.10615309\n",
      "Iteration 58, loss = 0.10341835\n",
      "Iteration 59, loss = 0.10077821\n",
      "Iteration 60, loss = 0.09862538\n",
      "Iteration 61, loss = 0.09642062\n",
      "Iteration 62, loss = 0.09445357\n",
      "Iteration 63, loss = 0.09278915\n",
      "Iteration 64, loss = 0.09108870\n",
      "Iteration 65, loss = 0.08956416\n",
      "Iteration 66, loss = 0.08830989\n",
      "Iteration 67, loss = 0.08706891\n",
      "Iteration 68, loss = 0.08585113\n",
      "Iteration 69, loss = 0.08484715\n",
      "Iteration 70, loss = 0.08398940\n",
      "Iteration 71, loss = 0.08312574\n",
      "Iteration 72, loss = 0.08227648\n",
      "Iteration 73, loss = 0.08154153\n",
      "Iteration 74, loss = 0.08093884\n",
      "Iteration 75, loss = 0.08041006\n",
      "Iteration 76, loss = 0.07989142\n",
      "Iteration 77, loss = 0.07936669\n",
      "Iteration 78, loss = 0.07885284\n",
      "Iteration 79, loss = 0.07840470\n",
      "Iteration 80, loss = 0.07803400\n",
      "Iteration 81, loss = 0.07772316\n",
      "Iteration 82, loss = 0.07746003\n",
      "Iteration 83, loss = 0.07721585\n",
      "Iteration 84, loss = 0.07696539\n",
      "Iteration 85, loss = 0.07665703\n",
      "Iteration 86, loss = 0.07633683\n",
      "Iteration 87, loss = 0.07606405\n",
      "Iteration 88, loss = 0.07587072\n",
      "Iteration 89, loss = 0.07573281\n",
      "Iteration 90, loss = 0.07560768\n",
      "Iteration 91, loss = 0.07547098\n",
      "Iteration 92, loss = 0.07527414\n",
      "Iteration 93, loss = 0.07505347\n",
      "Iteration 94, loss = 0.07484555\n",
      "Iteration 95, loss = 0.07469346\n",
      "Iteration 96, loss = 0.07459043\n",
      "Iteration 97, loss = 0.07450764\n",
      "Iteration 98, loss = 0.07442429\n",
      "Iteration 99, loss = 0.07430426\n",
      "Iteration 100, loss = 0.07415594\n",
      "Iteration 101, loss = 0.07398470\n",
      "Iteration 102, loss = 0.07383203\n",
      "Iteration 103, loss = 0.07371639\n",
      "Iteration 104, loss = 0.07363369\n",
      "Iteration 105, loss = 0.07356797\n",
      "Iteration 106, loss = 0.07349923\n",
      "Iteration 107, loss = 0.07342357\n",
      "Iteration 108, loss = 0.07331933\n",
      "Iteration 109, loss = 0.07320155\n",
      "Iteration 110, loss = 0.07307164\n",
      "Iteration 111, loss = 0.07295321\n",
      "Iteration 112, loss = 0.07285282\n",
      "Iteration 113, loss = 0.07277137\n",
      "Iteration 114, loss = 0.07270296\n",
      "Iteration 115, loss = 0.07264102\n",
      "Iteration 116, loss = 0.07258559\n",
      "Iteration 117, loss = 0.07252679\n",
      "Iteration 118, loss = 0.07247010\n",
      "Iteration 119, loss = 0.07239565\n",
      "Iteration 120, loss = 0.07231412\n",
      "Iteration 121, loss = 0.07220916\n",
      "Iteration 122, loss = 0.07210209\n",
      "Iteration 123, loss = 0.07199642\n",
      "Iteration 124, loss = 0.07190539\n",
      "Iteration 125, loss = 0.07183044\n",
      "Iteration 126, loss = 0.07176876\n",
      "Iteration 127, loss = 0.07171638\n",
      "Iteration 128, loss = 0.07166963\n",
      "Iteration 129, loss = 0.07163065\n",
      "Iteration 130, loss = 0.07159219\n",
      "Iteration 131, loss = 0.07156154\n",
      "Iteration 132, loss = 0.07151494\n",
      "Iteration 133, loss = 0.07146256\n",
      "Iteration 134, loss = 0.07137171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66585275\n",
      "Iteration 2, loss = 1.22005569\n",
      "Iteration 3, loss = 1.21559090\n",
      "Iteration 4, loss = 1.08868676\n",
      "Iteration 5, loss = 0.89972886\n",
      "Iteration 6, loss = 0.80172195\n",
      "Iteration 7, loss = 0.78376667\n",
      "Iteration 8, loss = 0.75133693\n",
      "Iteration 9, loss = 0.68606113\n",
      "Iteration 10, loss = 0.62221588\n",
      "Iteration 11, loss = 0.58906517\n",
      "Iteration 12, loss = 0.57557467\n",
      "Iteration 13, loss = 0.55595744\n",
      "Iteration 14, loss = 0.52609735\n",
      "Iteration 15, loss = 0.50184182\n",
      "Iteration 16, loss = 0.49127344\n",
      "Iteration 17, loss = 0.47858871\n",
      "Iteration 18, loss = 0.45474320\n",
      "Iteration 19, loss = 0.43315138\n",
      "Iteration 20, loss = 0.42238489\n",
      "Iteration 21, loss = 0.41433542\n",
      "Iteration 22, loss = 0.40047916\n",
      "Iteration 23, loss = 0.38447817\n",
      "Iteration 24, loss = 0.37381622\n",
      "Iteration 25, loss = 0.36270793\n",
      "Iteration 26, loss = 0.34691035\n",
      "Iteration 27, loss = 0.33230133\n",
      "Iteration 28, loss = 0.32117222\n",
      "Iteration 29, loss = 0.30809317\n",
      "Iteration 30, loss = 0.29433014\n",
      "Iteration 31, loss = 0.28387886\n",
      "Iteration 32, loss = 0.27171877\n",
      "Iteration 33, loss = 0.25855768\n",
      "Iteration 34, loss = 0.24792771\n",
      "Iteration 35, loss = 0.23558079\n",
      "Iteration 36, loss = 0.22339130\n",
      "Iteration 37, loss = 0.21323020\n",
      "Iteration 38, loss = 0.20158326\n",
      "Iteration 39, loss = 0.19176457\n",
      "Iteration 40, loss = 0.18179817\n",
      "Iteration 41, loss = 0.17215504\n",
      "Iteration 42, loss = 0.16371157\n",
      "Iteration 43, loss = 0.15469022\n",
      "Iteration 44, loss = 0.14734820\n",
      "Iteration 45, loss = 0.13933989\n",
      "Iteration 46, loss = 0.13281637\n",
      "Iteration 47, loss = 0.12599539\n",
      "Iteration 48, loss = 0.12032099\n",
      "Iteration 49, loss = 0.11461802\n",
      "Iteration 50, loss = 0.10964072\n",
      "Iteration 51, loss = 0.10485316\n",
      "Iteration 52, loss = 0.10055489\n",
      "Iteration 53, loss = 0.09658454\n",
      "Iteration 54, loss = 0.09291746\n",
      "Iteration 55, loss = 0.08959757\n",
      "Iteration 56, loss = 0.08653841\n",
      "Iteration 57, loss = 0.08377582\n",
      "Iteration 58, loss = 0.08125592\n",
      "Iteration 59, loss = 0.07892126\n",
      "Iteration 60, loss = 0.07687213\n",
      "Iteration 61, loss = 0.07491001\n",
      "Iteration 62, loss = 0.07323475\n",
      "Iteration 63, loss = 0.07158852\n",
      "Iteration 64, loss = 0.07023222\n",
      "Iteration 65, loss = 0.06885555\n",
      "Iteration 66, loss = 0.06773864\n",
      "Iteration 67, loss = 0.06661006\n",
      "Iteration 68, loss = 0.06566409\n",
      "Iteration 69, loss = 0.06476676\n",
      "Iteration 70, loss = 0.06393310\n",
      "Iteration 71, loss = 0.06323025\n",
      "Iteration 72, loss = 0.06250987\n",
      "Iteration 73, loss = 0.06190804\n",
      "Iteration 74, loss = 0.06135001\n",
      "Iteration 75, loss = 0.06078627\n",
      "Iteration 76, loss = 0.06032568\n",
      "Iteration 77, loss = 0.05989323\n",
      "Iteration 78, loss = 0.05944428\n",
      "Iteration 79, loss = 0.05906893\n",
      "Iteration 80, loss = 0.05874185\n",
      "Iteration 81, loss = 0.05839488\n",
      "Iteration 82, loss = 0.05806668\n",
      "Iteration 83, loss = 0.05779473\n",
      "Iteration 84, loss = 0.05754880\n",
      "Iteration 85, loss = 0.05729451\n",
      "Iteration 86, loss = 0.05703797\n",
      "Iteration 87, loss = 0.05681133\n",
      "Iteration 88, loss = 0.05661922\n",
      "Iteration 89, loss = 0.05644680\n",
      "Iteration 90, loss = 0.05627768\n",
      "Iteration 91, loss = 0.05609912\n",
      "Iteration 92, loss = 0.05592139\n",
      "Iteration 93, loss = 0.05576177\n",
      "Iteration 94, loss = 0.05562680\n",
      "Iteration 95, loss = 0.05551050\n",
      "Iteration 96, loss = 0.05540618\n",
      "Iteration 97, loss = 0.05530643\n",
      "Iteration 98, loss = 0.05519447\n",
      "Iteration 99, loss = 0.05506791\n",
      "Iteration 100, loss = 0.05494179\n",
      "Iteration 101, loss = 0.05483872\n",
      "Iteration 102, loss = 0.05475975\n",
      "Iteration 103, loss = 0.05469131\n",
      "Iteration 104, loss = 0.05461871\n",
      "Iteration 105, loss = 0.05452919\n",
      "Iteration 106, loss = 0.05442873\n",
      "Iteration 107, loss = 0.05433314\n",
      "Iteration 108, loss = 0.05425466\n",
      "Iteration 109, loss = 0.05419070\n",
      "Iteration 110, loss = 0.05413120\n",
      "Iteration 111, loss = 0.05406794\n",
      "Iteration 112, loss = 0.05399431\n",
      "Iteration 113, loss = 0.05391169\n",
      "Iteration 114, loss = 0.05382996\n",
      "Iteration 115, loss = 0.05375582\n",
      "Iteration 116, loss = 0.05369097\n",
      "Iteration 117, loss = 0.05363288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65830114\n",
      "Iteration 2, loss = 1.21550273\n",
      "Iteration 3, loss = 1.21129926\n",
      "Iteration 4, loss = 1.07841007\n",
      "Iteration 5, loss = 0.89151813\n",
      "Iteration 6, loss = 0.80165782\n",
      "Iteration 7, loss = 0.78219728\n",
      "Iteration 8, loss = 0.74374576\n",
      "Iteration 9, loss = 0.67866538\n",
      "Iteration 10, loss = 0.62065792\n",
      "Iteration 11, loss = 0.58987387\n",
      "Iteration 12, loss = 0.57229443\n",
      "Iteration 13, loss = 0.54905778\n",
      "Iteration 14, loss = 0.52190804\n",
      "Iteration 15, loss = 0.50374776\n",
      "Iteration 16, loss = 0.49267119\n",
      "Iteration 17, loss = 0.47352135\n",
      "Iteration 18, loss = 0.44877555\n",
      "Iteration 19, loss = 0.43145556\n",
      "Iteration 20, loss = 0.42225339\n",
      "Iteration 21, loss = 0.41097431\n",
      "Iteration 22, loss = 0.39562415\n",
      "Iteration 23, loss = 0.38290793\n",
      "Iteration 24, loss = 0.37294887\n",
      "Iteration 25, loss = 0.35870798\n",
      "Iteration 26, loss = 0.34310848\n",
      "Iteration 27, loss = 0.33160198\n",
      "Iteration 28, loss = 0.31948588\n",
      "Iteration 29, loss = 0.30502828\n",
      "Iteration 30, loss = 0.29351237\n",
      "Iteration 31, loss = 0.28283318\n",
      "Iteration 32, loss = 0.26963048\n",
      "Iteration 33, loss = 0.25831160\n",
      "Iteration 34, loss = 0.24736054\n",
      "Iteration 35, loss = 0.23461704\n",
      "Iteration 36, loss = 0.22391782\n",
      "Iteration 37, loss = 0.21317244\n",
      "Iteration 38, loss = 0.20187545\n",
      "Iteration 39, loss = 0.19267791\n",
      "Iteration 40, loss = 0.18214904\n",
      "Iteration 41, loss = 0.17337766\n",
      "Iteration 42, loss = 0.16446633\n",
      "Iteration 43, loss = 0.15597542\n",
      "Iteration 44, loss = 0.14842592\n",
      "Iteration 45, loss = 0.14045965\n",
      "Iteration 46, loss = 0.13397999\n",
      "Iteration 47, loss = 0.12699173\n",
      "Iteration 48, loss = 0.12135090\n",
      "Iteration 49, loss = 0.11534824\n",
      "Iteration 50, loss = 0.11039120\n",
      "Iteration 51, loss = 0.10531768\n",
      "Iteration 52, loss = 0.10113828\n",
      "Iteration 53, loss = 0.09676043\n",
      "Iteration 54, loss = 0.09316298\n",
      "Iteration 55, loss = 0.08942726\n",
      "Iteration 56, loss = 0.08640674\n",
      "Iteration 57, loss = 0.08321575\n",
      "Iteration 58, loss = 0.08066004\n",
      "Iteration 59, loss = 0.07793377\n",
      "Iteration 60, loss = 0.07578005\n",
      "Iteration 61, loss = 0.07346314\n",
      "Iteration 62, loss = 0.07162938\n",
      "Iteration 63, loss = 0.06968881\n",
      "Iteration 64, loss = 0.06805871\n",
      "Iteration 65, loss = 0.06647964\n",
      "Iteration 66, loss = 0.06497659\n",
      "Iteration 67, loss = 0.06371636\n",
      "Iteration 68, loss = 0.06237153\n",
      "Iteration 69, loss = 0.06128457\n",
      "Iteration 70, loss = 0.06018877\n",
      "Iteration 71, loss = 0.05914802\n",
      "Iteration 72, loss = 0.05828447\n",
      "Iteration 73, loss = 0.05736235\n",
      "Iteration 74, loss = 0.05653848\n",
      "Iteration 75, loss = 0.05583052\n",
      "Iteration 76, loss = 0.05507619\n",
      "Iteration 77, loss = 0.05438938\n",
      "Iteration 78, loss = 0.05380734\n",
      "Iteration 79, loss = 0.05320729\n",
      "Iteration 80, loss = 0.05260794\n",
      "Iteration 81, loss = 0.05210229\n",
      "Iteration 82, loss = 0.05164259\n",
      "Iteration 83, loss = 0.05115578\n",
      "Iteration 84, loss = 0.05067835\n",
      "Iteration 85, loss = 0.05026187\n",
      "Iteration 86, loss = 0.04989056\n",
      "Iteration 87, loss = 0.04952452\n",
      "Iteration 88, loss = 0.04914530\n",
      "Iteration 89, loss = 0.04877400\n",
      "Iteration 90, loss = 0.04843205\n",
      "Iteration 91, loss = 0.04812305\n",
      "Iteration 92, loss = 0.04783877\n",
      "Iteration 93, loss = 0.04757209\n",
      "Iteration 94, loss = 0.04732013\n",
      "Iteration 95, loss = 0.04706526\n",
      "Iteration 96, loss = 0.04680379\n",
      "Iteration 97, loss = 0.04652698\n",
      "Iteration 98, loss = 0.04626033\n",
      "Iteration 99, loss = 0.04602099\n",
      "Iteration 100, loss = 0.04581176\n",
      "Iteration 101, loss = 0.04562602\n",
      "Iteration 102, loss = 0.04545834\n",
      "Iteration 103, loss = 0.04530920\n",
      "Iteration 104, loss = 0.04515751\n",
      "Iteration 105, loss = 0.04499167\n",
      "Iteration 106, loss = 0.04477256\n",
      "Iteration 107, loss = 0.04454332\n",
      "Iteration 108, loss = 0.04434672\n",
      "Iteration 109, loss = 0.04420567\n",
      "Iteration 110, loss = 0.04410148\n",
      "Iteration 111, loss = 0.04399889\n",
      "Iteration 112, loss = 0.04387859\n",
      "Iteration 113, loss = 0.04371333\n",
      "Iteration 114, loss = 0.04353517\n",
      "Iteration 115, loss = 0.04337220\n",
      "Iteration 116, loss = 0.04324668\n",
      "Iteration 117, loss = 0.04315155\n",
      "Iteration 118, loss = 0.04306715\n",
      "Iteration 119, loss = 0.04298075\n",
      "Iteration 120, loss = 0.04287089\n",
      "Iteration 121, loss = 0.04274604\n",
      "Iteration 122, loss = 0.04260712\n",
      "Iteration 123, loss = 0.04247755\n",
      "Iteration 124, loss = 0.04236728\n",
      "Iteration 125, loss = 0.04227643\n",
      "Iteration 126, loss = 0.04219891\n",
      "Iteration 127, loss = 0.04212783\n",
      "Iteration 128, loss = 0.04206227\n",
      "Iteration 129, loss = 0.04199210\n",
      "Iteration 130, loss = 0.04192021\n",
      "Iteration 131, loss = 0.04182898\n",
      "Iteration 132, loss = 0.04172860\n",
      "Iteration 133, loss = 0.04161709\n",
      "Iteration 134, loss = 0.04151161\n",
      "Iteration 135, loss = 0.04141912\n",
      "Iteration 136, loss = 0.04134231\n",
      "Iteration 137, loss = 0.04127805\n",
      "Iteration 138, loss = 0.04122164\n",
      "Iteration 139, loss = 0.04117064\n",
      "Iteration 140, loss = 0.04112025\n",
      "Iteration 141, loss = 0.04107273\n",
      "Iteration 142, loss = 0.04101671\n",
      "Iteration 143, loss = 0.04095784\n",
      "Iteration 144, loss = 0.04088086\n",
      "Iteration 145, loss = 0.04079764\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66075849\n",
      "Iteration 2, loss = 1.22060516\n",
      "Iteration 3, loss = 1.22075016\n",
      "Iteration 4, loss = 1.07659161\n",
      "Iteration 5, loss = 0.88346167\n",
      "Iteration 6, loss = 0.79778681\n",
      "Iteration 7, loss = 0.77755050\n",
      "Iteration 8, loss = 0.73420261\n",
      "Iteration 9, loss = 0.66687033\n",
      "Iteration 10, loss = 0.61171224\n",
      "Iteration 11, loss = 0.58577516\n",
      "Iteration 12, loss = 0.56993295\n",
      "Iteration 13, loss = 0.54498869\n",
      "Iteration 14, loss = 0.51661834\n",
      "Iteration 15, loss = 0.50112076\n",
      "Iteration 16, loss = 0.49297861\n",
      "Iteration 17, loss = 0.47365242\n",
      "Iteration 18, loss = 0.44977715\n",
      "Iteration 19, loss = 0.43473388\n",
      "Iteration 20, loss = 0.42686930\n",
      "Iteration 21, loss = 0.41571375\n",
      "Iteration 22, loss = 0.40087608\n",
      "Iteration 23, loss = 0.38991747\n",
      "Iteration 24, loss = 0.38137514\n",
      "Iteration 25, loss = 0.36785462\n",
      "Iteration 26, loss = 0.35389823\n",
      "Iteration 27, loss = 0.34425243\n",
      "Iteration 28, loss = 0.33301004\n",
      "Iteration 29, loss = 0.31973533\n",
      "Iteration 30, loss = 0.30970989\n",
      "Iteration 31, loss = 0.29978998\n",
      "Iteration 32, loss = 0.28726561\n",
      "Iteration 33, loss = 0.27714723\n",
      "Iteration 34, loss = 0.26699079\n",
      "Iteration 35, loss = 0.25504395\n",
      "Iteration 36, loss = 0.24549917\n",
      "Iteration 37, loss = 0.23508299\n",
      "Iteration 38, loss = 0.22459978\n",
      "Iteration 39, loss = 0.21593155\n",
      "Iteration 40, loss = 0.20570132\n",
      "Iteration 41, loss = 0.19767067\n",
      "Iteration 42, loss = 0.18861993\n",
      "Iteration 43, loss = 0.18100227\n",
      "Iteration 44, loss = 0.17316822\n",
      "Iteration 45, loss = 0.16600680\n",
      "Iteration 46, loss = 0.15924038\n",
      "Iteration 47, loss = 0.15297378\n",
      "Iteration 48, loss = 0.14710638\n",
      "Iteration 49, loss = 0.14163036\n",
      "Iteration 50, loss = 0.13656913\n",
      "Iteration 51, loss = 0.13199760\n",
      "Iteration 52, loss = 0.12755503\n",
      "Iteration 53, loss = 0.12387512\n",
      "Iteration 54, loss = 0.11990718\n",
      "Iteration 55, loss = 0.11683178\n",
      "Iteration 56, loss = 0.11352680\n",
      "Iteration 57, loss = 0.11090464\n",
      "Iteration 58, loss = 0.10824249\n",
      "Iteration 59, loss = 0.10588508\n",
      "Iteration 60, loss = 0.10382614\n",
      "Iteration 61, loss = 0.10170563\n",
      "Iteration 62, loss = 0.10004255\n",
      "Iteration 63, loss = 0.09830798\n",
      "Iteration 64, loss = 0.09678405\n",
      "Iteration 65, loss = 0.09546977\n",
      "Iteration 66, loss = 0.09408198\n",
      "Iteration 67, loss = 0.09294693\n",
      "Iteration 68, loss = 0.09188117\n",
      "Iteration 69, loss = 0.09078386\n",
      "Iteration 70, loss = 0.08988385\n",
      "Iteration 71, loss = 0.08906683\n",
      "Iteration 72, loss = 0.08821800\n",
      "Iteration 73, loss = 0.08741484\n",
      "Iteration 74, loss = 0.08676250\n",
      "Iteration 75, loss = 0.08616508\n",
      "Iteration 76, loss = 0.08554349\n",
      "Iteration 77, loss = 0.08492222\n",
      "Iteration 78, loss = 0.08436691\n",
      "Iteration 79, loss = 0.08389004\n",
      "Iteration 80, loss = 0.08347760\n",
      "Iteration 81, loss = 0.08313007\n",
      "Iteration 82, loss = 0.08280729\n",
      "Iteration 83, loss = 0.08247175\n",
      "Iteration 84, loss = 0.08200404\n",
      "Iteration 85, loss = 0.08152641\n",
      "Iteration 86, loss = 0.08116510\n",
      "Iteration 87, loss = 0.08094092\n",
      "Iteration 88, loss = 0.08077127\n",
      "Iteration 89, loss = 0.08054081\n",
      "Iteration 90, loss = 0.08022795\n",
      "Iteration 91, loss = 0.07986329\n",
      "Iteration 92, loss = 0.07957198\n",
      "Iteration 93, loss = 0.07938465\n",
      "Iteration 94, loss = 0.07925221\n",
      "Iteration 95, loss = 0.07911561\n",
      "Iteration 96, loss = 0.07891191\n",
      "Iteration 97, loss = 0.07865959\n",
      "Iteration 98, loss = 0.07839365\n",
      "Iteration 99, loss = 0.07817881\n",
      "Iteration 100, loss = 0.07802694\n",
      "Iteration 101, loss = 0.07791520\n",
      "Iteration 102, loss = 0.07781997\n",
      "Iteration 103, loss = 0.07770704\n",
      "Iteration 104, loss = 0.07757211\n",
      "Iteration 105, loss = 0.07738380\n",
      "Iteration 106, loss = 0.07717801\n",
      "Iteration 107, loss = 0.07698050\n",
      "Iteration 108, loss = 0.07682088\n",
      "Iteration 109, loss = 0.07670124\n",
      "Iteration 110, loss = 0.07661100\n",
      "Iteration 111, loss = 0.07654137\n",
      "Iteration 112, loss = 0.07647905\n",
      "Iteration 113, loss = 0.07642692\n",
      "Iteration 114, loss = 0.07633739\n",
      "Iteration 115, loss = 0.07621902\n",
      "Iteration 116, loss = 0.07602978\n",
      "Iteration 117, loss = 0.07583393\n",
      "Iteration 118, loss = 0.07567036\n",
      "Iteration 119, loss = 0.07556681\n",
      "Iteration 120, loss = 0.07551089\n",
      "Iteration 121, loss = 0.07547290\n",
      "Iteration 122, loss = 0.07543440\n",
      "Iteration 123, loss = 0.07536009\n",
      "Iteration 124, loss = 0.07526084\n",
      "Iteration 125, loss = 0.07511738\n",
      "Iteration 126, loss = 0.07497168\n",
      "Iteration 127, loss = 0.07484008\n",
      "Iteration 128, loss = 0.07474125\n",
      "Iteration 129, loss = 0.07467030\n",
      "Iteration 130, loss = 0.07461745\n",
      "Iteration 131, loss = 0.07457596\n",
      "Iteration 132, loss = 0.07453953\n",
      "Iteration 133, loss = 0.07450184\n",
      "Iteration 134, loss = 0.07445173\n",
      "Iteration 135, loss = 0.07439109\n",
      "Iteration 136, loss = 0.07429251\n",
      "Iteration 137, loss = 0.07418026\n",
      "Iteration 138, loss = 0.07405343\n",
      "Iteration 139, loss = 0.07393890\n",
      "Iteration 140, loss = 0.07384554\n",
      "Iteration 141, loss = 0.07377597\n",
      "Iteration 142, loss = 0.07372468\n",
      "Iteration 143, loss = 0.07368491\n",
      "Iteration 144, loss = 0.07365616\n",
      "Iteration 145, loss = 0.07363438\n",
      "Iteration 146, loss = 0.07362992\n",
      "Iteration 147, loss = 0.07360640\n",
      "Iteration 148, loss = 0.07359044\n",
      "Iteration 149, loss = 0.07352424\n",
      "Iteration 150, loss = 0.07343102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66212118\n",
      "Iteration 2, loss = 1.36077107\n",
      "Iteration 3, loss = 1.21525789\n",
      "Iteration 4, loss = 1.17954466\n",
      "Iteration 5, loss = 1.08999806\n",
      "Iteration 6, loss = 0.98318756\n",
      "Iteration 7, loss = 0.90460028\n",
      "Iteration 8, loss = 0.85120918\n",
      "Iteration 9, loss = 0.80619209\n",
      "Iteration 10, loss = 0.76375149\n",
      "Iteration 11, loss = 0.72490423\n",
      "Iteration 12, loss = 0.68872095\n",
      "Iteration 13, loss = 0.65702137\n",
      "Iteration 14, loss = 0.62846302\n",
      "Iteration 15, loss = 0.60234698\n",
      "Iteration 16, loss = 0.57916056\n",
      "Iteration 17, loss = 0.55938091\n",
      "Iteration 18, loss = 0.54198706\n",
      "Iteration 19, loss = 0.52642743\n",
      "Iteration 20, loss = 0.51245960\n",
      "Iteration 21, loss = 0.49979672\n",
      "Iteration 22, loss = 0.48823170\n",
      "Iteration 23, loss = 0.47759609\n",
      "Iteration 24, loss = 0.46774718\n",
      "Iteration 25, loss = 0.45853649\n",
      "Iteration 26, loss = 0.44984395\n",
      "Iteration 27, loss = 0.44156738\n",
      "Iteration 28, loss = 0.43374103\n",
      "Iteration 29, loss = 0.42639265\n",
      "Iteration 30, loss = 0.41947509\n",
      "Iteration 31, loss = 0.41289507\n",
      "Iteration 32, loss = 0.40655816\n",
      "Iteration 33, loss = 0.40045143\n",
      "Iteration 34, loss = 0.39453810\n",
      "Iteration 35, loss = 0.38880429\n",
      "Iteration 36, loss = 0.38322760\n",
      "Iteration 37, loss = 0.37779697\n",
      "Iteration 38, loss = 0.37249899\n",
      "Iteration 39, loss = 0.36732821\n",
      "Iteration 40, loss = 0.36228756\n",
      "Iteration 41, loss = 0.35736579\n",
      "Iteration 42, loss = 0.35256087\n",
      "Iteration 43, loss = 0.34786286\n",
      "Iteration 44, loss = 0.34326736\n",
      "Iteration 45, loss = 0.33877878\n",
      "Iteration 46, loss = 0.33438472\n",
      "Iteration 47, loss = 0.33008680\n",
      "Iteration 48, loss = 0.32588298\n",
      "Iteration 49, loss = 0.32177193\n",
      "Iteration 50, loss = 0.31775566\n",
      "Iteration 51, loss = 0.31382000\n",
      "Iteration 52, loss = 0.30996951\n",
      "Iteration 53, loss = 0.30620485\n",
      "Iteration 54, loss = 0.30251947\n",
      "Iteration 55, loss = 0.29890466\n",
      "Iteration 56, loss = 0.29535276\n",
      "Iteration 57, loss = 0.29187221\n",
      "Iteration 58, loss = 0.28846009\n",
      "Iteration 59, loss = 0.28510632\n",
      "Iteration 60, loss = 0.28178098\n",
      "Iteration 61, loss = 0.27844625\n",
      "Iteration 62, loss = 0.27503109\n",
      "Iteration 63, loss = 0.27150019\n",
      "Iteration 64, loss = 0.26797202\n",
      "Iteration 65, loss = 0.26460850\n",
      "Iteration 66, loss = 0.26156779\n",
      "Iteration 67, loss = 0.25870140\n",
      "Iteration 68, loss = 0.25594523\n",
      "Iteration 69, loss = 0.25329656\n",
      "Iteration 70, loss = 0.25071338\n",
      "Iteration 71, loss = 0.24818860\n",
      "Iteration 72, loss = 0.24571793\n",
      "Iteration 73, loss = 0.24329918\n",
      "Iteration 74, loss = 0.24093117\n",
      "Iteration 75, loss = 0.23861411\n",
      "Iteration 76, loss = 0.23634578\n",
      "Iteration 77, loss = 0.23412510\n",
      "Iteration 78, loss = 0.23194964\n",
      "Iteration 79, loss = 0.22981930\n",
      "Iteration 80, loss = 0.22773311\n",
      "Iteration 81, loss = 0.22569008\n",
      "Iteration 82, loss = 0.22369080\n",
      "Iteration 83, loss = 0.22173227\n",
      "Iteration 84, loss = 0.21981548\n",
      "Iteration 85, loss = 0.21793522\n",
      "Iteration 86, loss = 0.21609438\n",
      "Iteration 87, loss = 0.21429049\n",
      "Iteration 88, loss = 0.21252359\n",
      "Iteration 89, loss = 0.21079105\n",
      "Iteration 90, loss = 0.20909243\n",
      "Iteration 91, loss = 0.20742499\n",
      "Iteration 92, loss = 0.20578924\n",
      "Iteration 93, loss = 0.20418664\n",
      "Iteration 94, loss = 0.20261384\n",
      "Iteration 95, loss = 0.20107164\n",
      "Iteration 96, loss = 0.19955985\n",
      "Iteration 97, loss = 0.19807616\n",
      "Iteration 98, loss = 0.19661929\n",
      "Iteration 99, loss = 0.19519008\n",
      "Iteration 100, loss = 0.19378842\n",
      "Iteration 101, loss = 0.19241374\n",
      "Iteration 102, loss = 0.19106482\n",
      "Iteration 103, loss = 0.18974047\n",
      "Iteration 104, loss = 0.18844087\n",
      "Iteration 105, loss = 0.18716403\n",
      "Iteration 106, loss = 0.18591394\n",
      "Iteration 107, loss = 0.18468895\n",
      "Iteration 108, loss = 0.18348801\n",
      "Iteration 109, loss = 0.18230822\n",
      "Iteration 110, loss = 0.18114991\n",
      "Iteration 111, loss = 0.18001575\n",
      "Iteration 112, loss = 0.17891232\n",
      "Iteration 113, loss = 0.17783610\n",
      "Iteration 114, loss = 0.17678243\n",
      "Iteration 115, loss = 0.17574904\n",
      "Iteration 116, loss = 0.17473734\n",
      "Iteration 117, loss = 0.17374419\n",
      "Iteration 118, loss = 0.17276854\n",
      "Iteration 119, loss = 0.17181061\n",
      "Iteration 120, loss = 0.17086977\n",
      "Iteration 121, loss = 0.16994604\n",
      "Iteration 122, loss = 0.16903843\n",
      "Iteration 123, loss = 0.16814712\n",
      "Iteration 124, loss = 0.16727168\n",
      "Iteration 125, loss = 0.16641180\n",
      "Iteration 126, loss = 0.16556718\n",
      "Iteration 127, loss = 0.16473756\n",
      "Iteration 128, loss = 0.16392253\n",
      "Iteration 129, loss = 0.16312178\n",
      "Iteration 130, loss = 0.16233498\n",
      "Iteration 131, loss = 0.16156174\n",
      "Iteration 132, loss = 0.16080175\n",
      "Iteration 133, loss = 0.16005540\n",
      "Iteration 134, loss = 0.15932190\n",
      "Iteration 135, loss = 0.15860166\n",
      "Iteration 136, loss = 0.15789372\n",
      "Iteration 137, loss = 0.15719808\n",
      "Iteration 138, loss = 0.15651488\n",
      "Iteration 139, loss = 0.15584247\n",
      "Iteration 140, loss = 0.15518063\n",
      "Iteration 141, loss = 0.15452920\n",
      "Iteration 142, loss = 0.15388838\n",
      "Iteration 143, loss = 0.15325854\n",
      "Iteration 144, loss = 0.15263882\n",
      "Iteration 145, loss = 0.15202930\n",
      "Iteration 146, loss = 0.15143145\n",
      "Iteration 147, loss = 0.15084384\n",
      "Iteration 148, loss = 0.15026584\n",
      "Iteration 149, loss = 0.14969678\n",
      "Iteration 150, loss = 0.14913654\n",
      "Iteration 151, loss = 0.14858511\n",
      "Iteration 152, loss = 0.14804304\n",
      "Iteration 153, loss = 0.14750978\n",
      "Iteration 154, loss = 0.14698448\n",
      "Iteration 155, loss = 0.14646721\n",
      "Iteration 156, loss = 0.14595805\n",
      "Iteration 157, loss = 0.14545692\n",
      "Iteration 158, loss = 0.14496303\n",
      "Iteration 159, loss = 0.14447729\n",
      "Iteration 160, loss = 0.14399891\n",
      "Iteration 161, loss = 0.14353006\n",
      "Iteration 162, loss = 0.14306833\n",
      "Iteration 163, loss = 0.14261329\n",
      "Iteration 164, loss = 0.14216433\n",
      "Iteration 165, loss = 0.14172155\n",
      "Iteration 166, loss = 0.14128500\n",
      "Iteration 167, loss = 0.14085409\n",
      "Iteration 168, loss = 0.14042917\n",
      "Iteration 169, loss = 0.14000992\n",
      "Iteration 170, loss = 0.13959624\n",
      "Iteration 171, loss = 0.13918803\n",
      "Iteration 172, loss = 0.13878535\n",
      "Iteration 173, loss = 0.13838781\n",
      "Iteration 174, loss = 0.13799561\n",
      "Iteration 175, loss = 0.13760857\n",
      "Iteration 176, loss = 0.13722651\n",
      "Iteration 177, loss = 0.13684940\n",
      "Iteration 178, loss = 0.13647717\n",
      "Iteration 179, loss = 0.13610963\n",
      "Iteration 180, loss = 0.13574688\n",
      "Iteration 181, loss = 0.13538862\n",
      "Iteration 182, loss = 0.13503497\n",
      "Iteration 183, loss = 0.13468578\n",
      "Iteration 184, loss = 0.13434091\n",
      "Iteration 185, loss = 0.13400042\n",
      "Iteration 186, loss = 0.13366405\n",
      "Iteration 187, loss = 0.13333185\n",
      "Iteration 188, loss = 0.13300384\n",
      "Iteration 189, loss = 0.13267980\n",
      "Iteration 190, loss = 0.13235980\n",
      "Iteration 191, loss = 0.13204375\n",
      "Iteration 192, loss = 0.13173144\n",
      "Iteration 193, loss = 0.13142310\n",
      "Iteration 194, loss = 0.13111831\n",
      "Iteration 195, loss = 0.13081717\n",
      "Iteration 196, loss = 0.13051939\n",
      "Iteration 197, loss = 0.13022498\n",
      "Iteration 198, loss = 0.12993408\n",
      "Iteration 199, loss = 0.12964641\n",
      "Iteration 200, loss = 0.12936221\n",
      "Iteration 201, loss = 0.12908112\n",
      "Iteration 202, loss = 0.12880327\n",
      "Iteration 203, loss = 0.12852854\n",
      "Iteration 204, loss = 0.12825685\n",
      "Iteration 205, loss = 0.12798830\n",
      "Iteration 206, loss = 0.12772258\n",
      "Iteration 207, loss = 0.12745994\n",
      "Iteration 208, loss = 0.12720009\n",
      "Iteration 209, loss = 0.12694305\n",
      "Iteration 210, loss = 0.12668891\n",
      "Iteration 211, loss = 0.12643737\n",
      "Iteration 212, loss = 0.12618861\n",
      "Iteration 213, loss = 0.12594244\n",
      "Iteration 214, loss = 0.12569899\n",
      "Iteration 215, loss = 0.12545800\n",
      "Iteration 216, loss = 0.12521968\n",
      "Iteration 217, loss = 0.12498376\n",
      "Iteration 218, loss = 0.12475044\n",
      "Iteration 219, loss = 0.12451955\n",
      "Iteration 220, loss = 0.12429111\n",
      "Iteration 221, loss = 0.12406506\n",
      "Iteration 222, loss = 0.12384123\n",
      "Iteration 223, loss = 0.12361979\n",
      "Iteration 224, loss = 0.12340047\n",
      "Iteration 225, loss = 0.12318346\n",
      "Iteration 226, loss = 0.12296853\n",
      "Iteration 227, loss = 0.12275582\n",
      "Iteration 228, loss = 0.12254515\n",
      "Iteration 229, loss = 0.12233665\n",
      "Iteration 230, loss = 0.12213010\n",
      "Iteration 231, loss = 0.12192565\n",
      "Iteration 232, loss = 0.12172312\n",
      "Iteration 233, loss = 0.12152264\n",
      "Iteration 234, loss = 0.12132401\n",
      "Iteration 235, loss = 0.12112737\n",
      "Iteration 236, loss = 0.12093254\n",
      "Iteration 237, loss = 0.12073963\n",
      "Iteration 238, loss = 0.12054847\n",
      "Iteration 239, loss = 0.12035923\n",
      "Iteration 240, loss = 0.12017165\n",
      "Iteration 241, loss = 0.11998586\n",
      "Iteration 242, loss = 0.11980177\n",
      "Iteration 243, loss = 0.11961946\n",
      "Iteration 244, loss = 0.11943874\n",
      "Iteration 245, loss = 0.11925976\n",
      "Iteration 246, loss = 0.11908233\n",
      "Iteration 247, loss = 0.11890659\n",
      "Iteration 248, loss = 0.11873239\n",
      "Iteration 249, loss = 0.11855983\n",
      "Iteration 250, loss = 0.11838874\n",
      "Iteration 251, loss = 0.11821924\n",
      "Iteration 252, loss = 0.11805119\n",
      "Iteration 253, loss = 0.11788470\n",
      "Iteration 254, loss = 0.11771959\n",
      "Iteration 255, loss = 0.11755602\n",
      "Iteration 256, loss = 0.11739380\n",
      "Iteration 257, loss = 0.11723304\n",
      "Iteration 258, loss = 0.11707363\n",
      "Iteration 259, loss = 0.11691566\n",
      "Iteration 260, loss = 0.11675897\n",
      "Iteration 261, loss = 0.11660359\n",
      "Iteration 262, loss = 0.11644929\n",
      "Iteration 263, loss = 0.11629637\n",
      "Iteration 264, loss = 0.11614462\n",
      "Iteration 265, loss = 0.11599409\n",
      "Iteration 266, loss = 0.11584487\n",
      "Iteration 267, loss = 0.11569681\n",
      "Iteration 268, loss = 0.11555002\n",
      "Iteration 269, loss = 0.11540437\n",
      "Iteration 270, loss = 0.11526000\n",
      "Iteration 271, loss = 0.11511675\n",
      "Iteration 272, loss = 0.11497467\n",
      "Iteration 273, loss = 0.11483375\n",
      "Iteration 274, loss = 0.11469398\n",
      "Iteration 275, loss = 0.11455529\n",
      "Iteration 276, loss = 0.11441777\n",
      "Iteration 277, loss = 0.11428128\n",
      "Iteration 278, loss = 0.11414594\n",
      "Iteration 279, loss = 0.11401163\n",
      "Iteration 280, loss = 0.11387839\n",
      "Iteration 281, loss = 0.11374619\n",
      "Iteration 282, loss = 0.11361505\n",
      "Iteration 283, loss = 0.11348484\n",
      "Iteration 284, loss = 0.11335565\n",
      "Iteration 285, loss = 0.11322753\n",
      "Iteration 286, loss = 0.11310036\n",
      "Iteration 287, loss = 0.11297424\n",
      "Iteration 288, loss = 0.11284904\n",
      "Iteration 289, loss = 0.11272485\n",
      "Iteration 290, loss = 0.11260159\n",
      "Iteration 291, loss = 0.11247930\n",
      "Iteration 292, loss = 0.11235792\n",
      "Iteration 293, loss = 0.11223755\n",
      "Iteration 294, loss = 0.11211810\n",
      "Iteration 295, loss = 0.11199967\n",
      "Iteration 296, loss = 0.11188216\n",
      "Iteration 297, loss = 0.11176549\n",
      "Iteration 298, loss = 0.11164986\n",
      "Iteration 299, loss = 0.11153511\n",
      "Iteration 300, loss = 0.11142140\n",
      "Iteration 301, loss = 0.11130837\n",
      "Iteration 302, loss = 0.11119605\n",
      "Iteration 303, loss = 0.11108453\n",
      "Iteration 304, loss = 0.11097368\n",
      "Iteration 305, loss = 0.11086377\n",
      "Iteration 306, loss = 0.11075462\n",
      "Iteration 307, loss = 0.11064625\n",
      "Iteration 308, loss = 0.11053872\n",
      "Iteration 309, loss = 0.11043197\n",
      "Iteration 310, loss = 0.11032593\n",
      "Iteration 311, loss = 0.11022063\n",
      "Iteration 312, loss = 0.11011609\n",
      "Iteration 313, loss = 0.11001223\n",
      "Iteration 314, loss = 0.10990911\n",
      "Iteration 315, loss = 0.10980672\n",
      "Iteration 316, loss = 0.10970503\n",
      "Iteration 317, loss = 0.10960405\n",
      "Iteration 318, loss = 0.10950374\n",
      "Iteration 319, loss = 0.10940414\n",
      "Iteration 320, loss = 0.10930517\n",
      "Iteration 321, loss = 0.10920693\n",
      "Iteration 322, loss = 0.10910930\n",
      "Iteration 323, loss = 0.10901234\n",
      "Iteration 324, loss = 0.10891605\n",
      "Iteration 325, loss = 0.10882036\n",
      "Iteration 326, loss = 0.10872534\n",
      "Iteration 327, loss = 0.10863092\n",
      "Iteration 328, loss = 0.10853714\n",
      "Iteration 329, loss = 0.10844398\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65392470\n",
      "Iteration 2, loss = 1.35337589\n",
      "Iteration 3, loss = 1.20795950\n",
      "Iteration 4, loss = 1.17426737\n",
      "Iteration 5, loss = 1.08914072\n",
      "Iteration 6, loss = 0.98422810\n",
      "Iteration 7, loss = 0.90761662\n",
      "Iteration 8, loss = 0.85447258\n",
      "Iteration 9, loss = 0.81110076\n",
      "Iteration 10, loss = 0.76959131\n",
      "Iteration 11, loss = 0.73093697\n",
      "Iteration 12, loss = 0.69536078\n",
      "Iteration 13, loss = 0.66412887\n",
      "Iteration 14, loss = 0.63623767\n",
      "Iteration 15, loss = 0.61144530\n",
      "Iteration 16, loss = 0.58930160\n",
      "Iteration 17, loss = 0.56944066\n",
      "Iteration 18, loss = 0.55157919\n",
      "Iteration 19, loss = 0.53548808\n",
      "Iteration 20, loss = 0.52092587\n",
      "Iteration 21, loss = 0.50765146\n",
      "Iteration 22, loss = 0.49548196\n",
      "Iteration 23, loss = 0.48425006\n",
      "Iteration 24, loss = 0.47381844\n",
      "Iteration 25, loss = 0.46409087\n",
      "Iteration 26, loss = 0.45496038\n",
      "Iteration 27, loss = 0.44633497\n",
      "Iteration 28, loss = 0.43811687\n",
      "Iteration 29, loss = 0.43026053\n",
      "Iteration 30, loss = 0.42272527\n",
      "Iteration 31, loss = 0.41545817\n",
      "Iteration 32, loss = 0.40847920\n",
      "Iteration 33, loss = 0.40183311\n",
      "Iteration 34, loss = 0.39552191\n",
      "Iteration 35, loss = 0.38944327\n",
      "Iteration 36, loss = 0.38353253\n",
      "Iteration 37, loss = 0.37778156\n",
      "Iteration 38, loss = 0.37217493\n",
      "Iteration 39, loss = 0.36670326\n",
      "Iteration 40, loss = 0.36135699\n",
      "Iteration 41, loss = 0.35612935\n",
      "Iteration 42, loss = 0.35101663\n",
      "Iteration 43, loss = 0.34601529\n",
      "Iteration 44, loss = 0.34112575\n",
      "Iteration 45, loss = 0.33634555\n",
      "Iteration 46, loss = 0.33166819\n",
      "Iteration 47, loss = 0.32709121\n",
      "Iteration 48, loss = 0.32261886\n",
      "Iteration 49, loss = 0.31824481\n",
      "Iteration 50, loss = 0.31396839\n",
      "Iteration 51, loss = 0.30978625\n",
      "Iteration 52, loss = 0.30570325\n",
      "Iteration 53, loss = 0.30171697\n",
      "Iteration 54, loss = 0.29781692\n",
      "Iteration 55, loss = 0.29400447\n",
      "Iteration 56, loss = 0.29027274\n",
      "Iteration 57, loss = 0.28662020\n",
      "Iteration 58, loss = 0.28304605\n",
      "Iteration 59, loss = 0.27954477\n",
      "Iteration 60, loss = 0.27611529\n",
      "Iteration 61, loss = 0.27275923\n",
      "Iteration 62, loss = 0.26947452\n",
      "Iteration 63, loss = 0.26626052\n",
      "Iteration 64, loss = 0.26311528\n",
      "Iteration 65, loss = 0.26003738\n",
      "Iteration 66, loss = 0.25702530\n",
      "Iteration 67, loss = 0.25407672\n",
      "Iteration 68, loss = 0.25118823\n",
      "Iteration 69, loss = 0.24835381\n",
      "Iteration 70, loss = 0.24557868\n",
      "Iteration 71, loss = 0.24286046\n",
      "Iteration 72, loss = 0.24019947\n",
      "Iteration 73, loss = 0.23758439\n",
      "Iteration 74, loss = 0.23499834\n",
      "Iteration 75, loss = 0.23243807\n",
      "Iteration 76, loss = 0.22988344\n",
      "Iteration 77, loss = 0.22727980\n",
      "Iteration 78, loss = 0.22461355\n",
      "Iteration 79, loss = 0.22196658\n",
      "Iteration 80, loss = 0.21939724\n",
      "Iteration 81, loss = 0.21704499\n",
      "Iteration 82, loss = 0.21483835\n",
      "Iteration 83, loss = 0.21271278\n",
      "Iteration 84, loss = 0.21066583\n",
      "Iteration 85, loss = 0.20868764\n",
      "Iteration 86, loss = 0.20675795\n",
      "Iteration 87, loss = 0.20487037\n",
      "Iteration 88, loss = 0.20302419\n",
      "Iteration 89, loss = 0.20121666\n",
      "Iteration 90, loss = 0.19944630\n",
      "Iteration 91, loss = 0.19771326\n",
      "Iteration 92, loss = 0.19601576\n",
      "Iteration 93, loss = 0.19435289\n",
      "Iteration 94, loss = 0.19272551\n",
      "Iteration 95, loss = 0.19113203\n",
      "Iteration 96, loss = 0.18957128\n",
      "Iteration 97, loss = 0.18804062\n",
      "Iteration 98, loss = 0.18654136\n",
      "Iteration 99, loss = 0.18507351\n",
      "Iteration 100, loss = 0.18364339\n",
      "Iteration 101, loss = 0.18225560\n",
      "Iteration 102, loss = 0.18090596\n",
      "Iteration 103, loss = 0.17958692\n",
      "Iteration 104, loss = 0.17829816\n",
      "Iteration 105, loss = 0.17703799\n",
      "Iteration 106, loss = 0.17580436\n",
      "Iteration 107, loss = 0.17459761\n",
      "Iteration 108, loss = 0.17341660\n",
      "Iteration 109, loss = 0.17225909\n",
      "Iteration 110, loss = 0.17112179\n",
      "Iteration 111, loss = 0.17000726\n",
      "Iteration 112, loss = 0.16891479\n",
      "Iteration 113, loss = 0.16784409\n",
      "Iteration 114, loss = 0.16679544\n",
      "Iteration 115, loss = 0.16576802\n",
      "Iteration 116, loss = 0.16476095\n",
      "Iteration 117, loss = 0.16377403\n",
      "Iteration 118, loss = 0.16280667\n",
      "Iteration 119, loss = 0.16185854\n",
      "Iteration 120, loss = 0.16092889\n",
      "Iteration 121, loss = 0.16001720\n",
      "Iteration 122, loss = 0.15912308\n",
      "Iteration 123, loss = 0.15824760\n",
      "Iteration 124, loss = 0.15739149\n",
      "Iteration 125, loss = 0.15655353\n",
      "Iteration 126, loss = 0.15573149\n",
      "Iteration 127, loss = 0.15492483\n",
      "Iteration 128, loss = 0.15413321\n",
      "Iteration 129, loss = 0.15335634\n",
      "Iteration 130, loss = 0.15259373\n",
      "Iteration 131, loss = 0.15184501\n",
      "Iteration 132, loss = 0.15111023\n",
      "Iteration 133, loss = 0.15039083\n",
      "Iteration 134, loss = 0.14968789\n",
      "Iteration 135, loss = 0.14900176\n",
      "Iteration 136, loss = 0.14832782\n",
      "Iteration 137, loss = 0.14766603\n",
      "Iteration 138, loss = 0.14701656\n",
      "Iteration 139, loss = 0.14637781\n",
      "Iteration 140, loss = 0.14574843\n",
      "Iteration 141, loss = 0.14512842\n",
      "Iteration 142, loss = 0.14451789\n",
      "Iteration 143, loss = 0.14391676\n",
      "Iteration 144, loss = 0.14332479\n",
      "Iteration 145, loss = 0.14274358\n",
      "Iteration 146, loss = 0.14217243\n",
      "Iteration 147, loss = 0.14161051\n",
      "Iteration 148, loss = 0.14105762\n",
      "Iteration 149, loss = 0.14051387\n",
      "Iteration 150, loss = 0.13997890\n",
      "Iteration 151, loss = 0.13945228\n",
      "Iteration 152, loss = 0.13893382\n",
      "Iteration 153, loss = 0.13842349\n",
      "Iteration 154, loss = 0.13792105\n",
      "Iteration 155, loss = 0.13742630\n",
      "Iteration 156, loss = 0.13693828\n",
      "Iteration 157, loss = 0.13645708\n",
      "Iteration 158, loss = 0.13598307\n",
      "Iteration 159, loss = 0.13551576\n",
      "Iteration 160, loss = 0.13505513\n",
      "Iteration 161, loss = 0.13460129\n",
      "Iteration 162, loss = 0.13415397\n",
      "Iteration 163, loss = 0.13371318\n",
      "Iteration 164, loss = 0.13327869\n",
      "Iteration 165, loss = 0.13285056\n",
      "Iteration 166, loss = 0.13242808\n",
      "Iteration 167, loss = 0.13201146\n",
      "Iteration 168, loss = 0.13160097\n",
      "Iteration 169, loss = 0.13119643\n",
      "Iteration 170, loss = 0.13079841\n",
      "Iteration 171, loss = 0.13040621\n",
      "Iteration 172, loss = 0.13001979\n",
      "Iteration 173, loss = 0.12963945\n",
      "Iteration 174, loss = 0.12926441\n",
      "Iteration 175, loss = 0.12889522\n",
      "Iteration 176, loss = 0.12853078\n",
      "Iteration 177, loss = 0.12817104\n",
      "Iteration 178, loss = 0.12781592\n",
      "Iteration 179, loss = 0.12746540\n",
      "Iteration 180, loss = 0.12711924\n",
      "Iteration 181, loss = 0.12677740\n",
      "Iteration 182, loss = 0.12643987\n",
      "Iteration 183, loss = 0.12610663\n",
      "Iteration 184, loss = 0.12577763\n",
      "Iteration 185, loss = 0.12545276\n",
      "Iteration 186, loss = 0.12513197\n",
      "Iteration 187, loss = 0.12481562\n",
      "Iteration 188, loss = 0.12450327\n",
      "Iteration 189, loss = 0.12419491\n",
      "Iteration 190, loss = 0.12389039\n",
      "Iteration 191, loss = 0.12358964\n",
      "Iteration 192, loss = 0.12329253\n",
      "Iteration 193, loss = 0.12299904\n",
      "Iteration 194, loss = 0.12270911\n",
      "Iteration 195, loss = 0.12242258\n",
      "Iteration 196, loss = 0.12213947\n",
      "Iteration 197, loss = 0.12185973\n",
      "Iteration 198, loss = 0.12158326\n",
      "Iteration 199, loss = 0.12131006\n",
      "Iteration 200, loss = 0.12104007\n",
      "Iteration 201, loss = 0.12077323\n",
      "Iteration 202, loss = 0.12050946\n",
      "Iteration 203, loss = 0.12024872\n",
      "Iteration 204, loss = 0.11999096\n",
      "Iteration 205, loss = 0.11973620\n",
      "Iteration 206, loss = 0.11948431\n",
      "Iteration 207, loss = 0.11923527\n",
      "Iteration 208, loss = 0.11898906\n",
      "Iteration 209, loss = 0.11874561\n",
      "Iteration 210, loss = 0.11850486\n",
      "Iteration 211, loss = 0.11826681\n",
      "Iteration 212, loss = 0.11803134\n",
      "Iteration 213, loss = 0.11779843\n",
      "Iteration 214, loss = 0.11756802\n",
      "Iteration 215, loss = 0.11734010\n",
      "Iteration 216, loss = 0.11711460\n",
      "Iteration 217, loss = 0.11689151\n",
      "Iteration 218, loss = 0.11667077\n",
      "Iteration 219, loss = 0.11645232\n",
      "Iteration 220, loss = 0.11623618\n",
      "Iteration 221, loss = 0.11602228\n",
      "Iteration 222, loss = 0.11581061\n",
      "Iteration 223, loss = 0.11560114\n",
      "Iteration 224, loss = 0.11539381\n",
      "Iteration 225, loss = 0.11518857\n",
      "Iteration 226, loss = 0.11498539\n",
      "Iteration 227, loss = 0.11478427\n",
      "Iteration 228, loss = 0.11458519\n",
      "Iteration 229, loss = 0.11438813\n",
      "Iteration 230, loss = 0.11419298\n",
      "Iteration 231, loss = 0.11399964\n",
      "Iteration 232, loss = 0.11380823\n",
      "Iteration 233, loss = 0.11361868\n",
      "Iteration 234, loss = 0.11343077\n",
      "Iteration 235, loss = 0.11324464\n",
      "Iteration 236, loss = 0.11306031\n",
      "Iteration 237, loss = 0.11287775\n",
      "Iteration 238, loss = 0.11269684\n",
      "Iteration 239, loss = 0.11251765\n",
      "Iteration 240, loss = 0.11233999\n",
      "Iteration 241, loss = 0.11216393\n",
      "Iteration 242, loss = 0.11198930\n",
      "Iteration 243, loss = 0.11181601\n",
      "Iteration 244, loss = 0.11164408\n",
      "Iteration 245, loss = 0.11147363\n",
      "Iteration 246, loss = 0.11130460\n",
      "Iteration 247, loss = 0.11113653\n",
      "Iteration 248, loss = 0.11096906\n",
      "Iteration 249, loss = 0.11080171\n",
      "Iteration 250, loss = 0.11063544\n",
      "Iteration 251, loss = 0.11047020\n",
      "Iteration 252, loss = 0.11030548\n",
      "Iteration 253, loss = 0.11014036\n",
      "Iteration 254, loss = 0.10997555\n",
      "Iteration 255, loss = 0.10980843\n",
      "Iteration 256, loss = 0.10963720\n",
      "Iteration 257, loss = 0.10946010\n",
      "Iteration 258, loss = 0.10928064\n",
      "Iteration 259, loss = 0.10911129\n",
      "Iteration 260, loss = 0.10895085\n",
      "Iteration 261, loss = 0.10879305\n",
      "Iteration 262, loss = 0.10863696\n",
      "Iteration 263, loss = 0.10848757\n",
      "Iteration 264, loss = 0.10834443\n",
      "Iteration 265, loss = 0.10820366\n",
      "Iteration 266, loss = 0.10806702\n",
      "Iteration 267, loss = 0.10793010\n",
      "Iteration 268, loss = 0.10779117\n",
      "Iteration 269, loss = 0.10765461\n",
      "Iteration 270, loss = 0.10752001\n",
      "Iteration 271, loss = 0.10738677\n",
      "Iteration 272, loss = 0.10725488\n",
      "Iteration 273, loss = 0.10712428\n",
      "Iteration 274, loss = 0.10699496\n",
      "Iteration 275, loss = 0.10686700\n",
      "Iteration 276, loss = 0.10674023\n",
      "Iteration 277, loss = 0.10661458\n",
      "Iteration 278, loss = 0.10648994\n",
      "Iteration 279, loss = 0.10636653\n",
      "Iteration 280, loss = 0.10624406\n",
      "Iteration 281, loss = 0.10612271\n",
      "Iteration 282, loss = 0.10600236\n",
      "Iteration 283, loss = 0.10588308\n",
      "Iteration 284, loss = 0.10576466\n",
      "Iteration 285, loss = 0.10564726\n",
      "Iteration 286, loss = 0.10553074\n",
      "Iteration 287, loss = 0.10541519\n",
      "Iteration 288, loss = 0.10530051\n",
      "Iteration 289, loss = 0.10518679\n",
      "Iteration 290, loss = 0.10507390\n",
      "Iteration 291, loss = 0.10496196\n",
      "Iteration 292, loss = 0.10485081\n",
      "Iteration 293, loss = 0.10474059\n",
      "Iteration 294, loss = 0.10463115\n",
      "Iteration 295, loss = 0.10452260\n",
      "Iteration 296, loss = 0.10441486\n",
      "Iteration 297, loss = 0.10430799\n",
      "Iteration 298, loss = 0.10420191\n",
      "Iteration 299, loss = 0.10409661\n",
      "Iteration 300, loss = 0.10399213\n",
      "Iteration 301, loss = 0.10388837\n",
      "Iteration 302, loss = 0.10378546\n",
      "Iteration 303, loss = 0.10368322\n",
      "Iteration 304, loss = 0.10358181\n",
      "Iteration 305, loss = 0.10348108\n",
      "Iteration 306, loss = 0.10338112\n",
      "Iteration 307, loss = 0.10328184\n",
      "Iteration 308, loss = 0.10318336\n",
      "Iteration 309, loss = 0.10308550\n",
      "Iteration 310, loss = 0.10298841\n",
      "Iteration 311, loss = 0.10289196\n",
      "Iteration 312, loss = 0.10279625\n",
      "Iteration 313, loss = 0.10270116\n",
      "Iteration 314, loss = 0.10260681\n",
      "Iteration 315, loss = 0.10251306\n",
      "Iteration 316, loss = 0.10242004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66585275\n",
      "Iteration 2, loss = 1.36112728\n",
      "Iteration 3, loss = 1.21108036\n",
      "Iteration 4, loss = 1.17313393\n",
      "Iteration 5, loss = 1.08500463\n",
      "Iteration 6, loss = 0.97688363\n",
      "Iteration 7, loss = 0.89692474\n",
      "Iteration 8, loss = 0.84484180\n",
      "Iteration 9, loss = 0.79932624\n",
      "Iteration 10, loss = 0.75600817\n",
      "Iteration 11, loss = 0.71521870\n",
      "Iteration 12, loss = 0.67755475\n",
      "Iteration 13, loss = 0.64420701\n",
      "Iteration 14, loss = 0.61651205\n",
      "Iteration 15, loss = 0.59179412\n",
      "Iteration 16, loss = 0.56976566\n",
      "Iteration 17, loss = 0.54997590\n",
      "Iteration 18, loss = 0.53213137\n",
      "Iteration 19, loss = 0.51595548\n",
      "Iteration 20, loss = 0.50135848\n",
      "Iteration 21, loss = 0.48807440\n",
      "Iteration 22, loss = 0.47578563\n",
      "Iteration 23, loss = 0.46446747\n",
      "Iteration 24, loss = 0.45409718\n",
      "Iteration 25, loss = 0.44449595\n",
      "Iteration 26, loss = 0.43543802\n",
      "Iteration 27, loss = 0.42685138\n",
      "Iteration 28, loss = 0.41867417\n",
      "Iteration 29, loss = 0.41084550\n",
      "Iteration 30, loss = 0.40332208\n",
      "Iteration 31, loss = 0.39608342\n",
      "Iteration 32, loss = 0.38908885\n",
      "Iteration 33, loss = 0.38231743\n",
      "Iteration 34, loss = 0.37575290\n",
      "Iteration 35, loss = 0.36937643\n",
      "Iteration 36, loss = 0.36317316\n",
      "Iteration 37, loss = 0.35713763\n",
      "Iteration 38, loss = 0.35125185\n",
      "Iteration 39, loss = 0.34551044\n",
      "Iteration 40, loss = 0.33991496\n",
      "Iteration 41, loss = 0.33445750\n",
      "Iteration 42, loss = 0.32912522\n",
      "Iteration 43, loss = 0.32392186\n",
      "Iteration 44, loss = 0.31885013\n",
      "Iteration 45, loss = 0.31389718\n",
      "Iteration 46, loss = 0.30906201\n",
      "Iteration 47, loss = 0.30433637\n",
      "Iteration 48, loss = 0.29971722\n",
      "Iteration 49, loss = 0.29519990\n",
      "Iteration 50, loss = 0.29078819\n",
      "Iteration 51, loss = 0.28648033\n",
      "Iteration 52, loss = 0.28227437\n",
      "Iteration 53, loss = 0.27816501\n",
      "Iteration 54, loss = 0.27415322\n",
      "Iteration 55, loss = 0.27023409\n",
      "Iteration 56, loss = 0.26640751\n",
      "Iteration 57, loss = 0.26266889\n",
      "Iteration 58, loss = 0.25901503\n",
      "Iteration 59, loss = 0.25544508\n",
      "Iteration 60, loss = 0.25195744\n",
      "Iteration 61, loss = 0.24854966\n",
      "Iteration 62, loss = 0.24521959\n",
      "Iteration 63, loss = 0.24196626\n",
      "Iteration 64, loss = 0.23878751\n",
      "Iteration 65, loss = 0.23568109\n",
      "Iteration 66, loss = 0.23264437\n",
      "Iteration 67, loss = 0.22967629\n",
      "Iteration 68, loss = 0.22677630\n",
      "Iteration 69, loss = 0.22394350\n",
      "Iteration 70, loss = 0.22117530\n",
      "Iteration 71, loss = 0.21847269\n",
      "Iteration 72, loss = 0.21583094\n",
      "Iteration 73, loss = 0.21325074\n",
      "Iteration 74, loss = 0.21073123\n",
      "Iteration 75, loss = 0.20826850\n",
      "Iteration 76, loss = 0.20586315\n",
      "Iteration 77, loss = 0.20351284\n",
      "Iteration 78, loss = 0.20121287\n",
      "Iteration 79, loss = 0.19896482\n",
      "Iteration 80, loss = 0.19676743\n",
      "Iteration 81, loss = 0.19461698\n",
      "Iteration 82, loss = 0.19251249\n",
      "Iteration 83, loss = 0.19045565\n",
      "Iteration 84, loss = 0.18844511\n",
      "Iteration 85, loss = 0.18647793\n",
      "Iteration 86, loss = 0.18455472\n",
      "Iteration 87, loss = 0.18267507\n",
      "Iteration 88, loss = 0.18083476\n",
      "Iteration 89, loss = 0.17903373\n",
      "Iteration 90, loss = 0.17727112\n",
      "Iteration 91, loss = 0.17554717\n",
      "Iteration 92, loss = 0.17385963\n",
      "Iteration 93, loss = 0.17220792\n",
      "Iteration 94, loss = 0.17059131\n",
      "Iteration 95, loss = 0.16900975\n",
      "Iteration 96, loss = 0.16746183\n",
      "Iteration 97, loss = 0.16594765\n",
      "Iteration 98, loss = 0.16446894\n",
      "Iteration 99, loss = 0.16302373\n",
      "Iteration 100, loss = 0.16160870\n",
      "Iteration 101, loss = 0.16022423\n",
      "Iteration 102, loss = 0.15887158\n",
      "Iteration 103, loss = 0.15755592\n",
      "Iteration 104, loss = 0.15627655\n",
      "Iteration 105, loss = 0.15502973\n",
      "Iteration 106, loss = 0.15381240\n",
      "Iteration 107, loss = 0.15262053\n",
      "Iteration 108, loss = 0.15145412\n",
      "Iteration 109, loss = 0.15031752\n",
      "Iteration 110, loss = 0.14919783\n",
      "Iteration 111, loss = 0.14809714\n",
      "Iteration 112, loss = 0.14701957\n",
      "Iteration 113, loss = 0.14596656\n",
      "Iteration 114, loss = 0.14493633\n",
      "Iteration 115, loss = 0.14392739\n",
      "Iteration 116, loss = 0.14293901\n",
      "Iteration 117, loss = 0.14197045\n",
      "Iteration 118, loss = 0.14102145\n",
      "Iteration 119, loss = 0.14009165\n",
      "Iteration 120, loss = 0.13918048\n",
      "Iteration 121, loss = 0.13828742\n",
      "Iteration 122, loss = 0.13741191\n",
      "Iteration 123, loss = 0.13655349\n",
      "Iteration 124, loss = 0.13571197\n",
      "Iteration 125, loss = 0.13488689\n",
      "Iteration 126, loss = 0.13407761\n",
      "Iteration 127, loss = 0.13328373\n",
      "Iteration 128, loss = 0.13250583\n",
      "Iteration 129, loss = 0.13174232\n",
      "Iteration 130, loss = 0.13099445\n",
      "Iteration 131, loss = 0.13025951\n",
      "Iteration 132, loss = 0.12953776\n",
      "Iteration 133, loss = 0.12882896\n",
      "Iteration 134, loss = 0.12813280\n",
      "Iteration 135, loss = 0.12745063\n",
      "Iteration 136, loss = 0.12678121\n",
      "Iteration 137, loss = 0.12612375\n",
      "Iteration 138, loss = 0.12547797\n",
      "Iteration 139, loss = 0.12484422\n",
      "Iteration 140, loss = 0.12422209\n",
      "Iteration 141, loss = 0.12361111\n",
      "Iteration 142, loss = 0.12300996\n",
      "Iteration 143, loss = 0.12241977\n",
      "Iteration 144, loss = 0.12183871\n",
      "Iteration 145, loss = 0.12126206\n",
      "Iteration 146, loss = 0.12068885\n",
      "Iteration 147, loss = 0.12011907\n",
      "Iteration 148, loss = 0.11955526\n",
      "Iteration 149, loss = 0.11898974\n",
      "Iteration 150, loss = 0.11842359\n",
      "Iteration 151, loss = 0.11785464\n",
      "Iteration 152, loss = 0.11726980\n",
      "Iteration 153, loss = 0.11669132\n",
      "Iteration 154, loss = 0.11612860\n",
      "Iteration 155, loss = 0.11558233\n",
      "Iteration 156, loss = 0.11506637\n",
      "Iteration 157, loss = 0.11458480\n",
      "Iteration 158, loss = 0.11411395\n",
      "Iteration 159, loss = 0.11365046\n",
      "Iteration 160, loss = 0.11320243\n",
      "Iteration 161, loss = 0.11276439\n",
      "Iteration 162, loss = 0.11233375\n",
      "Iteration 163, loss = 0.11190596\n",
      "Iteration 164, loss = 0.11148853\n",
      "Iteration 165, loss = 0.11107802\n",
      "Iteration 166, loss = 0.11067379\n",
      "Iteration 167, loss = 0.11027725\n",
      "Iteration 168, loss = 0.10988841\n",
      "Iteration 169, loss = 0.10950593\n",
      "Iteration 170, loss = 0.10912894\n",
      "Iteration 171, loss = 0.10875757\n",
      "Iteration 172, loss = 0.10839144\n",
      "Iteration 173, loss = 0.10803057\n",
      "Iteration 174, loss = 0.10767474\n",
      "Iteration 175, loss = 0.10732375\n",
      "Iteration 176, loss = 0.10697757\n",
      "Iteration 177, loss = 0.10663599\n",
      "Iteration 178, loss = 0.10629896\n",
      "Iteration 179, loss = 0.10596645\n",
      "Iteration 180, loss = 0.10563827\n",
      "Iteration 181, loss = 0.10531437\n",
      "Iteration 182, loss = 0.10499471\n",
      "Iteration 183, loss = 0.10467917\n",
      "Iteration 184, loss = 0.10436768\n",
      "Iteration 185, loss = 0.10406015\n",
      "Iteration 186, loss = 0.10375658\n",
      "Iteration 187, loss = 0.10345679\n",
      "Iteration 188, loss = 0.10316074\n",
      "Iteration 189, loss = 0.10286847\n",
      "Iteration 190, loss = 0.10257967\n",
      "Iteration 191, loss = 0.10229448\n",
      "Iteration 192, loss = 0.10201264\n",
      "Iteration 193, loss = 0.10173354\n",
      "Iteration 194, loss = 0.10145767\n",
      "Iteration 195, loss = 0.10118507\n",
      "Iteration 196, loss = 0.10091553\n",
      "Iteration 197, loss = 0.10064911\n",
      "Iteration 198, loss = 0.10038583\n",
      "Iteration 199, loss = 0.10012545\n",
      "Iteration 200, loss = 0.09986808\n",
      "Iteration 201, loss = 0.09961382\n",
      "Iteration 202, loss = 0.09936237\n",
      "Iteration 203, loss = 0.09911376\n",
      "Iteration 204, loss = 0.09886798\n",
      "Iteration 205, loss = 0.09862495\n",
      "Iteration 206, loss = 0.09838454\n",
      "Iteration 207, loss = 0.09814672\n",
      "Iteration 208, loss = 0.09791170\n",
      "Iteration 209, loss = 0.09767941\n",
      "Iteration 210, loss = 0.09745004\n",
      "Iteration 211, loss = 0.09722335\n",
      "Iteration 212, loss = 0.09699924\n",
      "Iteration 213, loss = 0.09677763\n",
      "Iteration 214, loss = 0.09655857\n",
      "Iteration 215, loss = 0.09634230\n",
      "Iteration 216, loss = 0.09612833\n",
      "Iteration 217, loss = 0.09591626\n",
      "Iteration 218, loss = 0.09570649\n",
      "Iteration 219, loss = 0.09549909\n",
      "Iteration 220, loss = 0.09529381\n",
      "Iteration 221, loss = 0.09509064\n",
      "Iteration 222, loss = 0.09488962\n",
      "Iteration 223, loss = 0.09469070\n",
      "Iteration 224, loss = 0.09449389\n",
      "Iteration 225, loss = 0.09429942\n",
      "Iteration 226, loss = 0.09410703\n",
      "Iteration 227, loss = 0.09391651\n",
      "Iteration 228, loss = 0.09372777\n",
      "Iteration 229, loss = 0.09354093\n",
      "Iteration 230, loss = 0.09335591\n",
      "Iteration 231, loss = 0.09317270\n",
      "Iteration 232, loss = 0.09299128\n",
      "Iteration 233, loss = 0.09281163\n",
      "Iteration 234, loss = 0.09263376\n",
      "Iteration 235, loss = 0.09245757\n",
      "Iteration 236, loss = 0.09228304\n",
      "Iteration 237, loss = 0.09211018\n",
      "Iteration 238, loss = 0.09193898\n",
      "Iteration 239, loss = 0.09176931\n",
      "Iteration 240, loss = 0.09160130\n",
      "Iteration 241, loss = 0.09143496\n",
      "Iteration 242, loss = 0.09126955\n",
      "Iteration 243, loss = 0.09110555\n",
      "Iteration 244, loss = 0.09094301\n",
      "Iteration 245, loss = 0.09078204\n",
      "Iteration 246, loss = 0.09062252\n",
      "Iteration 247, loss = 0.09046437\n",
      "Iteration 248, loss = 0.09030760\n",
      "Iteration 249, loss = 0.09015212\n",
      "Iteration 250, loss = 0.08999648\n",
      "Iteration 251, loss = 0.08984238\n",
      "Iteration 252, loss = 0.08968940\n",
      "Iteration 253, loss = 0.08953750\n",
      "Iteration 254, loss = 0.08938762\n",
      "Iteration 255, loss = 0.08923901\n",
      "Iteration 256, loss = 0.08909167\n",
      "Iteration 257, loss = 0.08894568\n",
      "Iteration 258, loss = 0.08880137\n",
      "Iteration 259, loss = 0.08865767\n",
      "Iteration 260, loss = 0.08851439\n",
      "Iteration 261, loss = 0.08837257\n",
      "Iteration 262, loss = 0.08823127\n",
      "Iteration 263, loss = 0.08809094\n",
      "Iteration 264, loss = 0.08795216\n",
      "Iteration 265, loss = 0.08781752\n",
      "Iteration 266, loss = 0.08768508\n",
      "Iteration 267, loss = 0.08755365\n",
      "Iteration 268, loss = 0.08742322\n",
      "Iteration 269, loss = 0.08729383\n",
      "Iteration 270, loss = 0.08716544\n",
      "Iteration 271, loss = 0.08703808\n",
      "Iteration 272, loss = 0.08691174\n",
      "Iteration 273, loss = 0.08678643\n",
      "Iteration 274, loss = 0.08666214\n",
      "Iteration 275, loss = 0.08653886\n",
      "Iteration 276, loss = 0.08641657\n",
      "Iteration 277, loss = 0.08629524\n",
      "Iteration 278, loss = 0.08617491\n",
      "Iteration 279, loss = 0.08605558\n",
      "Iteration 280, loss = 0.08593713\n",
      "Iteration 281, loss = 0.08581968\n",
      "Iteration 282, loss = 0.08570312\n",
      "Iteration 283, loss = 0.08558751\n",
      "Iteration 284, loss = 0.08547280\n",
      "Iteration 285, loss = 0.08535896\n",
      "Iteration 286, loss = 0.08524607\n",
      "Iteration 287, loss = 0.08513399\n",
      "Iteration 288, loss = 0.08502283\n",
      "Iteration 289, loss = 0.08491250\n",
      "Iteration 290, loss = 0.08480302\n",
      "Iteration 291, loss = 0.08469443\n",
      "Iteration 292, loss = 0.08458662\n",
      "Iteration 293, loss = 0.08447965\n",
      "Iteration 294, loss = 0.08437349\n",
      "Iteration 295, loss = 0.08426814\n",
      "Iteration 296, loss = 0.08416359\n",
      "Iteration 297, loss = 0.08405980\n",
      "Iteration 298, loss = 0.08395683\n",
      "Iteration 299, loss = 0.08385457\n",
      "Iteration 300, loss = 0.08375312\n",
      "Iteration 301, loss = 0.08365238\n",
      "Iteration 302, loss = 0.08355240\n",
      "Iteration 303, loss = 0.08345317\n",
      "Iteration 304, loss = 0.08335463\n",
      "Iteration 305, loss = 0.08325682\n",
      "Iteration 306, loss = 0.08315973\n",
      "Iteration 307, loss = 0.08306332\n",
      "Iteration 308, loss = 0.08296763\n",
      "Iteration 309, loss = 0.08287259\n",
      "Iteration 310, loss = 0.08277827\n",
      "Iteration 311, loss = 0.08268460\n",
      "Iteration 312, loss = 0.08259159\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65830114\n",
      "Iteration 2, loss = 1.35751008\n",
      "Iteration 3, loss = 1.20865399\n",
      "Iteration 4, loss = 1.17084382\n",
      "Iteration 5, loss = 1.08516738\n",
      "Iteration 6, loss = 0.97862066\n",
      "Iteration 7, loss = 0.90065408\n",
      "Iteration 8, loss = 0.84984695\n",
      "Iteration 9, loss = 0.80518887\n",
      "Iteration 10, loss = 0.76325585\n",
      "Iteration 11, loss = 0.72412730\n",
      "Iteration 12, loss = 0.68719622\n",
      "Iteration 13, loss = 0.65527763\n",
      "Iteration 14, loss = 0.62731432\n",
      "Iteration 15, loss = 0.60237800\n",
      "Iteration 16, loss = 0.58004948\n",
      "Iteration 17, loss = 0.55991978\n",
      "Iteration 18, loss = 0.54179116\n",
      "Iteration 19, loss = 0.52548575\n",
      "Iteration 20, loss = 0.51065356\n",
      "Iteration 21, loss = 0.49701799\n",
      "Iteration 22, loss = 0.48435954\n",
      "Iteration 23, loss = 0.47270135\n",
      "Iteration 24, loss = 0.46201479\n",
      "Iteration 25, loss = 0.45214794\n",
      "Iteration 26, loss = 0.44285924\n",
      "Iteration 27, loss = 0.43404460\n",
      "Iteration 28, loss = 0.42566189\n",
      "Iteration 29, loss = 0.41765051\n",
      "Iteration 30, loss = 0.40996240\n",
      "Iteration 31, loss = 0.40256331\n",
      "Iteration 32, loss = 0.39541628\n",
      "Iteration 33, loss = 0.38850690\n",
      "Iteration 34, loss = 0.38180181\n",
      "Iteration 35, loss = 0.37528940\n",
      "Iteration 36, loss = 0.36896140\n",
      "Iteration 37, loss = 0.36280403\n",
      "Iteration 38, loss = 0.35680484\n",
      "Iteration 39, loss = 0.35095543\n",
      "Iteration 40, loss = 0.34525012\n",
      "Iteration 41, loss = 0.33968204\n",
      "Iteration 42, loss = 0.33425518\n",
      "Iteration 43, loss = 0.32896767\n",
      "Iteration 44, loss = 0.32381140\n",
      "Iteration 45, loss = 0.31877186\n",
      "Iteration 46, loss = 0.31384591\n",
      "Iteration 47, loss = 0.30903005\n",
      "Iteration 48, loss = 0.30432867\n",
      "Iteration 49, loss = 0.29973598\n",
      "Iteration 50, loss = 0.29524817\n",
      "Iteration 51, loss = 0.29086336\n",
      "Iteration 52, loss = 0.28657976\n",
      "Iteration 53, loss = 0.28239452\n",
      "Iteration 54, loss = 0.27830457\n",
      "Iteration 55, loss = 0.27430907\n",
      "Iteration 56, loss = 0.27040476\n",
      "Iteration 57, loss = 0.26658726\n",
      "Iteration 58, loss = 0.26285585\n",
      "Iteration 59, loss = 0.25920858\n",
      "Iteration 60, loss = 0.25564229\n",
      "Iteration 61, loss = 0.25215288\n",
      "Iteration 62, loss = 0.24873999\n",
      "Iteration 63, loss = 0.24540384\n",
      "Iteration 64, loss = 0.24214327\n",
      "Iteration 65, loss = 0.23895460\n",
      "Iteration 66, loss = 0.23583668\n",
      "Iteration 67, loss = 0.23278861\n",
      "Iteration 68, loss = 0.22980788\n",
      "Iteration 69, loss = 0.22689205\n",
      "Iteration 70, loss = 0.22404093\n",
      "Iteration 71, loss = 0.22125298\n",
      "Iteration 72, loss = 0.21852518\n",
      "Iteration 73, loss = 0.21585733\n",
      "Iteration 74, loss = 0.21324527\n",
      "Iteration 75, loss = 0.21068694\n",
      "Iteration 76, loss = 0.20818294\n",
      "Iteration 77, loss = 0.20573305\n",
      "Iteration 78, loss = 0.20333433\n",
      "Iteration 79, loss = 0.20098728\n",
      "Iteration 80, loss = 0.19868976\n",
      "Iteration 81, loss = 0.19643860\n",
      "Iteration 82, loss = 0.19423539\n",
      "Iteration 83, loss = 0.19208061\n",
      "Iteration 84, loss = 0.18997205\n",
      "Iteration 85, loss = 0.18790746\n",
      "Iteration 86, loss = 0.18588597\n",
      "Iteration 87, loss = 0.18390583\n",
      "Iteration 88, loss = 0.18196570\n",
      "Iteration 89, loss = 0.18006795\n",
      "Iteration 90, loss = 0.17821501\n",
      "Iteration 91, loss = 0.17640132\n",
      "Iteration 92, loss = 0.17462371\n",
      "Iteration 93, loss = 0.17288301\n",
      "Iteration 94, loss = 0.17118721\n",
      "Iteration 95, loss = 0.16953185\n",
      "Iteration 96, loss = 0.16791010\n",
      "Iteration 97, loss = 0.16632682\n",
      "Iteration 98, loss = 0.16478108\n",
      "Iteration 99, loss = 0.16326965\n",
      "Iteration 100, loss = 0.16178474\n",
      "Iteration 101, loss = 0.16032819\n",
      "Iteration 102, loss = 0.15889547\n",
      "Iteration 103, loss = 0.15748010\n",
      "Iteration 104, loss = 0.15607360\n",
      "Iteration 105, loss = 0.15467622\n",
      "Iteration 106, loss = 0.15327065\n",
      "Iteration 107, loss = 0.15185380\n",
      "Iteration 108, loss = 0.15048441\n",
      "Iteration 109, loss = 0.14915192\n",
      "Iteration 110, loss = 0.14787252\n",
      "Iteration 111, loss = 0.14666496\n",
      "Iteration 112, loss = 0.14551041\n",
      "Iteration 113, loss = 0.14439368\n",
      "Iteration 114, loss = 0.14330341\n",
      "Iteration 115, loss = 0.14223776\n",
      "Iteration 116, loss = 0.14119786\n",
      "Iteration 117, loss = 0.14018080\n",
      "Iteration 118, loss = 0.13918398\n",
      "Iteration 119, loss = 0.13820627\n",
      "Iteration 120, loss = 0.13724902\n",
      "Iteration 121, loss = 0.13631074\n",
      "Iteration 122, loss = 0.13539031\n",
      "Iteration 123, loss = 0.13448837\n",
      "Iteration 124, loss = 0.13360470\n",
      "Iteration 125, loss = 0.13273870\n",
      "Iteration 126, loss = 0.13189292\n",
      "Iteration 127, loss = 0.13106360\n",
      "Iteration 128, loss = 0.13024944\n",
      "Iteration 129, loss = 0.12945012\n",
      "Iteration 130, loss = 0.12866708\n",
      "Iteration 131, loss = 0.12789345\n",
      "Iteration 132, loss = 0.12713354\n",
      "Iteration 133, loss = 0.12638667\n",
      "Iteration 134, loss = 0.12565196\n",
      "Iteration 135, loss = 0.12492908\n",
      "Iteration 136, loss = 0.12421810\n",
      "Iteration 137, loss = 0.12351900\n",
      "Iteration 138, loss = 0.12283183\n",
      "Iteration 139, loss = 0.12215594\n",
      "Iteration 140, loss = 0.12149133\n",
      "Iteration 141, loss = 0.12083750\n",
      "Iteration 142, loss = 0.12019411\n",
      "Iteration 143, loss = 0.11956104\n",
      "Iteration 144, loss = 0.11893807\n",
      "Iteration 145, loss = 0.11832490\n",
      "Iteration 146, loss = 0.11772130\n",
      "Iteration 147, loss = 0.11712628\n",
      "Iteration 148, loss = 0.11653938\n",
      "Iteration 149, loss = 0.11596122\n",
      "Iteration 150, loss = 0.11539180\n",
      "Iteration 151, loss = 0.11483112\n",
      "Iteration 152, loss = 0.11427880\n",
      "Iteration 153, loss = 0.11373471\n",
      "Iteration 154, loss = 0.11319857\n",
      "Iteration 155, loss = 0.11267026\n",
      "Iteration 156, loss = 0.11214955\n",
      "Iteration 157, loss = 0.11163594\n",
      "Iteration 158, loss = 0.11112932\n",
      "Iteration 159, loss = 0.11063035\n",
      "Iteration 160, loss = 0.11013929\n",
      "Iteration 161, loss = 0.10965548\n",
      "Iteration 162, loss = 0.10917915\n",
      "Iteration 163, loss = 0.10870975\n",
      "Iteration 164, loss = 0.10824693\n",
      "Iteration 165, loss = 0.10779185\n",
      "Iteration 166, loss = 0.10734290\n",
      "Iteration 167, loss = 0.10689929\n",
      "Iteration 168, loss = 0.10646149\n",
      "Iteration 169, loss = 0.10602941\n",
      "Iteration 170, loss = 0.10560296\n",
      "Iteration 171, loss = 0.10518202\n",
      "Iteration 172, loss = 0.10476657\n",
      "Iteration 173, loss = 0.10435628\n",
      "Iteration 174, loss = 0.10395045\n",
      "Iteration 175, loss = 0.10355014\n",
      "Iteration 176, loss = 0.10315561\n",
      "Iteration 177, loss = 0.10276640\n",
      "Iteration 178, loss = 0.10238197\n",
      "Iteration 179, loss = 0.10200231\n",
      "Iteration 180, loss = 0.10162737\n",
      "Iteration 181, loss = 0.10125704\n",
      "Iteration 182, loss = 0.10089121\n",
      "Iteration 183, loss = 0.10052984\n",
      "Iteration 184, loss = 0.10017278\n",
      "Iteration 185, loss = 0.09982001\n",
      "Iteration 186, loss = 0.09947144\n",
      "Iteration 187, loss = 0.09912699\n",
      "Iteration 188, loss = 0.09878675\n",
      "Iteration 189, loss = 0.09845066\n",
      "Iteration 190, loss = 0.09811844\n",
      "Iteration 191, loss = 0.09779005\n",
      "Iteration 192, loss = 0.09746543\n",
      "Iteration 193, loss = 0.09714453\n",
      "Iteration 194, loss = 0.09682733\n",
      "Iteration 195, loss = 0.09651390\n",
      "Iteration 196, loss = 0.09620390\n",
      "Iteration 197, loss = 0.09589736\n",
      "Iteration 198, loss = 0.09559433\n",
      "Iteration 199, loss = 0.09529465\n",
      "Iteration 200, loss = 0.09499834\n",
      "Iteration 201, loss = 0.09470538\n",
      "Iteration 202, loss = 0.09441559\n",
      "Iteration 203, loss = 0.09412899\n",
      "Iteration 204, loss = 0.09384554\n",
      "Iteration 205, loss = 0.09356511\n",
      "Iteration 206, loss = 0.09328770\n",
      "Iteration 207, loss = 0.09301330\n",
      "Iteration 208, loss = 0.09274177\n",
      "Iteration 209, loss = 0.09247317\n",
      "Iteration 210, loss = 0.09220737\n",
      "Iteration 211, loss = 0.09194437\n",
      "Iteration 212, loss = 0.09168414\n",
      "Iteration 213, loss = 0.09142657\n",
      "Iteration 214, loss = 0.09117174\n",
      "Iteration 215, loss = 0.09091946\n",
      "Iteration 216, loss = 0.09066982\n",
      "Iteration 217, loss = 0.09042269\n",
      "Iteration 218, loss = 0.09017804\n",
      "Iteration 219, loss = 0.08993591\n",
      "Iteration 220, loss = 0.08969614\n",
      "Iteration 221, loss = 0.08945884\n",
      "Iteration 222, loss = 0.08922387\n",
      "Iteration 223, loss = 0.08899128\n",
      "Iteration 224, loss = 0.08876101\n",
      "Iteration 225, loss = 0.08853301\n",
      "Iteration 226, loss = 0.08830724\n",
      "Iteration 227, loss = 0.08808371\n",
      "Iteration 228, loss = 0.08786227\n",
      "Iteration 229, loss = 0.08764300\n",
      "Iteration 230, loss = 0.08742582\n",
      "Iteration 231, loss = 0.08721070\n",
      "Iteration 232, loss = 0.08699762\n",
      "Iteration 233, loss = 0.08678657\n",
      "Iteration 234, loss = 0.08657753\n",
      "Iteration 235, loss = 0.08637042\n",
      "Iteration 236, loss = 0.08616524\n",
      "Iteration 237, loss = 0.08596202\n",
      "Iteration 238, loss = 0.08576067\n",
      "Iteration 239, loss = 0.08556117\n",
      "Iteration 240, loss = 0.08536351\n",
      "Iteration 241, loss = 0.08516764\n",
      "Iteration 242, loss = 0.08497354\n",
      "Iteration 243, loss = 0.08478118\n",
      "Iteration 244, loss = 0.08459055\n",
      "Iteration 245, loss = 0.08440166\n",
      "Iteration 246, loss = 0.08421442\n",
      "Iteration 247, loss = 0.08402892\n",
      "Iteration 248, loss = 0.08384507\n",
      "Iteration 249, loss = 0.08366286\n",
      "Iteration 250, loss = 0.08348221\n",
      "Iteration 251, loss = 0.08330316\n",
      "Iteration 252, loss = 0.08312566\n",
      "Iteration 253, loss = 0.08294970\n",
      "Iteration 254, loss = 0.08277526\n",
      "Iteration 255, loss = 0.08260233\n",
      "Iteration 256, loss = 0.08243088\n",
      "Iteration 257, loss = 0.08226088\n",
      "Iteration 258, loss = 0.08209232\n",
      "Iteration 259, loss = 0.08192518\n",
      "Iteration 260, loss = 0.08175945\n",
      "Iteration 261, loss = 0.08159512\n",
      "Iteration 262, loss = 0.08143215\n",
      "Iteration 263, loss = 0.08127055\n",
      "Iteration 264, loss = 0.08111032\n",
      "Iteration 265, loss = 0.08095141\n",
      "Iteration 266, loss = 0.08079380\n",
      "Iteration 267, loss = 0.08063748\n",
      "Iteration 268, loss = 0.08048246\n",
      "Iteration 269, loss = 0.08032866\n",
      "Iteration 270, loss = 0.08017612\n",
      "Iteration 271, loss = 0.08002481\n",
      "Iteration 272, loss = 0.07987471\n",
      "Iteration 273, loss = 0.07972581\n",
      "Iteration 274, loss = 0.07957809\n",
      "Iteration 275, loss = 0.07943155\n",
      "Iteration 276, loss = 0.07928616\n",
      "Iteration 277, loss = 0.07914192\n",
      "Iteration 278, loss = 0.07899883\n",
      "Iteration 279, loss = 0.07885682\n",
      "Iteration 280, loss = 0.07871592\n",
      "Iteration 281, loss = 0.07857612\n",
      "Iteration 282, loss = 0.07843740\n",
      "Iteration 283, loss = 0.07829973\n",
      "Iteration 284, loss = 0.07816313\n",
      "Iteration 285, loss = 0.07802757\n",
      "Iteration 286, loss = 0.07789304\n",
      "Iteration 287, loss = 0.07775952\n",
      "Iteration 288, loss = 0.07762700\n",
      "Iteration 289, loss = 0.07749549\n",
      "Iteration 290, loss = 0.07736496\n",
      "Iteration 291, loss = 0.07723541\n",
      "Iteration 292, loss = 0.07710682\n",
      "Iteration 293, loss = 0.07697918\n",
      "Iteration 294, loss = 0.07685247\n",
      "Iteration 295, loss = 0.07672670\n",
      "Iteration 296, loss = 0.07660185\n",
      "Iteration 297, loss = 0.07647792\n",
      "Iteration 298, loss = 0.07635488\n",
      "Iteration 299, loss = 0.07623274\n",
      "Iteration 300, loss = 0.07611152\n",
      "Iteration 301, loss = 0.07599117\n",
      "Iteration 302, loss = 0.07587169\n",
      "Iteration 303, loss = 0.07575306\n",
      "Iteration 304, loss = 0.07563528\n",
      "Iteration 305, loss = 0.07551835\n",
      "Iteration 306, loss = 0.07540224\n",
      "Iteration 307, loss = 0.07528695\n",
      "Iteration 308, loss = 0.07517248\n",
      "Iteration 309, loss = 0.07505880\n",
      "Iteration 310, loss = 0.07494592\n",
      "Iteration 311, loss = 0.07483382\n",
      "Iteration 312, loss = 0.07472250\n",
      "Iteration 313, loss = 0.07461195\n",
      "Iteration 314, loss = 0.07450216\n",
      "Iteration 315, loss = 0.07439313\n",
      "Iteration 316, loss = 0.07428485\n",
      "Iteration 317, loss = 0.07417732\n",
      "Iteration 318, loss = 0.07407052\n",
      "Iteration 319, loss = 0.07396444\n",
      "Iteration 320, loss = 0.07385907\n",
      "Iteration 321, loss = 0.07375442\n",
      "Iteration 322, loss = 0.07365047\n",
      "Iteration 323, loss = 0.07354721\n",
      "Iteration 324, loss = 0.07344464\n",
      "Iteration 325, loss = 0.07334275\n",
      "Iteration 326, loss = 0.07324154\n",
      "Iteration 327, loss = 0.07314100\n",
      "Iteration 328, loss = 0.07304112\n",
      "Iteration 329, loss = 0.07294190\n",
      "Iteration 330, loss = 0.07284334\n",
      "Iteration 331, loss = 0.07274541\n",
      "Iteration 332, loss = 0.07264811\n",
      "Iteration 333, loss = 0.07255144\n",
      "Iteration 334, loss = 0.07245539\n",
      "Iteration 335, loss = 0.07235996\n",
      "Iteration 336, loss = 0.07226514\n",
      "Iteration 337, loss = 0.07217094\n",
      "Iteration 338, loss = 0.07207733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66075849\n",
      "Iteration 2, loss = 1.35530244\n",
      "Iteration 3, loss = 1.21011263\n",
      "Iteration 4, loss = 1.17353181\n",
      "Iteration 5, loss = 1.07855536\n",
      "Iteration 6, loss = 0.97029392\n",
      "Iteration 7, loss = 0.89171227\n",
      "Iteration 8, loss = 0.83869525\n",
      "Iteration 9, loss = 0.79357123\n",
      "Iteration 10, loss = 0.75201646\n",
      "Iteration 11, loss = 0.71424351\n",
      "Iteration 12, loss = 0.68023966\n",
      "Iteration 13, loss = 0.64981228\n",
      "Iteration 14, loss = 0.62274891\n",
      "Iteration 15, loss = 0.59864517\n",
      "Iteration 16, loss = 0.57691476\n",
      "Iteration 17, loss = 0.55653301\n",
      "Iteration 18, loss = 0.53672367\n",
      "Iteration 19, loss = 0.52096495\n",
      "Iteration 20, loss = 0.50741829\n",
      "Iteration 21, loss = 0.49506520\n",
      "Iteration 22, loss = 0.48370283\n",
      "Iteration 23, loss = 0.47316233\n",
      "Iteration 24, loss = 0.46332089\n",
      "Iteration 25, loss = 0.45407922\n",
      "Iteration 26, loss = 0.44535112\n",
      "Iteration 27, loss = 0.43707187\n",
      "Iteration 28, loss = 0.42918346\n",
      "Iteration 29, loss = 0.42163832\n",
      "Iteration 30, loss = 0.41439900\n",
      "Iteration 31, loss = 0.40744466\n",
      "Iteration 32, loss = 0.40073676\n",
      "Iteration 33, loss = 0.39424986\n",
      "Iteration 34, loss = 0.38804400\n",
      "Iteration 35, loss = 0.38214462\n",
      "Iteration 36, loss = 0.37637313\n",
      "Iteration 37, loss = 0.37070540\n",
      "Iteration 38, loss = 0.36518131\n",
      "Iteration 39, loss = 0.35982434\n",
      "Iteration 40, loss = 0.35461203\n",
      "Iteration 41, loss = 0.34953116\n",
      "Iteration 42, loss = 0.34456152\n",
      "Iteration 43, loss = 0.33969834\n",
      "Iteration 44, loss = 0.33494176\n",
      "Iteration 45, loss = 0.33029476\n",
      "Iteration 46, loss = 0.32575730\n",
      "Iteration 47, loss = 0.32132503\n",
      "Iteration 48, loss = 0.31699507\n",
      "Iteration 49, loss = 0.31276755\n",
      "Iteration 50, loss = 0.30863877\n",
      "Iteration 51, loss = 0.30460487\n",
      "Iteration 52, loss = 0.30066163\n",
      "Iteration 53, loss = 0.29680816\n",
      "Iteration 54, loss = 0.29304040\n",
      "Iteration 55, loss = 0.28935801\n",
      "Iteration 56, loss = 0.28575632\n",
      "Iteration 57, loss = 0.28222608\n",
      "Iteration 58, loss = 0.27876843\n",
      "Iteration 59, loss = 0.27538694\n",
      "Iteration 60, loss = 0.27208033\n",
      "Iteration 61, loss = 0.26884340\n",
      "Iteration 62, loss = 0.26564787\n",
      "Iteration 63, loss = 0.26246951\n",
      "Iteration 64, loss = 0.25930906\n",
      "Iteration 65, loss = 0.25610976\n",
      "Iteration 66, loss = 0.25283771\n",
      "Iteration 67, loss = 0.24958304\n",
      "Iteration 68, loss = 0.24646276\n",
      "Iteration 69, loss = 0.24358746\n",
      "Iteration 70, loss = 0.24086519\n",
      "Iteration 71, loss = 0.23825080\n",
      "Iteration 72, loss = 0.23575079\n",
      "Iteration 73, loss = 0.23333690\n",
      "Iteration 74, loss = 0.23098350\n",
      "Iteration 75, loss = 0.22868426\n",
      "Iteration 76, loss = 0.22643460\n",
      "Iteration 77, loss = 0.22423517\n",
      "Iteration 78, loss = 0.22208624\n",
      "Iteration 79, loss = 0.21998508\n",
      "Iteration 80, loss = 0.21792931\n",
      "Iteration 81, loss = 0.21591622\n",
      "Iteration 82, loss = 0.21394702\n",
      "Iteration 83, loss = 0.21202146\n",
      "Iteration 84, loss = 0.21013808\n",
      "Iteration 85, loss = 0.20829552\n",
      "Iteration 86, loss = 0.20649339\n",
      "Iteration 87, loss = 0.20473135\n",
      "Iteration 88, loss = 0.20300760\n",
      "Iteration 89, loss = 0.20132181\n",
      "Iteration 90, loss = 0.19967141\n",
      "Iteration 91, loss = 0.19805672\n",
      "Iteration 92, loss = 0.19647557\n",
      "Iteration 93, loss = 0.19492691\n",
      "Iteration 94, loss = 0.19341070\n",
      "Iteration 95, loss = 0.19192631\n",
      "Iteration 96, loss = 0.19047172\n",
      "Iteration 97, loss = 0.18904643\n",
      "Iteration 98, loss = 0.18765068\n",
      "Iteration 99, loss = 0.18628369\n",
      "Iteration 100, loss = 0.18494556\n",
      "Iteration 101, loss = 0.18363950\n",
      "Iteration 102, loss = 0.18235965\n",
      "Iteration 103, loss = 0.18110600\n",
      "Iteration 104, loss = 0.17987891\n",
      "Iteration 105, loss = 0.17868363\n",
      "Iteration 106, loss = 0.17752403\n",
      "Iteration 107, loss = 0.17639183\n",
      "Iteration 108, loss = 0.17528487\n",
      "Iteration 109, loss = 0.17420121\n",
      "Iteration 110, loss = 0.17313969\n",
      "Iteration 111, loss = 0.17210697\n",
      "Iteration 112, loss = 0.17108921\n",
      "Iteration 113, loss = 0.17008807\n",
      "Iteration 114, loss = 0.16910351\n",
      "Iteration 115, loss = 0.16814041\n",
      "Iteration 116, loss = 0.16719990\n",
      "Iteration 117, loss = 0.16627834\n",
      "Iteration 118, loss = 0.16537490\n",
      "Iteration 119, loss = 0.16448937\n",
      "Iteration 120, loss = 0.16362105\n",
      "Iteration 121, loss = 0.16276943\n",
      "Iteration 122, loss = 0.16193412\n",
      "Iteration 123, loss = 0.16111481\n",
      "Iteration 124, loss = 0.16031114\n",
      "Iteration 125, loss = 0.15952343\n",
      "Iteration 126, loss = 0.15875252\n",
      "Iteration 127, loss = 0.15799623\n",
      "Iteration 128, loss = 0.15725479\n",
      "Iteration 129, loss = 0.15652827\n",
      "Iteration 130, loss = 0.15581543\n",
      "Iteration 131, loss = 0.15511563\n",
      "Iteration 132, loss = 0.15442921\n",
      "Iteration 133, loss = 0.15375602\n",
      "Iteration 134, loss = 0.15309740\n",
      "Iteration 135, loss = 0.15245183\n",
      "Iteration 136, loss = 0.15182155\n",
      "Iteration 137, loss = 0.15120337\n",
      "Iteration 138, loss = 0.15059523\n",
      "Iteration 139, loss = 0.14999780\n",
      "Iteration 140, loss = 0.14941114\n",
      "Iteration 141, loss = 0.14883445\n",
      "Iteration 142, loss = 0.14826660\n",
      "Iteration 143, loss = 0.14770770\n",
      "Iteration 144, loss = 0.14715774\n",
      "Iteration 145, loss = 0.14661631\n",
      "Iteration 146, loss = 0.14608331\n",
      "Iteration 147, loss = 0.14555865\n",
      "Iteration 148, loss = 0.14504232\n",
      "Iteration 149, loss = 0.14453432\n",
      "Iteration 150, loss = 0.14403481\n",
      "Iteration 151, loss = 0.14354318\n",
      "Iteration 152, loss = 0.14305898\n",
      "Iteration 153, loss = 0.14258262\n",
      "Iteration 154, loss = 0.14211354\n",
      "Iteration 155, loss = 0.14165183\n",
      "Iteration 156, loss = 0.14119721\n",
      "Iteration 157, loss = 0.14074939\n",
      "Iteration 158, loss = 0.14030827\n",
      "Iteration 159, loss = 0.13987373\n",
      "Iteration 160, loss = 0.13944567\n",
      "Iteration 161, loss = 0.13902382\n",
      "Iteration 162, loss = 0.13860818\n",
      "Iteration 163, loss = 0.13819868\n",
      "Iteration 164, loss = 0.13779510\n",
      "Iteration 165, loss = 0.13739729\n",
      "Iteration 166, loss = 0.13700513\n",
      "Iteration 167, loss = 0.13661852\n",
      "Iteration 168, loss = 0.13623735\n",
      "Iteration 169, loss = 0.13586145\n",
      "Iteration 170, loss = 0.13549076\n",
      "Iteration 171, loss = 0.13512525\n",
      "Iteration 172, loss = 0.13476463\n",
      "Iteration 173, loss = 0.13440897\n",
      "Iteration 174, loss = 0.13405812\n",
      "Iteration 175, loss = 0.13371197\n",
      "Iteration 176, loss = 0.13337044\n",
      "Iteration 177, loss = 0.13303344\n",
      "Iteration 178, loss = 0.13270088\n",
      "Iteration 179, loss = 0.13237267\n",
      "Iteration 180, loss = 0.13204873\n",
      "Iteration 181, loss = 0.13172898\n",
      "Iteration 182, loss = 0.13141341\n",
      "Iteration 183, loss = 0.13110179\n",
      "Iteration 184, loss = 0.13079417\n",
      "Iteration 185, loss = 0.13049043\n",
      "Iteration 186, loss = 0.13019052\n",
      "Iteration 187, loss = 0.12989435\n",
      "Iteration 188, loss = 0.12960186\n",
      "Iteration 189, loss = 0.12931300\n",
      "Iteration 190, loss = 0.12902775\n",
      "Iteration 191, loss = 0.12874599\n",
      "Iteration 192, loss = 0.12846764\n",
      "Iteration 193, loss = 0.12819267\n",
      "Iteration 194, loss = 0.12792101\n",
      "Iteration 195, loss = 0.12765268\n",
      "Iteration 196, loss = 0.12738748\n",
      "Iteration 197, loss = 0.12712545\n",
      "Iteration 198, loss = 0.12686651\n",
      "Iteration 199, loss = 0.12661060\n",
      "Iteration 200, loss = 0.12635767\n",
      "Iteration 201, loss = 0.12610769\n",
      "Iteration 202, loss = 0.12586060\n",
      "Iteration 203, loss = 0.12561633\n",
      "Iteration 204, loss = 0.12537492\n",
      "Iteration 205, loss = 0.12513624\n",
      "Iteration 206, loss = 0.12490026\n",
      "Iteration 207, loss = 0.12466692\n",
      "Iteration 208, loss = 0.12443619\n",
      "Iteration 209, loss = 0.12420800\n",
      "Iteration 210, loss = 0.12398235\n",
      "Iteration 211, loss = 0.12375917\n",
      "Iteration 212, loss = 0.12353843\n",
      "Iteration 213, loss = 0.12332009\n",
      "Iteration 214, loss = 0.12310413\n",
      "Iteration 215, loss = 0.12289048\n",
      "Iteration 216, loss = 0.12267912\n",
      "Iteration 217, loss = 0.12247003\n",
      "Iteration 218, loss = 0.12226314\n",
      "Iteration 219, loss = 0.12205843\n",
      "Iteration 220, loss = 0.12185587\n",
      "Iteration 221, loss = 0.12165543\n",
      "Iteration 222, loss = 0.12145706\n",
      "Iteration 223, loss = 0.12126075\n",
      "Iteration 224, loss = 0.12106646\n",
      "Iteration 225, loss = 0.12087415\n",
      "Iteration 226, loss = 0.12068379\n",
      "Iteration 227, loss = 0.12049537\n",
      "Iteration 228, loss = 0.12030884\n",
      "Iteration 229, loss = 0.12012417\n",
      "Iteration 230, loss = 0.11994135\n",
      "Iteration 231, loss = 0.11976034\n",
      "Iteration 232, loss = 0.11958112\n",
      "Iteration 233, loss = 0.11940366\n",
      "Iteration 234, loss = 0.11922794\n",
      "Iteration 235, loss = 0.11905393\n",
      "Iteration 236, loss = 0.11888160\n",
      "Iteration 237, loss = 0.11871093\n",
      "Iteration 238, loss = 0.11854189\n",
      "Iteration 239, loss = 0.11837447\n",
      "Iteration 240, loss = 0.11820863\n",
      "Iteration 241, loss = 0.11804436\n",
      "Iteration 242, loss = 0.11788164\n",
      "Iteration 243, loss = 0.11772043\n",
      "Iteration 244, loss = 0.11756073\n",
      "Iteration 245, loss = 0.11740251\n",
      "Iteration 246, loss = 0.11724574\n",
      "Iteration 247, loss = 0.11709041\n",
      "Iteration 248, loss = 0.11693650\n",
      "Iteration 249, loss = 0.11678399\n",
      "Iteration 250, loss = 0.11663286\n",
      "Iteration 251, loss = 0.11648308\n",
      "Iteration 252, loss = 0.11633465\n",
      "Iteration 253, loss = 0.11618754\n",
      "Iteration 254, loss = 0.11604174\n",
      "Iteration 255, loss = 0.11589722\n",
      "Iteration 256, loss = 0.11575398\n",
      "Iteration 257, loss = 0.11561199\n",
      "Iteration 258, loss = 0.11547124\n",
      "Iteration 259, loss = 0.11533171\n",
      "Iteration 260, loss = 0.11519338\n",
      "Iteration 261, loss = 0.11505624\n",
      "Iteration 262, loss = 0.11492028\n",
      "Iteration 263, loss = 0.11478547\n",
      "Iteration 264, loss = 0.11465181\n",
      "Iteration 265, loss = 0.11451936\n",
      "Iteration 266, loss = 0.11438807\n",
      "Iteration 267, loss = 0.11425787\n",
      "Iteration 268, loss = 0.11412875\n",
      "Iteration 269, loss = 0.11400071\n",
      "Iteration 270, loss = 0.11387371\n",
      "Iteration 271, loss = 0.11374776\n",
      "Iteration 272, loss = 0.11362284\n",
      "Iteration 273, loss = 0.11349894\n",
      "Iteration 274, loss = 0.11337604\n",
      "Iteration 275, loss = 0.11325414\n",
      "Iteration 276, loss = 0.11313322\n",
      "Iteration 277, loss = 0.11301331\n",
      "Iteration 278, loss = 0.11289433\n",
      "Iteration 279, loss = 0.11277629\n",
      "Iteration 280, loss = 0.11265918\n",
      "Iteration 281, loss = 0.11254303\n",
      "Iteration 282, loss = 0.11242776\n",
      "Iteration 283, loss = 0.11231340\n",
      "Iteration 284, loss = 0.11219992\n",
      "Iteration 285, loss = 0.11208734\n",
      "Iteration 286, loss = 0.11197562\n",
      "Iteration 287, loss = 0.11186475\n",
      "Iteration 288, loss = 0.11175472\n",
      "Iteration 289, loss = 0.11164554\n",
      "Iteration 290, loss = 0.11153721\n",
      "Iteration 291, loss = 0.11142967\n",
      "Iteration 292, loss = 0.11132295\n",
      "Iteration 293, loss = 0.11121701\n",
      "Iteration 294, loss = 0.11111187\n",
      "Iteration 295, loss = 0.11100752\n",
      "Iteration 296, loss = 0.11090392\n",
      "Iteration 297, loss = 0.11080109\n",
      "Iteration 298, loss = 0.11069902\n",
      "Iteration 299, loss = 0.11059770\n",
      "Iteration 300, loss = 0.11049710\n",
      "Iteration 301, loss = 0.11039724\n",
      "Iteration 302, loss = 0.11029809\n",
      "Iteration 303, loss = 0.11019968\n",
      "Iteration 304, loss = 0.11010194\n",
      "Iteration 305, loss = 0.11000491\n",
      "Iteration 306, loss = 0.10990856\n",
      "Iteration 307, loss = 0.10981289\n",
      "Iteration 308, loss = 0.10971791\n",
      "Iteration 309, loss = 0.10962358\n",
      "Iteration 310, loss = 0.10952990\n",
      "Iteration 311, loss = 0.10943689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66212118\n",
      "Iteration 2, loss = 1.59494215\n",
      "Iteration 3, loss = 1.53166961\n",
      "Iteration 4, loss = 1.47259591\n",
      "Iteration 5, loss = 1.41822357\n",
      "Iteration 6, loss = 1.36880665\n",
      "Iteration 7, loss = 1.32470150\n",
      "Iteration 8, loss = 1.28625946\n",
      "Iteration 9, loss = 1.25356433\n",
      "Iteration 10, loss = 1.22645956\n",
      "Iteration 11, loss = 1.20440601\n",
      "Iteration 12, loss = 1.18656250\n",
      "Iteration 13, loss = 1.17189561\n",
      "Iteration 14, loss = 1.15924097\n",
      "Iteration 15, loss = 1.14748199\n",
      "Iteration 16, loss = 1.13571561\n",
      "Iteration 17, loss = 1.12331906\n",
      "Iteration 18, loss = 1.10984897\n",
      "Iteration 19, loss = 1.09521930\n",
      "Iteration 20, loss = 1.07959348\n",
      "Iteration 21, loss = 1.06321173\n",
      "Iteration 22, loss = 1.04642308\n",
      "Iteration 23, loss = 1.02964117\n",
      "Iteration 24, loss = 1.01317603\n",
      "Iteration 25, loss = 0.99728768\n",
      "Iteration 26, loss = 0.98220536\n",
      "Iteration 27, loss = 0.96794020\n",
      "Iteration 28, loss = 0.95453523\n",
      "Iteration 29, loss = 0.94191425\n",
      "Iteration 30, loss = 0.92983503\n",
      "Iteration 31, loss = 0.91821509\n",
      "Iteration 32, loss = 0.90689808\n",
      "Iteration 33, loss = 0.89582061\n",
      "Iteration 34, loss = 0.88486964\n",
      "Iteration 35, loss = 0.87401647\n",
      "Iteration 36, loss = 0.86330742\n",
      "Iteration 37, loss = 0.85279519\n",
      "Iteration 38, loss = 0.84251538\n",
      "Iteration 39, loss = 0.83255678\n",
      "Iteration 40, loss = 0.82294774\n",
      "Iteration 41, loss = 0.81363301\n",
      "Iteration 42, loss = 0.80462993\n",
      "Iteration 43, loss = 0.79590536\n",
      "Iteration 44, loss = 0.78746613\n",
      "Iteration 45, loss = 0.77929079\n",
      "Iteration 46, loss = 0.77137454\n",
      "Iteration 47, loss = 0.76368956\n",
      "Iteration 48, loss = 0.75620945\n",
      "Iteration 49, loss = 0.74893641\n",
      "Iteration 50, loss = 0.74186662\n",
      "Iteration 51, loss = 0.73496959\n",
      "Iteration 52, loss = 0.72823547\n",
      "Iteration 53, loss = 0.72165027\n",
      "Iteration 54, loss = 0.71521358\n",
      "Iteration 55, loss = 0.70890400\n",
      "Iteration 56, loss = 0.70273761\n",
      "Iteration 57, loss = 0.69669815\n",
      "Iteration 58, loss = 0.69078458\n",
      "Iteration 59, loss = 0.68496118\n",
      "Iteration 60, loss = 0.67923894\n",
      "Iteration 61, loss = 0.67364631\n",
      "Iteration 62, loss = 0.66822280\n",
      "Iteration 63, loss = 0.66294281\n",
      "Iteration 64, loss = 0.65783104\n",
      "Iteration 65, loss = 0.65293295\n",
      "Iteration 66, loss = 0.64823973\n",
      "Iteration 67, loss = 0.64371443\n",
      "Iteration 68, loss = 0.63931861\n",
      "Iteration 69, loss = 0.63502156\n",
      "Iteration 70, loss = 0.63081621\n",
      "Iteration 71, loss = 0.62670403\n",
      "Iteration 72, loss = 0.62265573\n",
      "Iteration 73, loss = 0.61867536\n",
      "Iteration 74, loss = 0.61475194\n",
      "Iteration 75, loss = 0.61087857\n",
      "Iteration 76, loss = 0.60706180\n",
      "Iteration 77, loss = 0.60330363\n",
      "Iteration 78, loss = 0.59960310\n",
      "Iteration 79, loss = 0.59596041\n",
      "Iteration 80, loss = 0.59237881\n",
      "Iteration 81, loss = 0.58885741\n",
      "Iteration 82, loss = 0.58538669\n",
      "Iteration 83, loss = 0.58196808\n",
      "Iteration 84, loss = 0.57859966\n",
      "Iteration 85, loss = 0.57528178\n",
      "Iteration 86, loss = 0.57201420\n",
      "Iteration 87, loss = 0.56879311\n",
      "Iteration 88, loss = 0.56561599\n",
      "Iteration 89, loss = 0.56248307\n",
      "Iteration 90, loss = 0.55939486\n",
      "Iteration 91, loss = 0.55634964\n",
      "Iteration 92, loss = 0.55334597\n",
      "Iteration 93, loss = 0.55038436\n",
      "Iteration 94, loss = 0.54746518\n",
      "Iteration 95, loss = 0.54458790\n",
      "Iteration 96, loss = 0.54174671\n",
      "Iteration 97, loss = 0.53894309\n",
      "Iteration 98, loss = 0.53617524\n",
      "Iteration 99, loss = 0.53344149\n",
      "Iteration 100, loss = 0.53074142\n",
      "Iteration 101, loss = 0.52807485\n",
      "Iteration 102, loss = 0.52543990\n",
      "Iteration 103, loss = 0.52283724\n",
      "Iteration 104, loss = 0.52026843\n",
      "Iteration 105, loss = 0.51773006\n",
      "Iteration 106, loss = 0.51522095\n",
      "Iteration 107, loss = 0.51273946\n",
      "Iteration 108, loss = 0.51028516\n",
      "Iteration 109, loss = 0.50785855\n",
      "Iteration 110, loss = 0.50545804\n",
      "Iteration 111, loss = 0.50308276\n",
      "Iteration 112, loss = 0.50073183\n",
      "Iteration 113, loss = 0.49840473\n",
      "Iteration 114, loss = 0.49610460\n",
      "Iteration 115, loss = 0.49384012\n",
      "Iteration 116, loss = 0.49161738\n",
      "Iteration 117, loss = 0.48944087\n",
      "Iteration 118, loss = 0.48729794\n",
      "Iteration 119, loss = 0.48516976\n",
      "Iteration 120, loss = 0.48304088\n",
      "Iteration 121, loss = 0.48091798\n",
      "Iteration 122, loss = 0.47885153\n",
      "Iteration 123, loss = 0.47672685\n",
      "Iteration 124, loss = 0.47464845\n",
      "Iteration 125, loss = 0.47258905\n",
      "Iteration 126, loss = 0.47053852\n",
      "Iteration 127, loss = 0.46849469\n",
      "Iteration 128, loss = 0.46645861\n",
      "Iteration 129, loss = 0.46441612\n",
      "Iteration 130, loss = 0.46237355\n",
      "Iteration 131, loss = 0.46035196\n",
      "Iteration 132, loss = 0.45833874\n",
      "Iteration 133, loss = 0.45636254\n",
      "Iteration 134, loss = 0.45442728\n",
      "Iteration 135, loss = 0.45249905\n",
      "Iteration 136, loss = 0.45057045\n",
      "Iteration 137, loss = 0.44864174\n",
      "Iteration 138, loss = 0.44672262\n",
      "Iteration 139, loss = 0.44482381\n",
      "Iteration 140, loss = 0.44292871\n",
      "Iteration 141, loss = 0.44103812\n",
      "Iteration 142, loss = 0.43915442\n",
      "Iteration 143, loss = 0.43728526\n",
      "Iteration 144, loss = 0.43542660\n",
      "Iteration 145, loss = 0.43357202\n",
      "Iteration 146, loss = 0.43172217\n",
      "Iteration 147, loss = 0.42987752\n",
      "Iteration 148, loss = 0.42804173\n",
      "Iteration 149, loss = 0.42621652\n",
      "Iteration 150, loss = 0.42439622\n",
      "Iteration 151, loss = 0.42258008\n",
      "Iteration 152, loss = 0.42076896\n",
      "Iteration 153, loss = 0.41896203\n",
      "Iteration 154, loss = 0.41715837\n",
      "Iteration 155, loss = 0.41535393\n",
      "Iteration 156, loss = 0.41354932\n",
      "Iteration 157, loss = 0.41174674\n",
      "Iteration 158, loss = 0.40995552\n",
      "Iteration 159, loss = 0.40818092\n",
      "Iteration 160, loss = 0.40639985\n",
      "Iteration 161, loss = 0.40461554\n",
      "Iteration 162, loss = 0.40283518\n",
      "Iteration 163, loss = 0.40106617\n",
      "Iteration 164, loss = 0.39930205\n",
      "Iteration 165, loss = 0.39753870\n",
      "Iteration 166, loss = 0.39577566\n",
      "Iteration 167, loss = 0.39401456\n",
      "Iteration 168, loss = 0.39225605\n",
      "Iteration 169, loss = 0.39050412\n",
      "Iteration 170, loss = 0.38875376\n",
      "Iteration 171, loss = 0.38700422\n",
      "Iteration 172, loss = 0.38525622\n",
      "Iteration 173, loss = 0.38351017\n",
      "Iteration 174, loss = 0.38176598\n",
      "Iteration 175, loss = 0.38002410\n",
      "Iteration 176, loss = 0.37828431\n",
      "Iteration 177, loss = 0.37654686\n",
      "Iteration 178, loss = 0.37481185\n",
      "Iteration 179, loss = 0.37307760\n",
      "Iteration 180, loss = 0.37134456\n",
      "Iteration 181, loss = 0.36961386\n",
      "Iteration 182, loss = 0.36788558\n",
      "Iteration 183, loss = 0.36615927\n",
      "Iteration 184, loss = 0.36443475\n",
      "Iteration 185, loss = 0.36271240\n",
      "Iteration 186, loss = 0.36099290\n",
      "Iteration 187, loss = 0.35927572\n",
      "Iteration 188, loss = 0.35756159\n",
      "Iteration 189, loss = 0.35585034\n",
      "Iteration 190, loss = 0.35414194\n",
      "Iteration 191, loss = 0.35243713\n",
      "Iteration 192, loss = 0.35073594\n",
      "Iteration 193, loss = 0.34903817\n",
      "Iteration 194, loss = 0.34734393\n",
      "Iteration 195, loss = 0.34565336\n",
      "Iteration 196, loss = 0.34396690\n",
      "Iteration 197, loss = 0.34228474\n",
      "Iteration 198, loss = 0.34060743\n",
      "Iteration 199, loss = 0.33893334\n",
      "Iteration 200, loss = 0.33725731\n",
      "Iteration 201, loss = 0.33557478\n",
      "Iteration 202, loss = 0.33390164\n",
      "Iteration 203, loss = 0.33223833\n",
      "Iteration 204, loss = 0.33057621\n",
      "Iteration 205, loss = 0.32891378\n",
      "Iteration 206, loss = 0.32725127\n",
      "Iteration 207, loss = 0.32559019\n",
      "Iteration 208, loss = 0.32392984\n",
      "Iteration 209, loss = 0.32227023\n",
      "Iteration 210, loss = 0.32061146\n",
      "Iteration 211, loss = 0.31895367\n",
      "Iteration 212, loss = 0.31729714\n",
      "Iteration 213, loss = 0.31564210\n",
      "Iteration 214, loss = 0.31398899\n",
      "Iteration 215, loss = 0.31233774\n",
      "Iteration 216, loss = 0.31068882\n",
      "Iteration 217, loss = 0.30904221\n",
      "Iteration 218, loss = 0.30739815\n",
      "Iteration 219, loss = 0.30575730\n",
      "Iteration 220, loss = 0.30411974\n",
      "Iteration 221, loss = 0.30248554\n",
      "Iteration 222, loss = 0.30085510\n",
      "Iteration 223, loss = 0.29922864\n",
      "Iteration 224, loss = 0.29760654\n",
      "Iteration 225, loss = 0.29598896\n",
      "Iteration 226, loss = 0.29437637\n",
      "Iteration 227, loss = 0.29276905\n",
      "Iteration 228, loss = 0.29116732\n",
      "Iteration 229, loss = 0.28957130\n",
      "Iteration 230, loss = 0.28798148\n",
      "Iteration 231, loss = 0.28639788\n",
      "Iteration 232, loss = 0.28482069\n",
      "Iteration 233, loss = 0.28325033\n",
      "Iteration 234, loss = 0.28168684\n",
      "Iteration 235, loss = 0.28013042\n",
      "Iteration 236, loss = 0.27858143\n",
      "Iteration 237, loss = 0.27703998\n",
      "Iteration 238, loss = 0.27550624\n",
      "Iteration 239, loss = 0.27398071\n",
      "Iteration 240, loss = 0.27246272\n",
      "Iteration 241, loss = 0.27094873\n",
      "Iteration 242, loss = 0.26943650\n",
      "Iteration 243, loss = 0.26793998\n",
      "Iteration 244, loss = 0.26644966\n",
      "Iteration 245, loss = 0.26496533\n",
      "Iteration 246, loss = 0.26348698\n",
      "Iteration 247, loss = 0.26201497\n",
      "Iteration 248, loss = 0.26054952\n",
      "Iteration 249, loss = 0.25909063\n",
      "Iteration 250, loss = 0.25763816\n",
      "Iteration 251, loss = 0.25619192\n",
      "Iteration 252, loss = 0.25475185\n",
      "Iteration 253, loss = 0.25331796\n",
      "Iteration 254, loss = 0.25189015\n",
      "Iteration 255, loss = 0.25046807\n",
      "Iteration 256, loss = 0.24905147\n",
      "Iteration 257, loss = 0.24764043\n",
      "Iteration 258, loss = 0.24623376\n",
      "Iteration 259, loss = 0.24482811\n",
      "Iteration 260, loss = 0.24341899\n",
      "Iteration 261, loss = 0.24201973\n",
      "Iteration 262, loss = 0.24062779\n",
      "Iteration 263, loss = 0.23923931\n",
      "Iteration 264, loss = 0.23785454\n",
      "Iteration 265, loss = 0.23647367\n",
      "Iteration 266, loss = 0.23509689\n",
      "Iteration 267, loss = 0.23372450\n",
      "Iteration 268, loss = 0.23235663\n",
      "Iteration 269, loss = 0.23099345\n",
      "Iteration 270, loss = 0.22963535\n",
      "Iteration 271, loss = 0.22828221\n",
      "Iteration 272, loss = 0.22693447\n",
      "Iteration 273, loss = 0.22559241\n",
      "Iteration 274, loss = 0.22425648\n",
      "Iteration 275, loss = 0.22292660\n",
      "Iteration 276, loss = 0.22160301\n",
      "Iteration 277, loss = 0.22028608\n",
      "Iteration 278, loss = 0.21897593\n",
      "Iteration 279, loss = 0.21767357\n",
      "Iteration 280, loss = 0.21637847\n",
      "Iteration 281, loss = 0.21509092\n",
      "Iteration 282, loss = 0.21381113\n",
      "Iteration 283, loss = 0.21253873\n",
      "Iteration 284, loss = 0.21127741\n",
      "Iteration 285, loss = 0.21001994\n",
      "Iteration 286, loss = 0.20877557\n",
      "Iteration 287, loss = 0.20753922\n",
      "Iteration 288, loss = 0.20631055\n",
      "Iteration 289, loss = 0.20508916\n",
      "Iteration 290, loss = 0.20387523\n",
      "Iteration 291, loss = 0.20267282\n",
      "Iteration 292, loss = 0.20148113\n",
      "Iteration 293, loss = 0.20029563\n",
      "Iteration 294, loss = 0.19912246\n",
      "Iteration 295, loss = 0.19795637\n",
      "Iteration 296, loss = 0.19679779\n",
      "Iteration 297, loss = 0.19564793\n",
      "Iteration 298, loss = 0.19450976\n",
      "Iteration 299, loss = 0.19337767\n",
      "Iteration 300, loss = 0.19225416\n",
      "Iteration 301, loss = 0.19113994\n",
      "Iteration 302, loss = 0.19003224\n",
      "Iteration 303, loss = 0.18893136\n",
      "Iteration 304, loss = 0.18783903\n",
      "Iteration 305, loss = 0.18675346\n",
      "Iteration 306, loss = 0.18567491\n",
      "Iteration 307, loss = 0.18460457\n",
      "Iteration 308, loss = 0.18354096\n",
      "Iteration 309, loss = 0.18248483\n",
      "Iteration 310, loss = 0.18143605\n",
      "Iteration 311, loss = 0.18039447\n",
      "Iteration 312, loss = 0.17936088\n",
      "Iteration 313, loss = 0.17833520\n",
      "Iteration 314, loss = 0.17731673\n",
      "Iteration 315, loss = 0.17630609\n",
      "Iteration 316, loss = 0.17530355\n",
      "Iteration 317, loss = 0.17430911\n",
      "Iteration 318, loss = 0.17332259\n",
      "Iteration 319, loss = 0.17234434\n",
      "Iteration 320, loss = 0.17137442\n",
      "Iteration 321, loss = 0.17041292\n",
      "Iteration 322, loss = 0.16945980\n",
      "Iteration 323, loss = 0.16851541\n",
      "Iteration 324, loss = 0.16757978\n",
      "Iteration 325, loss = 0.16665301\n",
      "Iteration 326, loss = 0.16573551\n",
      "Iteration 327, loss = 0.16482699\n",
      "Iteration 328, loss = 0.16392722\n",
      "Iteration 329, loss = 0.16303626\n",
      "Iteration 330, loss = 0.16215445\n",
      "Iteration 331, loss = 0.16128172\n",
      "Iteration 332, loss = 0.16041825\n",
      "Iteration 333, loss = 0.15956380\n",
      "Iteration 334, loss = 0.15871857\n",
      "Iteration 335, loss = 0.15788245\n",
      "Iteration 336, loss = 0.15705544\n",
      "Iteration 337, loss = 0.15623778\n",
      "Iteration 338, loss = 0.15542925\n",
      "Iteration 339, loss = 0.15462983\n",
      "Iteration 340, loss = 0.15383943\n",
      "Iteration 341, loss = 0.15305812\n",
      "Iteration 342, loss = 0.15228598\n",
      "Iteration 343, loss = 0.15152295\n",
      "Iteration 344, loss = 0.15076895\n",
      "Iteration 345, loss = 0.15002400\n",
      "Iteration 346, loss = 0.14928806\n",
      "Iteration 347, loss = 0.14856089\n",
      "Iteration 348, loss = 0.14784256\n",
      "Iteration 349, loss = 0.14713308\n",
      "Iteration 350, loss = 0.14643236\n",
      "Iteration 351, loss = 0.14574043\n",
      "Iteration 352, loss = 0.14505752\n",
      "Iteration 353, loss = 0.14438306\n",
      "Iteration 354, loss = 0.14371680\n",
      "Iteration 355, loss = 0.14305869\n",
      "Iteration 356, loss = 0.14240940\n",
      "Iteration 357, loss = 0.14176708\n",
      "Iteration 358, loss = 0.14113327\n",
      "Iteration 359, loss = 0.14050748\n",
      "Iteration 360, loss = 0.13988869\n",
      "Iteration 361, loss = 0.13927694\n",
      "Iteration 362, loss = 0.13867385\n",
      "Iteration 363, loss = 0.13807685\n",
      "Iteration 364, loss = 0.13748743\n",
      "Iteration 365, loss = 0.13690536\n",
      "Iteration 366, loss = 0.13632978\n",
      "Iteration 367, loss = 0.13576085\n",
      "Iteration 368, loss = 0.13519925\n",
      "Iteration 369, loss = 0.13464441\n",
      "Iteration 370, loss = 0.13409594\n",
      "Iteration 371, loss = 0.13355445\n",
      "Iteration 372, loss = 0.13301886\n",
      "Iteration 373, loss = 0.13248924\n",
      "Iteration 374, loss = 0.13196616\n",
      "Iteration 375, loss = 0.13144916\n",
      "Iteration 376, loss = 0.13093791\n",
      "Iteration 377, loss = 0.13043284\n",
      "Iteration 378, loss = 0.12993361\n",
      "Iteration 379, loss = 0.12944007\n",
      "Iteration 380, loss = 0.12895221\n",
      "Iteration 381, loss = 0.12847021\n",
      "Iteration 382, loss = 0.12799373\n",
      "Iteration 383, loss = 0.12752287\n",
      "Iteration 384, loss = 0.12705764\n",
      "Iteration 385, loss = 0.12659732\n",
      "Iteration 386, loss = 0.12614162\n",
      "Iteration 387, loss = 0.12569182\n",
      "Iteration 388, loss = 0.12524692\n",
      "Iteration 389, loss = 0.12480658\n",
      "Iteration 390, loss = 0.12437138\n",
      "Iteration 391, loss = 0.12394033\n",
      "Iteration 392, loss = 0.12351407\n",
      "Iteration 393, loss = 0.12309171\n",
      "Iteration 394, loss = 0.12267366\n",
      "Iteration 395, loss = 0.12225971\n",
      "Iteration 396, loss = 0.12184961\n",
      "Iteration 397, loss = 0.12144302\n",
      "Iteration 398, loss = 0.12104018\n",
      "Iteration 399, loss = 0.12064033\n",
      "Iteration 400, loss = 0.12024306\n",
      "Iteration 401, loss = 0.11984885\n",
      "Iteration 402, loss = 0.11945725\n",
      "Iteration 403, loss = 0.11906828\n",
      "Iteration 404, loss = 0.11868195\n",
      "Iteration 405, loss = 0.11829777\n",
      "Iteration 406, loss = 0.11791570\n",
      "Iteration 407, loss = 0.11753606\n",
      "Iteration 408, loss = 0.11715891\n",
      "Iteration 409, loss = 0.11678370\n",
      "Iteration 410, loss = 0.11641066\n",
      "Iteration 411, loss = 0.11604015\n",
      "Iteration 412, loss = 0.11567182\n",
      "Iteration 413, loss = 0.11530553\n",
      "Iteration 414, loss = 0.11494160\n",
      "Iteration 415, loss = 0.11458016\n",
      "Iteration 416, loss = 0.11422099\n",
      "Iteration 417, loss = 0.11386420\n",
      "Iteration 418, loss = 0.11350977\n",
      "Iteration 419, loss = 0.11315783\n",
      "Iteration 420, loss = 0.11280835\n",
      "Iteration 421, loss = 0.11246124\n",
      "Iteration 422, loss = 0.11211677\n",
      "Iteration 423, loss = 0.11177486\n",
      "Iteration 424, loss = 0.11143554\n",
      "Iteration 425, loss = 0.11109895\n",
      "Iteration 426, loss = 0.11076506\n",
      "Iteration 427, loss = 0.11043384\n",
      "Iteration 428, loss = 0.11010547\n",
      "Iteration 429, loss = 0.10977986\n",
      "Iteration 430, loss = 0.10945712\n",
      "Iteration 431, loss = 0.10913738\n",
      "Iteration 432, loss = 0.10882044\n",
      "Iteration 433, loss = 0.10850643\n",
      "Iteration 434, loss = 0.10819525\n",
      "Iteration 435, loss = 0.10788700\n",
      "Iteration 436, loss = 0.10758164\n",
      "Iteration 437, loss = 0.10727914\n",
      "Iteration 438, loss = 0.10697973\n",
      "Iteration 439, loss = 0.10668324\n",
      "Iteration 440, loss = 0.10638973\n",
      "Iteration 441, loss = 0.10609906\n",
      "Iteration 442, loss = 0.10581137\n",
      "Iteration 443, loss = 0.10552668\n",
      "Iteration 444, loss = 0.10524476\n",
      "Iteration 445, loss = 0.10496577\n",
      "Iteration 446, loss = 0.10468986\n",
      "Iteration 447, loss = 0.10441618\n",
      "Iteration 448, loss = 0.10414591\n",
      "Iteration 449, loss = 0.10387837\n",
      "Iteration 450, loss = 0.10361368\n",
      "Iteration 451, loss = 0.10335193\n",
      "Iteration 452, loss = 0.10309273\n",
      "Iteration 453, loss = 0.10283629\n",
      "Iteration 454, loss = 0.10258237\n",
      "Iteration 455, loss = 0.10233096\n",
      "Iteration 456, loss = 0.10208227\n",
      "Iteration 457, loss = 0.10183603\n",
      "Iteration 458, loss = 0.10159228\n",
      "Iteration 459, loss = 0.10135101\n",
      "Iteration 460, loss = 0.10111213\n",
      "Iteration 461, loss = 0.10087578\n",
      "Iteration 462, loss = 0.10064182\n",
      "Iteration 463, loss = 0.10041011\n",
      "Iteration 464, loss = 0.10018061\n",
      "Iteration 465, loss = 0.09995335\n",
      "Iteration 466, loss = 0.09972821\n",
      "Iteration 467, loss = 0.09950521\n",
      "Iteration 468, loss = 0.09928448\n",
      "Iteration 469, loss = 0.09906580\n",
      "Iteration 470, loss = 0.09884943\n",
      "Iteration 471, loss = 0.09863493\n",
      "Iteration 472, loss = 0.09842278\n",
      "Iteration 473, loss = 0.09821275\n",
      "Iteration 474, loss = 0.09800462\n",
      "Iteration 475, loss = 0.09779846\n",
      "Iteration 476, loss = 0.09759439\n",
      "Iteration 477, loss = 0.09739229\n",
      "Iteration 478, loss = 0.09719212\n",
      "Iteration 479, loss = 0.09699389\n",
      "Iteration 480, loss = 0.09679778\n",
      "Iteration 481, loss = 0.09660372\n",
      "Iteration 482, loss = 0.09641157\n",
      "Iteration 483, loss = 0.09622136\n",
      "Iteration 484, loss = 0.09603315\n",
      "Iteration 485, loss = 0.09584678\n",
      "Iteration 486, loss = 0.09566247\n",
      "Iteration 487, loss = 0.09548025\n",
      "Iteration 488, loss = 0.09530009\n",
      "Iteration 489, loss = 0.09512201\n",
      "Iteration 490, loss = 0.09494600\n",
      "Iteration 491, loss = 0.09477183\n",
      "Iteration 492, loss = 0.09459956\n",
      "Iteration 493, loss = 0.09442910\n",
      "Iteration 494, loss = 0.09426046\n",
      "Iteration 495, loss = 0.09409363\n",
      "Iteration 496, loss = 0.09392867\n",
      "Iteration 497, loss = 0.09376582\n",
      "Iteration 498, loss = 0.09360497\n",
      "Iteration 499, loss = 0.09344606\n",
      "Iteration 500, loss = 0.09328885\n",
      "Iteration 501, loss = 0.09313320\n",
      "Iteration 502, loss = 0.09297917\n",
      "Iteration 503, loss = 0.09282690\n",
      "Iteration 504, loss = 0.09267673\n",
      "Iteration 505, loss = 0.09252828\n",
      "Iteration 506, loss = 0.09238150\n",
      "Iteration 507, loss = 0.09223648\n",
      "Iteration 508, loss = 0.09209306\n",
      "Iteration 509, loss = 0.09195124\n",
      "Iteration 510, loss = 0.09181108\n",
      "Iteration 511, loss = 0.09167263\n",
      "Iteration 512, loss = 0.09153593\n",
      "Iteration 513, loss = 0.09140109\n",
      "Iteration 514, loss = 0.09126772\n",
      "Iteration 515, loss = 0.09113575\n",
      "Iteration 516, loss = 0.09100523\n",
      "Iteration 517, loss = 0.09087627\n",
      "Iteration 518, loss = 0.09074879\n",
      "Iteration 519, loss = 0.09062297\n",
      "Iteration 520, loss = 0.09049858\n",
      "Iteration 521, loss = 0.09037564\n",
      "Iteration 522, loss = 0.09025409\n",
      "Iteration 523, loss = 0.09013407\n",
      "Iteration 524, loss = 0.09001547\n",
      "Iteration 525, loss = 0.08989818\n",
      "Iteration 526, loss = 0.08978222\n",
      "Iteration 527, loss = 0.08966754\n",
      "Iteration 528, loss = 0.08955415\n",
      "Iteration 529, loss = 0.08944214\n",
      "Iteration 530, loss = 0.08933158\n",
      "Iteration 531, loss = 0.08922232\n",
      "Iteration 532, loss = 0.08911429\n",
      "Iteration 533, loss = 0.08900808\n",
      "Iteration 534, loss = 0.08890303\n",
      "Iteration 535, loss = 0.08879903\n",
      "Iteration 536, loss = 0.08869498\n",
      "Iteration 537, loss = 0.08859241\n",
      "Iteration 538, loss = 0.08849151\n",
      "Iteration 539, loss = 0.08839205\n",
      "Iteration 540, loss = 0.08829358\n",
      "Iteration 541, loss = 0.08819576\n",
      "Iteration 542, loss = 0.08809901\n",
      "Iteration 543, loss = 0.08800358\n",
      "Iteration 544, loss = 0.08790926\n",
      "Iteration 545, loss = 0.08781601\n",
      "Iteration 546, loss = 0.08772342\n",
      "Iteration 547, loss = 0.08763174\n",
      "Iteration 548, loss = 0.08754138\n",
      "Iteration 549, loss = 0.08745207\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65392470\n",
      "Iteration 2, loss = 1.58696255\n",
      "Iteration 3, loss = 1.52372752\n",
      "Iteration 4, loss = 1.46448076\n",
      "Iteration 5, loss = 1.40978289\n",
      "Iteration 6, loss = 1.36001658\n",
      "Iteration 7, loss = 1.31552446\n",
      "Iteration 8, loss = 1.27660874\n",
      "Iteration 9, loss = 1.24331959\n",
      "Iteration 10, loss = 1.21558815\n",
      "Iteration 11, loss = 1.19291581\n",
      "Iteration 12, loss = 1.17453185\n",
      "Iteration 13, loss = 1.15944298\n",
      "Iteration 14, loss = 1.14653470\n",
      "Iteration 15, loss = 1.13487049\n",
      "Iteration 16, loss = 1.12354390\n",
      "Iteration 17, loss = 1.11169409\n",
      "Iteration 18, loss = 1.09885015\n",
      "Iteration 19, loss = 1.08493257\n",
      "Iteration 20, loss = 1.07003044\n",
      "Iteration 21, loss = 1.05434272\n",
      "Iteration 22, loss = 1.03825691\n",
      "Iteration 23, loss = 1.02210602\n",
      "Iteration 24, loss = 1.00625894\n",
      "Iteration 25, loss = 0.99093701\n",
      "Iteration 26, loss = 0.97634915\n",
      "Iteration 27, loss = 0.96257636\n",
      "Iteration 28, loss = 0.94966909\n",
      "Iteration 29, loss = 0.93756648\n",
      "Iteration 30, loss = 0.92609988\n",
      "Iteration 31, loss = 0.91514477\n",
      "Iteration 32, loss = 0.90457131\n",
      "Iteration 33, loss = 0.89425044\n",
      "Iteration 34, loss = 0.88411157\n",
      "Iteration 35, loss = 0.87405439\n",
      "Iteration 36, loss = 0.86403765\n",
      "Iteration 37, loss = 0.85409354\n",
      "Iteration 38, loss = 0.84421078\n",
      "Iteration 39, loss = 0.83449493\n",
      "Iteration 40, loss = 0.82501681\n",
      "Iteration 41, loss = 0.81576045\n",
      "Iteration 42, loss = 0.80676571\n",
      "Iteration 43, loss = 0.79803113\n",
      "Iteration 44, loss = 0.78960213\n",
      "Iteration 45, loss = 0.78147664\n",
      "Iteration 46, loss = 0.77360338\n",
      "Iteration 47, loss = 0.76598365\n",
      "Iteration 48, loss = 0.75860812\n",
      "Iteration 49, loss = 0.75144724\n",
      "Iteration 50, loss = 0.74451864\n",
      "Iteration 51, loss = 0.73776497\n",
      "Iteration 52, loss = 0.73117654\n",
      "Iteration 53, loss = 0.72476562\n",
      "Iteration 54, loss = 0.71849685\n",
      "Iteration 55, loss = 0.71234772\n",
      "Iteration 56, loss = 0.70631672\n",
      "Iteration 57, loss = 0.70039712\n",
      "Iteration 58, loss = 0.69458274\n",
      "Iteration 59, loss = 0.68884250\n",
      "Iteration 60, loss = 0.68318356\n",
      "Iteration 61, loss = 0.67765389\n",
      "Iteration 62, loss = 0.67224450\n",
      "Iteration 63, loss = 0.66693734\n",
      "Iteration 64, loss = 0.66180065\n",
      "Iteration 65, loss = 0.65688378\n",
      "Iteration 66, loss = 0.65214630\n",
      "Iteration 67, loss = 0.64760043\n",
      "Iteration 68, loss = 0.64317219\n",
      "Iteration 69, loss = 0.63883537\n",
      "Iteration 70, loss = 0.63459284\n",
      "Iteration 71, loss = 0.63043629\n",
      "Iteration 72, loss = 0.62636199\n",
      "Iteration 73, loss = 0.62234494\n",
      "Iteration 74, loss = 0.61838768\n",
      "Iteration 75, loss = 0.61449169\n",
      "Iteration 76, loss = 0.61065358\n",
      "Iteration 77, loss = 0.60687656\n",
      "Iteration 78, loss = 0.60316527\n",
      "Iteration 79, loss = 0.59951897\n",
      "Iteration 80, loss = 0.59593411\n",
      "Iteration 81, loss = 0.59241083\n",
      "Iteration 82, loss = 0.58894716\n",
      "Iteration 83, loss = 0.58553964\n",
      "Iteration 84, loss = 0.58218437\n",
      "Iteration 85, loss = 0.57888086\n",
      "Iteration 86, loss = 0.57562675\n",
      "Iteration 87, loss = 0.57241907\n",
      "Iteration 88, loss = 0.56925677\n",
      "Iteration 89, loss = 0.56613876\n",
      "Iteration 90, loss = 0.56306420\n",
      "Iteration 91, loss = 0.56003232\n",
      "Iteration 92, loss = 0.55704261\n",
      "Iteration 93, loss = 0.55409383\n",
      "Iteration 94, loss = 0.55118431\n",
      "Iteration 95, loss = 0.54831301\n",
      "Iteration 96, loss = 0.54547902\n",
      "Iteration 97, loss = 0.54267953\n",
      "Iteration 98, loss = 0.53991407\n",
      "Iteration 99, loss = 0.53718333\n",
      "Iteration 100, loss = 0.53448545\n",
      "Iteration 101, loss = 0.53181852\n",
      "Iteration 102, loss = 0.52918170\n",
      "Iteration 103, loss = 0.52657018\n",
      "Iteration 104, loss = 0.52398723\n",
      "Iteration 105, loss = 0.52143103\n",
      "Iteration 106, loss = 0.51890123\n",
      "Iteration 107, loss = 0.51639637\n",
      "Iteration 108, loss = 0.51391640\n",
      "Iteration 109, loss = 0.51145891\n",
      "Iteration 110, loss = 0.50902532\n",
      "Iteration 111, loss = 0.50661338\n",
      "Iteration 112, loss = 0.50422228\n",
      "Iteration 113, loss = 0.50185256\n",
      "Iteration 114, loss = 0.49950393\n",
      "Iteration 115, loss = 0.49717540\n",
      "Iteration 116, loss = 0.49486793\n",
      "Iteration 117, loss = 0.49258094\n",
      "Iteration 118, loss = 0.49031400\n",
      "Iteration 119, loss = 0.48806651\n",
      "Iteration 120, loss = 0.48583396\n",
      "Iteration 121, loss = 0.48360666\n",
      "Iteration 122, loss = 0.48137693\n",
      "Iteration 123, loss = 0.47918119\n",
      "Iteration 124, loss = 0.47702951\n",
      "Iteration 125, loss = 0.47485742\n",
      "Iteration 126, loss = 0.47273431\n",
      "Iteration 127, loss = 0.47061917\n",
      "Iteration 128, loss = 0.46851167\n",
      "Iteration 129, loss = 0.46641158\n",
      "Iteration 130, loss = 0.46432051\n",
      "Iteration 131, loss = 0.46223997\n",
      "Iteration 132, loss = 0.46018498\n",
      "Iteration 133, loss = 0.45814306\n",
      "Iteration 134, loss = 0.45610058\n",
      "Iteration 135, loss = 0.45407578\n",
      "Iteration 136, loss = 0.45206933\n",
      "Iteration 137, loss = 0.45007033\n",
      "Iteration 138, loss = 0.44807642\n",
      "Iteration 139, loss = 0.44609415\n",
      "Iteration 140, loss = 0.44412951\n",
      "Iteration 141, loss = 0.44217274\n",
      "Iteration 142, loss = 0.44022383\n",
      "Iteration 143, loss = 0.43828310\n",
      "Iteration 144, loss = 0.43635151\n",
      "Iteration 145, loss = 0.43442912\n",
      "Iteration 146, loss = 0.43251769\n",
      "Iteration 147, loss = 0.43061596\n",
      "Iteration 148, loss = 0.42871907\n",
      "Iteration 149, loss = 0.42683224\n",
      "Iteration 150, loss = 0.42495370\n",
      "Iteration 151, loss = 0.42310121\n",
      "Iteration 152, loss = 0.42126050\n",
      "Iteration 153, loss = 0.41943113\n",
      "Iteration 154, loss = 0.41761895\n",
      "Iteration 155, loss = 0.41582155\n",
      "Iteration 156, loss = 0.41402936\n",
      "Iteration 157, loss = 0.41225574\n",
      "Iteration 158, loss = 0.41048772\n",
      "Iteration 159, loss = 0.40870872\n",
      "Iteration 160, loss = 0.40690683\n",
      "Iteration 161, loss = 0.40510268\n",
      "Iteration 162, loss = 0.40331912\n",
      "Iteration 163, loss = 0.40154702\n",
      "Iteration 164, loss = 0.39978538\n",
      "Iteration 165, loss = 0.39802588\n",
      "Iteration 166, loss = 0.39626955\n",
      "Iteration 167, loss = 0.39452112\n",
      "Iteration 168, loss = 0.39278418\n",
      "Iteration 169, loss = 0.39105331\n",
      "Iteration 170, loss = 0.38932754\n",
      "Iteration 171, loss = 0.38760508\n",
      "Iteration 172, loss = 0.38588471\n",
      "Iteration 173, loss = 0.38416725\n",
      "Iteration 174, loss = 0.38245375\n",
      "Iteration 175, loss = 0.38074305\n",
      "Iteration 176, loss = 0.37903626\n",
      "Iteration 177, loss = 0.37733475\n",
      "Iteration 178, loss = 0.37564207\n",
      "Iteration 179, loss = 0.37395487\n",
      "Iteration 180, loss = 0.37227501\n",
      "Iteration 181, loss = 0.37059915\n",
      "Iteration 182, loss = 0.36892792\n",
      "Iteration 183, loss = 0.36726170\n",
      "Iteration 184, loss = 0.36560049\n",
      "Iteration 185, loss = 0.36394448\n",
      "Iteration 186, loss = 0.36229405\n",
      "Iteration 187, loss = 0.36064917\n",
      "Iteration 188, loss = 0.35900954\n",
      "Iteration 189, loss = 0.35737563\n",
      "Iteration 190, loss = 0.35574711\n",
      "Iteration 191, loss = 0.35412448\n",
      "Iteration 192, loss = 0.35250768\n",
      "Iteration 193, loss = 0.35089775\n",
      "Iteration 194, loss = 0.34929371\n",
      "Iteration 195, loss = 0.34769591\n",
      "Iteration 196, loss = 0.34610377\n",
      "Iteration 197, loss = 0.34451730\n",
      "Iteration 198, loss = 0.34293767\n",
      "Iteration 199, loss = 0.34136408\n",
      "Iteration 200, loss = 0.33979571\n",
      "Iteration 201, loss = 0.33822943\n",
      "Iteration 202, loss = 0.33665743\n",
      "Iteration 203, loss = 0.33509184\n",
      "Iteration 204, loss = 0.33353705\n",
      "Iteration 205, loss = 0.33198622\n",
      "Iteration 206, loss = 0.33043670\n",
      "Iteration 207, loss = 0.32888771\n",
      "Iteration 208, loss = 0.32733929\n",
      "Iteration 209, loss = 0.32579191\n",
      "Iteration 210, loss = 0.32424500\n",
      "Iteration 211, loss = 0.32269840\n",
      "Iteration 212, loss = 0.32115172\n",
      "Iteration 213, loss = 0.31960561\n",
      "Iteration 214, loss = 0.31805984\n",
      "Iteration 215, loss = 0.31651423\n",
      "Iteration 216, loss = 0.31496908\n",
      "Iteration 217, loss = 0.31342498\n",
      "Iteration 218, loss = 0.31188142\n",
      "Iteration 219, loss = 0.31033970\n",
      "Iteration 220, loss = 0.30879940\n",
      "Iteration 221, loss = 0.30726004\n",
      "Iteration 222, loss = 0.30572270\n",
      "Iteration 223, loss = 0.30418786\n",
      "Iteration 224, loss = 0.30265513\n",
      "Iteration 225, loss = 0.30112506\n",
      "Iteration 226, loss = 0.29959798\n",
      "Iteration 227, loss = 0.29807368\n",
      "Iteration 228, loss = 0.29655222\n",
      "Iteration 229, loss = 0.29503385\n",
      "Iteration 230, loss = 0.29351900\n",
      "Iteration 231, loss = 0.29200818\n",
      "Iteration 232, loss = 0.29050125\n",
      "Iteration 233, loss = 0.28899852\n",
      "Iteration 234, loss = 0.28750004\n",
      "Iteration 235, loss = 0.28600619\n",
      "Iteration 236, loss = 0.28451754\n",
      "Iteration 237, loss = 0.28303355\n",
      "Iteration 238, loss = 0.28155475\n",
      "Iteration 239, loss = 0.28008153\n",
      "Iteration 240, loss = 0.27861376\n",
      "Iteration 241, loss = 0.27715172\n",
      "Iteration 242, loss = 0.27569609\n",
      "Iteration 243, loss = 0.27424621\n",
      "Iteration 244, loss = 0.27280250\n",
      "Iteration 245, loss = 0.27136530\n",
      "Iteration 246, loss = 0.26993455\n",
      "Iteration 247, loss = 0.26850529\n",
      "Iteration 248, loss = 0.26708058\n",
      "Iteration 249, loss = 0.26566632\n",
      "Iteration 250, loss = 0.26425579\n",
      "Iteration 251, loss = 0.26284933\n",
      "Iteration 252, loss = 0.26144688\n",
      "Iteration 253, loss = 0.26004841\n",
      "Iteration 254, loss = 0.25865479\n",
      "Iteration 255, loss = 0.25726466\n",
      "Iteration 256, loss = 0.25587798\n",
      "Iteration 257, loss = 0.25449469\n",
      "Iteration 258, loss = 0.25311429\n",
      "Iteration 259, loss = 0.25173627\n",
      "Iteration 260, loss = 0.25035987\n",
      "Iteration 261, loss = 0.24898372\n",
      "Iteration 262, loss = 0.24760238\n",
      "Iteration 263, loss = 0.24622099\n",
      "Iteration 264, loss = 0.24484643\n",
      "Iteration 265, loss = 0.24347208\n",
      "Iteration 266, loss = 0.24209779\n",
      "Iteration 267, loss = 0.24072369\n",
      "Iteration 268, loss = 0.23935007\n",
      "Iteration 269, loss = 0.23797713\n",
      "Iteration 270, loss = 0.23660518\n",
      "Iteration 271, loss = 0.23523461\n",
      "Iteration 272, loss = 0.23386560\n",
      "Iteration 273, loss = 0.23249848\n",
      "Iteration 274, loss = 0.23113367\n",
      "Iteration 275, loss = 0.22977150\n",
      "Iteration 276, loss = 0.22841220\n",
      "Iteration 277, loss = 0.22705629\n",
      "Iteration 278, loss = 0.22570408\n",
      "Iteration 279, loss = 0.22435590\n",
      "Iteration 280, loss = 0.22301210\n",
      "Iteration 281, loss = 0.22167324\n",
      "Iteration 282, loss = 0.22033940\n",
      "Iteration 283, loss = 0.21901107\n",
      "Iteration 284, loss = 0.21768859\n",
      "Iteration 285, loss = 0.21637221\n",
      "Iteration 286, loss = 0.21506239\n",
      "Iteration 287, loss = 0.21375927\n",
      "Iteration 288, loss = 0.21246332\n",
      "Iteration 289, loss = 0.21117441\n",
      "Iteration 290, loss = 0.20989356\n",
      "Iteration 291, loss = 0.20861988\n",
      "Iteration 292, loss = 0.20735379\n",
      "Iteration 293, loss = 0.20609613\n",
      "Iteration 294, loss = 0.20484753\n",
      "Iteration 295, loss = 0.20360742\n",
      "Iteration 296, loss = 0.20237407\n",
      "Iteration 297, loss = 0.20114942\n",
      "Iteration 298, loss = 0.19993949\n",
      "Iteration 299, loss = 0.19873025\n",
      "Iteration 300, loss = 0.19753814\n",
      "Iteration 301, loss = 0.19635388\n",
      "Iteration 302, loss = 0.19517611\n",
      "Iteration 303, loss = 0.19400584\n",
      "Iteration 304, loss = 0.19284593\n",
      "Iteration 305, loss = 0.19170417\n",
      "Iteration 306, loss = 0.19056140\n",
      "Iteration 307, loss = 0.18943392\n",
      "Iteration 308, loss = 0.18831818\n",
      "Iteration 309, loss = 0.18721140\n",
      "Iteration 310, loss = 0.18611327\n",
      "Iteration 311, loss = 0.18502380\n",
      "Iteration 312, loss = 0.18394363\n",
      "Iteration 313, loss = 0.18287754\n",
      "Iteration 314, loss = 0.18181996\n",
      "Iteration 315, loss = 0.18077043\n",
      "Iteration 316, loss = 0.17973364\n",
      "Iteration 317, loss = 0.17870688\n",
      "Iteration 318, loss = 0.17768871\n",
      "Iteration 319, loss = 0.17667994\n",
      "Iteration 320, loss = 0.17568255\n",
      "Iteration 321, loss = 0.17469537\n",
      "Iteration 322, loss = 0.17371715\n",
      "Iteration 323, loss = 0.17274915\n",
      "Iteration 324, loss = 0.17179117\n",
      "Iteration 325, loss = 0.17084343\n",
      "Iteration 326, loss = 0.16990509\n",
      "Iteration 327, loss = 0.16897602\n",
      "Iteration 328, loss = 0.16805685\n",
      "Iteration 329, loss = 0.16714728\n",
      "Iteration 330, loss = 0.16624740\n",
      "Iteration 331, loss = 0.16535697\n",
      "Iteration 332, loss = 0.16447589\n",
      "Iteration 333, loss = 0.16360409\n",
      "Iteration 334, loss = 0.16274155\n",
      "Iteration 335, loss = 0.16188825\n",
      "Iteration 336, loss = 0.16104399\n",
      "Iteration 337, loss = 0.16020806\n",
      "Iteration 338, loss = 0.15938049\n",
      "Iteration 339, loss = 0.15856106\n",
      "Iteration 340, loss = 0.15774941\n",
      "Iteration 341, loss = 0.15694543\n",
      "Iteration 342, loss = 0.15614896\n",
      "Iteration 343, loss = 0.15535963\n",
      "Iteration 344, loss = 0.15457706\n",
      "Iteration 345, loss = 0.15380107\n",
      "Iteration 346, loss = 0.15303136\n",
      "Iteration 347, loss = 0.15226796\n",
      "Iteration 348, loss = 0.15151091\n",
      "Iteration 349, loss = 0.15075968\n",
      "Iteration 350, loss = 0.15001436\n",
      "Iteration 351, loss = 0.14927498\n",
      "Iteration 352, loss = 0.14854166\n",
      "Iteration 353, loss = 0.14781447\n",
      "Iteration 354, loss = 0.14709327\n",
      "Iteration 355, loss = 0.14637830\n",
      "Iteration 356, loss = 0.14566995\n",
      "Iteration 357, loss = 0.14496660\n",
      "Iteration 358, loss = 0.14426968\n",
      "Iteration 359, loss = 0.14357864\n",
      "Iteration 360, loss = 0.14289308\n",
      "Iteration 361, loss = 0.14221357\n",
      "Iteration 362, loss = 0.14153976\n",
      "Iteration 363, loss = 0.14087146\n",
      "Iteration 364, loss = 0.14020893\n",
      "Iteration 365, loss = 0.13955196\n",
      "Iteration 366, loss = 0.13890041\n",
      "Iteration 367, loss = 0.13825492\n",
      "Iteration 368, loss = 0.13761458\n",
      "Iteration 369, loss = 0.13697995\n",
      "Iteration 370, loss = 0.13635073\n",
      "Iteration 371, loss = 0.13572695\n",
      "Iteration 372, loss = 0.13510872\n",
      "Iteration 373, loss = 0.13449645\n",
      "Iteration 374, loss = 0.13389015\n",
      "Iteration 375, loss = 0.13328958\n",
      "Iteration 376, loss = 0.13269457\n",
      "Iteration 377, loss = 0.13210573\n",
      "Iteration 378, loss = 0.13152225\n",
      "Iteration 379, loss = 0.13094514\n",
      "Iteration 380, loss = 0.13037340\n",
      "Iteration 381, loss = 0.12980713\n",
      "Iteration 382, loss = 0.12924704\n",
      "Iteration 383, loss = 0.12869295\n",
      "Iteration 384, loss = 0.12814462\n",
      "Iteration 385, loss = 0.12760202\n",
      "Iteration 386, loss = 0.12706529\n",
      "Iteration 387, loss = 0.12653423\n",
      "Iteration 388, loss = 0.12600888\n",
      "Iteration 389, loss = 0.12548933\n",
      "Iteration 390, loss = 0.12497542\n",
      "Iteration 391, loss = 0.12446725\n",
      "Iteration 392, loss = 0.12396472\n",
      "Iteration 393, loss = 0.12346786\n",
      "Iteration 394, loss = 0.12297664\n",
      "Iteration 395, loss = 0.12249105\n",
      "Iteration 396, loss = 0.12201096\n",
      "Iteration 397, loss = 0.12153654\n",
      "Iteration 398, loss = 0.12106756\n",
      "Iteration 399, loss = 0.12060400\n",
      "Iteration 400, loss = 0.12014541\n",
      "Iteration 401, loss = 0.11969196\n",
      "Iteration 402, loss = 0.11924340\n",
      "Iteration 403, loss = 0.11879986\n",
      "Iteration 404, loss = 0.11836080\n",
      "Iteration 405, loss = 0.11792668\n",
      "Iteration 406, loss = 0.11749697\n",
      "Iteration 407, loss = 0.11707153\n",
      "Iteration 408, loss = 0.11665018\n",
      "Iteration 409, loss = 0.11623269\n",
      "Iteration 410, loss = 0.11581899\n",
      "Iteration 411, loss = 0.11540894\n",
      "Iteration 412, loss = 0.11500241\n",
      "Iteration 413, loss = 0.11459936\n",
      "Iteration 414, loss = 0.11419977\n",
      "Iteration 415, loss = 0.11380356\n",
      "Iteration 416, loss = 0.11341065\n",
      "Iteration 417, loss = 0.11302109\n",
      "Iteration 418, loss = 0.11263474\n",
      "Iteration 419, loss = 0.11225165\n",
      "Iteration 420, loss = 0.11187179\n",
      "Iteration 421, loss = 0.11149523\n",
      "Iteration 422, loss = 0.11112197\n",
      "Iteration 423, loss = 0.11075191\n",
      "Iteration 424, loss = 0.11038503\n",
      "Iteration 425, loss = 0.11002125\n",
      "Iteration 426, loss = 0.10966094\n",
      "Iteration 427, loss = 0.10930385\n",
      "Iteration 428, loss = 0.10895035\n",
      "Iteration 429, loss = 0.10859998\n",
      "Iteration 430, loss = 0.10825297\n",
      "Iteration 431, loss = 0.10790912\n",
      "Iteration 432, loss = 0.10756851\n",
      "Iteration 433, loss = 0.10723096\n",
      "Iteration 434, loss = 0.10689658\n",
      "Iteration 435, loss = 0.10656551\n",
      "Iteration 436, loss = 0.10623764\n",
      "Iteration 437, loss = 0.10591273\n",
      "Iteration 438, loss = 0.10559071\n",
      "Iteration 439, loss = 0.10527178\n",
      "Iteration 440, loss = 0.10495582\n",
      "Iteration 441, loss = 0.10464259\n",
      "Iteration 442, loss = 0.10433210\n",
      "Iteration 443, loss = 0.10402425\n",
      "Iteration 444, loss = 0.10371992\n",
      "Iteration 445, loss = 0.10341861\n",
      "Iteration 446, loss = 0.10312015\n",
      "Iteration 447, loss = 0.10282447\n",
      "Iteration 448, loss = 0.10253151\n",
      "Iteration 449, loss = 0.10224151\n",
      "Iteration 450, loss = 0.10195391\n",
      "Iteration 451, loss = 0.10166852\n",
      "Iteration 452, loss = 0.10138582\n",
      "Iteration 453, loss = 0.10110635\n",
      "Iteration 454, loss = 0.10082829\n",
      "Iteration 455, loss = 0.10055305\n",
      "Iteration 456, loss = 0.10027995\n",
      "Iteration 457, loss = 0.10000913\n",
      "Iteration 458, loss = 0.09974016\n",
      "Iteration 459, loss = 0.09947333\n",
      "Iteration 460, loss = 0.09920854\n",
      "Iteration 461, loss = 0.09894551\n",
      "Iteration 462, loss = 0.09868438\n",
      "Iteration 463, loss = 0.09842512\n",
      "Iteration 464, loss = 0.09816763\n",
      "Iteration 465, loss = 0.09791204\n",
      "Iteration 466, loss = 0.09765820\n",
      "Iteration 467, loss = 0.09740626\n",
      "Iteration 468, loss = 0.09715592\n",
      "Iteration 469, loss = 0.09690736\n",
      "Iteration 470, loss = 0.09666068\n",
      "Iteration 471, loss = 0.09641593\n",
      "Iteration 472, loss = 0.09617316\n",
      "Iteration 473, loss = 0.09593212\n",
      "Iteration 474, loss = 0.09569285\n",
      "Iteration 475, loss = 0.09545545\n",
      "Iteration 476, loss = 0.09521982\n",
      "Iteration 477, loss = 0.09498613\n",
      "Iteration 478, loss = 0.09475445\n",
      "Iteration 479, loss = 0.09452475\n",
      "Iteration 480, loss = 0.09429703\n",
      "Iteration 481, loss = 0.09407115\n",
      "Iteration 482, loss = 0.09384731\n",
      "Iteration 483, loss = 0.09362533\n",
      "Iteration 484, loss = 0.09340534\n",
      "Iteration 485, loss = 0.09318736\n",
      "Iteration 486, loss = 0.09297135\n",
      "Iteration 487, loss = 0.09275731\n",
      "Iteration 488, loss = 0.09254535\n",
      "Iteration 489, loss = 0.09233543\n",
      "Iteration 490, loss = 0.09212753\n",
      "Iteration 491, loss = 0.09192172\n",
      "Iteration 492, loss = 0.09171798\n",
      "Iteration 493, loss = 0.09151636\n",
      "Iteration 494, loss = 0.09131683\n",
      "Iteration 495, loss = 0.09111938\n",
      "Iteration 496, loss = 0.09092401\n",
      "Iteration 497, loss = 0.09073070\n",
      "Iteration 498, loss = 0.09053965\n",
      "Iteration 499, loss = 0.09035069\n",
      "Iteration 500, loss = 0.09016377\n",
      "Iteration 501, loss = 0.08997904\n",
      "Iteration 502, loss = 0.08979651\n",
      "Iteration 503, loss = 0.08961611\n",
      "Iteration 504, loss = 0.08943796\n",
      "Iteration 505, loss = 0.08926190\n",
      "Iteration 506, loss = 0.08908794\n",
      "Iteration 507, loss = 0.08891604\n",
      "Iteration 508, loss = 0.08874636\n",
      "Iteration 509, loss = 0.08857884\n",
      "Iteration 510, loss = 0.08841345\n",
      "Iteration 511, loss = 0.08825018\n",
      "Iteration 512, loss = 0.08808895\n",
      "Iteration 513, loss = 0.08792974\n",
      "Iteration 514, loss = 0.08777256\n",
      "Iteration 515, loss = 0.08761749\n",
      "Iteration 516, loss = 0.08746432\n",
      "Iteration 517, loss = 0.08731311\n",
      "Iteration 518, loss = 0.08716392\n",
      "Iteration 519, loss = 0.08701669\n",
      "Iteration 520, loss = 0.08687159\n",
      "Iteration 521, loss = 0.08672844\n",
      "Iteration 522, loss = 0.08658720\n",
      "Iteration 523, loss = 0.08644783\n",
      "Iteration 524, loss = 0.08631040\n",
      "Iteration 525, loss = 0.08617488\n",
      "Iteration 526, loss = 0.08604122\n",
      "Iteration 527, loss = 0.08590944\n",
      "Iteration 528, loss = 0.08577946\n",
      "Iteration 529, loss = 0.08565119\n",
      "Iteration 530, loss = 0.08552466\n",
      "Iteration 531, loss = 0.08539993\n",
      "Iteration 532, loss = 0.08527693\n",
      "Iteration 533, loss = 0.08515555\n",
      "Iteration 534, loss = 0.08503575\n",
      "Iteration 535, loss = 0.08491756\n",
      "Iteration 536, loss = 0.08480101\n",
      "Iteration 537, loss = 0.08468606\n",
      "Iteration 538, loss = 0.08457275\n",
      "Iteration 539, loss = 0.08446090\n",
      "Iteration 540, loss = 0.08435051\n",
      "Iteration 541, loss = 0.08424158\n",
      "Iteration 542, loss = 0.08413412\n",
      "Iteration 543, loss = 0.08402810\n",
      "Iteration 544, loss = 0.08392351\n",
      "Iteration 545, loss = 0.08382031\n",
      "Iteration 546, loss = 0.08371847\n",
      "Iteration 547, loss = 0.08361794\n",
      "Iteration 548, loss = 0.08351871\n",
      "Iteration 549, loss = 0.08342080\n",
      "Iteration 550, loss = 0.08332419\n",
      "Iteration 551, loss = 0.08322899\n",
      "Iteration 552, loss = 0.08313504\n",
      "Iteration 553, loss = 0.08304234\n",
      "Iteration 554, loss = 0.08295092\n",
      "Iteration 555, loss = 0.08286059\n",
      "Iteration 556, loss = 0.08277137\n",
      "Iteration 557, loss = 0.08268326\n",
      "Iteration 558, loss = 0.08259624\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66585275\n",
      "Iteration 2, loss = 1.59825295\n",
      "Iteration 3, loss = 1.53451783\n",
      "Iteration 4, loss = 1.47490800\n",
      "Iteration 5, loss = 1.41984966\n",
      "Iteration 6, loss = 1.36964392\n",
      "Iteration 7, loss = 1.32459963\n",
      "Iteration 8, loss = 1.28506514\n",
      "Iteration 9, loss = 1.25102737\n",
      "Iteration 10, loss = 1.22242438\n",
      "Iteration 11, loss = 1.19875806\n",
      "Iteration 12, loss = 1.17937983\n",
      "Iteration 13, loss = 1.16337714\n",
      "Iteration 14, loss = 1.14967549\n",
      "Iteration 15, loss = 1.13733352\n",
      "Iteration 16, loss = 1.12537235\n",
      "Iteration 17, loss = 1.11299468\n",
      "Iteration 18, loss = 1.09972199\n",
      "Iteration 19, loss = 1.08543645\n",
      "Iteration 20, loss = 1.07019443\n",
      "Iteration 21, loss = 1.05416772\n",
      "Iteration 22, loss = 1.03762675\n",
      "Iteration 23, loss = 1.02100303\n",
      "Iteration 24, loss = 1.00460144\n",
      "Iteration 25, loss = 0.98870024\n",
      "Iteration 26, loss = 0.97357882\n",
      "Iteration 27, loss = 0.95927556\n",
      "Iteration 28, loss = 0.94583076\n",
      "Iteration 29, loss = 0.93323125\n",
      "Iteration 30, loss = 0.92132930\n",
      "Iteration 31, loss = 0.91000152\n",
      "Iteration 32, loss = 0.89907659\n",
      "Iteration 33, loss = 0.88844346\n",
      "Iteration 34, loss = 0.87804372\n",
      "Iteration 35, loss = 0.86775247\n",
      "Iteration 36, loss = 0.85750552\n",
      "Iteration 37, loss = 0.84740095\n",
      "Iteration 38, loss = 0.83747924\n",
      "Iteration 39, loss = 0.82773389\n",
      "Iteration 40, loss = 0.81826689\n",
      "Iteration 41, loss = 0.80908094\n",
      "Iteration 42, loss = 0.80019198\n",
      "Iteration 43, loss = 0.79157883\n",
      "Iteration 44, loss = 0.78323907\n",
      "Iteration 45, loss = 0.77516881\n",
      "Iteration 46, loss = 0.76734769\n",
      "Iteration 47, loss = 0.75975135\n",
      "Iteration 48, loss = 0.75236305\n",
      "Iteration 49, loss = 0.74517908\n",
      "Iteration 50, loss = 0.73821368\n",
      "Iteration 51, loss = 0.73141942\n",
      "Iteration 52, loss = 0.72478488\n",
      "Iteration 53, loss = 0.71829979\n",
      "Iteration 54, loss = 0.71195729\n",
      "Iteration 55, loss = 0.70575976\n",
      "Iteration 56, loss = 0.69968902\n",
      "Iteration 57, loss = 0.69373246\n",
      "Iteration 58, loss = 0.68789632\n",
      "Iteration 59, loss = 0.68215392\n",
      "Iteration 60, loss = 0.67647396\n",
      "Iteration 61, loss = 0.67088763\n",
      "Iteration 62, loss = 0.66541938\n",
      "Iteration 63, loss = 0.66008020\n",
      "Iteration 64, loss = 0.65485289\n",
      "Iteration 65, loss = 0.64976455\n",
      "Iteration 66, loss = 0.64491272\n",
      "Iteration 67, loss = 0.64026625\n",
      "Iteration 68, loss = 0.63576461\n",
      "Iteration 69, loss = 0.63137409\n",
      "Iteration 70, loss = 0.62707242\n",
      "Iteration 71, loss = 0.62285100\n",
      "Iteration 72, loss = 0.61870675\n",
      "Iteration 73, loss = 0.61463189\n",
      "Iteration 74, loss = 0.61061719\n",
      "Iteration 75, loss = 0.60665549\n",
      "Iteration 76, loss = 0.60274138\n",
      "Iteration 77, loss = 0.59886955\n",
      "Iteration 78, loss = 0.59501563\n",
      "Iteration 79, loss = 0.59117445\n",
      "Iteration 80, loss = 0.58740716\n",
      "Iteration 81, loss = 0.58361200\n",
      "Iteration 82, loss = 0.57993664\n",
      "Iteration 83, loss = 0.57636095\n",
      "Iteration 84, loss = 0.57286672\n",
      "Iteration 85, loss = 0.56942763\n",
      "Iteration 86, loss = 0.56603280\n",
      "Iteration 87, loss = 0.56267825\n",
      "Iteration 88, loss = 0.55936338\n",
      "Iteration 89, loss = 0.55608956\n",
      "Iteration 90, loss = 0.55285861\n",
      "Iteration 91, loss = 0.54967065\n",
      "Iteration 92, loss = 0.54652715\n",
      "Iteration 93, loss = 0.54342561\n",
      "Iteration 94, loss = 0.54036296\n",
      "Iteration 95, loss = 0.53733864\n",
      "Iteration 96, loss = 0.53435180\n",
      "Iteration 97, loss = 0.53139600\n",
      "Iteration 98, loss = 0.52847404\n",
      "Iteration 99, loss = 0.52558659\n",
      "Iteration 100, loss = 0.52273426\n",
      "Iteration 101, loss = 0.51991638\n",
      "Iteration 102, loss = 0.51713139\n",
      "Iteration 103, loss = 0.51437800\n",
      "Iteration 104, loss = 0.51165471\n",
      "Iteration 105, loss = 0.50896016\n",
      "Iteration 106, loss = 0.50629299\n",
      "Iteration 107, loss = 0.50365215\n",
      "Iteration 108, loss = 0.50103752\n",
      "Iteration 109, loss = 0.49844899\n",
      "Iteration 110, loss = 0.49588592\n",
      "Iteration 111, loss = 0.49334814\n",
      "Iteration 112, loss = 0.49083512\n",
      "Iteration 113, loss = 0.48834610\n",
      "Iteration 114, loss = 0.48588000\n",
      "Iteration 115, loss = 0.48343580\n",
      "Iteration 116, loss = 0.48102560\n",
      "Iteration 117, loss = 0.47865063\n",
      "Iteration 118, loss = 0.47632259\n",
      "Iteration 119, loss = 0.47403632\n",
      "Iteration 120, loss = 0.47175369\n",
      "Iteration 121, loss = 0.46948496\n",
      "Iteration 122, loss = 0.46726840\n",
      "Iteration 123, loss = 0.46500670\n",
      "Iteration 124, loss = 0.46277094\n",
      "Iteration 125, loss = 0.46053749\n",
      "Iteration 126, loss = 0.45831465\n",
      "Iteration 127, loss = 0.45610697\n",
      "Iteration 128, loss = 0.45391270\n",
      "Iteration 129, loss = 0.45173737\n",
      "Iteration 130, loss = 0.44958000\n",
      "Iteration 131, loss = 0.44745166\n",
      "Iteration 132, loss = 0.44534888\n",
      "Iteration 133, loss = 0.44323493\n",
      "Iteration 134, loss = 0.44113546\n",
      "Iteration 135, loss = 0.43905946\n",
      "Iteration 136, loss = 0.43698816\n",
      "Iteration 137, loss = 0.43492056\n",
      "Iteration 138, loss = 0.43285608\n",
      "Iteration 139, loss = 0.43079459\n",
      "Iteration 140, loss = 0.42874546\n",
      "Iteration 141, loss = 0.42671906\n",
      "Iteration 142, loss = 0.42469566\n",
      "Iteration 143, loss = 0.42267421\n",
      "Iteration 144, loss = 0.42067040\n",
      "Iteration 145, loss = 0.41868719\n",
      "Iteration 146, loss = 0.41670937\n",
      "Iteration 147, loss = 0.41473779\n",
      "Iteration 148, loss = 0.41277130\n",
      "Iteration 149, loss = 0.41081455\n",
      "Iteration 150, loss = 0.40886368\n",
      "Iteration 151, loss = 0.40691149\n",
      "Iteration 152, loss = 0.40495925\n",
      "Iteration 153, loss = 0.40301022\n",
      "Iteration 154, loss = 0.40106380\n",
      "Iteration 155, loss = 0.39912638\n",
      "Iteration 156, loss = 0.39721337\n",
      "Iteration 157, loss = 0.39530584\n",
      "Iteration 158, loss = 0.39339266\n",
      "Iteration 159, loss = 0.39148103\n",
      "Iteration 160, loss = 0.38958187\n",
      "Iteration 161, loss = 0.38769513\n",
      "Iteration 162, loss = 0.38581108\n",
      "Iteration 163, loss = 0.38393002\n",
      "Iteration 164, loss = 0.38205100\n",
      "Iteration 165, loss = 0.38017684\n",
      "Iteration 166, loss = 0.37831141\n",
      "Iteration 167, loss = 0.37645508\n",
      "Iteration 168, loss = 0.37460459\n",
      "Iteration 169, loss = 0.37275751\n",
      "Iteration 170, loss = 0.37091435\n",
      "Iteration 171, loss = 0.36907584\n",
      "Iteration 172, loss = 0.36724400\n",
      "Iteration 173, loss = 0.36541803\n",
      "Iteration 174, loss = 0.36359865\n",
      "Iteration 175, loss = 0.36178405\n",
      "Iteration 176, loss = 0.35997347\n",
      "Iteration 177, loss = 0.35816820\n",
      "Iteration 178, loss = 0.35636984\n",
      "Iteration 179, loss = 0.35457673\n",
      "Iteration 180, loss = 0.35278919\n",
      "Iteration 181, loss = 0.35100774\n",
      "Iteration 182, loss = 0.34923202\n",
      "Iteration 183, loss = 0.34746130\n",
      "Iteration 184, loss = 0.34569700\n",
      "Iteration 185, loss = 0.34393920\n",
      "Iteration 186, loss = 0.34218701\n",
      "Iteration 187, loss = 0.34044035\n",
      "Iteration 188, loss = 0.33869989\n",
      "Iteration 189, loss = 0.33696586\n",
      "Iteration 190, loss = 0.33523806\n",
      "Iteration 191, loss = 0.33351626\n",
      "Iteration 192, loss = 0.33180020\n",
      "Iteration 193, loss = 0.33009005\n",
      "Iteration 194, loss = 0.32838641\n",
      "Iteration 195, loss = 0.32668941\n",
      "Iteration 196, loss = 0.32499839\n",
      "Iteration 197, loss = 0.32331346\n",
      "Iteration 198, loss = 0.32163478\n",
      "Iteration 199, loss = 0.31996270\n",
      "Iteration 200, loss = 0.31829622\n",
      "Iteration 201, loss = 0.31663067\n",
      "Iteration 202, loss = 0.31496015\n",
      "Iteration 203, loss = 0.31329801\n",
      "Iteration 204, loss = 0.31164617\n",
      "Iteration 205, loss = 0.30999653\n",
      "Iteration 206, loss = 0.30834781\n",
      "Iteration 207, loss = 0.30669956\n",
      "Iteration 208, loss = 0.30505187\n",
      "Iteration 209, loss = 0.30340503\n",
      "Iteration 210, loss = 0.30175883\n",
      "Iteration 211, loss = 0.30011314\n",
      "Iteration 212, loss = 0.29846799\n",
      "Iteration 213, loss = 0.29682353\n",
      "Iteration 214, loss = 0.29517982\n",
      "Iteration 215, loss = 0.29353688\n",
      "Iteration 216, loss = 0.29189491\n",
      "Iteration 217, loss = 0.29025459\n",
      "Iteration 218, loss = 0.28861543\n",
      "Iteration 219, loss = 0.28697758\n",
      "Iteration 220, loss = 0.28534146\n",
      "Iteration 221, loss = 0.28370782\n",
      "Iteration 222, loss = 0.28207655\n",
      "Iteration 223, loss = 0.28044748\n",
      "Iteration 224, loss = 0.27882110\n",
      "Iteration 225, loss = 0.27719809\n",
      "Iteration 226, loss = 0.27557937\n",
      "Iteration 227, loss = 0.27396410\n",
      "Iteration 228, loss = 0.27235270\n",
      "Iteration 229, loss = 0.27074557\n",
      "Iteration 230, loss = 0.26914295\n",
      "Iteration 231, loss = 0.26754483\n",
      "Iteration 232, loss = 0.26595170\n",
      "Iteration 233, loss = 0.26436374\n",
      "Iteration 234, loss = 0.26278189\n",
      "Iteration 235, loss = 0.26120539\n",
      "Iteration 236, loss = 0.25963444\n",
      "Iteration 237, loss = 0.25807028\n",
      "Iteration 238, loss = 0.25651237\n",
      "Iteration 239, loss = 0.25496085\n",
      "Iteration 240, loss = 0.25341561\n",
      "Iteration 241, loss = 0.25187265\n",
      "Iteration 242, loss = 0.25033058\n",
      "Iteration 243, loss = 0.24880202\n",
      "Iteration 244, loss = 0.24727834\n",
      "Iteration 245, loss = 0.24575843\n",
      "Iteration 246, loss = 0.24424237\n",
      "Iteration 247, loss = 0.24273088\n",
      "Iteration 248, loss = 0.24122384\n",
      "Iteration 249, loss = 0.23972103\n",
      "Iteration 250, loss = 0.23822226\n",
      "Iteration 251, loss = 0.23672768\n",
      "Iteration 252, loss = 0.23523717\n",
      "Iteration 253, loss = 0.23375072\n",
      "Iteration 254, loss = 0.23226857\n",
      "Iteration 255, loss = 0.23079057\n",
      "Iteration 256, loss = 0.22931665\n",
      "Iteration 257, loss = 0.22784682\n",
      "Iteration 258, loss = 0.22638058\n",
      "Iteration 259, loss = 0.22491349\n",
      "Iteration 260, loss = 0.22344143\n",
      "Iteration 261, loss = 0.22197858\n",
      "Iteration 262, loss = 0.22052087\n",
      "Iteration 263, loss = 0.21906561\n",
      "Iteration 264, loss = 0.21761291\n",
      "Iteration 265, loss = 0.21616276\n",
      "Iteration 266, loss = 0.21471533\n",
      "Iteration 267, loss = 0.21327084\n",
      "Iteration 268, loss = 0.21182972\n",
      "Iteration 269, loss = 0.21039262\n",
      "Iteration 270, loss = 0.20895922\n",
      "Iteration 271, loss = 0.20752978\n",
      "Iteration 272, loss = 0.20610489\n",
      "Iteration 273, loss = 0.20468499\n",
      "Iteration 274, loss = 0.20327043\n",
      "Iteration 275, loss = 0.20186138\n",
      "Iteration 276, loss = 0.20045795\n",
      "Iteration 277, loss = 0.19906072\n",
      "Iteration 278, loss = 0.19767010\n",
      "Iteration 279, loss = 0.19628641\n",
      "Iteration 280, loss = 0.19490984\n",
      "Iteration 281, loss = 0.19354095\n",
      "Iteration 282, loss = 0.19217960\n",
      "Iteration 283, loss = 0.19082660\n",
      "Iteration 284, loss = 0.18948223\n",
      "Iteration 285, loss = 0.18814675\n",
      "Iteration 286, loss = 0.18682037\n",
      "Iteration 287, loss = 0.18550316\n",
      "Iteration 288, loss = 0.18419542\n",
      "Iteration 289, loss = 0.18289759\n",
      "Iteration 290, loss = 0.18160949\n",
      "Iteration 291, loss = 0.18033139\n",
      "Iteration 292, loss = 0.17906325\n",
      "Iteration 293, loss = 0.17780521\n",
      "Iteration 294, loss = 0.17655741\n",
      "Iteration 295, loss = 0.17531976\n",
      "Iteration 296, loss = 0.17409193\n",
      "Iteration 297, loss = 0.17287443\n",
      "Iteration 298, loss = 0.17166742\n",
      "Iteration 299, loss = 0.17047080\n",
      "Iteration 300, loss = 0.16928442\n",
      "Iteration 301, loss = 0.16810849\n",
      "Iteration 302, loss = 0.16694303\n",
      "Iteration 303, loss = 0.16578798\n",
      "Iteration 304, loss = 0.16464325\n",
      "Iteration 305, loss = 0.16350991\n",
      "Iteration 306, loss = 0.16238669\n",
      "Iteration 307, loss = 0.16127438\n",
      "Iteration 308, loss = 0.16017293\n",
      "Iteration 309, loss = 0.15908218\n",
      "Iteration 310, loss = 0.15800295\n",
      "Iteration 311, loss = 0.15693374\n",
      "Iteration 312, loss = 0.15587555\n",
      "Iteration 313, loss = 0.15482844\n",
      "Iteration 314, loss = 0.15379203\n",
      "Iteration 315, loss = 0.15276646\n",
      "Iteration 316, loss = 0.15175190\n",
      "Iteration 317, loss = 0.15074824\n",
      "Iteration 318, loss = 0.14975530\n",
      "Iteration 319, loss = 0.14877344\n",
      "Iteration 320, loss = 0.14780285\n",
      "Iteration 321, loss = 0.14684311\n",
      "Iteration 322, loss = 0.14589403\n",
      "Iteration 323, loss = 0.14495563\n",
      "Iteration 324, loss = 0.14402797\n",
      "Iteration 325, loss = 0.14311092\n",
      "Iteration 326, loss = 0.14220448\n",
      "Iteration 327, loss = 0.14130862\n",
      "Iteration 328, loss = 0.14042346\n",
      "Iteration 329, loss = 0.13954881\n",
      "Iteration 330, loss = 0.13868442\n",
      "Iteration 331, loss = 0.13783049\n",
      "Iteration 332, loss = 0.13698702\n",
      "Iteration 333, loss = 0.13615361\n",
      "Iteration 334, loss = 0.13533025\n",
      "Iteration 335, loss = 0.13451647\n",
      "Iteration 336, loss = 0.13371195\n",
      "Iteration 337, loss = 0.13291651\n",
      "Iteration 338, loss = 0.13212997\n",
      "Iteration 339, loss = 0.13135217\n",
      "Iteration 340, loss = 0.13058270\n",
      "Iteration 341, loss = 0.12982134\n",
      "Iteration 342, loss = 0.12906787\n",
      "Iteration 343, loss = 0.12832230\n",
      "Iteration 344, loss = 0.12758379\n",
      "Iteration 345, loss = 0.12685275\n",
      "Iteration 346, loss = 0.12612871\n",
      "Iteration 347, loss = 0.12541121\n",
      "Iteration 348, loss = 0.12470020\n",
      "Iteration 349, loss = 0.12399623\n",
      "Iteration 350, loss = 0.12329861\n",
      "Iteration 351, loss = 0.12260725\n",
      "Iteration 352, loss = 0.12192247\n",
      "Iteration 353, loss = 0.12124396\n",
      "Iteration 354, loss = 0.12057168\n",
      "Iteration 355, loss = 0.11990583\n",
      "Iteration 356, loss = 0.11924680\n",
      "Iteration 357, loss = 0.11859306\n",
      "Iteration 358, loss = 0.11794645\n",
      "Iteration 359, loss = 0.11730585\n",
      "Iteration 360, loss = 0.11667112\n",
      "Iteration 361, loss = 0.11604219\n",
      "Iteration 362, loss = 0.11541968\n",
      "Iteration 363, loss = 0.11480356\n",
      "Iteration 364, loss = 0.11419260\n",
      "Iteration 365, loss = 0.11358832\n",
      "Iteration 366, loss = 0.11298975\n",
      "Iteration 367, loss = 0.11239662\n",
      "Iteration 368, loss = 0.11180969\n",
      "Iteration 369, loss = 0.11122897\n",
      "Iteration 370, loss = 0.11065355\n",
      "Iteration 371, loss = 0.11008483\n",
      "Iteration 372, loss = 0.10952146\n",
      "Iteration 373, loss = 0.10896362\n",
      "Iteration 374, loss = 0.10841164\n",
      "Iteration 375, loss = 0.10786584\n",
      "Iteration 376, loss = 0.10732549\n",
      "Iteration 377, loss = 0.10679114\n",
      "Iteration 378, loss = 0.10626249\n",
      "Iteration 379, loss = 0.10573933\n",
      "Iteration 380, loss = 0.10522201\n",
      "Iteration 381, loss = 0.10471047\n",
      "Iteration 382, loss = 0.10420444\n",
      "Iteration 383, loss = 0.10370409\n",
      "Iteration 384, loss = 0.10320942\n",
      "Iteration 385, loss = 0.10272028\n",
      "Iteration 386, loss = 0.10223675\n",
      "Iteration 387, loss = 0.10175885\n",
      "Iteration 388, loss = 0.10128622\n",
      "Iteration 389, loss = 0.10081903\n",
      "Iteration 390, loss = 0.10035705\n",
      "Iteration 391, loss = 0.09990025\n",
      "Iteration 392, loss = 0.09944826\n",
      "Iteration 393, loss = 0.09900151\n",
      "Iteration 394, loss = 0.09855945\n",
      "Iteration 395, loss = 0.09812270\n",
      "Iteration 396, loss = 0.09769012\n",
      "Iteration 397, loss = 0.09726181\n",
      "Iteration 398, loss = 0.09683777\n",
      "Iteration 399, loss = 0.09641807\n",
      "Iteration 400, loss = 0.09600220\n",
      "Iteration 401, loss = 0.09559004\n",
      "Iteration 402, loss = 0.09518183\n",
      "Iteration 403, loss = 0.09477714\n",
      "Iteration 404, loss = 0.09437610\n",
      "Iteration 405, loss = 0.09397864\n",
      "Iteration 406, loss = 0.09358484\n",
      "Iteration 407, loss = 0.09319456\n",
      "Iteration 408, loss = 0.09280773\n",
      "Iteration 409, loss = 0.09242462\n",
      "Iteration 410, loss = 0.09204513\n",
      "Iteration 411, loss = 0.09166877\n",
      "Iteration 412, loss = 0.09129548\n",
      "Iteration 413, loss = 0.09092559\n",
      "Iteration 414, loss = 0.09055895\n",
      "Iteration 415, loss = 0.09019559\n",
      "Iteration 416, loss = 0.08983556\n",
      "Iteration 417, loss = 0.08947860\n",
      "Iteration 418, loss = 0.08912479\n",
      "Iteration 419, loss = 0.08877418\n",
      "Iteration 420, loss = 0.08842663\n",
      "Iteration 421, loss = 0.08808208\n",
      "Iteration 422, loss = 0.08774051\n",
      "Iteration 423, loss = 0.08740193\n",
      "Iteration 424, loss = 0.08706630\n",
      "Iteration 425, loss = 0.08673353\n",
      "Iteration 426, loss = 0.08640365\n",
      "Iteration 427, loss = 0.08607657\n",
      "Iteration 428, loss = 0.08575235\n",
      "Iteration 429, loss = 0.08543093\n",
      "Iteration 430, loss = 0.08511227\n",
      "Iteration 431, loss = 0.08479647\n",
      "Iteration 432, loss = 0.08448334\n",
      "Iteration 433, loss = 0.08417301\n",
      "Iteration 434, loss = 0.08386546\n",
      "Iteration 435, loss = 0.08356068\n",
      "Iteration 436, loss = 0.08325867\n",
      "Iteration 437, loss = 0.08295947\n",
      "Iteration 438, loss = 0.08266293\n",
      "Iteration 439, loss = 0.08236929\n",
      "Iteration 440, loss = 0.08207842\n",
      "Iteration 441, loss = 0.08179023\n",
      "Iteration 442, loss = 0.08150489\n",
      "Iteration 443, loss = 0.08122224\n",
      "Iteration 444, loss = 0.08094247\n",
      "Iteration 445, loss = 0.08066553\n",
      "Iteration 446, loss = 0.08039129\n",
      "Iteration 447, loss = 0.08011985\n",
      "Iteration 448, loss = 0.07985125\n",
      "Iteration 449, loss = 0.07958541\n",
      "Iteration 450, loss = 0.07932240\n",
      "Iteration 451, loss = 0.07906223\n",
      "Iteration 452, loss = 0.07880477\n",
      "Iteration 453, loss = 0.07855016\n",
      "Iteration 454, loss = 0.07829835\n",
      "Iteration 455, loss = 0.07804929\n",
      "Iteration 456, loss = 0.07780303\n",
      "Iteration 457, loss = 0.07755952\n",
      "Iteration 458, loss = 0.07731879\n",
      "Iteration 459, loss = 0.07708085\n",
      "Iteration 460, loss = 0.07684561\n",
      "Iteration 461, loss = 0.07661312\n",
      "Iteration 462, loss = 0.07638339\n",
      "Iteration 463, loss = 0.07615637\n",
      "Iteration 464, loss = 0.07593203\n",
      "Iteration 465, loss = 0.07571032\n",
      "Iteration 466, loss = 0.07549142\n",
      "Iteration 467, loss = 0.07527510\n",
      "Iteration 468, loss = 0.07506134\n",
      "Iteration 469, loss = 0.07485022\n",
      "Iteration 470, loss = 0.07464176\n",
      "Iteration 471, loss = 0.07443586\n",
      "Iteration 472, loss = 0.07423243\n",
      "Iteration 473, loss = 0.07403160\n",
      "Iteration 474, loss = 0.07383339\n",
      "Iteration 475, loss = 0.07363759\n",
      "Iteration 476, loss = 0.07344417\n",
      "Iteration 477, loss = 0.07325322\n",
      "Iteration 478, loss = 0.07306490\n",
      "Iteration 479, loss = 0.07287888\n",
      "Iteration 480, loss = 0.07269515\n",
      "Iteration 481, loss = 0.07251370\n",
      "Iteration 482, loss = 0.07233462\n",
      "Iteration 483, loss = 0.07215791\n",
      "Iteration 484, loss = 0.07198356\n",
      "Iteration 485, loss = 0.07181126\n",
      "Iteration 486, loss = 0.07164125\n",
      "Iteration 487, loss = 0.07147337\n",
      "Iteration 488, loss = 0.07130774\n",
      "Iteration 489, loss = 0.07114428\n",
      "Iteration 490, loss = 0.07098291\n",
      "Iteration 491, loss = 0.07082359\n",
      "Iteration 492, loss = 0.07066626\n",
      "Iteration 493, loss = 0.07051100\n",
      "Iteration 494, loss = 0.07035767\n",
      "Iteration 495, loss = 0.07020627\n",
      "Iteration 496, loss = 0.07005682\n",
      "Iteration 497, loss = 0.06990927\n",
      "Iteration 498, loss = 0.06976358\n",
      "Iteration 499, loss = 0.06961974\n",
      "Iteration 500, loss = 0.06947768\n",
      "Iteration 501, loss = 0.06933747\n",
      "Iteration 502, loss = 0.06919900\n",
      "Iteration 503, loss = 0.06906225\n",
      "Iteration 504, loss = 0.06892732\n",
      "Iteration 505, loss = 0.06879403\n",
      "Iteration 506, loss = 0.06866241\n",
      "Iteration 507, loss = 0.06853245\n",
      "Iteration 508, loss = 0.06840416\n",
      "Iteration 509, loss = 0.06827749\n",
      "Iteration 510, loss = 0.06815239\n",
      "Iteration 511, loss = 0.06802889\n",
      "Iteration 512, loss = 0.06790694\n",
      "Iteration 513, loss = 0.06778658\n",
      "Iteration 514, loss = 0.06766775\n",
      "Iteration 515, loss = 0.06755035\n",
      "Iteration 516, loss = 0.06743454\n",
      "Iteration 517, loss = 0.06732015\n",
      "Iteration 518, loss = 0.06720722\n",
      "Iteration 519, loss = 0.06709570\n",
      "Iteration 520, loss = 0.06698564\n",
      "Iteration 521, loss = 0.06687691\n",
      "Iteration 522, loss = 0.06676962\n",
      "Iteration 523, loss = 0.06666363\n",
      "Iteration 524, loss = 0.06655890\n",
      "Iteration 525, loss = 0.06645550\n",
      "Iteration 526, loss = 0.06635349\n",
      "Iteration 527, loss = 0.06625277\n",
      "Iteration 528, loss = 0.06615323\n",
      "Iteration 529, loss = 0.06605494\n",
      "Iteration 530, loss = 0.06595783\n",
      "Iteration 531, loss = 0.06586192\n",
      "Iteration 532, loss = 0.06576721\n",
      "Iteration 533, loss = 0.06567369\n",
      "Iteration 534, loss = 0.06558129\n",
      "Iteration 535, loss = 0.06548998\n",
      "Iteration 536, loss = 0.06539981\n",
      "Iteration 537, loss = 0.06531074\n",
      "Iteration 538, loss = 0.06522273\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65830114\n",
      "Iteration 2, loss = 1.59105998\n",
      "Iteration 3, loss = 1.52761329\n",
      "Iteration 4, loss = 1.46829729\n",
      "Iteration 5, loss = 1.41357353\n",
      "Iteration 6, loss = 1.36368588\n",
      "Iteration 7, loss = 1.31892456\n",
      "Iteration 8, loss = 1.27967027\n",
      "Iteration 9, loss = 1.24595667\n",
      "Iteration 10, loss = 1.21767594\n",
      "Iteration 11, loss = 1.19436912\n",
      "Iteration 12, loss = 1.17537946\n",
      "Iteration 13, loss = 1.15983258\n",
      "Iteration 14, loss = 1.14662798\n",
      "Iteration 15, loss = 1.13465487\n",
      "Iteration 16, loss = 1.12296630\n",
      "Iteration 17, loss = 1.11078759\n",
      "Iteration 18, loss = 1.09770079\n",
      "Iteration 19, loss = 1.08357305\n",
      "Iteration 20, loss = 1.06847195\n",
      "Iteration 21, loss = 1.05257437\n",
      "Iteration 22, loss = 1.03621946\n",
      "Iteration 23, loss = 1.01977578\n",
      "Iteration 24, loss = 1.00361646\n",
      "Iteration 25, loss = 0.98801309\n",
      "Iteration 26, loss = 0.97318221\n",
      "Iteration 27, loss = 0.95916162\n",
      "Iteration 28, loss = 0.94597746\n",
      "Iteration 29, loss = 0.93360437\n",
      "Iteration 30, loss = 0.92190161\n",
      "Iteration 31, loss = 0.91075504\n",
      "Iteration 32, loss = 0.90002309\n",
      "Iteration 33, loss = 0.88952409\n",
      "Iteration 34, loss = 0.87918719\n",
      "Iteration 35, loss = 0.86890362\n",
      "Iteration 36, loss = 0.85871423\n",
      "Iteration 37, loss = 0.84865032\n",
      "Iteration 38, loss = 0.83872211\n",
      "Iteration 39, loss = 0.82896636\n",
      "Iteration 40, loss = 0.81942163\n",
      "Iteration 41, loss = 0.81014193\n",
      "Iteration 42, loss = 0.80118486\n",
      "Iteration 43, loss = 0.79251394\n",
      "Iteration 44, loss = 0.78415596\n",
      "Iteration 45, loss = 0.77605777\n",
      "Iteration 46, loss = 0.76819193\n",
      "Iteration 47, loss = 0.76058534\n",
      "Iteration 48, loss = 0.75320912\n",
      "Iteration 49, loss = 0.74602919\n",
      "Iteration 50, loss = 0.73906526\n",
      "Iteration 51, loss = 0.73228876\n",
      "Iteration 52, loss = 0.72568519\n",
      "Iteration 53, loss = 0.71924685\n",
      "Iteration 54, loss = 0.71296505\n",
      "Iteration 55, loss = 0.70682212\n",
      "Iteration 56, loss = 0.70079522\n",
      "Iteration 57, loss = 0.69486355\n",
      "Iteration 58, loss = 0.68903213\n",
      "Iteration 59, loss = 0.68327920\n",
      "Iteration 60, loss = 0.67758492\n",
      "Iteration 61, loss = 0.67196664\n",
      "Iteration 62, loss = 0.66648108\n",
      "Iteration 63, loss = 0.66111348\n",
      "Iteration 64, loss = 0.65587256\n",
      "Iteration 65, loss = 0.65080361\n",
      "Iteration 66, loss = 0.64595591\n",
      "Iteration 67, loss = 0.64130063\n",
      "Iteration 68, loss = 0.63681240\n",
      "Iteration 69, loss = 0.63244393\n",
      "Iteration 70, loss = 0.62816815\n",
      "Iteration 71, loss = 0.62398063\n",
      "Iteration 72, loss = 0.61985721\n",
      "Iteration 73, loss = 0.61580452\n",
      "Iteration 74, loss = 0.61180383\n",
      "Iteration 75, loss = 0.60785408\n",
      "Iteration 76, loss = 0.60395201\n",
      "Iteration 77, loss = 0.60010552\n",
      "Iteration 78, loss = 0.59630614\n",
      "Iteration 79, loss = 0.59254547\n",
      "Iteration 80, loss = 0.58879424\n",
      "Iteration 81, loss = 0.58508830\n",
      "Iteration 82, loss = 0.58148458\n",
      "Iteration 83, loss = 0.57798538\n",
      "Iteration 84, loss = 0.57456128\n",
      "Iteration 85, loss = 0.57118775\n",
      "Iteration 86, loss = 0.56785929\n",
      "Iteration 87, loss = 0.56457285\n",
      "Iteration 88, loss = 0.56132774\n",
      "Iteration 89, loss = 0.55812454\n",
      "Iteration 90, loss = 0.55496418\n",
      "Iteration 91, loss = 0.55184452\n",
      "Iteration 92, loss = 0.54876435\n",
      "Iteration 93, loss = 0.54572246\n",
      "Iteration 94, loss = 0.54271738\n",
      "Iteration 95, loss = 0.53974790\n",
      "Iteration 96, loss = 0.53681483\n",
      "Iteration 97, loss = 0.53391699\n",
      "Iteration 98, loss = 0.53105529\n",
      "Iteration 99, loss = 0.52822784\n",
      "Iteration 100, loss = 0.52543457\n",
      "Iteration 101, loss = 0.52267325\n",
      "Iteration 102, loss = 0.51994177\n",
      "Iteration 103, loss = 0.51723913\n",
      "Iteration 104, loss = 0.51456213\n",
      "Iteration 105, loss = 0.51190930\n",
      "Iteration 106, loss = 0.50927916\n",
      "Iteration 107, loss = 0.50667418\n",
      "Iteration 108, loss = 0.50409129\n",
      "Iteration 109, loss = 0.50153013\n",
      "Iteration 110, loss = 0.49899086\n",
      "Iteration 111, loss = 0.49647413\n",
      "Iteration 112, loss = 0.49397966\n",
      "Iteration 113, loss = 0.49150367\n",
      "Iteration 114, loss = 0.48904731\n",
      "Iteration 115, loss = 0.48660930\n",
      "Iteration 116, loss = 0.48418973\n",
      "Iteration 117, loss = 0.48178778\n",
      "Iteration 118, loss = 0.47940310\n",
      "Iteration 119, loss = 0.47702771\n",
      "Iteration 120, loss = 0.47465523\n",
      "Iteration 121, loss = 0.47228863\n",
      "Iteration 122, loss = 0.46996640\n",
      "Iteration 123, loss = 0.46765295\n",
      "Iteration 124, loss = 0.46535882\n",
      "Iteration 125, loss = 0.46307948\n",
      "Iteration 126, loss = 0.46081231\n",
      "Iteration 127, loss = 0.45855638\n",
      "Iteration 128, loss = 0.45631074\n",
      "Iteration 129, loss = 0.45408118\n",
      "Iteration 130, loss = 0.45185914\n",
      "Iteration 131, loss = 0.44965146\n",
      "Iteration 132, loss = 0.44745501\n",
      "Iteration 133, loss = 0.44526943\n",
      "Iteration 134, loss = 0.44309593\n",
      "Iteration 135, loss = 0.44093300\n",
      "Iteration 136, loss = 0.43878048\n",
      "Iteration 137, loss = 0.43663842\n",
      "Iteration 138, loss = 0.43450655\n",
      "Iteration 139, loss = 0.43238509\n",
      "Iteration 140, loss = 0.43027436\n",
      "Iteration 141, loss = 0.42817315\n",
      "Iteration 142, loss = 0.42608171\n",
      "Iteration 143, loss = 0.42399995\n",
      "Iteration 144, loss = 0.42192791\n",
      "Iteration 145, loss = 0.41986559\n",
      "Iteration 146, loss = 0.41781342\n",
      "Iteration 147, loss = 0.41577040\n",
      "Iteration 148, loss = 0.41373510\n",
      "Iteration 149, loss = 0.41170555\n",
      "Iteration 150, loss = 0.40967975\n",
      "Iteration 151, loss = 0.40765691\n",
      "Iteration 152, loss = 0.40563606\n",
      "Iteration 153, loss = 0.40362670\n",
      "Iteration 154, loss = 0.40165415\n",
      "Iteration 155, loss = 0.39970558\n",
      "Iteration 156, loss = 0.39777558\n",
      "Iteration 157, loss = 0.39585451\n",
      "Iteration 158, loss = 0.39394112\n",
      "Iteration 159, loss = 0.39203866\n",
      "Iteration 160, loss = 0.39014921\n",
      "Iteration 161, loss = 0.38825662\n",
      "Iteration 162, loss = 0.38635780\n",
      "Iteration 163, loss = 0.38445350\n",
      "Iteration 164, loss = 0.38255068\n",
      "Iteration 165, loss = 0.38065685\n",
      "Iteration 166, loss = 0.37877254\n",
      "Iteration 167, loss = 0.37689242\n",
      "Iteration 168, loss = 0.37501983\n",
      "Iteration 169, loss = 0.37315656\n",
      "Iteration 170, loss = 0.37130423\n",
      "Iteration 171, loss = 0.36946113\n",
      "Iteration 172, loss = 0.36762485\n",
      "Iteration 173, loss = 0.36579392\n",
      "Iteration 174, loss = 0.36396835\n",
      "Iteration 175, loss = 0.36214766\n",
      "Iteration 176, loss = 0.36033144\n",
      "Iteration 177, loss = 0.35852004\n",
      "Iteration 178, loss = 0.35671430\n",
      "Iteration 179, loss = 0.35491348\n",
      "Iteration 180, loss = 0.35311965\n",
      "Iteration 181, loss = 0.35133206\n",
      "Iteration 182, loss = 0.34955128\n",
      "Iteration 183, loss = 0.34777689\n",
      "Iteration 184, loss = 0.34600829\n",
      "Iteration 185, loss = 0.34424570\n",
      "Iteration 186, loss = 0.34249054\n",
      "Iteration 187, loss = 0.34074382\n",
      "Iteration 188, loss = 0.33900409\n",
      "Iteration 189, loss = 0.33727367\n",
      "Iteration 190, loss = 0.33554951\n",
      "Iteration 191, loss = 0.33383024\n",
      "Iteration 192, loss = 0.33211727\n",
      "Iteration 193, loss = 0.33041120\n",
      "Iteration 194, loss = 0.32871004\n",
      "Iteration 195, loss = 0.32701671\n",
      "Iteration 196, loss = 0.32533088\n",
      "Iteration 197, loss = 0.32365186\n",
      "Iteration 198, loss = 0.32198060\n",
      "Iteration 199, loss = 0.32031506\n",
      "Iteration 200, loss = 0.31865030\n",
      "Iteration 201, loss = 0.31698459\n",
      "Iteration 202, loss = 0.31532925\n",
      "Iteration 203, loss = 0.31368128\n",
      "Iteration 204, loss = 0.31203493\n",
      "Iteration 205, loss = 0.31039034\n",
      "Iteration 206, loss = 0.30874674\n",
      "Iteration 207, loss = 0.30710416\n",
      "Iteration 208, loss = 0.30546242\n",
      "Iteration 209, loss = 0.30382163\n",
      "Iteration 210, loss = 0.30218158\n",
      "Iteration 211, loss = 0.30054297\n",
      "Iteration 212, loss = 0.29890502\n",
      "Iteration 213, loss = 0.29726917\n",
      "Iteration 214, loss = 0.29563424\n",
      "Iteration 215, loss = 0.29400046\n",
      "Iteration 216, loss = 0.29236845\n",
      "Iteration 217, loss = 0.29073808\n",
      "Iteration 218, loss = 0.28910924\n",
      "Iteration 219, loss = 0.28748245\n",
      "Iteration 220, loss = 0.28585807\n",
      "Iteration 221, loss = 0.28423585\n",
      "Iteration 222, loss = 0.28261607\n",
      "Iteration 223, loss = 0.28099928\n",
      "Iteration 224, loss = 0.27938549\n",
      "Iteration 225, loss = 0.27777523\n",
      "Iteration 226, loss = 0.27616830\n",
      "Iteration 227, loss = 0.27456556\n",
      "Iteration 228, loss = 0.27296676\n",
      "Iteration 229, loss = 0.27137204\n",
      "Iteration 230, loss = 0.26978215\n",
      "Iteration 231, loss = 0.26819703\n",
      "Iteration 232, loss = 0.26661674\n",
      "Iteration 233, loss = 0.26504170\n",
      "Iteration 234, loss = 0.26347236\n",
      "Iteration 235, loss = 0.26190867\n",
      "Iteration 236, loss = 0.26035079\n",
      "Iteration 237, loss = 0.25879895\n",
      "Iteration 238, loss = 0.25725382\n",
      "Iteration 239, loss = 0.25571506\n",
      "Iteration 240, loss = 0.25418234\n",
      "Iteration 241, loss = 0.25265268\n",
      "Iteration 242, loss = 0.25112480\n",
      "Iteration 243, loss = 0.24960865\n",
      "Iteration 244, loss = 0.24809627\n",
      "Iteration 245, loss = 0.24658769\n",
      "Iteration 246, loss = 0.24508332\n",
      "Iteration 247, loss = 0.24358275\n",
      "Iteration 248, loss = 0.24208634\n",
      "Iteration 249, loss = 0.24059418\n",
      "Iteration 250, loss = 0.23910566\n",
      "Iteration 251, loss = 0.23762093\n",
      "Iteration 252, loss = 0.23614084\n",
      "Iteration 253, loss = 0.23466425\n",
      "Iteration 254, loss = 0.23319212\n",
      "Iteration 255, loss = 0.23172402\n",
      "Iteration 256, loss = 0.23025952\n",
      "Iteration 257, loss = 0.22879921\n",
      "Iteration 258, loss = 0.22734206\n",
      "Iteration 259, loss = 0.22588376\n",
      "Iteration 260, loss = 0.22442142\n",
      "Iteration 261, loss = 0.22296958\n",
      "Iteration 262, loss = 0.22152156\n",
      "Iteration 263, loss = 0.22007570\n",
      "Iteration 264, loss = 0.21863192\n",
      "Iteration 265, loss = 0.21719066\n",
      "Iteration 266, loss = 0.21575190\n",
      "Iteration 267, loss = 0.21431598\n",
      "Iteration 268, loss = 0.21288303\n",
      "Iteration 269, loss = 0.21145328\n",
      "Iteration 270, loss = 0.21002720\n",
      "Iteration 271, loss = 0.20860505\n",
      "Iteration 272, loss = 0.20718695\n",
      "Iteration 273, loss = 0.20577326\n",
      "Iteration 274, loss = 0.20436436\n",
      "Iteration 275, loss = 0.20296044\n",
      "Iteration 276, loss = 0.20156198\n",
      "Iteration 277, loss = 0.20016935\n",
      "Iteration 278, loss = 0.19878286\n",
      "Iteration 279, loss = 0.19740299\n",
      "Iteration 280, loss = 0.19602995\n",
      "Iteration 281, loss = 0.19466390\n",
      "Iteration 282, loss = 0.19330520\n",
      "Iteration 283, loss = 0.19195433\n",
      "Iteration 284, loss = 0.19061161\n",
      "Iteration 285, loss = 0.18927685\n",
      "Iteration 286, loss = 0.18795104\n",
      "Iteration 287, loss = 0.18663409\n",
      "Iteration 288, loss = 0.18532614\n",
      "Iteration 289, loss = 0.18402738\n",
      "Iteration 290, loss = 0.18273810\n",
      "Iteration 291, loss = 0.18145854\n",
      "Iteration 292, loss = 0.18018885\n",
      "Iteration 293, loss = 0.17892907\n",
      "Iteration 294, loss = 0.17767942\n",
      "Iteration 295, loss = 0.17644037\n",
      "Iteration 296, loss = 0.17521161\n",
      "Iteration 297, loss = 0.17399335\n",
      "Iteration 298, loss = 0.17278529\n",
      "Iteration 299, loss = 0.17158982\n",
      "Iteration 300, loss = 0.17040389\n",
      "Iteration 301, loss = 0.16923019\n",
      "Iteration 302, loss = 0.16806605\n",
      "Iteration 303, loss = 0.16691146\n",
      "Iteration 304, loss = 0.16576723\n",
      "Iteration 305, loss = 0.16464283\n",
      "Iteration 306, loss = 0.16351584\n",
      "Iteration 307, loss = 0.16241004\n",
      "Iteration 308, loss = 0.16131477\n",
      "Iteration 309, loss = 0.16022873\n",
      "Iteration 310, loss = 0.15915173\n",
      "Iteration 311, loss = 0.15808357\n",
      "Iteration 312, loss = 0.15702690\n",
      "Iteration 313, loss = 0.15599016\n",
      "Iteration 314, loss = 0.15495189\n",
      "Iteration 315, loss = 0.15393089\n",
      "Iteration 316, loss = 0.15292187\n",
      "Iteration 317, loss = 0.15192284\n",
      "Iteration 318, loss = 0.15093226\n",
      "Iteration 319, loss = 0.14995150\n",
      "Iteration 320, loss = 0.14898089\n",
      "Iteration 321, loss = 0.14802332\n",
      "Iteration 322, loss = 0.14707564\n",
      "Iteration 323, loss = 0.14613561\n",
      "Iteration 324, loss = 0.14520818\n",
      "Iteration 325, loss = 0.14429132\n",
      "Iteration 326, loss = 0.14338401\n",
      "Iteration 327, loss = 0.14248597\n",
      "Iteration 328, loss = 0.14159828\n",
      "Iteration 329, loss = 0.14072037\n",
      "Iteration 330, loss = 0.13985180\n",
      "Iteration 331, loss = 0.13899202\n",
      "Iteration 332, loss = 0.13814281\n",
      "Iteration 333, loss = 0.13730306\n",
      "Iteration 334, loss = 0.13647242\n",
      "Iteration 335, loss = 0.13565082\n",
      "Iteration 336, loss = 0.13483826\n",
      "Iteration 337, loss = 0.13403465\n",
      "Iteration 338, loss = 0.13323983\n",
      "Iteration 339, loss = 0.13245366\n",
      "Iteration 340, loss = 0.13167603\n",
      "Iteration 341, loss = 0.13090752\n",
      "Iteration 342, loss = 0.13014784\n",
      "Iteration 343, loss = 0.12939620\n",
      "Iteration 344, loss = 0.12865225\n",
      "Iteration 345, loss = 0.12791591\n",
      "Iteration 346, loss = 0.12718715\n",
      "Iteration 347, loss = 0.12646566\n",
      "Iteration 348, loss = 0.12575137\n",
      "Iteration 349, loss = 0.12504400\n",
      "Iteration 350, loss = 0.12434299\n",
      "Iteration 351, loss = 0.12364797\n",
      "Iteration 352, loss = 0.12295876\n",
      "Iteration 353, loss = 0.12227547\n",
      "Iteration 354, loss = 0.12159776\n",
      "Iteration 355, loss = 0.12092531\n",
      "Iteration 356, loss = 0.12025841\n",
      "Iteration 357, loss = 0.11959593\n",
      "Iteration 358, loss = 0.11893887\n",
      "Iteration 359, loss = 0.11828683\n",
      "Iteration 360, loss = 0.11763939\n",
      "Iteration 361, loss = 0.11699673\n",
      "Iteration 362, loss = 0.11635892\n",
      "Iteration 363, loss = 0.11572556\n",
      "Iteration 364, loss = 0.11509684\n",
      "Iteration 365, loss = 0.11447272\n",
      "Iteration 366, loss = 0.11385300\n",
      "Iteration 367, loss = 0.11323779\n",
      "Iteration 368, loss = 0.11262709\n",
      "Iteration 369, loss = 0.11202119\n",
      "Iteration 370, loss = 0.11141978\n",
      "Iteration 371, loss = 0.11082301\n",
      "Iteration 372, loss = 0.11023073\n",
      "Iteration 373, loss = 0.10964334\n",
      "Iteration 374, loss = 0.10906041\n",
      "Iteration 375, loss = 0.10848245\n",
      "Iteration 376, loss = 0.10790910\n",
      "Iteration 377, loss = 0.10734070\n",
      "Iteration 378, loss = 0.10677723\n",
      "Iteration 379, loss = 0.10621833\n",
      "Iteration 380, loss = 0.10566437\n",
      "Iteration 381, loss = 0.10511545\n",
      "Iteration 382, loss = 0.10457138\n",
      "Iteration 383, loss = 0.10403214\n",
      "Iteration 384, loss = 0.10349792\n",
      "Iteration 385, loss = 0.10296870\n",
      "Iteration 386, loss = 0.10244447\n",
      "Iteration 387, loss = 0.10192514\n",
      "Iteration 388, loss = 0.10141068\n",
      "Iteration 389, loss = 0.10090123\n",
      "Iteration 390, loss = 0.10039668\n",
      "Iteration 391, loss = 0.09989693\n",
      "Iteration 392, loss = 0.09940189\n",
      "Iteration 393, loss = 0.09891146\n",
      "Iteration 394, loss = 0.09842547\n",
      "Iteration 395, loss = 0.09794365\n",
      "Iteration 396, loss = 0.09746613\n",
      "Iteration 397, loss = 0.09699228\n",
      "Iteration 398, loss = 0.09652248\n",
      "Iteration 399, loss = 0.09605637\n",
      "Iteration 400, loss = 0.09559402\n",
      "Iteration 401, loss = 0.09513518\n",
      "Iteration 402, loss = 0.09467977\n",
      "Iteration 403, loss = 0.09422780\n",
      "Iteration 404, loss = 0.09377912\n",
      "Iteration 405, loss = 0.09333385\n",
      "Iteration 406, loss = 0.09289193\n",
      "Iteration 407, loss = 0.09245334\n",
      "Iteration 408, loss = 0.09201813\n",
      "Iteration 409, loss = 0.09158621\n",
      "Iteration 410, loss = 0.09115752\n",
      "Iteration 411, loss = 0.09073200\n",
      "Iteration 412, loss = 0.09030981\n",
      "Iteration 413, loss = 0.08989086\n",
      "Iteration 414, loss = 0.08947520\n",
      "Iteration 415, loss = 0.08906284\n",
      "Iteration 416, loss = 0.08865376\n",
      "Iteration 417, loss = 0.08824785\n",
      "Iteration 418, loss = 0.08784528\n",
      "Iteration 419, loss = 0.08744580\n",
      "Iteration 420, loss = 0.08704944\n",
      "Iteration 421, loss = 0.08665620\n",
      "Iteration 422, loss = 0.08626596\n",
      "Iteration 423, loss = 0.08587866\n",
      "Iteration 424, loss = 0.08549427\n",
      "Iteration 425, loss = 0.08511279\n",
      "Iteration 426, loss = 0.08473421\n",
      "Iteration 427, loss = 0.08435847\n",
      "Iteration 428, loss = 0.08398546\n",
      "Iteration 429, loss = 0.08361523\n",
      "Iteration 430, loss = 0.08324772\n",
      "Iteration 431, loss = 0.08288287\n",
      "Iteration 432, loss = 0.08252071\n",
      "Iteration 433, loss = 0.08216129\n",
      "Iteration 434, loss = 0.08180447\n",
      "Iteration 435, loss = 0.08145033\n",
      "Iteration 436, loss = 0.08109885\n",
      "Iteration 437, loss = 0.08075004\n",
      "Iteration 438, loss = 0.08040393\n",
      "Iteration 439, loss = 0.08006052\n",
      "Iteration 440, loss = 0.07971978\n",
      "Iteration 441, loss = 0.07938176\n",
      "Iteration 442, loss = 0.07904642\n",
      "Iteration 443, loss = 0.07871386\n",
      "Iteration 444, loss = 0.07838400\n",
      "Iteration 445, loss = 0.07805691\n",
      "Iteration 446, loss = 0.07773257\n",
      "Iteration 447, loss = 0.07741101\n",
      "Iteration 448, loss = 0.07709222\n",
      "Iteration 449, loss = 0.07677611\n",
      "Iteration 450, loss = 0.07646268\n",
      "Iteration 451, loss = 0.07615190\n",
      "Iteration 452, loss = 0.07584376\n",
      "Iteration 453, loss = 0.07553821\n",
      "Iteration 454, loss = 0.07523521\n",
      "Iteration 455, loss = 0.07493476\n",
      "Iteration 456, loss = 0.07463692\n",
      "Iteration 457, loss = 0.07434137\n",
      "Iteration 458, loss = 0.07404833\n",
      "Iteration 459, loss = 0.07375756\n",
      "Iteration 460, loss = 0.07346902\n",
      "Iteration 461, loss = 0.07318267\n",
      "Iteration 462, loss = 0.07289845\n",
      "Iteration 463, loss = 0.07261640\n",
      "Iteration 464, loss = 0.07233653\n",
      "Iteration 465, loss = 0.07205864\n",
      "Iteration 466, loss = 0.07178285\n",
      "Iteration 467, loss = 0.07150920\n",
      "Iteration 468, loss = 0.07123755\n",
      "Iteration 469, loss = 0.07096792\n",
      "Iteration 470, loss = 0.07070040\n",
      "Iteration 471, loss = 0.07043492\n",
      "Iteration 472, loss = 0.07017151\n",
      "Iteration 473, loss = 0.06991022\n",
      "Iteration 474, loss = 0.06965100\n",
      "Iteration 475, loss = 0.06939390\n",
      "Iteration 476, loss = 0.06913896\n",
      "Iteration 477, loss = 0.06888615\n",
      "Iteration 478, loss = 0.06863551\n",
      "Iteration 479, loss = 0.06838710\n",
      "Iteration 480, loss = 0.06814089\n",
      "Iteration 481, loss = 0.06789687\n",
      "Iteration 482, loss = 0.06765508\n",
      "Iteration 483, loss = 0.06741554\n",
      "Iteration 484, loss = 0.06717846\n",
      "Iteration 485, loss = 0.06694363\n",
      "Iteration 486, loss = 0.06671105\n",
      "Iteration 487, loss = 0.06648081\n",
      "Iteration 488, loss = 0.06625288\n",
      "Iteration 489, loss = 0.06602725\n",
      "Iteration 490, loss = 0.06580390\n",
      "Iteration 491, loss = 0.06558283\n",
      "Iteration 492, loss = 0.06536406\n",
      "Iteration 493, loss = 0.06514757\n",
      "Iteration 494, loss = 0.06493337\n",
      "Iteration 495, loss = 0.06472145\n",
      "Iteration 496, loss = 0.06451179\n",
      "Iteration 497, loss = 0.06430442\n",
      "Iteration 498, loss = 0.06409928\n",
      "Iteration 499, loss = 0.06389637\n",
      "Iteration 500, loss = 0.06369574\n",
      "Iteration 501, loss = 0.06349738\n",
      "Iteration 502, loss = 0.06330126\n",
      "Iteration 503, loss = 0.06310733\n",
      "Iteration 504, loss = 0.06291559\n",
      "Iteration 505, loss = 0.06272604\n",
      "Iteration 506, loss = 0.06253866\n",
      "Iteration 507, loss = 0.06235341\n",
      "Iteration 508, loss = 0.06217026\n",
      "Iteration 509, loss = 0.06198920\n",
      "Iteration 510, loss = 0.06181022\n",
      "Iteration 511, loss = 0.06163331\n",
      "Iteration 512, loss = 0.06145852\n",
      "Iteration 513, loss = 0.06128573\n",
      "Iteration 514, loss = 0.06111497\n",
      "Iteration 515, loss = 0.06094618\n",
      "Iteration 516, loss = 0.06077936\n",
      "Iteration 517, loss = 0.06061448\n",
      "Iteration 518, loss = 0.06045154\n",
      "Iteration 519, loss = 0.06029051\n",
      "Iteration 520, loss = 0.06013135\n",
      "Iteration 521, loss = 0.05997409\n",
      "Iteration 522, loss = 0.05981870\n",
      "Iteration 523, loss = 0.05966510\n",
      "Iteration 524, loss = 0.05951329\n",
      "Iteration 525, loss = 0.05936327\n",
      "Iteration 526, loss = 0.05921503\n",
      "Iteration 527, loss = 0.05906852\n",
      "Iteration 528, loss = 0.05892372\n",
      "Iteration 529, loss = 0.05878060\n",
      "Iteration 530, loss = 0.05863915\n",
      "Iteration 531, loss = 0.05849938\n",
      "Iteration 532, loss = 0.05836123\n",
      "Iteration 533, loss = 0.05822464\n",
      "Iteration 534, loss = 0.05808965\n",
      "Iteration 535, loss = 0.05795623\n",
      "Iteration 536, loss = 0.05782436\n",
      "Iteration 537, loss = 0.05769402\n",
      "Iteration 538, loss = 0.05756516\n",
      "Iteration 539, loss = 0.05743777\n",
      "Iteration 540, loss = 0.05731184\n",
      "Iteration 541, loss = 0.05718735\n",
      "Iteration 542, loss = 0.05706432\n",
      "Iteration 543, loss = 0.05694267\n",
      "Iteration 544, loss = 0.05682237\n",
      "Iteration 545, loss = 0.05670345\n",
      "Iteration 546, loss = 0.05658587\n",
      "Iteration 547, loss = 0.05646963\n",
      "Iteration 548, loss = 0.05635468\n",
      "Iteration 549, loss = 0.05624102\n",
      "Iteration 550, loss = 0.05612862\n",
      "Iteration 551, loss = 0.05601746\n",
      "Iteration 552, loss = 0.05590753\n",
      "Iteration 553, loss = 0.05579887\n",
      "Iteration 554, loss = 0.05569140\n",
      "Iteration 555, loss = 0.05558509\n",
      "Iteration 556, loss = 0.05547998\n",
      "Iteration 557, loss = 0.05537601\n",
      "Iteration 558, loss = 0.05527317\n",
      "Iteration 559, loss = 0.05517144\n",
      "Iteration 560, loss = 0.05507081\n",
      "Iteration 561, loss = 0.05497126\n",
      "Iteration 562, loss = 0.05487278\n",
      "Iteration 563, loss = 0.05477536\n",
      "Iteration 564, loss = 0.05467896\n",
      "Iteration 565, loss = 0.05458364\n",
      "Iteration 566, loss = 0.05448931\n",
      "Iteration 567, loss = 0.05439595\n",
      "Iteration 568, loss = 0.05430358\n",
      "Iteration 569, loss = 0.05421219\n",
      "Iteration 570, loss = 0.05412175\n",
      "Iteration 571, loss = 0.05403226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66075849\n",
      "Iteration 2, loss = 1.59315210\n",
      "Iteration 3, loss = 1.52935741\n",
      "Iteration 4, loss = 1.46970156\n",
      "Iteration 5, loss = 1.41467199\n",
      "Iteration 6, loss = 1.36462002\n",
      "Iteration 7, loss = 1.31996670\n",
      "Iteration 8, loss = 1.28110265\n",
      "Iteration 9, loss = 1.24804242\n",
      "Iteration 10, loss = 1.22074220\n",
      "Iteration 11, loss = 1.19859545\n",
      "Iteration 12, loss = 1.18071736\n",
      "Iteration 13, loss = 1.16603817\n",
      "Iteration 14, loss = 1.15333194\n",
      "Iteration 15, loss = 1.14147628\n",
      "Iteration 16, loss = 1.12955391\n",
      "Iteration 17, loss = 1.11693662\n",
      "Iteration 18, loss = 1.10318909\n",
      "Iteration 19, loss = 1.08828424\n",
      "Iteration 20, loss = 1.07242468\n",
      "Iteration 21, loss = 1.05582859\n",
      "Iteration 22, loss = 1.03894541\n",
      "Iteration 23, loss = 1.02212031\n",
      "Iteration 24, loss = 1.00574841\n",
      "Iteration 25, loss = 0.99004872\n",
      "Iteration 26, loss = 0.97519812\n",
      "Iteration 27, loss = 0.96121692\n",
      "Iteration 28, loss = 0.94810998\n",
      "Iteration 29, loss = 0.93583933\n",
      "Iteration 30, loss = 0.92422379\n",
      "Iteration 31, loss = 0.91307830\n",
      "Iteration 32, loss = 0.90221454\n",
      "Iteration 33, loss = 0.89151592\n",
      "Iteration 34, loss = 0.88094174\n",
      "Iteration 35, loss = 0.87036349\n",
      "Iteration 36, loss = 0.85982921\n",
      "Iteration 37, loss = 0.84936639\n",
      "Iteration 38, loss = 0.83898784\n",
      "Iteration 39, loss = 0.82870429\n",
      "Iteration 40, loss = 0.81862341\n",
      "Iteration 41, loss = 0.80882910\n",
      "Iteration 42, loss = 0.79936616\n",
      "Iteration 43, loss = 0.79020018\n",
      "Iteration 44, loss = 0.78133543\n",
      "Iteration 45, loss = 0.77274335\n",
      "Iteration 46, loss = 0.76445496\n",
      "Iteration 47, loss = 0.75648221\n",
      "Iteration 48, loss = 0.74880618\n",
      "Iteration 49, loss = 0.74137555\n",
      "Iteration 50, loss = 0.73420888\n",
      "Iteration 51, loss = 0.72729455\n",
      "Iteration 52, loss = 0.72058130\n",
      "Iteration 53, loss = 0.71404257\n",
      "Iteration 54, loss = 0.70768536\n",
      "Iteration 55, loss = 0.70146060\n",
      "Iteration 56, loss = 0.69535818\n",
      "Iteration 57, loss = 0.68936639\n",
      "Iteration 58, loss = 0.68347666\n",
      "Iteration 59, loss = 0.67768912\n",
      "Iteration 60, loss = 0.67197401\n",
      "Iteration 61, loss = 0.66636454\n",
      "Iteration 62, loss = 0.66092476\n",
      "Iteration 63, loss = 0.65562017\n",
      "Iteration 64, loss = 0.65046701\n",
      "Iteration 65, loss = 0.64552286\n",
      "Iteration 66, loss = 0.64077901\n",
      "Iteration 67, loss = 0.63620946\n",
      "Iteration 68, loss = 0.63177164\n",
      "Iteration 69, loss = 0.62746187\n",
      "Iteration 70, loss = 0.62324459\n",
      "Iteration 71, loss = 0.61912767\n",
      "Iteration 72, loss = 0.61509326\n",
      "Iteration 73, loss = 0.61113097\n",
      "Iteration 74, loss = 0.60722702\n",
      "Iteration 75, loss = 0.60336880\n",
      "Iteration 76, loss = 0.59954537\n",
      "Iteration 77, loss = 0.59576788\n",
      "Iteration 78, loss = 0.59202536\n",
      "Iteration 79, loss = 0.58826508\n",
      "Iteration 80, loss = 0.58457789\n",
      "Iteration 81, loss = 0.58101389\n",
      "Iteration 82, loss = 0.57754688\n",
      "Iteration 83, loss = 0.57414408\n",
      "Iteration 84, loss = 0.57079726\n",
      "Iteration 85, loss = 0.56749522\n",
      "Iteration 86, loss = 0.56423510\n",
      "Iteration 87, loss = 0.56101839\n",
      "Iteration 88, loss = 0.55784610\n",
      "Iteration 89, loss = 0.55471772\n",
      "Iteration 90, loss = 0.55163254\n",
      "Iteration 91, loss = 0.54858993\n",
      "Iteration 92, loss = 0.54558691\n",
      "Iteration 93, loss = 0.54262218\n",
      "Iteration 94, loss = 0.53969437\n",
      "Iteration 95, loss = 0.53680276\n",
      "Iteration 96, loss = 0.53394742\n",
      "Iteration 97, loss = 0.53112750\n",
      "Iteration 98, loss = 0.52834327\n",
      "Iteration 99, loss = 0.52559423\n",
      "Iteration 100, loss = 0.52287897\n",
      "Iteration 101, loss = 0.52019543\n",
      "Iteration 102, loss = 0.51754429\n",
      "Iteration 103, loss = 0.51492382\n",
      "Iteration 104, loss = 0.51233209\n",
      "Iteration 105, loss = 0.50976767\n",
      "Iteration 106, loss = 0.50723018\n",
      "Iteration 107, loss = 0.50471879\n",
      "Iteration 108, loss = 0.50223296\n",
      "Iteration 109, loss = 0.49977215\n",
      "Iteration 110, loss = 0.49733556\n",
      "Iteration 111, loss = 0.49492500\n",
      "Iteration 112, loss = 0.49253843\n",
      "Iteration 113, loss = 0.49017395\n",
      "Iteration 114, loss = 0.48783074\n",
      "Iteration 115, loss = 0.48551123\n",
      "Iteration 116, loss = 0.48326127\n",
      "Iteration 117, loss = 0.48105951\n",
      "Iteration 118, loss = 0.47888228\n",
      "Iteration 119, loss = 0.47671463\n",
      "Iteration 120, loss = 0.47454361\n",
      "Iteration 121, loss = 0.47236937\n",
      "Iteration 122, loss = 0.47023180\n",
      "Iteration 123, loss = 0.46807190\n",
      "Iteration 124, loss = 0.46595492\n",
      "Iteration 125, loss = 0.46384559\n",
      "Iteration 126, loss = 0.46174228\n",
      "Iteration 127, loss = 0.45964785\n",
      "Iteration 128, loss = 0.45758024\n",
      "Iteration 129, loss = 0.45553224\n",
      "Iteration 130, loss = 0.45350434\n",
      "Iteration 131, loss = 0.45149739\n",
      "Iteration 132, loss = 0.44949655\n",
      "Iteration 133, loss = 0.44750294\n",
      "Iteration 134, loss = 0.44551965\n",
      "Iteration 135, loss = 0.44355006\n",
      "Iteration 136, loss = 0.44158611\n",
      "Iteration 137, loss = 0.43963340\n",
      "Iteration 138, loss = 0.43769058\n",
      "Iteration 139, loss = 0.43575590\n",
      "Iteration 140, loss = 0.43382885\n",
      "Iteration 141, loss = 0.43191008\n",
      "Iteration 142, loss = 0.43000531\n",
      "Iteration 143, loss = 0.42811560\n",
      "Iteration 144, loss = 0.42623595\n",
      "Iteration 145, loss = 0.42436626\n",
      "Iteration 146, loss = 0.42250658\n",
      "Iteration 147, loss = 0.42065404\n",
      "Iteration 148, loss = 0.41880890\n",
      "Iteration 149, loss = 0.41697387\n",
      "Iteration 150, loss = 0.41514448\n",
      "Iteration 151, loss = 0.41331960\n",
      "Iteration 152, loss = 0.41150047\n",
      "Iteration 153, loss = 0.40968418\n",
      "Iteration 154, loss = 0.40787012\n",
      "Iteration 155, loss = 0.40605808\n",
      "Iteration 156, loss = 0.40424928\n",
      "Iteration 157, loss = 0.40245304\n",
      "Iteration 158, loss = 0.40067549\n",
      "Iteration 159, loss = 0.39889706\n",
      "Iteration 160, loss = 0.39711681\n",
      "Iteration 161, loss = 0.39534332\n",
      "Iteration 162, loss = 0.39357998\n",
      "Iteration 163, loss = 0.39182502\n",
      "Iteration 164, loss = 0.39007321\n",
      "Iteration 165, loss = 0.38832305\n",
      "Iteration 166, loss = 0.38657591\n",
      "Iteration 167, loss = 0.38483342\n",
      "Iteration 168, loss = 0.38310094\n",
      "Iteration 169, loss = 0.38137334\n",
      "Iteration 170, loss = 0.37965073\n",
      "Iteration 171, loss = 0.37793207\n",
      "Iteration 172, loss = 0.37621716\n",
      "Iteration 173, loss = 0.37450706\n",
      "Iteration 174, loss = 0.37280208\n",
      "Iteration 175, loss = 0.37110210\n",
      "Iteration 176, loss = 0.36940753\n",
      "Iteration 177, loss = 0.36771857\n",
      "Iteration 178, loss = 0.36603446\n",
      "Iteration 179, loss = 0.36435563\n",
      "Iteration 180, loss = 0.36268146\n",
      "Iteration 181, loss = 0.36101301\n",
      "Iteration 182, loss = 0.35935021\n",
      "Iteration 183, loss = 0.35769264\n",
      "Iteration 184, loss = 0.35604147\n",
      "Iteration 185, loss = 0.35439593\n",
      "Iteration 186, loss = 0.35275562\n",
      "Iteration 187, loss = 0.35112065\n",
      "Iteration 188, loss = 0.34949176\n",
      "Iteration 189, loss = 0.34786911\n",
      "Iteration 190, loss = 0.34625200\n",
      "Iteration 191, loss = 0.34464045\n",
      "Iteration 192, loss = 0.34303534\n",
      "Iteration 193, loss = 0.34143628\n",
      "Iteration 194, loss = 0.33984280\n",
      "Iteration 195, loss = 0.33825561\n",
      "Iteration 196, loss = 0.33667427\n",
      "Iteration 197, loss = 0.33509886\n",
      "Iteration 198, loss = 0.33352982\n",
      "Iteration 199, loss = 0.33196661\n",
      "Iteration 200, loss = 0.33040459\n",
      "Iteration 201, loss = 0.32884049\n",
      "Iteration 202, loss = 0.32728244\n",
      "Iteration 203, loss = 0.32573515\n",
      "Iteration 204, loss = 0.32419010\n",
      "Iteration 205, loss = 0.32264638\n",
      "Iteration 206, loss = 0.32110371\n",
      "Iteration 207, loss = 0.31956183\n",
      "Iteration 208, loss = 0.31802067\n",
      "Iteration 209, loss = 0.31648048\n",
      "Iteration 210, loss = 0.31494109\n",
      "Iteration 211, loss = 0.31340270\n",
      "Iteration 212, loss = 0.31186517\n",
      "Iteration 213, loss = 0.31032922\n",
      "Iteration 214, loss = 0.30879430\n",
      "Iteration 215, loss = 0.30726055\n",
      "Iteration 216, loss = 0.30572838\n",
      "Iteration 217, loss = 0.30419780\n",
      "Iteration 218, loss = 0.30266930\n",
      "Iteration 219, loss = 0.30114229\n",
      "Iteration 220, loss = 0.29961700\n",
      "Iteration 221, loss = 0.29809565\n",
      "Iteration 222, loss = 0.29657664\n",
      "Iteration 223, loss = 0.29506013\n",
      "Iteration 224, loss = 0.29354657\n",
      "Iteration 225, loss = 0.29203636\n",
      "Iteration 226, loss = 0.29052965\n",
      "Iteration 227, loss = 0.28902732\n",
      "Iteration 228, loss = 0.28752909\n",
      "Iteration 229, loss = 0.28603511\n",
      "Iteration 230, loss = 0.28454579\n",
      "Iteration 231, loss = 0.28306092\n",
      "Iteration 232, loss = 0.28158106\n",
      "Iteration 233, loss = 0.28010631\n",
      "Iteration 234, loss = 0.27863697\n",
      "Iteration 235, loss = 0.27717321\n",
      "Iteration 236, loss = 0.27571530\n",
      "Iteration 237, loss = 0.27426360\n",
      "Iteration 238, loss = 0.27281800\n",
      "Iteration 239, loss = 0.27137868\n",
      "Iteration 240, loss = 0.26994554\n",
      "Iteration 241, loss = 0.26851399\n",
      "Iteration 242, loss = 0.26708590\n",
      "Iteration 243, loss = 0.26566848\n",
      "Iteration 244, loss = 0.26425562\n",
      "Iteration 245, loss = 0.26284663\n",
      "Iteration 246, loss = 0.26144163\n",
      "Iteration 247, loss = 0.26004095\n",
      "Iteration 248, loss = 0.25864425\n",
      "Iteration 249, loss = 0.25725184\n",
      "Iteration 250, loss = 0.25586361\n",
      "Iteration 251, loss = 0.25447929\n",
      "Iteration 252, loss = 0.25309881\n",
      "Iteration 253, loss = 0.25172224\n",
      "Iteration 254, loss = 0.25034940\n",
      "Iteration 255, loss = 0.24898030\n",
      "Iteration 256, loss = 0.24761516\n",
      "Iteration 257, loss = 0.24625443\n",
      "Iteration 258, loss = 0.24489813\n",
      "Iteration 259, loss = 0.24354654\n",
      "Iteration 260, loss = 0.24219939\n",
      "Iteration 261, loss = 0.24085688\n",
      "Iteration 262, loss = 0.23951940\n",
      "Iteration 263, loss = 0.23818705\n",
      "Iteration 264, loss = 0.23685999\n",
      "Iteration 265, loss = 0.23553837\n",
      "Iteration 266, loss = 0.23422212\n",
      "Iteration 267, loss = 0.23291148\n",
      "Iteration 268, loss = 0.23160633\n",
      "Iteration 269, loss = 0.23030669\n",
      "Iteration 270, loss = 0.22901256\n",
      "Iteration 271, loss = 0.22772366\n",
      "Iteration 272, loss = 0.22643968\n",
      "Iteration 273, loss = 0.22516053\n",
      "Iteration 274, loss = 0.22388535\n",
      "Iteration 275, loss = 0.22261352\n",
      "Iteration 276, loss = 0.22134478\n",
      "Iteration 277, loss = 0.22007915\n",
      "Iteration 278, loss = 0.21881969\n",
      "Iteration 279, loss = 0.21756175\n",
      "Iteration 280, loss = 0.21630612\n",
      "Iteration 281, loss = 0.21505360\n",
      "Iteration 282, loss = 0.21380659\n",
      "Iteration 283, loss = 0.21256298\n",
      "Iteration 284, loss = 0.21131773\n",
      "Iteration 285, loss = 0.21007858\n",
      "Iteration 286, loss = 0.20884487\n",
      "Iteration 287, loss = 0.20761928\n",
      "Iteration 288, loss = 0.20640390\n",
      "Iteration 289, loss = 0.20519783\n",
      "Iteration 290, loss = 0.20400422\n",
      "Iteration 291, loss = 0.20282363\n",
      "Iteration 292, loss = 0.20165118\n",
      "Iteration 293, loss = 0.20048683\n",
      "Iteration 294, loss = 0.19932833\n",
      "Iteration 295, loss = 0.19818042\n",
      "Iteration 296, loss = 0.19704187\n",
      "Iteration 297, loss = 0.19591212\n",
      "Iteration 298, loss = 0.19479098\n",
      "Iteration 299, loss = 0.19367894\n",
      "Iteration 300, loss = 0.19257576\n",
      "Iteration 301, loss = 0.19148215\n",
      "Iteration 302, loss = 0.19039718\n",
      "Iteration 303, loss = 0.18932124\n",
      "Iteration 304, loss = 0.18825354\n",
      "Iteration 305, loss = 0.18719434\n",
      "Iteration 306, loss = 0.18614336\n",
      "Iteration 307, loss = 0.18509950\n",
      "Iteration 308, loss = 0.18406303\n",
      "Iteration 309, loss = 0.18303689\n",
      "Iteration 310, loss = 0.18201867\n",
      "Iteration 311, loss = 0.18100976\n",
      "Iteration 312, loss = 0.18000761\n",
      "Iteration 313, loss = 0.17901275\n",
      "Iteration 314, loss = 0.17802812\n",
      "Iteration 315, loss = 0.17705009\n",
      "Iteration 316, loss = 0.17608154\n",
      "Iteration 317, loss = 0.17512038\n",
      "Iteration 318, loss = 0.17416819\n",
      "Iteration 319, loss = 0.17322376\n",
      "Iteration 320, loss = 0.17228694\n",
      "Iteration 321, loss = 0.17135910\n",
      "Iteration 322, loss = 0.17043905\n",
      "Iteration 323, loss = 0.16952711\n",
      "Iteration 324, loss = 0.16862395\n",
      "Iteration 325, loss = 0.16772857\n",
      "Iteration 326, loss = 0.16684135\n",
      "Iteration 327, loss = 0.16596280\n",
      "Iteration 328, loss = 0.16509214\n",
      "Iteration 329, loss = 0.16422948\n",
      "Iteration 330, loss = 0.16337547\n",
      "Iteration 331, loss = 0.16252961\n",
      "Iteration 332, loss = 0.16169183\n",
      "Iteration 333, loss = 0.16086220\n",
      "Iteration 334, loss = 0.16004093\n",
      "Iteration 335, loss = 0.15922771\n",
      "Iteration 336, loss = 0.15842273\n",
      "Iteration 337, loss = 0.15762601\n",
      "Iteration 338, loss = 0.15683774\n",
      "Iteration 339, loss = 0.15605758\n",
      "Iteration 340, loss = 0.15528509\n",
      "Iteration 341, loss = 0.15452003\n",
      "Iteration 342, loss = 0.15376250\n",
      "Iteration 343, loss = 0.15301246\n",
      "Iteration 344, loss = 0.15226952\n",
      "Iteration 345, loss = 0.15153362\n",
      "Iteration 346, loss = 0.15080469\n",
      "Iteration 347, loss = 0.15008222\n",
      "Iteration 348, loss = 0.14936612\n",
      "Iteration 349, loss = 0.14865648\n",
      "Iteration 350, loss = 0.14795301\n",
      "Iteration 351, loss = 0.14725564\n",
      "Iteration 352, loss = 0.14656421\n",
      "Iteration 353, loss = 0.14587855\n",
      "Iteration 354, loss = 0.14519866\n",
      "Iteration 355, loss = 0.14452459\n",
      "Iteration 356, loss = 0.14385616\n",
      "Iteration 357, loss = 0.14319361\n",
      "Iteration 358, loss = 0.14253667\n",
      "Iteration 359, loss = 0.14188556\n",
      "Iteration 360, loss = 0.14124046\n",
      "Iteration 361, loss = 0.14060091\n",
      "Iteration 362, loss = 0.13996735\n",
      "Iteration 363, loss = 0.13934005\n",
      "Iteration 364, loss = 0.13871834\n",
      "Iteration 365, loss = 0.13810095\n",
      "Iteration 366, loss = 0.13748728\n",
      "Iteration 367, loss = 0.13688029\n",
      "Iteration 368, loss = 0.13627851\n",
      "Iteration 369, loss = 0.13568132\n",
      "Iteration 370, loss = 0.13508867\n",
      "Iteration 371, loss = 0.13450048\n",
      "Iteration 372, loss = 0.13391692\n",
      "Iteration 373, loss = 0.13333824\n",
      "Iteration 374, loss = 0.13276415\n",
      "Iteration 375, loss = 0.13219465\n",
      "Iteration 376, loss = 0.13163030\n",
      "Iteration 377, loss = 0.13107107\n",
      "Iteration 378, loss = 0.13051570\n",
      "Iteration 379, loss = 0.12996472\n",
      "Iteration 380, loss = 0.12941895\n",
      "Iteration 381, loss = 0.12887798\n",
      "Iteration 382, loss = 0.12834185\n",
      "Iteration 383, loss = 0.12781054\n",
      "Iteration 384, loss = 0.12728387\n",
      "Iteration 385, loss = 0.12676202\n",
      "Iteration 386, loss = 0.12624516\n",
      "Iteration 387, loss = 0.12573302\n",
      "Iteration 388, loss = 0.12522568\n",
      "Iteration 389, loss = 0.12472336\n",
      "Iteration 390, loss = 0.12422604\n",
      "Iteration 391, loss = 0.12373348\n",
      "Iteration 392, loss = 0.12324571\n",
      "Iteration 393, loss = 0.12276277\n",
      "Iteration 394, loss = 0.12228450\n",
      "Iteration 395, loss = 0.12181068\n",
      "Iteration 396, loss = 0.12134166\n",
      "Iteration 397, loss = 0.12087707\n",
      "Iteration 398, loss = 0.12041707\n",
      "Iteration 399, loss = 0.11996130\n",
      "Iteration 400, loss = 0.11950971\n",
      "Iteration 401, loss = 0.11906224\n",
      "Iteration 402, loss = 0.11861880\n",
      "Iteration 403, loss = 0.11817933\n",
      "Iteration 404, loss = 0.11774388\n",
      "Iteration 405, loss = 0.11731247\n",
      "Iteration 406, loss = 0.11688498\n",
      "Iteration 407, loss = 0.11646145\n",
      "Iteration 408, loss = 0.11604192\n",
      "Iteration 409, loss = 0.11562633\n",
      "Iteration 410, loss = 0.11521472\n",
      "Iteration 411, loss = 0.11480710\n",
      "Iteration 412, loss = 0.11440356\n",
      "Iteration 413, loss = 0.11400398\n",
      "Iteration 414, loss = 0.11360860\n",
      "Iteration 415, loss = 0.11321734\n",
      "Iteration 416, loss = 0.11283008\n",
      "Iteration 417, loss = 0.11244699\n",
      "Iteration 418, loss = 0.11206807\n",
      "Iteration 419, loss = 0.11169320\n",
      "Iteration 420, loss = 0.11132232\n",
      "Iteration 421, loss = 0.11095542\n",
      "Iteration 422, loss = 0.11059244\n",
      "Iteration 423, loss = 0.11023307\n",
      "Iteration 424, loss = 0.10987765\n",
      "Iteration 425, loss = 0.10952614\n",
      "Iteration 426, loss = 0.10917840\n",
      "Iteration 427, loss = 0.10883440\n",
      "Iteration 428, loss = 0.10849419\n",
      "Iteration 429, loss = 0.10815774\n",
      "Iteration 430, loss = 0.10782494\n",
      "Iteration 431, loss = 0.10749582\n",
      "Iteration 432, loss = 0.10717025\n",
      "Iteration 433, loss = 0.10684821\n",
      "Iteration 434, loss = 0.10652962\n",
      "Iteration 435, loss = 0.10621442\n",
      "Iteration 436, loss = 0.10590257\n",
      "Iteration 437, loss = 0.10559402\n",
      "Iteration 438, loss = 0.10528879\n",
      "Iteration 439, loss = 0.10498670\n",
      "Iteration 440, loss = 0.10468776\n",
      "Iteration 441, loss = 0.10439203\n",
      "Iteration 442, loss = 0.10409936\n",
      "Iteration 443, loss = 0.10380968\n",
      "Iteration 444, loss = 0.10352292\n",
      "Iteration 445, loss = 0.10323917\n",
      "Iteration 446, loss = 0.10295829\n",
      "Iteration 447, loss = 0.10268030\n",
      "Iteration 448, loss = 0.10240513\n",
      "Iteration 449, loss = 0.10213288\n",
      "Iteration 450, loss = 0.10186329\n",
      "Iteration 451, loss = 0.10159660\n",
      "Iteration 452, loss = 0.10133261\n",
      "Iteration 453, loss = 0.10107136\n",
      "Iteration 454, loss = 0.10081270\n",
      "Iteration 455, loss = 0.10055687\n",
      "Iteration 456, loss = 0.10030370\n",
      "Iteration 457, loss = 0.10005320\n",
      "Iteration 458, loss = 0.09980528\n",
      "Iteration 459, loss = 0.09955992\n",
      "Iteration 460, loss = 0.09931714\n",
      "Iteration 461, loss = 0.09907692\n",
      "Iteration 462, loss = 0.09883915\n",
      "Iteration 463, loss = 0.09860383\n",
      "Iteration 464, loss = 0.09837111\n",
      "Iteration 465, loss = 0.09814090\n",
      "Iteration 466, loss = 0.09791303\n",
      "Iteration 467, loss = 0.09768753\n",
      "Iteration 468, loss = 0.09746455\n",
      "Iteration 469, loss = 0.09724389\n",
      "Iteration 470, loss = 0.09702547\n",
      "Iteration 471, loss = 0.09680926\n",
      "Iteration 472, loss = 0.09659529\n",
      "Iteration 473, loss = 0.09638352\n",
      "Iteration 474, loss = 0.09617404\n",
      "Iteration 475, loss = 0.09596655\n",
      "Iteration 476, loss = 0.09576143\n",
      "Iteration 477, loss = 0.09555840\n",
      "Iteration 478, loss = 0.09535746\n",
      "Iteration 479, loss = 0.09515852\n",
      "Iteration 480, loss = 0.09496167\n",
      "Iteration 481, loss = 0.09476683\n",
      "Iteration 482, loss = 0.09457406\n",
      "Iteration 483, loss = 0.09438345\n",
      "Iteration 484, loss = 0.09419488\n",
      "Iteration 485, loss = 0.09400827\n",
      "Iteration 486, loss = 0.09382372\n",
      "Iteration 487, loss = 0.09364127\n",
      "Iteration 488, loss = 0.09346081\n",
      "Iteration 489, loss = 0.09328232\n",
      "Iteration 490, loss = 0.09310583\n",
      "Iteration 491, loss = 0.09293132\n",
      "Iteration 492, loss = 0.09275883\n",
      "Iteration 493, loss = 0.09258831\n",
      "Iteration 494, loss = 0.09241980\n",
      "Iteration 495, loss = 0.09225331\n",
      "Iteration 496, loss = 0.09208876\n",
      "Iteration 497, loss = 0.09192613\n",
      "Iteration 498, loss = 0.09176546\n",
      "Iteration 499, loss = 0.09160673\n",
      "Iteration 500, loss = 0.09144995\n",
      "Iteration 501, loss = 0.09129515\n",
      "Iteration 502, loss = 0.09114230\n",
      "Iteration 503, loss = 0.09099135\n",
      "Iteration 504, loss = 0.09084230\n",
      "Iteration 505, loss = 0.09069516\n",
      "Iteration 506, loss = 0.09054995\n",
      "Iteration 507, loss = 0.09040657\n",
      "Iteration 508, loss = 0.09026504\n",
      "Iteration 509, loss = 0.09012540\n",
      "Iteration 510, loss = 0.08998762\n",
      "Iteration 511, loss = 0.08985162\n",
      "Iteration 512, loss = 0.08971736\n",
      "Iteration 513, loss = 0.08958483\n",
      "Iteration 514, loss = 0.08945406\n",
      "Iteration 515, loss = 0.08932508\n",
      "Iteration 516, loss = 0.08919792\n",
      "Iteration 517, loss = 0.08907242\n",
      "Iteration 518, loss = 0.08894854\n",
      "Iteration 519, loss = 0.08882632\n",
      "Iteration 520, loss = 0.08870571\n",
      "Iteration 521, loss = 0.08858677\n",
      "Iteration 522, loss = 0.08846935\n",
      "Iteration 523, loss = 0.08835347\n",
      "Iteration 524, loss = 0.08823915\n",
      "Iteration 525, loss = 0.08812641\n",
      "Iteration 526, loss = 0.08801508\n",
      "Iteration 527, loss = 0.08790524\n",
      "Iteration 528, loss = 0.08779709\n",
      "Iteration 529, loss = 0.08769067\n",
      "Iteration 530, loss = 0.08758614\n",
      "Iteration 531, loss = 0.08748257\n",
      "Iteration 532, loss = 0.08737924\n",
      "Iteration 533, loss = 0.08727671\n",
      "Iteration 534, loss = 0.08717686\n",
      "Iteration 535, loss = 0.08707907\n",
      "Iteration 536, loss = 0.08698137\n",
      "Iteration 537, loss = 0.08688435\n",
      "Iteration 538, loss = 0.08678941\n",
      "Iteration 539, loss = 0.08669609\n",
      "Iteration 540, loss = 0.08660313\n",
      "Iteration 541, loss = 0.08651089\n",
      "Iteration 542, loss = 0.08642051\n",
      "Iteration 543, loss = 0.08633143\n",
      "Iteration 544, loss = 0.08624262\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66212118\n",
      "Iteration 2, loss = 1.62570402\n",
      "Iteration 3, loss = 1.57682010\n",
      "Iteration 4, loss = 1.51974280\n",
      "Iteration 5, loss = 1.45893919\n",
      "Iteration 6, loss = 1.39889203\n",
      "Iteration 7, loss = 1.34331287\n",
      "Iteration 8, loss = 1.29530610\n",
      "Iteration 9, loss = 1.25675812\n",
      "Iteration 10, loss = 1.22789841\n",
      "Iteration 11, loss = 1.20744254\n",
      "Iteration 12, loss = 1.19295399\n",
      "Iteration 13, loss = 1.18160684\n",
      "Iteration 14, loss = 1.17076766\n",
      "Iteration 15, loss = 1.15849353\n",
      "Iteration 16, loss = 1.14370805\n",
      "Iteration 17, loss = 1.12652765\n",
      "Iteration 18, loss = 1.10732691\n",
      "Iteration 19, loss = 1.08645030\n",
      "Iteration 20, loss = 1.06487701\n",
      "Iteration 21, loss = 1.04359787\n",
      "Iteration 22, loss = 1.02349592\n",
      "Iteration 23, loss = 1.00519789\n",
      "Iteration 24, loss = 0.98866827\n",
      "Iteration 25, loss = 0.97394585\n",
      "Iteration 26, loss = 0.96078301\n",
      "Iteration 27, loss = 0.94887654\n",
      "Iteration 28, loss = 0.93788412\n",
      "Iteration 29, loss = 0.92744302\n",
      "Iteration 30, loss = 0.91732945\n",
      "Iteration 31, loss = 0.90739655\n",
      "Iteration 32, loss = 0.89758373\n",
      "Iteration 33, loss = 0.88798730\n",
      "Iteration 34, loss = 0.87873773\n",
      "Iteration 35, loss = 0.86979518\n",
      "Iteration 36, loss = 0.86119824\n",
      "Iteration 37, loss = 0.85301655\n",
      "Iteration 38, loss = 0.84513816\n",
      "Iteration 39, loss = 0.83756595\n",
      "Iteration 40, loss = 0.83030391\n",
      "Iteration 41, loss = 0.82330530\n",
      "Iteration 42, loss = 0.81654807\n",
      "Iteration 43, loss = 0.80999720\n",
      "Iteration 44, loss = 0.80359172\n",
      "Iteration 45, loss = 0.79729105\n",
      "Iteration 46, loss = 0.79109317\n",
      "Iteration 47, loss = 0.78503159\n",
      "Iteration 48, loss = 0.77904732\n",
      "Iteration 49, loss = 0.77313854\n",
      "Iteration 50, loss = 0.76729171\n",
      "Iteration 51, loss = 0.76151905\n",
      "Iteration 52, loss = 0.75584695\n",
      "Iteration 53, loss = 0.75031049\n",
      "Iteration 54, loss = 0.74490187\n",
      "Iteration 55, loss = 0.73961615\n",
      "Iteration 56, loss = 0.73447619\n",
      "Iteration 57, loss = 0.72947140\n",
      "Iteration 58, loss = 0.72460764\n",
      "Iteration 59, loss = 0.71988514\n",
      "Iteration 60, loss = 0.71529304\n",
      "Iteration 61, loss = 0.71082389\n",
      "Iteration 62, loss = 0.70648466\n",
      "Iteration 63, loss = 0.70223810\n",
      "Iteration 64, loss = 0.69809809\n",
      "Iteration 65, loss = 0.69404592\n",
      "Iteration 66, loss = 0.69008470\n",
      "Iteration 67, loss = 0.68621105\n",
      "Iteration 68, loss = 0.68241794\n",
      "Iteration 69, loss = 0.67870252\n",
      "Iteration 70, loss = 0.67506124\n",
      "Iteration 71, loss = 0.67149221\n",
      "Iteration 72, loss = 0.66799380\n",
      "Iteration 73, loss = 0.66456247\n",
      "Iteration 74, loss = 0.66119579\n",
      "Iteration 75, loss = 0.65789286\n",
      "Iteration 76, loss = 0.65465153\n",
      "Iteration 77, loss = 0.65146547\n",
      "Iteration 78, loss = 0.64833382\n",
      "Iteration 79, loss = 0.64525788\n",
      "Iteration 80, loss = 0.64223721\n",
      "Iteration 81, loss = 0.63926853\n",
      "Iteration 82, loss = 0.63635327\n",
      "Iteration 83, loss = 0.63348978\n",
      "Iteration 84, loss = 0.63067609\n",
      "Iteration 85, loss = 0.62790389\n",
      "Iteration 86, loss = 0.62517316\n",
      "Iteration 87, loss = 0.62248592\n",
      "Iteration 88, loss = 0.61984330\n",
      "Iteration 89, loss = 0.61724461\n",
      "Iteration 90, loss = 0.61468565\n",
      "Iteration 91, loss = 0.61216694\n",
      "Iteration 92, loss = 0.60968813\n",
      "Iteration 93, loss = 0.60724706\n",
      "Iteration 94, loss = 0.60484221\n",
      "Iteration 95, loss = 0.60247533\n",
      "Iteration 96, loss = 0.60014616\n",
      "Iteration 97, loss = 0.59785337\n",
      "Iteration 98, loss = 0.59559398\n",
      "Iteration 99, loss = 0.59336685\n",
      "Iteration 100, loss = 0.59117249\n",
      "Iteration 101, loss = 0.58901143\n",
      "Iteration 102, loss = 0.58688267\n",
      "Iteration 103, loss = 0.58478515\n",
      "Iteration 104, loss = 0.58271892\n",
      "Iteration 105, loss = 0.58068307\n",
      "Iteration 106, loss = 0.57867674\n",
      "Iteration 107, loss = 0.57669889\n",
      "Iteration 108, loss = 0.57474933\n",
      "Iteration 109, loss = 0.57282735\n",
      "Iteration 110, loss = 0.57093234\n",
      "Iteration 111, loss = 0.56906382\n",
      "Iteration 112, loss = 0.56722118\n",
      "Iteration 113, loss = 0.56540460\n",
      "Iteration 114, loss = 0.56361308\n",
      "Iteration 115, loss = 0.56184300\n",
      "Iteration 116, loss = 0.56009632\n",
      "Iteration 117, loss = 0.55837262\n",
      "Iteration 118, loss = 0.55667132\n",
      "Iteration 119, loss = 0.55499194\n",
      "Iteration 120, loss = 0.55333433\n",
      "Iteration 121, loss = 0.55169762\n",
      "Iteration 122, loss = 0.55008136\n",
      "Iteration 123, loss = 0.54848509\n",
      "Iteration 124, loss = 0.54690849\n",
      "Iteration 125, loss = 0.54535120\n",
      "Iteration 126, loss = 0.54381277\n",
      "Iteration 127, loss = 0.54229272\n",
      "Iteration 128, loss = 0.54079061\n",
      "Iteration 129, loss = 0.53930615\n",
      "Iteration 130, loss = 0.53783882\n",
      "Iteration 131, loss = 0.53638820\n",
      "Iteration 132, loss = 0.53495422\n",
      "Iteration 133, loss = 0.53353630\n",
      "Iteration 134, loss = 0.53213408\n",
      "Iteration 135, loss = 0.53074761\n",
      "Iteration 136, loss = 0.52937624\n",
      "Iteration 137, loss = 0.52801916\n",
      "Iteration 138, loss = 0.52667577\n",
      "Iteration 139, loss = 0.52534642\n",
      "Iteration 140, loss = 0.52403078\n",
      "Iteration 141, loss = 0.52272857\n",
      "Iteration 142, loss = 0.52143953\n",
      "Iteration 143, loss = 0.52016348\n",
      "Iteration 144, loss = 0.51890166\n",
      "Iteration 145, loss = 0.51765224\n",
      "Iteration 146, loss = 0.51641499\n",
      "Iteration 147, loss = 0.51518872\n",
      "Iteration 148, loss = 0.51397404\n",
      "Iteration 149, loss = 0.51277050\n",
      "Iteration 150, loss = 0.51157827\n",
      "Iteration 151, loss = 0.51039709\n",
      "Iteration 152, loss = 0.50922634\n",
      "Iteration 153, loss = 0.50806526\n",
      "Iteration 154, loss = 0.50691457\n",
      "Iteration 155, loss = 0.50577417\n",
      "Iteration 156, loss = 0.50464377\n",
      "Iteration 157, loss = 0.50352313\n",
      "Iteration 158, loss = 0.50241137\n",
      "Iteration 159, loss = 0.50130927\n",
      "Iteration 160, loss = 0.50021724\n",
      "Iteration 161, loss = 0.49913401\n",
      "Iteration 162, loss = 0.49805953\n",
      "Iteration 163, loss = 0.49699393\n",
      "Iteration 164, loss = 0.49593585\n",
      "Iteration 165, loss = 0.49488371\n",
      "Iteration 166, loss = 0.49383953\n",
      "Iteration 167, loss = 0.49280336\n",
      "Iteration 168, loss = 0.49177540\n",
      "Iteration 169, loss = 0.49075656\n",
      "Iteration 170, loss = 0.48974603\n",
      "Iteration 171, loss = 0.48874162\n",
      "Iteration 172, loss = 0.48774254\n",
      "Iteration 173, loss = 0.48675049\n",
      "Iteration 174, loss = 0.48576605\n",
      "Iteration 175, loss = 0.48478763\n",
      "Iteration 176, loss = 0.48381544\n",
      "Iteration 177, loss = 0.48284788\n",
      "Iteration 178, loss = 0.48188203\n",
      "Iteration 179, loss = 0.48092327\n",
      "Iteration 180, loss = 0.47997032\n",
      "Iteration 181, loss = 0.47902210\n",
      "Iteration 182, loss = 0.47807762\n",
      "Iteration 183, loss = 0.47713583\n",
      "Iteration 184, loss = 0.47619737\n",
      "Iteration 185, loss = 0.47526141\n",
      "Iteration 186, loss = 0.47432547\n",
      "Iteration 187, loss = 0.47339066\n",
      "Iteration 188, loss = 0.47245931\n",
      "Iteration 189, loss = 0.47153017\n",
      "Iteration 190, loss = 0.47061191\n",
      "Iteration 191, loss = 0.46969445\n",
      "Iteration 192, loss = 0.46878099\n",
      "Iteration 193, loss = 0.46787769\n",
      "Iteration 194, loss = 0.46697875\n",
      "Iteration 195, loss = 0.46608788\n",
      "Iteration 196, loss = 0.46520866\n",
      "Iteration 197, loss = 0.46433977\n",
      "Iteration 198, loss = 0.46347709\n",
      "Iteration 199, loss = 0.46262525\n",
      "Iteration 200, loss = 0.46178176\n",
      "Iteration 201, loss = 0.46095206\n",
      "Iteration 202, loss = 0.46013137\n",
      "Iteration 203, loss = 0.45931913\n",
      "Iteration 204, loss = 0.45851177\n",
      "Iteration 205, loss = 0.45770945\n",
      "Iteration 206, loss = 0.45691469\n",
      "Iteration 207, loss = 0.45612499\n",
      "Iteration 208, loss = 0.45534085\n",
      "Iteration 209, loss = 0.45456073\n",
      "Iteration 210, loss = 0.45378459\n",
      "Iteration 211, loss = 0.45301237\n",
      "Iteration 212, loss = 0.45224350\n",
      "Iteration 213, loss = 0.45147904\n",
      "Iteration 214, loss = 0.45071817\n",
      "Iteration 215, loss = 0.44996138\n",
      "Iteration 216, loss = 0.44920805\n",
      "Iteration 217, loss = 0.44845803\n",
      "Iteration 218, loss = 0.44771157\n",
      "Iteration 219, loss = 0.44696823\n",
      "Iteration 220, loss = 0.44622809\n",
      "Iteration 221, loss = 0.44548968\n",
      "Iteration 222, loss = 0.44475493\n",
      "Iteration 223, loss = 0.44402329\n",
      "Iteration 224, loss = 0.44329485\n",
      "Iteration 225, loss = 0.44256955\n",
      "Iteration 226, loss = 0.44184738\n",
      "Iteration 227, loss = 0.44112827\n",
      "Iteration 228, loss = 0.44041196\n",
      "Iteration 229, loss = 0.43969868\n",
      "Iteration 230, loss = 0.43898843\n",
      "Iteration 231, loss = 0.43828117\n",
      "Iteration 232, loss = 0.43757752\n",
      "Iteration 233, loss = 0.43687530\n",
      "Iteration 234, loss = 0.43617097\n",
      "Iteration 235, loss = 0.43546953\n",
      "Iteration 236, loss = 0.43476694\n",
      "Iteration 237, loss = 0.43406636\n",
      "Iteration 238, loss = 0.43336783\n",
      "Iteration 239, loss = 0.43267142\n",
      "Iteration 240, loss = 0.43197628\n",
      "Iteration 241, loss = 0.43128200\n",
      "Iteration 242, loss = 0.43058952\n",
      "Iteration 243, loss = 0.42989897\n",
      "Iteration 244, loss = 0.42921067\n",
      "Iteration 245, loss = 0.42852452\n",
      "Iteration 246, loss = 0.42783675\n",
      "Iteration 247, loss = 0.42714823\n",
      "Iteration 248, loss = 0.42645895\n",
      "Iteration 249, loss = 0.42576471\n",
      "Iteration 250, loss = 0.42506508\n",
      "Iteration 251, loss = 0.42436104\n",
      "Iteration 252, loss = 0.42364829\n",
      "Iteration 253, loss = 0.42292781\n",
      "Iteration 254, loss = 0.42220226\n",
      "Iteration 255, loss = 0.42146031\n",
      "Iteration 256, loss = 0.42069426\n",
      "Iteration 257, loss = 0.41991332\n",
      "Iteration 258, loss = 0.41911859\n",
      "Iteration 259, loss = 0.41830568\n",
      "Iteration 260, loss = 0.41747449\n",
      "Iteration 261, loss = 0.41661485\n",
      "Iteration 262, loss = 0.41574421\n",
      "Iteration 263, loss = 0.41487229\n",
      "Iteration 264, loss = 0.41401236\n",
      "Iteration 265, loss = 0.41316504\n",
      "Iteration 266, loss = 0.41229745\n",
      "Iteration 267, loss = 0.41144295\n",
      "Iteration 268, loss = 0.41062536\n",
      "Iteration 269, loss = 0.40984575\n",
      "Iteration 270, loss = 0.40910055\n",
      "Iteration 271, loss = 0.40836580\n",
      "Iteration 272, loss = 0.40765527\n",
      "Iteration 273, loss = 0.40697637\n",
      "Iteration 274, loss = 0.40630910\n",
      "Iteration 275, loss = 0.40566002\n",
      "Iteration 276, loss = 0.40501993\n",
      "Iteration 277, loss = 0.40439097\n",
      "Iteration 278, loss = 0.40376333\n",
      "Iteration 279, loss = 0.40314390\n",
      "Iteration 280, loss = 0.40253692\n",
      "Iteration 281, loss = 0.40193508\n",
      "Iteration 282, loss = 0.40133911\n",
      "Iteration 283, loss = 0.40074597\n",
      "Iteration 284, loss = 0.40015652\n",
      "Iteration 285, loss = 0.39957013\n",
      "Iteration 286, loss = 0.39898943\n",
      "Iteration 287, loss = 0.39841452\n",
      "Iteration 288, loss = 0.39784224\n",
      "Iteration 289, loss = 0.39727521\n",
      "Iteration 290, loss = 0.39671015\n",
      "Iteration 291, loss = 0.39614743\n",
      "Iteration 292, loss = 0.39558709\n",
      "Iteration 293, loss = 0.39502908\n",
      "Iteration 294, loss = 0.39447304\n",
      "Iteration 295, loss = 0.39391890\n",
      "Iteration 296, loss = 0.39336653\n",
      "Iteration 297, loss = 0.39281600\n",
      "Iteration 298, loss = 0.39226718\n",
      "Iteration 299, loss = 0.39171988\n",
      "Iteration 300, loss = 0.39117418\n",
      "Iteration 301, loss = 0.39063056\n",
      "Iteration 302, loss = 0.39008795\n",
      "Iteration 303, loss = 0.38954681\n",
      "Iteration 304, loss = 0.38900740\n",
      "Iteration 305, loss = 0.38846933\n",
      "Iteration 306, loss = 0.38793284\n",
      "Iteration 307, loss = 0.38739791\n",
      "Iteration 308, loss = 0.38686481\n",
      "Iteration 309, loss = 0.38633252\n",
      "Iteration 310, loss = 0.38580201\n",
      "Iteration 311, loss = 0.38527286\n",
      "Iteration 312, loss = 0.38474506\n",
      "Iteration 313, loss = 0.38421870\n",
      "Iteration 314, loss = 0.38369375\n",
      "Iteration 315, loss = 0.38317020\n",
      "Iteration 316, loss = 0.38264810\n",
      "Iteration 317, loss = 0.38212766\n",
      "Iteration 318, loss = 0.38160881\n",
      "Iteration 319, loss = 0.38109112\n",
      "Iteration 320, loss = 0.38057469\n",
      "Iteration 321, loss = 0.38005952\n",
      "Iteration 322, loss = 0.37954578\n",
      "Iteration 323, loss = 0.37903340\n",
      "Iteration 324, loss = 0.37852238\n",
      "Iteration 325, loss = 0.37801260\n",
      "Iteration 326, loss = 0.37750413\n",
      "Iteration 327, loss = 0.37699698\n",
      "Iteration 328, loss = 0.37649076\n",
      "Iteration 329, loss = 0.37598578\n",
      "Iteration 330, loss = 0.37548200\n",
      "Iteration 331, loss = 0.37497945\n",
      "Iteration 332, loss = 0.37447823\n",
      "Iteration 333, loss = 0.37397816\n",
      "Iteration 334, loss = 0.37347934\n",
      "Iteration 335, loss = 0.37298189\n",
      "Iteration 336, loss = 0.37248558\n",
      "Iteration 337, loss = 0.37199056\n",
      "Iteration 338, loss = 0.37149676\n",
      "Iteration 339, loss = 0.37100438\n",
      "Iteration 340, loss = 0.37051327\n",
      "Iteration 341, loss = 0.37002340\n",
      "Iteration 342, loss = 0.36953493\n",
      "Iteration 343, loss = 0.36904794\n",
      "Iteration 344, loss = 0.36856212\n",
      "Iteration 345, loss = 0.36807752\n",
      "Iteration 346, loss = 0.36759397\n",
      "Iteration 347, loss = 0.36711170\n",
      "Iteration 348, loss = 0.36663092\n",
      "Iteration 349, loss = 0.36615112\n",
      "Iteration 350, loss = 0.36567253\n",
      "Iteration 351, loss = 0.36519502\n",
      "Iteration 352, loss = 0.36471864\n",
      "Iteration 353, loss = 0.36424350\n",
      "Iteration 354, loss = 0.36376944\n",
      "Iteration 355, loss = 0.36329655\n",
      "Iteration 356, loss = 0.36282480\n",
      "Iteration 357, loss = 0.36235405\n",
      "Iteration 358, loss = 0.36188437\n",
      "Iteration 359, loss = 0.36141611\n",
      "Iteration 360, loss = 0.36094893\n",
      "Iteration 361, loss = 0.36048275\n",
      "Iteration 362, loss = 0.36001765\n",
      "Iteration 363, loss = 0.35955354\n",
      "Iteration 364, loss = 0.35909060\n",
      "Iteration 365, loss = 0.35862874\n",
      "Iteration 366, loss = 0.35816798\n",
      "Iteration 367, loss = 0.35770826\n",
      "Iteration 368, loss = 0.35724975\n",
      "Iteration 369, loss = 0.35679231\n",
      "Iteration 370, loss = 0.35633587\n",
      "Iteration 371, loss = 0.35588037\n",
      "Iteration 372, loss = 0.35542591\n",
      "Iteration 373, loss = 0.35497254\n",
      "Iteration 374, loss = 0.35452005\n",
      "Iteration 375, loss = 0.35406862\n",
      "Iteration 376, loss = 0.35361830\n",
      "Iteration 377, loss = 0.35316904\n",
      "Iteration 378, loss = 0.35272089\n",
      "Iteration 379, loss = 0.35227364\n",
      "Iteration 380, loss = 0.35182742\n",
      "Iteration 381, loss = 0.35138218\n",
      "Iteration 382, loss = 0.35093789\n",
      "Iteration 383, loss = 0.35049436\n",
      "Iteration 384, loss = 0.35005193\n",
      "Iteration 385, loss = 0.34961024\n",
      "Iteration 386, loss = 0.34916963\n",
      "Iteration 387, loss = 0.34872993\n",
      "Iteration 388, loss = 0.34829119\n",
      "Iteration 389, loss = 0.34785353\n",
      "Iteration 390, loss = 0.34741683\n",
      "Iteration 391, loss = 0.34698102\n",
      "Iteration 392, loss = 0.34654615\n",
      "Iteration 393, loss = 0.34611228\n",
      "Iteration 394, loss = 0.34567924\n",
      "Iteration 395, loss = 0.34524715\n",
      "Iteration 396, loss = 0.34481601\n",
      "Iteration 397, loss = 0.34438574\n",
      "Iteration 398, loss = 0.34395629\n",
      "Iteration 399, loss = 0.34352791\n",
      "Iteration 400, loss = 0.34310027\n",
      "Iteration 401, loss = 0.34267386\n",
      "Iteration 402, loss = 0.34224824\n",
      "Iteration 403, loss = 0.34182372\n",
      "Iteration 404, loss = 0.34140019\n",
      "Iteration 405, loss = 0.34097769\n",
      "Iteration 406, loss = 0.34055591\n",
      "Iteration 407, loss = 0.34013513\n",
      "Iteration 408, loss = 0.33971521\n",
      "Iteration 409, loss = 0.33929609\n",
      "Iteration 410, loss = 0.33887792\n",
      "Iteration 411, loss = 0.33846063\n",
      "Iteration 412, loss = 0.33804426\n",
      "Iteration 413, loss = 0.33762879\n",
      "Iteration 414, loss = 0.33721420\n",
      "Iteration 415, loss = 0.33680041\n",
      "Iteration 416, loss = 0.33638759\n",
      "Iteration 417, loss = 0.33597546\n",
      "Iteration 418, loss = 0.33556438\n",
      "Iteration 419, loss = 0.33515425\n",
      "Iteration 420, loss = 0.33474505\n",
      "Iteration 421, loss = 0.33433662\n",
      "Iteration 422, loss = 0.33392914\n",
      "Iteration 423, loss = 0.33352238\n",
      "Iteration 424, loss = 0.33311650\n",
      "Iteration 425, loss = 0.33271143\n",
      "Iteration 426, loss = 0.33230711\n",
      "Iteration 427, loss = 0.33190369\n",
      "Iteration 428, loss = 0.33150108\n",
      "Iteration 429, loss = 0.33109929\n",
      "Iteration 430, loss = 0.33069827\n",
      "Iteration 431, loss = 0.33029809\n",
      "Iteration 432, loss = 0.32989868\n",
      "Iteration 433, loss = 0.32950011\n",
      "Iteration 434, loss = 0.32910233\n",
      "Iteration 435, loss = 0.32870529\n",
      "Iteration 436, loss = 0.32830921\n",
      "Iteration 437, loss = 0.32791376\n",
      "Iteration 438, loss = 0.32751925\n",
      "Iteration 439, loss = 0.32712553\n",
      "Iteration 440, loss = 0.32673262\n",
      "Iteration 441, loss = 0.32634040\n",
      "Iteration 442, loss = 0.32594915\n",
      "Iteration 443, loss = 0.32555845\n",
      "Iteration 444, loss = 0.32516874\n",
      "Iteration 445, loss = 0.32477975\n",
      "Iteration 446, loss = 0.32439150\n",
      "Iteration 447, loss = 0.32400409\n",
      "Iteration 448, loss = 0.32361752\n",
      "Iteration 449, loss = 0.32323164\n",
      "Iteration 450, loss = 0.32284659\n",
      "Iteration 451, loss = 0.32246232\n",
      "Iteration 452, loss = 0.32207881\n",
      "Iteration 453, loss = 0.32169610\n",
      "Iteration 454, loss = 0.32131415\n",
      "Iteration 455, loss = 0.32093292\n",
      "Iteration 456, loss = 0.32055257\n",
      "Iteration 457, loss = 0.32017290\n",
      "Iteration 458, loss = 0.31979399\n",
      "Iteration 459, loss = 0.31941593\n",
      "Iteration 460, loss = 0.31903851\n",
      "Iteration 461, loss = 0.31866197\n",
      "Iteration 462, loss = 0.31828608\n",
      "Iteration 463, loss = 0.31791103\n",
      "Iteration 464, loss = 0.31753670\n",
      "Iteration 465, loss = 0.31716311\n",
      "Iteration 466, loss = 0.31679028\n",
      "Iteration 467, loss = 0.31641833\n",
      "Iteration 468, loss = 0.31604713\n",
      "Iteration 469, loss = 0.31567662\n",
      "Iteration 470, loss = 0.31530695\n",
      "Iteration 471, loss = 0.31493791\n",
      "Iteration 472, loss = 0.31456972\n",
      "Iteration 473, loss = 0.31420216\n",
      "Iteration 474, loss = 0.31383542\n",
      "Iteration 475, loss = 0.31346942\n",
      "Iteration 476, loss = 0.31310405\n",
      "Iteration 477, loss = 0.31273939\n",
      "Iteration 478, loss = 0.31237567\n",
      "Iteration 479, loss = 0.31201243\n",
      "Iteration 480, loss = 0.31164999\n",
      "Iteration 481, loss = 0.31128826\n",
      "Iteration 482, loss = 0.31092729\n",
      "Iteration 483, loss = 0.31056714\n",
      "Iteration 484, loss = 0.31020782\n",
      "Iteration 485, loss = 0.30984931\n",
      "Iteration 486, loss = 0.30949144\n",
      "Iteration 487, loss = 0.30913432\n",
      "Iteration 488, loss = 0.30877792\n",
      "Iteration 489, loss = 0.30842221\n",
      "Iteration 490, loss = 0.30806725\n",
      "Iteration 491, loss = 0.30771295\n",
      "Iteration 492, loss = 0.30735939\n",
      "Iteration 493, loss = 0.30700655\n",
      "Iteration 494, loss = 0.30665444\n",
      "Iteration 495, loss = 0.30630302\n",
      "Iteration 496, loss = 0.30595234\n",
      "Iteration 497, loss = 0.30560235\n",
      "Iteration 498, loss = 0.30525306\n",
      "Iteration 499, loss = 0.30490445\n",
      "Iteration 500, loss = 0.30455659\n",
      "Iteration 501, loss = 0.30420931\n",
      "Iteration 502, loss = 0.30386286\n",
      "Iteration 503, loss = 0.30351703\n",
      "Iteration 504, loss = 0.30317189\n",
      "Iteration 505, loss = 0.30282740\n",
      "Iteration 506, loss = 0.30248364\n",
      "Iteration 507, loss = 0.30214053\n",
      "Iteration 508, loss = 0.30179814\n",
      "Iteration 509, loss = 0.30145638\n",
      "Iteration 510, loss = 0.30111533\n",
      "Iteration 511, loss = 0.30077496\n",
      "Iteration 512, loss = 0.30043517\n",
      "Iteration 513, loss = 0.30009617\n",
      "Iteration 514, loss = 0.29975775\n",
      "Iteration 515, loss = 0.29942010\n",
      "Iteration 516, loss = 0.29908302\n",
      "Iteration 517, loss = 0.29874666\n",
      "Iteration 518, loss = 0.29841094\n",
      "Iteration 519, loss = 0.29807588\n",
      "Iteration 520, loss = 0.29774153\n",
      "Iteration 521, loss = 0.29740773\n",
      "Iteration 522, loss = 0.29707472\n",
      "Iteration 523, loss = 0.29674228\n",
      "Iteration 524, loss = 0.29641048\n",
      "Iteration 525, loss = 0.29607935\n",
      "Iteration 526, loss = 0.29574887\n",
      "Iteration 527, loss = 0.29541909\n",
      "Iteration 528, loss = 0.29508990\n",
      "Iteration 529, loss = 0.29476142\n",
      "Iteration 530, loss = 0.29443353\n",
      "Iteration 531, loss = 0.29410634\n",
      "Iteration 532, loss = 0.29377979\n",
      "Iteration 533, loss = 0.29345381\n",
      "Iteration 534, loss = 0.29312856\n",
      "Iteration 535, loss = 0.29280386\n",
      "Iteration 536, loss = 0.29247990\n",
      "Iteration 537, loss = 0.29215650\n",
      "Iteration 538, loss = 0.29183379\n",
      "Iteration 539, loss = 0.29151175\n",
      "Iteration 540, loss = 0.29119026\n",
      "Iteration 541, loss = 0.29086951\n",
      "Iteration 542, loss = 0.29054927\n",
      "Iteration 543, loss = 0.29022994\n",
      "Iteration 544, loss = 0.28991113\n",
      "Iteration 545, loss = 0.28959301\n",
      "Iteration 546, loss = 0.28927558\n",
      "Iteration 547, loss = 0.28895868\n",
      "Iteration 548, loss = 0.28864252\n",
      "Iteration 549, loss = 0.28832688\n",
      "Iteration 550, loss = 0.28801195\n",
      "Iteration 551, loss = 0.28769757\n",
      "Iteration 552, loss = 0.28738397\n",
      "Iteration 553, loss = 0.28707097\n",
      "Iteration 554, loss = 0.28675872\n",
      "Iteration 555, loss = 0.28644703\n",
      "Iteration 556, loss = 0.28613596\n",
      "Iteration 557, loss = 0.28582562\n",
      "Iteration 558, loss = 0.28551582\n",
      "Iteration 559, loss = 0.28520663\n",
      "Iteration 560, loss = 0.28489819\n",
      "Iteration 561, loss = 0.28459031\n",
      "Iteration 562, loss = 0.28428308\n",
      "Iteration 563, loss = 0.28397648\n",
      "Iteration 564, loss = 0.28367040\n",
      "Iteration 565, loss = 0.28336504\n",
      "Iteration 566, loss = 0.28306018\n",
      "Iteration 567, loss = 0.28275602\n",
      "Iteration 568, loss = 0.28245236\n",
      "Iteration 569, loss = 0.28214942\n",
      "Iteration 570, loss = 0.28184697\n",
      "Iteration 571, loss = 0.28154521\n",
      "Iteration 572, loss = 0.28124401\n",
      "Iteration 573, loss = 0.28094335\n",
      "Iteration 574, loss = 0.28064336\n",
      "Iteration 575, loss = 0.28034391\n",
      "Iteration 576, loss = 0.28004509\n",
      "Iteration 577, loss = 0.27974683\n",
      "Iteration 578, loss = 0.27944916\n",
      "Iteration 579, loss = 0.27915211\n",
      "Iteration 580, loss = 0.27885557\n",
      "Iteration 581, loss = 0.27855976\n",
      "Iteration 582, loss = 0.27826437\n",
      "Iteration 583, loss = 0.27796972\n",
      "Iteration 584, loss = 0.27767553\n",
      "Iteration 585, loss = 0.27738197\n",
      "Iteration 586, loss = 0.27708898\n",
      "Iteration 587, loss = 0.27679657\n",
      "Iteration 588, loss = 0.27650471\n",
      "Iteration 589, loss = 0.27621349\n",
      "Iteration 590, loss = 0.27592278\n",
      "Iteration 591, loss = 0.27563263\n",
      "Iteration 592, loss = 0.27534309\n",
      "Iteration 593, loss = 0.27505413\n",
      "Iteration 594, loss = 0.27476570\n",
      "Iteration 595, loss = 0.27447798\n",
      "Iteration 596, loss = 0.27419068\n",
      "Iteration 597, loss = 0.27390402\n",
      "Iteration 598, loss = 0.27361786\n",
      "Iteration 599, loss = 0.27333239\n",
      "Iteration 600, loss = 0.27304734\n",
      "Iteration 601, loss = 0.27276301\n",
      "Iteration 602, loss = 0.27247913\n",
      "Iteration 603, loss = 0.27219591\n",
      "Iteration 604, loss = 0.27191337\n",
      "Iteration 605, loss = 0.27163135\n",
      "Iteration 606, loss = 0.27134994\n",
      "Iteration 607, loss = 0.27106905\n",
      "Iteration 608, loss = 0.27078882\n",
      "Iteration 609, loss = 0.27050903\n",
      "Iteration 610, loss = 0.27022998\n",
      "Iteration 611, loss = 0.26995142\n",
      "Iteration 612, loss = 0.26967349\n",
      "Iteration 613, loss = 0.26939608\n",
      "Iteration 614, loss = 0.26911925\n",
      "Iteration 615, loss = 0.26884298\n",
      "Iteration 616, loss = 0.26856725\n",
      "Iteration 617, loss = 0.26829211\n",
      "Iteration 618, loss = 0.26801756\n",
      "Iteration 619, loss = 0.26774370\n",
      "Iteration 620, loss = 0.26747031\n",
      "Iteration 621, loss = 0.26719756\n",
      "Iteration 622, loss = 0.26692527\n",
      "Iteration 623, loss = 0.26665363\n",
      "Iteration 624, loss = 0.26638246\n",
      "Iteration 625, loss = 0.26611186\n",
      "Iteration 626, loss = 0.26584184\n",
      "Iteration 627, loss = 0.26557228\n",
      "Iteration 628, loss = 0.26530341\n",
      "Iteration 629, loss = 0.26503488\n",
      "Iteration 630, loss = 0.26476702\n",
      "Iteration 631, loss = 0.26449958\n",
      "Iteration 632, loss = 0.26423283\n",
      "Iteration 633, loss = 0.26396655\n",
      "Iteration 634, loss = 0.26370082\n",
      "Iteration 635, loss = 0.26343559\n",
      "Iteration 636, loss = 0.26317088\n",
      "Iteration 637, loss = 0.26290669\n",
      "Iteration 638, loss = 0.26264309\n",
      "Iteration 639, loss = 0.26237994\n",
      "Iteration 640, loss = 0.26211736\n",
      "Iteration 641, loss = 0.26185532\n",
      "Iteration 642, loss = 0.26159387\n",
      "Iteration 643, loss = 0.26133298\n",
      "Iteration 644, loss = 0.26107257\n",
      "Iteration 645, loss = 0.26081276\n",
      "Iteration 646, loss = 0.26055348\n",
      "Iteration 647, loss = 0.26029478\n",
      "Iteration 648, loss = 0.26003658\n",
      "Iteration 649, loss = 0.25977893\n",
      "Iteration 650, loss = 0.25952176\n",
      "Iteration 651, loss = 0.25926516\n",
      "Iteration 652, loss = 0.25900902\n",
      "Iteration 653, loss = 0.25875349\n",
      "Iteration 654, loss = 0.25849834\n",
      "Iteration 655, loss = 0.25824382\n",
      "Iteration 656, loss = 0.25798973\n",
      "Iteration 657, loss = 0.25773616\n",
      "Iteration 658, loss = 0.25748307\n",
      "Iteration 659, loss = 0.25723054\n",
      "Iteration 660, loss = 0.25697842\n",
      "Iteration 661, loss = 0.25672690\n",
      "Iteration 662, loss = 0.25647579\n",
      "Iteration 663, loss = 0.25622521\n",
      "Iteration 664, loss = 0.25597517\n",
      "Iteration 665, loss = 0.25572564\n",
      "Iteration 666, loss = 0.25547653\n",
      "Iteration 667, loss = 0.25522804\n",
      "Iteration 668, loss = 0.25497991\n",
      "Iteration 669, loss = 0.25473236\n",
      "Iteration 670, loss = 0.25448527\n",
      "Iteration 671, loss = 0.25423871\n",
      "Iteration 672, loss = 0.25399261\n",
      "Iteration 673, loss = 0.25374705\n",
      "Iteration 674, loss = 0.25350191\n",
      "Iteration 675, loss = 0.25325735\n",
      "Iteration 676, loss = 0.25301321\n",
      "Iteration 677, loss = 0.25276954\n",
      "Iteration 678, loss = 0.25252643\n",
      "Iteration 679, loss = 0.25228375\n",
      "Iteration 680, loss = 0.25204158\n",
      "Iteration 681, loss = 0.25179997\n",
      "Iteration 682, loss = 0.25155881\n",
      "Iteration 683, loss = 0.25131812\n",
      "Iteration 684, loss = 0.25107795\n",
      "Iteration 685, loss = 0.25083818\n",
      "Iteration 686, loss = 0.25059892\n",
      "Iteration 687, loss = 0.25036011\n",
      "Iteration 688, loss = 0.25012184\n",
      "Iteration 689, loss = 0.24988396\n",
      "Iteration 690, loss = 0.24964658\n",
      "Iteration 691, loss = 0.24940971\n",
      "Iteration 692, loss = 0.24917325\n",
      "Iteration 693, loss = 0.24893725\n",
      "Iteration 694, loss = 0.24870179\n",
      "Iteration 695, loss = 0.24846673\n",
      "Iteration 696, loss = 0.24823213\n",
      "Iteration 697, loss = 0.24799803\n",
      "Iteration 698, loss = 0.24776440\n",
      "Iteration 699, loss = 0.24753122\n",
      "Iteration 700, loss = 0.24729848\n",
      "Iteration 701, loss = 0.24706631\n",
      "Iteration 702, loss = 0.24683444\n",
      "Iteration 703, loss = 0.24660317\n",
      "Iteration 704, loss = 0.24637229\n",
      "Iteration 705, loss = 0.24614189\n",
      "Iteration 706, loss = 0.24591191\n",
      "Iteration 707, loss = 0.24568242\n",
      "Iteration 708, loss = 0.24545340\n",
      "Iteration 709, loss = 0.24522483\n",
      "Iteration 710, loss = 0.24499668\n",
      "Iteration 711, loss = 0.24476907\n",
      "Iteration 712, loss = 0.24454186\n",
      "Iteration 713, loss = 0.24431509\n",
      "Iteration 714, loss = 0.24408879\n",
      "Iteration 715, loss = 0.24386291\n",
      "Iteration 716, loss = 0.24363751\n",
      "Iteration 717, loss = 0.24341250\n",
      "Iteration 718, loss = 0.24318800\n",
      "Iteration 719, loss = 0.24296392\n",
      "Iteration 720, loss = 0.24274028\n",
      "Iteration 721, loss = 0.24251710\n",
      "Iteration 722, loss = 0.24229433\n",
      "Iteration 723, loss = 0.24207210\n",
      "Iteration 724, loss = 0.24185024\n",
      "Iteration 725, loss = 0.24162887\n",
      "Iteration 726, loss = 0.24140789\n",
      "Iteration 727, loss = 0.24118742\n",
      "Iteration 728, loss = 0.24096732\n",
      "Iteration 729, loss = 0.24074765\n",
      "Iteration 730, loss = 0.24052850\n",
      "Iteration 731, loss = 0.24030973\n",
      "Iteration 732, loss = 0.24009146\n",
      "Iteration 733, loss = 0.23987355\n",
      "Iteration 734, loss = 0.23965612\n",
      "Iteration 735, loss = 0.23943907\n",
      "Iteration 736, loss = 0.23922231\n",
      "Iteration 737, loss = 0.23900595\n",
      "Iteration 738, loss = 0.23879000\n",
      "Iteration 739, loss = 0.23857455\n",
      "Iteration 740, loss = 0.23835942\n",
      "Iteration 741, loss = 0.23814476\n",
      "Iteration 742, loss = 0.23793047\n",
      "Iteration 743, loss = 0.23771665\n",
      "Iteration 744, loss = 0.23750316\n",
      "Iteration 745, loss = 0.23729015\n",
      "Iteration 746, loss = 0.23707752\n",
      "Iteration 747, loss = 0.23686531\n",
      "Iteration 748, loss = 0.23665360\n",
      "Iteration 749, loss = 0.23644222\n",
      "Iteration 750, loss = 0.23623128\n",
      "Iteration 751, loss = 0.23602074\n",
      "Iteration 752, loss = 0.23581068\n",
      "Iteration 753, loss = 0.23560096\n",
      "Iteration 754, loss = 0.23539167\n",
      "Iteration 755, loss = 0.23518283\n",
      "Iteration 756, loss = 0.23497439\n",
      "Iteration 757, loss = 0.23476634\n",
      "Iteration 758, loss = 0.23455872\n",
      "Iteration 759, loss = 0.23435149\n",
      "Iteration 760, loss = 0.23414463\n",
      "Iteration 761, loss = 0.23393820\n",
      "Iteration 762, loss = 0.23373219\n",
      "Iteration 763, loss = 0.23352652\n",
      "Iteration 764, loss = 0.23332133\n",
      "Iteration 765, loss = 0.23311650\n",
      "Iteration 766, loss = 0.23291210\n",
      "Iteration 767, loss = 0.23270810\n",
      "Iteration 768, loss = 0.23250447\n",
      "Iteration 769, loss = 0.23230130\n",
      "Iteration 770, loss = 0.23209841\n",
      "Iteration 771, loss = 0.23189593\n",
      "Iteration 772, loss = 0.23169386\n",
      "Iteration 773, loss = 0.23149209\n",
      "Iteration 774, loss = 0.23129074\n",
      "Iteration 775, loss = 0.23108976\n",
      "Iteration 776, loss = 0.23088918\n",
      "Iteration 777, loss = 0.23068902\n",
      "Iteration 778, loss = 0.23048923\n",
      "Iteration 779, loss = 0.23028983\n",
      "Iteration 780, loss = 0.23009079\n",
      "Iteration 781, loss = 0.22989215\n",
      "Iteration 782, loss = 0.22969390\n",
      "Iteration 783, loss = 0.22949604\n",
      "Iteration 784, loss = 0.22929859\n",
      "Iteration 785, loss = 0.22910150\n",
      "Iteration 786, loss = 0.22890481\n",
      "Iteration 787, loss = 0.22870849\n",
      "Iteration 788, loss = 0.22851256\n",
      "Iteration 789, loss = 0.22831701\n",
      "Iteration 790, loss = 0.22812181\n",
      "Iteration 791, loss = 0.22792701\n",
      "Iteration 792, loss = 0.22773258\n",
      "Iteration 793, loss = 0.22753853\n",
      "Iteration 794, loss = 0.22734484\n",
      "Iteration 795, loss = 0.22715153\n",
      "Iteration 796, loss = 0.22695855\n",
      "Iteration 797, loss = 0.22676599\n",
      "Iteration 798, loss = 0.22657377\n",
      "Iteration 799, loss = 0.22638193\n",
      "Iteration 800, loss = 0.22619041\n",
      "Iteration 801, loss = 0.22599933\n",
      "Iteration 802, loss = 0.22580857\n",
      "Iteration 803, loss = 0.22561820\n",
      "Iteration 804, loss = 0.22542824\n",
      "Iteration 805, loss = 0.22523862\n",
      "Iteration 806, loss = 0.22504932\n",
      "Iteration 807, loss = 0.22486045\n",
      "Iteration 808, loss = 0.22467187\n",
      "Iteration 809, loss = 0.22448372\n",
      "Iteration 810, loss = 0.22429588\n",
      "Iteration 811, loss = 0.22410834\n",
      "Iteration 812, loss = 0.22392121\n",
      "Iteration 813, loss = 0.22373434\n",
      "Iteration 814, loss = 0.22354779\n",
      "Iteration 815, loss = 0.22336161\n",
      "Iteration 816, loss = 0.22317577\n",
      "Iteration 817, loss = 0.22299041\n",
      "Iteration 818, loss = 0.22280523\n",
      "Iteration 819, loss = 0.22262059\n",
      "Iteration 820, loss = 0.22243619\n",
      "Iteration 821, loss = 0.22225216\n",
      "Iteration 822, loss = 0.22206850\n",
      "Iteration 823, loss = 0.22188513\n",
      "Iteration 824, loss = 0.22170208\n",
      "Iteration 825, loss = 0.22151944\n",
      "Iteration 826, loss = 0.22133708\n",
      "Iteration 827, loss = 0.22115515\n",
      "Iteration 828, loss = 0.22097353\n",
      "Iteration 829, loss = 0.22079225\n",
      "Iteration 830, loss = 0.22061135\n",
      "Iteration 831, loss = 0.22043081\n",
      "Iteration 832, loss = 0.22025062\n",
      "Iteration 833, loss = 0.22007080\n",
      "Iteration 834, loss = 0.21989131\n",
      "Iteration 835, loss = 0.21971218\n",
      "Iteration 836, loss = 0.21953334\n",
      "Iteration 837, loss = 0.21935484\n",
      "Iteration 838, loss = 0.21917675\n",
      "Iteration 839, loss = 0.21899897\n",
      "Iteration 840, loss = 0.21882152\n",
      "Iteration 841, loss = 0.21864447\n",
      "Iteration 842, loss = 0.21846770\n",
      "Iteration 843, loss = 0.21829115\n",
      "Iteration 844, loss = 0.21811494\n",
      "Iteration 845, loss = 0.21793908\n",
      "Iteration 846, loss = 0.21776355\n",
      "Iteration 847, loss = 0.21758836\n",
      "Iteration 848, loss = 0.21741350\n",
      "Iteration 849, loss = 0.21723905\n",
      "Iteration 850, loss = 0.21706476\n",
      "Iteration 851, loss = 0.21689074\n",
      "Iteration 852, loss = 0.21671694\n",
      "Iteration 853, loss = 0.21654338\n",
      "Iteration 854, loss = 0.21637013\n",
      "Iteration 855, loss = 0.21619725\n",
      "Iteration 856, loss = 0.21602466\n",
      "Iteration 857, loss = 0.21585237\n",
      "Iteration 858, loss = 0.21568041\n",
      "Iteration 859, loss = 0.21550879\n",
      "Iteration 860, loss = 0.21533751\n",
      "Iteration 861, loss = 0.21516652\n",
      "Iteration 862, loss = 0.21499581\n",
      "Iteration 863, loss = 0.21482542\n",
      "Iteration 864, loss = 0.21465530\n",
      "Iteration 865, loss = 0.21448551\n",
      "Iteration 866, loss = 0.21431606\n",
      "Iteration 867, loss = 0.21414691\n",
      "Iteration 868, loss = 0.21397814\n",
      "Iteration 869, loss = 0.21380962\n",
      "Iteration 870, loss = 0.21364150\n",
      "Iteration 871, loss = 0.21347366\n",
      "Iteration 872, loss = 0.21330615\n",
      "Iteration 873, loss = 0.21313902\n",
      "Iteration 874, loss = 0.21297209\n",
      "Iteration 875, loss = 0.21280543\n",
      "Iteration 876, loss = 0.21263919\n",
      "Iteration 877, loss = 0.21247314\n",
      "Iteration 878, loss = 0.21230736\n",
      "Iteration 879, loss = 0.21214183\n",
      "Iteration 880, loss = 0.21197654\n",
      "Iteration 881, loss = 0.21181155\n",
      "Iteration 882, loss = 0.21164689\n",
      "Iteration 883, loss = 0.21148251\n",
      "Iteration 884, loss = 0.21131844\n",
      "Iteration 885, loss = 0.21115469\n",
      "Iteration 886, loss = 0.21099123\n",
      "Iteration 887, loss = 0.21082810\n",
      "Iteration 888, loss = 0.21066520\n",
      "Iteration 889, loss = 0.21050242\n",
      "Iteration 890, loss = 0.21033993\n",
      "Iteration 891, loss = 0.21017774\n",
      "Iteration 892, loss = 0.21001585\n",
      "Iteration 893, loss = 0.20985424\n",
      "Iteration 894, loss = 0.20969282\n",
      "Iteration 895, loss = 0.20953162\n",
      "Iteration 896, loss = 0.20937066\n",
      "Iteration 897, loss = 0.20920991\n",
      "Iteration 898, loss = 0.20904950\n",
      "Iteration 899, loss = 0.20888937\n",
      "Iteration 900, loss = 0.20872953\n",
      "Iteration 901, loss = 0.20857000\n",
      "Iteration 902, loss = 0.20841079\n",
      "Iteration 903, loss = 0.20825186\n",
      "Iteration 904, loss = 0.20809323\n",
      "Iteration 905, loss = 0.20793492\n",
      "Iteration 906, loss = 0.20777688\n",
      "Iteration 907, loss = 0.20761920\n",
      "Iteration 908, loss = 0.20746177\n",
      "Iteration 909, loss = 0.20730467\n",
      "Iteration 910, loss = 0.20714785\n",
      "Iteration 911, loss = 0.20699116\n",
      "Iteration 912, loss = 0.20683477\n",
      "Iteration 913, loss = 0.20667865\n",
      "Iteration 914, loss = 0.20652277\n",
      "Iteration 915, loss = 0.20636721\n",
      "Iteration 916, loss = 0.20621187\n",
      "Iteration 917, loss = 0.20605669\n",
      "Iteration 918, loss = 0.20590179\n",
      "Iteration 919, loss = 0.20574722\n",
      "Iteration 920, loss = 0.20559285\n",
      "Iteration 921, loss = 0.20543879\n",
      "Iteration 922, loss = 0.20528505\n",
      "Iteration 923, loss = 0.20513158\n",
      "Iteration 924, loss = 0.20497843\n",
      "Iteration 925, loss = 0.20482556\n",
      "Iteration 926, loss = 0.20467297\n",
      "Iteration 927, loss = 0.20452071\n",
      "Iteration 928, loss = 0.20436871\n",
      "Iteration 929, loss = 0.20421702\n",
      "Iteration 930, loss = 0.20406565\n",
      "Iteration 931, loss = 0.20391456\n",
      "Iteration 932, loss = 0.20376375\n",
      "Iteration 933, loss = 0.20361328\n",
      "Iteration 934, loss = 0.20346303\n",
      "Iteration 935, loss = 0.20331311\n",
      "Iteration 936, loss = 0.20316346\n",
      "Iteration 937, loss = 0.20301412\n",
      "Iteration 938, loss = 0.20286502\n",
      "Iteration 939, loss = 0.20271617\n",
      "Iteration 940, loss = 0.20256760\n",
      "Iteration 941, loss = 0.20241924\n",
      "Iteration 942, loss = 0.20227091\n",
      "Iteration 943, loss = 0.20212282\n",
      "Iteration 944, loss = 0.20197502\n",
      "Iteration 945, loss = 0.20182750\n",
      "Iteration 946, loss = 0.20168023\n",
      "Iteration 947, loss = 0.20153326\n",
      "Iteration 948, loss = 0.20138650\n",
      "Iteration 949, loss = 0.20123990\n",
      "Iteration 950, loss = 0.20109361\n",
      "Iteration 951, loss = 0.20094754\n",
      "Iteration 952, loss = 0.20080176\n",
      "Iteration 953, loss = 0.20065628\n",
      "Iteration 954, loss = 0.20051106\n",
      "Iteration 955, loss = 0.20036612\n",
      "Iteration 956, loss = 0.20022150\n",
      "Iteration 957, loss = 0.20007709\n",
      "Iteration 958, loss = 0.19993302\n",
      "Iteration 959, loss = 0.19978924\n",
      "Iteration 960, loss = 0.19964571\n",
      "Iteration 961, loss = 0.19950245\n",
      "Iteration 962, loss = 0.19935946\n",
      "Iteration 963, loss = 0.19921665\n",
      "Iteration 964, loss = 0.19907410\n",
      "Iteration 965, loss = 0.19893183\n",
      "Iteration 966, loss = 0.19878981\n",
      "Iteration 967, loss = 0.19864808\n",
      "Iteration 968, loss = 0.19850665\n",
      "Iteration 969, loss = 0.19836546\n",
      "Iteration 970, loss = 0.19822455\n",
      "Iteration 971, loss = 0.19808392\n",
      "Iteration 972, loss = 0.19794355\n",
      "Iteration 973, loss = 0.19780349\n",
      "Iteration 974, loss = 0.19766366\n",
      "Iteration 975, loss = 0.19752411\n",
      "Iteration 976, loss = 0.19738487\n",
      "Iteration 977, loss = 0.19724586\n",
      "Iteration 978, loss = 0.19710708\n",
      "Iteration 979, loss = 0.19696859\n",
      "Iteration 980, loss = 0.19683035\n",
      "Iteration 981, loss = 0.19669237\n",
      "Iteration 982, loss = 0.19655470\n",
      "Iteration 983, loss = 0.19641724\n",
      "Iteration 984, loss = 0.19628010\n",
      "Iteration 985, loss = 0.19614321\n",
      "Iteration 986, loss = 0.19600655\n",
      "Iteration 987, loss = 0.19587012\n",
      "Iteration 988, loss = 0.19573399\n",
      "Iteration 989, loss = 0.19559808\n",
      "Iteration 990, loss = 0.19546243\n",
      "Iteration 991, loss = 0.19532698\n",
      "Iteration 992, loss = 0.19519175\n",
      "Iteration 993, loss = 0.19505679\n",
      "Iteration 994, loss = 0.19492212\n",
      "Iteration 995, loss = 0.19478763\n",
      "Iteration 996, loss = 0.19465340\n",
      "Iteration 997, loss = 0.19451941\n",
      "Iteration 998, loss = 0.19438564\n",
      "Iteration 999, loss = 0.19425205\n",
      "Iteration 1000, loss = 0.19411876\n",
      "Iteration 1, loss = 1.65392470\n",
      "Iteration 2, loss = 1.61776763\n",
      "Iteration 3, loss = 1.56920198\n",
      "Iteration 4, loss = 1.51241495\n",
      "Iteration 5, loss = 1.45169214\n",
      "Iteration 6, loss = 1.39154924\n",
      "Iteration 7, loss = 1.33582163\n",
      "Iteration 8, loss = 1.28766035\n",
      "Iteration 9, loss = 1.24911375\n",
      "Iteration 10, loss = 1.22041673\n",
      "Iteration 11, loss = 1.20026356\n",
      "Iteration 12, loss = 1.18626736\n",
      "Iteration 13, loss = 1.17550480\n",
      "Iteration 14, loss = 1.16538653\n",
      "Iteration 15, loss = 1.15392846\n",
      "Iteration 16, loss = 1.14038920\n",
      "Iteration 17, loss = 1.12423663\n",
      "Iteration 18, loss = 1.10573181\n",
      "Iteration 19, loss = 1.08546969\n",
      "Iteration 20, loss = 1.06443475\n",
      "Iteration 21, loss = 1.04370726\n",
      "Iteration 22, loss = 1.02407341\n",
      "Iteration 23, loss = 1.00617819\n",
      "Iteration 24, loss = 0.99015988\n",
      "Iteration 25, loss = 0.97587491\n",
      "Iteration 26, loss = 0.96312515\n",
      "Iteration 27, loss = 0.95156739\n",
      "Iteration 28, loss = 0.94087417\n",
      "Iteration 29, loss = 0.93073015\n",
      "Iteration 30, loss = 0.92090839\n",
      "Iteration 31, loss = 0.91130916\n",
      "Iteration 32, loss = 0.90187801\n",
      "Iteration 33, loss = 0.89267182\n",
      "Iteration 34, loss = 0.88366284\n",
      "Iteration 35, loss = 0.87483807\n",
      "Iteration 36, loss = 0.86624399\n",
      "Iteration 37, loss = 0.85792753\n",
      "Iteration 38, loss = 0.84994140\n",
      "Iteration 39, loss = 0.84228789\n",
      "Iteration 40, loss = 0.83502119\n",
      "Iteration 41, loss = 0.82812628\n",
      "Iteration 42, loss = 0.82157732\n",
      "Iteration 43, loss = 0.81536965\n",
      "Iteration 44, loss = 0.80933158\n",
      "Iteration 45, loss = 0.80340290\n",
      "Iteration 46, loss = 0.79755129\n",
      "Iteration 47, loss = 0.79176411\n",
      "Iteration 48, loss = 0.78604239\n",
      "Iteration 49, loss = 0.78037985\n",
      "Iteration 50, loss = 0.77479035\n",
      "Iteration 51, loss = 0.76929853\n",
      "Iteration 52, loss = 0.76395257\n",
      "Iteration 53, loss = 0.75875816\n",
      "Iteration 54, loss = 0.75370210\n",
      "Iteration 55, loss = 0.74876430\n",
      "Iteration 56, loss = 0.74393839\n",
      "Iteration 57, loss = 0.73922103\n",
      "Iteration 58, loss = 0.73460559\n",
      "Iteration 59, loss = 0.73008728\n",
      "Iteration 60, loss = 0.72566065\n",
      "Iteration 61, loss = 0.72132149\n",
      "Iteration 62, loss = 0.71706618\n",
      "Iteration 63, loss = 0.71289151\n",
      "Iteration 64, loss = 0.70879378\n",
      "Iteration 65, loss = 0.70477324\n",
      "Iteration 66, loss = 0.70082957\n",
      "Iteration 67, loss = 0.69696144\n",
      "Iteration 68, loss = 0.69316763\n",
      "Iteration 69, loss = 0.68944641\n",
      "Iteration 70, loss = 0.68579705\n",
      "Iteration 71, loss = 0.68221826\n",
      "Iteration 72, loss = 0.67870689\n",
      "Iteration 73, loss = 0.67526233\n",
      "Iteration 74, loss = 0.67188245\n",
      "Iteration 75, loss = 0.66856492\n",
      "Iteration 76, loss = 0.66530784\n",
      "Iteration 77, loss = 0.66211055\n",
      "Iteration 78, loss = 0.65897302\n",
      "Iteration 79, loss = 0.65588724\n",
      "Iteration 80, loss = 0.65285259\n",
      "Iteration 81, loss = 0.64987070\n",
      "Iteration 82, loss = 0.64693953\n",
      "Iteration 83, loss = 0.64405668\n",
      "Iteration 84, loss = 0.64122071\n",
      "Iteration 85, loss = 0.63843179\n",
      "Iteration 86, loss = 0.63568867\n",
      "Iteration 87, loss = 0.63299031\n",
      "Iteration 88, loss = 0.63033526\n",
      "Iteration 89, loss = 0.62772291\n",
      "Iteration 90, loss = 0.62515221\n",
      "Iteration 91, loss = 0.62262157\n",
      "Iteration 92, loss = 0.62013026\n",
      "Iteration 93, loss = 0.61767738\n",
      "Iteration 94, loss = 0.61526148\n",
      "Iteration 95, loss = 0.61288170\n",
      "Iteration 96, loss = 0.61053712\n",
      "Iteration 97, loss = 0.60822759\n",
      "Iteration 98, loss = 0.60595183\n",
      "Iteration 99, loss = 0.60370855\n",
      "Iteration 100, loss = 0.60149680\n",
      "Iteration 101, loss = 0.59931421\n",
      "Iteration 102, loss = 0.59716099\n",
      "Iteration 103, loss = 0.59503690\n",
      "Iteration 104, loss = 0.59294151\n",
      "Iteration 105, loss = 0.59087304\n",
      "Iteration 106, loss = 0.58883119\n",
      "Iteration 107, loss = 0.58681682\n",
      "Iteration 108, loss = 0.58482912\n",
      "Iteration 109, loss = 0.58286755\n",
      "Iteration 110, loss = 0.58093134\n",
      "Iteration 111, loss = 0.57901835\n",
      "Iteration 112, loss = 0.57712789\n",
      "Iteration 113, loss = 0.57525794\n",
      "Iteration 114, loss = 0.57341049\n",
      "Iteration 115, loss = 0.57158611\n",
      "Iteration 116, loss = 0.56978482\n",
      "Iteration 117, loss = 0.56800495\n",
      "Iteration 118, loss = 0.56624505\n",
      "Iteration 119, loss = 0.56450360\n",
      "Iteration 120, loss = 0.56278272\n",
      "Iteration 121, loss = 0.56108110\n",
      "Iteration 122, loss = 0.55939883\n",
      "Iteration 123, loss = 0.55773456\n",
      "Iteration 124, loss = 0.55608941\n",
      "Iteration 125, loss = 0.55446379\n",
      "Iteration 126, loss = 0.55285661\n",
      "Iteration 127, loss = 0.55126755\n",
      "Iteration 128, loss = 0.54969541\n",
      "Iteration 129, loss = 0.54813981\n",
      "Iteration 130, loss = 0.54660016\n",
      "Iteration 131, loss = 0.54507658\n",
      "Iteration 132, loss = 0.54356934\n",
      "Iteration 133, loss = 0.54207814\n",
      "Iteration 134, loss = 0.54060271\n",
      "Iteration 135, loss = 0.53914278\n",
      "Iteration 136, loss = 0.53769842\n",
      "Iteration 137, loss = 0.53626969\n",
      "Iteration 138, loss = 0.53485570\n",
      "Iteration 139, loss = 0.53345617\n",
      "Iteration 140, loss = 0.53207088\n",
      "Iteration 141, loss = 0.53069954\n",
      "Iteration 142, loss = 0.52934192\n",
      "Iteration 143, loss = 0.52799775\n",
      "Iteration 144, loss = 0.52666682\n",
      "Iteration 145, loss = 0.52534888\n",
      "Iteration 146, loss = 0.52404369\n",
      "Iteration 147, loss = 0.52275101\n",
      "Iteration 148, loss = 0.52147051\n",
      "Iteration 149, loss = 0.52020204\n",
      "Iteration 150, loss = 0.51894548\n",
      "Iteration 151, loss = 0.51769933\n",
      "Iteration 152, loss = 0.51646421\n",
      "Iteration 153, loss = 0.51524022\n",
      "Iteration 154, loss = 0.51402737\n",
      "Iteration 155, loss = 0.51282552\n",
      "Iteration 156, loss = 0.51163429\n",
      "Iteration 157, loss = 0.51045337\n",
      "Iteration 158, loss = 0.50928259\n",
      "Iteration 159, loss = 0.50812177\n",
      "Iteration 160, loss = 0.50697076\n",
      "Iteration 161, loss = 0.50582935\n",
      "Iteration 162, loss = 0.50469741\n",
      "Iteration 163, loss = 0.50357473\n",
      "Iteration 164, loss = 0.50246116\n",
      "Iteration 165, loss = 0.50135654\n",
      "Iteration 166, loss = 0.50026072\n",
      "Iteration 167, loss = 0.49917340\n",
      "Iteration 168, loss = 0.49809446\n",
      "Iteration 169, loss = 0.49702385\n",
      "Iteration 170, loss = 0.49596141\n",
      "Iteration 171, loss = 0.49490697\n",
      "Iteration 172, loss = 0.49386041\n",
      "Iteration 173, loss = 0.49282158\n",
      "Iteration 174, loss = 0.49179025\n",
      "Iteration 175, loss = 0.49076638\n",
      "Iteration 176, loss = 0.48974989\n",
      "Iteration 177, loss = 0.48874070\n",
      "Iteration 178, loss = 0.48773900\n",
      "Iteration 179, loss = 0.48674473\n",
      "Iteration 180, loss = 0.48575721\n",
      "Iteration 181, loss = 0.48477638\n",
      "Iteration 182, loss = 0.48380224\n",
      "Iteration 183, loss = 0.48283463\n",
      "Iteration 184, loss = 0.48187340\n",
      "Iteration 185, loss = 0.48091844\n",
      "Iteration 186, loss = 0.47996955\n",
      "Iteration 187, loss = 0.47902676\n",
      "Iteration 188, loss = 0.47808998\n",
      "Iteration 189, loss = 0.47715919\n",
      "Iteration 190, loss = 0.47623424\n",
      "Iteration 191, loss = 0.47531506\n",
      "Iteration 192, loss = 0.47440163\n",
      "Iteration 193, loss = 0.47349423\n",
      "Iteration 194, loss = 0.47259229\n",
      "Iteration 195, loss = 0.47169571\n",
      "Iteration 196, loss = 0.47080440\n",
      "Iteration 197, loss = 0.46991836\n",
      "Iteration 198, loss = 0.46903761\n",
      "Iteration 199, loss = 0.46816217\n",
      "Iteration 200, loss = 0.46729146\n",
      "Iteration 201, loss = 0.46642556\n",
      "Iteration 202, loss = 0.46556446\n",
      "Iteration 203, loss = 0.46470780\n",
      "Iteration 204, loss = 0.46385578\n",
      "Iteration 205, loss = 0.46300870\n",
      "Iteration 206, loss = 0.46216595\n",
      "Iteration 207, loss = 0.46132797\n",
      "Iteration 208, loss = 0.46049442\n",
      "Iteration 209, loss = 0.45966503\n",
      "Iteration 210, loss = 0.45883992\n",
      "Iteration 211, loss = 0.45801894\n",
      "Iteration 212, loss = 0.45720212\n",
      "Iteration 213, loss = 0.45638952\n",
      "Iteration 214, loss = 0.45558103\n",
      "Iteration 215, loss = 0.45477701\n",
      "Iteration 216, loss = 0.45397632\n",
      "Iteration 217, loss = 0.45317934\n",
      "Iteration 218, loss = 0.45238627\n",
      "Iteration 219, loss = 0.45159653\n",
      "Iteration 220, loss = 0.45081035\n",
      "Iteration 221, loss = 0.45002782\n",
      "Iteration 222, loss = 0.44924807\n",
      "Iteration 223, loss = 0.44847162\n",
      "Iteration 224, loss = 0.44769781\n",
      "Iteration 225, loss = 0.44692728\n",
      "Iteration 226, loss = 0.44616001\n",
      "Iteration 227, loss = 0.44539556\n",
      "Iteration 228, loss = 0.44463485\n",
      "Iteration 229, loss = 0.44387758\n",
      "Iteration 230, loss = 0.44312295\n",
      "Iteration 231, loss = 0.44237151\n",
      "Iteration 232, loss = 0.44162320\n",
      "Iteration 233, loss = 0.44087780\n",
      "Iteration 234, loss = 0.44013513\n",
      "Iteration 235, loss = 0.43939521\n",
      "Iteration 236, loss = 0.43865780\n",
      "Iteration 237, loss = 0.43792311\n",
      "Iteration 238, loss = 0.43719122\n",
      "Iteration 239, loss = 0.43646205\n",
      "Iteration 240, loss = 0.43573558\n",
      "Iteration 241, loss = 0.43501230\n",
      "Iteration 242, loss = 0.43429216\n",
      "Iteration 243, loss = 0.43357441\n",
      "Iteration 244, loss = 0.43285864\n",
      "Iteration 245, loss = 0.43214542\n",
      "Iteration 246, loss = 0.43143539\n",
      "Iteration 247, loss = 0.43072802\n",
      "Iteration 248, loss = 0.43002380\n",
      "Iteration 249, loss = 0.42932205\n",
      "Iteration 250, loss = 0.42862296\n",
      "Iteration 251, loss = 0.42792593\n",
      "Iteration 252, loss = 0.42723224\n",
      "Iteration 253, loss = 0.42654219\n",
      "Iteration 254, loss = 0.42585554\n",
      "Iteration 255, loss = 0.42517121\n",
      "Iteration 256, loss = 0.42448961\n",
      "Iteration 257, loss = 0.42381059\n",
      "Iteration 258, loss = 0.42313423\n",
      "Iteration 259, loss = 0.42246049\n",
      "Iteration 260, loss = 0.42178909\n",
      "Iteration 261, loss = 0.42111994\n",
      "Iteration 262, loss = 0.42045341\n",
      "Iteration 263, loss = 0.41979027\n",
      "Iteration 264, loss = 0.41912995\n",
      "Iteration 265, loss = 0.41847231\n",
      "Iteration 266, loss = 0.41781698\n",
      "Iteration 267, loss = 0.41716430\n",
      "Iteration 268, loss = 0.41651490\n",
      "Iteration 269, loss = 0.41586810\n",
      "Iteration 270, loss = 0.41522347\n",
      "Iteration 271, loss = 0.41458131\n",
      "Iteration 272, loss = 0.41394140\n",
      "Iteration 273, loss = 0.41330365\n",
      "Iteration 274, loss = 0.41266797\n",
      "Iteration 275, loss = 0.41203423\n",
      "Iteration 276, loss = 0.41140246\n",
      "Iteration 277, loss = 0.41077261\n",
      "Iteration 278, loss = 0.41014464\n",
      "Iteration 279, loss = 0.40951863\n",
      "Iteration 280, loss = 0.40889455\n",
      "Iteration 281, loss = 0.40827252\n",
      "Iteration 282, loss = 0.40765272\n",
      "Iteration 283, loss = 0.40703482\n",
      "Iteration 284, loss = 0.40641878\n",
      "Iteration 285, loss = 0.40580480\n",
      "Iteration 286, loss = 0.40519285\n",
      "Iteration 287, loss = 0.40458277\n",
      "Iteration 288, loss = 0.40397447\n",
      "Iteration 289, loss = 0.40336796\n",
      "Iteration 290, loss = 0.40276336\n",
      "Iteration 291, loss = 0.40216008\n",
      "Iteration 292, loss = 0.40155860\n",
      "Iteration 293, loss = 0.40095903\n",
      "Iteration 294, loss = 0.40036111\n",
      "Iteration 295, loss = 0.39976491\n",
      "Iteration 296, loss = 0.39917038\n",
      "Iteration 297, loss = 0.39857757\n",
      "Iteration 298, loss = 0.39798657\n",
      "Iteration 299, loss = 0.39739742\n",
      "Iteration 300, loss = 0.39681027\n",
      "Iteration 301, loss = 0.39622494\n",
      "Iteration 302, loss = 0.39564136\n",
      "Iteration 303, loss = 0.39505931\n",
      "Iteration 304, loss = 0.39447886\n",
      "Iteration 305, loss = 0.39390001\n",
      "Iteration 306, loss = 0.39332284\n",
      "Iteration 307, loss = 0.39274731\n",
      "Iteration 308, loss = 0.39217355\n",
      "Iteration 309, loss = 0.39160129\n",
      "Iteration 310, loss = 0.39103060\n",
      "Iteration 311, loss = 0.39046160\n",
      "Iteration 312, loss = 0.38989430\n",
      "Iteration 313, loss = 0.38932866\n",
      "Iteration 314, loss = 0.38876447\n",
      "Iteration 315, loss = 0.38820131\n",
      "Iteration 316, loss = 0.38763901\n",
      "Iteration 317, loss = 0.38707821\n",
      "Iteration 318, loss = 0.38651867\n",
      "Iteration 319, loss = 0.38596026\n",
      "Iteration 320, loss = 0.38540331\n",
      "Iteration 321, loss = 0.38484764\n",
      "Iteration 322, loss = 0.38429171\n",
      "Iteration 323, loss = 0.38373492\n",
      "Iteration 324, loss = 0.38317886\n",
      "Iteration 325, loss = 0.38262338\n",
      "Iteration 326, loss = 0.38206760\n",
      "Iteration 327, loss = 0.38151203\n",
      "Iteration 328, loss = 0.38095735\n",
      "Iteration 329, loss = 0.38040362\n",
      "Iteration 330, loss = 0.37985085\n",
      "Iteration 331, loss = 0.37929852\n",
      "Iteration 332, loss = 0.37874651\n",
      "Iteration 333, loss = 0.37819557\n",
      "Iteration 334, loss = 0.37764575\n",
      "Iteration 335, loss = 0.37709713\n",
      "Iteration 336, loss = 0.37654970\n",
      "Iteration 337, loss = 0.37600336\n",
      "Iteration 338, loss = 0.37545727\n",
      "Iteration 339, loss = 0.37491001\n",
      "Iteration 340, loss = 0.37436191\n",
      "Iteration 341, loss = 0.37381319\n",
      "Iteration 342, loss = 0.37326114\n",
      "Iteration 343, loss = 0.37270355\n",
      "Iteration 344, loss = 0.37213983\n",
      "Iteration 345, loss = 0.37157490\n",
      "Iteration 346, loss = 0.37100860\n",
      "Iteration 347, loss = 0.37043813\n",
      "Iteration 348, loss = 0.36986541\n",
      "Iteration 349, loss = 0.36928745\n",
      "Iteration 350, loss = 0.36869485\n",
      "Iteration 351, loss = 0.36808780\n",
      "Iteration 352, loss = 0.36746990\n",
      "Iteration 353, loss = 0.36683751\n",
      "Iteration 354, loss = 0.36619033\n",
      "Iteration 355, loss = 0.36552992\n",
      "Iteration 356, loss = 0.36484915\n",
      "Iteration 357, loss = 0.36415031\n",
      "Iteration 358, loss = 0.36344936\n",
      "Iteration 359, loss = 0.36274803\n",
      "Iteration 360, loss = 0.36204749\n",
      "Iteration 361, loss = 0.36134173\n",
      "Iteration 362, loss = 0.36062062\n",
      "Iteration 363, loss = 0.35991065\n",
      "Iteration 364, loss = 0.35922915\n",
      "Iteration 365, loss = 0.35857069\n",
      "Iteration 366, loss = 0.35793694\n",
      "Iteration 367, loss = 0.35731116\n",
      "Iteration 368, loss = 0.35669608\n",
      "Iteration 369, loss = 0.35611275\n",
      "Iteration 370, loss = 0.35554814\n",
      "Iteration 371, loss = 0.35499428\n",
      "Iteration 372, loss = 0.35444956\n",
      "Iteration 373, loss = 0.35390864\n",
      "Iteration 374, loss = 0.35337651\n",
      "Iteration 375, loss = 0.35284819\n",
      "Iteration 376, loss = 0.35232880\n",
      "Iteration 377, loss = 0.35181714\n",
      "Iteration 378, loss = 0.35130924\n",
      "Iteration 379, loss = 0.35080487\n",
      "Iteration 380, loss = 0.35030211\n",
      "Iteration 381, loss = 0.34980093\n",
      "Iteration 382, loss = 0.34930207\n",
      "Iteration 383, loss = 0.34880735\n",
      "Iteration 384, loss = 0.34831662\n",
      "Iteration 385, loss = 0.34782755\n",
      "Iteration 386, loss = 0.34734213\n",
      "Iteration 387, loss = 0.34685746\n",
      "Iteration 388, loss = 0.34637338\n",
      "Iteration 389, loss = 0.34589007\n",
      "Iteration 390, loss = 0.34540779\n",
      "Iteration 391, loss = 0.34492731\n",
      "Iteration 392, loss = 0.34444583\n",
      "Iteration 393, loss = 0.34396349\n",
      "Iteration 394, loss = 0.34348217\n",
      "Iteration 395, loss = 0.34300113\n",
      "Iteration 396, loss = 0.34252142\n",
      "Iteration 397, loss = 0.34204181\n",
      "Iteration 398, loss = 0.34156587\n",
      "Iteration 399, loss = 0.34109035\n",
      "Iteration 400, loss = 0.34061376\n",
      "Iteration 401, loss = 0.34013877\n",
      "Iteration 402, loss = 0.33966617\n",
      "Iteration 403, loss = 0.33919438\n",
      "Iteration 404, loss = 0.33872415\n",
      "Iteration 405, loss = 0.33825697\n",
      "Iteration 406, loss = 0.33779142\n",
      "Iteration 407, loss = 0.33733043\n",
      "Iteration 408, loss = 0.33687178\n",
      "Iteration 409, loss = 0.33641426\n",
      "Iteration 410, loss = 0.33595893\n",
      "Iteration 411, loss = 0.33550546\n",
      "Iteration 412, loss = 0.33505441\n",
      "Iteration 413, loss = 0.33460645\n",
      "Iteration 414, loss = 0.33416308\n",
      "Iteration 415, loss = 0.33372209\n",
      "Iteration 416, loss = 0.33328348\n",
      "Iteration 417, loss = 0.33284671\n",
      "Iteration 418, loss = 0.33241176\n",
      "Iteration 419, loss = 0.33197796\n",
      "Iteration 420, loss = 0.33154530\n",
      "Iteration 421, loss = 0.33111354\n",
      "Iteration 422, loss = 0.33068296\n",
      "Iteration 423, loss = 0.33025360\n",
      "Iteration 424, loss = 0.32982535\n",
      "Iteration 425, loss = 0.32939845\n",
      "Iteration 426, loss = 0.32897263\n",
      "Iteration 427, loss = 0.32854761\n",
      "Iteration 428, loss = 0.32812361\n",
      "Iteration 429, loss = 0.32770042\n",
      "Iteration 430, loss = 0.32727800\n",
      "Iteration 431, loss = 0.32685645\n",
      "Iteration 432, loss = 0.32643570\n",
      "Iteration 433, loss = 0.32601600\n",
      "Iteration 434, loss = 0.32559698\n",
      "Iteration 435, loss = 0.32517883\n",
      "Iteration 436, loss = 0.32476148\n",
      "Iteration 437, loss = 0.32434506\n",
      "Iteration 438, loss = 0.32392930\n",
      "Iteration 439, loss = 0.32351454\n",
      "Iteration 440, loss = 0.32310045\n",
      "Iteration 441, loss = 0.32268731\n",
      "Iteration 442, loss = 0.32227486\n",
      "Iteration 443, loss = 0.32186342\n",
      "Iteration 444, loss = 0.32145262\n",
      "Iteration 445, loss = 0.32104274\n",
      "Iteration 446, loss = 0.32063359\n",
      "Iteration 447, loss = 0.32022537\n",
      "Iteration 448, loss = 0.31981784\n",
      "Iteration 449, loss = 0.31941138\n",
      "Iteration 450, loss = 0.31900570\n",
      "Iteration 451, loss = 0.31860103\n",
      "Iteration 452, loss = 0.31819705\n",
      "Iteration 453, loss = 0.31779401\n",
      "Iteration 454, loss = 0.31739167\n",
      "Iteration 455, loss = 0.31699026\n",
      "Iteration 456, loss = 0.31658959\n",
      "Iteration 457, loss = 0.31619004\n",
      "Iteration 458, loss = 0.31579126\n",
      "Iteration 459, loss = 0.31539330\n",
      "Iteration 460, loss = 0.31499615\n",
      "Iteration 461, loss = 0.31459990\n",
      "Iteration 462, loss = 0.31420442\n",
      "Iteration 463, loss = 0.31380973\n",
      "Iteration 464, loss = 0.31341587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 465, loss = 0.31302284\n",
      "Iteration 466, loss = 0.31263048\n",
      "Iteration 467, loss = 0.31223910\n",
      "Iteration 468, loss = 0.31184828\n",
      "Iteration 469, loss = 0.31145843\n",
      "Iteration 470, loss = 0.31106920\n",
      "Iteration 471, loss = 0.31068079\n",
      "Iteration 472, loss = 0.31029321\n",
      "Iteration 473, loss = 0.30990636\n",
      "Iteration 474, loss = 0.30952034\n",
      "Iteration 475, loss = 0.30913510\n",
      "Iteration 476, loss = 0.30875066\n",
      "Iteration 477, loss = 0.30836694\n",
      "Iteration 478, loss = 0.30798409\n",
      "Iteration 479, loss = 0.30760191\n",
      "Iteration 480, loss = 0.30722046\n",
      "Iteration 481, loss = 0.30683994\n",
      "Iteration 482, loss = 0.30646025\n",
      "Iteration 483, loss = 0.30608127\n",
      "Iteration 484, loss = 0.30570312\n",
      "Iteration 485, loss = 0.30532581\n",
      "Iteration 486, loss = 0.30494920\n",
      "Iteration 487, loss = 0.30457343\n",
      "Iteration 488, loss = 0.30419836\n",
      "Iteration 489, loss = 0.30382409\n",
      "Iteration 490, loss = 0.30345054\n",
      "Iteration 491, loss = 0.30307780\n",
      "Iteration 492, loss = 0.30270573\n",
      "Iteration 493, loss = 0.30233465\n",
      "Iteration 494, loss = 0.30196432\n",
      "Iteration 495, loss = 0.30159473\n",
      "Iteration 496, loss = 0.30122602\n",
      "Iteration 497, loss = 0.30085793\n",
      "Iteration 498, loss = 0.30049062\n",
      "Iteration 499, loss = 0.30012409\n",
      "Iteration 500, loss = 0.29975833\n",
      "Iteration 501, loss = 0.29939342\n",
      "Iteration 502, loss = 0.29902931\n",
      "Iteration 503, loss = 0.29866591\n",
      "Iteration 504, loss = 0.29830336\n",
      "Iteration 505, loss = 0.29794154\n",
      "Iteration 506, loss = 0.29758054\n",
      "Iteration 507, loss = 0.29722025\n",
      "Iteration 508, loss = 0.29686079\n",
      "Iteration 509, loss = 0.29650207\n",
      "Iteration 510, loss = 0.29614409\n",
      "Iteration 511, loss = 0.29578684\n",
      "Iteration 512, loss = 0.29543030\n",
      "Iteration 513, loss = 0.29507452\n",
      "Iteration 514, loss = 0.29471941\n",
      "Iteration 515, loss = 0.29436511\n",
      "Iteration 516, loss = 0.29401145\n",
      "Iteration 517, loss = 0.29365856\n",
      "Iteration 518, loss = 0.29330634\n",
      "Iteration 519, loss = 0.29295487\n",
      "Iteration 520, loss = 0.29260409\n",
      "Iteration 521, loss = 0.29225403\n",
      "Iteration 522, loss = 0.29190465\n",
      "Iteration 523, loss = 0.29155600\n",
      "Iteration 524, loss = 0.29120804\n",
      "Iteration 525, loss = 0.29086076\n",
      "Iteration 526, loss = 0.29051422\n",
      "Iteration 527, loss = 0.29016833\n",
      "Iteration 528, loss = 0.28982319\n",
      "Iteration 529, loss = 0.28947864\n",
      "Iteration 530, loss = 0.28913481\n",
      "Iteration 531, loss = 0.28879165\n",
      "Iteration 532, loss = 0.28844917\n",
      "Iteration 533, loss = 0.28810744\n",
      "Iteration 534, loss = 0.28776636\n",
      "Iteration 535, loss = 0.28742597\n",
      "Iteration 536, loss = 0.28708630\n",
      "Iteration 537, loss = 0.28674731\n",
      "Iteration 538, loss = 0.28640916\n",
      "Iteration 539, loss = 0.28607169\n",
      "Iteration 540, loss = 0.28573496\n",
      "Iteration 541, loss = 0.28539886\n",
      "Iteration 542, loss = 0.28506354\n",
      "Iteration 543, loss = 0.28472885\n",
      "Iteration 544, loss = 0.28439488\n",
      "Iteration 545, loss = 0.28406171\n",
      "Iteration 546, loss = 0.28372920\n",
      "Iteration 547, loss = 0.28339754\n",
      "Iteration 548, loss = 0.28306653\n",
      "Iteration 549, loss = 0.28273629\n",
      "Iteration 550, loss = 0.28240690\n",
      "Iteration 551, loss = 0.28207823\n",
      "Iteration 552, loss = 0.28175015\n",
      "Iteration 553, loss = 0.28142275\n",
      "Iteration 554, loss = 0.28109611\n",
      "Iteration 555, loss = 0.28077004\n",
      "Iteration 556, loss = 0.28044465\n",
      "Iteration 557, loss = 0.28011996\n",
      "Iteration 558, loss = 0.27979592\n",
      "Iteration 559, loss = 0.27947256\n",
      "Iteration 560, loss = 0.27914988\n",
      "Iteration 561, loss = 0.27882792\n",
      "Iteration 562, loss = 0.27850663\n",
      "Iteration 563, loss = 0.27818603\n",
      "Iteration 564, loss = 0.27786605\n",
      "Iteration 565, loss = 0.27754675\n",
      "Iteration 566, loss = 0.27722814\n",
      "Iteration 567, loss = 0.27691023\n",
      "Iteration 568, loss = 0.27659301\n",
      "Iteration 569, loss = 0.27627652\n",
      "Iteration 570, loss = 0.27596071\n",
      "Iteration 571, loss = 0.27564552\n",
      "Iteration 572, loss = 0.27533098\n",
      "Iteration 573, loss = 0.27501708\n",
      "Iteration 574, loss = 0.27470389\n",
      "Iteration 575, loss = 0.27439132\n",
      "Iteration 576, loss = 0.27407937\n",
      "Iteration 577, loss = 0.27376808\n",
      "Iteration 578, loss = 0.27345743\n",
      "Iteration 579, loss = 0.27314742\n",
      "Iteration 580, loss = 0.27283804\n",
      "Iteration 581, loss = 0.27252933\n",
      "Iteration 582, loss = 0.27222119\n",
      "Iteration 583, loss = 0.27191370\n",
      "Iteration 584, loss = 0.27160684\n",
      "Iteration 585, loss = 0.27130061\n",
      "Iteration 586, loss = 0.27099518\n",
      "Iteration 587, loss = 0.27069036\n",
      "Iteration 588, loss = 0.27038616\n",
      "Iteration 589, loss = 0.27008261\n",
      "Iteration 590, loss = 0.26977968\n",
      "Iteration 591, loss = 0.26947737\n",
      "Iteration 592, loss = 0.26917568\n",
      "Iteration 593, loss = 0.26887463\n",
      "Iteration 594, loss = 0.26857416\n",
      "Iteration 595, loss = 0.26827430\n",
      "Iteration 596, loss = 0.26797505\n",
      "Iteration 597, loss = 0.26767641\n",
      "Iteration 598, loss = 0.26737841\n",
      "Iteration 599, loss = 0.26708099\n",
      "Iteration 600, loss = 0.26678417\n",
      "Iteration 601, loss = 0.26648795\n",
      "Iteration 602, loss = 0.26619234\n",
      "Iteration 603, loss = 0.26589736\n",
      "Iteration 604, loss = 0.26560291\n",
      "Iteration 605, loss = 0.26530909\n",
      "Iteration 606, loss = 0.26501587\n",
      "Iteration 607, loss = 0.26472327\n",
      "Iteration 608, loss = 0.26443123\n",
      "Iteration 609, loss = 0.26413981\n",
      "Iteration 610, loss = 0.26384897\n",
      "Iteration 611, loss = 0.26355874\n",
      "Iteration 612, loss = 0.26326911\n",
      "Iteration 613, loss = 0.26298010\n",
      "Iteration 614, loss = 0.26269164\n",
      "Iteration 615, loss = 0.26240378\n",
      "Iteration 616, loss = 0.26211654\n",
      "Iteration 617, loss = 0.26182989\n",
      "Iteration 618, loss = 0.26154382\n",
      "Iteration 619, loss = 0.26125833\n",
      "Iteration 620, loss = 0.26097341\n",
      "Iteration 621, loss = 0.26068908\n",
      "Iteration 622, loss = 0.26040534\n",
      "Iteration 623, loss = 0.26012219\n",
      "Iteration 624, loss = 0.25983965\n",
      "Iteration 625, loss = 0.25955763\n",
      "Iteration 626, loss = 0.25927622\n",
      "Iteration 627, loss = 0.25899539\n",
      "Iteration 628, loss = 0.25871513\n",
      "Iteration 629, loss = 0.25843540\n",
      "Iteration 630, loss = 0.25815623\n",
      "Iteration 631, loss = 0.25787761\n",
      "Iteration 632, loss = 0.25759956\n",
      "Iteration 633, loss = 0.25732207\n",
      "Iteration 634, loss = 0.25704514\n",
      "Iteration 635, loss = 0.25676878\n",
      "Iteration 636, loss = 0.25649299\n",
      "Iteration 637, loss = 0.25621774\n",
      "Iteration 638, loss = 0.25594306\n",
      "Iteration 639, loss = 0.25566894\n",
      "Iteration 640, loss = 0.25539534\n",
      "Iteration 641, loss = 0.25512229\n",
      "Iteration 642, loss = 0.25484981\n",
      "Iteration 643, loss = 0.25457788\n",
      "Iteration 644, loss = 0.25430648\n",
      "Iteration 645, loss = 0.25403561\n",
      "Iteration 646, loss = 0.25376530\n",
      "Iteration 647, loss = 0.25349540\n",
      "Iteration 648, loss = 0.25322599\n",
      "Iteration 649, loss = 0.25295711\n",
      "Iteration 650, loss = 0.25268875\n",
      "Iteration 651, loss = 0.25242080\n",
      "Iteration 652, loss = 0.25215340\n",
      "Iteration 653, loss = 0.25188652\n",
      "Iteration 654, loss = 0.25162015\n",
      "Iteration 655, loss = 0.25135432\n",
      "Iteration 656, loss = 0.25108897\n",
      "Iteration 657, loss = 0.25082415\n",
      "Iteration 658, loss = 0.25055983\n",
      "Iteration 659, loss = 0.25029605\n",
      "Iteration 660, loss = 0.25003279\n",
      "Iteration 661, loss = 0.24977003\n",
      "Iteration 662, loss = 0.24950779\n",
      "Iteration 663, loss = 0.24924606\n",
      "Iteration 664, loss = 0.24898487\n",
      "Iteration 665, loss = 0.24872421\n",
      "Iteration 666, loss = 0.24846406\n",
      "Iteration 667, loss = 0.24820443\n",
      "Iteration 668, loss = 0.24794534\n",
      "Iteration 669, loss = 0.24768677\n",
      "Iteration 670, loss = 0.24742869\n",
      "Iteration 671, loss = 0.24717111\n",
      "Iteration 672, loss = 0.24691406\n",
      "Iteration 673, loss = 0.24665753\n",
      "Iteration 674, loss = 0.24640155\n",
      "Iteration 675, loss = 0.24614606\n",
      "Iteration 676, loss = 0.24589111\n",
      "Iteration 677, loss = 0.24563668\n",
      "Iteration 678, loss = 0.24538278\n",
      "Iteration 679, loss = 0.24512930\n",
      "Iteration 680, loss = 0.24487622\n",
      "Iteration 681, loss = 0.24462363\n",
      "Iteration 682, loss = 0.24437145\n",
      "Iteration 683, loss = 0.24411977\n",
      "Iteration 684, loss = 0.24386860\n",
      "Iteration 685, loss = 0.24361792\n",
      "Iteration 686, loss = 0.24336772\n",
      "Iteration 687, loss = 0.24311797\n",
      "Iteration 688, loss = 0.24286873\n",
      "Iteration 689, loss = 0.24261995\n",
      "Iteration 690, loss = 0.24237163\n",
      "Iteration 691, loss = 0.24212381\n",
      "Iteration 692, loss = 0.24187650\n",
      "Iteration 693, loss = 0.24162970\n",
      "Iteration 694, loss = 0.24138331\n",
      "Iteration 695, loss = 0.24113730\n",
      "Iteration 696, loss = 0.24089173\n",
      "Iteration 697, loss = 0.24064664\n",
      "Iteration 698, loss = 0.24040203\n",
      "Iteration 699, loss = 0.24015791\n",
      "Iteration 700, loss = 0.23991427\n",
      "Iteration 701, loss = 0.23967112\n",
      "Iteration 702, loss = 0.23942846\n",
      "Iteration 703, loss = 0.23918628\n",
      "Iteration 704, loss = 0.23894449\n",
      "Iteration 705, loss = 0.23870319\n",
      "Iteration 706, loss = 0.23846238\n",
      "Iteration 707, loss = 0.23822206\n",
      "Iteration 708, loss = 0.23798223\n",
      "Iteration 709, loss = 0.23774290\n",
      "Iteration 710, loss = 0.23750404\n",
      "Iteration 711, loss = 0.23726557\n",
      "Iteration 712, loss = 0.23702757\n",
      "Iteration 713, loss = 0.23679008\n",
      "Iteration 714, loss = 0.23655301\n",
      "Iteration 715, loss = 0.23631638\n",
      "Iteration 716, loss = 0.23608023\n",
      "Iteration 717, loss = 0.23584453\n",
      "Iteration 718, loss = 0.23560928\n",
      "Iteration 719, loss = 0.23537450\n",
      "Iteration 720, loss = 0.23514020\n",
      "Iteration 721, loss = 0.23490614\n",
      "Iteration 722, loss = 0.23467235\n",
      "Iteration 723, loss = 0.23443901\n",
      "Iteration 724, loss = 0.23420612\n",
      "Iteration 725, loss = 0.23397359\n",
      "Iteration 726, loss = 0.23374146\n",
      "Iteration 727, loss = 0.23350979\n",
      "Iteration 728, loss = 0.23327854\n",
      "Iteration 729, loss = 0.23304761\n",
      "Iteration 730, loss = 0.23281713\n",
      "Iteration 731, loss = 0.23258711\n",
      "Iteration 732, loss = 0.23235753\n",
      "Iteration 733, loss = 0.23212842\n",
      "Iteration 734, loss = 0.23189980\n",
      "Iteration 735, loss = 0.23167165\n",
      "Iteration 736, loss = 0.23144373\n",
      "Iteration 737, loss = 0.23121604\n",
      "Iteration 738, loss = 0.23098875\n",
      "Iteration 739, loss = 0.23076189\n",
      "Iteration 740, loss = 0.23053547\n",
      "Iteration 741, loss = 0.23030937\n",
      "Iteration 742, loss = 0.23008365\n",
      "Iteration 743, loss = 0.22985810\n",
      "Iteration 744, loss = 0.22963288\n",
      "Iteration 745, loss = 0.22940802\n",
      "Iteration 746, loss = 0.22918348\n",
      "Iteration 747, loss = 0.22895936\n",
      "Iteration 748, loss = 0.22873567\n",
      "Iteration 749, loss = 0.22851234\n",
      "Iteration 750, loss = 0.22828927\n",
      "Iteration 751, loss = 0.22806662\n",
      "Iteration 752, loss = 0.22784439\n",
      "Iteration 753, loss = 0.22762260\n",
      "Iteration 754, loss = 0.22740124\n",
      "Iteration 755, loss = 0.22718032\n",
      "Iteration 756, loss = 0.22695966\n",
      "Iteration 757, loss = 0.22673935\n",
      "Iteration 758, loss = 0.22651946\n",
      "Iteration 759, loss = 0.22630001\n",
      "Iteration 760, loss = 0.22608102\n",
      "Iteration 761, loss = 0.22586247\n",
      "Iteration 762, loss = 0.22564436\n",
      "Iteration 763, loss = 0.22542671\n",
      "Iteration 764, loss = 0.22520951\n",
      "Iteration 765, loss = 0.22499258\n",
      "Iteration 766, loss = 0.22477599\n",
      "Iteration 767, loss = 0.22455964\n",
      "Iteration 768, loss = 0.22434365\n",
      "Iteration 769, loss = 0.22412809\n",
      "Iteration 770, loss = 0.22391296\n",
      "Iteration 771, loss = 0.22369827\n",
      "Iteration 772, loss = 0.22348402\n",
      "Iteration 773, loss = 0.22327020\n",
      "Iteration 774, loss = 0.22305683\n",
      "Iteration 775, loss = 0.22284389\n",
      "Iteration 776, loss = 0.22263140\n",
      "Iteration 777, loss = 0.22241936\n",
      "Iteration 778, loss = 0.22220777\n",
      "Iteration 779, loss = 0.22199662\n",
      "Iteration 780, loss = 0.22178596\n",
      "Iteration 781, loss = 0.22157569\n",
      "Iteration 782, loss = 0.22136588\n",
      "Iteration 783, loss = 0.22115653\n",
      "Iteration 784, loss = 0.22094765\n",
      "Iteration 785, loss = 0.22073922\n",
      "Iteration 786, loss = 0.22053124\n",
      "Iteration 787, loss = 0.22032370\n",
      "Iteration 788, loss = 0.22011661\n",
      "Iteration 789, loss = 0.21990997\n",
      "Iteration 790, loss = 0.21970378\n",
      "Iteration 791, loss = 0.21949804\n",
      "Iteration 792, loss = 0.21929274\n",
      "Iteration 793, loss = 0.21908776\n",
      "Iteration 794, loss = 0.21888289\n",
      "Iteration 795, loss = 0.21867845\n",
      "Iteration 796, loss = 0.21847443\n",
      "Iteration 797, loss = 0.21827084\n",
      "Iteration 798, loss = 0.21806764\n",
      "Iteration 799, loss = 0.21786484\n",
      "Iteration 800, loss = 0.21766245\n",
      "Iteration 801, loss = 0.21746042\n",
      "Iteration 802, loss = 0.21725882\n",
      "Iteration 803, loss = 0.21705765\n",
      "Iteration 804, loss = 0.21685691\n",
      "Iteration 805, loss = 0.21665661\n",
      "Iteration 806, loss = 0.21645674\n",
      "Iteration 807, loss = 0.21625729\n",
      "Iteration 808, loss = 0.21605830\n",
      "Iteration 809, loss = 0.21585969\n",
      "Iteration 810, loss = 0.21566150\n",
      "Iteration 811, loss = 0.21546372\n",
      "Iteration 812, loss = 0.21526637\n",
      "Iteration 813, loss = 0.21506944\n",
      "Iteration 814, loss = 0.21487293\n",
      "Iteration 815, loss = 0.21467684\n",
      "Iteration 816, loss = 0.21448120\n",
      "Iteration 817, loss = 0.21428594\n",
      "Iteration 818, loss = 0.21409112\n",
      "Iteration 819, loss = 0.21389671\n",
      "Iteration 820, loss = 0.21370273\n",
      "Iteration 821, loss = 0.21350917\n",
      "Iteration 822, loss = 0.21331602\n",
      "Iteration 823, loss = 0.21312329\n",
      "Iteration 824, loss = 0.21293087\n",
      "Iteration 825, loss = 0.21273887\n",
      "Iteration 826, loss = 0.21254729\n",
      "Iteration 827, loss = 0.21235611\n",
      "Iteration 828, loss = 0.21216534\n",
      "Iteration 829, loss = 0.21197498\n",
      "Iteration 830, loss = 0.21178504\n",
      "Iteration 831, loss = 0.21159549\n",
      "Iteration 832, loss = 0.21140609\n",
      "Iteration 833, loss = 0.21121708\n",
      "Iteration 834, loss = 0.21102845\n",
      "Iteration 835, loss = 0.21084021\n",
      "Iteration 836, loss = 0.21065240\n",
      "Iteration 837, loss = 0.21046496\n",
      "Iteration 838, loss = 0.21027793\n",
      "Iteration 839, loss = 0.21009123\n",
      "Iteration 840, loss = 0.20990489\n",
      "Iteration 841, loss = 0.20971895\n",
      "Iteration 842, loss = 0.20953339\n",
      "Iteration 843, loss = 0.20934824\n",
      "Iteration 844, loss = 0.20916346\n",
      "Iteration 845, loss = 0.20897908\n",
      "Iteration 846, loss = 0.20879508\n",
      "Iteration 847, loss = 0.20861147\n",
      "Iteration 848, loss = 0.20842826\n",
      "Iteration 849, loss = 0.20824543\n",
      "Iteration 850, loss = 0.20806302\n",
      "Iteration 851, loss = 0.20788094\n",
      "Iteration 852, loss = 0.20769923\n",
      "Iteration 853, loss = 0.20751791\n",
      "Iteration 854, loss = 0.20733698\n",
      "Iteration 855, loss = 0.20715645\n",
      "Iteration 856, loss = 0.20697629\n",
      "Iteration 857, loss = 0.20679653\n",
      "Iteration 858, loss = 0.20661714\n",
      "Iteration 859, loss = 0.20643813\n",
      "Iteration 860, loss = 0.20625945\n",
      "Iteration 861, loss = 0.20608115\n",
      "Iteration 862, loss = 0.20590324\n",
      "Iteration 863, loss = 0.20572570\n",
      "Iteration 864, loss = 0.20554854\n",
      "Iteration 865, loss = 0.20537160\n",
      "Iteration 866, loss = 0.20519498\n",
      "Iteration 867, loss = 0.20501863\n",
      "Iteration 868, loss = 0.20484256\n",
      "Iteration 869, loss = 0.20466684\n",
      "Iteration 870, loss = 0.20449145\n",
      "Iteration 871, loss = 0.20431632\n",
      "Iteration 872, loss = 0.20414154\n",
      "Iteration 873, loss = 0.20396715\n",
      "Iteration 874, loss = 0.20379305\n",
      "Iteration 875, loss = 0.20361934\n",
      "Iteration 876, loss = 0.20344596\n",
      "Iteration 877, loss = 0.20327280\n",
      "Iteration 878, loss = 0.20309996\n",
      "Iteration 879, loss = 0.20292741\n",
      "Iteration 880, loss = 0.20275520\n",
      "Iteration 881, loss = 0.20258334\n",
      "Iteration 882, loss = 0.20241179\n",
      "Iteration 883, loss = 0.20224047\n",
      "Iteration 884, loss = 0.20206949\n",
      "Iteration 885, loss = 0.20189886\n",
      "Iteration 886, loss = 0.20172857\n",
      "Iteration 887, loss = 0.20155863\n",
      "Iteration 888, loss = 0.20138905\n",
      "Iteration 889, loss = 0.20121969\n",
      "Iteration 890, loss = 0.20105059\n",
      "Iteration 891, loss = 0.20088179\n",
      "Iteration 892, loss = 0.20071336\n",
      "Iteration 893, loss = 0.20054515\n",
      "Iteration 894, loss = 0.20037722\n",
      "Iteration 895, loss = 0.20020964\n",
      "Iteration 896, loss = 0.20004239\n",
      "Iteration 897, loss = 0.19987549\n",
      "Iteration 898, loss = 0.19970897\n",
      "Iteration 899, loss = 0.19954275\n",
      "Iteration 900, loss = 0.19937680\n",
      "Iteration 901, loss = 0.19921129\n",
      "Iteration 902, loss = 0.19904599\n",
      "Iteration 903, loss = 0.19888112\n",
      "Iteration 904, loss = 0.19871639\n",
      "Iteration 905, loss = 0.19855191\n",
      "Iteration 906, loss = 0.19838775\n",
      "Iteration 907, loss = 0.19822388\n",
      "Iteration 908, loss = 0.19806025\n",
      "Iteration 909, loss = 0.19789696\n",
      "Iteration 910, loss = 0.19773397\n",
      "Iteration 911, loss = 0.19757133\n",
      "Iteration 912, loss = 0.19740902\n",
      "Iteration 913, loss = 0.19724703\n",
      "Iteration 914, loss = 0.19708537\n",
      "Iteration 915, loss = 0.19692386\n",
      "Iteration 916, loss = 0.19676265\n",
      "Iteration 917, loss = 0.19660175\n",
      "Iteration 918, loss = 0.19644114\n",
      "Iteration 919, loss = 0.19628073\n",
      "Iteration 920, loss = 0.19612064\n",
      "Iteration 921, loss = 0.19596085\n",
      "Iteration 922, loss = 0.19580139\n",
      "Iteration 923, loss = 0.19564228\n",
      "Iteration 924, loss = 0.19548346\n",
      "Iteration 925, loss = 0.19532498\n",
      "Iteration 926, loss = 0.19516682\n",
      "Iteration 927, loss = 0.19500902\n",
      "Iteration 928, loss = 0.19485151\n",
      "Iteration 929, loss = 0.19469431\n",
      "Iteration 930, loss = 0.19453735\n",
      "Iteration 931, loss = 0.19438067\n",
      "Iteration 932, loss = 0.19422431\n",
      "Iteration 933, loss = 0.19406831\n",
      "Iteration 934, loss = 0.19391258\n",
      "Iteration 935, loss = 0.19375715\n",
      "Iteration 936, loss = 0.19360199\n",
      "Iteration 937, loss = 0.19344718\n",
      "Iteration 938, loss = 0.19329261\n",
      "Iteration 939, loss = 0.19313833\n",
      "Iteration 940, loss = 0.19298438\n",
      "Iteration 941, loss = 0.19283074\n",
      "Iteration 942, loss = 0.19267741\n",
      "Iteration 943, loss = 0.19252442\n",
      "Iteration 944, loss = 0.19237171\n",
      "Iteration 945, loss = 0.19221929\n",
      "Iteration 946, loss = 0.19206694\n",
      "Iteration 947, loss = 0.19191468\n",
      "Iteration 948, loss = 0.19176265\n",
      "Iteration 949, loss = 0.19161091\n",
      "Iteration 950, loss = 0.19145947\n",
      "Iteration 951, loss = 0.19130829\n",
      "Iteration 952, loss = 0.19115741\n",
      "Iteration 953, loss = 0.19100686\n",
      "Iteration 954, loss = 0.19085655\n",
      "Iteration 955, loss = 0.19070657\n",
      "Iteration 956, loss = 0.19055688\n",
      "Iteration 957, loss = 0.19040751\n",
      "Iteration 958, loss = 0.19025844\n",
      "Iteration 959, loss = 0.19010966\n",
      "Iteration 960, loss = 0.18996120\n",
      "Iteration 961, loss = 0.18981304\n",
      "Iteration 962, loss = 0.18966518\n",
      "Iteration 963, loss = 0.18951765\n",
      "Iteration 964, loss = 0.18937040\n",
      "Iteration 965, loss = 0.18922346\n",
      "Iteration 966, loss = 0.18907684\n",
      "Iteration 967, loss = 0.18893052\n",
      "Iteration 968, loss = 0.18878452\n",
      "Iteration 969, loss = 0.18863881\n",
      "Iteration 970, loss = 0.18849339\n",
      "Iteration 971, loss = 0.18834827\n",
      "Iteration 972, loss = 0.18820347\n",
      "Iteration 973, loss = 0.18805897\n",
      "Iteration 974, loss = 0.18791491\n",
      "Iteration 975, loss = 0.18777161\n",
      "Iteration 976, loss = 0.18762854\n",
      "Iteration 977, loss = 0.18748578\n",
      "Iteration 978, loss = 0.18734333\n",
      "Iteration 979, loss = 0.18720119\n",
      "Iteration 980, loss = 0.18705936\n",
      "Iteration 981, loss = 0.18691784\n",
      "Iteration 982, loss = 0.18677663\n",
      "Iteration 983, loss = 0.18663575\n",
      "Iteration 984, loss = 0.18649515\n",
      "Iteration 985, loss = 0.18635627\n",
      "Iteration 986, loss = 0.18621787\n",
      "Iteration 987, loss = 0.18607985\n",
      "Iteration 988, loss = 0.18594215\n",
      "Iteration 989, loss = 0.18580532\n",
      "Iteration 990, loss = 0.18566935\n",
      "Iteration 991, loss = 0.18553369\n",
      "Iteration 992, loss = 0.18539837\n",
      "Iteration 993, loss = 0.18526334\n",
      "Iteration 994, loss = 0.18512860\n",
      "Iteration 995, loss = 0.18499419\n",
      "Iteration 996, loss = 0.18486005\n",
      "Iteration 997, loss = 0.18472622\n",
      "Iteration 998, loss = 0.18459269\n",
      "Iteration 999, loss = 0.18445970\n",
      "Iteration 1000, loss = 0.18432704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.66585275\n",
      "Iteration 2, loss = 1.62912514\n",
      "Iteration 3, loss = 1.57983559\n",
      "Iteration 4, loss = 1.52226751\n",
      "Iteration 5, loss = 1.46080908\n",
      "Iteration 6, loss = 1.39990779\n",
      "Iteration 7, loss = 1.34341118\n",
      "Iteration 8, loss = 1.29438294\n",
      "Iteration 9, loss = 1.25481799\n",
      "Iteration 10, loss = 1.22501209\n",
      "Iteration 11, loss = 1.20368430\n",
      "Iteration 12, loss = 1.18843672\n",
      "Iteration 13, loss = 1.17651553\n",
      "Iteration 14, loss = 1.16538625\n",
      "Iteration 15, loss = 1.15314176\n",
      "Iteration 16, loss = 1.13889774\n",
      "Iteration 17, loss = 1.12203322\n",
      "Iteration 18, loss = 1.10283474\n",
      "Iteration 19, loss = 1.08177142\n",
      "Iteration 20, loss = 1.05990638\n",
      "Iteration 21, loss = 1.03833776\n",
      "Iteration 22, loss = 1.01791930\n",
      "Iteration 23, loss = 0.99930274\n",
      "Iteration 24, loss = 0.98260565\n",
      "Iteration 25, loss = 0.96774938\n",
      "Iteration 26, loss = 0.95445292\n",
      "Iteration 27, loss = 0.94243325\n",
      "Iteration 28, loss = 0.93130953\n",
      "Iteration 29, loss = 0.92074937\n",
      "Iteration 30, loss = 0.91050598\n",
      "Iteration 31, loss = 0.90045733\n",
      "Iteration 32, loss = 0.89059965\n",
      "Iteration 33, loss = 0.88108193\n",
      "Iteration 34, loss = 0.87189368\n",
      "Iteration 35, loss = 0.86293818\n",
      "Iteration 36, loss = 0.85427372\n",
      "Iteration 37, loss = 0.84585682\n",
      "Iteration 38, loss = 0.83771309\n",
      "Iteration 39, loss = 0.82984559\n",
      "Iteration 40, loss = 0.82225928\n",
      "Iteration 41, loss = 0.81494155\n",
      "Iteration 42, loss = 0.80782838\n",
      "Iteration 43, loss = 0.80091809\n",
      "Iteration 44, loss = 0.79421374\n",
      "Iteration 45, loss = 0.78770388\n",
      "Iteration 46, loss = 0.78134704\n",
      "Iteration 47, loss = 0.77511591\n",
      "Iteration 48, loss = 0.76900163\n",
      "Iteration 49, loss = 0.76303755\n",
      "Iteration 50, loss = 0.75725460\n",
      "Iteration 51, loss = 0.75162858\n",
      "Iteration 52, loss = 0.74616704\n",
      "Iteration 53, loss = 0.74084629\n",
      "Iteration 54, loss = 0.73568476\n",
      "Iteration 55, loss = 0.73065207\n",
      "Iteration 56, loss = 0.72574492\n",
      "Iteration 57, loss = 0.72095590\n",
      "Iteration 58, loss = 0.71627337\n",
      "Iteration 59, loss = 0.71168795\n",
      "Iteration 60, loss = 0.70719534\n",
      "Iteration 61, loss = 0.70279262\n",
      "Iteration 62, loss = 0.69847839\n",
      "Iteration 63, loss = 0.69425099\n",
      "Iteration 64, loss = 0.69010931\n",
      "Iteration 65, loss = 0.68605211\n",
      "Iteration 66, loss = 0.68207789\n",
      "Iteration 67, loss = 0.67818490\n",
      "Iteration 68, loss = 0.67437162\n",
      "Iteration 69, loss = 0.67063617\n",
      "Iteration 70, loss = 0.66697577\n",
      "Iteration 71, loss = 0.66338678\n",
      "Iteration 72, loss = 0.65986625\n",
      "Iteration 73, loss = 0.65641188\n",
      "Iteration 74, loss = 0.65302223\n",
      "Iteration 75, loss = 0.64969525\n",
      "Iteration 76, loss = 0.64642940\n",
      "Iteration 77, loss = 0.64322294\n",
      "Iteration 78, loss = 0.64007430\n",
      "Iteration 79, loss = 0.63698141\n",
      "Iteration 80, loss = 0.63394365\n",
      "Iteration 81, loss = 0.63095942\n",
      "Iteration 82, loss = 0.62802721\n",
      "Iteration 83, loss = 0.62514553\n",
      "Iteration 84, loss = 0.62231290\n",
      "Iteration 85, loss = 0.61952809\n",
      "Iteration 86, loss = 0.61678963\n",
      "Iteration 87, loss = 0.61409630\n",
      "Iteration 88, loss = 0.61144691\n",
      "Iteration 89, loss = 0.60884242\n",
      "Iteration 90, loss = 0.60628078\n",
      "Iteration 91, loss = 0.60375993\n",
      "Iteration 92, loss = 0.60128151\n",
      "Iteration 93, loss = 0.59884169\n",
      "Iteration 94, loss = 0.59643985\n",
      "Iteration 95, loss = 0.59407517\n",
      "Iteration 96, loss = 0.59174677\n",
      "Iteration 97, loss = 0.58945299\n",
      "Iteration 98, loss = 0.58719301\n",
      "Iteration 99, loss = 0.58496685\n",
      "Iteration 100, loss = 0.58277320\n",
      "Iteration 101, loss = 0.58061100\n",
      "Iteration 102, loss = 0.57847946\n",
      "Iteration 103, loss = 0.57637791\n",
      "Iteration 104, loss = 0.57430505\n",
      "Iteration 105, loss = 0.57226001\n",
      "Iteration 106, loss = 0.57024221\n",
      "Iteration 107, loss = 0.56825118\n",
      "Iteration 108, loss = 0.56628666\n",
      "Iteration 109, loss = 0.56434806\n",
      "Iteration 110, loss = 0.56243474\n",
      "Iteration 111, loss = 0.56054624\n",
      "Iteration 112, loss = 0.55868193\n",
      "Iteration 113, loss = 0.55684043\n",
      "Iteration 114, loss = 0.55502153\n",
      "Iteration 115, loss = 0.55322510\n",
      "Iteration 116, loss = 0.55145070\n",
      "Iteration 117, loss = 0.54969776\n",
      "Iteration 118, loss = 0.54796524\n",
      "Iteration 119, loss = 0.54625279\n",
      "Iteration 120, loss = 0.54455986\n",
      "Iteration 121, loss = 0.54288659\n",
      "Iteration 122, loss = 0.54123261\n",
      "Iteration 123, loss = 0.53959709\n",
      "Iteration 124, loss = 0.53797880\n",
      "Iteration 125, loss = 0.53637707\n",
      "Iteration 126, loss = 0.53479151\n",
      "Iteration 127, loss = 0.53322272\n",
      "Iteration 128, loss = 0.53167017\n",
      "Iteration 129, loss = 0.53013359\n",
      "Iteration 130, loss = 0.52861245\n",
      "Iteration 131, loss = 0.52710602\n",
      "Iteration 132, loss = 0.52561483\n",
      "Iteration 133, loss = 0.52413877\n",
      "Iteration 134, loss = 0.52267776\n",
      "Iteration 135, loss = 0.52123120\n",
      "Iteration 136, loss = 0.51979967\n",
      "Iteration 137, loss = 0.51838115\n",
      "Iteration 138, loss = 0.51697451\n",
      "Iteration 139, loss = 0.51557854\n",
      "Iteration 140, loss = 0.51419559\n",
      "Iteration 141, loss = 0.51282552\n",
      "Iteration 142, loss = 0.51146807\n",
      "Iteration 143, loss = 0.51012325\n",
      "Iteration 144, loss = 0.50879074\n",
      "Iteration 145, loss = 0.50746894\n",
      "Iteration 146, loss = 0.50615689\n",
      "Iteration 147, loss = 0.50485506\n",
      "Iteration 148, loss = 0.50356295\n",
      "Iteration 149, loss = 0.50228059\n",
      "Iteration 150, loss = 0.50100516\n",
      "Iteration 151, loss = 0.49973942\n",
      "Iteration 152, loss = 0.49848045\n",
      "Iteration 153, loss = 0.49722992\n",
      "Iteration 154, loss = 0.49598862\n",
      "Iteration 155, loss = 0.49475904\n",
      "Iteration 156, loss = 0.49353937\n",
      "Iteration 157, loss = 0.49232882\n",
      "Iteration 158, loss = 0.49112444\n",
      "Iteration 159, loss = 0.48992819\n",
      "Iteration 160, loss = 0.48873880\n",
      "Iteration 161, loss = 0.48755453\n",
      "Iteration 162, loss = 0.48638142\n",
      "Iteration 163, loss = 0.48521745\n",
      "Iteration 164, loss = 0.48405884\n",
      "Iteration 165, loss = 0.48290532\n",
      "Iteration 166, loss = 0.48175631\n",
      "Iteration 167, loss = 0.48061256\n",
      "Iteration 168, loss = 0.47947297\n",
      "Iteration 169, loss = 0.47833535\n",
      "Iteration 170, loss = 0.47720470\n",
      "Iteration 171, loss = 0.47608845\n",
      "Iteration 172, loss = 0.47497257\n",
      "Iteration 173, loss = 0.47386680\n",
      "Iteration 174, loss = 0.47277458\n",
      "Iteration 175, loss = 0.47168963\n",
      "Iteration 176, loss = 0.47061787\n",
      "Iteration 177, loss = 0.46956309\n",
      "Iteration 178, loss = 0.46851933\n",
      "Iteration 179, loss = 0.46748516\n",
      "Iteration 180, loss = 0.46646055\n",
      "Iteration 181, loss = 0.46545180\n",
      "Iteration 182, loss = 0.46445693\n",
      "Iteration 183, loss = 0.46347074\n",
      "Iteration 184, loss = 0.46249439\n",
      "Iteration 185, loss = 0.46152486\n",
      "Iteration 186, loss = 0.46056242\n",
      "Iteration 187, loss = 0.45960861\n",
      "Iteration 188, loss = 0.45866158\n",
      "Iteration 189, loss = 0.45772041\n",
      "Iteration 190, loss = 0.45678621\n",
      "Iteration 191, loss = 0.45585680\n",
      "Iteration 192, loss = 0.45493312\n",
      "Iteration 193, loss = 0.45401510\n",
      "Iteration 194, loss = 0.45310241\n",
      "Iteration 195, loss = 0.45219477\n",
      "Iteration 196, loss = 0.45129210\n",
      "Iteration 197, loss = 0.45039419\n",
      "Iteration 198, loss = 0.44950126\n",
      "Iteration 199, loss = 0.44861294\n",
      "Iteration 200, loss = 0.44772897\n",
      "Iteration 201, loss = 0.44684954\n",
      "Iteration 202, loss = 0.44597458\n",
      "Iteration 203, loss = 0.44510430\n",
      "Iteration 204, loss = 0.44423863\n",
      "Iteration 205, loss = 0.44337709\n",
      "Iteration 206, loss = 0.44251956\n",
      "Iteration 207, loss = 0.44166610\n",
      "Iteration 208, loss = 0.44081687\n",
      "Iteration 209, loss = 0.43997233\n",
      "Iteration 210, loss = 0.43913189\n",
      "Iteration 211, loss = 0.43829558\n",
      "Iteration 212, loss = 0.43746322\n",
      "Iteration 213, loss = 0.43663486\n",
      "Iteration 214, loss = 0.43581049\n",
      "Iteration 215, loss = 0.43498978\n",
      "Iteration 216, loss = 0.43417295\n",
      "Iteration 217, loss = 0.43335981\n",
      "Iteration 218, loss = 0.43255051\n",
      "Iteration 219, loss = 0.43174495\n",
      "Iteration 220, loss = 0.43094251\n",
      "Iteration 221, loss = 0.43014322\n",
      "Iteration 222, loss = 0.42934664\n",
      "Iteration 223, loss = 0.42855306\n",
      "Iteration 224, loss = 0.42776282\n",
      "Iteration 225, loss = 0.42697540\n",
      "Iteration 226, loss = 0.42619063\n",
      "Iteration 227, loss = 0.42540851\n",
      "Iteration 228, loss = 0.42462929\n",
      "Iteration 229, loss = 0.42385306\n",
      "Iteration 230, loss = 0.42307976\n",
      "Iteration 231, loss = 0.42231044\n",
      "Iteration 232, loss = 0.42154389\n",
      "Iteration 233, loss = 0.42077996\n",
      "Iteration 234, loss = 0.42001900\n",
      "Iteration 235, loss = 0.41926080\n",
      "Iteration 236, loss = 0.41850525\n",
      "Iteration 237, loss = 0.41775266\n",
      "Iteration 238, loss = 0.41700280\n",
      "Iteration 239, loss = 0.41625629\n",
      "Iteration 240, loss = 0.41551262\n",
      "Iteration 241, loss = 0.41477153\n",
      "Iteration 242, loss = 0.41403307\n",
      "Iteration 243, loss = 0.41329800\n",
      "Iteration 244, loss = 0.41256564\n",
      "Iteration 245, loss = 0.41183560\n",
      "Iteration 246, loss = 0.41110784\n",
      "Iteration 247, loss = 0.41038256\n",
      "Iteration 248, loss = 0.40966047\n",
      "Iteration 249, loss = 0.40894084\n",
      "Iteration 250, loss = 0.40822372\n",
      "Iteration 251, loss = 0.40750898\n",
      "Iteration 252, loss = 0.40679657\n",
      "Iteration 253, loss = 0.40608656\n",
      "Iteration 254, loss = 0.40537910\n",
      "Iteration 255, loss = 0.40467502\n",
      "Iteration 256, loss = 0.40397435\n",
      "Iteration 257, loss = 0.40327604\n",
      "Iteration 258, loss = 0.40258048\n",
      "Iteration 259, loss = 0.40188764\n",
      "Iteration 260, loss = 0.40119791\n",
      "Iteration 261, loss = 0.40051069\n",
      "Iteration 262, loss = 0.39982579\n",
      "Iteration 263, loss = 0.39914305\n",
      "Iteration 264, loss = 0.39846338\n",
      "Iteration 265, loss = 0.39778699\n",
      "Iteration 266, loss = 0.39711322\n",
      "Iteration 267, loss = 0.39644164\n",
      "Iteration 268, loss = 0.39577220\n",
      "Iteration 269, loss = 0.39510583\n",
      "Iteration 270, loss = 0.39444234\n",
      "Iteration 271, loss = 0.39378114\n",
      "Iteration 272, loss = 0.39312277\n",
      "Iteration 273, loss = 0.39246668\n",
      "Iteration 274, loss = 0.39181309\n",
      "Iteration 275, loss = 0.39116174\n",
      "Iteration 276, loss = 0.39051241\n",
      "Iteration 277, loss = 0.38986547\n",
      "Iteration 278, loss = 0.38922048\n",
      "Iteration 279, loss = 0.38857752\n",
      "Iteration 280, loss = 0.38793648\n",
      "Iteration 281, loss = 0.38729756\n",
      "Iteration 282, loss = 0.38666052\n",
      "Iteration 283, loss = 0.38602549\n",
      "Iteration 284, loss = 0.38539223\n",
      "Iteration 285, loss = 0.38476072\n",
      "Iteration 286, loss = 0.38413119\n",
      "Iteration 287, loss = 0.38350347\n",
      "Iteration 288, loss = 0.38287751\n",
      "Iteration 289, loss = 0.38225333\n",
      "Iteration 290, loss = 0.38163097\n",
      "Iteration 291, loss = 0.38101036\n",
      "Iteration 292, loss = 0.38039181\n",
      "Iteration 293, loss = 0.37977497\n",
      "Iteration 294, loss = 0.37916014\n",
      "Iteration 295, loss = 0.37854704\n",
      "Iteration 296, loss = 0.37793566\n",
      "Iteration 297, loss = 0.37732601\n",
      "Iteration 298, loss = 0.37671809\n",
      "Iteration 299, loss = 0.37611182\n",
      "Iteration 300, loss = 0.37550728\n",
      "Iteration 301, loss = 0.37490441\n",
      "Iteration 302, loss = 0.37430318\n",
      "Iteration 303, loss = 0.37370353\n",
      "Iteration 304, loss = 0.37310530\n",
      "Iteration 305, loss = 0.37250865\n",
      "Iteration 306, loss = 0.37191351\n",
      "Iteration 307, loss = 0.37132002\n",
      "Iteration 308, loss = 0.37072803\n",
      "Iteration 309, loss = 0.37013769\n",
      "Iteration 310, loss = 0.36954875\n",
      "Iteration 311, loss = 0.36896146\n",
      "Iteration 312, loss = 0.36837589\n",
      "Iteration 313, loss = 0.36779187\n",
      "Iteration 314, loss = 0.36720924\n",
      "Iteration 315, loss = 0.36662815\n",
      "Iteration 316, loss = 0.36604835\n",
      "Iteration 317, loss = 0.36547038\n",
      "Iteration 318, loss = 0.36489394\n",
      "Iteration 319, loss = 0.36431902\n",
      "Iteration 320, loss = 0.36374561\n",
      "Iteration 321, loss = 0.36317358\n",
      "Iteration 322, loss = 0.36260306\n",
      "Iteration 323, loss = 0.36203397\n",
      "Iteration 324, loss = 0.36146645\n",
      "Iteration 325, loss = 0.36090017\n",
      "Iteration 326, loss = 0.36033551\n",
      "Iteration 327, loss = 0.35977206\n",
      "Iteration 328, loss = 0.35920987\n",
      "Iteration 329, loss = 0.35864901\n",
      "Iteration 330, loss = 0.35808940\n",
      "Iteration 331, loss = 0.35753127\n",
      "Iteration 332, loss = 0.35697466\n",
      "Iteration 333, loss = 0.35641976\n",
      "Iteration 334, loss = 0.35586599\n",
      "Iteration 335, loss = 0.35531365\n",
      "Iteration 336, loss = 0.35476246\n",
      "Iteration 337, loss = 0.35421259\n",
      "Iteration 338, loss = 0.35366400\n",
      "Iteration 339, loss = 0.35311678\n",
      "Iteration 340, loss = 0.35257075\n",
      "Iteration 341, loss = 0.35202643\n",
      "Iteration 342, loss = 0.35148330\n",
      "Iteration 343, loss = 0.35094139\n",
      "Iteration 344, loss = 0.35040077\n",
      "Iteration 345, loss = 0.34986131\n",
      "Iteration 346, loss = 0.34932324\n",
      "Iteration 347, loss = 0.34878650\n",
      "Iteration 348, loss = 0.34825117\n",
      "Iteration 349, loss = 0.34771688\n",
      "Iteration 350, loss = 0.34718387\n",
      "Iteration 351, loss = 0.34665215\n",
      "Iteration 352, loss = 0.34612171\n",
      "Iteration 353, loss = 0.34559245\n",
      "Iteration 354, loss = 0.34506460\n",
      "Iteration 355, loss = 0.34453797\n",
      "Iteration 356, loss = 0.34401255\n",
      "Iteration 357, loss = 0.34348834\n",
      "Iteration 358, loss = 0.34296526\n",
      "Iteration 359, loss = 0.34244344\n",
      "Iteration 360, loss = 0.34192284\n",
      "Iteration 361, loss = 0.34140366\n",
      "Iteration 362, loss = 0.34088559\n",
      "Iteration 363, loss = 0.34036879\n",
      "Iteration 364, loss = 0.33985313\n",
      "Iteration 365, loss = 0.33933864\n",
      "Iteration 366, loss = 0.33882531\n",
      "Iteration 367, loss = 0.33831328\n",
      "Iteration 368, loss = 0.33780265\n",
      "Iteration 369, loss = 0.33729306\n",
      "Iteration 370, loss = 0.33678488\n",
      "Iteration 371, loss = 0.33627774\n",
      "Iteration 372, loss = 0.33577204\n",
      "Iteration 373, loss = 0.33526741\n",
      "Iteration 374, loss = 0.33476388\n",
      "Iteration 375, loss = 0.33426152\n",
      "Iteration 376, loss = 0.33376027\n",
      "Iteration 377, loss = 0.33326027\n",
      "Iteration 378, loss = 0.33276133\n",
      "Iteration 379, loss = 0.33226364\n",
      "Iteration 380, loss = 0.33176697\n",
      "Iteration 381, loss = 0.33127140\n",
      "Iteration 382, loss = 0.33077697\n",
      "Iteration 383, loss = 0.33028360\n",
      "Iteration 384, loss = 0.32979143\n",
      "Iteration 385, loss = 0.32930033\n",
      "Iteration 386, loss = 0.32881052\n",
      "Iteration 387, loss = 0.32832170\n",
      "Iteration 388, loss = 0.32783401\n",
      "Iteration 389, loss = 0.32734735\n",
      "Iteration 390, loss = 0.32686191\n",
      "Iteration 391, loss = 0.32637729\n",
      "Iteration 392, loss = 0.32589389\n",
      "Iteration 393, loss = 0.32541142\n",
      "Iteration 394, loss = 0.32492997\n",
      "Iteration 395, loss = 0.32444964\n",
      "Iteration 396, loss = 0.32397025\n",
      "Iteration 397, loss = 0.32349202\n",
      "Iteration 398, loss = 0.32301477\n",
      "Iteration 399, loss = 0.32253864\n",
      "Iteration 400, loss = 0.32206348\n",
      "Iteration 401, loss = 0.32158955\n",
      "Iteration 402, loss = 0.32111654\n",
      "Iteration 403, loss = 0.32064457\n",
      "Iteration 404, loss = 0.32017361\n",
      "Iteration 405, loss = 0.31970370\n",
      "Iteration 406, loss = 0.31923487\n",
      "Iteration 407, loss = 0.31876707\n",
      "Iteration 408, loss = 0.31830032\n",
      "Iteration 409, loss = 0.31783451\n",
      "Iteration 410, loss = 0.31736984\n",
      "Iteration 411, loss = 0.31690602\n",
      "Iteration 412, loss = 0.31644324\n",
      "Iteration 413, loss = 0.31598133\n",
      "Iteration 414, loss = 0.31552046\n",
      "Iteration 415, loss = 0.31506049\n",
      "Iteration 416, loss = 0.31460164\n",
      "Iteration 417, loss = 0.31414369\n",
      "Iteration 418, loss = 0.31368683\n",
      "Iteration 419, loss = 0.31323094\n",
      "Iteration 420, loss = 0.31277612\n",
      "Iteration 421, loss = 0.31232229\n",
      "Iteration 422, loss = 0.31186945\n",
      "Iteration 423, loss = 0.31141757\n",
      "Iteration 424, loss = 0.31096661\n",
      "Iteration 425, loss = 0.31051675\n",
      "Iteration 426, loss = 0.31006770\n",
      "Iteration 427, loss = 0.30961971\n",
      "Iteration 428, loss = 0.30917261\n",
      "Iteration 429, loss = 0.30872651\n",
      "Iteration 430, loss = 0.30828130\n",
      "Iteration 431, loss = 0.30783716\n",
      "Iteration 432, loss = 0.30739357\n",
      "Iteration 433, loss = 0.30695100\n",
      "Iteration 434, loss = 0.30650928\n",
      "Iteration 435, loss = 0.30606854\n",
      "Iteration 436, loss = 0.30562869\n",
      "Iteration 437, loss = 0.30518991\n",
      "Iteration 438, loss = 0.30475202\n",
      "Iteration 439, loss = 0.30431519\n",
      "Iteration 440, loss = 0.30387922\n",
      "Iteration 441, loss = 0.30344424\n",
      "Iteration 442, loss = 0.30301024\n",
      "Iteration 443, loss = 0.30257711\n",
      "Iteration 444, loss = 0.30214499\n",
      "Iteration 445, loss = 0.30171403\n",
      "Iteration 446, loss = 0.30128397\n",
      "Iteration 447, loss = 0.30085490\n",
      "Iteration 448, loss = 0.30042666\n",
      "Iteration 449, loss = 0.29999906\n",
      "Iteration 450, loss = 0.29957225\n",
      "Iteration 451, loss = 0.29914648\n",
      "Iteration 452, loss = 0.29872146\n",
      "Iteration 453, loss = 0.29829748\n",
      "Iteration 454, loss = 0.29787434\n",
      "Iteration 455, loss = 0.29745222\n",
      "Iteration 456, loss = 0.29703094\n",
      "Iteration 457, loss = 0.29661067\n",
      "Iteration 458, loss = 0.29619057\n",
      "Iteration 459, loss = 0.29577016\n",
      "Iteration 460, loss = 0.29535041\n",
      "Iteration 461, loss = 0.29493153\n",
      "Iteration 462, loss = 0.29451328\n",
      "Iteration 463, loss = 0.29409488\n",
      "Iteration 464, loss = 0.29367717\n",
      "Iteration 465, loss = 0.29326024\n",
      "Iteration 466, loss = 0.29284396\n",
      "Iteration 467, loss = 0.29242856\n",
      "Iteration 468, loss = 0.29201382\n",
      "Iteration 469, loss = 0.29159994\n",
      "Iteration 470, loss = 0.29118674\n",
      "Iteration 471, loss = 0.29077385\n",
      "Iteration 472, loss = 0.29036173\n",
      "Iteration 473, loss = 0.28995040\n",
      "Iteration 474, loss = 0.28953993\n",
      "Iteration 475, loss = 0.28913020\n",
      "Iteration 476, loss = 0.28872136\n",
      "Iteration 477, loss = 0.28831325\n",
      "Iteration 478, loss = 0.28790611\n",
      "Iteration 479, loss = 0.28749970\n",
      "Iteration 480, loss = 0.28709369\n",
      "Iteration 481, loss = 0.28668769\n",
      "Iteration 482, loss = 0.28628066\n",
      "Iteration 483, loss = 0.28587311\n",
      "Iteration 484, loss = 0.28546562\n",
      "Iteration 485, loss = 0.28505672\n",
      "Iteration 486, loss = 0.28464516\n",
      "Iteration 487, loss = 0.28423128\n",
      "Iteration 488, loss = 0.28381406\n",
      "Iteration 489, loss = 0.28339633\n",
      "Iteration 490, loss = 0.28297545\n",
      "Iteration 491, loss = 0.28255128\n",
      "Iteration 492, loss = 0.28212393\n",
      "Iteration 493, loss = 0.28169580\n",
      "Iteration 494, loss = 0.28126610\n",
      "Iteration 495, loss = 0.28083002\n",
      "Iteration 496, loss = 0.28038641\n",
      "Iteration 497, loss = 0.27993901\n",
      "Iteration 498, loss = 0.27948742\n",
      "Iteration 499, loss = 0.27902972\n",
      "Iteration 500, loss = 0.27856781\n",
      "Iteration 501, loss = 0.27810055\n",
      "Iteration 502, loss = 0.27762144\n",
      "Iteration 503, loss = 0.27713427\n",
      "Iteration 504, loss = 0.27664019\n",
      "Iteration 505, loss = 0.27614470\n",
      "Iteration 506, loss = 0.27565012\n",
      "Iteration 507, loss = 0.27515335\n",
      "Iteration 508, loss = 0.27466147\n",
      "Iteration 509, loss = 0.27416967\n",
      "Iteration 510, loss = 0.27367651\n",
      "Iteration 511, loss = 0.27319198\n",
      "Iteration 512, loss = 0.27271428\n",
      "Iteration 513, loss = 0.27225324\n",
      "Iteration 514, loss = 0.27179816\n",
      "Iteration 515, loss = 0.27135931\n",
      "Iteration 516, loss = 0.27093160\n",
      "Iteration 517, loss = 0.27050476\n",
      "Iteration 518, loss = 0.27007952\n",
      "Iteration 519, loss = 0.26966249\n",
      "Iteration 520, loss = 0.26925553\n",
      "Iteration 521, loss = 0.26885782\n",
      "Iteration 522, loss = 0.26846547\n",
      "Iteration 523, loss = 0.26807831\n",
      "Iteration 524, loss = 0.26769483\n",
      "Iteration 525, loss = 0.26731436\n",
      "Iteration 526, loss = 0.26693908\n",
      "Iteration 527, loss = 0.26656360\n",
      "Iteration 528, loss = 0.26619036\n",
      "Iteration 529, loss = 0.26582202\n",
      "Iteration 530, loss = 0.26545761\n",
      "Iteration 531, loss = 0.26509477\n",
      "Iteration 532, loss = 0.26473332\n",
      "Iteration 533, loss = 0.26437405\n",
      "Iteration 534, loss = 0.26401625\n",
      "Iteration 535, loss = 0.26365974\n",
      "Iteration 536, loss = 0.26330430\n",
      "Iteration 537, loss = 0.26295007\n",
      "Iteration 538, loss = 0.26259725\n",
      "Iteration 539, loss = 0.26224636\n",
      "Iteration 540, loss = 0.26189704\n",
      "Iteration 541, loss = 0.26155028\n",
      "Iteration 542, loss = 0.26120501\n",
      "Iteration 543, loss = 0.26086077\n",
      "Iteration 544, loss = 0.26051744\n",
      "Iteration 545, loss = 0.26017569\n",
      "Iteration 546, loss = 0.25983507\n",
      "Iteration 547, loss = 0.25949526\n",
      "Iteration 548, loss = 0.25915642\n",
      "Iteration 549, loss = 0.25881825\n",
      "Iteration 550, loss = 0.25848100\n",
      "Iteration 551, loss = 0.25814462\n",
      "Iteration 552, loss = 0.25780927\n",
      "Iteration 553, loss = 0.25747465\n",
      "Iteration 554, loss = 0.25714084\n",
      "Iteration 555, loss = 0.25680788\n",
      "Iteration 556, loss = 0.25647559\n",
      "Iteration 557, loss = 0.25614419\n",
      "Iteration 558, loss = 0.25581377\n",
      "Iteration 559, loss = 0.25548411\n",
      "Iteration 560, loss = 0.25515526\n",
      "Iteration 561, loss = 0.25482718\n",
      "Iteration 562, loss = 0.25449987\n",
      "Iteration 563, loss = 0.25417331\n",
      "Iteration 564, loss = 0.25384756\n",
      "Iteration 565, loss = 0.25352252\n",
      "Iteration 566, loss = 0.25319827\n",
      "Iteration 567, loss = 0.25287467\n",
      "Iteration 568, loss = 0.25255181\n",
      "Iteration 569, loss = 0.25222972\n",
      "Iteration 570, loss = 0.25190846\n",
      "Iteration 571, loss = 0.25158779\n",
      "Iteration 572, loss = 0.25126785\n",
      "Iteration 573, loss = 0.25094862\n",
      "Iteration 574, loss = 0.25063012\n",
      "Iteration 575, loss = 0.25031229\n",
      "Iteration 576, loss = 0.24999518\n",
      "Iteration 577, loss = 0.24967873\n",
      "Iteration 578, loss = 0.24936298\n",
      "Iteration 579, loss = 0.24904803\n",
      "Iteration 580, loss = 0.24873390\n",
      "Iteration 581, loss = 0.24842041\n",
      "Iteration 582, loss = 0.24810770\n",
      "Iteration 583, loss = 0.24779567\n",
      "Iteration 584, loss = 0.24748437\n",
      "Iteration 585, loss = 0.24717382\n",
      "Iteration 586, loss = 0.24686389\n",
      "Iteration 587, loss = 0.24655468\n",
      "Iteration 588, loss = 0.24624614\n",
      "Iteration 589, loss = 0.24593828\n",
      "Iteration 590, loss = 0.24563108\n",
      "Iteration 591, loss = 0.24532458\n",
      "Iteration 592, loss = 0.24501871\n",
      "Iteration 593, loss = 0.24471353\n",
      "Iteration 594, loss = 0.24440913\n",
      "Iteration 595, loss = 0.24410533\n",
      "Iteration 596, loss = 0.24380221\n",
      "Iteration 597, loss = 0.24349977\n",
      "Iteration 598, loss = 0.24319794\n",
      "Iteration 599, loss = 0.24289683\n",
      "Iteration 600, loss = 0.24259637\n",
      "Iteration 601, loss = 0.24229661\n",
      "Iteration 602, loss = 0.24199749\n",
      "Iteration 603, loss = 0.24169901\n",
      "Iteration 604, loss = 0.24140125\n",
      "Iteration 605, loss = 0.24110411\n",
      "Iteration 606, loss = 0.24080764\n",
      "Iteration 607, loss = 0.24051183\n",
      "Iteration 608, loss = 0.24021665\n",
      "Iteration 609, loss = 0.23992211\n",
      "Iteration 610, loss = 0.23962820\n",
      "Iteration 611, loss = 0.23933495\n",
      "Iteration 612, loss = 0.23904237\n",
      "Iteration 613, loss = 0.23875036\n",
      "Iteration 614, loss = 0.23845901\n",
      "Iteration 615, loss = 0.23816831\n",
      "Iteration 616, loss = 0.23787825\n",
      "Iteration 617, loss = 0.23758883\n",
      "Iteration 618, loss = 0.23730009\n",
      "Iteration 619, loss = 0.23701189\n",
      "Iteration 620, loss = 0.23672432\n",
      "Iteration 621, loss = 0.23643740\n",
      "Iteration 622, loss = 0.23615108\n",
      "Iteration 623, loss = 0.23586538\n",
      "Iteration 624, loss = 0.23558029\n",
      "Iteration 625, loss = 0.23529590\n",
      "Iteration 626, loss = 0.23501204\n",
      "Iteration 627, loss = 0.23472884\n",
      "Iteration 628, loss = 0.23444627\n",
      "Iteration 629, loss = 0.23416429\n",
      "Iteration 630, loss = 0.23388297\n",
      "Iteration 631, loss = 0.23360222\n",
      "Iteration 632, loss = 0.23332210\n",
      "Iteration 633, loss = 0.23304257\n",
      "Iteration 634, loss = 0.23276365\n",
      "Iteration 635, loss = 0.23248533\n",
      "Iteration 636, loss = 0.23220767\n",
      "Iteration 637, loss = 0.23193055\n",
      "Iteration 638, loss = 0.23165405\n",
      "Iteration 639, loss = 0.23137817\n",
      "Iteration 640, loss = 0.23110286\n",
      "Iteration 641, loss = 0.23082819\n",
      "Iteration 642, loss = 0.23055408\n",
      "Iteration 643, loss = 0.23028057\n",
      "Iteration 644, loss = 0.23000764\n",
      "Iteration 645, loss = 0.22973530\n",
      "Iteration 646, loss = 0.22946362\n",
      "Iteration 647, loss = 0.22919243\n",
      "Iteration 648, loss = 0.22892187\n",
      "Iteration 649, loss = 0.22865187\n",
      "Iteration 650, loss = 0.22838247\n",
      "Iteration 651, loss = 0.22811361\n",
      "Iteration 652, loss = 0.22784535\n",
      "Iteration 653, loss = 0.22757766\n",
      "Iteration 654, loss = 0.22731057\n",
      "Iteration 655, loss = 0.22704397\n",
      "Iteration 656, loss = 0.22677781\n",
      "Iteration 657, loss = 0.22651223\n",
      "Iteration 658, loss = 0.22624720\n",
      "Iteration 659, loss = 0.22598274\n",
      "Iteration 660, loss = 0.22571888\n",
      "Iteration 661, loss = 0.22545559\n",
      "Iteration 662, loss = 0.22519284\n",
      "Iteration 663, loss = 0.22493061\n",
      "Iteration 664, loss = 0.22466891\n",
      "Iteration 665, loss = 0.22440772\n",
      "Iteration 666, loss = 0.22414710\n",
      "Iteration 667, loss = 0.22388703\n",
      "Iteration 668, loss = 0.22362754\n",
      "Iteration 669, loss = 0.22336855\n",
      "Iteration 670, loss = 0.22311010\n",
      "Iteration 671, loss = 0.22285220\n",
      "Iteration 672, loss = 0.22259480\n",
      "Iteration 673, loss = 0.22233784\n",
      "Iteration 674, loss = 0.22208140\n",
      "Iteration 675, loss = 0.22182549\n",
      "Iteration 676, loss = 0.22157013\n",
      "Iteration 677, loss = 0.22131523\n",
      "Iteration 678, loss = 0.22106090\n",
      "Iteration 679, loss = 0.22080708\n",
      "Iteration 680, loss = 0.22055377\n",
      "Iteration 681, loss = 0.22030101\n",
      "Iteration 682, loss = 0.22004878\n",
      "Iteration 683, loss = 0.21979711\n",
      "Iteration 684, loss = 0.21954599\n",
      "Iteration 685, loss = 0.21929534\n",
      "Iteration 686, loss = 0.21904523\n",
      "Iteration 687, loss = 0.21879569\n",
      "Iteration 688, loss = 0.21854665\n",
      "Iteration 689, loss = 0.21829815\n",
      "Iteration 690, loss = 0.21805018\n",
      "Iteration 691, loss = 0.21780279\n",
      "Iteration 692, loss = 0.21755590\n",
      "Iteration 693, loss = 0.21730953\n",
      "Iteration 694, loss = 0.21706362\n",
      "Iteration 695, loss = 0.21681829\n",
      "Iteration 696, loss = 0.21657348\n",
      "Iteration 697, loss = 0.21632918\n",
      "Iteration 698, loss = 0.21608544\n",
      "Iteration 699, loss = 0.21584217\n",
      "Iteration 700, loss = 0.21559942\n",
      "Iteration 701, loss = 0.21535717\n",
      "Iteration 702, loss = 0.21511539\n",
      "Iteration 703, loss = 0.21487413\n",
      "Iteration 704, loss = 0.21463334\n",
      "Iteration 705, loss = 0.21439303\n",
      "Iteration 706, loss = 0.21415325\n",
      "Iteration 707, loss = 0.21391393\n",
      "Iteration 708, loss = 0.21367513\n",
      "Iteration 709, loss = 0.21343681\n",
      "Iteration 710, loss = 0.21319899\n",
      "Iteration 711, loss = 0.21296174\n",
      "Iteration 712, loss = 0.21272490\n",
      "Iteration 713, loss = 0.21248857\n",
      "Iteration 714, loss = 0.21225274\n",
      "Iteration 715, loss = 0.21201725\n",
      "Iteration 716, loss = 0.21178219\n",
      "Iteration 717, loss = 0.21154762\n",
      "Iteration 718, loss = 0.21131345\n",
      "Iteration 719, loss = 0.21107973\n",
      "Iteration 720, loss = 0.21084653\n",
      "Iteration 721, loss = 0.21061380\n",
      "Iteration 722, loss = 0.21038156\n",
      "Iteration 723, loss = 0.21014985\n",
      "Iteration 724, loss = 0.20991860\n",
      "Iteration 725, loss = 0.20968773\n",
      "Iteration 726, loss = 0.20945738\n",
      "Iteration 727, loss = 0.20922751\n",
      "Iteration 728, loss = 0.20899814\n",
      "Iteration 729, loss = 0.20876925\n",
      "Iteration 730, loss = 0.20854088\n",
      "Iteration 731, loss = 0.20831292\n",
      "Iteration 732, loss = 0.20808539\n",
      "Iteration 733, loss = 0.20785834\n",
      "Iteration 734, loss = 0.20763162\n",
      "Iteration 735, loss = 0.20740541\n",
      "Iteration 736, loss = 0.20717953\n",
      "Iteration 737, loss = 0.20695411\n",
      "Iteration 738, loss = 0.20672915\n",
      "Iteration 739, loss = 0.20650455\n",
      "Iteration 740, loss = 0.20628003\n",
      "Iteration 741, loss = 0.20605591\n",
      "Iteration 742, loss = 0.20583215\n",
      "Iteration 743, loss = 0.20560886\n",
      "Iteration 744, loss = 0.20538588\n",
      "Iteration 745, loss = 0.20516329\n",
      "Iteration 746, loss = 0.20494110\n",
      "Iteration 747, loss = 0.20471943\n",
      "Iteration 748, loss = 0.20449814\n",
      "Iteration 749, loss = 0.20427741\n",
      "Iteration 750, loss = 0.20405705\n",
      "Iteration 751, loss = 0.20383723\n",
      "Iteration 752, loss = 0.20361787\n",
      "Iteration 753, loss = 0.20339898\n",
      "Iteration 754, loss = 0.20318054\n",
      "Iteration 755, loss = 0.20296238\n",
      "Iteration 756, loss = 0.20274456\n",
      "Iteration 757, loss = 0.20252713\n",
      "Iteration 758, loss = 0.20231011\n",
      "Iteration 759, loss = 0.20209354\n",
      "Iteration 760, loss = 0.20187742\n",
      "Iteration 761, loss = 0.20166157\n",
      "Iteration 762, loss = 0.20144615\n",
      "Iteration 763, loss = 0.20123115\n",
      "Iteration 764, loss = 0.20101650\n",
      "Iteration 765, loss = 0.20080221\n",
      "Iteration 766, loss = 0.20058838\n",
      "Iteration 767, loss = 0.20037480\n",
      "Iteration 768, loss = 0.20016140\n",
      "Iteration 769, loss = 0.19994841\n",
      "Iteration 770, loss = 0.19973589\n",
      "Iteration 771, loss = 0.19952372\n",
      "Iteration 772, loss = 0.19931206\n",
      "Iteration 773, loss = 0.19910078\n",
      "Iteration 774, loss = 0.19888993\n",
      "Iteration 775, loss = 0.19867959\n",
      "Iteration 776, loss = 0.19846964\n",
      "Iteration 777, loss = 0.19826013\n",
      "Iteration 778, loss = 0.19805111\n",
      "Iteration 779, loss = 0.19784251\n",
      "Iteration 780, loss = 0.19763436\n",
      "Iteration 781, loss = 0.19742674\n",
      "Iteration 782, loss = 0.19721950\n",
      "Iteration 783, loss = 0.19701270\n",
      "Iteration 784, loss = 0.19680644\n",
      "Iteration 785, loss = 0.19660054\n",
      "Iteration 786, loss = 0.19639517\n",
      "Iteration 787, loss = 0.19619024\n",
      "Iteration 788, loss = 0.19598574\n",
      "Iteration 789, loss = 0.19578170\n",
      "Iteration 790, loss = 0.19557817\n",
      "Iteration 791, loss = 0.19537502\n",
      "Iteration 792, loss = 0.19517239\n",
      "Iteration 793, loss = 0.19497019\n",
      "Iteration 794, loss = 0.19476847\n",
      "Iteration 795, loss = 0.19456717\n",
      "Iteration 796, loss = 0.19436639\n",
      "Iteration 797, loss = 0.19416600\n",
      "Iteration 798, loss = 0.19396608\n",
      "Iteration 799, loss = 0.19376665\n",
      "Iteration 800, loss = 0.19356761\n",
      "Iteration 801, loss = 0.19336907\n",
      "Iteration 802, loss = 0.19317100\n",
      "Iteration 803, loss = 0.19297331\n",
      "Iteration 804, loss = 0.19277616\n",
      "Iteration 805, loss = 0.19257944\n",
      "Iteration 806, loss = 0.19238316\n",
      "Iteration 807, loss = 0.19218735\n",
      "Iteration 808, loss = 0.19199198\n",
      "Iteration 809, loss = 0.19179707\n",
      "Iteration 810, loss = 0.19160256\n",
      "Iteration 811, loss = 0.19140855\n",
      "Iteration 812, loss = 0.19121465\n",
      "Iteration 813, loss = 0.19102100\n",
      "Iteration 814, loss = 0.19082773\n",
      "Iteration 815, loss = 0.19063466\n",
      "Iteration 816, loss = 0.19044204\n",
      "Iteration 817, loss = 0.19024979\n",
      "Iteration 818, loss = 0.19005796\n",
      "Iteration 819, loss = 0.18986652\n",
      "Iteration 820, loss = 0.18967552\n",
      "Iteration 821, loss = 0.18948490\n",
      "Iteration 822, loss = 0.18929471\n",
      "Iteration 823, loss = 0.18910493\n",
      "Iteration 824, loss = 0.18891556\n",
      "Iteration 825, loss = 0.18872662\n",
      "Iteration 826, loss = 0.18853805\n",
      "Iteration 827, loss = 0.18834992\n",
      "Iteration 828, loss = 0.18816213\n",
      "Iteration 829, loss = 0.18797476\n",
      "Iteration 830, loss = 0.18778787\n",
      "Iteration 831, loss = 0.18760131\n",
      "Iteration 832, loss = 0.18741522\n",
      "Iteration 833, loss = 0.18722956\n",
      "Iteration 834, loss = 0.18704433\n",
      "Iteration 835, loss = 0.18685950\n",
      "Iteration 836, loss = 0.18667511\n",
      "Iteration 837, loss = 0.18649111\n",
      "Iteration 838, loss = 0.18630752\n",
      "Iteration 839, loss = 0.18612438\n",
      "Iteration 840, loss = 0.18594163\n",
      "Iteration 841, loss = 0.18575928\n",
      "Iteration 842, loss = 0.18557740\n",
      "Iteration 843, loss = 0.18539586\n",
      "Iteration 844, loss = 0.18521467\n",
      "Iteration 845, loss = 0.18503371\n",
      "Iteration 846, loss = 0.18485313\n",
      "Iteration 847, loss = 0.18467293\n",
      "Iteration 848, loss = 0.18449311\n",
      "Iteration 849, loss = 0.18431371\n",
      "Iteration 850, loss = 0.18413467\n",
      "Iteration 851, loss = 0.18395601\n",
      "Iteration 852, loss = 0.18377772\n",
      "Iteration 853, loss = 0.18359979\n",
      "Iteration 854, loss = 0.18342221\n",
      "Iteration 855, loss = 0.18324502\n",
      "Iteration 856, loss = 0.18306814\n",
      "Iteration 857, loss = 0.18289167\n",
      "Iteration 858, loss = 0.18271557\n",
      "Iteration 859, loss = 0.18253985\n",
      "Iteration 860, loss = 0.18236450\n",
      "Iteration 861, loss = 0.18218957\n",
      "Iteration 862, loss = 0.18201496\n",
      "Iteration 863, loss = 0.18184077\n",
      "Iteration 864, loss = 0.18166695\n",
      "Iteration 865, loss = 0.18149350\n",
      "Iteration 866, loss = 0.18132044\n",
      "Iteration 867, loss = 0.18114780\n",
      "Iteration 868, loss = 0.18097550\n",
      "Iteration 869, loss = 0.18080358\n",
      "Iteration 870, loss = 0.18063208\n",
      "Iteration 871, loss = 0.18046093\n",
      "Iteration 872, loss = 0.18029016\n",
      "Iteration 873, loss = 0.18011976\n",
      "Iteration 874, loss = 0.17994974\n",
      "Iteration 875, loss = 0.17978001\n",
      "Iteration 876, loss = 0.17961065\n",
      "Iteration 877, loss = 0.17944170\n",
      "Iteration 878, loss = 0.17927306\n",
      "Iteration 879, loss = 0.17910470\n",
      "Iteration 880, loss = 0.17893672\n",
      "Iteration 881, loss = 0.17876902\n",
      "Iteration 882, loss = 0.17860164\n",
      "Iteration 883, loss = 0.17843458\n",
      "Iteration 884, loss = 0.17826782\n",
      "Iteration 885, loss = 0.17810136\n",
      "Iteration 886, loss = 0.17793523\n",
      "Iteration 887, loss = 0.17776943\n",
      "Iteration 888, loss = 0.17760396\n",
      "Iteration 889, loss = 0.17743887\n",
      "Iteration 890, loss = 0.17727402\n",
      "Iteration 891, loss = 0.17710942\n",
      "Iteration 892, loss = 0.17694521\n",
      "Iteration 893, loss = 0.17678124\n",
      "Iteration 894, loss = 0.17661759\n",
      "Iteration 895, loss = 0.17645430\n",
      "Iteration 896, loss = 0.17629130\n",
      "Iteration 897, loss = 0.17612866\n",
      "Iteration 898, loss = 0.17596634\n",
      "Iteration 899, loss = 0.17580429\n",
      "Iteration 900, loss = 0.17564259\n",
      "Iteration 901, loss = 0.17548126\n",
      "Iteration 902, loss = 0.17532023\n",
      "Iteration 903, loss = 0.17515946\n",
      "Iteration 904, loss = 0.17499897\n",
      "Iteration 905, loss = 0.17483870\n",
      "Iteration 906, loss = 0.17467870\n",
      "Iteration 907, loss = 0.17451902\n",
      "Iteration 908, loss = 0.17435959\n",
      "Iteration 909, loss = 0.17420037\n",
      "Iteration 910, loss = 0.17404138\n",
      "Iteration 911, loss = 0.17388271\n",
      "Iteration 912, loss = 0.17372433\n",
      "Iteration 913, loss = 0.17356627\n",
      "Iteration 914, loss = 0.17340855\n",
      "Iteration 915, loss = 0.17325113\n",
      "Iteration 916, loss = 0.17309397\n",
      "Iteration 917, loss = 0.17293716\n",
      "Iteration 918, loss = 0.17278060\n",
      "Iteration 919, loss = 0.17262432\n",
      "Iteration 920, loss = 0.17246830\n",
      "Iteration 921, loss = 0.17231255\n",
      "Iteration 922, loss = 0.17215710\n",
      "Iteration 923, loss = 0.17200193\n",
      "Iteration 924, loss = 0.17184705\n",
      "Iteration 925, loss = 0.17169248\n",
      "Iteration 926, loss = 0.17153825\n",
      "Iteration 927, loss = 0.17138431\n",
      "Iteration 928, loss = 0.17123071\n",
      "Iteration 929, loss = 0.17107743\n",
      "Iteration 930, loss = 0.17092447\n",
      "Iteration 931, loss = 0.17077183\n",
      "Iteration 932, loss = 0.17061950\n",
      "Iteration 933, loss = 0.17046745\n",
      "Iteration 934, loss = 0.17031574\n",
      "Iteration 935, loss = 0.17016423\n",
      "Iteration 936, loss = 0.17001301\n",
      "Iteration 937, loss = 0.16986211\n",
      "Iteration 938, loss = 0.16971152\n",
      "Iteration 939, loss = 0.16956124\n",
      "Iteration 940, loss = 0.16941130\n",
      "Iteration 941, loss = 0.16926165\n",
      "Iteration 942, loss = 0.16911232\n",
      "Iteration 943, loss = 0.16896334\n",
      "Iteration 944, loss = 0.16881464\n",
      "Iteration 945, loss = 0.16866627\n",
      "Iteration 946, loss = 0.16851820\n",
      "Iteration 947, loss = 0.16837036\n",
      "Iteration 948, loss = 0.16822280\n",
      "Iteration 949, loss = 0.16807556\n",
      "Iteration 950, loss = 0.16792864\n",
      "Iteration 951, loss = 0.16778201\n",
      "Iteration 952, loss = 0.16763573\n",
      "Iteration 953, loss = 0.16748973\n",
      "Iteration 954, loss = 0.16734404\n",
      "Iteration 955, loss = 0.16719859\n",
      "Iteration 956, loss = 0.16705339\n",
      "Iteration 957, loss = 0.16690850\n",
      "Iteration 958, loss = 0.16676441\n",
      "Iteration 959, loss = 0.16662087\n",
      "Iteration 960, loss = 0.16647769\n",
      "Iteration 961, loss = 0.16633488\n",
      "Iteration 962, loss = 0.16619234\n",
      "Iteration 963, loss = 0.16605013\n",
      "Iteration 964, loss = 0.16590823\n",
      "Iteration 965, loss = 0.16576659\n",
      "Iteration 966, loss = 0.16562514\n",
      "Iteration 967, loss = 0.16548398\n",
      "Iteration 968, loss = 0.16534313\n",
      "Iteration 969, loss = 0.16520258\n",
      "Iteration 970, loss = 0.16506233\n",
      "Iteration 971, loss = 0.16492239\n",
      "Iteration 972, loss = 0.16478275\n",
      "Iteration 973, loss = 0.16464339\n",
      "Iteration 974, loss = 0.16450438\n",
      "Iteration 975, loss = 0.16436561\n",
      "Iteration 976, loss = 0.16422715\n",
      "Iteration 977, loss = 0.16408898\n",
      "Iteration 978, loss = 0.16395110\n",
      "Iteration 979, loss = 0.16381350\n",
      "Iteration 980, loss = 0.16367621\n",
      "Iteration 981, loss = 0.16353921\n",
      "Iteration 982, loss = 0.16340249\n",
      "Iteration 983, loss = 0.16326609\n",
      "Iteration 984, loss = 0.16312995\n",
      "Iteration 985, loss = 0.16299411\n",
      "Iteration 986, loss = 0.16285859\n",
      "Iteration 987, loss = 0.16272333\n",
      "Iteration 988, loss = 0.16258836\n",
      "Iteration 989, loss = 0.16245370\n",
      "Iteration 990, loss = 0.16231932\n",
      "Iteration 991, loss = 0.16218523\n",
      "Iteration 992, loss = 0.16205143\n",
      "Iteration 993, loss = 0.16191791\n",
      "Iteration 994, loss = 0.16178466\n",
      "Iteration 995, loss = 0.16165213\n",
      "Iteration 996, loss = 0.16152034\n",
      "Iteration 997, loss = 0.16138886\n",
      "Iteration 998, loss = 0.16125769\n",
      "Iteration 999, loss = 0.16112686\n",
      "Iteration 1000, loss = 0.16099625\n",
      "Iteration 1, loss = 1.65830114\n",
      "Iteration 2, loss = 1.62211189\n",
      "Iteration 3, loss = 1.57349632\n",
      "Iteration 4, loss = 1.51659112\n",
      "Iteration 5, loss = 1.45574514\n",
      "Iteration 6, loss = 1.39546428\n",
      "Iteration 7, loss = 1.33949488\n",
      "Iteration 8, loss = 1.29090254\n",
      "Iteration 9, loss = 1.25160218\n",
      "Iteration 10, loss = 1.22193554\n",
      "Iteration 11, loss = 1.20067602\n",
      "Iteration 12, loss = 1.18548685\n",
      "Iteration 13, loss = 1.17370058\n",
      "Iteration 14, loss = 1.16283085\n",
      "Iteration 15, loss = 1.15110710\n",
      "Iteration 16, loss = 1.13715699\n",
      "Iteration 17, loss = 1.12058367\n",
      "Iteration 18, loss = 1.10164507\n",
      "Iteration 19, loss = 1.08094332\n",
      "Iteration 20, loss = 1.05946966\n",
      "Iteration 21, loss = 1.03828325\n",
      "Iteration 22, loss = 1.01826693\n",
      "Iteration 23, loss = 0.99998905\n",
      "Iteration 24, loss = 0.98362861\n",
      "Iteration 25, loss = 0.96917524\n",
      "Iteration 26, loss = 0.95622154\n",
      "Iteration 27, loss = 0.94448588\n",
      "Iteration 28, loss = 0.93362703\n",
      "Iteration 29, loss = 0.92329511\n",
      "Iteration 30, loss = 0.91328850\n",
      "Iteration 31, loss = 0.90355694\n",
      "Iteration 32, loss = 0.89412824\n",
      "Iteration 33, loss = 0.88496435\n",
      "Iteration 34, loss = 0.87612091\n",
      "Iteration 35, loss = 0.86757613\n",
      "Iteration 36, loss = 0.85923582\n",
      "Iteration 37, loss = 0.85112397\n",
      "Iteration 38, loss = 0.84332205\n",
      "Iteration 39, loss = 0.83581651\n",
      "Iteration 40, loss = 0.82860608\n",
      "Iteration 41, loss = 0.82167597\n",
      "Iteration 42, loss = 0.81499998\n",
      "Iteration 43, loss = 0.80852805\n",
      "Iteration 44, loss = 0.80226352\n",
      "Iteration 45, loss = 0.79615101\n",
      "Iteration 46, loss = 0.79016807\n",
      "Iteration 47, loss = 0.78427672\n",
      "Iteration 48, loss = 0.77847534\n",
      "Iteration 49, loss = 0.77277321\n",
      "Iteration 50, loss = 0.76718265\n",
      "Iteration 51, loss = 0.76171230\n",
      "Iteration 52, loss = 0.75637081\n",
      "Iteration 53, loss = 0.75115857\n",
      "Iteration 54, loss = 0.74607349\n",
      "Iteration 55, loss = 0.74110961\n",
      "Iteration 56, loss = 0.73625973\n",
      "Iteration 57, loss = 0.73151462\n",
      "Iteration 58, loss = 0.72686917\n",
      "Iteration 59, loss = 0.72231790\n",
      "Iteration 60, loss = 0.71785555\n",
      "Iteration 61, loss = 0.71347779\n",
      "Iteration 62, loss = 0.70918117\n",
      "Iteration 63, loss = 0.70496688\n",
      "Iteration 64, loss = 0.70083681\n",
      "Iteration 65, loss = 0.69679105\n",
      "Iteration 66, loss = 0.69282732\n",
      "Iteration 67, loss = 0.68894377\n",
      "Iteration 68, loss = 0.68513726\n",
      "Iteration 69, loss = 0.68140482\n",
      "Iteration 70, loss = 0.67774413\n",
      "Iteration 71, loss = 0.67415301\n",
      "Iteration 72, loss = 0.67062972\n",
      "Iteration 73, loss = 0.66717199\n",
      "Iteration 74, loss = 0.66377770\n",
      "Iteration 75, loss = 0.66044421\n",
      "Iteration 76, loss = 0.65717001\n",
      "Iteration 77, loss = 0.65395382\n",
      "Iteration 78, loss = 0.65079434\n",
      "Iteration 79, loss = 0.64769024\n",
      "Iteration 80, loss = 0.64464020\n",
      "Iteration 81, loss = 0.64164273\n",
      "Iteration 82, loss = 0.63869698\n",
      "Iteration 83, loss = 0.63580053\n",
      "Iteration 84, loss = 0.63295248\n",
      "Iteration 85, loss = 0.63015107\n",
      "Iteration 86, loss = 0.62739504\n",
      "Iteration 87, loss = 0.62468275\n",
      "Iteration 88, loss = 0.62201289\n",
      "Iteration 89, loss = 0.61938499\n",
      "Iteration 90, loss = 0.61679783\n",
      "Iteration 91, loss = 0.61425060\n",
      "Iteration 92, loss = 0.61174216\n",
      "Iteration 93, loss = 0.60927163\n",
      "Iteration 94, loss = 0.60683809\n",
      "Iteration 95, loss = 0.60444128\n",
      "Iteration 96, loss = 0.60208079\n",
      "Iteration 97, loss = 0.59975519\n",
      "Iteration 98, loss = 0.59746307\n",
      "Iteration 99, loss = 0.59520359\n",
      "Iteration 100, loss = 0.59297615\n",
      "Iteration 101, loss = 0.59078073\n",
      "Iteration 102, loss = 0.58861594\n",
      "Iteration 103, loss = 0.58648102\n",
      "Iteration 104, loss = 0.58437534\n",
      "Iteration 105, loss = 0.58229835\n",
      "Iteration 106, loss = 0.58024938\n",
      "Iteration 107, loss = 0.57822840\n",
      "Iteration 108, loss = 0.57623394\n",
      "Iteration 109, loss = 0.57426512\n",
      "Iteration 110, loss = 0.57232148\n",
      "Iteration 111, loss = 0.57040156\n",
      "Iteration 112, loss = 0.56850476\n",
      "Iteration 113, loss = 0.56663060\n",
      "Iteration 114, loss = 0.56477954\n",
      "Iteration 115, loss = 0.56295098\n",
      "Iteration 116, loss = 0.56114417\n",
      "Iteration 117, loss = 0.55935865\n",
      "Iteration 118, loss = 0.55759400\n",
      "Iteration 119, loss = 0.55584990\n",
      "Iteration 120, loss = 0.55412584\n",
      "Iteration 121, loss = 0.55242128\n",
      "Iteration 122, loss = 0.55073582\n",
      "Iteration 123, loss = 0.54906904\n",
      "Iteration 124, loss = 0.54742051\n",
      "Iteration 125, loss = 0.54578866\n",
      "Iteration 126, loss = 0.54417428\n",
      "Iteration 127, loss = 0.54257698\n",
      "Iteration 128, loss = 0.54099640\n",
      "Iteration 129, loss = 0.53943174\n",
      "Iteration 130, loss = 0.53788272\n",
      "Iteration 131, loss = 0.53635073\n",
      "Iteration 132, loss = 0.53483347\n",
      "Iteration 133, loss = 0.53333128\n",
      "Iteration 134, loss = 0.53184320\n",
      "Iteration 135, loss = 0.53036945\n",
      "Iteration 136, loss = 0.52890719\n",
      "Iteration 137, loss = 0.52745478\n",
      "Iteration 138, loss = 0.52601486\n",
      "Iteration 139, loss = 0.52458506\n",
      "Iteration 140, loss = 0.52316647\n",
      "Iteration 141, loss = 0.52175977\n",
      "Iteration 142, loss = 0.52036482\n",
      "Iteration 143, loss = 0.51898166\n",
      "Iteration 144, loss = 0.51760952\n",
      "Iteration 145, loss = 0.51624608\n",
      "Iteration 146, loss = 0.51489325\n",
      "Iteration 147, loss = 0.51354878\n",
      "Iteration 148, loss = 0.51221063\n",
      "Iteration 149, loss = 0.51088489\n",
      "Iteration 150, loss = 0.50956848\n",
      "Iteration 151, loss = 0.50825856\n",
      "Iteration 152, loss = 0.50695345\n",
      "Iteration 153, loss = 0.50565547\n",
      "Iteration 154, loss = 0.50435756\n",
      "Iteration 155, loss = 0.50306656\n",
      "Iteration 156, loss = 0.50178493\n",
      "Iteration 157, loss = 0.50051269\n",
      "Iteration 158, loss = 0.49924831\n",
      "Iteration 159, loss = 0.49799783\n",
      "Iteration 160, loss = 0.49676188\n",
      "Iteration 161, loss = 0.49553698\n",
      "Iteration 162, loss = 0.49432776\n",
      "Iteration 163, loss = 0.49313862\n",
      "Iteration 164, loss = 0.49196126\n",
      "Iteration 165, loss = 0.49079881\n",
      "Iteration 166, loss = 0.48965192\n",
      "Iteration 167, loss = 0.48852189\n",
      "Iteration 168, loss = 0.48740510\n",
      "Iteration 169, loss = 0.48629965\n",
      "Iteration 170, loss = 0.48520292\n",
      "Iteration 171, loss = 0.48411780\n",
      "Iteration 172, loss = 0.48304467\n",
      "Iteration 173, loss = 0.48197988\n",
      "Iteration 174, loss = 0.48092303\n",
      "Iteration 175, loss = 0.47987374\n",
      "Iteration 176, loss = 0.47883141\n",
      "Iteration 177, loss = 0.47779603\n",
      "Iteration 178, loss = 0.47676703\n",
      "Iteration 179, loss = 0.47574403\n",
      "Iteration 180, loss = 0.47472729\n",
      "Iteration 181, loss = 0.47371735\n",
      "Iteration 182, loss = 0.47271279\n",
      "Iteration 183, loss = 0.47171378\n",
      "Iteration 184, loss = 0.47072032\n",
      "Iteration 185, loss = 0.46973253\n",
      "Iteration 186, loss = 0.46875077\n",
      "Iteration 187, loss = 0.46777529\n",
      "Iteration 188, loss = 0.46680558\n",
      "Iteration 189, loss = 0.46584169\n",
      "Iteration 190, loss = 0.46488345\n",
      "Iteration 191, loss = 0.46393074\n",
      "Iteration 192, loss = 0.46298340\n",
      "Iteration 193, loss = 0.46204142\n",
      "Iteration 194, loss = 0.46110489\n",
      "Iteration 195, loss = 0.46017408\n",
      "Iteration 196, loss = 0.45924848\n",
      "Iteration 197, loss = 0.45832799\n",
      "Iteration 198, loss = 0.45741249\n",
      "Iteration 199, loss = 0.45650193\n",
      "Iteration 200, loss = 0.45559655\n",
      "Iteration 201, loss = 0.45469655\n",
      "Iteration 202, loss = 0.45380128\n",
      "Iteration 203, loss = 0.45291065\n",
      "Iteration 204, loss = 0.45202451\n",
      "Iteration 205, loss = 0.45114294\n",
      "Iteration 206, loss = 0.45026589\n",
      "Iteration 207, loss = 0.44939328\n",
      "Iteration 208, loss = 0.44852513\n",
      "Iteration 209, loss = 0.44766122\n",
      "Iteration 210, loss = 0.44680140\n",
      "Iteration 211, loss = 0.44594573\n",
      "Iteration 212, loss = 0.44509417\n",
      "Iteration 213, loss = 0.44424668\n",
      "Iteration 214, loss = 0.44340318\n",
      "Iteration 215, loss = 0.44256365\n",
      "Iteration 216, loss = 0.44172802\n",
      "Iteration 217, loss = 0.44089628\n",
      "Iteration 218, loss = 0.44006840\n",
      "Iteration 219, loss = 0.43924424\n",
      "Iteration 220, loss = 0.43842383\n",
      "Iteration 221, loss = 0.43760711\n",
      "Iteration 222, loss = 0.43679391\n",
      "Iteration 223, loss = 0.43598404\n",
      "Iteration 224, loss = 0.43517741\n",
      "Iteration 225, loss = 0.43437409\n",
      "Iteration 226, loss = 0.43357417\n",
      "Iteration 227, loss = 0.43277761\n",
      "Iteration 228, loss = 0.43198441\n",
      "Iteration 229, loss = 0.43119441\n",
      "Iteration 230, loss = 0.43040774\n",
      "Iteration 231, loss = 0.42962381\n",
      "Iteration 232, loss = 0.42884309\n",
      "Iteration 233, loss = 0.42806549\n",
      "Iteration 234, loss = 0.42729094\n",
      "Iteration 235, loss = 0.42651961\n",
      "Iteration 236, loss = 0.42575128\n",
      "Iteration 237, loss = 0.42498575\n",
      "Iteration 238, loss = 0.42422223\n",
      "Iteration 239, loss = 0.42346149\n",
      "Iteration 240, loss = 0.42270378\n",
      "Iteration 241, loss = 0.42194894\n",
      "Iteration 242, loss = 0.42119670\n",
      "Iteration 243, loss = 0.42044690\n",
      "Iteration 244, loss = 0.41969935\n",
      "Iteration 245, loss = 0.41895438\n",
      "Iteration 246, loss = 0.41821181\n",
      "Iteration 247, loss = 0.41747164\n",
      "Iteration 248, loss = 0.41673393\n",
      "Iteration 249, loss = 0.41599846\n",
      "Iteration 250, loss = 0.41526541\n",
      "Iteration 251, loss = 0.41453477\n",
      "Iteration 252, loss = 0.41380656\n",
      "Iteration 253, loss = 0.41308076\n",
      "Iteration 254, loss = 0.41235714\n",
      "Iteration 255, loss = 0.41163555\n",
      "Iteration 256, loss = 0.41091674\n",
      "Iteration 257, loss = 0.41020020\n",
      "Iteration 258, loss = 0.40948606\n",
      "Iteration 259, loss = 0.40877409\n",
      "Iteration 260, loss = 0.40806437\n",
      "Iteration 261, loss = 0.40735694\n",
      "Iteration 262, loss = 0.40665208\n",
      "Iteration 263, loss = 0.40594882\n",
      "Iteration 264, loss = 0.40524753\n",
      "Iteration 265, loss = 0.40454891\n",
      "Iteration 266, loss = 0.40385267\n",
      "Iteration 267, loss = 0.40315887\n",
      "Iteration 268, loss = 0.40246714\n",
      "Iteration 269, loss = 0.40177751\n",
      "Iteration 270, loss = 0.40109009\n",
      "Iteration 271, loss = 0.40040546\n",
      "Iteration 272, loss = 0.39972490\n",
      "Iteration 273, loss = 0.39904709\n",
      "Iteration 274, loss = 0.39837180\n",
      "Iteration 275, loss = 0.39769940\n",
      "Iteration 276, loss = 0.39702960\n",
      "Iteration 277, loss = 0.39636275\n",
      "Iteration 278, loss = 0.39569809\n",
      "Iteration 279, loss = 0.39503532\n",
      "Iteration 280, loss = 0.39437483\n",
      "Iteration 281, loss = 0.39371719\n",
      "Iteration 282, loss = 0.39306199\n",
      "Iteration 283, loss = 0.39240914\n",
      "Iteration 284, loss = 0.39175839\n",
      "Iteration 285, loss = 0.39110966\n",
      "Iteration 286, loss = 0.39046362\n",
      "Iteration 287, loss = 0.38981994\n",
      "Iteration 288, loss = 0.38917822\n",
      "Iteration 289, loss = 0.38853898\n",
      "Iteration 290, loss = 0.38790195\n",
      "Iteration 291, loss = 0.38726693\n",
      "Iteration 292, loss = 0.38663371\n",
      "Iteration 293, loss = 0.38600237\n",
      "Iteration 294, loss = 0.38537290\n",
      "Iteration 295, loss = 0.38474511\n",
      "Iteration 296, loss = 0.38411897\n",
      "Iteration 297, loss = 0.38349472\n",
      "Iteration 298, loss = 0.38287225\n",
      "Iteration 299, loss = 0.38225164\n",
      "Iteration 300, loss = 0.38163295\n",
      "Iteration 301, loss = 0.38101578\n",
      "Iteration 302, loss = 0.38040046\n",
      "Iteration 303, loss = 0.37978695\n",
      "Iteration 304, loss = 0.37917511\n",
      "Iteration 305, loss = 0.37856510\n",
      "Iteration 306, loss = 0.37795663\n",
      "Iteration 307, loss = 0.37734983\n",
      "Iteration 308, loss = 0.37674465\n",
      "Iteration 309, loss = 0.37614109\n",
      "Iteration 310, loss = 0.37553909\n",
      "Iteration 311, loss = 0.37493859\n",
      "Iteration 312, loss = 0.37433977\n",
      "Iteration 313, loss = 0.37374257\n",
      "Iteration 314, loss = 0.37314690\n",
      "Iteration 315, loss = 0.37255304\n",
      "Iteration 316, loss = 0.37196078\n",
      "Iteration 317, loss = 0.37137024\n",
      "Iteration 318, loss = 0.37078132\n",
      "Iteration 319, loss = 0.37019410\n",
      "Iteration 320, loss = 0.36960843\n",
      "Iteration 321, loss = 0.36902429\n",
      "Iteration 322, loss = 0.36844169\n",
      "Iteration 323, loss = 0.36786043\n",
      "Iteration 324, loss = 0.36728062\n",
      "Iteration 325, loss = 0.36670225\n",
      "Iteration 326, loss = 0.36612531\n",
      "Iteration 327, loss = 0.36554962\n",
      "Iteration 328, loss = 0.36497520\n",
      "Iteration 329, loss = 0.36440243\n",
      "Iteration 330, loss = 0.36383109\n",
      "Iteration 331, loss = 0.36326122\n",
      "Iteration 332, loss = 0.36269281\n",
      "Iteration 333, loss = 0.36212585\n",
      "Iteration 334, loss = 0.36156028\n",
      "Iteration 335, loss = 0.36099610\n",
      "Iteration 336, loss = 0.36043337\n",
      "Iteration 337, loss = 0.35987207\n",
      "Iteration 338, loss = 0.35931235\n",
      "Iteration 339, loss = 0.35875397\n",
      "Iteration 340, loss = 0.35819688\n",
      "Iteration 341, loss = 0.35764126\n",
      "Iteration 342, loss = 0.35708689\n",
      "Iteration 343, loss = 0.35653392\n",
      "Iteration 344, loss = 0.35598221\n",
      "Iteration 345, loss = 0.35543177\n",
      "Iteration 346, loss = 0.35488295\n",
      "Iteration 347, loss = 0.35433549\n",
      "Iteration 348, loss = 0.35378938\n",
      "Iteration 349, loss = 0.35324474\n",
      "Iteration 350, loss = 0.35270136\n",
      "Iteration 351, loss = 0.35215935\n",
      "Iteration 352, loss = 0.35161864\n",
      "Iteration 353, loss = 0.35107913\n",
      "Iteration 354, loss = 0.35054094\n",
      "Iteration 355, loss = 0.35000425\n",
      "Iteration 356, loss = 0.34946903\n",
      "Iteration 357, loss = 0.34893504\n",
      "Iteration 358, loss = 0.34840236\n",
      "Iteration 359, loss = 0.34787093\n",
      "Iteration 360, loss = 0.34734076\n",
      "Iteration 361, loss = 0.34681198\n",
      "Iteration 362, loss = 0.34628457\n",
      "Iteration 363, loss = 0.34575854\n",
      "Iteration 364, loss = 0.34523382\n",
      "Iteration 365, loss = 0.34471034\n",
      "Iteration 366, loss = 0.34418808\n",
      "Iteration 367, loss = 0.34366715\n",
      "Iteration 368, loss = 0.34314751\n",
      "Iteration 369, loss = 0.34262914\n",
      "Iteration 370, loss = 0.34211195\n",
      "Iteration 371, loss = 0.34159596\n",
      "Iteration 372, loss = 0.34108122\n",
      "Iteration 373, loss = 0.34056759\n",
      "Iteration 374, loss = 0.34005516\n",
      "Iteration 375, loss = 0.33954390\n",
      "Iteration 376, loss = 0.33903384\n",
      "Iteration 377, loss = 0.33852495\n",
      "Iteration 378, loss = 0.33801724\n",
      "Iteration 379, loss = 0.33751066\n",
      "Iteration 380, loss = 0.33700532\n",
      "Iteration 381, loss = 0.33650116\n",
      "Iteration 382, loss = 0.33599813\n",
      "Iteration 383, loss = 0.33549622\n",
      "Iteration 384, loss = 0.33499545\n",
      "Iteration 385, loss = 0.33449586\n",
      "Iteration 386, loss = 0.33399729\n",
      "Iteration 387, loss = 0.33349994\n",
      "Iteration 388, loss = 0.33300370\n",
      "Iteration 389, loss = 0.33250856\n",
      "Iteration 390, loss = 0.33201452\n",
      "Iteration 391, loss = 0.33152162\n",
      "Iteration 392, loss = 0.33102981\n",
      "Iteration 393, loss = 0.33053914\n",
      "Iteration 394, loss = 0.33004955\n",
      "Iteration 395, loss = 0.32956104\n",
      "Iteration 396, loss = 0.32907364\n",
      "Iteration 397, loss = 0.32858728\n",
      "Iteration 398, loss = 0.32810212\n",
      "Iteration 399, loss = 0.32761804\n",
      "Iteration 400, loss = 0.32713505\n",
      "Iteration 401, loss = 0.32665311\n",
      "Iteration 402, loss = 0.32617222\n",
      "Iteration 403, loss = 0.32569240\n",
      "Iteration 404, loss = 0.32521363\n",
      "Iteration 405, loss = 0.32473592\n",
      "Iteration 406, loss = 0.32425931\n",
      "Iteration 407, loss = 0.32378369\n",
      "Iteration 408, loss = 0.32330913\n",
      "Iteration 409, loss = 0.32283559\n",
      "Iteration 410, loss = 0.32236312\n",
      "Iteration 411, loss = 0.32189184\n",
      "Iteration 412, loss = 0.32142158\n",
      "Iteration 413, loss = 0.32095234\n",
      "Iteration 414, loss = 0.32048414\n",
      "Iteration 415, loss = 0.32001698\n",
      "Iteration 416, loss = 0.31955077\n",
      "Iteration 417, loss = 0.31908562\n",
      "Iteration 418, loss = 0.31862149\n",
      "Iteration 419, loss = 0.31815838\n",
      "Iteration 420, loss = 0.31769630\n",
      "Iteration 421, loss = 0.31723522\n",
      "Iteration 422, loss = 0.31677512\n",
      "Iteration 423, loss = 0.31631627\n",
      "Iteration 424, loss = 0.31585856\n",
      "Iteration 425, loss = 0.31540198\n",
      "Iteration 426, loss = 0.31494633\n",
      "Iteration 427, loss = 0.31449173\n",
      "Iteration 428, loss = 0.31403813\n",
      "Iteration 429, loss = 0.31358552\n",
      "Iteration 430, loss = 0.31313387\n",
      "Iteration 431, loss = 0.31268319\n",
      "Iteration 432, loss = 0.31223351\n",
      "Iteration 433, loss = 0.31178494\n",
      "Iteration 434, loss = 0.31133737\n",
      "Iteration 435, loss = 0.31089075\n",
      "Iteration 436, loss = 0.31044509\n",
      "Iteration 437, loss = 0.31000040\n",
      "Iteration 438, loss = 0.30955668\n",
      "Iteration 439, loss = 0.30911387\n",
      "Iteration 440, loss = 0.30867204\n",
      "Iteration 441, loss = 0.30823114\n",
      "Iteration 442, loss = 0.30779118\n",
      "Iteration 443, loss = 0.30735218\n",
      "Iteration 444, loss = 0.30691409\n",
      "Iteration 445, loss = 0.30647694\n",
      "Iteration 446, loss = 0.30604072\n",
      "Iteration 447, loss = 0.30560542\n",
      "Iteration 448, loss = 0.30517104\n",
      "Iteration 449, loss = 0.30473760\n",
      "Iteration 450, loss = 0.30430505\n",
      "Iteration 451, loss = 0.30387344\n",
      "Iteration 452, loss = 0.30344273\n",
      "Iteration 453, loss = 0.30301294\n",
      "Iteration 454, loss = 0.30258407\n",
      "Iteration 455, loss = 0.30215612\n",
      "Iteration 456, loss = 0.30172905\n",
      "Iteration 457, loss = 0.30130289\n",
      "Iteration 458, loss = 0.30087760\n",
      "Iteration 459, loss = 0.30045324\n",
      "Iteration 460, loss = 0.30002975\n",
      "Iteration 461, loss = 0.29960718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 462, loss = 0.29918552\n",
      "Iteration 463, loss = 0.29876474\n",
      "Iteration 464, loss = 0.29834484\n",
      "Iteration 465, loss = 0.29792584\n",
      "Iteration 466, loss = 0.29750772\n",
      "Iteration 467, loss = 0.29709048\n",
      "Iteration 468, loss = 0.29667414\n",
      "Iteration 469, loss = 0.29625869\n",
      "Iteration 470, loss = 0.29584412\n",
      "Iteration 471, loss = 0.29543042\n",
      "Iteration 472, loss = 0.29501758\n",
      "Iteration 473, loss = 0.29460562\n",
      "Iteration 474, loss = 0.29419454\n",
      "Iteration 475, loss = 0.29378433\n",
      "Iteration 476, loss = 0.29337498\n",
      "Iteration 477, loss = 0.29296648\n",
      "Iteration 478, loss = 0.29255886\n",
      "Iteration 479, loss = 0.29215208\n",
      "Iteration 480, loss = 0.29174616\n",
      "Iteration 481, loss = 0.29134109\n",
      "Iteration 482, loss = 0.29093687\n",
      "Iteration 483, loss = 0.29053351\n",
      "Iteration 484, loss = 0.29013097\n",
      "Iteration 485, loss = 0.28972929\n",
      "Iteration 486, loss = 0.28932844\n",
      "Iteration 487, loss = 0.28892843\n",
      "Iteration 488, loss = 0.28852927\n",
      "Iteration 489, loss = 0.28813093\n",
      "Iteration 490, loss = 0.28773345\n",
      "Iteration 491, loss = 0.28733684\n",
      "Iteration 492, loss = 0.28694105\n",
      "Iteration 493, loss = 0.28654613\n",
      "Iteration 494, loss = 0.28615203\n",
      "Iteration 495, loss = 0.28575877\n",
      "Iteration 496, loss = 0.28536636\n",
      "Iteration 497, loss = 0.28497478\n",
      "Iteration 498, loss = 0.28458404\n",
      "Iteration 499, loss = 0.28419411\n",
      "Iteration 500, loss = 0.28380502\n",
      "Iteration 501, loss = 0.28341670\n",
      "Iteration 502, loss = 0.28302917\n",
      "Iteration 503, loss = 0.28264243\n",
      "Iteration 504, loss = 0.28225647\n",
      "Iteration 505, loss = 0.28187131\n",
      "Iteration 506, loss = 0.28148696\n",
      "Iteration 507, loss = 0.28110341\n",
      "Iteration 508, loss = 0.28072066\n",
      "Iteration 509, loss = 0.28033871\n",
      "Iteration 510, loss = 0.27995791\n",
      "Iteration 511, loss = 0.27957806\n",
      "Iteration 512, loss = 0.27919905\n",
      "Iteration 513, loss = 0.27882083\n",
      "Iteration 514, loss = 0.27844344\n",
      "Iteration 515, loss = 0.27806691\n",
      "Iteration 516, loss = 0.27769128\n",
      "Iteration 517, loss = 0.27731650\n",
      "Iteration 518, loss = 0.27694265\n",
      "Iteration 519, loss = 0.27656963\n",
      "Iteration 520, loss = 0.27619758\n",
      "Iteration 521, loss = 0.27582646\n",
      "Iteration 522, loss = 0.27545616\n",
      "Iteration 523, loss = 0.27508665\n",
      "Iteration 524, loss = 0.27471795\n",
      "Iteration 525, loss = 0.27435004\n",
      "Iteration 526, loss = 0.27398294\n",
      "Iteration 527, loss = 0.27361662\n",
      "Iteration 528, loss = 0.27325110\n",
      "Iteration 529, loss = 0.27288636\n",
      "Iteration 530, loss = 0.27252241\n",
      "Iteration 531, loss = 0.27215930\n",
      "Iteration 532, loss = 0.27179700\n",
      "Iteration 533, loss = 0.27143547\n",
      "Iteration 534, loss = 0.27107472\n",
      "Iteration 535, loss = 0.27071474\n",
      "Iteration 536, loss = 0.27035552\n",
      "Iteration 537, loss = 0.26999721\n",
      "Iteration 538, loss = 0.26963971\n",
      "Iteration 539, loss = 0.26928298\n",
      "Iteration 540, loss = 0.26892701\n",
      "Iteration 541, loss = 0.26857180\n",
      "Iteration 542, loss = 0.26821735\n",
      "Iteration 543, loss = 0.26786372\n",
      "Iteration 544, loss = 0.26751085\n",
      "Iteration 545, loss = 0.26715873\n",
      "Iteration 546, loss = 0.26680736\n",
      "Iteration 547, loss = 0.26645674\n",
      "Iteration 548, loss = 0.26610689\n",
      "Iteration 549, loss = 0.26575778\n",
      "Iteration 550, loss = 0.26540946\n",
      "Iteration 551, loss = 0.26506180\n",
      "Iteration 552, loss = 0.26471492\n",
      "Iteration 553, loss = 0.26436877\n",
      "Iteration 554, loss = 0.26402335\n",
      "Iteration 555, loss = 0.26367873\n",
      "Iteration 556, loss = 0.26333484\n",
      "Iteration 557, loss = 0.26299168\n",
      "Iteration 558, loss = 0.26264925\n",
      "Iteration 559, loss = 0.26230753\n",
      "Iteration 560, loss = 0.26196654\n",
      "Iteration 561, loss = 0.26162626\n",
      "Iteration 562, loss = 0.26128666\n",
      "Iteration 563, loss = 0.26094777\n",
      "Iteration 564, loss = 0.26060961\n",
      "Iteration 565, loss = 0.26027216\n",
      "Iteration 566, loss = 0.25993542\n",
      "Iteration 567, loss = 0.25959939\n",
      "Iteration 568, loss = 0.25926407\n",
      "Iteration 569, loss = 0.25892946\n",
      "Iteration 570, loss = 0.25859555\n",
      "Iteration 571, loss = 0.25826233\n",
      "Iteration 572, loss = 0.25792984\n",
      "Iteration 573, loss = 0.25759801\n",
      "Iteration 574, loss = 0.25726690\n",
      "Iteration 575, loss = 0.25693649\n",
      "Iteration 576, loss = 0.25660679\n",
      "Iteration 577, loss = 0.25627780\n",
      "Iteration 578, loss = 0.25594950\n",
      "Iteration 579, loss = 0.25562190\n",
      "Iteration 580, loss = 0.25529498\n",
      "Iteration 581, loss = 0.25496875\n",
      "Iteration 582, loss = 0.25464320\n",
      "Iteration 583, loss = 0.25431833\n",
      "Iteration 584, loss = 0.25399416\n",
      "Iteration 585, loss = 0.25367068\n",
      "Iteration 586, loss = 0.25334788\n",
      "Iteration 587, loss = 0.25302576\n",
      "Iteration 588, loss = 0.25270432\n",
      "Iteration 589, loss = 0.25238355\n",
      "Iteration 590, loss = 0.25206346\n",
      "Iteration 591, loss = 0.25174405\n",
      "Iteration 592, loss = 0.25142536\n",
      "Iteration 593, loss = 0.25110749\n",
      "Iteration 594, loss = 0.25079027\n",
      "Iteration 595, loss = 0.25047371\n",
      "Iteration 596, loss = 0.25015780\n",
      "Iteration 597, loss = 0.24984234\n",
      "Iteration 598, loss = 0.24952747\n",
      "Iteration 599, loss = 0.24921323\n",
      "Iteration 600, loss = 0.24889965\n",
      "Iteration 601, loss = 0.24858677\n",
      "Iteration 602, loss = 0.24827445\n",
      "Iteration 603, loss = 0.24796272\n",
      "Iteration 604, loss = 0.24765167\n",
      "Iteration 605, loss = 0.24734125\n",
      "Iteration 606, loss = 0.24703139\n",
      "Iteration 607, loss = 0.24672220\n",
      "Iteration 608, loss = 0.24641357\n",
      "Iteration 609, loss = 0.24610555\n",
      "Iteration 610, loss = 0.24579811\n",
      "Iteration 611, loss = 0.24549131\n",
      "Iteration 612, loss = 0.24518514\n",
      "Iteration 613, loss = 0.24487953\n",
      "Iteration 614, loss = 0.24457451\n",
      "Iteration 615, loss = 0.24427007\n",
      "Iteration 616, loss = 0.24396628\n",
      "Iteration 617, loss = 0.24366309\n",
      "Iteration 618, loss = 0.24336056\n",
      "Iteration 619, loss = 0.24305862\n",
      "Iteration 620, loss = 0.24275735\n",
      "Iteration 621, loss = 0.24245668\n",
      "Iteration 622, loss = 0.24215664\n",
      "Iteration 623, loss = 0.24185720\n",
      "Iteration 624, loss = 0.24155841\n",
      "Iteration 625, loss = 0.24126017\n",
      "Iteration 626, loss = 0.24096254\n",
      "Iteration 627, loss = 0.24066546\n",
      "Iteration 628, loss = 0.24036896\n",
      "Iteration 629, loss = 0.24007302\n",
      "Iteration 630, loss = 0.23977772\n",
      "Iteration 631, loss = 0.23948303\n",
      "Iteration 632, loss = 0.23918894\n",
      "Iteration 633, loss = 0.23889550\n",
      "Iteration 634, loss = 0.23860258\n",
      "Iteration 635, loss = 0.23831016\n",
      "Iteration 636, loss = 0.23801831\n",
      "Iteration 637, loss = 0.23772707\n",
      "Iteration 638, loss = 0.23743640\n",
      "Iteration 639, loss = 0.23714622\n",
      "Iteration 640, loss = 0.23685649\n",
      "Iteration 641, loss = 0.23656733\n",
      "Iteration 642, loss = 0.23627880\n",
      "Iteration 643, loss = 0.23599083\n",
      "Iteration 644, loss = 0.23570350\n",
      "Iteration 645, loss = 0.23541665\n",
      "Iteration 646, loss = 0.23513036\n",
      "Iteration 647, loss = 0.23484455\n",
      "Iteration 648, loss = 0.23455926\n",
      "Iteration 649, loss = 0.23427452\n",
      "Iteration 650, loss = 0.23399027\n",
      "Iteration 651, loss = 0.23370659\n",
      "Iteration 652, loss = 0.23342349\n",
      "Iteration 653, loss = 0.23314097\n",
      "Iteration 654, loss = 0.23285907\n",
      "Iteration 655, loss = 0.23257773\n",
      "Iteration 656, loss = 0.23229693\n",
      "Iteration 657, loss = 0.23201665\n",
      "Iteration 658, loss = 0.23173690\n",
      "Iteration 659, loss = 0.23145778\n",
      "Iteration 660, loss = 0.23117923\n",
      "Iteration 661, loss = 0.23090127\n",
      "Iteration 662, loss = 0.23062382\n",
      "Iteration 663, loss = 0.23034679\n",
      "Iteration 664, loss = 0.23007036\n",
      "Iteration 665, loss = 0.22979450\n",
      "Iteration 666, loss = 0.22951921\n",
      "Iteration 667, loss = 0.22924450\n",
      "Iteration 668, loss = 0.22897025\n",
      "Iteration 669, loss = 0.22869631\n",
      "Iteration 670, loss = 0.22842297\n",
      "Iteration 671, loss = 0.22815008\n",
      "Iteration 672, loss = 0.22787763\n",
      "Iteration 673, loss = 0.22760575\n",
      "Iteration 674, loss = 0.22733441\n",
      "Iteration 675, loss = 0.22706346\n",
      "Iteration 676, loss = 0.22679304\n",
      "Iteration 677, loss = 0.22652312\n",
      "Iteration 678, loss = 0.22625377\n",
      "Iteration 679, loss = 0.22598495\n",
      "Iteration 680, loss = 0.22571667\n",
      "Iteration 681, loss = 0.22544894\n",
      "Iteration 682, loss = 0.22518181\n",
      "Iteration 683, loss = 0.22491504\n",
      "Iteration 684, loss = 0.22464857\n",
      "Iteration 685, loss = 0.22438252\n",
      "Iteration 686, loss = 0.22411691\n",
      "Iteration 687, loss = 0.22385186\n",
      "Iteration 688, loss = 0.22358726\n",
      "Iteration 689, loss = 0.22332295\n",
      "Iteration 690, loss = 0.22305898\n",
      "Iteration 691, loss = 0.22279554\n",
      "Iteration 692, loss = 0.22253259\n",
      "Iteration 693, loss = 0.22227004\n",
      "Iteration 694, loss = 0.22200790\n",
      "Iteration 695, loss = 0.22174611\n",
      "Iteration 696, loss = 0.22148482\n",
      "Iteration 697, loss = 0.22122405\n",
      "Iteration 698, loss = 0.22096379\n",
      "Iteration 699, loss = 0.22070386\n",
      "Iteration 700, loss = 0.22044429\n",
      "Iteration 701, loss = 0.22018522\n",
      "Iteration 702, loss = 0.21992666\n",
      "Iteration 703, loss = 0.21966862\n",
      "Iteration 704, loss = 0.21941113\n",
      "Iteration 705, loss = 0.21915413\n",
      "Iteration 706, loss = 0.21889768\n",
      "Iteration 707, loss = 0.21864180\n",
      "Iteration 708, loss = 0.21838643\n",
      "Iteration 709, loss = 0.21813160\n",
      "Iteration 710, loss = 0.21787733\n",
      "Iteration 711, loss = 0.21762335\n",
      "Iteration 712, loss = 0.21736956\n",
      "Iteration 713, loss = 0.21711616\n",
      "Iteration 714, loss = 0.21686328\n",
      "Iteration 715, loss = 0.21661090\n",
      "Iteration 716, loss = 0.21635904\n",
      "Iteration 717, loss = 0.21610770\n",
      "Iteration 718, loss = 0.21585691\n",
      "Iteration 719, loss = 0.21560663\n",
      "Iteration 720, loss = 0.21535688\n",
      "Iteration 721, loss = 0.21510766\n",
      "Iteration 722, loss = 0.21485900\n",
      "Iteration 723, loss = 0.21461085\n",
      "Iteration 724, loss = 0.21436324\n",
      "Iteration 725, loss = 0.21411617\n",
      "Iteration 726, loss = 0.21386965\n",
      "Iteration 727, loss = 0.21362368\n",
      "Iteration 728, loss = 0.21337826\n",
      "Iteration 729, loss = 0.21313337\n",
      "Iteration 730, loss = 0.21288903\n",
      "Iteration 731, loss = 0.21264519\n",
      "Iteration 732, loss = 0.21240131\n",
      "Iteration 733, loss = 0.21215792\n",
      "Iteration 734, loss = 0.21191505\n",
      "Iteration 735, loss = 0.21167267\n",
      "Iteration 736, loss = 0.21143080\n",
      "Iteration 737, loss = 0.21118929\n",
      "Iteration 738, loss = 0.21094824\n",
      "Iteration 739, loss = 0.21070770\n",
      "Iteration 740, loss = 0.21046767\n",
      "Iteration 741, loss = 0.21022816\n",
      "Iteration 742, loss = 0.20998915\n",
      "Iteration 743, loss = 0.20975067\n",
      "Iteration 744, loss = 0.20951269\n",
      "Iteration 745, loss = 0.20927524\n",
      "Iteration 746, loss = 0.20903830\n",
      "Iteration 747, loss = 0.20880189\n",
      "Iteration 748, loss = 0.20856600\n",
      "Iteration 749, loss = 0.20833065\n",
      "Iteration 750, loss = 0.20809583\n",
      "Iteration 751, loss = 0.20786153\n",
      "Iteration 752, loss = 0.20762777\n",
      "Iteration 753, loss = 0.20739453\n",
      "Iteration 754, loss = 0.20716179\n",
      "Iteration 755, loss = 0.20692958\n",
      "Iteration 756, loss = 0.20669778\n",
      "Iteration 757, loss = 0.20646645\n",
      "Iteration 758, loss = 0.20623563\n",
      "Iteration 759, loss = 0.20600533\n",
      "Iteration 760, loss = 0.20577554\n",
      "Iteration 761, loss = 0.20554627\n",
      "Iteration 762, loss = 0.20531751\n",
      "Iteration 763, loss = 0.20508928\n",
      "Iteration 764, loss = 0.20486150\n",
      "Iteration 765, loss = 0.20463414\n",
      "Iteration 766, loss = 0.20440727\n",
      "Iteration 767, loss = 0.20418090\n",
      "Iteration 768, loss = 0.20395502\n",
      "Iteration 769, loss = 0.20372964\n",
      "Iteration 770, loss = 0.20350476\n",
      "Iteration 771, loss = 0.20328037\n",
      "Iteration 772, loss = 0.20305648\n",
      "Iteration 773, loss = 0.20283301\n",
      "Iteration 774, loss = 0.20260995\n",
      "Iteration 775, loss = 0.20238732\n",
      "Iteration 776, loss = 0.20216518\n",
      "Iteration 777, loss = 0.20194352\n",
      "Iteration 778, loss = 0.20172233\n",
      "Iteration 779, loss = 0.20150163\n",
      "Iteration 780, loss = 0.20128141\n",
      "Iteration 781, loss = 0.20106176\n",
      "Iteration 782, loss = 0.20084246\n",
      "Iteration 783, loss = 0.20062372\n",
      "Iteration 784, loss = 0.20040546\n",
      "Iteration 785, loss = 0.20018760\n",
      "Iteration 786, loss = 0.19997026\n",
      "Iteration 787, loss = 0.19975337\n",
      "Iteration 788, loss = 0.19953695\n",
      "Iteration 789, loss = 0.19932100\n",
      "Iteration 790, loss = 0.19910554\n",
      "Iteration 791, loss = 0.19889053\n",
      "Iteration 792, loss = 0.19867599\n",
      "Iteration 793, loss = 0.19846184\n",
      "Iteration 794, loss = 0.19824813\n",
      "Iteration 795, loss = 0.19803488\n",
      "Iteration 796, loss = 0.19782212\n",
      "Iteration 797, loss = 0.19760973\n",
      "Iteration 798, loss = 0.19739777\n",
      "Iteration 799, loss = 0.19718611\n",
      "Iteration 800, loss = 0.19697486\n",
      "Iteration 801, loss = 0.19676406\n",
      "Iteration 802, loss = 0.19655362\n",
      "Iteration 803, loss = 0.19634360\n",
      "Iteration 804, loss = 0.19613403\n",
      "Iteration 805, loss = 0.19592489\n",
      "Iteration 806, loss = 0.19571622\n",
      "Iteration 807, loss = 0.19550797\n",
      "Iteration 808, loss = 0.19530019\n",
      "Iteration 809, loss = 0.19509286\n",
      "Iteration 810, loss = 0.19488598\n",
      "Iteration 811, loss = 0.19467956\n",
      "Iteration 812, loss = 0.19447358\n",
      "Iteration 813, loss = 0.19426804\n",
      "Iteration 814, loss = 0.19406296\n",
      "Iteration 815, loss = 0.19385826\n",
      "Iteration 816, loss = 0.19365391\n",
      "Iteration 817, loss = 0.19344997\n",
      "Iteration 818, loss = 0.19324647\n",
      "Iteration 819, loss = 0.19304341\n",
      "Iteration 820, loss = 0.19284078\n",
      "Iteration 821, loss = 0.19263859\n",
      "Iteration 822, loss = 0.19243681\n",
      "Iteration 823, loss = 0.19223515\n",
      "Iteration 824, loss = 0.19203388\n",
      "Iteration 825, loss = 0.19183303\n",
      "Iteration 826, loss = 0.19163258\n",
      "Iteration 827, loss = 0.19143243\n",
      "Iteration 828, loss = 0.19123253\n",
      "Iteration 829, loss = 0.19103300\n",
      "Iteration 830, loss = 0.19083388\n",
      "Iteration 831, loss = 0.19063515\n",
      "Iteration 832, loss = 0.19043683\n",
      "Iteration 833, loss = 0.19023891\n",
      "Iteration 834, loss = 0.19004135\n",
      "Iteration 835, loss = 0.18984405\n",
      "Iteration 836, loss = 0.18964718\n",
      "Iteration 837, loss = 0.18945064\n",
      "Iteration 838, loss = 0.18925437\n",
      "Iteration 839, loss = 0.18905852\n",
      "Iteration 840, loss = 0.18886298\n",
      "Iteration 841, loss = 0.18866776\n",
      "Iteration 842, loss = 0.18847288\n",
      "Iteration 843, loss = 0.18827838\n",
      "Iteration 844, loss = 0.18808427\n",
      "Iteration 845, loss = 0.18789053\n",
      "Iteration 846, loss = 0.18769713\n",
      "Iteration 847, loss = 0.18750415\n",
      "Iteration 848, loss = 0.18731145\n",
      "Iteration 849, loss = 0.18711894\n",
      "Iteration 850, loss = 0.18692681\n",
      "Iteration 851, loss = 0.18673503\n",
      "Iteration 852, loss = 0.18654358\n",
      "Iteration 853, loss = 0.18635236\n",
      "Iteration 854, loss = 0.18616136\n",
      "Iteration 855, loss = 0.18597075\n",
      "Iteration 856, loss = 0.18578049\n",
      "Iteration 857, loss = 0.18559061\n",
      "Iteration 858, loss = 0.18540111\n",
      "Iteration 859, loss = 0.18521199\n",
      "Iteration 860, loss = 0.18502326\n",
      "Iteration 861, loss = 0.18483493\n",
      "Iteration 862, loss = 0.18464698\n",
      "Iteration 863, loss = 0.18445942\n",
      "Iteration 864, loss = 0.18427222\n",
      "Iteration 865, loss = 0.18408531\n",
      "Iteration 866, loss = 0.18389880\n",
      "Iteration 867, loss = 0.18371267\n",
      "Iteration 868, loss = 0.18352693\n",
      "Iteration 869, loss = 0.18334151\n",
      "Iteration 870, loss = 0.18315650\n",
      "Iteration 871, loss = 0.18297270\n",
      "Iteration 872, loss = 0.18278922\n",
      "Iteration 873, loss = 0.18260616\n",
      "Iteration 874, loss = 0.18242353\n",
      "Iteration 875, loss = 0.18224132\n",
      "Iteration 876, loss = 0.18205955\n",
      "Iteration 877, loss = 0.18187819\n",
      "Iteration 878, loss = 0.18169715\n",
      "Iteration 879, loss = 0.18151624\n",
      "Iteration 880, loss = 0.18133567\n",
      "Iteration 881, loss = 0.18115538\n",
      "Iteration 882, loss = 0.18097538\n",
      "Iteration 883, loss = 0.18079575\n",
      "Iteration 884, loss = 0.18061649\n",
      "Iteration 885, loss = 0.18043760\n",
      "Iteration 886, loss = 0.18025911\n",
      "Iteration 887, loss = 0.18008099\n",
      "Iteration 888, loss = 0.17990323\n",
      "Iteration 889, loss = 0.17972582\n",
      "Iteration 890, loss = 0.17954875\n",
      "Iteration 891, loss = 0.17937205\n",
      "Iteration 892, loss = 0.17919575\n",
      "Iteration 893, loss = 0.17901977\n",
      "Iteration 894, loss = 0.17884416\n",
      "Iteration 895, loss = 0.17866892\n",
      "Iteration 896, loss = 0.17849407\n",
      "Iteration 897, loss = 0.17831958\n",
      "Iteration 898, loss = 0.17814548\n",
      "Iteration 899, loss = 0.17797175\n",
      "Iteration 900, loss = 0.17779838\n",
      "Iteration 901, loss = 0.17762538\n",
      "Iteration 902, loss = 0.17745278\n",
      "Iteration 903, loss = 0.17728073\n",
      "Iteration 904, loss = 0.17710943\n",
      "Iteration 905, loss = 0.17693856\n",
      "Iteration 906, loss = 0.17676918\n",
      "Iteration 907, loss = 0.17660028\n",
      "Iteration 908, loss = 0.17643174\n",
      "Iteration 909, loss = 0.17626363\n",
      "Iteration 910, loss = 0.17609592\n",
      "Iteration 911, loss = 0.17592862\n",
      "Iteration 912, loss = 0.17576164\n",
      "Iteration 913, loss = 0.17559498\n",
      "Iteration 914, loss = 0.17542871\n",
      "Iteration 915, loss = 0.17526280\n",
      "Iteration 916, loss = 0.17509728\n",
      "Iteration 917, loss = 0.17493213\n",
      "Iteration 918, loss = 0.17476735\n",
      "Iteration 919, loss = 0.17460293\n",
      "Iteration 920, loss = 0.17443891\n",
      "Iteration 921, loss = 0.17427517\n",
      "Iteration 922, loss = 0.17411182\n",
      "Iteration 923, loss = 0.17394882\n",
      "Iteration 924, loss = 0.17378616\n",
      "Iteration 925, loss = 0.17362387\n",
      "Iteration 926, loss = 0.17346224\n",
      "Iteration 927, loss = 0.17330113\n",
      "Iteration 928, loss = 0.17314038\n",
      "Iteration 929, loss = 0.17297998\n",
      "Iteration 930, loss = 0.17281994\n",
      "Iteration 931, loss = 0.17266025\n",
      "Iteration 932, loss = 0.17250090\n",
      "Iteration 933, loss = 0.17234194\n",
      "Iteration 934, loss = 0.17218347\n",
      "Iteration 935, loss = 0.17202552\n",
      "Iteration 936, loss = 0.17186792\n",
      "Iteration 937, loss = 0.17171066\n",
      "Iteration 938, loss = 0.17155374\n",
      "Iteration 939, loss = 0.17139719\n",
      "Iteration 940, loss = 0.17124097\n",
      "Iteration 941, loss = 0.17108509\n",
      "Iteration 942, loss = 0.17092956\n",
      "Iteration 943, loss = 0.17077436\n",
      "Iteration 944, loss = 0.17061950\n",
      "Iteration 945, loss = 0.17046496\n",
      "Iteration 946, loss = 0.17031074\n",
      "Iteration 947, loss = 0.17015685\n",
      "Iteration 948, loss = 0.17000328\n",
      "Iteration 949, loss = 0.16985004\n",
      "Iteration 950, loss = 0.16969710\n",
      "Iteration 951, loss = 0.16954448\n",
      "Iteration 952, loss = 0.16939218\n",
      "Iteration 953, loss = 0.16924018\n",
      "Iteration 954, loss = 0.16908850\n",
      "Iteration 955, loss = 0.16893713\n",
      "Iteration 956, loss = 0.16878608\n",
      "Iteration 957, loss = 0.16863534\n",
      "Iteration 958, loss = 0.16848493\n",
      "Iteration 959, loss = 0.16833479\n",
      "Iteration 960, loss = 0.16818498\n",
      "Iteration 961, loss = 0.16803546\n",
      "Iteration 962, loss = 0.16788625\n",
      "Iteration 963, loss = 0.16773733\n",
      "Iteration 964, loss = 0.16758872\n",
      "Iteration 965, loss = 0.16744042\n",
      "Iteration 966, loss = 0.16729244\n",
      "Iteration 967, loss = 0.16714475\n",
      "Iteration 968, loss = 0.16699737\n",
      "Iteration 969, loss = 0.16685028\n",
      "Iteration 970, loss = 0.16670349\n",
      "Iteration 971, loss = 0.16655699\n",
      "Iteration 972, loss = 0.16641079\n",
      "Iteration 973, loss = 0.16626489\n",
      "Iteration 974, loss = 0.16611927\n",
      "Iteration 975, loss = 0.16597395\n",
      "Iteration 976, loss = 0.16582893\n",
      "Iteration 977, loss = 0.16568419\n",
      "Iteration 978, loss = 0.16553975\n",
      "Iteration 979, loss = 0.16539560\n",
      "Iteration 980, loss = 0.16525174\n",
      "Iteration 981, loss = 0.16510818\n",
      "Iteration 982, loss = 0.16496511\n",
      "Iteration 983, loss = 0.16482223\n",
      "Iteration 984, loss = 0.16467971\n",
      "Iteration 985, loss = 0.16453745\n",
      "Iteration 986, loss = 0.16439547\n",
      "Iteration 987, loss = 0.16425384\n",
      "Iteration 988, loss = 0.16411241\n",
      "Iteration 989, loss = 0.16397127\n",
      "Iteration 990, loss = 0.16383041\n",
      "Iteration 991, loss = 0.16368994\n",
      "Iteration 992, loss = 0.16354958\n",
      "Iteration 993, loss = 0.16340956\n",
      "Iteration 994, loss = 0.16326986\n",
      "Iteration 995, loss = 0.16313042\n",
      "Iteration 996, loss = 0.16299125\n",
      "Iteration 997, loss = 0.16285234\n",
      "Iteration 998, loss = 0.16271369\n",
      "Iteration 999, loss = 0.16257545\n",
      "Iteration 1000, loss = 0.16243730\n",
      "Iteration 1, loss = 1.66075849\n",
      "Iteration 2, loss = 1.62388596\n",
      "Iteration 3, loss = 1.57434597\n",
      "Iteration 4, loss = 1.51652159\n",
      "Iteration 5, loss = 1.45485615\n",
      "Iteration 6, loss = 1.39392087\n",
      "Iteration 7, loss = 1.33762662\n",
      "Iteration 8, loss = 1.28919141\n",
      "Iteration 9, loss = 1.25059449\n",
      "Iteration 10, loss = 1.22200674\n",
      "Iteration 11, loss = 1.20197852\n",
      "Iteration 12, loss = 1.18788886\n",
      "Iteration 13, loss = 1.17669170\n",
      "Iteration 14, loss = 1.16574471\n",
      "Iteration 15, loss = 1.15300393\n",
      "Iteration 16, loss = 1.13744268\n",
      "Iteration 17, loss = 1.11901001\n",
      "Iteration 18, loss = 1.09866789\n",
      "Iteration 19, loss = 1.07688439\n",
      "Iteration 20, loss = 1.05470220\n",
      "Iteration 21, loss = 1.03305136\n",
      "Iteration 22, loss = 1.01266515\n",
      "Iteration 23, loss = 0.99413781\n",
      "Iteration 24, loss = 0.97754961\n",
      "Iteration 25, loss = 0.96282171\n",
      "Iteration 26, loss = 0.94958846\n",
      "Iteration 27, loss = 0.93758139\n",
      "Iteration 28, loss = 0.92638356\n",
      "Iteration 29, loss = 0.91567730\n",
      "Iteration 30, loss = 0.90527386\n",
      "Iteration 31, loss = 0.89504738\n",
      "Iteration 32, loss = 0.88503865\n",
      "Iteration 33, loss = 0.87534103\n",
      "Iteration 34, loss = 0.86606897\n",
      "Iteration 35, loss = 0.85726890\n",
      "Iteration 36, loss = 0.84881851\n",
      "Iteration 37, loss = 0.84066406\n",
      "Iteration 38, loss = 0.83284230\n",
      "Iteration 39, loss = 0.82537415\n",
      "Iteration 40, loss = 0.81822893\n",
      "Iteration 41, loss = 0.81135424\n",
      "Iteration 42, loss = 0.80468422\n",
      "Iteration 43, loss = 0.79821176\n",
      "Iteration 44, loss = 0.79190750\n",
      "Iteration 45, loss = 0.78576647\n",
      "Iteration 46, loss = 0.77976610\n",
      "Iteration 47, loss = 0.77387221\n",
      "Iteration 48, loss = 0.76808781\n",
      "Iteration 49, loss = 0.76241777\n",
      "Iteration 50, loss = 0.75686946\n",
      "Iteration 51, loss = 0.75145200\n",
      "Iteration 52, loss = 0.74617179\n",
      "Iteration 53, loss = 0.74102654\n",
      "Iteration 54, loss = 0.73601291\n",
      "Iteration 55, loss = 0.73112303\n",
      "Iteration 56, loss = 0.72634884\n",
      "Iteration 57, loss = 0.72168226\n",
      "Iteration 58, loss = 0.71711613\n",
      "Iteration 59, loss = 0.71264486\n",
      "Iteration 60, loss = 0.70826467\n",
      "Iteration 61, loss = 0.70397281\n",
      "Iteration 62, loss = 0.69976757\n",
      "Iteration 63, loss = 0.69564778\n",
      "Iteration 64, loss = 0.69161407\n",
      "Iteration 65, loss = 0.68766380\n",
      "Iteration 66, loss = 0.68379564\n",
      "Iteration 67, loss = 0.68000731\n",
      "Iteration 68, loss = 0.67629442\n",
      "Iteration 69, loss = 0.67265737\n",
      "Iteration 70, loss = 0.66909244\n",
      "Iteration 71, loss = 0.66559681\n",
      "Iteration 72, loss = 0.66216598\n",
      "Iteration 73, loss = 0.65879859\n",
      "Iteration 74, loss = 0.65549453\n",
      "Iteration 75, loss = 0.65225207\n",
      "Iteration 76, loss = 0.64906687\n",
      "Iteration 77, loss = 0.64593804\n",
      "Iteration 78, loss = 0.64286508\n",
      "Iteration 79, loss = 0.63984658\n",
      "Iteration 80, loss = 0.63688115\n",
      "Iteration 81, loss = 0.63396513\n",
      "Iteration 82, loss = 0.63109808\n",
      "Iteration 83, loss = 0.62827891\n",
      "Iteration 84, loss = 0.62550601\n",
      "Iteration 85, loss = 0.62277950\n",
      "Iteration 86, loss = 0.62009857\n",
      "Iteration 87, loss = 0.61746215\n",
      "Iteration 88, loss = 0.61486583\n",
      "Iteration 89, loss = 0.61230601\n",
      "Iteration 90, loss = 0.60977871\n",
      "Iteration 91, loss = 0.60728919\n",
      "Iteration 92, loss = 0.60483815\n",
      "Iteration 93, loss = 0.60242469\n",
      "Iteration 94, loss = 0.60004573\n",
      "Iteration 95, loss = 0.59769995\n",
      "Iteration 96, loss = 0.59538755\n",
      "Iteration 97, loss = 0.59310831\n",
      "Iteration 98, loss = 0.59086110\n",
      "Iteration 99, loss = 0.58864671\n",
      "Iteration 100, loss = 0.58646382\n",
      "Iteration 101, loss = 0.58431184\n",
      "Iteration 102, loss = 0.58218826\n",
      "Iteration 103, loss = 0.58009333\n",
      "Iteration 104, loss = 0.57802543\n",
      "Iteration 105, loss = 0.57598433\n",
      "Iteration 106, loss = 0.57397210\n",
      "Iteration 107, loss = 0.57198652\n",
      "Iteration 108, loss = 0.57002847\n",
      "Iteration 109, loss = 0.56809647\n",
      "Iteration 110, loss = 0.56618844\n",
      "Iteration 111, loss = 0.56430495\n",
      "Iteration 112, loss = 0.56244058\n",
      "Iteration 113, loss = 0.56059714\n",
      "Iteration 114, loss = 0.55877675\n",
      "Iteration 115, loss = 0.55698246\n",
      "Iteration 116, loss = 0.55520687\n",
      "Iteration 117, loss = 0.55344732\n",
      "Iteration 118, loss = 0.55170693\n",
      "Iteration 119, loss = 0.54998810\n",
      "Iteration 120, loss = 0.54827901\n",
      "Iteration 121, loss = 0.54659049\n",
      "Iteration 122, loss = 0.54492043\n",
      "Iteration 123, loss = 0.54326629\n",
      "Iteration 124, loss = 0.54162510\n",
      "Iteration 125, loss = 0.53999864\n",
      "Iteration 126, loss = 0.53838432\n",
      "Iteration 127, loss = 0.53679324\n",
      "Iteration 128, loss = 0.53522170\n",
      "Iteration 129, loss = 0.53366660\n",
      "Iteration 130, loss = 0.53213036\n",
      "Iteration 131, loss = 0.53061516\n",
      "Iteration 132, loss = 0.52912059\n",
      "Iteration 133, loss = 0.52765639\n",
      "Iteration 134, loss = 0.52621776\n",
      "Iteration 135, loss = 0.52479634\n",
      "Iteration 136, loss = 0.52339530\n",
      "Iteration 137, loss = 0.52201447\n",
      "Iteration 138, loss = 0.52065154\n",
      "Iteration 139, loss = 0.51930866\n",
      "Iteration 140, loss = 0.51798445\n",
      "Iteration 141, loss = 0.51667485\n",
      "Iteration 142, loss = 0.51538137\n",
      "Iteration 143, loss = 0.51410476\n",
      "Iteration 144, loss = 0.51284247\n",
      "Iteration 145, loss = 0.51159338\n",
      "Iteration 146, loss = 0.51035741\n",
      "Iteration 147, loss = 0.50913373\n",
      "Iteration 148, loss = 0.50792160\n",
      "Iteration 149, loss = 0.50672130\n",
      "Iteration 150, loss = 0.50553154\n",
      "Iteration 151, loss = 0.50435169\n",
      "Iteration 152, loss = 0.50318161\n",
      "Iteration 153, loss = 0.50202140\n",
      "Iteration 154, loss = 0.50087069\n",
      "Iteration 155, loss = 0.49972933\n",
      "Iteration 156, loss = 0.49859722\n",
      "Iteration 157, loss = 0.49747481\n",
      "Iteration 158, loss = 0.49636137\n",
      "Iteration 159, loss = 0.49525680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 160, loss = 0.49416128\n",
      "Iteration 161, loss = 0.49307507\n",
      "Iteration 162, loss = 0.49199808\n",
      "Iteration 163, loss = 0.49092967\n",
      "Iteration 164, loss = 0.48986968\n",
      "Iteration 165, loss = 0.48881855\n",
      "Iteration 166, loss = 0.48777556\n",
      "Iteration 167, loss = 0.48674043\n",
      "Iteration 168, loss = 0.48571312\n",
      "Iteration 169, loss = 0.48469403\n",
      "Iteration 170, loss = 0.48368259\n",
      "Iteration 171, loss = 0.48267859\n",
      "Iteration 172, loss = 0.48168182\n",
      "Iteration 173, loss = 0.48069216\n",
      "Iteration 174, loss = 0.47970947\n",
      "Iteration 175, loss = 0.47873372\n",
      "Iteration 176, loss = 0.47776525\n",
      "Iteration 177, loss = 0.47680317\n",
      "Iteration 178, loss = 0.47584756\n",
      "Iteration 179, loss = 0.47489813\n",
      "Iteration 180, loss = 0.47395483\n",
      "Iteration 181, loss = 0.47301801\n",
      "Iteration 182, loss = 0.47208751\n",
      "Iteration 183, loss = 0.47116294\n",
      "Iteration 184, loss = 0.47024421\n",
      "Iteration 185, loss = 0.46933136\n",
      "Iteration 186, loss = 0.46842401\n",
      "Iteration 187, loss = 0.46752241\n",
      "Iteration 188, loss = 0.46662628\n",
      "Iteration 189, loss = 0.46573583\n",
      "Iteration 190, loss = 0.46485063\n",
      "Iteration 191, loss = 0.46397080\n",
      "Iteration 192, loss = 0.46309626\n",
      "Iteration 193, loss = 0.46222697\n",
      "Iteration 194, loss = 0.46136267\n",
      "Iteration 195, loss = 0.46050352\n",
      "Iteration 196, loss = 0.45964913\n",
      "Iteration 197, loss = 0.45879972\n",
      "Iteration 198, loss = 0.45795515\n",
      "Iteration 199, loss = 0.45711534\n",
      "Iteration 200, loss = 0.45628025\n",
      "Iteration 201, loss = 0.45544960\n",
      "Iteration 202, loss = 0.45462358\n",
      "Iteration 203, loss = 0.45380149\n",
      "Iteration 204, loss = 0.45298390\n",
      "Iteration 205, loss = 0.45217052\n",
      "Iteration 206, loss = 0.45136139\n",
      "Iteration 207, loss = 0.45055687\n",
      "Iteration 208, loss = 0.44975619\n",
      "Iteration 209, loss = 0.44895961\n",
      "Iteration 210, loss = 0.44816693\n",
      "Iteration 211, loss = 0.44737782\n",
      "Iteration 212, loss = 0.44659254\n",
      "Iteration 213, loss = 0.44581085\n",
      "Iteration 214, loss = 0.44503296\n",
      "Iteration 215, loss = 0.44425892\n",
      "Iteration 216, loss = 0.44348863\n",
      "Iteration 217, loss = 0.44272182\n",
      "Iteration 218, loss = 0.44195861\n",
      "Iteration 219, loss = 0.44119824\n",
      "Iteration 220, loss = 0.44044103\n",
      "Iteration 221, loss = 0.43968714\n",
      "Iteration 222, loss = 0.43893638\n",
      "Iteration 223, loss = 0.43818863\n",
      "Iteration 224, loss = 0.43744373\n",
      "Iteration 225, loss = 0.43670139\n",
      "Iteration 226, loss = 0.43596185\n",
      "Iteration 227, loss = 0.43522551\n",
      "Iteration 228, loss = 0.43449176\n",
      "Iteration 229, loss = 0.43376094\n",
      "Iteration 230, loss = 0.43303396\n",
      "Iteration 231, loss = 0.43230967\n",
      "Iteration 232, loss = 0.43158818\n",
      "Iteration 233, loss = 0.43086926\n",
      "Iteration 234, loss = 0.43015210\n",
      "Iteration 235, loss = 0.42943772\n",
      "Iteration 236, loss = 0.42872549\n",
      "Iteration 237, loss = 0.42801581\n",
      "Iteration 238, loss = 0.42730897\n",
      "Iteration 239, loss = 0.42660474\n",
      "Iteration 240, loss = 0.42590309\n",
      "Iteration 241, loss = 0.42520388\n",
      "Iteration 242, loss = 0.42450708\n",
      "Iteration 243, loss = 0.42381207\n",
      "Iteration 244, loss = 0.42311970\n",
      "Iteration 245, loss = 0.42242953\n",
      "Iteration 246, loss = 0.42174076\n",
      "Iteration 247, loss = 0.42105398\n",
      "Iteration 248, loss = 0.42036629\n",
      "Iteration 249, loss = 0.41967899\n",
      "Iteration 250, loss = 0.41899448\n",
      "Iteration 251, loss = 0.41831194\n",
      "Iteration 252, loss = 0.41763174\n",
      "Iteration 253, loss = 0.41695344\n",
      "Iteration 254, loss = 0.41627726\n",
      "Iteration 255, loss = 0.41560388\n",
      "Iteration 256, loss = 0.41493407\n",
      "Iteration 257, loss = 0.41426711\n",
      "Iteration 258, loss = 0.41360217\n",
      "Iteration 259, loss = 0.41293944\n",
      "Iteration 260, loss = 0.41227893\n",
      "Iteration 261, loss = 0.41162113\n",
      "Iteration 262, loss = 0.41096525\n",
      "Iteration 263, loss = 0.41030691\n",
      "Iteration 264, loss = 0.40964857\n",
      "Iteration 265, loss = 0.40898574\n",
      "Iteration 266, loss = 0.40831942\n",
      "Iteration 267, loss = 0.40764717\n",
      "Iteration 268, loss = 0.40697233\n",
      "Iteration 269, loss = 0.40628932\n",
      "Iteration 270, loss = 0.40560366\n",
      "Iteration 271, loss = 0.40491614\n",
      "Iteration 272, loss = 0.40421801\n",
      "Iteration 273, loss = 0.40351260\n",
      "Iteration 274, loss = 0.40279469\n",
      "Iteration 275, loss = 0.40206688\n",
      "Iteration 276, loss = 0.40132111\n",
      "Iteration 277, loss = 0.40056318\n",
      "Iteration 278, loss = 0.39978670\n",
      "Iteration 279, loss = 0.39898941\n",
      "Iteration 280, loss = 0.39818121\n",
      "Iteration 281, loss = 0.39737298\n",
      "Iteration 282, loss = 0.39656732\n",
      "Iteration 283, loss = 0.39575804\n",
      "Iteration 284, loss = 0.39492105\n",
      "Iteration 285, loss = 0.39409101\n",
      "Iteration 286, loss = 0.39329334\n",
      "Iteration 287, loss = 0.39253069\n",
      "Iteration 288, loss = 0.39178893\n",
      "Iteration 289, loss = 0.39106766\n",
      "Iteration 290, loss = 0.39035648\n",
      "Iteration 291, loss = 0.38966845\n",
      "Iteration 292, loss = 0.38900266\n",
      "Iteration 293, loss = 0.38834650\n",
      "Iteration 294, loss = 0.38769999\n",
      "Iteration 295, loss = 0.38706637\n",
      "Iteration 296, loss = 0.38643886\n",
      "Iteration 297, loss = 0.38582411\n",
      "Iteration 298, loss = 0.38521939\n",
      "Iteration 299, loss = 0.38462117\n",
      "Iteration 300, loss = 0.38402811\n",
      "Iteration 301, loss = 0.38343837\n",
      "Iteration 302, loss = 0.38285490\n",
      "Iteration 303, loss = 0.38228149\n",
      "Iteration 304, loss = 0.38171306\n",
      "Iteration 305, loss = 0.38115054\n",
      "Iteration 306, loss = 0.38059023\n",
      "Iteration 307, loss = 0.38003219\n",
      "Iteration 308, loss = 0.37947686\n",
      "Iteration 309, loss = 0.37892516\n",
      "Iteration 310, loss = 0.37837552\n",
      "Iteration 311, loss = 0.37782806\n",
      "Iteration 312, loss = 0.37728328\n",
      "Iteration 313, loss = 0.37674145\n",
      "Iteration 314, loss = 0.37620124\n",
      "Iteration 315, loss = 0.37566271\n",
      "Iteration 316, loss = 0.37512578\n",
      "Iteration 317, loss = 0.37459014\n",
      "Iteration 318, loss = 0.37405618\n",
      "Iteration 319, loss = 0.37352378\n",
      "Iteration 320, loss = 0.37299292\n",
      "Iteration 321, loss = 0.37246344\n",
      "Iteration 322, loss = 0.37193544\n",
      "Iteration 323, loss = 0.37140898\n",
      "Iteration 324, loss = 0.37088407\n",
      "Iteration 325, loss = 0.37036062\n",
      "Iteration 326, loss = 0.36983858\n",
      "Iteration 327, loss = 0.36931798\n",
      "Iteration 328, loss = 0.36879885\n",
      "Iteration 329, loss = 0.36828113\n",
      "Iteration 330, loss = 0.36776484\n",
      "Iteration 331, loss = 0.36725009\n",
      "Iteration 332, loss = 0.36673673\n",
      "Iteration 333, loss = 0.36622467\n",
      "Iteration 334, loss = 0.36571387\n",
      "Iteration 335, loss = 0.36520481\n",
      "Iteration 336, loss = 0.36469706\n",
      "Iteration 337, loss = 0.36419054\n",
      "Iteration 338, loss = 0.36368531\n",
      "Iteration 339, loss = 0.36318125\n",
      "Iteration 340, loss = 0.36267848\n",
      "Iteration 341, loss = 0.36217709\n",
      "Iteration 342, loss = 0.36167703\n",
      "Iteration 343, loss = 0.36117829\n",
      "Iteration 344, loss = 0.36068079\n",
      "Iteration 345, loss = 0.36018460\n",
      "Iteration 346, loss = 0.35968959\n",
      "Iteration 347, loss = 0.35919586\n",
      "Iteration 348, loss = 0.35870340\n",
      "Iteration 349, loss = 0.35821209\n",
      "Iteration 350, loss = 0.35772184\n",
      "Iteration 351, loss = 0.35723279\n",
      "Iteration 352, loss = 0.35674505\n",
      "Iteration 353, loss = 0.35625858\n",
      "Iteration 354, loss = 0.35577341\n",
      "Iteration 355, loss = 0.35528940\n",
      "Iteration 356, loss = 0.35480656\n",
      "Iteration 357, loss = 0.35432472\n",
      "Iteration 358, loss = 0.35384408\n",
      "Iteration 359, loss = 0.35336456\n",
      "Iteration 360, loss = 0.35288616\n",
      "Iteration 361, loss = 0.35240891\n",
      "Iteration 362, loss = 0.35193286\n",
      "Iteration 363, loss = 0.35145789\n",
      "Iteration 364, loss = 0.35098401\n",
      "Iteration 365, loss = 0.35051122\n",
      "Iteration 366, loss = 0.35003953\n",
      "Iteration 367, loss = 0.34956888\n",
      "Iteration 368, loss = 0.34909924\n",
      "Iteration 369, loss = 0.34863072\n",
      "Iteration 370, loss = 0.34816321\n",
      "Iteration 371, loss = 0.34769690\n",
      "Iteration 372, loss = 0.34723165\n",
      "Iteration 373, loss = 0.34676736\n",
      "Iteration 374, loss = 0.34630414\n",
      "Iteration 375, loss = 0.34584194\n",
      "Iteration 376, loss = 0.34538078\n",
      "Iteration 377, loss = 0.34492064\n",
      "Iteration 378, loss = 0.34446150\n",
      "Iteration 379, loss = 0.34400346\n",
      "Iteration 380, loss = 0.34354634\n",
      "Iteration 381, loss = 0.34309027\n",
      "Iteration 382, loss = 0.34263527\n",
      "Iteration 383, loss = 0.34218118\n",
      "Iteration 384, loss = 0.34172827\n",
      "Iteration 385, loss = 0.34127630\n",
      "Iteration 386, loss = 0.34082553\n",
      "Iteration 387, loss = 0.34037577\n",
      "Iteration 388, loss = 0.33992704\n",
      "Iteration 389, loss = 0.33947943\n",
      "Iteration 390, loss = 0.33903275\n",
      "Iteration 391, loss = 0.33858712\n",
      "Iteration 392, loss = 0.33814254\n",
      "Iteration 393, loss = 0.33769888\n",
      "Iteration 394, loss = 0.33725626\n",
      "Iteration 395, loss = 0.33681463\n",
      "Iteration 396, loss = 0.33637403\n",
      "Iteration 397, loss = 0.33593438\n",
      "Iteration 398, loss = 0.33549569\n",
      "Iteration 399, loss = 0.33505799\n",
      "Iteration 400, loss = 0.33462122\n",
      "Iteration 401, loss = 0.33418537\n",
      "Iteration 402, loss = 0.33375051\n",
      "Iteration 403, loss = 0.33331659\n",
      "Iteration 404, loss = 0.33288367\n",
      "Iteration 405, loss = 0.33245169\n",
      "Iteration 406, loss = 0.33202063\n",
      "Iteration 407, loss = 0.33159050\n",
      "Iteration 408, loss = 0.33116134\n",
      "Iteration 409, loss = 0.33073306\n",
      "Iteration 410, loss = 0.33030570\n",
      "Iteration 411, loss = 0.32987934\n",
      "Iteration 412, loss = 0.32945396\n",
      "Iteration 413, loss = 0.32902942\n",
      "Iteration 414, loss = 0.32860586\n",
      "Iteration 415, loss = 0.32818319\n",
      "Iteration 416, loss = 0.32776147\n",
      "Iteration 417, loss = 0.32734070\n",
      "Iteration 418, loss = 0.32692081\n",
      "Iteration 419, loss = 0.32650180\n",
      "Iteration 420, loss = 0.32608373\n",
      "Iteration 421, loss = 0.32566659\n",
      "Iteration 422, loss = 0.32525040\n",
      "Iteration 423, loss = 0.32483514\n",
      "Iteration 424, loss = 0.32442075\n",
      "Iteration 425, loss = 0.32400722\n",
      "Iteration 426, loss = 0.32359463\n",
      "Iteration 427, loss = 0.32318290\n",
      "Iteration 428, loss = 0.32277204\n",
      "Iteration 429, loss = 0.32236214\n",
      "Iteration 430, loss = 0.32195304\n",
      "Iteration 431, loss = 0.32154481\n",
      "Iteration 432, loss = 0.32113753\n",
      "Iteration 433, loss = 0.32073104\n",
      "Iteration 434, loss = 0.32032540\n",
      "Iteration 435, loss = 0.31992068\n",
      "Iteration 436, loss = 0.31951673\n",
      "Iteration 437, loss = 0.31911372\n",
      "Iteration 438, loss = 0.31871150\n",
      "Iteration 439, loss = 0.31831013\n",
      "Iteration 440, loss = 0.31790971\n",
      "Iteration 441, loss = 0.31751004\n",
      "Iteration 442, loss = 0.31711128\n",
      "Iteration 443, loss = 0.31671343\n",
      "Iteration 444, loss = 0.31631635\n",
      "Iteration 445, loss = 0.31592013\n",
      "Iteration 446, loss = 0.31552477\n",
      "Iteration 447, loss = 0.31513020\n",
      "Iteration 448, loss = 0.31473654\n",
      "Iteration 449, loss = 0.31434360\n",
      "Iteration 450, loss = 0.31395155\n",
      "Iteration 451, loss = 0.31356051\n",
      "Iteration 452, loss = 0.31317030\n",
      "Iteration 453, loss = 0.31278098\n",
      "Iteration 454, loss = 0.31239245\n",
      "Iteration 455, loss = 0.31200482\n",
      "Iteration 456, loss = 0.31161798\n",
      "Iteration 457, loss = 0.31123193\n",
      "Iteration 458, loss = 0.31084677\n",
      "Iteration 459, loss = 0.31046242\n",
      "Iteration 460, loss = 0.31007884\n",
      "Iteration 461, loss = 0.30969605\n",
      "Iteration 462, loss = 0.30931408\n",
      "Iteration 463, loss = 0.30893301\n",
      "Iteration 464, loss = 0.30855274\n",
      "Iteration 465, loss = 0.30817322\n",
      "Iteration 466, loss = 0.30779452\n",
      "Iteration 467, loss = 0.30741691\n",
      "Iteration 468, loss = 0.30704007\n",
      "Iteration 469, loss = 0.30666403\n",
      "Iteration 470, loss = 0.30628878\n",
      "Iteration 471, loss = 0.30591437\n",
      "Iteration 472, loss = 0.30554076\n",
      "Iteration 473, loss = 0.30516791\n",
      "Iteration 474, loss = 0.30479594\n",
      "Iteration 475, loss = 0.30442467\n",
      "Iteration 476, loss = 0.30405426\n",
      "Iteration 477, loss = 0.30368462\n",
      "Iteration 478, loss = 0.30331589\n",
      "Iteration 479, loss = 0.30294788\n",
      "Iteration 480, loss = 0.30258069\n",
      "Iteration 481, loss = 0.30221428\n",
      "Iteration 482, loss = 0.30184865\n",
      "Iteration 483, loss = 0.30148384\n",
      "Iteration 484, loss = 0.30111974\n",
      "Iteration 485, loss = 0.30075649\n",
      "Iteration 486, loss = 0.30039397\n",
      "Iteration 487, loss = 0.30003216\n",
      "Iteration 488, loss = 0.29967119\n",
      "Iteration 489, loss = 0.29931092\n",
      "Iteration 490, loss = 0.29895146\n",
      "Iteration 491, loss = 0.29859271\n",
      "Iteration 492, loss = 0.29823473\n",
      "Iteration 493, loss = 0.29787749\n",
      "Iteration 494, loss = 0.29752100\n",
      "Iteration 495, loss = 0.29716529\n",
      "Iteration 496, loss = 0.29681028\n",
      "Iteration 497, loss = 0.29645605\n",
      "Iteration 498, loss = 0.29610254\n",
      "Iteration 499, loss = 0.29574980\n",
      "Iteration 500, loss = 0.29539780\n",
      "Iteration 501, loss = 0.29504652\n",
      "Iteration 502, loss = 0.29469603\n",
      "Iteration 503, loss = 0.29434630\n",
      "Iteration 504, loss = 0.29399727\n",
      "Iteration 505, loss = 0.29364907\n",
      "Iteration 506, loss = 0.29330153\n",
      "Iteration 507, loss = 0.29295479\n",
      "Iteration 508, loss = 0.29260873\n",
      "Iteration 509, loss = 0.29226340\n",
      "Iteration 510, loss = 0.29191883\n",
      "Iteration 511, loss = 0.29157498\n",
      "Iteration 512, loss = 0.29123181\n",
      "Iteration 513, loss = 0.29088940\n",
      "Iteration 514, loss = 0.29054766\n",
      "Iteration 515, loss = 0.29020672\n",
      "Iteration 516, loss = 0.28986642\n",
      "Iteration 517, loss = 0.28952685\n",
      "Iteration 518, loss = 0.28918802\n",
      "Iteration 519, loss = 0.28884988\n",
      "Iteration 520, loss = 0.28851245\n",
      "Iteration 521, loss = 0.28817577\n",
      "Iteration 522, loss = 0.28783971\n",
      "Iteration 523, loss = 0.28750449\n",
      "Iteration 524, loss = 0.28717009\n",
      "Iteration 525, loss = 0.28683639\n",
      "Iteration 526, loss = 0.28650338\n",
      "Iteration 527, loss = 0.28617114\n",
      "Iteration 528, loss = 0.28583955\n",
      "Iteration 529, loss = 0.28550867\n",
      "Iteration 530, loss = 0.28517844\n",
      "Iteration 531, loss = 0.28484890\n",
      "Iteration 532, loss = 0.28452008\n",
      "Iteration 533, loss = 0.28419211\n",
      "Iteration 534, loss = 0.28386475\n",
      "Iteration 535, loss = 0.28353814\n",
      "Iteration 536, loss = 0.28321226\n",
      "Iteration 537, loss = 0.28288705\n",
      "Iteration 538, loss = 0.28256252\n",
      "Iteration 539, loss = 0.28223871\n",
      "Iteration 540, loss = 0.28191560\n",
      "Iteration 541, loss = 0.28159321\n",
      "Iteration 542, loss = 0.28127152\n",
      "Iteration 543, loss = 0.28095053\n",
      "Iteration 544, loss = 0.28063016\n",
      "Iteration 545, loss = 0.28031054\n",
      "Iteration 546, loss = 0.27999158\n",
      "Iteration 547, loss = 0.27967328\n",
      "Iteration 548, loss = 0.27935565\n",
      "Iteration 549, loss = 0.27903867\n",
      "Iteration 550, loss = 0.27872240\n",
      "Iteration 551, loss = 0.27840680\n",
      "Iteration 552, loss = 0.27809182\n",
      "Iteration 553, loss = 0.27777753\n",
      "Iteration 554, loss = 0.27746389\n",
      "Iteration 555, loss = 0.27715093\n",
      "Iteration 556, loss = 0.27683859\n",
      "Iteration 557, loss = 0.27652696\n",
      "Iteration 558, loss = 0.27621592\n",
      "Iteration 559, loss = 0.27590556\n",
      "Iteration 560, loss = 0.27559585\n",
      "Iteration 561, loss = 0.27528681\n",
      "Iteration 562, loss = 0.27497840\n",
      "Iteration 563, loss = 0.27467065\n",
      "Iteration 564, loss = 0.27436356\n",
      "Iteration 565, loss = 0.27405714\n",
      "Iteration 566, loss = 0.27375134\n",
      "Iteration 567, loss = 0.27344619\n",
      "Iteration 568, loss = 0.27314168\n",
      "Iteration 569, loss = 0.27283784\n",
      "Iteration 570, loss = 0.27253462\n",
      "Iteration 571, loss = 0.27223208\n",
      "Iteration 572, loss = 0.27193018\n",
      "Iteration 573, loss = 0.27162891\n",
      "Iteration 574, loss = 0.27132830\n",
      "Iteration 575, loss = 0.27102827\n",
      "Iteration 576, loss = 0.27072888\n",
      "Iteration 577, loss = 0.27043011\n",
      "Iteration 578, loss = 0.27013199\n",
      "Iteration 579, loss = 0.26983452\n",
      "Iteration 580, loss = 0.26953763\n",
      "Iteration 581, loss = 0.26924143\n",
      "Iteration 582, loss = 0.26894589\n",
      "Iteration 583, loss = 0.26865107\n",
      "Iteration 584, loss = 0.26835695\n",
      "Iteration 585, loss = 0.26806344\n",
      "Iteration 586, loss = 0.26777057\n",
      "Iteration 587, loss = 0.26747834\n",
      "Iteration 588, loss = 0.26718672\n",
      "Iteration 589, loss = 0.26689579\n",
      "Iteration 590, loss = 0.26660552\n",
      "Iteration 591, loss = 0.26631586\n",
      "Iteration 592, loss = 0.26602681\n",
      "Iteration 593, loss = 0.26573843\n",
      "Iteration 594, loss = 0.26545067\n",
      "Iteration 595, loss = 0.26516352\n",
      "Iteration 596, loss = 0.26487700\n",
      "Iteration 597, loss = 0.26459114\n",
      "Iteration 598, loss = 0.26430589\n",
      "Iteration 599, loss = 0.26402127\n",
      "Iteration 600, loss = 0.26373725\n",
      "Iteration 601, loss = 0.26345386\n",
      "Iteration 602, loss = 0.26317105\n",
      "Iteration 603, loss = 0.26288884\n",
      "Iteration 604, loss = 0.26260722\n",
      "Iteration 605, loss = 0.26232626\n",
      "Iteration 606, loss = 0.26204587\n",
      "Iteration 607, loss = 0.26176610\n",
      "Iteration 608, loss = 0.26148691\n",
      "Iteration 609, loss = 0.26120831\n",
      "Iteration 610, loss = 0.26093028\n",
      "Iteration 611, loss = 0.26065285\n",
      "Iteration 612, loss = 0.26037599\n",
      "Iteration 613, loss = 0.26009973\n",
      "Iteration 614, loss = 0.25982407\n",
      "Iteration 615, loss = 0.25954904\n",
      "Iteration 616, loss = 0.25927458\n",
      "Iteration 617, loss = 0.25900073\n",
      "Iteration 618, loss = 0.25872746\n",
      "Iteration 619, loss = 0.25845476\n",
      "Iteration 620, loss = 0.25818263\n",
      "Iteration 621, loss = 0.25791107\n",
      "Iteration 622, loss = 0.25764010\n",
      "Iteration 623, loss = 0.25736966\n",
      "Iteration 624, loss = 0.25709976\n",
      "Iteration 625, loss = 0.25683043\n",
      "Iteration 626, loss = 0.25656164\n",
      "Iteration 627, loss = 0.25629345\n",
      "Iteration 628, loss = 0.25602587\n",
      "Iteration 629, loss = 0.25575888\n",
      "Iteration 630, loss = 0.25549245\n",
      "Iteration 631, loss = 0.25522661\n",
      "Iteration 632, loss = 0.25496136\n",
      "Iteration 633, loss = 0.25469670\n",
      "Iteration 634, loss = 0.25443261\n",
      "Iteration 635, loss = 0.25416912\n",
      "Iteration 636, loss = 0.25390617\n",
      "Iteration 637, loss = 0.25364380\n",
      "Iteration 638, loss = 0.25338202\n",
      "Iteration 639, loss = 0.25312078\n",
      "Iteration 640, loss = 0.25286007\n",
      "Iteration 641, loss = 0.25259990\n",
      "Iteration 642, loss = 0.25234019\n",
      "Iteration 643, loss = 0.25208105\n",
      "Iteration 644, loss = 0.25182238\n",
      "Iteration 645, loss = 0.25156419\n",
      "Iteration 646, loss = 0.25130656\n",
      "Iteration 647, loss = 0.25104944\n",
      "Iteration 648, loss = 0.25079283\n",
      "Iteration 649, loss = 0.25053672\n",
      "Iteration 650, loss = 0.25028112\n",
      "Iteration 651, loss = 0.25002604\n",
      "Iteration 652, loss = 0.24977150\n",
      "Iteration 653, loss = 0.24951746\n",
      "Iteration 654, loss = 0.24926390\n",
      "Iteration 655, loss = 0.24901089\n",
      "Iteration 656, loss = 0.24875834\n",
      "Iteration 657, loss = 0.24850633\n",
      "Iteration 658, loss = 0.24825486\n",
      "Iteration 659, loss = 0.24800390\n",
      "Iteration 660, loss = 0.24775348\n",
      "Iteration 661, loss = 0.24750360\n",
      "Iteration 662, loss = 0.24725423\n",
      "Iteration 663, loss = 0.24700540\n",
      "Iteration 664, loss = 0.24675708\n",
      "Iteration 665, loss = 0.24650921\n",
      "Iteration 666, loss = 0.24626177\n",
      "Iteration 667, loss = 0.24601485\n",
      "Iteration 668, loss = 0.24576844\n",
      "Iteration 669, loss = 0.24552255\n",
      "Iteration 670, loss = 0.24527717\n",
      "Iteration 671, loss = 0.24503231\n",
      "Iteration 672, loss = 0.24478800\n",
      "Iteration 673, loss = 0.24454427\n",
      "Iteration 674, loss = 0.24430102\n",
      "Iteration 675, loss = 0.24405815\n",
      "Iteration 676, loss = 0.24381574\n",
      "Iteration 677, loss = 0.24357378\n",
      "Iteration 678, loss = 0.24333233\n",
      "Iteration 679, loss = 0.24309138\n",
      "Iteration 680, loss = 0.24285094\n",
      "Iteration 681, loss = 0.24261100\n",
      "Iteration 682, loss = 0.24237156\n",
      "Iteration 683, loss = 0.24213261\n",
      "Iteration 684, loss = 0.24189415\n",
      "Iteration 685, loss = 0.24165620\n",
      "Iteration 686, loss = 0.24141865\n",
      "Iteration 687, loss = 0.24118150\n",
      "Iteration 688, loss = 0.24094487\n",
      "Iteration 689, loss = 0.24070869\n",
      "Iteration 690, loss = 0.24047284\n",
      "Iteration 691, loss = 0.24023739\n",
      "Iteration 692, loss = 0.24000241\n",
      "Iteration 693, loss = 0.23976789\n",
      "Iteration 694, loss = 0.23953385\n",
      "Iteration 695, loss = 0.23930028\n",
      "Iteration 696, loss = 0.23906720\n",
      "Iteration 697, loss = 0.23883462\n",
      "Iteration 698, loss = 0.23860251\n",
      "Iteration 699, loss = 0.23837091\n",
      "Iteration 700, loss = 0.23813979\n",
      "Iteration 701, loss = 0.23790916\n",
      "Iteration 702, loss = 0.23767898\n",
      "Iteration 703, loss = 0.23744927\n",
      "Iteration 704, loss = 0.23721997\n",
      "Iteration 705, loss = 0.23699108\n",
      "Iteration 706, loss = 0.23676256\n",
      "Iteration 707, loss = 0.23653451\n",
      "Iteration 708, loss = 0.23630693\n",
      "Iteration 709, loss = 0.23607973\n",
      "Iteration 710, loss = 0.23585294\n",
      "Iteration 711, loss = 0.23562662\n",
      "Iteration 712, loss = 0.23540076\n",
      "Iteration 713, loss = 0.23517537\n",
      "Iteration 714, loss = 0.23495041\n",
      "Iteration 715, loss = 0.23472582\n",
      "Iteration 716, loss = 0.23450140\n",
      "Iteration 717, loss = 0.23427708\n",
      "Iteration 718, loss = 0.23405314\n",
      "Iteration 719, loss = 0.23382962\n",
      "Iteration 720, loss = 0.23360651\n",
      "Iteration 721, loss = 0.23338382\n",
      "Iteration 722, loss = 0.23316149\n",
      "Iteration 723, loss = 0.23293949\n",
      "Iteration 724, loss = 0.23271793\n",
      "Iteration 725, loss = 0.23249680\n",
      "Iteration 726, loss = 0.23227611\n",
      "Iteration 727, loss = 0.23205587\n",
      "Iteration 728, loss = 0.23183601\n",
      "Iteration 729, loss = 0.23161649\n",
      "Iteration 730, loss = 0.23139734\n",
      "Iteration 731, loss = 0.23117853\n",
      "Iteration 732, loss = 0.23096016\n",
      "Iteration 733, loss = 0.23074223\n",
      "Iteration 734, loss = 0.23052475\n",
      "Iteration 735, loss = 0.23030770\n",
      "Iteration 736, loss = 0.23009074\n",
      "Iteration 737, loss = 0.22987412\n",
      "Iteration 738, loss = 0.22965794\n",
      "Iteration 739, loss = 0.22944218\n",
      "Iteration 740, loss = 0.22922685\n",
      "Iteration 741, loss = 0.22901197\n",
      "Iteration 742, loss = 0.22879751\n",
      "Iteration 743, loss = 0.22858330\n",
      "Iteration 744, loss = 0.22836951\n",
      "Iteration 745, loss = 0.22815611\n",
      "Iteration 746, loss = 0.22794315\n",
      "Iteration 747, loss = 0.22773061\n",
      "Iteration 748, loss = 0.22751823\n",
      "Iteration 749, loss = 0.22730626\n",
      "Iteration 750, loss = 0.22709473\n",
      "Iteration 751, loss = 0.22688366\n",
      "Iteration 752, loss = 0.22667302\n",
      "Iteration 753, loss = 0.22646283\n",
      "Iteration 754, loss = 0.22625307\n",
      "Iteration 755, loss = 0.22604375\n",
      "Iteration 756, loss = 0.22583488\n",
      "Iteration 757, loss = 0.22562625\n",
      "Iteration 758, loss = 0.22541796\n",
      "Iteration 759, loss = 0.22521009\n",
      "Iteration 760, loss = 0.22500265\n",
      "Iteration 761, loss = 0.22479554\n",
      "Iteration 762, loss = 0.22458865\n",
      "Iteration 763, loss = 0.22438218\n",
      "Iteration 764, loss = 0.22417612\n",
      "Iteration 765, loss = 0.22397050\n",
      "Iteration 766, loss = 0.22376527\n",
      "Iteration 767, loss = 0.22356048\n",
      "Iteration 768, loss = 0.22335613\n",
      "Iteration 769, loss = 0.22315220\n",
      "Iteration 770, loss = 0.22294870\n",
      "Iteration 771, loss = 0.22274562\n",
      "Iteration 772, loss = 0.22254299\n",
      "Iteration 773, loss = 0.22234078\n",
      "Iteration 774, loss = 0.22213902\n",
      "Iteration 775, loss = 0.22193769\n",
      "Iteration 776, loss = 0.22173680\n",
      "Iteration 777, loss = 0.22153635\n",
      "Iteration 778, loss = 0.22133634\n",
      "Iteration 779, loss = 0.22113678\n",
      "Iteration 780, loss = 0.22093765\n",
      "Iteration 781, loss = 0.22073895\n",
      "Iteration 782, loss = 0.22054070\n",
      "Iteration 783, loss = 0.22034288\n",
      "Iteration 784, loss = 0.22014550\n",
      "Iteration 785, loss = 0.21994853\n",
      "Iteration 786, loss = 0.21975204\n",
      "Iteration 787, loss = 0.21955595\n",
      "Iteration 788, loss = 0.21936030\n",
      "Iteration 789, loss = 0.21916508\n",
      "Iteration 790, loss = 0.21897029\n",
      "Iteration 791, loss = 0.21877595\n",
      "Iteration 792, loss = 0.21858204\n",
      "Iteration 793, loss = 0.21838856\n",
      "Iteration 794, loss = 0.21819551\n",
      "Iteration 795, loss = 0.21800290\n",
      "Iteration 796, loss = 0.21781073\n",
      "Iteration 797, loss = 0.21761899\n",
      "Iteration 798, loss = 0.21742767\n",
      "Iteration 799, loss = 0.21723677\n",
      "Iteration 800, loss = 0.21704630\n",
      "Iteration 801, loss = 0.21685622\n",
      "Iteration 802, loss = 0.21666659\n",
      "Iteration 803, loss = 0.21647736\n",
      "Iteration 804, loss = 0.21628856\n",
      "Iteration 805, loss = 0.21610017\n",
      "Iteration 806, loss = 0.21591222\n",
      "Iteration 807, loss = 0.21572466\n",
      "Iteration 808, loss = 0.21553752\n",
      "Iteration 809, loss = 0.21535076\n",
      "Iteration 810, loss = 0.21516441\n",
      "Iteration 811, loss = 0.21497847\n",
      "Iteration 812, loss = 0.21479297\n",
      "Iteration 813, loss = 0.21460786\n",
      "Iteration 814, loss = 0.21442317\n",
      "Iteration 815, loss = 0.21423888\n",
      "Iteration 816, loss = 0.21405501\n",
      "Iteration 817, loss = 0.21387156\n",
      "Iteration 818, loss = 0.21368850\n",
      "Iteration 819, loss = 0.21350584\n",
      "Iteration 820, loss = 0.21332357\n",
      "Iteration 821, loss = 0.21314170\n",
      "Iteration 822, loss = 0.21296022\n",
      "Iteration 823, loss = 0.21277912\n",
      "Iteration 824, loss = 0.21259843\n",
      "Iteration 825, loss = 0.21241814\n",
      "Iteration 826, loss = 0.21223823\n",
      "Iteration 827, loss = 0.21205873\n",
      "Iteration 828, loss = 0.21187961\n",
      "Iteration 829, loss = 0.21170089\n",
      "Iteration 830, loss = 0.21152258\n",
      "Iteration 831, loss = 0.21134465\n",
      "Iteration 832, loss = 0.21116713\n",
      "Iteration 833, loss = 0.21099000\n",
      "Iteration 834, loss = 0.21081327\n",
      "Iteration 835, loss = 0.21063694\n",
      "Iteration 836, loss = 0.21046098\n",
      "Iteration 837, loss = 0.21028539\n",
      "Iteration 838, loss = 0.21011018\n",
      "Iteration 839, loss = 0.20993536\n",
      "Iteration 840, loss = 0.20976089\n",
      "Iteration 841, loss = 0.20958680\n",
      "Iteration 842, loss = 0.20941307\n",
      "Iteration 843, loss = 0.20923974\n",
      "Iteration 844, loss = 0.20906673\n",
      "Iteration 845, loss = 0.20889409\n",
      "Iteration 846, loss = 0.20872181\n",
      "Iteration 847, loss = 0.20854990\n",
      "Iteration 848, loss = 0.20837837\n",
      "Iteration 849, loss = 0.20820722\n",
      "Iteration 850, loss = 0.20803638\n",
      "Iteration 851, loss = 0.20786590\n",
      "Iteration 852, loss = 0.20769577\n",
      "Iteration 853, loss = 0.20752599\n",
      "Iteration 854, loss = 0.20735657\n",
      "Iteration 855, loss = 0.20718749\n",
      "Iteration 856, loss = 0.20701878\n",
      "Iteration 857, loss = 0.20685043\n",
      "Iteration 858, loss = 0.20668243\n",
      "Iteration 859, loss = 0.20651480\n",
      "Iteration 860, loss = 0.20634755\n",
      "Iteration 861, loss = 0.20618066\n",
      "Iteration 862, loss = 0.20601412\n",
      "Iteration 863, loss = 0.20584795\n",
      "Iteration 864, loss = 0.20568213\n",
      "Iteration 865, loss = 0.20551670\n",
      "Iteration 866, loss = 0.20535157\n",
      "Iteration 867, loss = 0.20518682\n",
      "Iteration 868, loss = 0.20502240\n",
      "Iteration 869, loss = 0.20485834\n",
      "Iteration 870, loss = 0.20469463\n",
      "Iteration 871, loss = 0.20453130\n",
      "Iteration 872, loss = 0.20436830\n",
      "Iteration 873, loss = 0.20420568\n",
      "Iteration 874, loss = 0.20404340\n",
      "Iteration 875, loss = 0.20388148\n",
      "Iteration 876, loss = 0.20371985\n",
      "Iteration 877, loss = 0.20355851\n",
      "Iteration 878, loss = 0.20339752\n",
      "Iteration 879, loss = 0.20323687\n",
      "Iteration 880, loss = 0.20307655\n",
      "Iteration 881, loss = 0.20291659\n",
      "Iteration 882, loss = 0.20275697\n",
      "Iteration 883, loss = 0.20259771\n",
      "Iteration 884, loss = 0.20243875\n",
      "Iteration 885, loss = 0.20228013\n",
      "Iteration 886, loss = 0.20212184\n",
      "Iteration 887, loss = 0.20196388\n",
      "Iteration 888, loss = 0.20180625\n",
      "Iteration 889, loss = 0.20164874\n",
      "Iteration 890, loss = 0.20149154\n",
      "Iteration 891, loss = 0.20133466\n",
      "Iteration 892, loss = 0.20117805\n",
      "Iteration 893, loss = 0.20102173\n",
      "Iteration 894, loss = 0.20086569\n",
      "Iteration 895, loss = 0.20070998\n",
      "Iteration 896, loss = 0.20055458\n",
      "Iteration 897, loss = 0.20039949\n",
      "Iteration 898, loss = 0.20024474\n",
      "Iteration 899, loss = 0.20009029\n",
      "Iteration 900, loss = 0.19993615\n",
      "Iteration 901, loss = 0.19978235\n",
      "Iteration 902, loss = 0.19962888\n",
      "Iteration 903, loss = 0.19947574\n",
      "Iteration 904, loss = 0.19932290\n",
      "Iteration 905, loss = 0.19917040\n",
      "Iteration 906, loss = 0.19901821\n",
      "Iteration 907, loss = 0.19886630\n",
      "Iteration 908, loss = 0.19871472\n",
      "Iteration 909, loss = 0.19856342\n",
      "Iteration 910, loss = 0.19841245\n",
      "Iteration 911, loss = 0.19826179\n",
      "Iteration 912, loss = 0.19811142\n",
      "Iteration 913, loss = 0.19796136\n",
      "Iteration 914, loss = 0.19781162\n",
      "Iteration 915, loss = 0.19766219\n",
      "Iteration 916, loss = 0.19751309\n",
      "Iteration 917, loss = 0.19736430\n",
      "Iteration 918, loss = 0.19721580\n",
      "Iteration 919, loss = 0.19706757\n",
      "Iteration 920, loss = 0.19691964\n",
      "Iteration 921, loss = 0.19677199\n",
      "Iteration 922, loss = 0.19662463\n",
      "Iteration 923, loss = 0.19647757\n",
      "Iteration 924, loss = 0.19633083\n",
      "Iteration 925, loss = 0.19618438\n",
      "Iteration 926, loss = 0.19603819\n",
      "Iteration 927, loss = 0.19589217\n",
      "Iteration 928, loss = 0.19574635\n",
      "Iteration 929, loss = 0.19560081\n",
      "Iteration 930, loss = 0.19545557\n",
      "Iteration 931, loss = 0.19531060\n",
      "Iteration 932, loss = 0.19516586\n",
      "Iteration 933, loss = 0.19502139\n",
      "Iteration 934, loss = 0.19487721\n",
      "Iteration 935, loss = 0.19473331\n",
      "Iteration 936, loss = 0.19458970\n",
      "Iteration 937, loss = 0.19444638\n",
      "Iteration 938, loss = 0.19430335\n",
      "Iteration 939, loss = 0.19416059\n",
      "Iteration 940, loss = 0.19401810\n",
      "Iteration 941, loss = 0.19387573\n",
      "Iteration 942, loss = 0.19373363\n",
      "Iteration 943, loss = 0.19359183\n",
      "Iteration 944, loss = 0.19345030\n",
      "Iteration 945, loss = 0.19330898\n",
      "Iteration 946, loss = 0.19316790\n",
      "Iteration 947, loss = 0.19302712\n",
      "Iteration 948, loss = 0.19288661\n",
      "Iteration 949, loss = 0.19274639\n",
      "Iteration 950, loss = 0.19260645\n",
      "Iteration 951, loss = 0.19246679\n",
      "Iteration 952, loss = 0.19232742\n",
      "Iteration 953, loss = 0.19218834\n",
      "Iteration 954, loss = 0.19204944\n",
      "Iteration 955, loss = 0.19191062\n",
      "Iteration 956, loss = 0.19177204\n",
      "Iteration 957, loss = 0.19163371\n",
      "Iteration 958, loss = 0.19149565\n",
      "Iteration 959, loss = 0.19135787\n",
      "Iteration 960, loss = 0.19122035\n",
      "Iteration 961, loss = 0.19108309\n",
      "Iteration 962, loss = 0.19094611\n",
      "Iteration 963, loss = 0.19080941\n",
      "Iteration 964, loss = 0.19067299\n",
      "Iteration 965, loss = 0.19053685\n",
      "Iteration 966, loss = 0.19040098\n",
      "Iteration 967, loss = 0.19026534\n",
      "Iteration 968, loss = 0.19012996\n",
      "Iteration 969, loss = 0.18999485\n",
      "Iteration 970, loss = 0.18986001\n",
      "Iteration 971, loss = 0.18972545\n",
      "Iteration 972, loss = 0.18959113\n",
      "Iteration 973, loss = 0.18945691\n",
      "Iteration 974, loss = 0.18932291\n",
      "Iteration 975, loss = 0.18918918\n",
      "Iteration 976, loss = 0.18905570\n",
      "Iteration 977, loss = 0.18892249\n",
      "Iteration 978, loss = 0.18878952\n",
      "Iteration 979, loss = 0.18865683\n",
      "Iteration 980, loss = 0.18852429\n",
      "Iteration 981, loss = 0.18839193\n",
      "Iteration 982, loss = 0.18825982\n",
      "Iteration 983, loss = 0.18812796\n",
      "Iteration 984, loss = 0.18799636\n",
      "Iteration 985, loss = 0.18786501\n",
      "Iteration 986, loss = 0.18773379\n",
      "Iteration 987, loss = 0.18760283\n",
      "Iteration 988, loss = 0.18747211\n",
      "Iteration 989, loss = 0.18734166\n",
      "Iteration 990, loss = 0.18721146\n",
      "Iteration 991, loss = 0.18708143\n",
      "Iteration 992, loss = 0.18695164\n",
      "Iteration 993, loss = 0.18682211\n",
      "Iteration 994, loss = 0.18669283\n",
      "Iteration 995, loss = 0.18656381\n",
      "Iteration 996, loss = 0.18643504\n",
      "Iteration 997, loss = 0.18630653\n",
      "Iteration 998, loss = 0.18617828\n",
      "Iteration 999, loss = 0.18605027\n",
      "Iteration 1000, loss = 0.18592242\n",
      "Iteration 1, loss = 1.92297910\n",
      "Iteration 2, loss = 1.54655419\n",
      "Iteration 3, loss = 1.22958849\n",
      "Iteration 4, loss = 1.08789872\n",
      "Iteration 5, loss = 1.10478581\n",
      "Iteration 6, loss = 1.05798564\n",
      "Iteration 7, loss = 0.96373703\n",
      "Iteration 8, loss = 0.88494460\n",
      "Iteration 9, loss = 0.81523066\n",
      "Iteration 10, loss = 0.75556560\n",
      "Iteration 11, loss = 0.70939626\n",
      "Iteration 12, loss = 0.67871216\n",
      "Iteration 13, loss = 0.65093841\n",
      "Iteration 14, loss = 0.62223110\n",
      "Iteration 15, loss = 0.59304100\n",
      "Iteration 16, loss = 0.56577725\n",
      "Iteration 17, loss = 0.54299241\n",
      "Iteration 18, loss = 0.52280194\n",
      "Iteration 19, loss = 0.50447089\n",
      "Iteration 20, loss = 0.48771945\n",
      "Iteration 21, loss = 0.47237390\n",
      "Iteration 22, loss = 0.45826048\n",
      "Iteration 23, loss = 0.44523285\n",
      "Iteration 24, loss = 0.43309978\n",
      "Iteration 25, loss = 0.42186616\n",
      "Iteration 26, loss = 0.41153311\n",
      "Iteration 27, loss = 0.40191926\n",
      "Iteration 28, loss = 0.39287899\n",
      "Iteration 29, loss = 0.38442256\n",
      "Iteration 30, loss = 0.37649335\n",
      "Iteration 31, loss = 0.36901602\n",
      "Iteration 32, loss = 0.36192931\n",
      "Iteration 33, loss = 0.35518569\n",
      "Iteration 34, loss = 0.34874456\n",
      "Iteration 35, loss = 0.34257262\n",
      "Iteration 36, loss = 0.33664932\n",
      "Iteration 37, loss = 0.33095138\n",
      "Iteration 38, loss = 0.32546271\n",
      "Iteration 39, loss = 0.32016946\n",
      "Iteration 40, loss = 0.31505537\n",
      "Iteration 41, loss = 0.31011469\n",
      "Iteration 42, loss = 0.30532398\n",
      "Iteration 43, loss = 0.30067578\n",
      "Iteration 44, loss = 0.29616234\n",
      "Iteration 45, loss = 0.29177672\n",
      "Iteration 46, loss = 0.28751277\n",
      "Iteration 47, loss = 0.28336581\n",
      "Iteration 48, loss = 0.27932987\n",
      "Iteration 49, loss = 0.27539944\n",
      "Iteration 50, loss = 0.27157060\n",
      "Iteration 51, loss = 0.26783992\n",
      "Iteration 52, loss = 0.26420507\n",
      "Iteration 53, loss = 0.26066271\n",
      "Iteration 54, loss = 0.25721097\n",
      "Iteration 55, loss = 0.25384407\n",
      "Iteration 56, loss = 0.25056083\n",
      "Iteration 57, loss = 0.24736021\n",
      "Iteration 58, loss = 0.24423873\n",
      "Iteration 59, loss = 0.24119432\n",
      "Iteration 60, loss = 0.23822480\n",
      "Iteration 61, loss = 0.23532834\n",
      "Iteration 62, loss = 0.23250279\n",
      "Iteration 63, loss = 0.22974676\n",
      "Iteration 64, loss = 0.22705783\n",
      "Iteration 65, loss = 0.22443454\n",
      "Iteration 66, loss = 0.22187497\n",
      "Iteration 67, loss = 0.21937766\n",
      "Iteration 68, loss = 0.21694072\n",
      "Iteration 69, loss = 0.21456274\n",
      "Iteration 70, loss = 0.21224228\n",
      "Iteration 71, loss = 0.20997781\n",
      "Iteration 72, loss = 0.20776739\n",
      "Iteration 73, loss = 0.20560955\n",
      "Iteration 74, loss = 0.20350295\n",
      "Iteration 75, loss = 0.20144633\n",
      "Iteration 76, loss = 0.19943819\n",
      "Iteration 77, loss = 0.19747716\n",
      "Iteration 78, loss = 0.19556196\n",
      "Iteration 79, loss = 0.19369150\n",
      "Iteration 80, loss = 0.19186454\n",
      "Iteration 81, loss = 0.19007970\n",
      "Iteration 82, loss = 0.18833606\n",
      "Iteration 83, loss = 0.18663245\n",
      "Iteration 84, loss = 0.18496768\n",
      "Iteration 85, loss = 0.18334086\n",
      "Iteration 86, loss = 0.18175088\n",
      "Iteration 87, loss = 0.18019651\n",
      "Iteration 88, loss = 0.17867708\n",
      "Iteration 89, loss = 0.17719188\n",
      "Iteration 90, loss = 0.17573967\n",
      "Iteration 91, loss = 0.17431997\n",
      "Iteration 92, loss = 0.17293170\n",
      "Iteration 93, loss = 0.17157366\n",
      "Iteration 94, loss = 0.17024494\n",
      "Iteration 95, loss = 0.16894514\n",
      "Iteration 96, loss = 0.16767456\n",
      "Iteration 97, loss = 0.16643190\n",
      "Iteration 98, loss = 0.16521612\n",
      "Iteration 99, loss = 0.16402600\n",
      "Iteration 100, loss = 0.16286089\n",
      "Iteration 101, loss = 0.16172021\n",
      "Iteration 102, loss = 0.16060334\n",
      "Iteration 103, loss = 0.15950965\n",
      "Iteration 104, loss = 0.15843854\n",
      "Iteration 105, loss = 0.15738924\n",
      "Iteration 106, loss = 0.15636105\n",
      "Iteration 107, loss = 0.15535344\n",
      "Iteration 108, loss = 0.15436612\n",
      "Iteration 109, loss = 0.15339842\n",
      "Iteration 110, loss = 0.15244992\n",
      "Iteration 111, loss = 0.15151994\n",
      "Iteration 112, loss = 0.15060803\n",
      "Iteration 113, loss = 0.14971364\n",
      "Iteration 114, loss = 0.14883642\n",
      "Iteration 115, loss = 0.14797583\n",
      "Iteration 116, loss = 0.14713143\n",
      "Iteration 117, loss = 0.14630289\n",
      "Iteration 118, loss = 0.14548974\n",
      "Iteration 119, loss = 0.14469159\n",
      "Iteration 120, loss = 0.14390801\n",
      "Iteration 121, loss = 0.14313872\n",
      "Iteration 122, loss = 0.14238334\n",
      "Iteration 123, loss = 0.14164157\n",
      "Iteration 124, loss = 0.14091301\n",
      "Iteration 125, loss = 0.14019738\n",
      "Iteration 126, loss = 0.13949433\n",
      "Iteration 127, loss = 0.13880354\n",
      "Iteration 128, loss = 0.13812470\n",
      "Iteration 129, loss = 0.13745754\n",
      "Iteration 130, loss = 0.13680177\n",
      "Iteration 131, loss = 0.13615713\n",
      "Iteration 132, loss = 0.13552341\n",
      "Iteration 133, loss = 0.13490032\n",
      "Iteration 134, loss = 0.13428759\n",
      "Iteration 135, loss = 0.13368503\n",
      "Iteration 136, loss = 0.13309241\n",
      "Iteration 137, loss = 0.13250946\n",
      "Iteration 138, loss = 0.13193597\n",
      "Iteration 139, loss = 0.13137172\n",
      "Iteration 140, loss = 0.13081651\n",
      "Iteration 141, loss = 0.13027013\n",
      "Iteration 142, loss = 0.12973239\n",
      "Iteration 143, loss = 0.12920313\n",
      "Iteration 144, loss = 0.12868214\n",
      "Iteration 145, loss = 0.12816927\n",
      "Iteration 146, loss = 0.12766431\n",
      "Iteration 147, loss = 0.12716709\n",
      "Iteration 148, loss = 0.12667746\n",
      "Iteration 149, loss = 0.12619523\n",
      "Iteration 150, loss = 0.12572027\n",
      "Iteration 151, loss = 0.12525242\n",
      "Iteration 152, loss = 0.12479153\n",
      "Iteration 153, loss = 0.12433749\n",
      "Iteration 154, loss = 0.12389012\n",
      "Iteration 155, loss = 0.12344927\n",
      "Iteration 156, loss = 0.12301483\n",
      "Iteration 157, loss = 0.12258664\n",
      "Iteration 158, loss = 0.12216462\n",
      "Iteration 159, loss = 0.12174862\n",
      "Iteration 160, loss = 0.12133851\n",
      "Iteration 161, loss = 0.12093418\n",
      "Iteration 162, loss = 0.12053551\n",
      "Iteration 163, loss = 0.12014238\n",
      "Iteration 164, loss = 0.11975470\n",
      "Iteration 165, loss = 0.11937234\n",
      "Iteration 166, loss = 0.11899521\n",
      "Iteration 167, loss = 0.11862321\n",
      "Iteration 168, loss = 0.11825624\n",
      "Iteration 169, loss = 0.11789420\n",
      "Iteration 170, loss = 0.11753700\n",
      "Iteration 171, loss = 0.11718454\n",
      "Iteration 172, loss = 0.11683674\n",
      "Iteration 173, loss = 0.11649352\n",
      "Iteration 174, loss = 0.11615480\n",
      "Iteration 175, loss = 0.11582049\n",
      "Iteration 176, loss = 0.11549049\n",
      "Iteration 177, loss = 0.11516474\n",
      "Iteration 178, loss = 0.11484314\n",
      "Iteration 179, loss = 0.11452562\n",
      "Iteration 180, loss = 0.11421212\n",
      "Iteration 181, loss = 0.11390255\n",
      "Iteration 182, loss = 0.11359686\n",
      "Iteration 183, loss = 0.11329502\n",
      "Iteration 184, loss = 0.11299692\n",
      "Iteration 185, loss = 0.11270248\n",
      "Iteration 186, loss = 0.11241165\n",
      "Iteration 187, loss = 0.11212436\n",
      "Iteration 188, loss = 0.11184054\n",
      "Iteration 189, loss = 0.11156013\n",
      "Iteration 190, loss = 0.11128307\n",
      "Iteration 191, loss = 0.11100931\n",
      "Iteration 192, loss = 0.11073879\n",
      "Iteration 193, loss = 0.11047146\n",
      "Iteration 194, loss = 0.11020719\n",
      "Iteration 195, loss = 0.10994599\n",
      "Iteration 196, loss = 0.10968782\n",
      "Iteration 197, loss = 0.10943258\n",
      "Iteration 198, loss = 0.10918024\n",
      "Iteration 199, loss = 0.10893078\n",
      "Iteration 200, loss = 0.10868414\n",
      "Iteration 201, loss = 0.10844025\n",
      "Iteration 202, loss = 0.10819910\n",
      "Iteration 203, loss = 0.10796065\n",
      "Iteration 204, loss = 0.10772483\n",
      "Iteration 205, loss = 0.10749160\n",
      "Iteration 206, loss = 0.10726093\n",
      "Iteration 207, loss = 0.10703280\n",
      "Iteration 208, loss = 0.10680717\n",
      "Iteration 209, loss = 0.10658400\n",
      "Iteration 210, loss = 0.10636325\n",
      "Iteration 211, loss = 0.10614488\n",
      "Iteration 212, loss = 0.10592886\n",
      "Iteration 213, loss = 0.10571514\n",
      "Iteration 214, loss = 0.10550366\n",
      "Iteration 215, loss = 0.10529441\n",
      "Iteration 216, loss = 0.10508735\n",
      "Iteration 217, loss = 0.10488240\n",
      "Iteration 218, loss = 0.10467958\n",
      "Iteration 219, loss = 0.10447884\n",
      "Iteration 220, loss = 0.10428015\n",
      "Iteration 221, loss = 0.10408344\n",
      "Iteration 222, loss = 0.10388872\n",
      "Iteration 223, loss = 0.10369589\n",
      "Iteration 224, loss = 0.10350500\n",
      "Iteration 225, loss = 0.10331601\n",
      "Iteration 226, loss = 0.10312888\n",
      "Iteration 227, loss = 0.10294360\n",
      "Iteration 228, loss = 0.10276014\n",
      "Iteration 229, loss = 0.10257847\n",
      "Iteration 230, loss = 0.10239855\n",
      "Iteration 231, loss = 0.10222039\n",
      "Iteration 232, loss = 0.10204391\n",
      "Iteration 233, loss = 0.10186912\n",
      "Iteration 234, loss = 0.10169603\n",
      "Iteration 235, loss = 0.10152460\n",
      "Iteration 236, loss = 0.10135480\n",
      "Iteration 237, loss = 0.10118658\n",
      "Iteration 238, loss = 0.10101993\n",
      "Iteration 239, loss = 0.10085486\n",
      "Iteration 240, loss = 0.10069134\n",
      "Iteration 241, loss = 0.10052931\n",
      "Iteration 242, loss = 0.10036878\n",
      "Iteration 243, loss = 0.10020971\n",
      "Iteration 244, loss = 0.10005209\n",
      "Iteration 245, loss = 0.09989588\n",
      "Iteration 246, loss = 0.09974109\n",
      "Iteration 247, loss = 0.09958767\n",
      "Iteration 248, loss = 0.09943560\n",
      "Iteration 249, loss = 0.09928488\n",
      "Iteration 250, loss = 0.09913550\n",
      "Iteration 251, loss = 0.09898745\n",
      "Iteration 252, loss = 0.09884072\n",
      "Iteration 253, loss = 0.09869536\n",
      "Iteration 254, loss = 0.09855129\n",
      "Iteration 255, loss = 0.09840851\n",
      "Iteration 256, loss = 0.09826703\n",
      "Iteration 257, loss = 0.09812667\n",
      "Iteration 258, loss = 0.09798749\n",
      "Iteration 259, loss = 0.09784996\n",
      "Iteration 260, loss = 0.09771364\n",
      "Iteration 261, loss = 0.09757851\n",
      "Iteration 262, loss = 0.09744453\n",
      "Iteration 263, loss = 0.09731170\n",
      "Iteration 264, loss = 0.09718000\n",
      "Iteration 265, loss = 0.09704849\n",
      "Iteration 266, loss = 0.09691407\n",
      "Iteration 267, loss = 0.09677970\n",
      "Iteration 268, loss = 0.09664548\n",
      "Iteration 269, loss = 0.09651155\n",
      "Iteration 270, loss = 0.09637785\n",
      "Iteration 271, loss = 0.09624425\n",
      "Iteration 272, loss = 0.09611142\n",
      "Iteration 273, loss = 0.09597943\n",
      "Iteration 274, loss = 0.09585108\n",
      "Iteration 275, loss = 0.09572069\n",
      "Iteration 276, loss = 0.09557650\n",
      "Iteration 277, loss = 0.09542532\n",
      "Iteration 278, loss = 0.09527986\n",
      "Iteration 279, loss = 0.09515244\n",
      "Iteration 280, loss = 0.09502578\n",
      "Iteration 281, loss = 0.09490379\n",
      "Iteration 282, loss = 0.09480090\n",
      "Iteration 283, loss = 0.09469795\n",
      "Iteration 284, loss = 0.09459241\n",
      "Iteration 285, loss = 0.09448457\n",
      "Iteration 286, loss = 0.09437442\n",
      "Iteration 287, loss = 0.09426252\n",
      "Iteration 288, loss = 0.09414915\n",
      "Iteration 289, loss = 0.09403423\n",
      "Iteration 290, loss = 0.09391975\n",
      "Iteration 291, loss = 0.09380835\n",
      "Iteration 292, loss = 0.09370543\n",
      "Iteration 293, loss = 0.09360275\n",
      "Iteration 294, loss = 0.09350027\n",
      "Iteration 295, loss = 0.09339799\n",
      "Iteration 296, loss = 0.09329594\n",
      "Iteration 297, loss = 0.09319415\n",
      "Iteration 298, loss = 0.09309269\n",
      "Iteration 299, loss = 0.09299162\n",
      "Iteration 300, loss = 0.09289091\n",
      "Iteration 301, loss = 0.09279564\n",
      "Iteration 302, loss = 0.09269753\n",
      "Iteration 303, loss = 0.09259832\n",
      "Iteration 304, loss = 0.09250438\n",
      "Iteration 305, loss = 0.09240910\n",
      "Iteration 306, loss = 0.09231400\n",
      "Iteration 307, loss = 0.09221909\n",
      "Iteration 308, loss = 0.09212654\n",
      "Iteration 309, loss = 0.09203292\n",
      "Iteration 310, loss = 0.09194180\n",
      "Iteration 311, loss = 0.09185034\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Best params: {'hidden_layer_sizes': 40, 'learning_rate_init': 0.01, 'solver': 'sgd'}\n",
      "Iteration 1, loss = 3.68044240\n",
      "Iteration 2, loss = 2.67690486\n",
      "Iteration 3, loss = 2.03540591\n",
      "Iteration 4, loss = 1.89392304\n",
      "Iteration 5, loss = 1.90289871\n",
      "Iteration 6, loss = 1.90140698\n",
      "Iteration 7, loss = 1.86349850\n",
      "Iteration 8, loss = 1.79487059\n",
      "Iteration 9, loss = 1.71608602\n",
      "Iteration 10, loss = 1.64141096\n",
      "Iteration 11, loss = 1.56715538\n",
      "Iteration 12, loss = 1.49510387\n",
      "Iteration 13, loss = 1.42822561\n",
      "Iteration 14, loss = 1.36985715\n",
      "Iteration 15, loss = 1.32210208\n",
      "Iteration 16, loss = 1.28440521\n",
      "Iteration 17, loss = 1.25374546\n",
      "Iteration 18, loss = 1.22671712\n",
      "Iteration 19, loss = 1.20116586\n",
      "Iteration 20, loss = 1.17662622\n",
      "Iteration 21, loss = 1.15295021\n",
      "Iteration 22, loss = 1.12966145\n",
      "Iteration 23, loss = 1.10660650\n",
      "Iteration 24, loss = 1.08390685\n",
      "Iteration 25, loss = 1.06195550\n",
      "Iteration 26, loss = 1.04121043\n",
      "Iteration 27, loss = 1.02210882\n",
      "Iteration 28, loss = 1.00483979\n",
      "Iteration 29, loss = 0.98938502\n",
      "Iteration 30, loss = 0.97552210\n",
      "Iteration 31, loss = 0.96306457\n",
      "Iteration 32, loss = 0.95176073\n",
      "Iteration 33, loss = 0.94146965\n",
      "Iteration 34, loss = 0.93205894\n",
      "Iteration 35, loss = 0.92341527\n",
      "Iteration 36, loss = 0.91541813\n",
      "Iteration 37, loss = 0.90799581\n",
      "Iteration 38, loss = 0.90123241\n",
      "Iteration 39, loss = 0.89519640\n",
      "Iteration 40, loss = 0.88984345\n",
      "Iteration 41, loss = 0.88438173\n",
      "Iteration 42, loss = 0.87871793\n",
      "Iteration 43, loss = 0.87343079\n",
      "Iteration 44, loss = 0.86848334\n",
      "Iteration 45, loss = 0.86375600\n",
      "Iteration 46, loss = 0.85920027\n",
      "Iteration 47, loss = 0.85480118\n",
      "Iteration 48, loss = 0.85054483\n",
      "Iteration 49, loss = 0.84641567\n",
      "Iteration 50, loss = 0.84239599\n",
      "Iteration 51, loss = 0.83847242\n",
      "Iteration 52, loss = 0.83465112\n",
      "Iteration 53, loss = 0.83093421\n",
      "Iteration 54, loss = 0.82732272\n",
      "Iteration 55, loss = 0.82381124\n",
      "Iteration 56, loss = 0.82040303\n",
      "Iteration 57, loss = 0.81709669\n",
      "Iteration 58, loss = 0.81388305\n",
      "Iteration 59, loss = 0.81076312\n",
      "Iteration 60, loss = 0.80771875\n",
      "Iteration 61, loss = 0.80474319\n",
      "Iteration 62, loss = 0.80183199\n",
      "Iteration 63, loss = 0.79897825\n",
      "Iteration 64, loss = 0.79617397\n",
      "Iteration 65, loss = 0.79341304\n",
      "Iteration 66, loss = 0.79068741\n",
      "Iteration 67, loss = 0.78799359\n",
      "Iteration 68, loss = 0.78533203\n",
      "Iteration 69, loss = 0.78270449\n",
      "Iteration 70, loss = 0.78010429\n",
      "Iteration 71, loss = 0.77753073\n",
      "Iteration 72, loss = 0.77497978\n",
      "Iteration 73, loss = 0.77245094\n",
      "Iteration 74, loss = 0.76994129\n",
      "Iteration 75, loss = 0.76744672\n",
      "Iteration 76, loss = 0.76496742\n",
      "Iteration 77, loss = 0.76250099\n",
      "Iteration 78, loss = 0.76004516\n",
      "Iteration 79, loss = 0.75759580\n",
      "Iteration 80, loss = 0.75515162\n",
      "Iteration 81, loss = 0.75271352\n",
      "Iteration 82, loss = 0.75027887\n",
      "Iteration 83, loss = 0.74784529\n",
      "Iteration 84, loss = 0.74541093\n",
      "Iteration 85, loss = 0.74297392\n",
      "Iteration 86, loss = 0.74053291\n",
      "Iteration 87, loss = 0.73808596\n",
      "Iteration 88, loss = 0.73563304\n",
      "Iteration 89, loss = 0.73317134\n",
      "Iteration 90, loss = 0.73069859\n",
      "Iteration 91, loss = 0.72821089\n",
      "Iteration 92, loss = 0.72570687\n",
      "Iteration 93, loss = 0.72318712\n",
      "Iteration 94, loss = 0.72064772\n",
      "Iteration 95, loss = 0.71808655\n",
      "Iteration 96, loss = 0.71550155\n",
      "Iteration 97, loss = 0.71289087\n",
      "Iteration 98, loss = 0.71025268\n",
      "Iteration 99, loss = 0.70758474\n",
      "Iteration 100, loss = 0.70488522\n",
      "Iteration 101, loss = 0.70215180\n",
      "Iteration 102, loss = 0.69938261\n",
      "Iteration 103, loss = 0.69657533\n",
      "Iteration 104, loss = 0.69372826\n",
      "Iteration 105, loss = 0.69083962\n",
      "Iteration 106, loss = 0.68790683\n",
      "Iteration 107, loss = 0.68492972\n",
      "Iteration 108, loss = 0.68190279\n",
      "Iteration 109, loss = 0.67882718\n",
      "Iteration 110, loss = 0.67570089\n",
      "Iteration 111, loss = 0.67252091\n",
      "Iteration 112, loss = 0.66928632\n",
      "Iteration 113, loss = 0.66603450\n",
      "Iteration 114, loss = 0.66278617\n",
      "Iteration 115, loss = 0.65954633\n",
      "Iteration 116, loss = 0.65626906\n",
      "Iteration 117, loss = 0.65296189\n",
      "Iteration 118, loss = 0.64970410\n",
      "Iteration 119, loss = 0.64646325\n",
      "Iteration 120, loss = 0.64326549\n",
      "Iteration 121, loss = 0.64026298\n",
      "Iteration 122, loss = 0.63743546\n",
      "Iteration 123, loss = 0.63462021\n",
      "Iteration 124, loss = 0.63175330\n",
      "Iteration 125, loss = 0.62882156\n",
      "Iteration 126, loss = 0.62581849\n",
      "Iteration 127, loss = 0.62275060\n",
      "Iteration 128, loss = 0.61962806\n",
      "Iteration 129, loss = 0.61650019\n",
      "Iteration 130, loss = 0.61334959\n",
      "Iteration 131, loss = 0.61017891\n",
      "Iteration 132, loss = 0.60700572\n",
      "Iteration 133, loss = 0.60392038\n",
      "Iteration 134, loss = 0.60084839\n",
      "Iteration 135, loss = 0.59776790\n",
      "Iteration 136, loss = 0.59468034\n",
      "Iteration 137, loss = 0.59157100\n",
      "Iteration 138, loss = 0.58843847\n",
      "Iteration 139, loss = 0.58528969\n",
      "Iteration 140, loss = 0.58213065\n",
      "Iteration 141, loss = 0.57896288\n",
      "Iteration 142, loss = 0.57578566\n",
      "Iteration 143, loss = 0.57259992\n",
      "Iteration 144, loss = 0.56941129\n",
      "Iteration 145, loss = 0.56622683\n",
      "Iteration 146, loss = 0.56303702\n",
      "Iteration 147, loss = 0.55984600\n",
      "Iteration 148, loss = 0.55663896\n",
      "Iteration 149, loss = 0.55343073\n",
      "Iteration 150, loss = 0.55021249\n",
      "Iteration 151, loss = 0.54698554\n",
      "Iteration 152, loss = 0.54375363\n",
      "Iteration 153, loss = 0.54051571\n",
      "Iteration 154, loss = 0.53728562\n",
      "Iteration 155, loss = 0.53403401\n",
      "Iteration 156, loss = 0.53078652\n",
      "Iteration 157, loss = 0.52752716\n",
      "Iteration 158, loss = 0.52427184\n",
      "Iteration 159, loss = 0.52100694\n",
      "Iteration 160, loss = 0.51774279\n",
      "Iteration 161, loss = 0.51447423\n",
      "Iteration 162, loss = 0.51119805\n",
      "Iteration 163, loss = 0.50791802\n",
      "Iteration 164, loss = 0.50463374\n",
      "Iteration 165, loss = 0.50135129\n",
      "Iteration 166, loss = 0.49806800\n",
      "Iteration 167, loss = 0.49479583\n",
      "Iteration 168, loss = 0.49151140\n",
      "Iteration 169, loss = 0.48823571\n",
      "Iteration 170, loss = 0.48495790\n",
      "Iteration 171, loss = 0.48168859\n",
      "Iteration 172, loss = 0.47842127\n",
      "Iteration 173, loss = 0.47516588\n",
      "Iteration 174, loss = 0.47191589\n",
      "Iteration 175, loss = 0.46867531\n",
      "Iteration 176, loss = 0.46544573\n",
      "Iteration 177, loss = 0.46223088\n",
      "Iteration 178, loss = 0.45902986\n",
      "Iteration 179, loss = 0.45583204\n",
      "Iteration 180, loss = 0.45265354\n",
      "Iteration 181, loss = 0.44948108\n",
      "Iteration 182, loss = 0.44632456\n",
      "Iteration 183, loss = 0.44318423\n",
      "Iteration 184, loss = 0.44006008\n",
      "Iteration 185, loss = 0.43695108\n",
      "Iteration 186, loss = 0.43386133\n",
      "Iteration 187, loss = 0.43079010\n",
      "Iteration 188, loss = 0.42773869\n",
      "Iteration 189, loss = 0.42470756\n",
      "Iteration 190, loss = 0.42169736\n",
      "Iteration 191, loss = 0.41870870\n",
      "Iteration 192, loss = 0.41574249\n",
      "Iteration 193, loss = 0.41280351\n",
      "Iteration 194, loss = 0.40988044\n",
      "Iteration 195, loss = 0.40698004\n",
      "Iteration 196, loss = 0.40410725\n",
      "Iteration 197, loss = 0.40126360\n",
      "Iteration 198, loss = 0.39844744\n",
      "Iteration 199, loss = 0.39565636\n",
      "Iteration 200, loss = 0.39290385\n",
      "Iteration 201, loss = 0.39016617\n",
      "Iteration 202, loss = 0.38745940\n",
      "Iteration 203, loss = 0.38478359\n",
      "Iteration 204, loss = 0.38214427\n",
      "Iteration 205, loss = 0.37955790\n",
      "Iteration 206, loss = 0.37700355\n",
      "Iteration 207, loss = 0.37447715\n",
      "Iteration 208, loss = 0.37198477\n",
      "Iteration 209, loss = 0.36951978\n",
      "Iteration 210, loss = 0.36708670\n",
      "Iteration 211, loss = 0.36468371\n",
      "Iteration 212, loss = 0.36231173\n",
      "Iteration 213, loss = 0.35997024\n",
      "Iteration 214, loss = 0.35765687\n",
      "Iteration 215, loss = 0.35537638\n",
      "Iteration 216, loss = 0.35313444\n",
      "Iteration 217, loss = 0.35093687\n",
      "Iteration 218, loss = 0.34877730\n",
      "Iteration 219, loss = 0.34665072\n",
      "Iteration 220, loss = 0.34455954\n",
      "Iteration 221, loss = 0.34250001\n",
      "Iteration 222, loss = 0.34048006\n",
      "Iteration 223, loss = 0.33848518\n",
      "Iteration 224, loss = 0.33651693\n",
      "Iteration 225, loss = 0.33458005\n",
      "Iteration 226, loss = 0.33266058\n",
      "Iteration 227, loss = 0.33076702\n",
      "Iteration 228, loss = 0.32889980\n",
      "Iteration 229, loss = 0.32705547\n",
      "Iteration 230, loss = 0.32523629\n",
      "Iteration 231, loss = 0.32344233\n",
      "Iteration 232, loss = 0.32167089\n",
      "Iteration 233, loss = 0.31992363\n",
      "Iteration 234, loss = 0.31820099\n",
      "Iteration 235, loss = 0.31650524\n",
      "Iteration 236, loss = 0.31483302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 237, loss = 0.31318331\n",
      "Iteration 238, loss = 0.31155735\n",
      "Iteration 239, loss = 0.30995427\n",
      "Iteration 240, loss = 0.30837452\n",
      "Iteration 241, loss = 0.30681582\n",
      "Iteration 242, loss = 0.30527921\n",
      "Iteration 243, loss = 0.30376572\n",
      "Iteration 244, loss = 0.30227255\n",
      "Iteration 245, loss = 0.30080091\n",
      "Iteration 246, loss = 0.29934950\n",
      "Iteration 247, loss = 0.29792007\n",
      "Iteration 248, loss = 0.29651083\n",
      "Iteration 249, loss = 0.29512292\n",
      "Iteration 250, loss = 0.29375656\n",
      "Iteration 251, loss = 0.29241186\n",
      "Iteration 252, loss = 0.29108634\n",
      "Iteration 253, loss = 0.28977951\n",
      "Iteration 254, loss = 0.28849133\n",
      "Iteration 255, loss = 0.28722272\n",
      "Iteration 256, loss = 0.28596995\n",
      "Iteration 257, loss = 0.28473746\n",
      "Iteration 258, loss = 0.28352368\n",
      "Iteration 259, loss = 0.28232718\n",
      "Iteration 260, loss = 0.28114724\n",
      "Iteration 261, loss = 0.27998427\n",
      "Iteration 262, loss = 0.27883727\n",
      "Iteration 263, loss = 0.27770716\n",
      "Iteration 264, loss = 0.27659282\n",
      "Iteration 265, loss = 0.27549590\n",
      "Iteration 266, loss = 0.27441417\n",
      "Iteration 267, loss = 0.27334756\n",
      "Iteration 268, loss = 0.27229806\n",
      "Iteration 269, loss = 0.27126649\n",
      "Iteration 270, loss = 0.27025489\n",
      "Iteration 271, loss = 0.26926003\n",
      "Iteration 272, loss = 0.26827961\n",
      "Iteration 273, loss = 0.26731229\n",
      "Iteration 274, loss = 0.26635759\n",
      "Iteration 275, loss = 0.26541532\n",
      "Iteration 276, loss = 0.26448597\n",
      "Iteration 277, loss = 0.26356742\n",
      "Iteration 278, loss = 0.26266136\n",
      "Iteration 279, loss = 0.26176699\n",
      "Iteration 280, loss = 0.26088415\n",
      "Iteration 281, loss = 0.26001266\n",
      "Iteration 282, loss = 0.25915237\n",
      "Iteration 283, loss = 0.25830310\n",
      "Iteration 284, loss = 0.25746463\n",
      "Iteration 285, loss = 0.25663682\n",
      "Iteration 286, loss = 0.25581956\n",
      "Iteration 287, loss = 0.25501266\n",
      "Iteration 288, loss = 0.25421599\n",
      "Iteration 289, loss = 0.25342932\n",
      "Iteration 290, loss = 0.25265260\n",
      "Iteration 291, loss = 0.25188562\n",
      "Iteration 292, loss = 0.25112822\n",
      "Iteration 293, loss = 0.25038026\n",
      "Iteration 294, loss = 0.24964302\n",
      "Iteration 295, loss = 0.24891506\n",
      "Iteration 296, loss = 0.24819609\n",
      "Iteration 297, loss = 0.24748590\n",
      "Iteration 298, loss = 0.24678438\n",
      "Iteration 299, loss = 0.24609142\n",
      "Iteration 300, loss = 0.24540766\n",
      "Iteration 301, loss = 0.24473195\n",
      "Iteration 302, loss = 0.24406454\n",
      "Iteration 303, loss = 0.24340507\n",
      "Iteration 304, loss = 0.24275333\n",
      "Iteration 305, loss = 0.24210925\n",
      "Iteration 306, loss = 0.24147275\n",
      "Iteration 307, loss = 0.24084371\n",
      "Iteration 308, loss = 0.24022244\n",
      "Iteration 309, loss = 0.23960869\n",
      "Iteration 310, loss = 0.23900229\n",
      "Iteration 311, loss = 0.23840336\n",
      "Iteration 312, loss = 0.23781152\n",
      "Iteration 313, loss = 0.23722620\n",
      "Iteration 314, loss = 0.23664788\n",
      "Iteration 315, loss = 0.23607601\n",
      "Iteration 316, loss = 0.23551068\n",
      "Iteration 317, loss = 0.23495172\n",
      "Iteration 318, loss = 0.23439927\n",
      "Iteration 319, loss = 0.23385300\n",
      "Iteration 320, loss = 0.23331261\n",
      "Iteration 321, loss = 0.23277810\n",
      "Iteration 322, loss = 0.23224999\n",
      "Iteration 323, loss = 0.23172716\n",
      "Iteration 324, loss = 0.23121011\n",
      "Iteration 325, loss = 0.23069913\n",
      "Iteration 326, loss = 0.23019321\n",
      "Iteration 327, loss = 0.22969278\n",
      "Iteration 328, loss = 0.22919797\n",
      "Iteration 329, loss = 0.22870832\n",
      "Iteration 330, loss = 0.22822373\n",
      "Iteration 331, loss = 0.22774436\n",
      "Iteration 332, loss = 0.22727025\n",
      "Iteration 333, loss = 0.22680081\n",
      "Iteration 334, loss = 0.22633622\n",
      "Iteration 335, loss = 0.22587693\n",
      "Iteration 336, loss = 0.22542197\n",
      "Iteration 337, loss = 0.22497160\n",
      "Iteration 338, loss = 0.22452673\n",
      "Iteration 339, loss = 0.22408524\n",
      "Iteration 340, loss = 0.22364934\n",
      "Iteration 341, loss = 0.22321767\n",
      "Iteration 342, loss = 0.22279012\n",
      "Iteration 343, loss = 0.22236679\n",
      "Iteration 344, loss = 0.22194804\n",
      "Iteration 345, loss = 0.22153309\n",
      "Iteration 346, loss = 0.22112244\n",
      "Iteration 347, loss = 0.22071605\n",
      "Iteration 348, loss = 0.22031338\n",
      "Iteration 349, loss = 0.21991463\n",
      "Iteration 350, loss = 0.21952050\n",
      "Iteration 351, loss = 0.21912923\n",
      "Iteration 352, loss = 0.21874272\n",
      "Iteration 353, loss = 0.21835946\n",
      "Iteration 354, loss = 0.21797974\n",
      "Iteration 355, loss = 0.21760370\n",
      "Iteration 356, loss = 0.21723154\n",
      "Iteration 357, loss = 0.21686273\n",
      "Iteration 358, loss = 0.21649784\n",
      "Iteration 359, loss = 0.21613576\n",
      "Iteration 360, loss = 0.21577773\n",
      "Iteration 361, loss = 0.21542266\n",
      "Iteration 362, loss = 0.21507089\n",
      "Iteration 363, loss = 0.21472282\n",
      "Iteration 364, loss = 0.21437737\n",
      "Iteration 365, loss = 0.21403548\n",
      "Iteration 366, loss = 0.21369665\n",
      "Iteration 367, loss = 0.21336078\n",
      "Iteration 368, loss = 0.21302814\n",
      "Iteration 369, loss = 0.21269843\n",
      "Iteration 370, loss = 0.21237159\n",
      "Iteration 371, loss = 0.21204810\n",
      "Iteration 372, loss = 0.21172711\n",
      "Iteration 373, loss = 0.21140899\n",
      "Iteration 374, loss = 0.21109393\n",
      "Iteration 375, loss = 0.21078136\n",
      "Iteration 376, loss = 0.21047205\n",
      "Iteration 377, loss = 0.21016486\n",
      "Iteration 378, loss = 0.20986077\n",
      "Iteration 379, loss = 0.20955909\n",
      "Iteration 380, loss = 0.20925995\n",
      "Iteration 381, loss = 0.20896375\n",
      "Iteration 382, loss = 0.20866974\n",
      "Iteration 383, loss = 0.20837855\n",
      "Iteration 384, loss = 0.20808947\n",
      "Iteration 385, loss = 0.20780332\n",
      "Iteration 386, loss = 0.20751911\n",
      "Iteration 387, loss = 0.20723766\n",
      "Iteration 388, loss = 0.20695838\n",
      "Iteration 389, loss = 0.20668138\n",
      "Iteration 390, loss = 0.20640704\n",
      "Iteration 391, loss = 0.20613463\n",
      "Iteration 392, loss = 0.20586447\n",
      "Iteration 393, loss = 0.20559672\n",
      "Iteration 394, loss = 0.20533094\n",
      "Iteration 395, loss = 0.20506766\n",
      "Iteration 396, loss = 0.20480637\n",
      "Iteration 397, loss = 0.20454741\n",
      "Iteration 398, loss = 0.20429048\n",
      "Iteration 399, loss = 0.20403581\n",
      "Iteration 400, loss = 0.20378300\n",
      "Iteration 401, loss = 0.20353240\n",
      "Iteration 402, loss = 0.20328373\n",
      "Iteration 403, loss = 0.20303689\n",
      "Iteration 404, loss = 0.20279231\n",
      "Iteration 405, loss = 0.20254929\n",
      "Iteration 406, loss = 0.20230853\n",
      "Iteration 407, loss = 0.20206939\n",
      "Iteration 408, loss = 0.20183212\n",
      "Iteration 409, loss = 0.20159690\n",
      "Iteration 410, loss = 0.20136333\n",
      "Iteration 411, loss = 0.20113158\n",
      "Iteration 412, loss = 0.20090168\n",
      "Iteration 413, loss = 0.20067357\n",
      "Iteration 414, loss = 0.20044716\n",
      "Iteration 415, loss = 0.20022226\n",
      "Iteration 416, loss = 0.19999949\n",
      "Iteration 417, loss = 0.19977800\n",
      "Iteration 418, loss = 0.19955827\n",
      "Iteration 419, loss = 0.19934016\n",
      "Iteration 420, loss = 0.19912391\n",
      "Iteration 421, loss = 0.19890898\n",
      "Iteration 422, loss = 0.19869569\n",
      "Iteration 423, loss = 0.19848401\n",
      "Iteration 424, loss = 0.19827388\n",
      "Iteration 425, loss = 0.19806529\n",
      "Iteration 426, loss = 0.19785823\n",
      "Iteration 427, loss = 0.19765275\n",
      "Iteration 428, loss = 0.19744858\n",
      "Iteration 429, loss = 0.19724614\n",
      "Iteration 430, loss = 0.19704483\n",
      "Iteration 431, loss = 0.19684519\n",
      "Iteration 432, loss = 0.19664681\n",
      "Iteration 433, loss = 0.19644982\n",
      "Iteration 434, loss = 0.19625437\n",
      "Iteration 435, loss = 0.19606010\n",
      "Iteration 436, loss = 0.19586741\n",
      "Iteration 437, loss = 0.19567580\n",
      "Iteration 438, loss = 0.19548581\n",
      "Iteration 439, loss = 0.19529693\n",
      "Iteration 440, loss = 0.19510930\n",
      "Iteration 441, loss = 0.19492311\n",
      "Iteration 442, loss = 0.19473801\n",
      "Iteration 443, loss = 0.19455439\n",
      "Iteration 444, loss = 0.19437182\n",
      "Iteration 445, loss = 0.19419055\n",
      "Iteration 446, loss = 0.19401060\n",
      "Iteration 447, loss = 0.19383172\n",
      "Iteration 448, loss = 0.19365399\n",
      "Iteration 449, loss = 0.19347770\n",
      "Iteration 450, loss = 0.19330225\n",
      "Iteration 451, loss = 0.19312819\n",
      "Iteration 452, loss = 0.19295516\n",
      "Iteration 453, loss = 0.19278322\n",
      "Iteration 454, loss = 0.19261250\n",
      "Iteration 455, loss = 0.19244282\n",
      "Iteration 456, loss = 0.19227425\n",
      "Iteration 457, loss = 0.19210684\n",
      "Iteration 458, loss = 0.19194047\n",
      "Iteration 459, loss = 0.19177513\n",
      "Iteration 460, loss = 0.19161100\n",
      "Iteration 461, loss = 0.19144780\n",
      "Iteration 462, loss = 0.19128567\n",
      "Iteration 463, loss = 0.19112458\n",
      "Iteration 464, loss = 0.19096444\n",
      "Iteration 465, loss = 0.19080542\n",
      "Iteration 466, loss = 0.19064726\n",
      "Iteration 467, loss = 0.19049011\n",
      "Iteration 468, loss = 0.19033406\n",
      "Iteration 469, loss = 0.19017887\n",
      "Iteration 470, loss = 0.19002463\n",
      "Iteration 471, loss = 0.18987133\n",
      "Iteration 472, loss = 0.18971902\n",
      "Iteration 473, loss = 0.18956759\n",
      "Iteration 474, loss = 0.18941710\n",
      "Iteration 475, loss = 0.18926756\n",
      "Iteration 476, loss = 0.18911885\n",
      "Iteration 477, loss = 0.18897102\n",
      "Iteration 478, loss = 0.18882419\n",
      "Iteration 479, loss = 0.18867816\n",
      "Iteration 480, loss = 0.18853296\n",
      "Iteration 481, loss = 0.18838866\n",
      "Iteration 482, loss = 0.18824522\n",
      "Iteration 483, loss = 0.18810258\n",
      "Iteration 484, loss = 0.18796078\n",
      "Iteration 485, loss = 0.18781989\n",
      "Iteration 486, loss = 0.18767974\n",
      "Iteration 487, loss = 0.18754039\n",
      "Iteration 488, loss = 0.18740186\n",
      "Iteration 489, loss = 0.18726418\n",
      "Iteration 490, loss = 0.18712724\n",
      "Iteration 491, loss = 0.18699104\n",
      "Iteration 492, loss = 0.18685572\n",
      "Iteration 493, loss = 0.18672106\n",
      "Iteration 494, loss = 0.18658719\n",
      "Iteration 495, loss = 0.18645411\n",
      "Iteration 496, loss = 0.18632184\n",
      "Iteration 497, loss = 0.18619023\n",
      "Iteration 498, loss = 0.18605935\n",
      "Iteration 499, loss = 0.18592925\n",
      "Iteration 500, loss = 0.18579987\n",
      "Iteration 501, loss = 0.18567114\n",
      "Iteration 502, loss = 0.18554320\n",
      "Iteration 503, loss = 0.18541589\n",
      "Iteration 504, loss = 0.18528926\n",
      "Iteration 505, loss = 0.18516346\n",
      "Iteration 506, loss = 0.18503815\n",
      "Iteration 507, loss = 0.18491369\n",
      "Iteration 508, loss = 0.18478980\n",
      "Iteration 509, loss = 0.18466655\n",
      "Iteration 510, loss = 0.18454400\n",
      "Iteration 511, loss = 0.18442207\n",
      "Iteration 512, loss = 0.18430083\n",
      "Iteration 513, loss = 0.18418021\n",
      "Iteration 514, loss = 0.18406019\n",
      "Iteration 515, loss = 0.18394091\n",
      "Iteration 516, loss = 0.18382210\n",
      "Iteration 517, loss = 0.18370403\n",
      "Iteration 518, loss = 0.18358651\n",
      "Iteration 519, loss = 0.18346958\n",
      "Iteration 520, loss = 0.18335335\n",
      "Iteration 521, loss = 0.18323760\n",
      "Iteration 522, loss = 0.18312250\n",
      "Iteration 523, loss = 0.18300800\n",
      "Iteration 524, loss = 0.18289404\n",
      "Iteration 525, loss = 0.18278066\n",
      "Iteration 526, loss = 0.18266789\n",
      "Iteration 527, loss = 0.18255564\n",
      "Iteration 528, loss = 0.18244402\n",
      "Iteration 529, loss = 0.18233286\n",
      "Iteration 530, loss = 0.18222233\n",
      "Iteration 531, loss = 0.18211233\n",
      "Iteration 532, loss = 0.18200284\n",
      "Iteration 533, loss = 0.18189393\n",
      "Iteration 534, loss = 0.18178553\n",
      "Iteration 535, loss = 0.18167764\n",
      "Iteration 536, loss = 0.18157038\n",
      "Iteration 537, loss = 0.18146349\n",
      "Iteration 538, loss = 0.18135726\n",
      "Iteration 539, loss = 0.18125147\n",
      "Iteration 540, loss = 0.18114616\n",
      "Iteration 541, loss = 0.18104135\n",
      "Iteration 542, loss = 0.18093711\n",
      "Iteration 543, loss = 0.18083330\n",
      "Iteration 544, loss = 0.18073007\n",
      "Iteration 545, loss = 0.18062720\n",
      "Iteration 546, loss = 0.18052494\n",
      "Iteration 547, loss = 0.18042307\n",
      "Iteration 548, loss = 0.18032170\n",
      "Iteration 549, loss = 0.18022087\n",
      "Iteration 550, loss = 0.18012043\n",
      "Iteration 551, loss = 0.18002045\n",
      "Iteration 552, loss = 0.17992100\n",
      "Iteration 553, loss = 0.17982191\n",
      "Iteration 554, loss = 0.17972339\n",
      "Iteration 555, loss = 0.17962522\n",
      "Iteration 556, loss = 0.17952754\n",
      "Iteration 557, loss = 0.17943034\n",
      "Iteration 558, loss = 0.17933353\n",
      "Iteration 559, loss = 0.17923714\n",
      "Iteration 560, loss = 0.17914128\n",
      "Iteration 561, loss = 0.17904573\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Skuteczno (train): 97.5%\n",
      "Skuteczno (test): 96.66666666666667%\n",
      "[[40  0  0]\n",
      " [ 1 38  1]\n",
      " [ 0  0 40]]\n",
      "[[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  1  9]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Wczytanie danych iris\n",
    "dt = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "dt.columns = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width', 'Class label']\n",
    "x = dt[['Sepal length', 'Sepal width', 'Petal length', 'Petal width']].values\n",
    "y1 = pd.factorize(dt['Class label'])[0] #zmiana nazw klas na liczby\n",
    "y = y1.reshape(-1,1) #uoenie w kolumn\n",
    "\n",
    "hotone = OneHotEncoder()\n",
    "hotone.fit(y)\n",
    "y_encoded = hotone.transform(y).toarray()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=8, stratify=y)\n",
    "\n",
    "mlp = MLPClassifier(alpha=0.1, hidden_layer_sizes=20, learning_rate_init=0.001, max_iter=1000, random_state=1, solver=\"sgd\", verbose=10)\n",
    "\n",
    "parametry = {'learning_rate_init' : (0.1,0.01,0.001), 'hidden_layer_sizes' : [20, 40, 60, 80, 100], 'solver': ['adam', 'lbfgs', 'sgd']}\n",
    "\n",
    "clf = GridSearchCV(mlp, parametry) #Poszukiwanie najlepszych parametrw\n",
    "clf.fit(x,y1)\n",
    "print(\"Best params: \" + str(clf.best_params_))\n",
    "\n",
    "# Uruchomienie z najlepszymi parametrami\n",
    "mlp = MLPClassifier(alpha=0.1, hidden_layer_sizes=clf.best_params_['hidden_layer_sizes'],\n",
    "                    learning_rate_init=clf.best_params_['learning_rate_init'], max_iter=1000, random_state=1,\n",
    "                    solver=clf.best_params_['solver'], verbose=10)\n",
    "mlp.fit(x_train, y_train)\n",
    "\n",
    "print(\"Skuteczno (train): \" + str(mlp.score(x_train,y_train)*100) + '%') \n",
    "print(\"Skuteczno (test): \" + str(mlp.score(x_test,y_test)*100) + '%')\n",
    "\n",
    "Y_pred_train = mlp.predict(x_train)\n",
    "print(confusion_matrix(y_train.argmax(axis=1),Y_pred_train.argmax(axis=1)))\n",
    "Y_pred_test = mlp.predict(x_test)\n",
    "print(confusion_matrix(y_test.argmax(axis=1),Y_pred_test.argmax(axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 890)\n",
      "Iteration 1, loss = 2.42498720\n",
      "Iteration 2, loss = 1.05595824\n",
      "Iteration 3, loss = 0.68806434\n",
      "Iteration 4, loss = 0.53480530\n",
      "Iteration 5, loss = 0.42046984\n",
      "Iteration 6, loss = 0.34097320\n",
      "Iteration 7, loss = 0.24984552\n",
      "Iteration 8, loss = 0.20866523\n",
      "Iteration 9, loss = 0.16883390\n",
      "Iteration 10, loss = 0.15054699\n",
      "Iteration 11, loss = 0.16309979\n",
      "Iteration 12, loss = 0.17524360\n",
      "Iteration 13, loss = 0.24535465\n",
      "Iteration 14, loss = 0.34126804\n",
      "Iteration 15, loss = 0.36396553\n",
      "Iteration 16, loss = 0.40010736\n",
      "Iteration 17, loss = 0.40258511\n",
      "Iteration 18, loss = 0.40312291\n",
      "Iteration 19, loss = 0.38215558\n",
      "Iteration 20, loss = 0.31407150\n",
      "Iteration 21, loss = 0.25693487\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.62646265\n",
      "Iteration 2, loss = 1.01828269\n",
      "Iteration 3, loss = 0.72342915\n",
      "Iteration 4, loss = 0.55885638\n",
      "Iteration 5, loss = 0.43357203\n",
      "Iteration 6, loss = 0.33336715\n",
      "Iteration 7, loss = 0.25168740\n",
      "Iteration 8, loss = 0.19987305\n",
      "Iteration 9, loss = 0.16568185\n",
      "Iteration 10, loss = 0.14584419\n",
      "Iteration 11, loss = 0.16921038\n",
      "Iteration 12, loss = 0.21945577\n",
      "Iteration 13, loss = 0.35917543\n",
      "Iteration 14, loss = 0.41573742\n",
      "Iteration 15, loss = 0.42988520\n",
      "Iteration 16, loss = 0.38205889\n",
      "Iteration 17, loss = 0.35220790\n",
      "Iteration 18, loss = 0.29704501\n",
      "Iteration 19, loss = 0.25206157\n",
      "Iteration 20, loss = 0.20016357\n",
      "Iteration 21, loss = 0.16887908\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.88971613\n",
      "Iteration 2, loss = 1.44444687\n",
      "Iteration 3, loss = 0.94109301\n",
      "Iteration 4, loss = 0.65062266\n",
      "Iteration 5, loss = 0.49879553\n",
      "Iteration 6, loss = 0.38665402\n",
      "Iteration 7, loss = 0.29493503\n",
      "Iteration 8, loss = 0.22014774\n",
      "Iteration 9, loss = 0.17288285\n",
      "Iteration 10, loss = 0.14183267\n",
      "Iteration 11, loss = 0.12219767\n",
      "Iteration 12, loss = 0.12405082\n",
      "Iteration 13, loss = 0.13018808\n",
      "Iteration 14, loss = 0.14386398\n",
      "Iteration 15, loss = 0.14129014\n",
      "Iteration 16, loss = 0.22526557\n",
      "Iteration 17, loss = 0.31008567\n",
      "Iteration 18, loss = 0.37277878\n",
      "Iteration 19, loss = 0.44427155\n",
      "Iteration 20, loss = 0.42253588\n",
      "Iteration 21, loss = 0.39787763\n",
      "Iteration 22, loss = 0.37726737\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.58774228\n",
      "Iteration 2, loss = 1.63910210\n",
      "Iteration 3, loss = 1.13353630\n",
      "Iteration 4, loss = 0.81101112\n",
      "Iteration 5, loss = 0.59271699\n",
      "Iteration 6, loss = 0.45455511\n",
      "Iteration 7, loss = 0.36735186\n",
      "Iteration 8, loss = 0.30287524\n",
      "Iteration 9, loss = 0.26457656\n",
      "Iteration 10, loss = 0.21999977\n",
      "Iteration 11, loss = 0.20209118\n",
      "Iteration 12, loss = 0.20420768\n",
      "Iteration 13, loss = 0.21707986\n",
      "Iteration 14, loss = 0.25043291\n",
      "Iteration 15, loss = 0.36090769\n",
      "Iteration 16, loss = 0.42709006\n",
      "Iteration 17, loss = 0.50367572\n",
      "Iteration 18, loss = 0.46130950\n",
      "Iteration 19, loss = 0.40340962\n",
      "Iteration 20, loss = 0.35784422\n",
      "Iteration 21, loss = 0.34002235\n",
      "Iteration 22, loss = 0.30682417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.63307504\n",
      "Iteration 2, loss = 1.35171919\n",
      "Iteration 3, loss = 0.78069596\n",
      "Iteration 4, loss = 0.58439414\n",
      "Iteration 5, loss = 0.48573496\n",
      "Iteration 6, loss = 0.38122365\n",
      "Iteration 7, loss = 0.31729483\n",
      "Iteration 8, loss = 0.27817152\n",
      "Iteration 9, loss = 0.29495789\n",
      "Iteration 10, loss = 0.26681100\n",
      "Iteration 11, loss = 0.32297199\n",
      "Iteration 12, loss = 0.31568004\n",
      "Iteration 13, loss = 0.28631462\n",
      "Iteration 14, loss = 0.26533985\n",
      "Iteration 15, loss = 0.31191698\n",
      "Iteration 16, loss = 0.30284337\n",
      "Iteration 17, loss = 0.29213787\n",
      "Iteration 18, loss = 0.24965738\n",
      "Iteration 19, loss = 0.25293899\n",
      "Iteration 20, loss = 0.24645692\n",
      "Iteration 21, loss = 0.24733011\n",
      "Iteration 22, loss = 0.29580125\n",
      "Iteration 23, loss = 0.34136318\n",
      "Iteration 24, loss = 0.30539761\n",
      "Iteration 25, loss = 0.39049766\n",
      "Iteration 26, loss = 0.37657481\n",
      "Iteration 27, loss = 0.38116077\n",
      "Iteration 28, loss = 0.38317122\n",
      "Iteration 29, loss = 0.35857596\n",
      "Iteration 30, loss = 0.32462375\n",
      "Iteration 31, loss = 0.28789869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15950945\n",
      "Iteration 2, loss = 1.19706844\n",
      "Iteration 3, loss = 0.52689596\n",
      "Iteration 4, loss = 0.30203617\n",
      "Iteration 5, loss = 0.19212049\n",
      "Iteration 6, loss = 0.14049812\n",
      "Iteration 7, loss = 0.11501365\n",
      "Iteration 8, loss = 0.08995642\n",
      "Iteration 9, loss = 0.07475982\n",
      "Iteration 10, loss = 0.06753835\n",
      "Iteration 11, loss = 0.06361380\n",
      "Iteration 12, loss = 0.06025336\n",
      "Iteration 13, loss = 0.05830183\n",
      "Iteration 14, loss = 0.05648141\n",
      "Iteration 15, loss = 0.05516011\n",
      "Iteration 16, loss = 0.05412192\n",
      "Iteration 17, loss = 0.05332148\n",
      "Iteration 18, loss = 0.05270639\n",
      "Iteration 19, loss = 0.05234161\n",
      "Iteration 20, loss = 0.05137817\n",
      "Iteration 21, loss = 0.05097291\n",
      "Iteration 22, loss = 0.05059125\n",
      "Iteration 23, loss = 0.05017417\n",
      "Iteration 24, loss = 0.04995948\n",
      "Iteration 25, loss = 0.04953254\n",
      "Iteration 26, loss = 0.04923678\n",
      "Iteration 27, loss = 0.04892008\n",
      "Iteration 28, loss = 0.04866815\n",
      "Iteration 29, loss = 0.04845229\n",
      "Iteration 30, loss = 0.04820778\n",
      "Iteration 31, loss = 0.04806416\n",
      "Iteration 32, loss = 0.04787852\n",
      "Iteration 33, loss = 0.04756984\n",
      "Iteration 34, loss = 0.04738763\n",
      "Iteration 35, loss = 0.04723621\n",
      "Iteration 36, loss = 0.04700547\n",
      "Iteration 37, loss = 0.04691014\n",
      "Iteration 38, loss = 0.04668431\n",
      "Iteration 39, loss = 0.04660078\n",
      "Iteration 40, loss = 0.04641321\n",
      "Iteration 41, loss = 0.04626066\n",
      "Iteration 42, loss = 0.04603724\n",
      "Iteration 43, loss = 0.04593556\n",
      "Iteration 44, loss = 0.04578796\n",
      "Iteration 45, loss = 0.04566954\n",
      "Iteration 46, loss = 0.04551691\n",
      "Iteration 47, loss = 0.04538390\n",
      "Iteration 48, loss = 0.04527022\n",
      "Iteration 49, loss = 0.04512003\n",
      "Iteration 50, loss = 0.04497926\n",
      "Iteration 51, loss = 0.04492674\n",
      "Iteration 52, loss = 0.04475772\n",
      "Iteration 53, loss = 0.04465887\n",
      "Iteration 54, loss = 0.04453483\n",
      "Iteration 55, loss = 0.04442799\n",
      "Iteration 56, loss = 0.04441335\n",
      "Iteration 57, loss = 0.04423874\n",
      "Iteration 58, loss = 0.04410523\n",
      "Iteration 59, loss = 0.04397605\n",
      "Iteration 60, loss = 0.04388607\n",
      "Iteration 61, loss = 0.04378054\n",
      "Iteration 62, loss = 0.04366141\n",
      "Iteration 63, loss = 0.04358742\n",
      "Iteration 64, loss = 0.04349665\n",
      "Iteration 65, loss = 0.04340621\n",
      "Iteration 66, loss = 0.04331446\n",
      "Iteration 67, loss = 0.04322412\n",
      "Iteration 68, loss = 0.04317034\n",
      "Iteration 69, loss = 0.04305991\n",
      "Iteration 70, loss = 0.04299911\n",
      "Iteration 71, loss = 0.04287149\n",
      "Iteration 72, loss = 0.04285260\n",
      "Iteration 73, loss = 0.04270172\n",
      "Iteration 74, loss = 0.04264706\n",
      "Iteration 75, loss = 0.04254628\n",
      "Iteration 76, loss = 0.04250222\n",
      "Iteration 77, loss = 0.04249985\n",
      "Iteration 78, loss = 0.04235041\n",
      "Iteration 79, loss = 0.04227184\n",
      "Iteration 80, loss = 0.04226904\n",
      "Iteration 81, loss = 0.04208920\n",
      "Iteration 82, loss = 0.04203206\n",
      "Iteration 83, loss = 0.04198303\n",
      "Iteration 84, loss = 0.04190356\n",
      "Iteration 85, loss = 0.04181497\n",
      "Iteration 86, loss = 0.04177094\n",
      "Iteration 87, loss = 0.04167465\n",
      "Iteration 88, loss = 0.04163164\n",
      "Iteration 89, loss = 0.04160213\n",
      "Iteration 90, loss = 0.04149814\n",
      "Iteration 91, loss = 0.04146022\n",
      "Iteration 92, loss = 0.04141169\n",
      "Iteration 93, loss = 0.04138277\n",
      "Iteration 94, loss = 0.04125053\n",
      "Iteration 95, loss = 0.04120429\n",
      "Iteration 96, loss = 0.04111767\n",
      "Iteration 97, loss = 0.04109627\n",
      "Iteration 98, loss = 0.04104656\n",
      "Iteration 99, loss = 0.04096560\n",
      "Iteration 100, loss = 0.04089626\n",
      "Iteration 101, loss = 0.04086153\n",
      "Iteration 102, loss = 0.04077933\n",
      "Iteration 103, loss = 0.04073463\n",
      "Iteration 104, loss = 0.04068794\n",
      "Iteration 105, loss = 0.04064713\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15173987\n",
      "Iteration 2, loss = 1.16027132\n",
      "Iteration 3, loss = 0.49915649\n",
      "Iteration 4, loss = 0.28762505\n",
      "Iteration 5, loss = 0.19138953\n",
      "Iteration 6, loss = 0.14335086\n",
      "Iteration 7, loss = 0.11993633\n",
      "Iteration 8, loss = 0.08928917\n",
      "Iteration 9, loss = 0.07799114\n",
      "Iteration 10, loss = 0.06813393\n",
      "Iteration 11, loss = 0.06442947\n",
      "Iteration 12, loss = 0.05955580\n",
      "Iteration 13, loss = 0.05723415\n",
      "Iteration 14, loss = 0.05556197\n",
      "Iteration 15, loss = 0.05423774\n",
      "Iteration 16, loss = 0.05338351\n",
      "Iteration 17, loss = 0.05275732\n",
      "Iteration 18, loss = 0.05187530\n",
      "Iteration 19, loss = 0.05125259\n",
      "Iteration 20, loss = 0.05069892\n",
      "Iteration 21, loss = 0.05043975\n",
      "Iteration 22, loss = 0.04989406\n",
      "Iteration 23, loss = 0.04948112\n",
      "Iteration 24, loss = 0.04925470\n",
      "Iteration 25, loss = 0.04885200\n",
      "Iteration 26, loss = 0.04865586\n",
      "Iteration 27, loss = 0.04822816\n",
      "Iteration 28, loss = 0.04804200\n",
      "Iteration 29, loss = 0.04777304\n",
      "Iteration 30, loss = 0.04757124\n",
      "Iteration 31, loss = 0.04746650\n",
      "Iteration 32, loss = 0.04721428\n",
      "Iteration 33, loss = 0.04691655\n",
      "Iteration 34, loss = 0.04671650\n",
      "Iteration 35, loss = 0.04657535\n",
      "Iteration 36, loss = 0.04636973\n",
      "Iteration 37, loss = 0.04621643\n",
      "Iteration 38, loss = 0.04604389\n",
      "Iteration 39, loss = 0.04589251\n",
      "Iteration 40, loss = 0.04578123\n",
      "Iteration 41, loss = 0.04562492\n",
      "Iteration 42, loss = 0.04539361\n",
      "Iteration 43, loss = 0.04531798\n",
      "Iteration 44, loss = 0.04516752\n",
      "Iteration 45, loss = 0.04500451\n",
      "Iteration 46, loss = 0.04488810\n",
      "Iteration 47, loss = 0.04472938\n",
      "Iteration 48, loss = 0.04463301\n",
      "Iteration 49, loss = 0.04447870\n",
      "Iteration 50, loss = 0.04441056\n",
      "Iteration 51, loss = 0.04428326\n",
      "Iteration 52, loss = 0.04410933\n",
      "Iteration 53, loss = 0.04402262\n",
      "Iteration 54, loss = 0.04388534\n",
      "Iteration 55, loss = 0.04378813\n",
      "Iteration 56, loss = 0.04372539\n",
      "Iteration 57, loss = 0.04357299\n",
      "Iteration 58, loss = 0.04349046\n",
      "Iteration 59, loss = 0.04339023\n",
      "Iteration 60, loss = 0.04326311\n",
      "Iteration 61, loss = 0.04314349\n",
      "Iteration 62, loss = 0.04305779\n",
      "Iteration 63, loss = 0.04295222\n",
      "Iteration 64, loss = 0.04288018\n",
      "Iteration 65, loss = 0.04278892\n",
      "Iteration 66, loss = 0.04269737\n",
      "Iteration 67, loss = 0.04265239\n",
      "Iteration 68, loss = 0.04253786\n",
      "Iteration 69, loss = 0.04245428\n",
      "Iteration 70, loss = 0.04234248\n",
      "Iteration 71, loss = 0.04223610\n",
      "Iteration 72, loss = 0.04226940\n",
      "Iteration 73, loss = 0.04205628\n",
      "Iteration 74, loss = 0.04202473\n",
      "Iteration 75, loss = 0.04194065\n",
      "Iteration 76, loss = 0.04189975\n",
      "Iteration 77, loss = 0.04189433\n",
      "Iteration 78, loss = 0.04170314\n",
      "Iteration 79, loss = 0.04166439\n",
      "Iteration 80, loss = 0.04165530\n",
      "Iteration 81, loss = 0.04146334\n",
      "Iteration 82, loss = 0.04140179\n",
      "Iteration 83, loss = 0.04135939\n",
      "Iteration 84, loss = 0.04127725\n",
      "Iteration 85, loss = 0.04119996\n",
      "Iteration 86, loss = 0.04114892\n",
      "Iteration 87, loss = 0.04104928\n",
      "Iteration 88, loss = 0.04105082\n",
      "Iteration 89, loss = 0.04094183\n",
      "Iteration 90, loss = 0.04091419\n",
      "Iteration 91, loss = 0.04084916\n",
      "Iteration 92, loss = 0.04077173\n",
      "Iteration 93, loss = 0.04073015\n",
      "Iteration 94, loss = 0.04065334\n",
      "Iteration 95, loss = 0.04055202\n",
      "Iteration 96, loss = 0.04050203\n",
      "Iteration 97, loss = 0.04048513\n",
      "Iteration 98, loss = 0.04045138\n",
      "Iteration 99, loss = 0.04034762\n",
      "Iteration 100, loss = 0.04029784\n",
      "Iteration 101, loss = 0.04023767\n",
      "Iteration 102, loss = 0.04019801\n",
      "Iteration 103, loss = 0.04013685\n",
      "Iteration 104, loss = 0.04009399\n",
      "Iteration 105, loss = 0.04004477\n",
      "Iteration 106, loss = 0.04000021\n",
      "Iteration 107, loss = 0.03996075\n",
      "Iteration 108, loss = 0.03988230\n",
      "Iteration 109, loss = 0.03987408\n",
      "Iteration 110, loss = 0.03981286\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17230839\n",
      "Iteration 2, loss = 1.16142586\n",
      "Iteration 3, loss = 0.50650474\n",
      "Iteration 4, loss = 0.29459874\n",
      "Iteration 5, loss = 0.20029868\n",
      "Iteration 6, loss = 0.14147700\n",
      "Iteration 7, loss = 0.11338330\n",
      "Iteration 8, loss = 0.08822202\n",
      "Iteration 9, loss = 0.07742019\n",
      "Iteration 10, loss = 0.06932078\n",
      "Iteration 11, loss = 0.06438180\n",
      "Iteration 12, loss = 0.06076272\n",
      "Iteration 13, loss = 0.05827326\n",
      "Iteration 14, loss = 0.05682570\n",
      "Iteration 15, loss = 0.05511271\n",
      "Iteration 16, loss = 0.05420608\n",
      "Iteration 17, loss = 0.05351131\n",
      "Iteration 18, loss = 0.05273098\n",
      "Iteration 19, loss = 0.05202178\n",
      "Iteration 20, loss = 0.05168281\n",
      "Iteration 21, loss = 0.05099989\n",
      "Iteration 22, loss = 0.05073128\n",
      "Iteration 23, loss = 0.05029309\n",
      "Iteration 24, loss = 0.05002036\n",
      "Iteration 25, loss = 0.04964425\n",
      "Iteration 26, loss = 0.04939744\n",
      "Iteration 27, loss = 0.04915386\n",
      "Iteration 28, loss = 0.04884200\n",
      "Iteration 29, loss = 0.04859164\n",
      "Iteration 30, loss = 0.04827924\n",
      "Iteration 31, loss = 0.04808992\n",
      "Iteration 32, loss = 0.04788103\n",
      "Iteration 33, loss = 0.04771308\n",
      "Iteration 34, loss = 0.04745325\n",
      "Iteration 35, loss = 0.04727763\n",
      "Iteration 36, loss = 0.04712537\n",
      "Iteration 37, loss = 0.04692955\n",
      "Iteration 38, loss = 0.04678126\n",
      "Iteration 39, loss = 0.04659629\n",
      "Iteration 40, loss = 0.04646346\n",
      "Iteration 41, loss = 0.04630604\n",
      "Iteration 42, loss = 0.04622980\n",
      "Iteration 43, loss = 0.04601416\n",
      "Iteration 44, loss = 0.04588641\n",
      "Iteration 45, loss = 0.04570208\n",
      "Iteration 46, loss = 0.04563532\n",
      "Iteration 47, loss = 0.04548679\n",
      "Iteration 48, loss = 0.04529304\n",
      "Iteration 49, loss = 0.04523834\n",
      "Iteration 50, loss = 0.04508233\n",
      "Iteration 51, loss = 0.04497931\n",
      "Iteration 52, loss = 0.04483379\n",
      "Iteration 53, loss = 0.04470543\n",
      "Iteration 54, loss = 0.04464178\n",
      "Iteration 55, loss = 0.04449003\n",
      "Iteration 56, loss = 0.04443363\n",
      "Iteration 57, loss = 0.04432975\n",
      "Iteration 58, loss = 0.04416891\n",
      "Iteration 59, loss = 0.04411315\n",
      "Iteration 60, loss = 0.04395563\n",
      "Iteration 61, loss = 0.04386545\n",
      "Iteration 62, loss = 0.04374636\n",
      "Iteration 63, loss = 0.04374688\n",
      "Iteration 64, loss = 0.04356407\n",
      "Iteration 65, loss = 0.04345695\n",
      "Iteration 66, loss = 0.04340087\n",
      "Iteration 67, loss = 0.04330402\n",
      "Iteration 68, loss = 0.04320892\n",
      "Iteration 69, loss = 0.04315248\n",
      "Iteration 70, loss = 0.04308295\n",
      "Iteration 71, loss = 0.04296956\n",
      "Iteration 72, loss = 0.04286496\n",
      "Iteration 73, loss = 0.04278734\n",
      "Iteration 74, loss = 0.04271400\n",
      "Iteration 75, loss = 0.04264095\n",
      "Iteration 76, loss = 0.04254529\n",
      "Iteration 77, loss = 0.04247779\n",
      "Iteration 78, loss = 0.04238005\n",
      "Iteration 79, loss = 0.04233319\n",
      "Iteration 80, loss = 0.04225356\n",
      "Iteration 81, loss = 0.04215703\n",
      "Iteration 82, loss = 0.04209313\n",
      "Iteration 83, loss = 0.04202983\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17835481\n",
      "Iteration 2, loss = 1.21504013\n",
      "Iteration 3, loss = 0.54236527\n",
      "Iteration 4, loss = 0.30497909\n",
      "Iteration 5, loss = 0.20318605\n",
      "Iteration 6, loss = 0.14673890\n",
      "Iteration 7, loss = 0.11505968\n",
      "Iteration 8, loss = 0.09007991\n",
      "Iteration 9, loss = 0.07827906\n",
      "Iteration 10, loss = 0.07128291\n",
      "Iteration 11, loss = 0.06532656\n",
      "Iteration 12, loss = 0.06214012\n",
      "Iteration 13, loss = 0.05947747\n",
      "Iteration 14, loss = 0.05771509\n",
      "Iteration 15, loss = 0.05638910\n",
      "Iteration 16, loss = 0.05526184\n",
      "Iteration 17, loss = 0.05453533\n",
      "Iteration 18, loss = 0.05370568\n",
      "Iteration 19, loss = 0.05301155\n",
      "Iteration 20, loss = 0.05278375\n",
      "Iteration 21, loss = 0.05203978\n",
      "Iteration 22, loss = 0.05160011\n",
      "Iteration 23, loss = 0.05111133\n",
      "Iteration 24, loss = 0.05083079\n",
      "Iteration 25, loss = 0.05049039\n",
      "Iteration 26, loss = 0.05018856\n",
      "Iteration 27, loss = 0.04995187\n",
      "Iteration 28, loss = 0.04960907\n",
      "Iteration 29, loss = 0.04941385\n",
      "Iteration 30, loss = 0.04904376\n",
      "Iteration 31, loss = 0.04885072\n",
      "Iteration 32, loss = 0.04863977\n",
      "Iteration 33, loss = 0.04853112\n",
      "Iteration 34, loss = 0.04819343\n",
      "Iteration 35, loss = 0.04802583\n",
      "Iteration 36, loss = 0.04784506\n",
      "Iteration 37, loss = 0.04762775\n",
      "Iteration 38, loss = 0.04746775\n",
      "Iteration 39, loss = 0.04731743\n",
      "Iteration 40, loss = 0.04713299\n",
      "Iteration 41, loss = 0.04704679\n",
      "Iteration 42, loss = 0.04687249\n",
      "Iteration 43, loss = 0.04668346\n",
      "Iteration 44, loss = 0.04654265\n",
      "Iteration 45, loss = 0.04639438\n",
      "Iteration 46, loss = 0.04633671\n",
      "Iteration 47, loss = 0.04617196\n",
      "Iteration 48, loss = 0.04600736\n",
      "Iteration 49, loss = 0.04591286\n",
      "Iteration 50, loss = 0.04577770\n",
      "Iteration 51, loss = 0.04567277\n",
      "Iteration 52, loss = 0.04555135\n",
      "Iteration 53, loss = 0.04540287\n",
      "Iteration 54, loss = 0.04532905\n",
      "Iteration 55, loss = 0.04516981\n",
      "Iteration 56, loss = 0.04507164\n",
      "Iteration 57, loss = 0.04496434\n",
      "Iteration 58, loss = 0.04488238\n",
      "Iteration 59, loss = 0.04475887\n",
      "Iteration 60, loss = 0.04465658\n",
      "Iteration 61, loss = 0.04452330\n",
      "Iteration 62, loss = 0.04443469\n",
      "Iteration 63, loss = 0.04441618\n",
      "Iteration 64, loss = 0.04424150\n",
      "Iteration 65, loss = 0.04417448\n",
      "Iteration 66, loss = 0.04411422\n",
      "Iteration 67, loss = 0.04399747\n",
      "Iteration 68, loss = 0.04389724\n",
      "Iteration 69, loss = 0.04381474\n",
      "Iteration 70, loss = 0.04373656\n",
      "Iteration 71, loss = 0.04366040\n",
      "Iteration 72, loss = 0.04355985\n",
      "Iteration 73, loss = 0.04348864\n",
      "Iteration 74, loss = 0.04337913\n",
      "Iteration 75, loss = 0.04330750\n",
      "Iteration 76, loss = 0.04325991\n",
      "Iteration 77, loss = 0.04315776\n",
      "Iteration 78, loss = 0.04306577\n",
      "Iteration 79, loss = 0.04303808\n",
      "Iteration 80, loss = 0.04296611\n",
      "Iteration 81, loss = 0.04286750\n",
      "Iteration 82, loss = 0.04278993\n",
      "Iteration 83, loss = 0.04273377\n",
      "Iteration 84, loss = 0.04266480\n",
      "Iteration 85, loss = 0.04258065\n",
      "Iteration 86, loss = 0.04250422\n",
      "Iteration 87, loss = 0.04244799\n",
      "Iteration 88, loss = 0.04239782\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17203993\n",
      "Iteration 2, loss = 1.19142249\n",
      "Iteration 3, loss = 0.48505036\n",
      "Iteration 4, loss = 0.27798188\n",
      "Iteration 5, loss = 0.18890549\n",
      "Iteration 6, loss = 0.13011208\n",
      "Iteration 7, loss = 0.10098441\n",
      "Iteration 8, loss = 0.07953738\n",
      "Iteration 9, loss = 0.06992903\n",
      "Iteration 10, loss = 0.06395398\n",
      "Iteration 11, loss = 0.05979715\n",
      "Iteration 12, loss = 0.05774605\n",
      "Iteration 13, loss = 0.05549000\n",
      "Iteration 14, loss = 0.05433839\n",
      "Iteration 15, loss = 0.05321035\n",
      "Iteration 16, loss = 0.05241879\n",
      "Iteration 17, loss = 0.05178656\n",
      "Iteration 18, loss = 0.05116431\n",
      "Iteration 19, loss = 0.05068497\n",
      "Iteration 20, loss = 0.05012247\n",
      "Iteration 21, loss = 0.04965241\n",
      "Iteration 22, loss = 0.04920940\n",
      "Iteration 23, loss = 0.04897616\n",
      "Iteration 24, loss = 0.04860287\n",
      "Iteration 25, loss = 0.04828581\n",
      "Iteration 26, loss = 0.04802006\n",
      "Iteration 27, loss = 0.04779709\n",
      "Iteration 28, loss = 0.04747703\n",
      "Iteration 29, loss = 0.04725637\n",
      "Iteration 30, loss = 0.04699184\n",
      "Iteration 31, loss = 0.04679639\n",
      "Iteration 32, loss = 0.04657595\n",
      "Iteration 33, loss = 0.04645728\n",
      "Iteration 34, loss = 0.04613606\n",
      "Iteration 35, loss = 0.04599023\n",
      "Iteration 36, loss = 0.04580673\n",
      "Iteration 37, loss = 0.04562425\n",
      "Iteration 38, loss = 0.04546377\n",
      "Iteration 39, loss = 0.04529127\n",
      "Iteration 40, loss = 0.04514740\n",
      "Iteration 41, loss = 0.04500154\n",
      "Iteration 42, loss = 0.04484266\n",
      "Iteration 43, loss = 0.04467271\n",
      "Iteration 44, loss = 0.04457133\n",
      "Iteration 45, loss = 0.04441246\n",
      "Iteration 46, loss = 0.04431061\n",
      "Iteration 47, loss = 0.04418812\n",
      "Iteration 48, loss = 0.04402449\n",
      "Iteration 49, loss = 0.04391230\n",
      "Iteration 50, loss = 0.04377446\n",
      "Iteration 51, loss = 0.04367317\n",
      "Iteration 52, loss = 0.04357027\n",
      "Iteration 53, loss = 0.04340388\n",
      "Iteration 54, loss = 0.04334926\n",
      "Iteration 55, loss = 0.04316709\n",
      "Iteration 56, loss = 0.04304649\n",
      "Iteration 57, loss = 0.04295858\n",
      "Iteration 58, loss = 0.04286747\n",
      "Iteration 59, loss = 0.04272295\n",
      "Iteration 60, loss = 0.04263842\n",
      "Iteration 61, loss = 0.04256740\n",
      "Iteration 62, loss = 0.04244846\n",
      "Iteration 63, loss = 0.04234076\n",
      "Iteration 64, loss = 0.04225570\n",
      "Iteration 65, loss = 0.04217577\n",
      "Iteration 66, loss = 0.04208738\n",
      "Iteration 67, loss = 0.04199283\n",
      "Iteration 68, loss = 0.04188552\n",
      "Iteration 69, loss = 0.04181632\n",
      "Iteration 70, loss = 0.04175351\n",
      "Iteration 71, loss = 0.04167044\n",
      "Iteration 72, loss = 0.04154491\n",
      "Iteration 73, loss = 0.04148208\n",
      "Iteration 74, loss = 0.04137834\n",
      "Iteration 75, loss = 0.04133375\n",
      "Iteration 76, loss = 0.04121210\n",
      "Iteration 77, loss = 0.04115616\n",
      "Iteration 78, loss = 0.04106932\n",
      "Iteration 79, loss = 0.04100304\n",
      "Iteration 80, loss = 0.04095239\n",
      "Iteration 81, loss = 0.04085997\n",
      "Iteration 82, loss = 0.04078194\n",
      "Iteration 83, loss = 0.04070573\n",
      "Iteration 84, loss = 0.04063408\n",
      "Iteration 85, loss = 0.04059846\n",
      "Iteration 86, loss = 0.04049351\n",
      "Iteration 87, loss = 0.04043915\n",
      "Iteration 88, loss = 0.04036919\n",
      "Iteration 89, loss = 0.04029150\n",
      "Iteration 90, loss = 0.04022892\n",
      "Iteration 91, loss = 0.04016297\n",
      "Iteration 92, loss = 0.04017152\n",
      "Iteration 93, loss = 0.04005464\n",
      "Iteration 94, loss = 0.04003079\n",
      "Iteration 95, loss = 0.03995173\n",
      "Iteration 96, loss = 0.03988687\n",
      "Iteration 97, loss = 0.03982516\n",
      "Iteration 98, loss = 0.03976165\n",
      "Iteration 99, loss = 0.03970941\n",
      "Iteration 100, loss = 0.03965123\n",
      "Iteration 101, loss = 0.03959905\n",
      "Iteration 102, loss = 0.03955172\n",
      "Iteration 103, loss = 0.03950296\n",
      "Iteration 104, loss = 0.03944054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.86092633\n",
      "Iteration 2, loss = 0.65952785\n",
      "Iteration 3, loss = 0.28487801\n",
      "Iteration 4, loss = 0.16423167\n",
      "Iteration 5, loss = 0.11769060\n",
      "Iteration 6, loss = 0.09692079\n",
      "Iteration 7, loss = 0.08403813\n",
      "Iteration 8, loss = 0.07706317\n",
      "Iteration 9, loss = 0.07270863\n",
      "Iteration 10, loss = 0.06905528\n",
      "Iteration 11, loss = 0.06594376\n",
      "Iteration 12, loss = 0.06352630\n",
      "Iteration 13, loss = 0.06109629\n",
      "Iteration 14, loss = 0.05916101\n",
      "Iteration 15, loss = 0.05731881\n",
      "Iteration 16, loss = 0.05571567\n",
      "Iteration 17, loss = 0.05426037\n",
      "Iteration 18, loss = 0.05302672\n",
      "Iteration 19, loss = 0.05209743\n",
      "Iteration 20, loss = 0.05091091\n",
      "Iteration 21, loss = 0.04993477\n",
      "Iteration 22, loss = 0.04881585\n",
      "Iteration 23, loss = 0.04794312\n",
      "Iteration 24, loss = 0.04745788\n",
      "Iteration 25, loss = 0.04656964\n",
      "Iteration 26, loss = 0.04593096\n",
      "Iteration 27, loss = 0.04528172\n",
      "Iteration 28, loss = 0.04469603\n",
      "Iteration 29, loss = 0.04431115\n",
      "Iteration 30, loss = 0.04385564\n",
      "Iteration 31, loss = 0.04359540\n",
      "Iteration 32, loss = 0.04328203\n",
      "Iteration 33, loss = 0.04281190\n",
      "Iteration 34, loss = 0.04257330\n",
      "Iteration 35, loss = 0.04205630\n",
      "Iteration 36, loss = 0.04176256\n",
      "Iteration 37, loss = 0.04168488\n",
      "Iteration 38, loss = 0.04154336\n",
      "Iteration 39, loss = 0.04136601\n",
      "Iteration 40, loss = 0.04130446\n",
      "Iteration 41, loss = 0.04131525\n",
      "Iteration 42, loss = 0.04122560\n",
      "Iteration 43, loss = 0.04059258\n",
      "Iteration 44, loss = 0.04054484\n",
      "Iteration 45, loss = 0.04033374\n",
      "Iteration 46, loss = 0.03992544\n",
      "Iteration 47, loss = 0.03974972\n",
      "Iteration 48, loss = 0.03969640\n",
      "Iteration 49, loss = 0.03969083\n",
      "Iteration 50, loss = 0.03949172\n",
      "Iteration 51, loss = 0.03962318\n",
      "Iteration 52, loss = 0.03944078\n",
      "Iteration 53, loss = 0.03913657\n",
      "Iteration 54, loss = 0.03922104\n",
      "Iteration 55, loss = 0.03918808\n",
      "Iteration 56, loss = 0.03943393\n",
      "Iteration 57, loss = 0.03915908\n",
      "Iteration 58, loss = 0.03908810\n",
      "Iteration 59, loss = 0.03890903\n",
      "Iteration 60, loss = 0.03902822\n",
      "Iteration 61, loss = 0.03884719\n",
      "Iteration 62, loss = 0.03888688\n",
      "Iteration 63, loss = 0.03877830\n",
      "Iteration 64, loss = 0.03856474\n",
      "Iteration 65, loss = 0.03855767\n",
      "Iteration 66, loss = 0.03864737\n",
      "Iteration 67, loss = 0.03860325\n",
      "Iteration 68, loss = 0.03879087\n",
      "Iteration 69, loss = 0.03868516\n",
      "Iteration 70, loss = 0.03894764\n",
      "Iteration 71, loss = 0.03924931\n",
      "Iteration 72, loss = 0.03933270\n",
      "Iteration 73, loss = 0.03897875\n",
      "Iteration 74, loss = 0.03855125\n",
      "Iteration 75, loss = 0.03827350\n",
      "Iteration 76, loss = 0.03824373\n",
      "Iteration 77, loss = 0.03870861\n",
      "Iteration 78, loss = 0.03883783\n",
      "Iteration 79, loss = 0.03881121\n",
      "Iteration 80, loss = 0.03850908\n",
      "Iteration 81, loss = 0.03839409\n",
      "Iteration 82, loss = 0.03836420\n",
      "Iteration 83, loss = 0.03832854\n",
      "Iteration 84, loss = 0.03836954\n",
      "Iteration 85, loss = 0.03815525\n",
      "Iteration 86, loss = 0.03826658\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.85693063\n",
      "Iteration 2, loss = 0.65323863\n",
      "Iteration 3, loss = 0.29421224\n",
      "Iteration 4, loss = 0.17184849\n",
      "Iteration 5, loss = 0.12149003\n",
      "Iteration 6, loss = 0.10090723\n",
      "Iteration 7, loss = 0.08662567\n",
      "Iteration 8, loss = 0.07802510\n",
      "Iteration 9, loss = 0.07354720\n",
      "Iteration 10, loss = 0.06961815\n",
      "Iteration 11, loss = 0.06651827\n",
      "Iteration 12, loss = 0.06376061\n",
      "Iteration 13, loss = 0.06137883\n",
      "Iteration 14, loss = 0.05939286\n",
      "Iteration 15, loss = 0.05751074\n",
      "Iteration 16, loss = 0.05603543\n",
      "Iteration 17, loss = 0.05458971\n",
      "Iteration 18, loss = 0.05327560\n",
      "Iteration 19, loss = 0.05202318\n",
      "Iteration 20, loss = 0.05081075\n",
      "Iteration 21, loss = 0.04993334\n",
      "Iteration 22, loss = 0.04882087\n",
      "Iteration 23, loss = 0.04799301\n",
      "Iteration 24, loss = 0.04725239\n",
      "Iteration 25, loss = 0.04647066\n",
      "Iteration 26, loss = 0.04605066\n",
      "Iteration 27, loss = 0.04527782\n",
      "Iteration 28, loss = 0.04475509\n",
      "Iteration 29, loss = 0.04423469\n",
      "Iteration 30, loss = 0.04377153\n",
      "Iteration 31, loss = 0.04342855\n",
      "Iteration 32, loss = 0.04296621\n",
      "Iteration 33, loss = 0.04262758\n",
      "Iteration 34, loss = 0.04220024\n",
      "Iteration 35, loss = 0.04165970\n",
      "Iteration 36, loss = 0.04148569\n",
      "Iteration 37, loss = 0.04129301\n",
      "Iteration 38, loss = 0.04107868\n",
      "Iteration 39, loss = 0.04087308\n",
      "Iteration 40, loss = 0.04106647\n",
      "Iteration 41, loss = 0.04064677\n",
      "Iteration 42, loss = 0.04033331\n",
      "Iteration 43, loss = 0.04028003\n",
      "Iteration 44, loss = 0.04030873\n",
      "Iteration 45, loss = 0.04012306\n",
      "Iteration 46, loss = 0.03984498\n",
      "Iteration 47, loss = 0.03951376\n",
      "Iteration 48, loss = 0.03930768\n",
      "Iteration 49, loss = 0.03941382\n",
      "Iteration 50, loss = 0.03910366\n",
      "Iteration 51, loss = 0.03899792\n",
      "Iteration 52, loss = 0.03892155\n",
      "Iteration 53, loss = 0.03885445\n",
      "Iteration 54, loss = 0.03886714\n",
      "Iteration 55, loss = 0.03880140\n",
      "Iteration 56, loss = 0.03902055\n",
      "Iteration 57, loss = 0.03871219\n",
      "Iteration 58, loss = 0.03843606\n",
      "Iteration 59, loss = 0.03833537\n",
      "Iteration 60, loss = 0.03829368\n",
      "Iteration 61, loss = 0.03817227\n",
      "Iteration 62, loss = 0.03810779\n",
      "Iteration 63, loss = 0.03819019\n",
      "Iteration 64, loss = 0.03803561\n",
      "Iteration 65, loss = 0.03808770\n",
      "Iteration 66, loss = 0.03793396\n",
      "Iteration 67, loss = 0.03804448\n",
      "Iteration 68, loss = 0.03814821\n",
      "Iteration 69, loss = 0.03808789\n",
      "Iteration 70, loss = 0.03791335\n",
      "Iteration 71, loss = 0.03797748\n",
      "Iteration 72, loss = 0.03813558\n",
      "Iteration 73, loss = 0.03795834\n",
      "Iteration 74, loss = 0.03787076\n",
      "Iteration 75, loss = 0.03775258\n",
      "Iteration 76, loss = 0.03771758\n",
      "Iteration 77, loss = 0.03809214\n",
      "Iteration 78, loss = 0.03825030\n",
      "Iteration 79, loss = 0.03819542\n",
      "Iteration 80, loss = 0.03774317\n",
      "Iteration 81, loss = 0.03761480\n",
      "Iteration 82, loss = 0.03769511\n",
      "Iteration 83, loss = 0.03764922\n",
      "Iteration 84, loss = 0.03761453\n",
      "Iteration 85, loss = 0.03761427\n",
      "Iteration 86, loss = 0.03777446\n",
      "Iteration 87, loss = 0.03779236\n",
      "Iteration 88, loss = 0.03798431\n",
      "Iteration 89, loss = 0.03809854\n",
      "Iteration 90, loss = 0.03800357\n",
      "Iteration 91, loss = 0.03898255\n",
      "Iteration 92, loss = 0.03817260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.86875692\n",
      "Iteration 2, loss = 0.74548596\n",
      "Iteration 3, loss = 0.33463185\n",
      "Iteration 4, loss = 0.18558023\n",
      "Iteration 5, loss = 0.12270039\n",
      "Iteration 6, loss = 0.09784862\n",
      "Iteration 7, loss = 0.08722642\n",
      "Iteration 8, loss = 0.08022003\n",
      "Iteration 9, loss = 0.07580040\n",
      "Iteration 10, loss = 0.07181303\n",
      "Iteration 11, loss = 0.06856281\n",
      "Iteration 12, loss = 0.06569594\n",
      "Iteration 13, loss = 0.06324426\n",
      "Iteration 14, loss = 0.06107574\n",
      "Iteration 15, loss = 0.05910291\n",
      "Iteration 16, loss = 0.05742208\n",
      "Iteration 17, loss = 0.05603060\n",
      "Iteration 18, loss = 0.05453138\n",
      "Iteration 19, loss = 0.05318375\n",
      "Iteration 20, loss = 0.05207317\n",
      "Iteration 21, loss = 0.05092381\n",
      "Iteration 22, loss = 0.05008220\n",
      "Iteration 23, loss = 0.04911615\n",
      "Iteration 24, loss = 0.04831527\n",
      "Iteration 25, loss = 0.04763886\n",
      "Iteration 26, loss = 0.04708383\n",
      "Iteration 27, loss = 0.04656908\n",
      "Iteration 28, loss = 0.04580222\n",
      "Iteration 29, loss = 0.04542231\n",
      "Iteration 30, loss = 0.04460718\n",
      "Iteration 31, loss = 0.04415562\n",
      "Iteration 32, loss = 0.04387485\n",
      "Iteration 33, loss = 0.04349632\n",
      "Iteration 34, loss = 0.04308737\n",
      "Iteration 35, loss = 0.04295394\n",
      "Iteration 36, loss = 0.04240068\n",
      "Iteration 37, loss = 0.04208805\n",
      "Iteration 38, loss = 0.04178780\n",
      "Iteration 39, loss = 0.04175300\n",
      "Iteration 40, loss = 0.04142367\n",
      "Iteration 41, loss = 0.04140241\n",
      "Iteration 42, loss = 0.04137907\n",
      "Iteration 43, loss = 0.04101413\n",
      "Iteration 44, loss = 0.04081005\n",
      "Iteration 45, loss = 0.04052984\n",
      "Iteration 46, loss = 0.04043238\n",
      "Iteration 47, loss = 0.04031709\n",
      "Iteration 48, loss = 0.04011540\n",
      "Iteration 49, loss = 0.04003960\n",
      "Iteration 50, loss = 0.04005808\n",
      "Iteration 51, loss = 0.04047248\n",
      "Iteration 52, loss = 0.04034392\n",
      "Iteration 53, loss = 0.04012465\n",
      "Iteration 54, loss = 0.03963995\n",
      "Iteration 55, loss = 0.03942902\n",
      "Iteration 56, loss = 0.03951239\n",
      "Iteration 57, loss = 0.03973071\n",
      "Iteration 58, loss = 0.03956477\n",
      "Iteration 59, loss = 0.03928626\n",
      "Iteration 60, loss = 0.03898874\n",
      "Iteration 61, loss = 0.03889730\n",
      "Iteration 62, loss = 0.03878194\n",
      "Iteration 63, loss = 0.03934582\n",
      "Iteration 64, loss = 0.03926941\n",
      "Iteration 65, loss = 0.03898183\n",
      "Iteration 66, loss = 0.03894157\n",
      "Iteration 67, loss = 0.03875874\n",
      "Iteration 68, loss = 0.03855900\n",
      "Iteration 69, loss = 0.03877416\n",
      "Iteration 70, loss = 0.03906624\n",
      "Iteration 71, loss = 0.03904263\n",
      "Iteration 72, loss = 0.03864052\n",
      "Iteration 73, loss = 0.03838090\n",
      "Iteration 74, loss = 0.03841166\n",
      "Iteration 75, loss = 0.03866695\n",
      "Iteration 76, loss = 0.03905240\n",
      "Iteration 77, loss = 0.03898896\n",
      "Iteration 78, loss = 0.03900097\n",
      "Iteration 79, loss = 0.03912619\n",
      "Iteration 80, loss = 0.03901794\n",
      "Iteration 81, loss = 0.03882018\n",
      "Iteration 82, loss = 0.03859819\n",
      "Iteration 83, loss = 0.03864839\n",
      "Iteration 84, loss = 0.03828092\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.89692987\n",
      "Iteration 2, loss = 0.73261897\n",
      "Iteration 3, loss = 0.31399767\n",
      "Iteration 4, loss = 0.17869262\n",
      "Iteration 5, loss = 0.11901713\n",
      "Iteration 6, loss = 0.09584676\n",
      "Iteration 7, loss = 0.08558029\n",
      "Iteration 8, loss = 0.07881008\n",
      "Iteration 9, loss = 0.07429611\n",
      "Iteration 10, loss = 0.07086373\n",
      "Iteration 11, loss = 0.06766835\n",
      "Iteration 12, loss = 0.06490139\n",
      "Iteration 13, loss = 0.06263187\n",
      "Iteration 14, loss = 0.06053236\n",
      "Iteration 15, loss = 0.05865456\n",
      "Iteration 16, loss = 0.05707234\n",
      "Iteration 17, loss = 0.05567531\n",
      "Iteration 18, loss = 0.05424237\n",
      "Iteration 19, loss = 0.05314560\n",
      "Iteration 20, loss = 0.05205517\n",
      "Iteration 21, loss = 0.05085692\n",
      "Iteration 22, loss = 0.04995370\n",
      "Iteration 23, loss = 0.04894272\n",
      "Iteration 24, loss = 0.04822371\n",
      "Iteration 25, loss = 0.04763824\n",
      "Iteration 26, loss = 0.04716551\n",
      "Iteration 27, loss = 0.04677818\n",
      "Iteration 28, loss = 0.04596302\n",
      "Iteration 29, loss = 0.04573632\n",
      "Iteration 30, loss = 0.04489070\n",
      "Iteration 31, loss = 0.04433128\n",
      "Iteration 32, loss = 0.04420337\n",
      "Iteration 33, loss = 0.04387654\n",
      "Iteration 34, loss = 0.04348862\n",
      "Iteration 35, loss = 0.04328936\n",
      "Iteration 36, loss = 0.04286748\n",
      "Iteration 37, loss = 0.04243036\n",
      "Iteration 38, loss = 0.04207901\n",
      "Iteration 39, loss = 0.04206077\n",
      "Iteration 40, loss = 0.04173890\n",
      "Iteration 41, loss = 0.04160757\n",
      "Iteration 42, loss = 0.04160929\n",
      "Iteration 43, loss = 0.04128793\n",
      "Iteration 44, loss = 0.04106497\n",
      "Iteration 45, loss = 0.04085828\n",
      "Iteration 46, loss = 0.04093701\n",
      "Iteration 47, loss = 0.04073647\n",
      "Iteration 48, loss = 0.04075977\n",
      "Iteration 49, loss = 0.04051590\n",
      "Iteration 50, loss = 0.04049601\n",
      "Iteration 51, loss = 0.04050660\n",
      "Iteration 52, loss = 0.04038540\n",
      "Iteration 53, loss = 0.04041386\n",
      "Iteration 54, loss = 0.04017750\n",
      "Iteration 55, loss = 0.03999952\n",
      "Iteration 56, loss = 0.03978672\n",
      "Iteration 57, loss = 0.03990841\n",
      "Iteration 58, loss = 0.03987524\n",
      "Iteration 59, loss = 0.03980057\n",
      "Iteration 60, loss = 0.03954040\n",
      "Iteration 61, loss = 0.03942351\n",
      "Iteration 62, loss = 0.03938026\n",
      "Iteration 63, loss = 0.03978436\n",
      "Iteration 64, loss = 0.03972708\n",
      "Iteration 65, loss = 0.03957367\n",
      "Iteration 66, loss = 0.03960165\n",
      "Iteration 67, loss = 0.03966410\n",
      "Iteration 68, loss = 0.03960132\n",
      "Iteration 69, loss = 0.03963909\n",
      "Iteration 70, loss = 0.03959996\n",
      "Iteration 71, loss = 0.03939122\n",
      "Iteration 72, loss = 0.03924694\n",
      "Iteration 73, loss = 0.03901097\n",
      "Iteration 74, loss = 0.03882747\n",
      "Iteration 75, loss = 0.03904516\n",
      "Iteration 76, loss = 0.03940498\n",
      "Iteration 77, loss = 0.03944422\n",
      "Iteration 78, loss = 0.03960294\n",
      "Iteration 79, loss = 0.03975459\n",
      "Iteration 80, loss = 0.03970760\n",
      "Iteration 81, loss = 0.03947438\n",
      "Iteration 82, loss = 0.03916643\n",
      "Iteration 83, loss = 0.03890907\n",
      "Iteration 84, loss = 0.03885879\n",
      "Iteration 85, loss = 0.03890692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.89114516\n",
      "Iteration 2, loss = 0.70539865\n",
      "Iteration 3, loss = 0.30194458\n",
      "Iteration 4, loss = 0.17321793\n",
      "Iteration 5, loss = 0.11686516\n",
      "Iteration 6, loss = 0.09445068\n",
      "Iteration 7, loss = 0.08678683\n",
      "Iteration 8, loss = 0.07821284\n",
      "Iteration 9, loss = 0.07354692\n",
      "Iteration 10, loss = 0.07035617\n",
      "Iteration 11, loss = 0.06663811\n",
      "Iteration 12, loss = 0.06401917\n",
      "Iteration 13, loss = 0.06172729\n",
      "Iteration 14, loss = 0.05943985\n",
      "Iteration 15, loss = 0.05752123\n",
      "Iteration 16, loss = 0.05576835\n",
      "Iteration 17, loss = 0.05431834\n",
      "Iteration 18, loss = 0.05290425\n",
      "Iteration 19, loss = 0.05152221\n",
      "Iteration 20, loss = 0.05036603\n",
      "Iteration 21, loss = 0.04915240\n",
      "Iteration 22, loss = 0.04799964\n",
      "Iteration 23, loss = 0.04727445\n",
      "Iteration 24, loss = 0.04676035\n",
      "Iteration 25, loss = 0.04566963\n",
      "Iteration 26, loss = 0.04502503\n",
      "Iteration 27, loss = 0.04436943\n",
      "Iteration 28, loss = 0.04357074\n",
      "Iteration 29, loss = 0.04313338\n",
      "Iteration 30, loss = 0.04268183\n",
      "Iteration 31, loss = 0.04268166\n",
      "Iteration 32, loss = 0.04216379\n",
      "Iteration 33, loss = 0.04169558\n",
      "Iteration 34, loss = 0.04137551\n",
      "Iteration 35, loss = 0.04123500\n",
      "Iteration 36, loss = 0.04092956\n",
      "Iteration 37, loss = 0.04066631\n",
      "Iteration 38, loss = 0.04023085\n",
      "Iteration 39, loss = 0.03994780\n",
      "Iteration 40, loss = 0.04015539\n",
      "Iteration 41, loss = 0.03966997\n",
      "Iteration 42, loss = 0.03952203\n",
      "Iteration 43, loss = 0.03911234\n",
      "Iteration 44, loss = 0.03896056\n",
      "Iteration 45, loss = 0.03881146\n",
      "Iteration 46, loss = 0.03874342\n",
      "Iteration 47, loss = 0.03881869\n",
      "Iteration 48, loss = 0.03877004\n",
      "Iteration 49, loss = 0.03897319\n",
      "Iteration 50, loss = 0.03885307\n",
      "Iteration 51, loss = 0.03877371\n",
      "Iteration 52, loss = 0.03862928\n",
      "Iteration 53, loss = 0.03841080\n",
      "Iteration 54, loss = 0.03799724\n",
      "Iteration 55, loss = 0.03799764\n",
      "Iteration 56, loss = 0.03796988\n",
      "Iteration 57, loss = 0.03824624\n",
      "Iteration 58, loss = 0.03807165\n",
      "Iteration 59, loss = 0.03782507\n",
      "Iteration 60, loss = 0.03759136\n",
      "Iteration 61, loss = 0.03771877\n",
      "Iteration 62, loss = 0.03766821\n",
      "Iteration 63, loss = 0.03756616\n",
      "Iteration 64, loss = 0.03768787\n",
      "Iteration 65, loss = 0.03772368\n",
      "Iteration 66, loss = 0.03758896\n",
      "Iteration 67, loss = 0.03752041\n",
      "Iteration 68, loss = 0.03755868\n",
      "Iteration 69, loss = 0.03757207\n",
      "Iteration 70, loss = 0.03769232\n",
      "Iteration 71, loss = 0.03770794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34032948\n",
      "Iteration 2, loss = 2.23581413\n",
      "Iteration 3, loss = 2.08703487\n",
      "Iteration 4, loss = 1.90121895\n",
      "Iteration 5, loss = 1.68126220\n",
      "Iteration 6, loss = 1.45111328\n",
      "Iteration 7, loss = 1.23937184\n",
      "Iteration 8, loss = 1.06011262\n",
      "Iteration 9, loss = 0.91595801\n",
      "Iteration 10, loss = 0.80109514\n",
      "Iteration 11, loss = 0.70710779\n",
      "Iteration 12, loss = 0.63315436\n",
      "Iteration 13, loss = 0.57171661\n",
      "Iteration 14, loss = 0.52141636\n",
      "Iteration 15, loss = 0.47760689\n",
      "Iteration 16, loss = 0.44195877\n",
      "Iteration 17, loss = 0.40967211\n",
      "Iteration 18, loss = 0.38251059\n",
      "Iteration 19, loss = 0.35779238\n",
      "Iteration 20, loss = 0.33549360\n",
      "Iteration 21, loss = 0.31587615\n",
      "Iteration 22, loss = 0.29879583\n",
      "Iteration 23, loss = 0.28252792\n",
      "Iteration 24, loss = 0.26804251\n",
      "Iteration 25, loss = 0.25504892\n",
      "Iteration 26, loss = 0.24290637\n",
      "Iteration 27, loss = 0.23110005\n",
      "Iteration 28, loss = 0.22110787\n",
      "Iteration 29, loss = 0.21202170\n",
      "Iteration 30, loss = 0.20345791\n",
      "Iteration 31, loss = 0.19608529\n",
      "Iteration 32, loss = 0.18913479\n",
      "Iteration 33, loss = 0.18172490\n",
      "Iteration 34, loss = 0.17551789\n",
      "Iteration 35, loss = 0.16978381\n",
      "Iteration 36, loss = 0.16397459\n",
      "Iteration 37, loss = 0.15930094\n",
      "Iteration 38, loss = 0.15429668\n",
      "Iteration 39, loss = 0.15040234\n",
      "Iteration 40, loss = 0.14571365\n",
      "Iteration 41, loss = 0.14139164\n",
      "Iteration 42, loss = 0.13728436\n",
      "Iteration 43, loss = 0.13394316\n",
      "Iteration 44, loss = 0.13067262\n",
      "Iteration 45, loss = 0.12736634\n",
      "Iteration 46, loss = 0.12451290\n",
      "Iteration 47, loss = 0.12169799\n",
      "Iteration 48, loss = 0.11907574\n",
      "Iteration 49, loss = 0.11634326\n",
      "Iteration 50, loss = 0.11361967\n",
      "Iteration 51, loss = 0.11155673\n",
      "Iteration 52, loss = 0.10908631\n",
      "Iteration 53, loss = 0.10729592\n",
      "Iteration 54, loss = 0.10492264\n",
      "Iteration 55, loss = 0.10312896\n",
      "Iteration 56, loss = 0.10164152\n",
      "Iteration 57, loss = 0.09954978\n",
      "Iteration 58, loss = 0.09775744\n",
      "Iteration 59, loss = 0.09596484\n",
      "Iteration 60, loss = 0.09451266\n",
      "Iteration 61, loss = 0.09286096\n",
      "Iteration 62, loss = 0.09144199\n",
      "Iteration 63, loss = 0.09006965\n",
      "Iteration 64, loss = 0.08879098\n",
      "Iteration 65, loss = 0.08766838\n",
      "Iteration 66, loss = 0.08639589\n",
      "Iteration 67, loss = 0.08531026\n",
      "Iteration 68, loss = 0.08426452\n",
      "Iteration 69, loss = 0.08317539\n",
      "Iteration 70, loss = 0.08217504\n",
      "Iteration 71, loss = 0.08115827\n",
      "Iteration 72, loss = 0.08024019\n",
      "Iteration 73, loss = 0.07935191\n",
      "Iteration 74, loss = 0.07845950\n",
      "Iteration 75, loss = 0.07759302\n",
      "Iteration 76, loss = 0.07686492\n",
      "Iteration 77, loss = 0.07626473\n",
      "Iteration 78, loss = 0.07535413\n",
      "Iteration 79, loss = 0.07452231\n",
      "Iteration 80, loss = 0.07399809\n",
      "Iteration 81, loss = 0.07324899\n",
      "Iteration 82, loss = 0.07243869\n",
      "Iteration 83, loss = 0.07189468\n",
      "Iteration 84, loss = 0.07124629\n",
      "Iteration 85, loss = 0.07062454\n",
      "Iteration 86, loss = 0.07006884\n",
      "Iteration 87, loss = 0.06950634\n",
      "Iteration 88, loss = 0.06898276\n",
      "Iteration 89, loss = 0.06845292\n",
      "Iteration 90, loss = 0.06794437\n",
      "Iteration 91, loss = 0.06737777\n",
      "Iteration 92, loss = 0.06703045\n",
      "Iteration 93, loss = 0.06657244\n",
      "Iteration 94, loss = 0.06605427\n",
      "Iteration 95, loss = 0.06554372\n",
      "Iteration 96, loss = 0.06510173\n",
      "Iteration 97, loss = 0.06477763\n",
      "Iteration 98, loss = 0.06455176\n",
      "Iteration 99, loss = 0.06403627\n",
      "Iteration 100, loss = 0.06354685\n",
      "Iteration 101, loss = 0.06324876\n",
      "Iteration 102, loss = 0.06286350\n",
      "Iteration 103, loss = 0.06251858\n",
      "Iteration 104, loss = 0.06224939\n",
      "Iteration 105, loss = 0.06195043\n",
      "Iteration 106, loss = 0.06161323\n",
      "Iteration 107, loss = 0.06137979\n",
      "Iteration 108, loss = 0.06101156\n",
      "Iteration 109, loss = 0.06078153\n",
      "Iteration 110, loss = 0.06046435\n",
      "Iteration 111, loss = 0.06022431\n",
      "Iteration 112, loss = 0.06002707\n",
      "Iteration 113, loss = 0.05976987\n",
      "Iteration 114, loss = 0.05947456\n",
      "Iteration 115, loss = 0.05924365\n",
      "Iteration 116, loss = 0.05898860\n",
      "Iteration 117, loss = 0.05879848\n",
      "Iteration 118, loss = 0.05858337\n",
      "Iteration 119, loss = 0.05837859\n",
      "Iteration 120, loss = 0.05813692\n",
      "Iteration 121, loss = 0.05791790\n",
      "Iteration 122, loss = 0.05771699\n",
      "Iteration 123, loss = 0.05754481\n",
      "Iteration 124, loss = 0.05734078\n",
      "Iteration 125, loss = 0.05715860\n",
      "Iteration 126, loss = 0.05698337\n",
      "Iteration 127, loss = 0.05682011\n",
      "Iteration 128, loss = 0.05671107\n",
      "Iteration 129, loss = 0.05657114\n",
      "Iteration 130, loss = 0.05633290\n",
      "Iteration 131, loss = 0.05614406\n",
      "Iteration 132, loss = 0.05598759\n",
      "Iteration 133, loss = 0.05583253\n",
      "Iteration 134, loss = 0.05567782\n",
      "Iteration 135, loss = 0.05555516\n",
      "Iteration 136, loss = 0.05538386\n",
      "Iteration 137, loss = 0.05525566\n",
      "Iteration 138, loss = 0.05513156\n",
      "Iteration 139, loss = 0.05497860\n",
      "Iteration 140, loss = 0.05487767\n",
      "Iteration 141, loss = 0.05472694\n",
      "Iteration 142, loss = 0.05458369\n",
      "Iteration 143, loss = 0.05446303\n",
      "Iteration 144, loss = 0.05433509\n",
      "Iteration 145, loss = 0.05422364\n",
      "Iteration 146, loss = 0.05411076\n",
      "Iteration 147, loss = 0.05399250\n",
      "Iteration 148, loss = 0.05387106\n",
      "Iteration 149, loss = 0.05374698\n",
      "Iteration 150, loss = 0.05365146\n",
      "Iteration 151, loss = 0.05352635\n",
      "Iteration 152, loss = 0.05341687\n",
      "Iteration 153, loss = 0.05328648\n",
      "Iteration 154, loss = 0.05318663\n",
      "Iteration 155, loss = 0.05309544\n",
      "Iteration 156, loss = 0.05298196\n",
      "Iteration 157, loss = 0.05290071\n",
      "Iteration 158, loss = 0.05281186\n",
      "Iteration 159, loss = 0.05270877\n",
      "Iteration 160, loss = 0.05263845\n",
      "Iteration 161, loss = 0.05253352\n",
      "Iteration 162, loss = 0.05244733\n",
      "Iteration 163, loss = 0.05233227\n",
      "Iteration 164, loss = 0.05225511\n",
      "Iteration 165, loss = 0.05216461\n",
      "Iteration 166, loss = 0.05209206\n",
      "Iteration 167, loss = 0.05198427\n",
      "Iteration 168, loss = 0.05192638\n",
      "Iteration 169, loss = 0.05183282\n",
      "Iteration 170, loss = 0.05176480\n",
      "Iteration 171, loss = 0.05172233\n",
      "Iteration 172, loss = 0.05162740\n",
      "Iteration 173, loss = 0.05155841\n",
      "Iteration 174, loss = 0.05145889\n",
      "Iteration 175, loss = 0.05138346\n",
      "Iteration 176, loss = 0.05133475\n",
      "Iteration 177, loss = 0.05125020\n",
      "Iteration 178, loss = 0.05119161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33951044\n",
      "Iteration 2, loss = 2.23302726\n",
      "Iteration 3, loss = 2.07743173\n",
      "Iteration 4, loss = 1.87945573\n",
      "Iteration 5, loss = 1.65144297\n",
      "Iteration 6, loss = 1.41501707\n",
      "Iteration 7, loss = 1.19899883\n",
      "Iteration 8, loss = 1.02008033\n",
      "Iteration 9, loss = 0.87549467\n",
      "Iteration 10, loss = 0.76173612\n",
      "Iteration 11, loss = 0.67104606\n",
      "Iteration 12, loss = 0.59958010\n",
      "Iteration 13, loss = 0.54153434\n",
      "Iteration 14, loss = 0.49514766\n",
      "Iteration 15, loss = 0.45244762\n",
      "Iteration 16, loss = 0.41973731\n",
      "Iteration 17, loss = 0.38958448\n",
      "Iteration 18, loss = 0.36453288\n",
      "Iteration 19, loss = 0.34023578\n",
      "Iteration 20, loss = 0.32105858\n",
      "Iteration 21, loss = 0.30243367\n",
      "Iteration 22, loss = 0.28701351\n",
      "Iteration 23, loss = 0.27133259\n",
      "Iteration 24, loss = 0.25830700\n",
      "Iteration 25, loss = 0.24570540\n",
      "Iteration 26, loss = 0.23540139\n",
      "Iteration 27, loss = 0.22369227\n",
      "Iteration 28, loss = 0.21467805\n",
      "Iteration 29, loss = 0.20588332\n",
      "Iteration 30, loss = 0.19816145\n",
      "Iteration 31, loss = 0.19107988\n",
      "Iteration 32, loss = 0.18394003\n",
      "Iteration 33, loss = 0.17709049\n",
      "Iteration 34, loss = 0.17103364\n",
      "Iteration 35, loss = 0.16555611\n",
      "Iteration 36, loss = 0.16020785\n",
      "Iteration 37, loss = 0.15542102\n",
      "Iteration 38, loss = 0.15103043\n",
      "Iteration 39, loss = 0.14680908\n",
      "Iteration 40, loss = 0.14288668\n",
      "Iteration 41, loss = 0.13905090\n",
      "Iteration 42, loss = 0.13490315\n",
      "Iteration 43, loss = 0.13222936\n",
      "Iteration 44, loss = 0.12865585\n",
      "Iteration 45, loss = 0.12533079\n",
      "Iteration 46, loss = 0.12254892\n",
      "Iteration 47, loss = 0.11967158\n",
      "Iteration 48, loss = 0.11725074\n",
      "Iteration 49, loss = 0.11445393\n",
      "Iteration 50, loss = 0.11210329\n",
      "Iteration 51, loss = 0.10979679\n",
      "Iteration 52, loss = 0.10732191\n",
      "Iteration 53, loss = 0.10540619\n",
      "Iteration 54, loss = 0.10323657\n",
      "Iteration 55, loss = 0.10150088\n",
      "Iteration 56, loss = 0.09993015\n",
      "Iteration 57, loss = 0.09802729\n",
      "Iteration 58, loss = 0.09631176\n",
      "Iteration 59, loss = 0.09449124\n",
      "Iteration 60, loss = 0.09294113\n",
      "Iteration 61, loss = 0.09141267\n",
      "Iteration 62, loss = 0.09001985\n",
      "Iteration 63, loss = 0.08869109\n",
      "Iteration 64, loss = 0.08746300\n",
      "Iteration 65, loss = 0.08619568\n",
      "Iteration 66, loss = 0.08510910\n",
      "Iteration 67, loss = 0.08419960\n",
      "Iteration 68, loss = 0.08296598\n",
      "Iteration 69, loss = 0.08182622\n",
      "Iteration 70, loss = 0.08079284\n",
      "Iteration 71, loss = 0.07978558\n",
      "Iteration 72, loss = 0.07912959\n",
      "Iteration 73, loss = 0.07794691\n",
      "Iteration 74, loss = 0.07710728\n",
      "Iteration 75, loss = 0.07644941\n",
      "Iteration 76, loss = 0.07565081\n",
      "Iteration 77, loss = 0.07502080\n",
      "Iteration 78, loss = 0.07401023\n",
      "Iteration 79, loss = 0.07326610\n",
      "Iteration 80, loss = 0.07278454\n",
      "Iteration 81, loss = 0.07199806\n",
      "Iteration 82, loss = 0.07114695\n",
      "Iteration 83, loss = 0.07062934\n",
      "Iteration 84, loss = 0.06995603\n",
      "Iteration 85, loss = 0.06939054\n",
      "Iteration 86, loss = 0.06880705\n",
      "Iteration 87, loss = 0.06825269\n",
      "Iteration 88, loss = 0.06784765\n",
      "Iteration 89, loss = 0.06726592\n",
      "Iteration 90, loss = 0.06681786\n",
      "Iteration 91, loss = 0.06618013\n",
      "Iteration 92, loss = 0.06586463\n",
      "Iteration 93, loss = 0.06537535\n",
      "Iteration 94, loss = 0.06495846\n",
      "Iteration 95, loss = 0.06445110\n",
      "Iteration 96, loss = 0.06399984\n",
      "Iteration 97, loss = 0.06364003\n",
      "Iteration 98, loss = 0.06342811\n",
      "Iteration 99, loss = 0.06289535\n",
      "Iteration 100, loss = 0.06248360\n",
      "Iteration 101, loss = 0.06215971\n",
      "Iteration 102, loss = 0.06179658\n",
      "Iteration 103, loss = 0.06145795\n",
      "Iteration 104, loss = 0.06116692\n",
      "Iteration 105, loss = 0.06086477\n",
      "Iteration 106, loss = 0.06058701\n",
      "Iteration 107, loss = 0.06029154\n",
      "Iteration 108, loss = 0.05995645\n",
      "Iteration 109, loss = 0.05976041\n",
      "Iteration 110, loss = 0.05943372\n",
      "Iteration 111, loss = 0.05919359\n",
      "Iteration 112, loss = 0.05897572\n",
      "Iteration 113, loss = 0.05869590\n",
      "Iteration 114, loss = 0.05840027\n",
      "Iteration 115, loss = 0.05819562\n",
      "Iteration 116, loss = 0.05794774\n",
      "Iteration 117, loss = 0.05778866\n",
      "Iteration 118, loss = 0.05758056\n",
      "Iteration 119, loss = 0.05732371\n",
      "Iteration 120, loss = 0.05711839\n",
      "Iteration 121, loss = 0.05690849\n",
      "Iteration 122, loss = 0.05676589\n",
      "Iteration 123, loss = 0.05656592\n",
      "Iteration 124, loss = 0.05633490\n",
      "Iteration 125, loss = 0.05618322\n",
      "Iteration 126, loss = 0.05600696\n",
      "Iteration 127, loss = 0.05583122\n",
      "Iteration 128, loss = 0.05570539\n",
      "Iteration 129, loss = 0.05553170\n",
      "Iteration 130, loss = 0.05533200\n",
      "Iteration 131, loss = 0.05517930\n",
      "Iteration 132, loss = 0.05500479\n",
      "Iteration 133, loss = 0.05488339\n",
      "Iteration 134, loss = 0.05473674\n",
      "Iteration 135, loss = 0.05460230\n",
      "Iteration 136, loss = 0.05444908\n",
      "Iteration 137, loss = 0.05429854\n",
      "Iteration 138, loss = 0.05420441\n",
      "Iteration 139, loss = 0.05408803\n",
      "Iteration 140, loss = 0.05391868\n",
      "Iteration 141, loss = 0.05377988\n",
      "Iteration 142, loss = 0.05364870\n",
      "Iteration 143, loss = 0.05353564\n",
      "Iteration 144, loss = 0.05341648\n",
      "Iteration 145, loss = 0.05331649\n",
      "Iteration 146, loss = 0.05319417\n",
      "Iteration 147, loss = 0.05304985\n",
      "Iteration 148, loss = 0.05294486\n",
      "Iteration 149, loss = 0.05282928\n",
      "Iteration 150, loss = 0.05271901\n",
      "Iteration 151, loss = 0.05261306\n",
      "Iteration 152, loss = 0.05250532\n",
      "Iteration 153, loss = 0.05239895\n",
      "Iteration 154, loss = 0.05229088\n",
      "Iteration 155, loss = 0.05220418\n",
      "Iteration 156, loss = 0.05210332\n",
      "Iteration 157, loss = 0.05201335\n",
      "Iteration 158, loss = 0.05191156\n",
      "Iteration 159, loss = 0.05182093\n",
      "Iteration 160, loss = 0.05175038\n",
      "Iteration 161, loss = 0.05165189\n",
      "Iteration 162, loss = 0.05155437\n",
      "Iteration 163, loss = 0.05145416\n",
      "Iteration 164, loss = 0.05138901\n",
      "Iteration 165, loss = 0.05128280\n",
      "Iteration 166, loss = 0.05121394\n",
      "Iteration 167, loss = 0.05112248\n",
      "Iteration 168, loss = 0.05105081\n",
      "Iteration 169, loss = 0.05097554\n",
      "Iteration 170, loss = 0.05089560\n",
      "Iteration 171, loss = 0.05084974\n",
      "Iteration 172, loss = 0.05076473\n",
      "Iteration 173, loss = 0.05068368\n",
      "Iteration 174, loss = 0.05060217\n",
      "Iteration 175, loss = 0.05053066\n",
      "Iteration 176, loss = 0.05047828\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34544114\n",
      "Iteration 2, loss = 2.24313731\n",
      "Iteration 3, loss = 2.09961522\n",
      "Iteration 4, loss = 1.91251475\n",
      "Iteration 5, loss = 1.69440019\n",
      "Iteration 6, loss = 1.46003621\n",
      "Iteration 7, loss = 1.24499373\n",
      "Iteration 8, loss = 1.06019632\n",
      "Iteration 9, loss = 0.91252156\n",
      "Iteration 10, loss = 0.79446233\n",
      "Iteration 11, loss = 0.70102180\n",
      "Iteration 12, loss = 0.62775347\n",
      "Iteration 13, loss = 0.56777197\n",
      "Iteration 14, loss = 0.51792670\n",
      "Iteration 15, loss = 0.47660415\n",
      "Iteration 16, loss = 0.44087668\n",
      "Iteration 17, loss = 0.41062284\n",
      "Iteration 18, loss = 0.38321599\n",
      "Iteration 19, loss = 0.35887619\n",
      "Iteration 20, loss = 0.33699186\n",
      "Iteration 21, loss = 0.31869295\n",
      "Iteration 22, loss = 0.30163462\n",
      "Iteration 23, loss = 0.28572040\n",
      "Iteration 24, loss = 0.27224030\n",
      "Iteration 25, loss = 0.25939129\n",
      "Iteration 26, loss = 0.24697015\n",
      "Iteration 27, loss = 0.23661225\n",
      "Iteration 28, loss = 0.22584996\n",
      "Iteration 29, loss = 0.21731060\n",
      "Iteration 30, loss = 0.20813987\n",
      "Iteration 31, loss = 0.20058550\n",
      "Iteration 32, loss = 0.19275653\n",
      "Iteration 33, loss = 0.18623977\n",
      "Iteration 34, loss = 0.17921698\n",
      "Iteration 35, loss = 0.17293598\n",
      "Iteration 36, loss = 0.16797509\n",
      "Iteration 37, loss = 0.16227101\n",
      "Iteration 38, loss = 0.15757352\n",
      "Iteration 39, loss = 0.15301431\n",
      "Iteration 40, loss = 0.14863200\n",
      "Iteration 41, loss = 0.14492360\n",
      "Iteration 42, loss = 0.14130944\n",
      "Iteration 43, loss = 0.13730150\n",
      "Iteration 44, loss = 0.13361885\n",
      "Iteration 45, loss = 0.13021025\n",
      "Iteration 46, loss = 0.12774859\n",
      "Iteration 47, loss = 0.12430510\n",
      "Iteration 48, loss = 0.12112991\n",
      "Iteration 49, loss = 0.11882404\n",
      "Iteration 50, loss = 0.11645072\n",
      "Iteration 51, loss = 0.11394428\n",
      "Iteration 52, loss = 0.11134624\n",
      "Iteration 53, loss = 0.10888405\n",
      "Iteration 54, loss = 0.10720872\n",
      "Iteration 55, loss = 0.10525017\n",
      "Iteration 56, loss = 0.10311672\n",
      "Iteration 57, loss = 0.10120086\n",
      "Iteration 58, loss = 0.09937192\n",
      "Iteration 59, loss = 0.09757717\n",
      "Iteration 60, loss = 0.09591627\n",
      "Iteration 61, loss = 0.09430279\n",
      "Iteration 62, loss = 0.09271689\n",
      "Iteration 63, loss = 0.09163148\n",
      "Iteration 64, loss = 0.09000796\n",
      "Iteration 65, loss = 0.08858726\n",
      "Iteration 66, loss = 0.08745045\n",
      "Iteration 67, loss = 0.08616911\n",
      "Iteration 68, loss = 0.08511426\n",
      "Iteration 69, loss = 0.08398378\n",
      "Iteration 70, loss = 0.08302343\n",
      "Iteration 71, loss = 0.08182663\n",
      "Iteration 72, loss = 0.08078913\n",
      "Iteration 73, loss = 0.07982738\n",
      "Iteration 74, loss = 0.07893498\n",
      "Iteration 75, loss = 0.07814560\n",
      "Iteration 76, loss = 0.07724595\n",
      "Iteration 77, loss = 0.07654403\n",
      "Iteration 78, loss = 0.07570820\n",
      "Iteration 79, loss = 0.07484787\n",
      "Iteration 80, loss = 0.07419627\n",
      "Iteration 81, loss = 0.07342221\n",
      "Iteration 82, loss = 0.07273290\n",
      "Iteration 83, loss = 0.07219527\n",
      "Iteration 84, loss = 0.07152526\n",
      "Iteration 85, loss = 0.07086341\n",
      "Iteration 86, loss = 0.07030963\n",
      "Iteration 87, loss = 0.06978185\n",
      "Iteration 88, loss = 0.06923436\n",
      "Iteration 89, loss = 0.06865168\n",
      "Iteration 90, loss = 0.06809910\n",
      "Iteration 91, loss = 0.06769478\n",
      "Iteration 92, loss = 0.06717271\n",
      "Iteration 93, loss = 0.06673122\n",
      "Iteration 94, loss = 0.06627173\n",
      "Iteration 95, loss = 0.06581458\n",
      "Iteration 96, loss = 0.06539509\n",
      "Iteration 97, loss = 0.06500068\n",
      "Iteration 98, loss = 0.06464037\n",
      "Iteration 99, loss = 0.06425994\n",
      "Iteration 100, loss = 0.06382496\n",
      "Iteration 101, loss = 0.06352653\n",
      "Iteration 102, loss = 0.06314942\n",
      "Iteration 103, loss = 0.06284313\n",
      "Iteration 104, loss = 0.06249936\n",
      "Iteration 105, loss = 0.06219839\n",
      "Iteration 106, loss = 0.06187846\n",
      "Iteration 107, loss = 0.06160085\n",
      "Iteration 108, loss = 0.06130309\n",
      "Iteration 109, loss = 0.06098127\n",
      "Iteration 110, loss = 0.06072808\n",
      "Iteration 111, loss = 0.06045063\n",
      "Iteration 112, loss = 0.06023572\n",
      "Iteration 113, loss = 0.05996039\n",
      "Iteration 114, loss = 0.05966417\n",
      "Iteration 115, loss = 0.05944422\n",
      "Iteration 116, loss = 0.05919042\n",
      "Iteration 117, loss = 0.05894580\n",
      "Iteration 118, loss = 0.05873026\n",
      "Iteration 119, loss = 0.05855025\n",
      "Iteration 120, loss = 0.05828988\n",
      "Iteration 121, loss = 0.05809146\n",
      "Iteration 122, loss = 0.05788249\n",
      "Iteration 123, loss = 0.05769311\n",
      "Iteration 124, loss = 0.05749831\n",
      "Iteration 125, loss = 0.05733049\n",
      "Iteration 126, loss = 0.05717501\n",
      "Iteration 127, loss = 0.05692721\n",
      "Iteration 128, loss = 0.05673434\n",
      "Iteration 129, loss = 0.05656428\n",
      "Iteration 130, loss = 0.05637950\n",
      "Iteration 131, loss = 0.05624674\n",
      "Iteration 132, loss = 0.05606479\n",
      "Iteration 133, loss = 0.05591599\n",
      "Iteration 134, loss = 0.05577914\n",
      "Iteration 135, loss = 0.05562845\n",
      "Iteration 136, loss = 0.05547165\n",
      "Iteration 137, loss = 0.05534313\n",
      "Iteration 138, loss = 0.05519398\n",
      "Iteration 139, loss = 0.05511358\n",
      "Iteration 140, loss = 0.05492017\n",
      "Iteration 141, loss = 0.05477364\n",
      "Iteration 142, loss = 0.05464300\n",
      "Iteration 143, loss = 0.05449520\n",
      "Iteration 144, loss = 0.05439270\n",
      "Iteration 145, loss = 0.05427977\n",
      "Iteration 146, loss = 0.05413982\n",
      "Iteration 147, loss = 0.05401650\n",
      "Iteration 148, loss = 0.05390980\n",
      "Iteration 149, loss = 0.05377433\n",
      "Iteration 150, loss = 0.05367039\n",
      "Iteration 151, loss = 0.05356652\n",
      "Iteration 152, loss = 0.05345887\n",
      "Iteration 153, loss = 0.05333817\n",
      "Iteration 154, loss = 0.05324484\n",
      "Iteration 155, loss = 0.05315540\n",
      "Iteration 156, loss = 0.05301956\n",
      "Iteration 157, loss = 0.05293543\n",
      "Iteration 158, loss = 0.05284842\n",
      "Iteration 159, loss = 0.05274150\n",
      "Iteration 160, loss = 0.05265054\n",
      "Iteration 161, loss = 0.05256080\n",
      "Iteration 162, loss = 0.05247401\n",
      "Iteration 163, loss = 0.05239222\n",
      "Iteration 164, loss = 0.05229192\n",
      "Iteration 165, loss = 0.05221762\n",
      "Iteration 166, loss = 0.05213448\n",
      "Iteration 167, loss = 0.05205670\n",
      "Iteration 168, loss = 0.05196952\n",
      "Iteration 169, loss = 0.05189551\n",
      "Iteration 170, loss = 0.05182211\n",
      "Iteration 171, loss = 0.05174279\n",
      "Iteration 172, loss = 0.05167921\n",
      "Iteration 173, loss = 0.05160329\n",
      "Iteration 174, loss = 0.05152253\n",
      "Iteration 175, loss = 0.05144169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34458489\n",
      "Iteration 2, loss = 2.24406760\n",
      "Iteration 3, loss = 2.10828564\n",
      "Iteration 4, loss = 1.92919297\n",
      "Iteration 5, loss = 1.72151045\n",
      "Iteration 6, loss = 1.49867147\n",
      "Iteration 7, loss = 1.28555244\n",
      "Iteration 8, loss = 1.10181735\n",
      "Iteration 9, loss = 0.95277085\n",
      "Iteration 10, loss = 0.83397131\n",
      "Iteration 11, loss = 0.73596210\n",
      "Iteration 12, loss = 0.65854680\n",
      "Iteration 13, loss = 0.59624841\n",
      "Iteration 14, loss = 0.54311413\n",
      "Iteration 15, loss = 0.50136759\n",
      "Iteration 16, loss = 0.46150082\n",
      "Iteration 17, loss = 0.42958424\n",
      "Iteration 18, loss = 0.40078284\n",
      "Iteration 19, loss = 0.37504845\n",
      "Iteration 20, loss = 0.35259075\n",
      "Iteration 21, loss = 0.33194822\n",
      "Iteration 22, loss = 0.31310987\n",
      "Iteration 23, loss = 0.29705512\n",
      "Iteration 24, loss = 0.28197969\n",
      "Iteration 25, loss = 0.26961346\n",
      "Iteration 26, loss = 0.25705224\n",
      "Iteration 27, loss = 0.24558972\n",
      "Iteration 28, loss = 0.23430058\n",
      "Iteration 29, loss = 0.22515688\n",
      "Iteration 30, loss = 0.21566969\n",
      "Iteration 31, loss = 0.20828878\n",
      "Iteration 32, loss = 0.20039291\n",
      "Iteration 33, loss = 0.19362946\n",
      "Iteration 34, loss = 0.18605044\n",
      "Iteration 35, loss = 0.17972792\n",
      "Iteration 36, loss = 0.17403048\n",
      "Iteration 37, loss = 0.16834325\n",
      "Iteration 38, loss = 0.16346798\n",
      "Iteration 39, loss = 0.15855615\n",
      "Iteration 40, loss = 0.15400919\n",
      "Iteration 41, loss = 0.15073112\n",
      "Iteration 42, loss = 0.14596716\n",
      "Iteration 43, loss = 0.14197260\n",
      "Iteration 44, loss = 0.13832054\n",
      "Iteration 45, loss = 0.13478356\n",
      "Iteration 46, loss = 0.13243899\n",
      "Iteration 47, loss = 0.12880181\n",
      "Iteration 48, loss = 0.12546863\n",
      "Iteration 49, loss = 0.12311307\n",
      "Iteration 50, loss = 0.12055462\n",
      "Iteration 51, loss = 0.11795288\n",
      "Iteration 52, loss = 0.11540938\n",
      "Iteration 53, loss = 0.11284106\n",
      "Iteration 54, loss = 0.11085408\n",
      "Iteration 55, loss = 0.10879545\n",
      "Iteration 56, loss = 0.10671050\n",
      "Iteration 57, loss = 0.10470914\n",
      "Iteration 58, loss = 0.10274703\n",
      "Iteration 59, loss = 0.10101375\n",
      "Iteration 60, loss = 0.09916448\n",
      "Iteration 61, loss = 0.09755609\n",
      "Iteration 62, loss = 0.09597902\n",
      "Iteration 63, loss = 0.09488036\n",
      "Iteration 64, loss = 0.09307110\n",
      "Iteration 65, loss = 0.09177474\n",
      "Iteration 66, loss = 0.09067066\n",
      "Iteration 67, loss = 0.08920178\n",
      "Iteration 68, loss = 0.08796138\n",
      "Iteration 69, loss = 0.08668155\n",
      "Iteration 70, loss = 0.08561252\n",
      "Iteration 71, loss = 0.08456652\n",
      "Iteration 72, loss = 0.08345243\n",
      "Iteration 73, loss = 0.08247559\n",
      "Iteration 74, loss = 0.08142738\n",
      "Iteration 75, loss = 0.08061287\n",
      "Iteration 76, loss = 0.07982330\n",
      "Iteration 77, loss = 0.07901002\n",
      "Iteration 78, loss = 0.07805863\n",
      "Iteration 79, loss = 0.07728156\n",
      "Iteration 80, loss = 0.07656621\n",
      "Iteration 81, loss = 0.07576179\n",
      "Iteration 82, loss = 0.07505303\n",
      "Iteration 83, loss = 0.07439366\n",
      "Iteration 84, loss = 0.07376817\n",
      "Iteration 85, loss = 0.07308996\n",
      "Iteration 86, loss = 0.07247285\n",
      "Iteration 87, loss = 0.07187179\n",
      "Iteration 88, loss = 0.07131305\n",
      "Iteration 89, loss = 0.07069093\n",
      "Iteration 90, loss = 0.07016121\n",
      "Iteration 91, loss = 0.06968241\n",
      "Iteration 92, loss = 0.06913551\n",
      "Iteration 93, loss = 0.06866917\n",
      "Iteration 94, loss = 0.06830003\n",
      "Iteration 95, loss = 0.06778273\n",
      "Iteration 96, loss = 0.06733232\n",
      "Iteration 97, loss = 0.06691689\n",
      "Iteration 98, loss = 0.06653558\n",
      "Iteration 99, loss = 0.06606538\n",
      "Iteration 100, loss = 0.06564647\n",
      "Iteration 101, loss = 0.06533125\n",
      "Iteration 102, loss = 0.06491604\n",
      "Iteration 103, loss = 0.06465040\n",
      "Iteration 104, loss = 0.06426324\n",
      "Iteration 105, loss = 0.06389069\n",
      "Iteration 106, loss = 0.06358867\n",
      "Iteration 107, loss = 0.06323199\n",
      "Iteration 108, loss = 0.06294572\n",
      "Iteration 109, loss = 0.06261570\n",
      "Iteration 110, loss = 0.06231437\n",
      "Iteration 111, loss = 0.06204314\n",
      "Iteration 112, loss = 0.06178131\n",
      "Iteration 113, loss = 0.06151783\n",
      "Iteration 114, loss = 0.06125967\n",
      "Iteration 115, loss = 0.06098008\n",
      "Iteration 116, loss = 0.06068659\n",
      "Iteration 117, loss = 0.06045565\n",
      "Iteration 118, loss = 0.06022394\n",
      "Iteration 119, loss = 0.06000263\n",
      "Iteration 120, loss = 0.05973611\n",
      "Iteration 121, loss = 0.05956701\n",
      "Iteration 122, loss = 0.05929484\n",
      "Iteration 123, loss = 0.05906308\n",
      "Iteration 124, loss = 0.05887045\n",
      "Iteration 125, loss = 0.05868422\n",
      "Iteration 126, loss = 0.05852195\n",
      "Iteration 127, loss = 0.05824993\n",
      "Iteration 128, loss = 0.05807099\n",
      "Iteration 129, loss = 0.05789716\n",
      "Iteration 130, loss = 0.05772108\n",
      "Iteration 131, loss = 0.05756276\n",
      "Iteration 132, loss = 0.05736348\n",
      "Iteration 133, loss = 0.05720741\n",
      "Iteration 134, loss = 0.05703761\n",
      "Iteration 135, loss = 0.05689222\n",
      "Iteration 136, loss = 0.05674593\n",
      "Iteration 137, loss = 0.05660369\n",
      "Iteration 138, loss = 0.05643774\n",
      "Iteration 139, loss = 0.05634888\n",
      "Iteration 140, loss = 0.05614104\n",
      "Iteration 141, loss = 0.05598399\n",
      "Iteration 142, loss = 0.05585543\n",
      "Iteration 143, loss = 0.05571389\n",
      "Iteration 144, loss = 0.05558138\n",
      "Iteration 145, loss = 0.05548192\n",
      "Iteration 146, loss = 0.05531227\n",
      "Iteration 147, loss = 0.05516750\n",
      "Iteration 148, loss = 0.05509358\n",
      "Iteration 149, loss = 0.05493967\n",
      "Iteration 150, loss = 0.05481918\n",
      "Iteration 151, loss = 0.05469878\n",
      "Iteration 152, loss = 0.05458185\n",
      "Iteration 153, loss = 0.05446050\n",
      "Iteration 154, loss = 0.05437468\n",
      "Iteration 155, loss = 0.05427179\n",
      "Iteration 156, loss = 0.05414408\n",
      "Iteration 157, loss = 0.05403289\n",
      "Iteration 158, loss = 0.05393616\n",
      "Iteration 159, loss = 0.05384177\n",
      "Iteration 160, loss = 0.05374629\n",
      "Iteration 161, loss = 0.05365444\n",
      "Iteration 162, loss = 0.05356101\n",
      "Iteration 163, loss = 0.05346163\n",
      "Iteration 164, loss = 0.05336863\n",
      "Iteration 165, loss = 0.05328117\n",
      "Iteration 166, loss = 0.05320446\n",
      "Iteration 167, loss = 0.05312263\n",
      "Iteration 168, loss = 0.05302036\n",
      "Iteration 169, loss = 0.05294631\n",
      "Iteration 170, loss = 0.05286534\n",
      "Iteration 171, loss = 0.05277235\n",
      "Iteration 172, loss = 0.05269738\n",
      "Iteration 173, loss = 0.05262432\n",
      "Iteration 174, loss = 0.05252598\n",
      "Iteration 175, loss = 0.05245492\n",
      "Iteration 176, loss = 0.05237823\n",
      "Iteration 177, loss = 0.05230837\n",
      "Iteration 178, loss = 0.05222988\n",
      "Iteration 179, loss = 0.05215397\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34107021\n",
      "Iteration 2, loss = 2.23919928\n",
      "Iteration 3, loss = 2.09501699\n",
      "Iteration 4, loss = 1.90418415\n",
      "Iteration 5, loss = 1.68822386\n",
      "Iteration 6, loss = 1.46039033\n",
      "Iteration 7, loss = 1.24467165\n",
      "Iteration 8, loss = 1.05801995\n",
      "Iteration 9, loss = 0.90903776\n",
      "Iteration 10, loss = 0.78860969\n",
      "Iteration 11, loss = 0.69023692\n",
      "Iteration 12, loss = 0.61429279\n",
      "Iteration 13, loss = 0.55190550\n",
      "Iteration 14, loss = 0.49958547\n",
      "Iteration 15, loss = 0.45896973\n",
      "Iteration 16, loss = 0.42129755\n",
      "Iteration 17, loss = 0.39065249\n",
      "Iteration 18, loss = 0.36492079\n",
      "Iteration 19, loss = 0.33884328\n",
      "Iteration 20, loss = 0.31816667\n",
      "Iteration 21, loss = 0.29972799\n",
      "Iteration 22, loss = 0.28264153\n",
      "Iteration 23, loss = 0.26890876\n",
      "Iteration 24, loss = 0.25368573\n",
      "Iteration 25, loss = 0.24202864\n",
      "Iteration 26, loss = 0.23064330\n",
      "Iteration 27, loss = 0.21985649\n",
      "Iteration 28, loss = 0.21008578\n",
      "Iteration 29, loss = 0.20160404\n",
      "Iteration 30, loss = 0.19304000\n",
      "Iteration 31, loss = 0.18630338\n",
      "Iteration 32, loss = 0.17893944\n",
      "Iteration 33, loss = 0.17253359\n",
      "Iteration 34, loss = 0.16564813\n",
      "Iteration 35, loss = 0.16055651\n",
      "Iteration 36, loss = 0.15506536\n",
      "Iteration 37, loss = 0.14967684\n",
      "Iteration 38, loss = 0.14518119\n",
      "Iteration 39, loss = 0.14080331\n",
      "Iteration 40, loss = 0.13662703\n",
      "Iteration 41, loss = 0.13339029\n",
      "Iteration 42, loss = 0.12936814\n",
      "Iteration 43, loss = 0.12572736\n",
      "Iteration 44, loss = 0.12271477\n",
      "Iteration 45, loss = 0.11951506\n",
      "Iteration 46, loss = 0.11702091\n",
      "Iteration 47, loss = 0.11444575\n",
      "Iteration 48, loss = 0.11148795\n",
      "Iteration 49, loss = 0.10925019\n",
      "Iteration 50, loss = 0.10696822\n",
      "Iteration 51, loss = 0.10494404\n",
      "Iteration 52, loss = 0.10255448\n",
      "Iteration 53, loss = 0.10045162\n",
      "Iteration 54, loss = 0.09863361\n",
      "Iteration 55, loss = 0.09685242\n",
      "Iteration 56, loss = 0.09498191\n",
      "Iteration 57, loss = 0.09317685\n",
      "Iteration 58, loss = 0.09172765\n",
      "Iteration 59, loss = 0.09011300\n",
      "Iteration 60, loss = 0.08867652\n",
      "Iteration 61, loss = 0.08742267\n",
      "Iteration 62, loss = 0.08613190\n",
      "Iteration 63, loss = 0.08485434\n",
      "Iteration 64, loss = 0.08362030\n",
      "Iteration 65, loss = 0.08259425\n",
      "Iteration 66, loss = 0.08155546\n",
      "Iteration 67, loss = 0.08050374\n",
      "Iteration 68, loss = 0.07941027\n",
      "Iteration 69, loss = 0.07842228\n",
      "Iteration 70, loss = 0.07756029\n",
      "Iteration 71, loss = 0.07664659\n",
      "Iteration 72, loss = 0.07572557\n",
      "Iteration 73, loss = 0.07483256\n",
      "Iteration 74, loss = 0.07401156\n",
      "Iteration 75, loss = 0.07345322\n",
      "Iteration 76, loss = 0.07256172\n",
      "Iteration 77, loss = 0.07184365\n",
      "Iteration 78, loss = 0.07114909\n",
      "Iteration 79, loss = 0.07057123\n",
      "Iteration 80, loss = 0.06993709\n",
      "Iteration 81, loss = 0.06932471\n",
      "Iteration 82, loss = 0.06874508\n",
      "Iteration 83, loss = 0.06816887\n",
      "Iteration 84, loss = 0.06765404\n",
      "Iteration 85, loss = 0.06713098\n",
      "Iteration 86, loss = 0.06667475\n",
      "Iteration 87, loss = 0.06615959\n",
      "Iteration 88, loss = 0.06564475\n",
      "Iteration 89, loss = 0.06515712\n",
      "Iteration 90, loss = 0.06473047\n",
      "Iteration 91, loss = 0.06429389\n",
      "Iteration 92, loss = 0.06391398\n",
      "Iteration 93, loss = 0.06350677\n",
      "Iteration 94, loss = 0.06316294\n",
      "Iteration 95, loss = 0.06273013\n",
      "Iteration 96, loss = 0.06241270\n",
      "Iteration 97, loss = 0.06203394\n",
      "Iteration 98, loss = 0.06165746\n",
      "Iteration 99, loss = 0.06132399\n",
      "Iteration 100, loss = 0.06100269\n",
      "Iteration 101, loss = 0.06071718\n",
      "Iteration 102, loss = 0.06040222\n",
      "Iteration 103, loss = 0.06008381\n",
      "Iteration 104, loss = 0.05982669\n",
      "Iteration 105, loss = 0.05951674\n",
      "Iteration 106, loss = 0.05924298\n",
      "Iteration 107, loss = 0.05895573\n",
      "Iteration 108, loss = 0.05873602\n",
      "Iteration 109, loss = 0.05847008\n",
      "Iteration 110, loss = 0.05822949\n",
      "Iteration 111, loss = 0.05797200\n",
      "Iteration 112, loss = 0.05774337\n",
      "Iteration 113, loss = 0.05753821\n",
      "Iteration 114, loss = 0.05732036\n",
      "Iteration 115, loss = 0.05707748\n",
      "Iteration 116, loss = 0.05687067\n",
      "Iteration 117, loss = 0.05666773\n",
      "Iteration 118, loss = 0.05649683\n",
      "Iteration 119, loss = 0.05629742\n",
      "Iteration 120, loss = 0.05608556\n",
      "Iteration 121, loss = 0.05593619\n",
      "Iteration 122, loss = 0.05570221\n",
      "Iteration 123, loss = 0.05550767\n",
      "Iteration 124, loss = 0.05533079\n",
      "Iteration 125, loss = 0.05516482\n",
      "Iteration 126, loss = 0.05503228\n",
      "Iteration 127, loss = 0.05484177\n",
      "Iteration 128, loss = 0.05467360\n",
      "Iteration 129, loss = 0.05452925\n",
      "Iteration 130, loss = 0.05438327\n",
      "Iteration 131, loss = 0.05425024\n",
      "Iteration 132, loss = 0.05407476\n",
      "Iteration 133, loss = 0.05393621\n",
      "Iteration 134, loss = 0.05379737\n",
      "Iteration 135, loss = 0.05367832\n",
      "Iteration 136, loss = 0.05355615\n",
      "Iteration 137, loss = 0.05342876\n",
      "Iteration 138, loss = 0.05327618\n",
      "Iteration 139, loss = 0.05315624\n",
      "Iteration 140, loss = 0.05303079\n",
      "Iteration 141, loss = 0.05290315\n",
      "Iteration 142, loss = 0.05277087\n",
      "Iteration 143, loss = 0.05267098\n",
      "Iteration 144, loss = 0.05257204\n",
      "Iteration 145, loss = 0.05246065\n",
      "Iteration 146, loss = 0.05233219\n",
      "Iteration 147, loss = 0.05222232\n",
      "Iteration 148, loss = 0.05214531\n",
      "Iteration 149, loss = 0.05203142\n",
      "Iteration 150, loss = 0.05191562\n",
      "Iteration 151, loss = 0.05181085\n",
      "Iteration 152, loss = 0.05170805\n",
      "Iteration 153, loss = 0.05160425\n",
      "Iteration 154, loss = 0.05151654\n",
      "Iteration 155, loss = 0.05143316\n",
      "Iteration 156, loss = 0.05135139\n",
      "Iteration 157, loss = 0.05124027\n",
      "Iteration 158, loss = 0.05114750\n",
      "Iteration 159, loss = 0.05108295\n",
      "Iteration 160, loss = 0.05099360\n",
      "Iteration 161, loss = 0.05091259\n",
      "Iteration 162, loss = 0.05081937\n",
      "Iteration 163, loss = 0.05073610\n",
      "Iteration 164, loss = 0.05066666\n",
      "Iteration 165, loss = 0.05058893\n",
      "Iteration 166, loss = 0.05051408\n",
      "Iteration 167, loss = 0.05042993\n",
      "Iteration 168, loss = 0.05036652\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30790966\n",
      "Iteration 2, loss = 2.12538497\n",
      "Iteration 3, loss = 1.92918326\n",
      "Iteration 4, loss = 1.72707461\n",
      "Iteration 5, loss = 1.52335614\n",
      "Iteration 6, loss = 1.33341545\n",
      "Iteration 7, loss = 1.16229610\n",
      "Iteration 8, loss = 1.01129063\n",
      "Iteration 9, loss = 0.88503027\n",
      "Iteration 10, loss = 0.77787775\n",
      "Iteration 11, loss = 0.68930330\n",
      "Iteration 12, loss = 0.61264692\n",
      "Iteration 13, loss = 0.55106271\n",
      "Iteration 14, loss = 0.49643429\n",
      "Iteration 15, loss = 0.45069982\n",
      "Iteration 16, loss = 0.41130569\n",
      "Iteration 17, loss = 0.37659738\n",
      "Iteration 18, loss = 0.34681110\n",
      "Iteration 19, loss = 0.32054878\n",
      "Iteration 20, loss = 0.29767355\n",
      "Iteration 21, loss = 0.27663942\n",
      "Iteration 22, loss = 0.25873231\n",
      "Iteration 23, loss = 0.24232322\n",
      "Iteration 24, loss = 0.22722837\n",
      "Iteration 25, loss = 0.21383319\n",
      "Iteration 26, loss = 0.20190558\n",
      "Iteration 27, loss = 0.19081066\n",
      "Iteration 28, loss = 0.18084047\n",
      "Iteration 29, loss = 0.17218315\n",
      "Iteration 30, loss = 0.16392979\n",
      "Iteration 31, loss = 0.15673434\n",
      "Iteration 32, loss = 0.14989530\n",
      "Iteration 33, loss = 0.14367230\n",
      "Iteration 34, loss = 0.13786130\n",
      "Iteration 35, loss = 0.13283872\n",
      "Iteration 36, loss = 0.12783616\n",
      "Iteration 37, loss = 0.12352136\n",
      "Iteration 38, loss = 0.11928630\n",
      "Iteration 39, loss = 0.11573570\n",
      "Iteration 40, loss = 0.11204889\n",
      "Iteration 41, loss = 0.10872496\n",
      "Iteration 42, loss = 0.10549397\n",
      "Iteration 43, loss = 0.10274429\n",
      "Iteration 44, loss = 0.10020371\n",
      "Iteration 45, loss = 0.09774868\n",
      "Iteration 46, loss = 0.09555140\n",
      "Iteration 47, loss = 0.09342046\n",
      "Iteration 48, loss = 0.09136391\n",
      "Iteration 49, loss = 0.08948558\n",
      "Iteration 50, loss = 0.08770470\n",
      "Iteration 51, loss = 0.08620066\n",
      "Iteration 52, loss = 0.08453159\n",
      "Iteration 53, loss = 0.08313236\n",
      "Iteration 54, loss = 0.08169989\n",
      "Iteration 55, loss = 0.08045850\n",
      "Iteration 56, loss = 0.07933417\n",
      "Iteration 57, loss = 0.07812112\n",
      "Iteration 58, loss = 0.07695559\n",
      "Iteration 59, loss = 0.07593290\n",
      "Iteration 60, loss = 0.07496163\n",
      "Iteration 61, loss = 0.07396026\n",
      "Iteration 62, loss = 0.07313071\n",
      "Iteration 63, loss = 0.07227294\n",
      "Iteration 64, loss = 0.07147378\n",
      "Iteration 65, loss = 0.07073274\n",
      "Iteration 66, loss = 0.07001835\n",
      "Iteration 67, loss = 0.06930589\n",
      "Iteration 68, loss = 0.06868236\n",
      "Iteration 69, loss = 0.06804295\n",
      "Iteration 70, loss = 0.06743063\n",
      "Iteration 71, loss = 0.06683419\n",
      "Iteration 72, loss = 0.06625158\n",
      "Iteration 73, loss = 0.06574517\n",
      "Iteration 74, loss = 0.06519671\n",
      "Iteration 75, loss = 0.06466510\n",
      "Iteration 76, loss = 0.06419578\n",
      "Iteration 77, loss = 0.06376525\n",
      "Iteration 78, loss = 0.06329296\n",
      "Iteration 79, loss = 0.06283318\n",
      "Iteration 80, loss = 0.06244566\n",
      "Iteration 81, loss = 0.06204231\n",
      "Iteration 82, loss = 0.06161202\n",
      "Iteration 83, loss = 0.06124461\n",
      "Iteration 84, loss = 0.06085840\n",
      "Iteration 85, loss = 0.06048909\n",
      "Iteration 86, loss = 0.06014448\n",
      "Iteration 87, loss = 0.05980030\n",
      "Iteration 88, loss = 0.05949213\n",
      "Iteration 89, loss = 0.05916452\n",
      "Iteration 90, loss = 0.05885508\n",
      "Iteration 91, loss = 0.05853176\n",
      "Iteration 92, loss = 0.05825630\n",
      "Iteration 93, loss = 0.05798478\n",
      "Iteration 94, loss = 0.05769005\n",
      "Iteration 95, loss = 0.05740016\n",
      "Iteration 96, loss = 0.05710732\n",
      "Iteration 97, loss = 0.05686002\n",
      "Iteration 98, loss = 0.05663285\n",
      "Iteration 99, loss = 0.05637045\n",
      "Iteration 100, loss = 0.05609534\n",
      "Iteration 101, loss = 0.05586312\n",
      "Iteration 102, loss = 0.05562468\n",
      "Iteration 103, loss = 0.05537634\n",
      "Iteration 104, loss = 0.05516850\n",
      "Iteration 105, loss = 0.05495035\n",
      "Iteration 106, loss = 0.05472744\n",
      "Iteration 107, loss = 0.05452653\n",
      "Iteration 108, loss = 0.05429957\n",
      "Iteration 109, loss = 0.05410468\n",
      "Iteration 110, loss = 0.05389231\n",
      "Iteration 111, loss = 0.05369877\n",
      "Iteration 112, loss = 0.05353573\n",
      "Iteration 113, loss = 0.05334102\n",
      "Iteration 114, loss = 0.05312359\n",
      "Iteration 115, loss = 0.05294008\n",
      "Iteration 116, loss = 0.05274931\n",
      "Iteration 117, loss = 0.05258674\n",
      "Iteration 118, loss = 0.05241790\n",
      "Iteration 119, loss = 0.05224785\n",
      "Iteration 120, loss = 0.05205568\n",
      "Iteration 121, loss = 0.05188382\n",
      "Iteration 122, loss = 0.05171430\n",
      "Iteration 123, loss = 0.05157330\n",
      "Iteration 124, loss = 0.05140236\n",
      "Iteration 125, loss = 0.05124627\n",
      "Iteration 126, loss = 0.05109587\n",
      "Iteration 127, loss = 0.05095552\n",
      "Iteration 128, loss = 0.05081465\n",
      "Iteration 129, loss = 0.05068424\n",
      "Iteration 130, loss = 0.05051152\n",
      "Iteration 131, loss = 0.05034877\n",
      "Iteration 132, loss = 0.05020937\n",
      "Iteration 133, loss = 0.05006824\n",
      "Iteration 134, loss = 0.04993721\n",
      "Iteration 135, loss = 0.04980882\n",
      "Iteration 136, loss = 0.04965687\n",
      "Iteration 137, loss = 0.04952715\n",
      "Iteration 138, loss = 0.04940300\n",
      "Iteration 139, loss = 0.04925781\n",
      "Iteration 140, loss = 0.04915436\n",
      "Iteration 141, loss = 0.04900915\n",
      "Iteration 142, loss = 0.04887581\n",
      "Iteration 143, loss = 0.04875491\n",
      "Iteration 144, loss = 0.04862593\n",
      "Iteration 145, loss = 0.04851042\n",
      "Iteration 146, loss = 0.04840037\n",
      "Iteration 147, loss = 0.04828013\n",
      "Iteration 148, loss = 0.04814870\n",
      "Iteration 149, loss = 0.04803863\n",
      "Iteration 150, loss = 0.04792892\n",
      "Iteration 151, loss = 0.04781514\n",
      "Iteration 152, loss = 0.04768892\n",
      "Iteration 153, loss = 0.04757642\n",
      "Iteration 154, loss = 0.04745253\n",
      "Iteration 155, loss = 0.04734811\n",
      "Iteration 156, loss = 0.04723228\n",
      "Iteration 157, loss = 0.04713656\n",
      "Iteration 158, loss = 0.04704047\n",
      "Iteration 159, loss = 0.04692885\n",
      "Iteration 160, loss = 0.04684919\n",
      "Iteration 161, loss = 0.04673447\n",
      "Iteration 162, loss = 0.04662409\n",
      "Iteration 163, loss = 0.04650832\n",
      "Iteration 164, loss = 0.04642669\n",
      "Iteration 165, loss = 0.04632160\n",
      "Iteration 166, loss = 0.04623509\n",
      "Iteration 167, loss = 0.04612336\n",
      "Iteration 168, loss = 0.04605493\n",
      "Iteration 169, loss = 0.04594078\n",
      "Iteration 170, loss = 0.04586561\n",
      "Iteration 171, loss = 0.04578537\n",
      "Iteration 172, loss = 0.04568067\n",
      "Iteration 173, loss = 0.04559128\n",
      "Iteration 174, loss = 0.04548723\n",
      "Iteration 175, loss = 0.04540964\n",
      "Iteration 176, loss = 0.04533654\n",
      "Iteration 177, loss = 0.04524396\n",
      "Iteration 178, loss = 0.04516789\n",
      "Iteration 179, loss = 0.04507945\n",
      "Iteration 180, loss = 0.04500244\n",
      "Iteration 181, loss = 0.04492295\n",
      "Iteration 182, loss = 0.04482504\n",
      "Iteration 183, loss = 0.04474331\n",
      "Iteration 184, loss = 0.04467776\n",
      "Iteration 185, loss = 0.04459726\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30778754\n",
      "Iteration 2, loss = 2.12065266\n",
      "Iteration 3, loss = 1.91780494\n",
      "Iteration 4, loss = 1.71139943\n",
      "Iteration 5, loss = 1.50558737\n",
      "Iteration 6, loss = 1.31483860\n",
      "Iteration 7, loss = 1.14368609\n",
      "Iteration 8, loss = 0.99640034\n",
      "Iteration 9, loss = 0.86997807\n",
      "Iteration 10, loss = 0.76389552\n",
      "Iteration 11, loss = 0.67666817\n",
      "Iteration 12, loss = 0.60174920\n",
      "Iteration 13, loss = 0.54019320\n",
      "Iteration 14, loss = 0.48744855\n",
      "Iteration 15, loss = 0.44187125\n",
      "Iteration 16, loss = 0.40404005\n",
      "Iteration 17, loss = 0.36974631\n",
      "Iteration 18, loss = 0.34136456\n",
      "Iteration 19, loss = 0.31554084\n",
      "Iteration 20, loss = 0.29338572\n",
      "Iteration 21, loss = 0.27348507\n",
      "Iteration 22, loss = 0.25577255\n",
      "Iteration 23, loss = 0.23997486\n",
      "Iteration 24, loss = 0.22555388\n",
      "Iteration 25, loss = 0.21224874\n",
      "Iteration 26, loss = 0.20055131\n",
      "Iteration 27, loss = 0.18960202\n",
      "Iteration 28, loss = 0.18001281\n",
      "Iteration 29, loss = 0.17130323\n",
      "Iteration 30, loss = 0.16320970\n",
      "Iteration 31, loss = 0.15606035\n",
      "Iteration 32, loss = 0.14904502\n",
      "Iteration 33, loss = 0.14277992\n",
      "Iteration 34, loss = 0.13698848\n",
      "Iteration 35, loss = 0.13194147\n",
      "Iteration 36, loss = 0.12710024\n",
      "Iteration 37, loss = 0.12276251\n",
      "Iteration 38, loss = 0.11872775\n",
      "Iteration 39, loss = 0.11501170\n",
      "Iteration 40, loss = 0.11153886\n",
      "Iteration 41, loss = 0.10834941\n",
      "Iteration 42, loss = 0.10501454\n",
      "Iteration 43, loss = 0.10232859\n",
      "Iteration 44, loss = 0.09979835\n",
      "Iteration 45, loss = 0.09734250\n",
      "Iteration 46, loss = 0.09506556\n",
      "Iteration 47, loss = 0.09293028\n",
      "Iteration 48, loss = 0.09090109\n",
      "Iteration 49, loss = 0.08901895\n",
      "Iteration 50, loss = 0.08729653\n",
      "Iteration 51, loss = 0.08570540\n",
      "Iteration 52, loss = 0.08400536\n",
      "Iteration 53, loss = 0.08257652\n",
      "Iteration 54, loss = 0.08116509\n",
      "Iteration 55, loss = 0.07994647\n",
      "Iteration 56, loss = 0.07875063\n",
      "Iteration 57, loss = 0.07762398\n",
      "Iteration 58, loss = 0.07648772\n",
      "Iteration 59, loss = 0.07539886\n",
      "Iteration 60, loss = 0.07443527\n",
      "Iteration 61, loss = 0.07348670\n",
      "Iteration 62, loss = 0.07265338\n",
      "Iteration 63, loss = 0.07179589\n",
      "Iteration 64, loss = 0.07098548\n",
      "Iteration 65, loss = 0.07024326\n",
      "Iteration 66, loss = 0.06950432\n",
      "Iteration 67, loss = 0.06888125\n",
      "Iteration 68, loss = 0.06820632\n",
      "Iteration 69, loss = 0.06753450\n",
      "Iteration 70, loss = 0.06691572\n",
      "Iteration 71, loss = 0.06631430\n",
      "Iteration 72, loss = 0.06577019\n",
      "Iteration 73, loss = 0.06524506\n",
      "Iteration 74, loss = 0.06469064\n",
      "Iteration 75, loss = 0.06419223\n",
      "Iteration 76, loss = 0.06372656\n",
      "Iteration 77, loss = 0.06329549\n",
      "Iteration 78, loss = 0.06278978\n",
      "Iteration 79, loss = 0.06233467\n",
      "Iteration 80, loss = 0.06195351\n",
      "Iteration 81, loss = 0.06153890\n",
      "Iteration 82, loss = 0.06109669\n",
      "Iteration 83, loss = 0.06073391\n",
      "Iteration 84, loss = 0.06035316\n",
      "Iteration 85, loss = 0.05997697\n",
      "Iteration 86, loss = 0.05963926\n",
      "Iteration 87, loss = 0.05929799\n",
      "Iteration 88, loss = 0.05898407\n",
      "Iteration 89, loss = 0.05866073\n",
      "Iteration 90, loss = 0.05836441\n",
      "Iteration 91, loss = 0.05801191\n",
      "Iteration 92, loss = 0.05773826\n",
      "Iteration 93, loss = 0.05746629\n",
      "Iteration 94, loss = 0.05719934\n",
      "Iteration 95, loss = 0.05690881\n",
      "Iteration 96, loss = 0.05661492\n",
      "Iteration 97, loss = 0.05634720\n",
      "Iteration 98, loss = 0.05612846\n",
      "Iteration 99, loss = 0.05586355\n",
      "Iteration 100, loss = 0.05559482\n",
      "Iteration 101, loss = 0.05535787\n",
      "Iteration 102, loss = 0.05512449\n",
      "Iteration 103, loss = 0.05488026\n",
      "Iteration 104, loss = 0.05466849\n",
      "Iteration 105, loss = 0.05444597\n",
      "Iteration 106, loss = 0.05425006\n",
      "Iteration 107, loss = 0.05401395\n",
      "Iteration 108, loss = 0.05379487\n",
      "Iteration 109, loss = 0.05359725\n",
      "Iteration 110, loss = 0.05339076\n",
      "Iteration 111, loss = 0.05319560\n",
      "Iteration 112, loss = 0.05301346\n",
      "Iteration 113, loss = 0.05281762\n",
      "Iteration 114, loss = 0.05259214\n",
      "Iteration 115, loss = 0.05242991\n",
      "Iteration 116, loss = 0.05223707\n",
      "Iteration 117, loss = 0.05207859\n",
      "Iteration 118, loss = 0.05190727\n",
      "Iteration 119, loss = 0.05171771\n",
      "Iteration 120, loss = 0.05153612\n",
      "Iteration 121, loss = 0.05137115\n",
      "Iteration 122, loss = 0.05121296\n",
      "Iteration 123, loss = 0.05105858\n",
      "Iteration 124, loss = 0.05089088\n",
      "Iteration 125, loss = 0.05073889\n",
      "Iteration 126, loss = 0.05058584\n",
      "Iteration 127, loss = 0.05043380\n",
      "Iteration 128, loss = 0.05030870\n",
      "Iteration 129, loss = 0.05016248\n",
      "Iteration 130, loss = 0.04999665\n",
      "Iteration 131, loss = 0.04984844\n",
      "Iteration 132, loss = 0.04969288\n",
      "Iteration 133, loss = 0.04956272\n",
      "Iteration 134, loss = 0.04942697\n",
      "Iteration 135, loss = 0.04929641\n",
      "Iteration 136, loss = 0.04915306\n",
      "Iteration 137, loss = 0.04901459\n",
      "Iteration 138, loss = 0.04890363\n",
      "Iteration 139, loss = 0.04877173\n",
      "Iteration 140, loss = 0.04863350\n",
      "Iteration 141, loss = 0.04849430\n",
      "Iteration 142, loss = 0.04836719\n",
      "Iteration 143, loss = 0.04824994\n",
      "Iteration 144, loss = 0.04812657\n",
      "Iteration 145, loss = 0.04800723\n",
      "Iteration 146, loss = 0.04788905\n",
      "Iteration 147, loss = 0.04775758\n",
      "Iteration 148, loss = 0.04763636\n",
      "Iteration 149, loss = 0.04752367\n",
      "Iteration 150, loss = 0.04740764\n",
      "Iteration 151, loss = 0.04729410\n",
      "Iteration 152, loss = 0.04718560\n",
      "Iteration 153, loss = 0.04707413\n",
      "Iteration 154, loss = 0.04695297\n",
      "Iteration 155, loss = 0.04685111\n",
      "Iteration 156, loss = 0.04674234\n",
      "Iteration 157, loss = 0.04663640\n",
      "Iteration 158, loss = 0.04652188\n",
      "Iteration 159, loss = 0.04643280\n",
      "Iteration 160, loss = 0.04634461\n",
      "Iteration 161, loss = 0.04623385\n",
      "Iteration 162, loss = 0.04611564\n",
      "Iteration 163, loss = 0.04600980\n",
      "Iteration 164, loss = 0.04592941\n",
      "Iteration 165, loss = 0.04581455\n",
      "Iteration 166, loss = 0.04573368\n",
      "Iteration 167, loss = 0.04562445\n",
      "Iteration 168, loss = 0.04554070\n",
      "Iteration 169, loss = 0.04544203\n",
      "Iteration 170, loss = 0.04536535\n",
      "Iteration 171, loss = 0.04529883\n",
      "Iteration 172, loss = 0.04518047\n",
      "Iteration 173, loss = 0.04508381\n",
      "Iteration 174, loss = 0.04499380\n",
      "Iteration 175, loss = 0.04491473\n",
      "Iteration 176, loss = 0.04483772\n",
      "Iteration 177, loss = 0.04474413\n",
      "Iteration 178, loss = 0.04465516\n",
      "Iteration 179, loss = 0.04457484\n",
      "Iteration 180, loss = 0.04450104\n",
      "Iteration 181, loss = 0.04443414\n",
      "Iteration 182, loss = 0.04432079\n",
      "Iteration 183, loss = 0.04424053\n",
      "Iteration 184, loss = 0.04416730\n",
      "Iteration 185, loss = 0.04409991\n",
      "Iteration 186, loss = 0.04400563\n",
      "Iteration 187, loss = 0.04394709\n",
      "Iteration 188, loss = 0.04385873\n",
      "Iteration 189, loss = 0.04377717\n",
      "Iteration 190, loss = 0.04371056\n",
      "Iteration 191, loss = 0.04365093\n",
      "Iteration 192, loss = 0.04356396\n",
      "Iteration 193, loss = 0.04348509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31432238\n",
      "Iteration 2, loss = 2.13480825\n",
      "Iteration 3, loss = 1.93910641\n",
      "Iteration 4, loss = 1.73009815\n",
      "Iteration 5, loss = 1.52622343\n",
      "Iteration 6, loss = 1.33623818\n",
      "Iteration 7, loss = 1.16801340\n",
      "Iteration 8, loss = 1.01870181\n",
      "Iteration 9, loss = 0.89233626\n",
      "Iteration 10, loss = 0.78566214\n",
      "Iteration 11, loss = 0.69608428\n",
      "Iteration 12, loss = 0.62123744\n",
      "Iteration 13, loss = 0.55810964\n",
      "Iteration 14, loss = 0.50404004\n",
      "Iteration 15, loss = 0.45865161\n",
      "Iteration 16, loss = 0.41814442\n",
      "Iteration 17, loss = 0.38390712\n",
      "Iteration 18, loss = 0.35352364\n",
      "Iteration 19, loss = 0.32635378\n",
      "Iteration 20, loss = 0.30308840\n",
      "Iteration 21, loss = 0.28221537\n",
      "Iteration 22, loss = 0.26363994\n",
      "Iteration 23, loss = 0.24620443\n",
      "Iteration 24, loss = 0.23114900\n",
      "Iteration 25, loss = 0.21800379\n",
      "Iteration 26, loss = 0.20503021\n",
      "Iteration 27, loss = 0.19400188\n",
      "Iteration 28, loss = 0.18410291\n",
      "Iteration 29, loss = 0.17452494\n",
      "Iteration 30, loss = 0.16596782\n",
      "Iteration 31, loss = 0.15835261\n",
      "Iteration 32, loss = 0.15125722\n",
      "Iteration 33, loss = 0.14484123\n",
      "Iteration 34, loss = 0.13873430\n",
      "Iteration 35, loss = 0.13333405\n",
      "Iteration 36, loss = 0.12866381\n",
      "Iteration 37, loss = 0.12409950\n",
      "Iteration 38, loss = 0.11981596\n",
      "Iteration 39, loss = 0.11614940\n",
      "Iteration 40, loss = 0.11252265\n",
      "Iteration 41, loss = 0.10938443\n",
      "Iteration 42, loss = 0.10622738\n",
      "Iteration 43, loss = 0.10341912\n",
      "Iteration 44, loss = 0.10061103\n",
      "Iteration 45, loss = 0.09812115\n",
      "Iteration 46, loss = 0.09609301\n",
      "Iteration 47, loss = 0.09366897\n",
      "Iteration 48, loss = 0.09160656\n",
      "Iteration 49, loss = 0.08977919\n",
      "Iteration 50, loss = 0.08806981\n",
      "Iteration 51, loss = 0.08643337\n",
      "Iteration 52, loss = 0.08478247\n",
      "Iteration 53, loss = 0.08327494\n",
      "Iteration 54, loss = 0.08199035\n",
      "Iteration 55, loss = 0.08076109\n",
      "Iteration 56, loss = 0.07952775\n",
      "Iteration 57, loss = 0.07830042\n",
      "Iteration 58, loss = 0.07715258\n",
      "Iteration 59, loss = 0.07610900\n",
      "Iteration 60, loss = 0.07511214\n",
      "Iteration 61, loss = 0.07419353\n",
      "Iteration 62, loss = 0.07326858\n",
      "Iteration 63, loss = 0.07247740\n",
      "Iteration 64, loss = 0.07167280\n",
      "Iteration 65, loss = 0.07086797\n",
      "Iteration 66, loss = 0.07014844\n",
      "Iteration 67, loss = 0.06948404\n",
      "Iteration 68, loss = 0.06881315\n",
      "Iteration 69, loss = 0.06818313\n",
      "Iteration 70, loss = 0.06759547\n",
      "Iteration 71, loss = 0.06698583\n",
      "Iteration 72, loss = 0.06641287\n",
      "Iteration 73, loss = 0.06584015\n",
      "Iteration 74, loss = 0.06530154\n",
      "Iteration 75, loss = 0.06482043\n",
      "Iteration 76, loss = 0.06434659\n",
      "Iteration 77, loss = 0.06387758\n",
      "Iteration 78, loss = 0.06341981\n",
      "Iteration 79, loss = 0.06298490\n",
      "Iteration 80, loss = 0.06256063\n",
      "Iteration 81, loss = 0.06214116\n",
      "Iteration 82, loss = 0.06173708\n",
      "Iteration 83, loss = 0.06139962\n",
      "Iteration 84, loss = 0.06101502\n",
      "Iteration 85, loss = 0.06064573\n",
      "Iteration 86, loss = 0.06030818\n",
      "Iteration 87, loss = 0.05996544\n",
      "Iteration 88, loss = 0.05963460\n",
      "Iteration 89, loss = 0.05929031\n",
      "Iteration 90, loss = 0.05897584\n",
      "Iteration 91, loss = 0.05870135\n",
      "Iteration 92, loss = 0.05838310\n",
      "Iteration 93, loss = 0.05810326\n",
      "Iteration 94, loss = 0.05781912\n",
      "Iteration 95, loss = 0.05753598\n",
      "Iteration 96, loss = 0.05726433\n",
      "Iteration 97, loss = 0.05700366\n",
      "Iteration 98, loss = 0.05674339\n",
      "Iteration 99, loss = 0.05649939\n",
      "Iteration 100, loss = 0.05624546\n",
      "Iteration 101, loss = 0.05601978\n",
      "Iteration 102, loss = 0.05577066\n",
      "Iteration 103, loss = 0.05554635\n",
      "Iteration 104, loss = 0.05532719\n",
      "Iteration 105, loss = 0.05510820\n",
      "Iteration 106, loss = 0.05488724\n",
      "Iteration 107, loss = 0.05466257\n",
      "Iteration 108, loss = 0.05446922\n",
      "Iteration 109, loss = 0.05426617\n",
      "Iteration 110, loss = 0.05406284\n",
      "Iteration 111, loss = 0.05385667\n",
      "Iteration 112, loss = 0.05368518\n",
      "Iteration 113, loss = 0.05348173\n",
      "Iteration 114, loss = 0.05328089\n",
      "Iteration 115, loss = 0.05310129\n",
      "Iteration 116, loss = 0.05292791\n",
      "Iteration 117, loss = 0.05274431\n",
      "Iteration 118, loss = 0.05256731\n",
      "Iteration 119, loss = 0.05241518\n",
      "Iteration 120, loss = 0.05221636\n",
      "Iteration 121, loss = 0.05206044\n",
      "Iteration 122, loss = 0.05189132\n",
      "Iteration 123, loss = 0.05172469\n",
      "Iteration 124, loss = 0.05155630\n",
      "Iteration 125, loss = 0.05142049\n",
      "Iteration 126, loss = 0.05126713\n",
      "Iteration 127, loss = 0.05109651\n",
      "Iteration 128, loss = 0.05093682\n",
      "Iteration 129, loss = 0.05079477\n",
      "Iteration 130, loss = 0.05063831\n",
      "Iteration 131, loss = 0.05049699\n",
      "Iteration 132, loss = 0.05034350\n",
      "Iteration 133, loss = 0.05021781\n",
      "Iteration 134, loss = 0.05008630\n",
      "Iteration 135, loss = 0.04994255\n",
      "Iteration 136, loss = 0.04980136\n",
      "Iteration 137, loss = 0.04967506\n",
      "Iteration 138, loss = 0.04954701\n",
      "Iteration 139, loss = 0.04944995\n",
      "Iteration 140, loss = 0.04928458\n",
      "Iteration 141, loss = 0.04915455\n",
      "Iteration 142, loss = 0.04902917\n",
      "Iteration 143, loss = 0.04889561\n",
      "Iteration 144, loss = 0.04877858\n",
      "Iteration 145, loss = 0.04866175\n",
      "Iteration 146, loss = 0.04853396\n",
      "Iteration 147, loss = 0.04840700\n",
      "Iteration 148, loss = 0.04829680\n",
      "Iteration 149, loss = 0.04816833\n",
      "Iteration 150, loss = 0.04805716\n",
      "Iteration 151, loss = 0.04794736\n",
      "Iteration 152, loss = 0.04783332\n",
      "Iteration 153, loss = 0.04771381\n",
      "Iteration 154, loss = 0.04759865\n",
      "Iteration 155, loss = 0.04750816\n",
      "Iteration 156, loss = 0.04737890\n",
      "Iteration 157, loss = 0.04728135\n",
      "Iteration 158, loss = 0.04717399\n",
      "Iteration 159, loss = 0.04706579\n",
      "Iteration 160, loss = 0.04696718\n",
      "Iteration 161, loss = 0.04686657\n",
      "Iteration 162, loss = 0.04676587\n",
      "Iteration 163, loss = 0.04666801\n",
      "Iteration 164, loss = 0.04656030\n",
      "Iteration 165, loss = 0.04646942\n",
      "Iteration 166, loss = 0.04637713\n",
      "Iteration 167, loss = 0.04628216\n",
      "Iteration 168, loss = 0.04618993\n",
      "Iteration 169, loss = 0.04609934\n",
      "Iteration 170, loss = 0.04601347\n",
      "Iteration 171, loss = 0.04592295\n",
      "Iteration 172, loss = 0.04583150\n",
      "Iteration 173, loss = 0.04574414\n",
      "Iteration 174, loss = 0.04564910\n",
      "Iteration 175, loss = 0.04556150\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31260286\n",
      "Iteration 2, loss = 2.13731215\n",
      "Iteration 3, loss = 1.94416291\n",
      "Iteration 4, loss = 1.73615051\n",
      "Iteration 5, loss = 1.53434670\n",
      "Iteration 6, loss = 1.34446981\n",
      "Iteration 7, loss = 1.17374865\n",
      "Iteration 8, loss = 1.02442934\n",
      "Iteration 9, loss = 0.89707850\n",
      "Iteration 10, loss = 0.78986884\n",
      "Iteration 11, loss = 0.69921834\n",
      "Iteration 12, loss = 0.62375619\n",
      "Iteration 13, loss = 0.56060696\n",
      "Iteration 14, loss = 0.50617158\n",
      "Iteration 15, loss = 0.46088860\n",
      "Iteration 16, loss = 0.42040579\n",
      "Iteration 17, loss = 0.38580472\n",
      "Iteration 18, loss = 0.35613622\n",
      "Iteration 19, loss = 0.32869704\n",
      "Iteration 20, loss = 0.30540640\n",
      "Iteration 21, loss = 0.28404489\n",
      "Iteration 22, loss = 0.26567262\n",
      "Iteration 23, loss = 0.24838346\n",
      "Iteration 24, loss = 0.23337385\n",
      "Iteration 25, loss = 0.22033350\n",
      "Iteration 26, loss = 0.20748412\n",
      "Iteration 27, loss = 0.19625622\n",
      "Iteration 28, loss = 0.18627288\n",
      "Iteration 29, loss = 0.17669339\n",
      "Iteration 30, loss = 0.16797768\n",
      "Iteration 31, loss = 0.16030332\n",
      "Iteration 32, loss = 0.15348287\n",
      "Iteration 33, loss = 0.14684778\n",
      "Iteration 34, loss = 0.14083274\n",
      "Iteration 35, loss = 0.13530814\n",
      "Iteration 36, loss = 0.13040773\n",
      "Iteration 37, loss = 0.12577965\n",
      "Iteration 38, loss = 0.12158373\n",
      "Iteration 39, loss = 0.11763424\n",
      "Iteration 40, loss = 0.11403257\n",
      "Iteration 41, loss = 0.11095786\n",
      "Iteration 42, loss = 0.10749064\n",
      "Iteration 43, loss = 0.10462519\n",
      "Iteration 44, loss = 0.10187102\n",
      "Iteration 45, loss = 0.09933562\n",
      "Iteration 46, loss = 0.09736829\n",
      "Iteration 47, loss = 0.09496701\n",
      "Iteration 48, loss = 0.09278741\n",
      "Iteration 49, loss = 0.09097194\n",
      "Iteration 50, loss = 0.08921095\n",
      "Iteration 51, loss = 0.08754430\n",
      "Iteration 52, loss = 0.08589345\n",
      "Iteration 53, loss = 0.08431928\n",
      "Iteration 54, loss = 0.08293020\n",
      "Iteration 55, loss = 0.08169274\n",
      "Iteration 56, loss = 0.08040430\n",
      "Iteration 57, loss = 0.07921126\n",
      "Iteration 58, loss = 0.07808844\n",
      "Iteration 59, loss = 0.07700646\n",
      "Iteration 60, loss = 0.07597288\n",
      "Iteration 61, loss = 0.07503965\n",
      "Iteration 62, loss = 0.07414563\n",
      "Iteration 63, loss = 0.07335650\n",
      "Iteration 64, loss = 0.07249296\n",
      "Iteration 65, loss = 0.07170488\n",
      "Iteration 66, loss = 0.07101433\n",
      "Iteration 67, loss = 0.07027321\n",
      "Iteration 68, loss = 0.06959557\n",
      "Iteration 69, loss = 0.06892797\n",
      "Iteration 70, loss = 0.06831455\n",
      "Iteration 71, loss = 0.06770885\n",
      "Iteration 72, loss = 0.06711183\n",
      "Iteration 73, loss = 0.06657797\n",
      "Iteration 74, loss = 0.06600414\n",
      "Iteration 75, loss = 0.06552002\n",
      "Iteration 76, loss = 0.06505365\n",
      "Iteration 77, loss = 0.06457827\n",
      "Iteration 78, loss = 0.06409526\n",
      "Iteration 79, loss = 0.06367618\n",
      "Iteration 80, loss = 0.06324040\n",
      "Iteration 81, loss = 0.06280854\n",
      "Iteration 82, loss = 0.06240274\n",
      "Iteration 83, loss = 0.06204667\n",
      "Iteration 84, loss = 0.06165842\n",
      "Iteration 85, loss = 0.06129070\n",
      "Iteration 86, loss = 0.06094764\n",
      "Iteration 87, loss = 0.06059357\n",
      "Iteration 88, loss = 0.06025767\n",
      "Iteration 89, loss = 0.05991179\n",
      "Iteration 90, loss = 0.05960620\n",
      "Iteration 91, loss = 0.05931163\n",
      "Iteration 92, loss = 0.05900464\n",
      "Iteration 93, loss = 0.05871970\n",
      "Iteration 94, loss = 0.05846974\n",
      "Iteration 95, loss = 0.05816993\n",
      "Iteration 96, loss = 0.05788696\n",
      "Iteration 97, loss = 0.05762858\n",
      "Iteration 98, loss = 0.05737450\n",
      "Iteration 99, loss = 0.05710058\n",
      "Iteration 100, loss = 0.05685583\n",
      "Iteration 101, loss = 0.05663180\n",
      "Iteration 102, loss = 0.05638212\n",
      "Iteration 103, loss = 0.05616429\n",
      "Iteration 104, loss = 0.05593609\n",
      "Iteration 105, loss = 0.05570317\n",
      "Iteration 106, loss = 0.05548355\n",
      "Iteration 107, loss = 0.05525528\n",
      "Iteration 108, loss = 0.05506053\n",
      "Iteration 109, loss = 0.05485142\n",
      "Iteration 110, loss = 0.05464239\n",
      "Iteration 111, loss = 0.05444475\n",
      "Iteration 112, loss = 0.05425941\n",
      "Iteration 113, loss = 0.05406963\n",
      "Iteration 114, loss = 0.05387668\n",
      "Iteration 115, loss = 0.05368752\n",
      "Iteration 116, loss = 0.05350102\n",
      "Iteration 117, loss = 0.05332789\n",
      "Iteration 118, loss = 0.05314652\n",
      "Iteration 119, loss = 0.05297907\n",
      "Iteration 120, loss = 0.05279937\n",
      "Iteration 121, loss = 0.05264933\n",
      "Iteration 122, loss = 0.05245688\n",
      "Iteration 123, loss = 0.05227780\n",
      "Iteration 124, loss = 0.05212325\n",
      "Iteration 125, loss = 0.05197749\n",
      "Iteration 126, loss = 0.05183601\n",
      "Iteration 127, loss = 0.05164946\n",
      "Iteration 128, loss = 0.05149615\n",
      "Iteration 129, loss = 0.05135307\n",
      "Iteration 130, loss = 0.05120726\n",
      "Iteration 131, loss = 0.05105520\n",
      "Iteration 132, loss = 0.05090757\n",
      "Iteration 133, loss = 0.05076730\n",
      "Iteration 134, loss = 0.05062554\n",
      "Iteration 135, loss = 0.05049441\n",
      "Iteration 136, loss = 0.05036109\n",
      "Iteration 137, loss = 0.05023185\n",
      "Iteration 138, loss = 0.05010334\n",
      "Iteration 139, loss = 0.05000687\n",
      "Iteration 140, loss = 0.04983701\n",
      "Iteration 141, loss = 0.04970114\n",
      "Iteration 142, loss = 0.04958522\n",
      "Iteration 143, loss = 0.04946343\n",
      "Iteration 144, loss = 0.04932804\n",
      "Iteration 145, loss = 0.04922929\n",
      "Iteration 146, loss = 0.04908996\n",
      "Iteration 147, loss = 0.04895377\n",
      "Iteration 148, loss = 0.04886450\n",
      "Iteration 149, loss = 0.04873293\n",
      "Iteration 150, loss = 0.04861740\n",
      "Iteration 151, loss = 0.04849532\n",
      "Iteration 152, loss = 0.04838518\n",
      "Iteration 153, loss = 0.04826071\n",
      "Iteration 154, loss = 0.04815604\n",
      "Iteration 155, loss = 0.04806368\n",
      "Iteration 156, loss = 0.04794170\n",
      "Iteration 157, loss = 0.04783399\n",
      "Iteration 158, loss = 0.04772670\n",
      "Iteration 159, loss = 0.04763214\n",
      "Iteration 160, loss = 0.04752717\n",
      "Iteration 161, loss = 0.04743172\n",
      "Iteration 162, loss = 0.04732546\n",
      "Iteration 163, loss = 0.04722403\n",
      "Iteration 164, loss = 0.04713096\n",
      "Iteration 165, loss = 0.04704063\n",
      "Iteration 166, loss = 0.04695574\n",
      "Iteration 167, loss = 0.04686286\n",
      "Iteration 168, loss = 0.04676030\n",
      "Iteration 169, loss = 0.04667181\n",
      "Iteration 170, loss = 0.04658790\n",
      "Iteration 171, loss = 0.04649359\n",
      "Iteration 172, loss = 0.04639051\n",
      "Iteration 173, loss = 0.04630595\n",
      "Iteration 174, loss = 0.04620472\n",
      "Iteration 175, loss = 0.04612791\n",
      "Iteration 176, loss = 0.04603499\n",
      "Iteration 177, loss = 0.04596085\n",
      "Iteration 178, loss = 0.04587135\n",
      "Iteration 179, loss = 0.04578952\n",
      "Iteration 180, loss = 0.04570630\n",
      "Iteration 181, loss = 0.04561165\n",
      "Iteration 182, loss = 0.04553116\n",
      "Iteration 183, loss = 0.04547541\n",
      "Iteration 184, loss = 0.04537575\n",
      "Iteration 185, loss = 0.04529786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30822685\n",
      "Iteration 2, loss = 2.12829518\n",
      "Iteration 3, loss = 1.92889987\n",
      "Iteration 4, loss = 1.71266597\n",
      "Iteration 5, loss = 1.50448147\n",
      "Iteration 6, loss = 1.30862701\n",
      "Iteration 7, loss = 1.13356968\n",
      "Iteration 8, loss = 0.98211419\n",
      "Iteration 9, loss = 0.85388082\n",
      "Iteration 10, loss = 0.74780809\n",
      "Iteration 11, loss = 0.65764316\n",
      "Iteration 12, loss = 0.58405875\n",
      "Iteration 13, loss = 0.52240660\n",
      "Iteration 14, loss = 0.46924907\n",
      "Iteration 15, loss = 0.42571506\n",
      "Iteration 16, loss = 0.38716824\n",
      "Iteration 17, loss = 0.35448326\n",
      "Iteration 18, loss = 0.32657192\n",
      "Iteration 19, loss = 0.30058176\n",
      "Iteration 20, loss = 0.27872223\n",
      "Iteration 21, loss = 0.25916805\n",
      "Iteration 22, loss = 0.24224007\n",
      "Iteration 23, loss = 0.22660597\n",
      "Iteration 24, loss = 0.21235336\n",
      "Iteration 25, loss = 0.20008182\n",
      "Iteration 26, loss = 0.18880448\n",
      "Iteration 27, loss = 0.17841825\n",
      "Iteration 28, loss = 0.16913031\n",
      "Iteration 29, loss = 0.16093563\n",
      "Iteration 30, loss = 0.15318010\n",
      "Iteration 31, loss = 0.14638153\n",
      "Iteration 32, loss = 0.14013213\n",
      "Iteration 33, loss = 0.13431289\n",
      "Iteration 34, loss = 0.12889599\n",
      "Iteration 35, loss = 0.12406745\n",
      "Iteration 36, loss = 0.11964421\n",
      "Iteration 37, loss = 0.11550609\n",
      "Iteration 38, loss = 0.11184459\n",
      "Iteration 39, loss = 0.10828881\n",
      "Iteration 40, loss = 0.10505627\n",
      "Iteration 41, loss = 0.10224263\n",
      "Iteration 42, loss = 0.09937901\n",
      "Iteration 43, loss = 0.09680437\n",
      "Iteration 44, loss = 0.09450391\n",
      "Iteration 45, loss = 0.09222030\n",
      "Iteration 46, loss = 0.09034812\n",
      "Iteration 47, loss = 0.08845236\n",
      "Iteration 48, loss = 0.08646834\n",
      "Iteration 49, loss = 0.08484096\n",
      "Iteration 50, loss = 0.08325897\n",
      "Iteration 51, loss = 0.08180163\n",
      "Iteration 52, loss = 0.08036834\n",
      "Iteration 53, loss = 0.07897965\n",
      "Iteration 54, loss = 0.07776529\n",
      "Iteration 55, loss = 0.07666480\n",
      "Iteration 56, loss = 0.07548544\n",
      "Iteration 57, loss = 0.07439191\n",
      "Iteration 58, loss = 0.07343042\n",
      "Iteration 59, loss = 0.07250181\n",
      "Iteration 60, loss = 0.07160392\n",
      "Iteration 61, loss = 0.07081392\n",
      "Iteration 62, loss = 0.07001909\n",
      "Iteration 63, loss = 0.06924510\n",
      "Iteration 64, loss = 0.06851901\n",
      "Iteration 65, loss = 0.06785278\n",
      "Iteration 66, loss = 0.06718977\n",
      "Iteration 67, loss = 0.06657142\n",
      "Iteration 68, loss = 0.06592191\n",
      "Iteration 69, loss = 0.06534646\n",
      "Iteration 70, loss = 0.06480584\n",
      "Iteration 71, loss = 0.06426175\n",
      "Iteration 72, loss = 0.06372121\n",
      "Iteration 73, loss = 0.06321446\n",
      "Iteration 74, loss = 0.06274071\n",
      "Iteration 75, loss = 0.06230424\n",
      "Iteration 76, loss = 0.06183251\n",
      "Iteration 77, loss = 0.06138057\n",
      "Iteration 78, loss = 0.06096537\n",
      "Iteration 79, loss = 0.06056773\n",
      "Iteration 80, loss = 0.06018098\n",
      "Iteration 81, loss = 0.05982128\n",
      "Iteration 82, loss = 0.05942727\n",
      "Iteration 83, loss = 0.05907406\n",
      "Iteration 84, loss = 0.05873684\n",
      "Iteration 85, loss = 0.05839745\n",
      "Iteration 86, loss = 0.05810185\n",
      "Iteration 87, loss = 0.05777538\n",
      "Iteration 88, loss = 0.05744879\n",
      "Iteration 89, loss = 0.05713586\n",
      "Iteration 90, loss = 0.05684611\n",
      "Iteration 91, loss = 0.05656151\n",
      "Iteration 92, loss = 0.05630135\n",
      "Iteration 93, loss = 0.05602598\n",
      "Iteration 94, loss = 0.05578439\n",
      "Iteration 95, loss = 0.05549513\n",
      "Iteration 96, loss = 0.05525461\n",
      "Iteration 97, loss = 0.05500220\n",
      "Iteration 98, loss = 0.05474062\n",
      "Iteration 99, loss = 0.05449850\n",
      "Iteration 100, loss = 0.05427522\n",
      "Iteration 101, loss = 0.05405517\n",
      "Iteration 102, loss = 0.05383086\n",
      "Iteration 103, loss = 0.05360013\n",
      "Iteration 104, loss = 0.05339230\n",
      "Iteration 105, loss = 0.05317638\n",
      "Iteration 106, loss = 0.05296985\n",
      "Iteration 107, loss = 0.05275683\n",
      "Iteration 108, loss = 0.05256417\n",
      "Iteration 109, loss = 0.05237281\n",
      "Iteration 110, loss = 0.05218078\n",
      "Iteration 111, loss = 0.05197842\n",
      "Iteration 112, loss = 0.05180042\n",
      "Iteration 113, loss = 0.05162357\n",
      "Iteration 114, loss = 0.05143966\n",
      "Iteration 115, loss = 0.05126179\n",
      "Iteration 116, loss = 0.05108984\n",
      "Iteration 117, loss = 0.05092175\n",
      "Iteration 118, loss = 0.05076081\n",
      "Iteration 119, loss = 0.05059966\n",
      "Iteration 120, loss = 0.05042758\n",
      "Iteration 121, loss = 0.05027396\n",
      "Iteration 122, loss = 0.05009793\n",
      "Iteration 123, loss = 0.04992097\n",
      "Iteration 124, loss = 0.04976594\n",
      "Iteration 125, loss = 0.04962381\n",
      "Iteration 126, loss = 0.04948238\n",
      "Iteration 127, loss = 0.04932340\n",
      "Iteration 128, loss = 0.04916966\n",
      "Iteration 129, loss = 0.04903321\n",
      "Iteration 130, loss = 0.04889272\n",
      "Iteration 131, loss = 0.04875868\n",
      "Iteration 132, loss = 0.04860450\n",
      "Iteration 133, loss = 0.04846551\n",
      "Iteration 134, loss = 0.04833718\n",
      "Iteration 135, loss = 0.04821278\n",
      "Iteration 136, loss = 0.04809079\n",
      "Iteration 137, loss = 0.04795582\n",
      "Iteration 138, loss = 0.04782559\n",
      "Iteration 139, loss = 0.04770957\n",
      "Iteration 140, loss = 0.04757558\n",
      "Iteration 141, loss = 0.04744638\n",
      "Iteration 142, loss = 0.04731797\n",
      "Iteration 143, loss = 0.04720829\n",
      "Iteration 144, loss = 0.04709214\n",
      "Iteration 145, loss = 0.04698472\n",
      "Iteration 146, loss = 0.04685910\n",
      "Iteration 147, loss = 0.04673607\n",
      "Iteration 148, loss = 0.04664039\n",
      "Iteration 149, loss = 0.04652222\n",
      "Iteration 150, loss = 0.04640061\n",
      "Iteration 151, loss = 0.04628533\n",
      "Iteration 152, loss = 0.04616732\n",
      "Iteration 153, loss = 0.04605388\n",
      "Iteration 154, loss = 0.04594917\n",
      "Iteration 155, loss = 0.04585603\n",
      "Iteration 156, loss = 0.04576460\n",
      "Iteration 157, loss = 0.04564850\n",
      "Iteration 158, loss = 0.04553622\n",
      "Iteration 159, loss = 0.04545618\n",
      "Iteration 160, loss = 0.04535165\n",
      "Iteration 161, loss = 0.04524564\n",
      "Iteration 162, loss = 0.04514006\n",
      "Iteration 163, loss = 0.04504528\n",
      "Iteration 164, loss = 0.04496602\n",
      "Iteration 165, loss = 0.04487677\n",
      "Iteration 166, loss = 0.04478356\n",
      "Iteration 167, loss = 0.04468758\n",
      "Iteration 168, loss = 0.04460956\n",
      "Iteration 169, loss = 0.04452294\n",
      "Iteration 170, loss = 0.04442112\n",
      "Iteration 171, loss = 0.04433808\n",
      "Iteration 172, loss = 0.04424259\n",
      "Iteration 173, loss = 0.04415572\n",
      "Iteration 174, loss = 0.04406613\n",
      "Iteration 175, loss = 0.04397955\n",
      "Iteration 176, loss = 0.04390315\n",
      "Iteration 177, loss = 0.04382117\n",
      "Iteration 178, loss = 0.04372363\n",
      "Iteration 179, loss = 0.04366239\n",
      "Iteration 180, loss = 0.04357841\n",
      "Iteration 181, loss = 0.04349936\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.36191273\n",
      "Iteration 2, loss = 2.34932956\n",
      "Iteration 3, loss = 2.33271287\n",
      "Iteration 4, loss = 2.31501079\n",
      "Iteration 5, loss = 2.29660646\n",
      "Iteration 6, loss = 2.27854627\n",
      "Iteration 7, loss = 2.26026332\n",
      "Iteration 8, loss = 2.24201115\n",
      "Iteration 9, loss = 2.22330040\n",
      "Iteration 10, loss = 2.20412356\n",
      "Iteration 11, loss = 2.18431007\n",
      "Iteration 12, loss = 2.16404386\n",
      "Iteration 13, loss = 2.14341999\n",
      "Iteration 14, loss = 2.12267350\n",
      "Iteration 15, loss = 2.10124255\n",
      "Iteration 16, loss = 2.07900004\n",
      "Iteration 17, loss = 2.05628159\n",
      "Iteration 18, loss = 2.03313520\n",
      "Iteration 19, loss = 2.00960194\n",
      "Iteration 20, loss = 1.98568444\n",
      "Iteration 21, loss = 1.96141239\n",
      "Iteration 22, loss = 1.93679219\n",
      "Iteration 23, loss = 1.91172745\n",
      "Iteration 24, loss = 1.88687313\n",
      "Iteration 25, loss = 1.86157307\n",
      "Iteration 26, loss = 1.83594066\n",
      "Iteration 27, loss = 1.80991940\n",
      "Iteration 28, loss = 1.78366856\n",
      "Iteration 29, loss = 1.75718701\n",
      "Iteration 30, loss = 1.73073086\n",
      "Iteration 31, loss = 1.70461809\n",
      "Iteration 32, loss = 1.67818839\n",
      "Iteration 33, loss = 1.65179792\n",
      "Iteration 34, loss = 1.62533227\n",
      "Iteration 35, loss = 1.59892047\n",
      "Iteration 36, loss = 1.57243637\n",
      "Iteration 37, loss = 1.54614946\n",
      "Iteration 38, loss = 1.52052795\n",
      "Iteration 39, loss = 1.49521731\n",
      "Iteration 40, loss = 1.47043100\n",
      "Iteration 41, loss = 1.44580980\n",
      "Iteration 42, loss = 1.42158579\n",
      "Iteration 43, loss = 1.39773788\n",
      "Iteration 44, loss = 1.37437773\n",
      "Iteration 45, loss = 1.35112576\n",
      "Iteration 46, loss = 1.32865372\n",
      "Iteration 47, loss = 1.30622752\n",
      "Iteration 48, loss = 1.28452392\n",
      "Iteration 49, loss = 1.26300104\n",
      "Iteration 50, loss = 1.24247273\n",
      "Iteration 51, loss = 1.22215049\n",
      "Iteration 52, loss = 1.20202882\n",
      "Iteration 53, loss = 1.18292984\n",
      "Iteration 54, loss = 1.16386906\n",
      "Iteration 55, loss = 1.14556166\n",
      "Iteration 56, loss = 1.12770272\n",
      "Iteration 57, loss = 1.11029745\n",
      "Iteration 58, loss = 1.09332685\n",
      "Iteration 59, loss = 1.07675828\n",
      "Iteration 60, loss = 1.06050794\n",
      "Iteration 61, loss = 1.04454578\n",
      "Iteration 62, loss = 1.02912407\n",
      "Iteration 63, loss = 1.01396581\n",
      "Iteration 64, loss = 0.99928078\n",
      "Iteration 65, loss = 0.98472469\n",
      "Iteration 66, loss = 0.97072151\n",
      "Iteration 67, loss = 0.95690424\n",
      "Iteration 68, loss = 0.94370652\n",
      "Iteration 69, loss = 0.93049422\n",
      "Iteration 70, loss = 0.91771121\n",
      "Iteration 71, loss = 0.90513736\n",
      "Iteration 72, loss = 0.89297644\n",
      "Iteration 73, loss = 0.88114178\n",
      "Iteration 74, loss = 0.86949578\n",
      "Iteration 75, loss = 0.85802108\n",
      "Iteration 76, loss = 0.84687294\n",
      "Iteration 77, loss = 0.83615826\n",
      "Iteration 78, loss = 0.82559347\n",
      "Iteration 79, loss = 0.81517456\n",
      "Iteration 80, loss = 0.80499306\n",
      "Iteration 81, loss = 0.79506871\n",
      "Iteration 82, loss = 0.78539759\n",
      "Iteration 83, loss = 0.77587792\n",
      "Iteration 84, loss = 0.76668424\n",
      "Iteration 85, loss = 0.75738046\n",
      "Iteration 86, loss = 0.74840571\n",
      "Iteration 87, loss = 0.73962861\n",
      "Iteration 88, loss = 0.73100559\n",
      "Iteration 89, loss = 0.72269864\n",
      "Iteration 90, loss = 0.71464480\n",
      "Iteration 91, loss = 0.70663386\n",
      "Iteration 92, loss = 0.69887268\n",
      "Iteration 93, loss = 0.69109018\n",
      "Iteration 94, loss = 0.68350550\n",
      "Iteration 95, loss = 0.67600132\n",
      "Iteration 96, loss = 0.66870333\n",
      "Iteration 97, loss = 0.66173259\n",
      "Iteration 98, loss = 0.65492198\n",
      "Iteration 99, loss = 0.64813855\n",
      "Iteration 100, loss = 0.64141594\n",
      "Iteration 101, loss = 0.63466305\n",
      "Iteration 102, loss = 0.62823594\n",
      "Iteration 103, loss = 0.62180941\n",
      "Iteration 104, loss = 0.61565913\n",
      "Iteration 105, loss = 0.60970070\n",
      "Iteration 106, loss = 0.60361208\n",
      "Iteration 107, loss = 0.59764862\n",
      "Iteration 108, loss = 0.59177525\n",
      "Iteration 109, loss = 0.58599326\n",
      "Iteration 110, loss = 0.58030014\n",
      "Iteration 111, loss = 0.57494763\n",
      "Iteration 112, loss = 0.56972324\n",
      "Iteration 113, loss = 0.56445831\n",
      "Iteration 114, loss = 0.55897745\n",
      "Iteration 115, loss = 0.55379785\n",
      "Iteration 116, loss = 0.54867467\n",
      "Iteration 117, loss = 0.54384027\n",
      "Iteration 118, loss = 0.53877963\n",
      "Iteration 119, loss = 0.53397370\n",
      "Iteration 120, loss = 0.52933277\n",
      "Iteration 121, loss = 0.52460272\n",
      "Iteration 122, loss = 0.51990831\n",
      "Iteration 123, loss = 0.51543718\n",
      "Iteration 124, loss = 0.51091636\n",
      "Iteration 125, loss = 0.50667135\n",
      "Iteration 126, loss = 0.50237738\n",
      "Iteration 127, loss = 0.49821584\n",
      "Iteration 128, loss = 0.49412721\n",
      "Iteration 129, loss = 0.49019671\n",
      "Iteration 130, loss = 0.48598195\n",
      "Iteration 131, loss = 0.48195813\n",
      "Iteration 132, loss = 0.47796282\n",
      "Iteration 133, loss = 0.47409888\n",
      "Iteration 134, loss = 0.47029695\n",
      "Iteration 135, loss = 0.46660353\n",
      "Iteration 136, loss = 0.46279883\n",
      "Iteration 137, loss = 0.45914737\n",
      "Iteration 138, loss = 0.45556457\n",
      "Iteration 139, loss = 0.45201721\n",
      "Iteration 140, loss = 0.44866593\n",
      "Iteration 141, loss = 0.44509986\n",
      "Iteration 142, loss = 0.44170560\n",
      "Iteration 143, loss = 0.43838786\n",
      "Iteration 144, loss = 0.43506381\n",
      "Iteration 145, loss = 0.43175889\n",
      "Iteration 146, loss = 0.42859014\n",
      "Iteration 147, loss = 0.42537012\n",
      "Iteration 148, loss = 0.42221803\n",
      "Iteration 149, loss = 0.41910744\n",
      "Iteration 150, loss = 0.41601673\n",
      "Iteration 151, loss = 0.41312328\n",
      "Iteration 152, loss = 0.41008602\n",
      "Iteration 153, loss = 0.40713423\n",
      "Iteration 154, loss = 0.40426290\n",
      "Iteration 155, loss = 0.40146885\n",
      "Iteration 156, loss = 0.39863926\n",
      "Iteration 157, loss = 0.39581898\n",
      "Iteration 158, loss = 0.39309600\n",
      "Iteration 159, loss = 0.39033955\n",
      "Iteration 160, loss = 0.38772396\n",
      "Iteration 161, loss = 0.38500006\n",
      "Iteration 162, loss = 0.38226690\n",
      "Iteration 163, loss = 0.37957081\n",
      "Iteration 164, loss = 0.37713238\n",
      "Iteration 165, loss = 0.37447638\n",
      "Iteration 166, loss = 0.37211592\n",
      "Iteration 167, loss = 0.36951649\n",
      "Iteration 168, loss = 0.36708856\n",
      "Iteration 169, loss = 0.36464107\n",
      "Iteration 170, loss = 0.36221438\n",
      "Iteration 171, loss = 0.36005433\n",
      "Iteration 172, loss = 0.35763101\n",
      "Iteration 173, loss = 0.35530439\n",
      "Iteration 174, loss = 0.35299785\n",
      "Iteration 175, loss = 0.35075390\n",
      "Iteration 176, loss = 0.34864340\n",
      "Iteration 177, loss = 0.34641413\n",
      "Iteration 178, loss = 0.34429202\n",
      "Iteration 179, loss = 0.34199341\n",
      "Iteration 180, loss = 0.33990353\n",
      "Iteration 181, loss = 0.33775035\n",
      "Iteration 182, loss = 0.33558915\n",
      "Iteration 183, loss = 0.33348515\n",
      "Iteration 184, loss = 0.33149074\n",
      "Iteration 185, loss = 0.32947919\n",
      "Iteration 186, loss = 0.32752837\n",
      "Iteration 187, loss = 0.32569646\n",
      "Iteration 188, loss = 0.32363569\n",
      "Iteration 189, loss = 0.32165193\n",
      "Iteration 190, loss = 0.31968891\n",
      "Iteration 191, loss = 0.31787751\n",
      "Iteration 192, loss = 0.31588283\n",
      "Iteration 193, loss = 0.31404269\n",
      "Iteration 194, loss = 0.31216692\n",
      "Iteration 195, loss = 0.31031638\n",
      "Iteration 196, loss = 0.30842573\n",
      "Iteration 197, loss = 0.30667006\n",
      "Iteration 198, loss = 0.30489712\n",
      "Iteration 199, loss = 0.30311374\n",
      "Iteration 200, loss = 0.30149415\n",
      "Iteration 201, loss = 0.29981712\n",
      "Iteration 202, loss = 0.29813223\n",
      "Iteration 203, loss = 0.29651606\n",
      "Iteration 204, loss = 0.29478641\n",
      "Iteration 205, loss = 0.29312976\n",
      "Iteration 206, loss = 0.29150316\n",
      "Iteration 207, loss = 0.28999825\n",
      "Iteration 208, loss = 0.28831219\n",
      "Iteration 209, loss = 0.28672912\n",
      "Iteration 210, loss = 0.28515730\n",
      "Iteration 211, loss = 0.28366311\n",
      "Iteration 212, loss = 0.28219064\n",
      "Iteration 213, loss = 0.28058969\n",
      "Iteration 214, loss = 0.27916998\n",
      "Iteration 215, loss = 0.27771083\n",
      "Iteration 216, loss = 0.27623318\n",
      "Iteration 217, loss = 0.27479021\n",
      "Iteration 218, loss = 0.27337096\n",
      "Iteration 219, loss = 0.27195636\n",
      "Iteration 220, loss = 0.27063704\n",
      "Iteration 221, loss = 0.26928238\n",
      "Iteration 222, loss = 0.26788560\n",
      "Iteration 223, loss = 0.26648613\n",
      "Iteration 224, loss = 0.26511363\n",
      "Iteration 225, loss = 0.26375448\n",
      "Iteration 226, loss = 0.26242404\n",
      "Iteration 227, loss = 0.26104231\n",
      "Iteration 228, loss = 0.25977294\n",
      "Iteration 229, loss = 0.25858971\n",
      "Iteration 230, loss = 0.25726211\n",
      "Iteration 231, loss = 0.25590581\n",
      "Iteration 232, loss = 0.25466156\n",
      "Iteration 233, loss = 0.25335123\n",
      "Iteration 234, loss = 0.25218097\n",
      "Iteration 235, loss = 0.25093219\n",
      "Iteration 236, loss = 0.24975175\n",
      "Iteration 237, loss = 0.24853539\n",
      "Iteration 238, loss = 0.24734247\n",
      "Iteration 239, loss = 0.24618025\n",
      "Iteration 240, loss = 0.24510078\n",
      "Iteration 241, loss = 0.24391228\n",
      "Iteration 242, loss = 0.24277955\n",
      "Iteration 243, loss = 0.24156385\n",
      "Iteration 244, loss = 0.24048482\n",
      "Iteration 245, loss = 0.23930591\n",
      "Iteration 246, loss = 0.23824149\n",
      "Iteration 247, loss = 0.23710074\n",
      "Iteration 248, loss = 0.23607113\n",
      "Iteration 249, loss = 0.23490584\n",
      "Iteration 250, loss = 0.23389899\n",
      "Iteration 251, loss = 0.23284333\n",
      "Iteration 252, loss = 0.23171675\n",
      "Iteration 253, loss = 0.23072016\n",
      "Iteration 254, loss = 0.22972746\n",
      "Iteration 255, loss = 0.22864973\n",
      "Iteration 256, loss = 0.22764754\n",
      "Iteration 257, loss = 0.22660643\n",
      "Iteration 258, loss = 0.22556035\n",
      "Iteration 259, loss = 0.22463964\n",
      "Iteration 260, loss = 0.22360068\n",
      "Iteration 261, loss = 0.22267131\n",
      "Iteration 262, loss = 0.22167080\n",
      "Iteration 263, loss = 0.22076748\n",
      "Iteration 264, loss = 0.21976819\n",
      "Iteration 265, loss = 0.21881167\n",
      "Iteration 266, loss = 0.21788923\n",
      "Iteration 267, loss = 0.21699086\n",
      "Iteration 268, loss = 0.21602900\n",
      "Iteration 269, loss = 0.21513110\n",
      "Iteration 270, loss = 0.21428807\n",
      "Iteration 271, loss = 0.21339033\n",
      "Iteration 272, loss = 0.21245118\n",
      "Iteration 273, loss = 0.21149790\n",
      "Iteration 274, loss = 0.21058943\n",
      "Iteration 275, loss = 0.20973764\n",
      "Iteration 276, loss = 0.20888576\n",
      "Iteration 277, loss = 0.20804692\n",
      "Iteration 278, loss = 0.20722457\n",
      "Iteration 279, loss = 0.20635404\n",
      "Iteration 280, loss = 0.20552546\n",
      "Iteration 281, loss = 0.20472237\n",
      "Iteration 282, loss = 0.20388647\n",
      "Iteration 283, loss = 0.20304808\n",
      "Iteration 284, loss = 0.20225283\n",
      "Iteration 285, loss = 0.20146050\n",
      "Iteration 286, loss = 0.20072153\n",
      "Iteration 287, loss = 0.19988545\n",
      "Iteration 288, loss = 0.19909785\n",
      "Iteration 289, loss = 0.19833881\n",
      "Iteration 290, loss = 0.19758175\n",
      "Iteration 291, loss = 0.19684357\n",
      "Iteration 292, loss = 0.19603677\n",
      "Iteration 293, loss = 0.19533035\n",
      "Iteration 294, loss = 0.19455598\n",
      "Iteration 295, loss = 0.19381647\n",
      "Iteration 296, loss = 0.19306205\n",
      "Iteration 297, loss = 0.19239985\n",
      "Iteration 298, loss = 0.19160781\n",
      "Iteration 299, loss = 0.19094022\n",
      "Iteration 300, loss = 0.19017854\n",
      "Iteration 301, loss = 0.18949563\n",
      "Iteration 302, loss = 0.18878316\n",
      "Iteration 303, loss = 0.18813547\n",
      "Iteration 304, loss = 0.18742721\n",
      "Iteration 305, loss = 0.18673690\n",
      "Iteration 306, loss = 0.18607591\n",
      "Iteration 307, loss = 0.18535979\n",
      "Iteration 308, loss = 0.18471437\n",
      "Iteration 309, loss = 0.18407295\n",
      "Iteration 310, loss = 0.18338049\n",
      "Iteration 311, loss = 0.18268921\n",
      "Iteration 312, loss = 0.18205025\n",
      "Iteration 313, loss = 0.18138523\n",
      "Iteration 314, loss = 0.18067943\n",
      "Iteration 315, loss = 0.18006914\n",
      "Iteration 316, loss = 0.17940608\n",
      "Iteration 317, loss = 0.17874675\n",
      "Iteration 318, loss = 0.17812230\n",
      "Iteration 319, loss = 0.17746413\n",
      "Iteration 320, loss = 0.17689638\n",
      "Iteration 321, loss = 0.17619721\n",
      "Iteration 322, loss = 0.17561258\n",
      "Iteration 323, loss = 0.17497315\n",
      "Iteration 324, loss = 0.17434748\n",
      "Iteration 325, loss = 0.17372317\n",
      "Iteration 326, loss = 0.17311152\n",
      "Iteration 327, loss = 0.17251341\n",
      "Iteration 328, loss = 0.17191381\n",
      "Iteration 329, loss = 0.17131162\n",
      "Iteration 330, loss = 0.17074561\n",
      "Iteration 331, loss = 0.17015623\n",
      "Iteration 332, loss = 0.16956518\n",
      "Iteration 333, loss = 0.16902714\n",
      "Iteration 334, loss = 0.16847470\n",
      "Iteration 335, loss = 0.16795372\n",
      "Iteration 336, loss = 0.16736784\n",
      "Iteration 337, loss = 0.16683325\n",
      "Iteration 338, loss = 0.16629513\n",
      "Iteration 339, loss = 0.16573577\n",
      "Iteration 340, loss = 0.16517598\n",
      "Iteration 341, loss = 0.16461052\n",
      "Iteration 342, loss = 0.16410256\n",
      "Iteration 343, loss = 0.16356069\n",
      "Iteration 344, loss = 0.16302887\n",
      "Iteration 345, loss = 0.16251405\n",
      "Iteration 346, loss = 0.16199844\n",
      "Iteration 347, loss = 0.16145266\n",
      "Iteration 348, loss = 0.16094629\n",
      "Iteration 349, loss = 0.16042238\n",
      "Iteration 350, loss = 0.15989634\n",
      "Iteration 351, loss = 0.15937728\n",
      "Iteration 352, loss = 0.15885288\n",
      "Iteration 353, loss = 0.15843180\n",
      "Iteration 354, loss = 0.15786559\n",
      "Iteration 355, loss = 0.15737265\n",
      "Iteration 356, loss = 0.15687669\n",
      "Iteration 357, loss = 0.15635725\n",
      "Iteration 358, loss = 0.15587589\n",
      "Iteration 359, loss = 0.15541488\n",
      "Iteration 360, loss = 0.15492811\n",
      "Iteration 361, loss = 0.15448570\n",
      "Iteration 362, loss = 0.15402028\n",
      "Iteration 363, loss = 0.15356260\n",
      "Iteration 364, loss = 0.15304897\n",
      "Iteration 365, loss = 0.15261117\n",
      "Iteration 366, loss = 0.15216082\n",
      "Iteration 367, loss = 0.15174775\n",
      "Iteration 368, loss = 0.15131143\n",
      "Iteration 369, loss = 0.15086151\n",
      "Iteration 370, loss = 0.15038797\n",
      "Iteration 371, loss = 0.14991291\n",
      "Iteration 372, loss = 0.14939800\n",
      "Iteration 373, loss = 0.14895722\n",
      "Iteration 374, loss = 0.14854305\n",
      "Iteration 375, loss = 0.14810579\n",
      "Iteration 376, loss = 0.14766975\n",
      "Iteration 377, loss = 0.14729432\n",
      "Iteration 378, loss = 0.14682057\n",
      "Iteration 379, loss = 0.14640733\n",
      "Iteration 380, loss = 0.14603034\n",
      "Iteration 381, loss = 0.14561182\n",
      "Iteration 382, loss = 0.14519258\n",
      "Iteration 383, loss = 0.14476522\n",
      "Iteration 384, loss = 0.14433646\n",
      "Iteration 385, loss = 0.14393005\n",
      "Iteration 386, loss = 0.14347944\n",
      "Iteration 387, loss = 0.14307237\n",
      "Iteration 388, loss = 0.14266343\n",
      "Iteration 389, loss = 0.14225615\n",
      "Iteration 390, loss = 0.14186221\n",
      "Iteration 391, loss = 0.14149682\n",
      "Iteration 392, loss = 0.14109648\n",
      "Iteration 393, loss = 0.14068197\n",
      "Iteration 394, loss = 0.14027877\n",
      "Iteration 395, loss = 0.13988182\n",
      "Iteration 396, loss = 0.13952349\n",
      "Iteration 397, loss = 0.13913271\n",
      "Iteration 398, loss = 0.13879229\n",
      "Iteration 399, loss = 0.13835905\n",
      "Iteration 400, loss = 0.13799662\n",
      "Iteration 401, loss = 0.13762748\n",
      "Iteration 402, loss = 0.13723437\n",
      "Iteration 403, loss = 0.13688604\n",
      "Iteration 404, loss = 0.13651294\n",
      "Iteration 405, loss = 0.13612306\n",
      "Iteration 406, loss = 0.13577593\n",
      "Iteration 407, loss = 0.13545401\n",
      "Iteration 408, loss = 0.13507975\n",
      "Iteration 409, loss = 0.13469280\n",
      "Iteration 410, loss = 0.13434938\n",
      "Iteration 411, loss = 0.13401808\n",
      "Iteration 412, loss = 0.13366489\n",
      "Iteration 413, loss = 0.13331869\n",
      "Iteration 414, loss = 0.13294793\n",
      "Iteration 415, loss = 0.13260016\n",
      "Iteration 416, loss = 0.13227628\n",
      "Iteration 417, loss = 0.13193482\n",
      "Iteration 418, loss = 0.13159834\n",
      "Iteration 419, loss = 0.13125919\n",
      "Iteration 420, loss = 0.13097521\n",
      "Iteration 421, loss = 0.13064049\n",
      "Iteration 422, loss = 0.13028846\n",
      "Iteration 423, loss = 0.12994530\n",
      "Iteration 424, loss = 0.12959196\n",
      "Iteration 425, loss = 0.12926930\n",
      "Iteration 426, loss = 0.12895869\n",
      "Iteration 427, loss = 0.12865212\n",
      "Iteration 428, loss = 0.12831723\n",
      "Iteration 429, loss = 0.12799226\n",
      "Iteration 430, loss = 0.12770669\n",
      "Iteration 431, loss = 0.12735644\n",
      "Iteration 432, loss = 0.12704415\n",
      "Iteration 433, loss = 0.12673177\n",
      "Iteration 434, loss = 0.12643257\n",
      "Iteration 435, loss = 0.12614212\n",
      "Iteration 436, loss = 0.12583642\n",
      "Iteration 437, loss = 0.12554673\n",
      "Iteration 438, loss = 0.12524441\n",
      "Iteration 439, loss = 0.12494140\n",
      "Iteration 440, loss = 0.12464038\n",
      "Iteration 441, loss = 0.12434261\n",
      "Iteration 442, loss = 0.12405350\n",
      "Iteration 443, loss = 0.12377764\n",
      "Iteration 444, loss = 0.12351090\n",
      "Iteration 445, loss = 0.12320970\n",
      "Iteration 446, loss = 0.12292370\n",
      "Iteration 447, loss = 0.12262047\n",
      "Iteration 448, loss = 0.12230151\n",
      "Iteration 449, loss = 0.12201530\n",
      "Iteration 450, loss = 0.12173077\n",
      "Iteration 451, loss = 0.12146028\n",
      "Iteration 452, loss = 0.12116789\n",
      "Iteration 453, loss = 0.12088871\n",
      "Iteration 454, loss = 0.12059327\n",
      "Iteration 455, loss = 0.12030065\n",
      "Iteration 456, loss = 0.12002018\n",
      "Iteration 457, loss = 0.11976440\n",
      "Iteration 458, loss = 0.11947202\n",
      "Iteration 459, loss = 0.11923480\n",
      "Iteration 460, loss = 0.11893800\n",
      "Iteration 461, loss = 0.11867337\n",
      "Iteration 462, loss = 0.11840555\n",
      "Iteration 463, loss = 0.11815141\n",
      "Iteration 464, loss = 0.11787000\n",
      "Iteration 465, loss = 0.11760476\n",
      "Iteration 466, loss = 0.11735172\n",
      "Iteration 467, loss = 0.11709339\n",
      "Iteration 468, loss = 0.11683692\n",
      "Iteration 469, loss = 0.11657771\n",
      "Iteration 470, loss = 0.11631009\n",
      "Iteration 471, loss = 0.11603704\n",
      "Iteration 472, loss = 0.11577369\n",
      "Iteration 473, loss = 0.11553708\n",
      "Iteration 474, loss = 0.11528618\n",
      "Iteration 475, loss = 0.11504378\n",
      "Iteration 476, loss = 0.11480613\n",
      "Iteration 477, loss = 0.11455957\n",
      "Iteration 478, loss = 0.11431715\n",
      "Iteration 479, loss = 0.11404093\n",
      "Iteration 480, loss = 0.11381083\n",
      "Iteration 481, loss = 0.11360056\n",
      "Iteration 482, loss = 0.11334278\n",
      "Iteration 483, loss = 0.11311828\n",
      "Iteration 484, loss = 0.11285203\n",
      "Iteration 485, loss = 0.11262835\n",
      "Iteration 486, loss = 0.11239587\n",
      "Iteration 487, loss = 0.11216084\n",
      "Iteration 488, loss = 0.11192014\n",
      "Iteration 489, loss = 0.11165843\n",
      "Iteration 490, loss = 0.11141710\n",
      "Iteration 491, loss = 0.11121321\n",
      "Iteration 492, loss = 0.11096143\n",
      "Iteration 493, loss = 0.11072510\n",
      "Iteration 494, loss = 0.11053628\n",
      "Iteration 495, loss = 0.11030629\n",
      "Iteration 496, loss = 0.11008126\n",
      "Iteration 497, loss = 0.10985929\n",
      "Iteration 498, loss = 0.10962233\n",
      "Iteration 499, loss = 0.10942046\n",
      "Iteration 500, loss = 0.10917862\n",
      "Iteration 501, loss = 0.10894539\n",
      "Iteration 502, loss = 0.10872029\n",
      "Iteration 503, loss = 0.10852980\n",
      "Iteration 504, loss = 0.10831958\n",
      "Iteration 505, loss = 0.10809134\n",
      "Iteration 506, loss = 0.10785238\n",
      "Iteration 507, loss = 0.10764480\n",
      "Iteration 508, loss = 0.10743410\n",
      "Iteration 509, loss = 0.10721779\n",
      "Iteration 510, loss = 0.10700991\n",
      "Iteration 511, loss = 0.10681300\n",
      "Iteration 512, loss = 0.10658883\n",
      "Iteration 513, loss = 0.10636317\n",
      "Iteration 514, loss = 0.10616443\n",
      "Iteration 515, loss = 0.10593645\n",
      "Iteration 516, loss = 0.10572878\n",
      "Iteration 517, loss = 0.10549889\n",
      "Iteration 518, loss = 0.10531198\n",
      "Iteration 519, loss = 0.10506998\n",
      "Iteration 520, loss = 0.10487573\n",
      "Iteration 521, loss = 0.10471498\n",
      "Iteration 522, loss = 0.10451002\n",
      "Iteration 523, loss = 0.10433741\n",
      "Iteration 524, loss = 0.10411811\n",
      "Iteration 525, loss = 0.10393444\n",
      "Iteration 526, loss = 0.10372498\n",
      "Iteration 527, loss = 0.10353517\n",
      "Iteration 528, loss = 0.10334259\n",
      "Iteration 529, loss = 0.10313907\n",
      "Iteration 530, loss = 0.10296578\n",
      "Iteration 531, loss = 0.10278814\n",
      "Iteration 532, loss = 0.10260529\n",
      "Iteration 533, loss = 0.10240990\n",
      "Iteration 534, loss = 0.10222347\n",
      "Iteration 535, loss = 0.10203498\n",
      "Iteration 536, loss = 0.10182652\n",
      "Iteration 537, loss = 0.10163986\n",
      "Iteration 538, loss = 0.10146041\n",
      "Iteration 539, loss = 0.10129418\n",
      "Iteration 540, loss = 0.10110411\n",
      "Iteration 541, loss = 0.10092272\n",
      "Iteration 542, loss = 0.10073325\n",
      "Iteration 543, loss = 0.10054988\n",
      "Iteration 544, loss = 0.10037839\n",
      "Iteration 545, loss = 0.10019561\n",
      "Iteration 546, loss = 0.10003483\n",
      "Iteration 547, loss = 0.09986139\n",
      "Iteration 548, loss = 0.09971574\n",
      "Iteration 549, loss = 0.09950489\n",
      "Iteration 550, loss = 0.09932904\n",
      "Iteration 551, loss = 0.09914726\n",
      "Iteration 552, loss = 0.09897439\n",
      "Iteration 553, loss = 0.09880019\n",
      "Iteration 554, loss = 0.09861195\n",
      "Iteration 555, loss = 0.09845438\n",
      "Iteration 556, loss = 0.09829040\n",
      "Iteration 557, loss = 0.09813120\n",
      "Iteration 558, loss = 0.09796306\n",
      "Iteration 559, loss = 0.09778742\n",
      "Iteration 560, loss = 0.09760695\n",
      "Iteration 561, loss = 0.09744340\n",
      "Iteration 562, loss = 0.09727105\n",
      "Iteration 563, loss = 0.09710354\n",
      "Iteration 564, loss = 0.09694279\n",
      "Iteration 565, loss = 0.09677290\n",
      "Iteration 566, loss = 0.09662115\n",
      "Iteration 567, loss = 0.09646320\n",
      "Iteration 568, loss = 0.09631088\n",
      "Iteration 569, loss = 0.09613777\n",
      "Iteration 570, loss = 0.09597924\n",
      "Iteration 571, loss = 0.09582539\n",
      "Iteration 572, loss = 0.09566707\n",
      "Iteration 573, loss = 0.09551623\n",
      "Iteration 574, loss = 0.09536032\n",
      "Iteration 575, loss = 0.09521876\n",
      "Iteration 576, loss = 0.09506451\n",
      "Iteration 577, loss = 0.09489239\n",
      "Iteration 578, loss = 0.09476360\n",
      "Iteration 579, loss = 0.09459849\n",
      "Iteration 580, loss = 0.09445556\n",
      "Iteration 581, loss = 0.09429550\n",
      "Iteration 582, loss = 0.09414413\n",
      "Iteration 583, loss = 0.09400108\n",
      "Iteration 584, loss = 0.09384581\n",
      "Iteration 585, loss = 0.09370123\n",
      "Iteration 586, loss = 0.09355555\n",
      "Iteration 587, loss = 0.09341067\n",
      "Iteration 588, loss = 0.09326958\n",
      "Iteration 589, loss = 0.09309581\n",
      "Iteration 590, loss = 0.09294474\n",
      "Iteration 591, loss = 0.09281234\n",
      "Iteration 592, loss = 0.09267283\n",
      "Iteration 593, loss = 0.09252777\n",
      "Iteration 594, loss = 0.09240323\n",
      "Iteration 595, loss = 0.09224549\n",
      "Iteration 596, loss = 0.09209613\n",
      "Iteration 597, loss = 0.09196231\n",
      "Iteration 598, loss = 0.09182144\n",
      "Iteration 599, loss = 0.09167857\n",
      "Iteration 600, loss = 0.09153024\n",
      "Iteration 601, loss = 0.09138516\n",
      "Iteration 602, loss = 0.09124082\n",
      "Iteration 603, loss = 0.09110474\n",
      "Iteration 604, loss = 0.09097986\n",
      "Iteration 605, loss = 0.09084661\n",
      "Iteration 606, loss = 0.09070971\n",
      "Iteration 607, loss = 0.09058048\n",
      "Iteration 608, loss = 0.09043268\n",
      "Iteration 609, loss = 0.09029802\n",
      "Iteration 610, loss = 0.09017085\n",
      "Iteration 611, loss = 0.09003538\n",
      "Iteration 612, loss = 0.08990361\n",
      "Iteration 613, loss = 0.08977755\n",
      "Iteration 614, loss = 0.08965223\n",
      "Iteration 615, loss = 0.08951752\n",
      "Iteration 616, loss = 0.08938784\n",
      "Iteration 617, loss = 0.08925491\n",
      "Iteration 618, loss = 0.08913524\n",
      "Iteration 619, loss = 0.08900414\n",
      "Iteration 620, loss = 0.08887067\n",
      "Iteration 621, loss = 0.08875013\n",
      "Iteration 622, loss = 0.08863111\n",
      "Iteration 623, loss = 0.08849956\n",
      "Iteration 624, loss = 0.08835534\n",
      "Iteration 625, loss = 0.08823605\n",
      "Iteration 626, loss = 0.08811514\n",
      "Iteration 627, loss = 0.08798859\n",
      "Iteration 628, loss = 0.08785736\n",
      "Iteration 629, loss = 0.08773272\n",
      "Iteration 630, loss = 0.08760355\n",
      "Iteration 631, loss = 0.08746611\n",
      "Iteration 632, loss = 0.08734370\n",
      "Iteration 633, loss = 0.08722918\n",
      "Iteration 634, loss = 0.08711076\n",
      "Iteration 635, loss = 0.08698601\n",
      "Iteration 636, loss = 0.08687769\n",
      "Iteration 637, loss = 0.08674758\n",
      "Iteration 638, loss = 0.08662293\n",
      "Iteration 639, loss = 0.08649134\n",
      "Iteration 640, loss = 0.08638954\n",
      "Iteration 641, loss = 0.08625005\n",
      "Iteration 642, loss = 0.08613999\n",
      "Iteration 643, loss = 0.08602341\n",
      "Iteration 644, loss = 0.08589804\n",
      "Iteration 645, loss = 0.08578723\n",
      "Iteration 646, loss = 0.08566459\n",
      "Iteration 647, loss = 0.08556363\n",
      "Iteration 648, loss = 0.08544317\n",
      "Iteration 649, loss = 0.08532965\n",
      "Iteration 650, loss = 0.08522112\n",
      "Iteration 651, loss = 0.08511203\n",
      "Iteration 652, loss = 0.08499858\n",
      "Iteration 653, loss = 0.08487882\n",
      "Iteration 654, loss = 0.08477077\n",
      "Iteration 655, loss = 0.08465889\n",
      "Iteration 656, loss = 0.08455435\n",
      "Iteration 657, loss = 0.08444646\n",
      "Iteration 658, loss = 0.08434404\n",
      "Iteration 659, loss = 0.08422339\n",
      "Iteration 660, loss = 0.08411557\n",
      "Iteration 661, loss = 0.08401302\n",
      "Iteration 662, loss = 0.08390439\n",
      "Iteration 663, loss = 0.08378291\n",
      "Iteration 664, loss = 0.08369052\n",
      "Iteration 665, loss = 0.08357466\n",
      "Iteration 666, loss = 0.08346609\n",
      "Iteration 667, loss = 0.08335937\n",
      "Iteration 668, loss = 0.08326271\n",
      "Iteration 669, loss = 0.08315739\n",
      "Iteration 670, loss = 0.08305332\n",
      "Iteration 671, loss = 0.08296097\n",
      "Iteration 672, loss = 0.08284994\n",
      "Iteration 673, loss = 0.08274777\n",
      "Iteration 674, loss = 0.08263313\n",
      "Iteration 675, loss = 0.08253994\n",
      "Iteration 676, loss = 0.08241907\n",
      "Iteration 677, loss = 0.08232995\n",
      "Iteration 678, loss = 0.08222961\n",
      "Iteration 679, loss = 0.08211855\n",
      "Iteration 680, loss = 0.08201135\n",
      "Iteration 681, loss = 0.08193373\n",
      "Iteration 682, loss = 0.08181115\n",
      "Iteration 683, loss = 0.08170850\n",
      "Iteration 684, loss = 0.08160277\n",
      "Iteration 685, loss = 0.08151980\n",
      "Iteration 686, loss = 0.08141009\n",
      "Iteration 687, loss = 0.08131640\n",
      "Iteration 688, loss = 0.08120962\n",
      "Iteration 689, loss = 0.08111169\n",
      "Iteration 690, loss = 0.08100617\n",
      "Iteration 691, loss = 0.08092749\n",
      "Iteration 692, loss = 0.08083012\n",
      "Iteration 693, loss = 0.08073032\n",
      "Iteration 694, loss = 0.08063528\n",
      "Iteration 695, loss = 0.08053728\n",
      "Iteration 696, loss = 0.08043923\n",
      "Iteration 697, loss = 0.08034132\n",
      "Iteration 698, loss = 0.08024947\n",
      "Iteration 699, loss = 0.08015894\n",
      "Iteration 700, loss = 0.08006251\n",
      "Iteration 701, loss = 0.07997248\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.36191905\n",
      "Iteration 2, loss = 2.34842328\n",
      "Iteration 3, loss = 2.33090023\n",
      "Iteration 4, loss = 2.31243559\n",
      "Iteration 5, loss = 2.29376020\n",
      "Iteration 6, loss = 2.27588703\n",
      "Iteration 7, loss = 2.25787170\n",
      "Iteration 8, loss = 2.23950500\n",
      "Iteration 9, loss = 2.22034627\n",
      "Iteration 10, loss = 2.20044910\n",
      "Iteration 11, loss = 2.17996168\n",
      "Iteration 12, loss = 2.15879387\n",
      "Iteration 13, loss = 2.13711439\n",
      "Iteration 14, loss = 2.11464285\n",
      "Iteration 15, loss = 2.09142658\n",
      "Iteration 16, loss = 2.06765780\n",
      "Iteration 17, loss = 2.04323819\n",
      "Iteration 18, loss = 2.01862983\n",
      "Iteration 19, loss = 1.99338911\n",
      "Iteration 20, loss = 1.96798771\n",
      "Iteration 21, loss = 1.94170832\n",
      "Iteration 22, loss = 1.91515344\n",
      "Iteration 23, loss = 1.88811103\n",
      "Iteration 24, loss = 1.86181312\n",
      "Iteration 25, loss = 1.83482349\n",
      "Iteration 26, loss = 1.80773512\n",
      "Iteration 27, loss = 1.78050912\n",
      "Iteration 28, loss = 1.75300689\n",
      "Iteration 29, loss = 1.72509681\n",
      "Iteration 30, loss = 1.69745641\n",
      "Iteration 31, loss = 1.66996494\n",
      "Iteration 32, loss = 1.64275919\n",
      "Iteration 33, loss = 1.61512012\n",
      "Iteration 34, loss = 1.58759828\n",
      "Iteration 35, loss = 1.56029322\n",
      "Iteration 36, loss = 1.53297352\n",
      "Iteration 37, loss = 1.50638004\n",
      "Iteration 38, loss = 1.47999589\n",
      "Iteration 39, loss = 1.45401183\n",
      "Iteration 40, loss = 1.42851203\n",
      "Iteration 41, loss = 1.40348705\n",
      "Iteration 42, loss = 1.37876728\n",
      "Iteration 43, loss = 1.35417216\n",
      "Iteration 44, loss = 1.33040172\n",
      "Iteration 45, loss = 1.30676469\n",
      "Iteration 46, loss = 1.28396190\n",
      "Iteration 47, loss = 1.26144283\n",
      "Iteration 48, loss = 1.23967504\n",
      "Iteration 49, loss = 1.21818432\n",
      "Iteration 50, loss = 1.19760379\n",
      "Iteration 51, loss = 1.17739591\n",
      "Iteration 52, loss = 1.15753340\n",
      "Iteration 53, loss = 1.13833795\n",
      "Iteration 54, loss = 1.11939998\n",
      "Iteration 55, loss = 1.10116031\n",
      "Iteration 56, loss = 1.08312833\n",
      "Iteration 57, loss = 1.06603654\n",
      "Iteration 58, loss = 1.04917523\n",
      "Iteration 59, loss = 1.03241891\n",
      "Iteration 60, loss = 1.01640323\n",
      "Iteration 61, loss = 1.00064018\n",
      "Iteration 62, loss = 0.98534880\n",
      "Iteration 63, loss = 0.97032281\n",
      "Iteration 64, loss = 0.95581236\n",
      "Iteration 65, loss = 0.94134231\n",
      "Iteration 66, loss = 0.92752439\n",
      "Iteration 67, loss = 0.91413986\n",
      "Iteration 68, loss = 0.90099867\n",
      "Iteration 69, loss = 0.88809648\n",
      "Iteration 70, loss = 0.87552743\n",
      "Iteration 71, loss = 0.86307959\n",
      "Iteration 72, loss = 0.85120194\n",
      "Iteration 73, loss = 0.83982169\n",
      "Iteration 74, loss = 0.82828924\n",
      "Iteration 75, loss = 0.81711055\n",
      "Iteration 76, loss = 0.80618850\n",
      "Iteration 77, loss = 0.79583855\n",
      "Iteration 78, loss = 0.78545006\n",
      "Iteration 79, loss = 0.77540764\n",
      "Iteration 80, loss = 0.76567846\n",
      "Iteration 81, loss = 0.75593079\n",
      "Iteration 82, loss = 0.74650234\n",
      "Iteration 83, loss = 0.73730856\n",
      "Iteration 84, loss = 0.72821388\n",
      "Iteration 85, loss = 0.71931706\n",
      "Iteration 86, loss = 0.71087355\n",
      "Iteration 87, loss = 0.70229031\n",
      "Iteration 88, loss = 0.69403012\n",
      "Iteration 89, loss = 0.68596745\n",
      "Iteration 90, loss = 0.67823329\n",
      "Iteration 91, loss = 0.67035455\n",
      "Iteration 92, loss = 0.66284188\n",
      "Iteration 93, loss = 0.65550524\n",
      "Iteration 94, loss = 0.64823821\n",
      "Iteration 95, loss = 0.64112603\n",
      "Iteration 96, loss = 0.63414081\n",
      "Iteration 97, loss = 0.62727181\n",
      "Iteration 98, loss = 0.62074398\n",
      "Iteration 99, loss = 0.61424477\n",
      "Iteration 100, loss = 0.60781403\n",
      "Iteration 101, loss = 0.60132222\n",
      "Iteration 102, loss = 0.59513769\n",
      "Iteration 103, loss = 0.58898734\n",
      "Iteration 104, loss = 0.58307335\n",
      "Iteration 105, loss = 0.57717756\n",
      "Iteration 106, loss = 0.57147350\n",
      "Iteration 107, loss = 0.56567994\n",
      "Iteration 108, loss = 0.56018169\n",
      "Iteration 109, loss = 0.55463266\n",
      "Iteration 110, loss = 0.54926244\n",
      "Iteration 111, loss = 0.54411979\n",
      "Iteration 112, loss = 0.53909794\n",
      "Iteration 113, loss = 0.53397392\n",
      "Iteration 114, loss = 0.52866905\n",
      "Iteration 115, loss = 0.52391406\n",
      "Iteration 116, loss = 0.51909557\n",
      "Iteration 117, loss = 0.51457928\n",
      "Iteration 118, loss = 0.50994467\n",
      "Iteration 119, loss = 0.50521970\n",
      "Iteration 120, loss = 0.50072304\n",
      "Iteration 121, loss = 0.49626593\n",
      "Iteration 122, loss = 0.49193161\n",
      "Iteration 123, loss = 0.48762462\n",
      "Iteration 124, loss = 0.48343324\n",
      "Iteration 125, loss = 0.47939003\n",
      "Iteration 126, loss = 0.47527963\n",
      "Iteration 127, loss = 0.47117441\n",
      "Iteration 128, loss = 0.46739728\n",
      "Iteration 129, loss = 0.46350652\n",
      "Iteration 130, loss = 0.45955255\n",
      "Iteration 131, loss = 0.45584965\n",
      "Iteration 132, loss = 0.45198505\n",
      "Iteration 133, loss = 0.44843267\n",
      "Iteration 134, loss = 0.44482774\n",
      "Iteration 135, loss = 0.44127900\n",
      "Iteration 136, loss = 0.43773140\n",
      "Iteration 137, loss = 0.43430251\n",
      "Iteration 138, loss = 0.43097928\n",
      "Iteration 139, loss = 0.42759962\n",
      "Iteration 140, loss = 0.42427100\n",
      "Iteration 141, loss = 0.42100779\n",
      "Iteration 142, loss = 0.41779498\n",
      "Iteration 143, loss = 0.41463554\n",
      "Iteration 144, loss = 0.41159569\n",
      "Iteration 145, loss = 0.40850440\n",
      "Iteration 146, loss = 0.40550747\n",
      "Iteration 147, loss = 0.40239791\n",
      "Iteration 148, loss = 0.39947931\n",
      "Iteration 149, loss = 0.39651697\n",
      "Iteration 150, loss = 0.39366924\n",
      "Iteration 151, loss = 0.39092886\n",
      "Iteration 152, loss = 0.38815635\n",
      "Iteration 153, loss = 0.38535189\n",
      "Iteration 154, loss = 0.38258758\n",
      "Iteration 155, loss = 0.38006744\n",
      "Iteration 156, loss = 0.37739689\n",
      "Iteration 157, loss = 0.37479689\n",
      "Iteration 158, loss = 0.37214926\n",
      "Iteration 159, loss = 0.36962083\n",
      "Iteration 160, loss = 0.36717721\n",
      "Iteration 161, loss = 0.36468058\n",
      "Iteration 162, loss = 0.36212573\n",
      "Iteration 163, loss = 0.35966377\n",
      "Iteration 164, loss = 0.35734659\n",
      "Iteration 165, loss = 0.35484683\n",
      "Iteration 166, loss = 0.35267357\n",
      "Iteration 167, loss = 0.35033687\n",
      "Iteration 168, loss = 0.34803144\n",
      "Iteration 169, loss = 0.34579507\n",
      "Iteration 170, loss = 0.34353259\n",
      "Iteration 171, loss = 0.34149128\n",
      "Iteration 172, loss = 0.33929155\n",
      "Iteration 173, loss = 0.33712503\n",
      "Iteration 174, loss = 0.33498824\n",
      "Iteration 175, loss = 0.33293661\n",
      "Iteration 176, loss = 0.33095149\n",
      "Iteration 177, loss = 0.32882749\n",
      "Iteration 178, loss = 0.32674279\n",
      "Iteration 179, loss = 0.32474046\n",
      "Iteration 180, loss = 0.32283420\n",
      "Iteration 181, loss = 0.32095221\n",
      "Iteration 182, loss = 0.31892010\n",
      "Iteration 183, loss = 0.31696453\n",
      "Iteration 184, loss = 0.31504494\n",
      "Iteration 185, loss = 0.31323177\n",
      "Iteration 186, loss = 0.31136526\n",
      "Iteration 187, loss = 0.30957330\n",
      "Iteration 188, loss = 0.30773474\n",
      "Iteration 189, loss = 0.30591213\n",
      "Iteration 190, loss = 0.30417300\n",
      "Iteration 191, loss = 0.30247187\n",
      "Iteration 192, loss = 0.30065272\n",
      "Iteration 193, loss = 0.29890032\n",
      "Iteration 194, loss = 0.29716957\n",
      "Iteration 195, loss = 0.29551916\n",
      "Iteration 196, loss = 0.29382336\n",
      "Iteration 197, loss = 0.29221894\n",
      "Iteration 198, loss = 0.29052702\n",
      "Iteration 199, loss = 0.28892231\n",
      "Iteration 200, loss = 0.28739526\n",
      "Iteration 201, loss = 0.28582060\n",
      "Iteration 202, loss = 0.28425406\n",
      "Iteration 203, loss = 0.28271517\n",
      "Iteration 204, loss = 0.28117513\n",
      "Iteration 205, loss = 0.27962463\n",
      "Iteration 206, loss = 0.27819534\n",
      "Iteration 207, loss = 0.27670061\n",
      "Iteration 208, loss = 0.27520417\n",
      "Iteration 209, loss = 0.27376269\n",
      "Iteration 210, loss = 0.27235927\n",
      "Iteration 211, loss = 0.27089433\n",
      "Iteration 212, loss = 0.26943259\n",
      "Iteration 213, loss = 0.26811476\n",
      "Iteration 214, loss = 0.26675054\n",
      "Iteration 215, loss = 0.26534079\n",
      "Iteration 216, loss = 0.26403345\n",
      "Iteration 217, loss = 0.26262680\n",
      "Iteration 218, loss = 0.26134350\n",
      "Iteration 219, loss = 0.26004858\n",
      "Iteration 220, loss = 0.25877657\n",
      "Iteration 221, loss = 0.25752513\n",
      "Iteration 222, loss = 0.25624287\n",
      "Iteration 223, loss = 0.25501319\n",
      "Iteration 224, loss = 0.25374796\n",
      "Iteration 225, loss = 0.25250712\n",
      "Iteration 226, loss = 0.25133917\n",
      "Iteration 227, loss = 0.25002262\n",
      "Iteration 228, loss = 0.24885377\n",
      "Iteration 229, loss = 0.24775681\n",
      "Iteration 230, loss = 0.24654255\n",
      "Iteration 231, loss = 0.24530770\n",
      "Iteration 232, loss = 0.24420456\n",
      "Iteration 233, loss = 0.24297957\n",
      "Iteration 234, loss = 0.24188324\n",
      "Iteration 235, loss = 0.24069296\n",
      "Iteration 236, loss = 0.23954326\n",
      "Iteration 237, loss = 0.23844316\n",
      "Iteration 238, loss = 0.23734513\n",
      "Iteration 239, loss = 0.23623821\n",
      "Iteration 240, loss = 0.23525001\n",
      "Iteration 241, loss = 0.23409823\n",
      "Iteration 242, loss = 0.23305547\n",
      "Iteration 243, loss = 0.23197714\n",
      "Iteration 244, loss = 0.23097088\n",
      "Iteration 245, loss = 0.22992072\n",
      "Iteration 246, loss = 0.22898184\n",
      "Iteration 247, loss = 0.22788282\n",
      "Iteration 248, loss = 0.22692777\n",
      "Iteration 249, loss = 0.22586959\n",
      "Iteration 250, loss = 0.22486217\n",
      "Iteration 251, loss = 0.22382295\n",
      "Iteration 252, loss = 0.22290396\n",
      "Iteration 253, loss = 0.22192942\n",
      "Iteration 254, loss = 0.22105020\n",
      "Iteration 255, loss = 0.22001184\n",
      "Iteration 256, loss = 0.21907132\n",
      "Iteration 257, loss = 0.21815001\n",
      "Iteration 258, loss = 0.21714865\n",
      "Iteration 259, loss = 0.21626983\n",
      "Iteration 260, loss = 0.21528583\n",
      "Iteration 261, loss = 0.21441109\n",
      "Iteration 262, loss = 0.21344022\n",
      "Iteration 263, loss = 0.21255518\n",
      "Iteration 264, loss = 0.21171031\n",
      "Iteration 265, loss = 0.21084140\n",
      "Iteration 266, loss = 0.20992946\n",
      "Iteration 267, loss = 0.20908329\n",
      "Iteration 268, loss = 0.20823214\n",
      "Iteration 269, loss = 0.20738270\n",
      "Iteration 270, loss = 0.20657597\n",
      "Iteration 271, loss = 0.20570786\n",
      "Iteration 272, loss = 0.20486463\n",
      "Iteration 273, loss = 0.20397866\n",
      "Iteration 274, loss = 0.20318357\n",
      "Iteration 275, loss = 0.20236711\n",
      "Iteration 276, loss = 0.20160090\n",
      "Iteration 277, loss = 0.20079909\n",
      "Iteration 278, loss = 0.20003656\n",
      "Iteration 279, loss = 0.19925258\n",
      "Iteration 280, loss = 0.19846146\n",
      "Iteration 281, loss = 0.19770594\n",
      "Iteration 282, loss = 0.19692489\n",
      "Iteration 283, loss = 0.19614570\n",
      "Iteration 284, loss = 0.19541532\n",
      "Iteration 285, loss = 0.19471096\n",
      "Iteration 286, loss = 0.19396800\n",
      "Iteration 287, loss = 0.19323420\n",
      "Iteration 288, loss = 0.19248446\n",
      "Iteration 289, loss = 0.19177363\n",
      "Iteration 290, loss = 0.19108348\n",
      "Iteration 291, loss = 0.19037946\n",
      "Iteration 292, loss = 0.18971148\n",
      "Iteration 293, loss = 0.18902887\n",
      "Iteration 294, loss = 0.18826382\n",
      "Iteration 295, loss = 0.18754678\n",
      "Iteration 296, loss = 0.18685179\n",
      "Iteration 297, loss = 0.18618876\n",
      "Iteration 298, loss = 0.18551580\n",
      "Iteration 299, loss = 0.18489420\n",
      "Iteration 300, loss = 0.18419068\n",
      "Iteration 301, loss = 0.18353498\n",
      "Iteration 302, loss = 0.18287823\n",
      "Iteration 303, loss = 0.18219478\n",
      "Iteration 304, loss = 0.18161181\n",
      "Iteration 305, loss = 0.18093121\n",
      "Iteration 306, loss = 0.18029997\n",
      "Iteration 307, loss = 0.17961485\n",
      "Iteration 308, loss = 0.17902704\n",
      "Iteration 309, loss = 0.17840147\n",
      "Iteration 310, loss = 0.17772757\n",
      "Iteration 311, loss = 0.17710969\n",
      "Iteration 312, loss = 0.17653187\n",
      "Iteration 313, loss = 0.17594330\n",
      "Iteration 314, loss = 0.17529977\n",
      "Iteration 315, loss = 0.17464953\n",
      "Iteration 316, loss = 0.17400716\n",
      "Iteration 317, loss = 0.17346718\n",
      "Iteration 318, loss = 0.17290087\n",
      "Iteration 319, loss = 0.17234134\n",
      "Iteration 320, loss = 0.17178420\n",
      "Iteration 321, loss = 0.17112087\n",
      "Iteration 322, loss = 0.17050939\n",
      "Iteration 323, loss = 0.16987511\n",
      "Iteration 324, loss = 0.16935376\n",
      "Iteration 325, loss = 0.16875429\n",
      "Iteration 326, loss = 0.16819705\n",
      "Iteration 327, loss = 0.16766581\n",
      "Iteration 328, loss = 0.16711292\n",
      "Iteration 329, loss = 0.16652490\n",
      "Iteration 330, loss = 0.16599270\n",
      "Iteration 331, loss = 0.16545736\n",
      "Iteration 332, loss = 0.16490715\n",
      "Iteration 333, loss = 0.16435102\n",
      "Iteration 334, loss = 0.16385384\n",
      "Iteration 335, loss = 0.16336306\n",
      "Iteration 336, loss = 0.16283109\n",
      "Iteration 337, loss = 0.16231247\n",
      "Iteration 338, loss = 0.16180774\n",
      "Iteration 339, loss = 0.16133501\n",
      "Iteration 340, loss = 0.16083876\n",
      "Iteration 341, loss = 0.16026522\n",
      "Iteration 342, loss = 0.15976913\n",
      "Iteration 343, loss = 0.15928218\n",
      "Iteration 344, loss = 0.15878870\n",
      "Iteration 345, loss = 0.15829883\n",
      "Iteration 346, loss = 0.15779802\n",
      "Iteration 347, loss = 0.15728316\n",
      "Iteration 348, loss = 0.15683883\n",
      "Iteration 349, loss = 0.15633606\n",
      "Iteration 350, loss = 0.15585508\n",
      "Iteration 351, loss = 0.15538450\n",
      "Iteration 352, loss = 0.15492525\n",
      "Iteration 353, loss = 0.15446415\n",
      "Iteration 354, loss = 0.15401507\n",
      "Iteration 355, loss = 0.15355460\n",
      "Iteration 356, loss = 0.15308327\n",
      "Iteration 357, loss = 0.15261613\n",
      "Iteration 358, loss = 0.15213752\n",
      "Iteration 359, loss = 0.15171401\n",
      "Iteration 360, loss = 0.15124070\n",
      "Iteration 361, loss = 0.15083625\n",
      "Iteration 362, loss = 0.15041435\n",
      "Iteration 363, loss = 0.14998352\n",
      "Iteration 364, loss = 0.14949307\n",
      "Iteration 365, loss = 0.14901905\n",
      "Iteration 366, loss = 0.14863362\n",
      "Iteration 367, loss = 0.14818928\n",
      "Iteration 368, loss = 0.14778711\n",
      "Iteration 369, loss = 0.14733691\n",
      "Iteration 370, loss = 0.14692892\n",
      "Iteration 371, loss = 0.14646218\n",
      "Iteration 372, loss = 0.14598334\n",
      "Iteration 373, loss = 0.14558099\n",
      "Iteration 374, loss = 0.14518156\n",
      "Iteration 375, loss = 0.14474486\n",
      "Iteration 376, loss = 0.14432535\n",
      "Iteration 377, loss = 0.14395932\n",
      "Iteration 378, loss = 0.14352713\n",
      "Iteration 379, loss = 0.14311978\n",
      "Iteration 380, loss = 0.14278521\n",
      "Iteration 381, loss = 0.14238950\n",
      "Iteration 382, loss = 0.14200180\n",
      "Iteration 383, loss = 0.14157175\n",
      "Iteration 384, loss = 0.14118243\n",
      "Iteration 385, loss = 0.14075180\n",
      "Iteration 386, loss = 0.14034712\n",
      "Iteration 387, loss = 0.13996626\n",
      "Iteration 388, loss = 0.13956464\n",
      "Iteration 389, loss = 0.13920758\n",
      "Iteration 390, loss = 0.13885301\n",
      "Iteration 391, loss = 0.13852206\n",
      "Iteration 392, loss = 0.13813672\n",
      "Iteration 393, loss = 0.13780223\n",
      "Iteration 394, loss = 0.13737928\n",
      "Iteration 395, loss = 0.13702096\n",
      "Iteration 396, loss = 0.13667308\n",
      "Iteration 397, loss = 0.13629355\n",
      "Iteration 398, loss = 0.13598694\n",
      "Iteration 399, loss = 0.13555751\n",
      "Iteration 400, loss = 0.13521203\n",
      "Iteration 401, loss = 0.13483520\n",
      "Iteration 402, loss = 0.13448059\n",
      "Iteration 403, loss = 0.13415419\n",
      "Iteration 404, loss = 0.13381396\n",
      "Iteration 405, loss = 0.13346700\n",
      "Iteration 406, loss = 0.13313559\n",
      "Iteration 407, loss = 0.13277282\n",
      "Iteration 408, loss = 0.13242700\n",
      "Iteration 409, loss = 0.13208958\n",
      "Iteration 410, loss = 0.13173895\n",
      "Iteration 411, loss = 0.13139404\n",
      "Iteration 412, loss = 0.13105064\n",
      "Iteration 413, loss = 0.13072277\n",
      "Iteration 414, loss = 0.13039381\n",
      "Iteration 415, loss = 0.13005152\n",
      "Iteration 416, loss = 0.12974628\n",
      "Iteration 417, loss = 0.12940186\n",
      "Iteration 418, loss = 0.12906910\n",
      "Iteration 419, loss = 0.12877699\n",
      "Iteration 420, loss = 0.12843860\n",
      "Iteration 421, loss = 0.12816929\n",
      "Iteration 422, loss = 0.12781965\n",
      "Iteration 423, loss = 0.12748632\n",
      "Iteration 424, loss = 0.12717304\n",
      "Iteration 425, loss = 0.12684808\n",
      "Iteration 426, loss = 0.12654289\n",
      "Iteration 427, loss = 0.12624874\n",
      "Iteration 428, loss = 0.12592689\n",
      "Iteration 429, loss = 0.12562839\n",
      "Iteration 430, loss = 0.12535776\n",
      "Iteration 431, loss = 0.12504690\n",
      "Iteration 432, loss = 0.12474783\n",
      "Iteration 433, loss = 0.12444100\n",
      "Iteration 434, loss = 0.12414909\n",
      "Iteration 435, loss = 0.12386872\n",
      "Iteration 436, loss = 0.12354075\n",
      "Iteration 437, loss = 0.12328920\n",
      "Iteration 438, loss = 0.12298868\n",
      "Iteration 439, loss = 0.12269103\n",
      "Iteration 440, loss = 0.12240654\n",
      "Iteration 441, loss = 0.12212493\n",
      "Iteration 442, loss = 0.12185319\n",
      "Iteration 443, loss = 0.12158613\n",
      "Iteration 444, loss = 0.12128428\n",
      "Iteration 445, loss = 0.12102138\n",
      "Iteration 446, loss = 0.12075239\n",
      "Iteration 447, loss = 0.12045284\n",
      "Iteration 448, loss = 0.12021892\n",
      "Iteration 449, loss = 0.11991049\n",
      "Iteration 450, loss = 0.11962184\n",
      "Iteration 451, loss = 0.11935327\n",
      "Iteration 452, loss = 0.11908257\n",
      "Iteration 453, loss = 0.11880853\n",
      "Iteration 454, loss = 0.11852693\n",
      "Iteration 455, loss = 0.11823803\n",
      "Iteration 456, loss = 0.11799692\n",
      "Iteration 457, loss = 0.11776049\n",
      "Iteration 458, loss = 0.11741533\n",
      "Iteration 459, loss = 0.11716932\n",
      "Iteration 460, loss = 0.11689651\n",
      "Iteration 461, loss = 0.11662131\n",
      "Iteration 462, loss = 0.11635481\n",
      "Iteration 463, loss = 0.11614374\n",
      "Iteration 464, loss = 0.11586320\n",
      "Iteration 465, loss = 0.11559523\n",
      "Iteration 466, loss = 0.11537598\n",
      "Iteration 467, loss = 0.11511634\n",
      "Iteration 468, loss = 0.11483961\n",
      "Iteration 469, loss = 0.11458242\n",
      "Iteration 470, loss = 0.11433639\n",
      "Iteration 471, loss = 0.11405925\n",
      "Iteration 472, loss = 0.11381392\n",
      "Iteration 473, loss = 0.11354951\n",
      "Iteration 474, loss = 0.11331326\n",
      "Iteration 475, loss = 0.11306248\n",
      "Iteration 476, loss = 0.11283376\n",
      "Iteration 477, loss = 0.11259749\n",
      "Iteration 478, loss = 0.11237486\n",
      "Iteration 479, loss = 0.11210046\n",
      "Iteration 480, loss = 0.11186285\n",
      "Iteration 481, loss = 0.11165734\n",
      "Iteration 482, loss = 0.11141814\n",
      "Iteration 483, loss = 0.11120582\n",
      "Iteration 484, loss = 0.11094539\n",
      "Iteration 485, loss = 0.11072307\n",
      "Iteration 486, loss = 0.11050580\n",
      "Iteration 487, loss = 0.11027488\n",
      "Iteration 488, loss = 0.11003081\n",
      "Iteration 489, loss = 0.10977383\n",
      "Iteration 490, loss = 0.10955472\n",
      "Iteration 491, loss = 0.10933946\n",
      "Iteration 492, loss = 0.10911686\n",
      "Iteration 493, loss = 0.10890678\n",
      "Iteration 494, loss = 0.10869259\n",
      "Iteration 495, loss = 0.10846612\n",
      "Iteration 496, loss = 0.10824450\n",
      "Iteration 497, loss = 0.10805334\n",
      "Iteration 498, loss = 0.10781669\n",
      "Iteration 499, loss = 0.10766039\n",
      "Iteration 500, loss = 0.10742000\n",
      "Iteration 501, loss = 0.10719404\n",
      "Iteration 502, loss = 0.10694537\n",
      "Iteration 503, loss = 0.10675158\n",
      "Iteration 504, loss = 0.10655107\n",
      "Iteration 505, loss = 0.10632206\n",
      "Iteration 506, loss = 0.10610077\n",
      "Iteration 507, loss = 0.10588854\n",
      "Iteration 508, loss = 0.10568018\n",
      "Iteration 509, loss = 0.10549450\n",
      "Iteration 510, loss = 0.10526948\n",
      "Iteration 511, loss = 0.10505979\n",
      "Iteration 512, loss = 0.10485446\n",
      "Iteration 513, loss = 0.10464008\n",
      "Iteration 514, loss = 0.10445823\n",
      "Iteration 515, loss = 0.10425653\n",
      "Iteration 516, loss = 0.10404291\n",
      "Iteration 517, loss = 0.10382214\n",
      "Iteration 518, loss = 0.10363218\n",
      "Iteration 519, loss = 0.10341005\n",
      "Iteration 520, loss = 0.10319214\n",
      "Iteration 521, loss = 0.10300462\n",
      "Iteration 522, loss = 0.10284777\n",
      "Iteration 523, loss = 0.10266566\n",
      "Iteration 524, loss = 0.10246406\n",
      "Iteration 525, loss = 0.10227206\n",
      "Iteration 526, loss = 0.10208851\n",
      "Iteration 527, loss = 0.10189052\n",
      "Iteration 528, loss = 0.10168955\n",
      "Iteration 529, loss = 0.10151300\n",
      "Iteration 530, loss = 0.10132252\n",
      "Iteration 531, loss = 0.10115233\n",
      "Iteration 532, loss = 0.10096975\n",
      "Iteration 533, loss = 0.10077563\n",
      "Iteration 534, loss = 0.10059798\n",
      "Iteration 535, loss = 0.10041959\n",
      "Iteration 536, loss = 0.10023821\n",
      "Iteration 537, loss = 0.10006006\n",
      "Iteration 538, loss = 0.09988134\n",
      "Iteration 539, loss = 0.09971640\n",
      "Iteration 540, loss = 0.09953834\n",
      "Iteration 541, loss = 0.09937371\n",
      "Iteration 542, loss = 0.09918787\n",
      "Iteration 543, loss = 0.09900597\n",
      "Iteration 544, loss = 0.09885480\n",
      "Iteration 545, loss = 0.09867393\n",
      "Iteration 546, loss = 0.09851142\n",
      "Iteration 547, loss = 0.09832845\n",
      "Iteration 548, loss = 0.09817219\n",
      "Iteration 549, loss = 0.09799412\n",
      "Iteration 550, loss = 0.09783509\n",
      "Iteration 551, loss = 0.09766497\n",
      "Iteration 552, loss = 0.09749267\n",
      "Iteration 553, loss = 0.09733016\n",
      "Iteration 554, loss = 0.09716610\n",
      "Iteration 555, loss = 0.09699419\n",
      "Iteration 556, loss = 0.09682928\n",
      "Iteration 557, loss = 0.09668219\n",
      "Iteration 558, loss = 0.09651836\n",
      "Iteration 559, loss = 0.09634715\n",
      "Iteration 560, loss = 0.09618145\n",
      "Iteration 561, loss = 0.09603337\n",
      "Iteration 562, loss = 0.09583575\n",
      "Iteration 563, loss = 0.09568442\n",
      "Iteration 564, loss = 0.09553000\n",
      "Iteration 565, loss = 0.09536557\n",
      "Iteration 566, loss = 0.09521529\n",
      "Iteration 567, loss = 0.09505188\n",
      "Iteration 568, loss = 0.09491396\n",
      "Iteration 569, loss = 0.09474230\n",
      "Iteration 570, loss = 0.09458168\n",
      "Iteration 571, loss = 0.09443767\n",
      "Iteration 572, loss = 0.09428307\n",
      "Iteration 573, loss = 0.09413123\n",
      "Iteration 574, loss = 0.09397280\n",
      "Iteration 575, loss = 0.09383342\n",
      "Iteration 576, loss = 0.09367400\n",
      "Iteration 577, loss = 0.09352643\n",
      "Iteration 578, loss = 0.09338718\n",
      "Iteration 579, loss = 0.09322650\n",
      "Iteration 580, loss = 0.09308018\n",
      "Iteration 581, loss = 0.09293279\n",
      "Iteration 582, loss = 0.09278437\n",
      "Iteration 583, loss = 0.09263517\n",
      "Iteration 584, loss = 0.09248559\n",
      "Iteration 585, loss = 0.09235557\n",
      "Iteration 586, loss = 0.09220930\n",
      "Iteration 587, loss = 0.09207236\n",
      "Iteration 588, loss = 0.09192266\n",
      "Iteration 589, loss = 0.09176071\n",
      "Iteration 590, loss = 0.09161401\n",
      "Iteration 591, loss = 0.09149452\n",
      "Iteration 592, loss = 0.09135185\n",
      "Iteration 593, loss = 0.09119852\n",
      "Iteration 594, loss = 0.09106243\n",
      "Iteration 595, loss = 0.09091674\n",
      "Iteration 596, loss = 0.09077290\n",
      "Iteration 597, loss = 0.09062859\n",
      "Iteration 598, loss = 0.09049233\n",
      "Iteration 599, loss = 0.09036417\n",
      "Iteration 600, loss = 0.09022697\n",
      "Iteration 601, loss = 0.09007853\n",
      "Iteration 602, loss = 0.08993390\n",
      "Iteration 603, loss = 0.08979790\n",
      "Iteration 604, loss = 0.08968323\n",
      "Iteration 605, loss = 0.08955125\n",
      "Iteration 606, loss = 0.08941310\n",
      "Iteration 607, loss = 0.08929316\n",
      "Iteration 608, loss = 0.08914470\n",
      "Iteration 609, loss = 0.08902454\n",
      "Iteration 610, loss = 0.08889160\n",
      "Iteration 611, loss = 0.08876067\n",
      "Iteration 612, loss = 0.08863216\n",
      "Iteration 613, loss = 0.08850818\n",
      "Iteration 614, loss = 0.08838313\n",
      "Iteration 615, loss = 0.08824771\n",
      "Iteration 616, loss = 0.08812860\n",
      "Iteration 617, loss = 0.08799360\n",
      "Iteration 618, loss = 0.08787360\n",
      "Iteration 619, loss = 0.08773350\n",
      "Iteration 620, loss = 0.08762120\n",
      "Iteration 621, loss = 0.08750531\n",
      "Iteration 622, loss = 0.08737613\n",
      "Iteration 623, loss = 0.08723283\n",
      "Iteration 624, loss = 0.08711287\n",
      "Iteration 625, loss = 0.08698981\n",
      "Iteration 626, loss = 0.08687310\n",
      "Iteration 627, loss = 0.08675300\n",
      "Iteration 628, loss = 0.08661975\n",
      "Iteration 629, loss = 0.08649292\n",
      "Iteration 630, loss = 0.08636673\n",
      "Iteration 631, loss = 0.08623997\n",
      "Iteration 632, loss = 0.08611265\n",
      "Iteration 633, loss = 0.08602386\n",
      "Iteration 634, loss = 0.08587303\n",
      "Iteration 635, loss = 0.08578737\n",
      "Iteration 636, loss = 0.08565299\n",
      "Iteration 637, loss = 0.08551908\n",
      "Iteration 638, loss = 0.08538333\n",
      "Iteration 639, loss = 0.08525266\n",
      "Iteration 640, loss = 0.08513475\n",
      "Iteration 641, loss = 0.08502087\n",
      "Iteration 642, loss = 0.08490075\n",
      "Iteration 643, loss = 0.08478534\n",
      "Iteration 644, loss = 0.08466789\n",
      "Iteration 645, loss = 0.08456568\n",
      "Iteration 646, loss = 0.08444522\n",
      "Iteration 647, loss = 0.08434006\n",
      "Iteration 648, loss = 0.08420803\n",
      "Iteration 649, loss = 0.08408928\n",
      "Iteration 650, loss = 0.08397831\n",
      "Iteration 651, loss = 0.08386154\n",
      "Iteration 652, loss = 0.08374810\n",
      "Iteration 653, loss = 0.08363270\n",
      "Iteration 654, loss = 0.08352352\n",
      "Iteration 655, loss = 0.08340972\n",
      "Iteration 656, loss = 0.08330369\n",
      "Iteration 657, loss = 0.08320255\n",
      "Iteration 658, loss = 0.08309848\n",
      "Iteration 659, loss = 0.08299414\n",
      "Iteration 660, loss = 0.08287357\n",
      "Iteration 661, loss = 0.08277262\n",
      "Iteration 662, loss = 0.08266656\n",
      "Iteration 663, loss = 0.08254972\n",
      "Iteration 664, loss = 0.08245569\n",
      "Iteration 665, loss = 0.08234204\n",
      "Iteration 666, loss = 0.08224262\n",
      "Iteration 667, loss = 0.08213887\n",
      "Iteration 668, loss = 0.08203951\n",
      "Iteration 669, loss = 0.08193410\n",
      "Iteration 670, loss = 0.08182728\n",
      "Iteration 671, loss = 0.08173327\n",
      "Iteration 672, loss = 0.08162395\n",
      "Iteration 673, loss = 0.08153132\n",
      "Iteration 674, loss = 0.08142381\n",
      "Iteration 675, loss = 0.08132381\n",
      "Iteration 676, loss = 0.08122706\n",
      "Iteration 677, loss = 0.08112621\n",
      "Iteration 678, loss = 0.08102355\n",
      "Iteration 679, loss = 0.08091056\n",
      "Iteration 680, loss = 0.08080597\n",
      "Iteration 681, loss = 0.08072500\n",
      "Iteration 682, loss = 0.08061042\n",
      "Iteration 683, loss = 0.08051081\n",
      "Iteration 684, loss = 0.08039041\n",
      "Iteration 685, loss = 0.08030669\n",
      "Iteration 686, loss = 0.08019596\n",
      "Iteration 687, loss = 0.08010520\n",
      "Iteration 688, loss = 0.08000462\n",
      "Iteration 689, loss = 0.07991120\n",
      "Iteration 690, loss = 0.07980748\n",
      "Iteration 691, loss = 0.07972765\n",
      "Iteration 692, loss = 0.07960929\n",
      "Iteration 693, loss = 0.07952037\n",
      "Iteration 694, loss = 0.07943687\n",
      "Iteration 695, loss = 0.07933277\n",
      "Iteration 696, loss = 0.07923762\n",
      "Iteration 697, loss = 0.07914627\n",
      "Iteration 698, loss = 0.07906586\n",
      "Iteration 699, loss = 0.07896354\n",
      "Iteration 700, loss = 0.07887018\n",
      "Iteration 701, loss = 0.07877175\n",
      "Iteration 702, loss = 0.07869064\n",
      "Iteration 703, loss = 0.07859160\n",
      "Iteration 704, loss = 0.07849884\n",
      "Iteration 705, loss = 0.07841110\n",
      "Iteration 706, loss = 0.07830722\n",
      "Iteration 707, loss = 0.07822662\n",
      "Iteration 708, loss = 0.07815917\n",
      "Iteration 709, loss = 0.07804855\n",
      "Iteration 710, loss = 0.07795535\n",
      "Iteration 711, loss = 0.07786441\n",
      "Iteration 712, loss = 0.07777364\n",
      "Iteration 713, loss = 0.07768500\n",
      "Iteration 714, loss = 0.07760110\n",
      "Iteration 715, loss = 0.07752604\n",
      "Iteration 716, loss = 0.07743338\n",
      "Iteration 717, loss = 0.07734715\n",
      "Iteration 718, loss = 0.07726379\n",
      "Iteration 719, loss = 0.07717540\n",
      "Iteration 720, loss = 0.07709281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.36569620\n",
      "Iteration 2, loss = 2.35281531\n",
      "Iteration 3, loss = 2.33584438\n",
      "Iteration 4, loss = 2.31832536\n",
      "Iteration 5, loss = 2.30117181\n",
      "Iteration 6, loss = 2.28401814\n",
      "Iteration 7, loss = 2.26658733\n",
      "Iteration 8, loss = 2.24872888\n",
      "Iteration 9, loss = 2.23061719\n",
      "Iteration 10, loss = 2.21186338\n",
      "Iteration 11, loss = 2.19283239\n",
      "Iteration 12, loss = 2.17307920\n",
      "Iteration 13, loss = 2.15294726\n",
      "Iteration 14, loss = 2.13229178\n",
      "Iteration 15, loss = 2.11138306\n",
      "Iteration 16, loss = 2.08982688\n",
      "Iteration 17, loss = 2.06743301\n",
      "Iteration 18, loss = 2.04467459\n",
      "Iteration 19, loss = 2.02101804\n",
      "Iteration 20, loss = 1.99686569\n",
      "Iteration 21, loss = 1.97246791\n",
      "Iteration 22, loss = 1.94742593\n",
      "Iteration 23, loss = 1.92212536\n",
      "Iteration 24, loss = 1.89633092\n",
      "Iteration 25, loss = 1.87043932\n",
      "Iteration 26, loss = 1.84374007\n",
      "Iteration 27, loss = 1.81664665\n",
      "Iteration 28, loss = 1.78935265\n",
      "Iteration 29, loss = 1.76178446\n",
      "Iteration 30, loss = 1.73418120\n",
      "Iteration 31, loss = 1.70692762\n",
      "Iteration 32, loss = 1.67966029\n",
      "Iteration 33, loss = 1.65272139\n",
      "Iteration 34, loss = 1.62552047\n",
      "Iteration 35, loss = 1.59850022\n",
      "Iteration 36, loss = 1.57176217\n",
      "Iteration 37, loss = 1.54544612\n",
      "Iteration 38, loss = 1.51919419\n",
      "Iteration 39, loss = 1.49305450\n",
      "Iteration 40, loss = 1.46737819\n",
      "Iteration 41, loss = 1.44193080\n",
      "Iteration 42, loss = 1.41734327\n",
      "Iteration 43, loss = 1.39299742\n",
      "Iteration 44, loss = 1.36914575\n",
      "Iteration 45, loss = 1.34560703\n",
      "Iteration 46, loss = 1.32286774\n",
      "Iteration 47, loss = 1.30058825\n",
      "Iteration 48, loss = 1.27858329\n",
      "Iteration 49, loss = 1.25748621\n",
      "Iteration 50, loss = 1.23619625\n",
      "Iteration 51, loss = 1.21603710\n",
      "Iteration 52, loss = 1.19636279\n",
      "Iteration 53, loss = 1.17672504\n",
      "Iteration 54, loss = 1.15784444\n",
      "Iteration 55, loss = 1.13951616\n",
      "Iteration 56, loss = 1.12139014\n",
      "Iteration 57, loss = 1.10374577\n",
      "Iteration 58, loss = 1.08649343\n",
      "Iteration 59, loss = 1.06969662\n",
      "Iteration 60, loss = 1.05307654\n",
      "Iteration 61, loss = 1.03727157\n",
      "Iteration 62, loss = 1.02170116\n",
      "Iteration 63, loss = 1.00670634\n",
      "Iteration 64, loss = 0.99188616\n",
      "Iteration 65, loss = 0.97730055\n",
      "Iteration 66, loss = 0.96306568\n",
      "Iteration 67, loss = 0.94938570\n",
      "Iteration 68, loss = 0.93593619\n",
      "Iteration 69, loss = 0.92303438\n",
      "Iteration 70, loss = 0.91038837\n",
      "Iteration 71, loss = 0.89782445\n",
      "Iteration 72, loss = 0.88556221\n",
      "Iteration 73, loss = 0.87364110\n",
      "Iteration 74, loss = 0.86189124\n",
      "Iteration 75, loss = 0.85078701\n",
      "Iteration 76, loss = 0.83950610\n",
      "Iteration 77, loss = 0.82865409\n",
      "Iteration 78, loss = 0.81807345\n",
      "Iteration 79, loss = 0.80798936\n",
      "Iteration 80, loss = 0.79781282\n",
      "Iteration 81, loss = 0.78794324\n",
      "Iteration 82, loss = 0.77814288\n",
      "Iteration 83, loss = 0.76875147\n",
      "Iteration 84, loss = 0.75945660\n",
      "Iteration 85, loss = 0.75039049\n",
      "Iteration 86, loss = 0.74149988\n",
      "Iteration 87, loss = 0.73285891\n",
      "Iteration 88, loss = 0.72446530\n",
      "Iteration 89, loss = 0.71607778\n",
      "Iteration 90, loss = 0.70789210\n",
      "Iteration 91, loss = 0.70001007\n",
      "Iteration 92, loss = 0.69208256\n",
      "Iteration 93, loss = 0.68451735\n",
      "Iteration 94, loss = 0.67705963\n",
      "Iteration 95, loss = 0.66955974\n",
      "Iteration 96, loss = 0.66242544\n",
      "Iteration 97, loss = 0.65538788\n",
      "Iteration 98, loss = 0.64858595\n",
      "Iteration 99, loss = 0.64187014\n",
      "Iteration 100, loss = 0.63504801\n",
      "Iteration 101, loss = 0.62850125\n",
      "Iteration 102, loss = 0.62211285\n",
      "Iteration 103, loss = 0.61575248\n",
      "Iteration 104, loss = 0.60973404\n",
      "Iteration 105, loss = 0.60377189\n",
      "Iteration 106, loss = 0.59775339\n",
      "Iteration 107, loss = 0.59184810\n",
      "Iteration 108, loss = 0.58615604\n",
      "Iteration 109, loss = 0.58060886\n",
      "Iteration 110, loss = 0.57520988\n",
      "Iteration 111, loss = 0.56974076\n",
      "Iteration 112, loss = 0.56446332\n",
      "Iteration 113, loss = 0.55919680\n",
      "Iteration 114, loss = 0.55401869\n",
      "Iteration 115, loss = 0.54912815\n",
      "Iteration 116, loss = 0.54419850\n",
      "Iteration 117, loss = 0.53919672\n",
      "Iteration 118, loss = 0.53439140\n",
      "Iteration 119, loss = 0.52971392\n",
      "Iteration 120, loss = 0.52498962\n",
      "Iteration 121, loss = 0.52053354\n",
      "Iteration 122, loss = 0.51607023\n",
      "Iteration 123, loss = 0.51176675\n",
      "Iteration 124, loss = 0.50727180\n",
      "Iteration 125, loss = 0.50313692\n",
      "Iteration 126, loss = 0.49898092\n",
      "Iteration 127, loss = 0.49486101\n",
      "Iteration 128, loss = 0.49053199\n",
      "Iteration 129, loss = 0.48646776\n",
      "Iteration 130, loss = 0.48250048\n",
      "Iteration 131, loss = 0.47857411\n",
      "Iteration 132, loss = 0.47472858\n",
      "Iteration 133, loss = 0.47108241\n",
      "Iteration 134, loss = 0.46735956\n",
      "Iteration 135, loss = 0.46364735\n",
      "Iteration 136, loss = 0.45997139\n",
      "Iteration 137, loss = 0.45638624\n",
      "Iteration 138, loss = 0.45288028\n",
      "Iteration 139, loss = 0.44956963\n",
      "Iteration 140, loss = 0.44609210\n",
      "Iteration 141, loss = 0.44271495\n",
      "Iteration 142, loss = 0.43934928\n",
      "Iteration 143, loss = 0.43595827\n",
      "Iteration 144, loss = 0.43271491\n",
      "Iteration 145, loss = 0.42966445\n",
      "Iteration 146, loss = 0.42656497\n",
      "Iteration 147, loss = 0.42344458\n",
      "Iteration 148, loss = 0.42040399\n",
      "Iteration 149, loss = 0.41732307\n",
      "Iteration 150, loss = 0.41428998\n",
      "Iteration 151, loss = 0.41136299\n",
      "Iteration 152, loss = 0.40842346\n",
      "Iteration 153, loss = 0.40556621\n",
      "Iteration 154, loss = 0.40276047\n",
      "Iteration 155, loss = 0.40002820\n",
      "Iteration 156, loss = 0.39715175\n",
      "Iteration 157, loss = 0.39454895\n",
      "Iteration 158, loss = 0.39182408\n",
      "Iteration 159, loss = 0.38906855\n",
      "Iteration 160, loss = 0.38653336\n",
      "Iteration 161, loss = 0.38393232\n",
      "Iteration 162, loss = 0.38136400\n",
      "Iteration 163, loss = 0.37886850\n",
      "Iteration 164, loss = 0.37631994\n",
      "Iteration 165, loss = 0.37396770\n",
      "Iteration 166, loss = 0.37152754\n",
      "Iteration 167, loss = 0.36912734\n",
      "Iteration 168, loss = 0.36676274\n",
      "Iteration 169, loss = 0.36443591\n",
      "Iteration 170, loss = 0.36222436\n",
      "Iteration 171, loss = 0.35979844\n",
      "Iteration 172, loss = 0.35757261\n",
      "Iteration 173, loss = 0.35538209\n",
      "Iteration 174, loss = 0.35317267\n",
      "Iteration 175, loss = 0.35095424\n",
      "Iteration 176, loss = 0.34878820\n",
      "Iteration 177, loss = 0.34657418\n",
      "Iteration 178, loss = 0.34450280\n",
      "Iteration 179, loss = 0.34249188\n",
      "Iteration 180, loss = 0.34039751\n",
      "Iteration 181, loss = 0.33831135\n",
      "Iteration 182, loss = 0.33623411\n",
      "Iteration 183, loss = 0.33431088\n",
      "Iteration 184, loss = 0.33222323\n",
      "Iteration 185, loss = 0.33027368\n",
      "Iteration 186, loss = 0.32835727\n",
      "Iteration 187, loss = 0.32641670\n",
      "Iteration 188, loss = 0.32452335\n",
      "Iteration 189, loss = 0.32256758\n",
      "Iteration 190, loss = 0.32079478\n",
      "Iteration 191, loss = 0.31889712\n",
      "Iteration 192, loss = 0.31705409\n",
      "Iteration 193, loss = 0.31534884\n",
      "Iteration 194, loss = 0.31348331\n",
      "Iteration 195, loss = 0.31169642\n",
      "Iteration 196, loss = 0.30997632\n",
      "Iteration 197, loss = 0.30825567\n",
      "Iteration 198, loss = 0.30655709\n",
      "Iteration 199, loss = 0.30478185\n",
      "Iteration 200, loss = 0.30298794\n",
      "Iteration 201, loss = 0.30137018\n",
      "Iteration 202, loss = 0.29971513\n",
      "Iteration 203, loss = 0.29813208\n",
      "Iteration 204, loss = 0.29649498\n",
      "Iteration 205, loss = 0.29493545\n",
      "Iteration 206, loss = 0.29336379\n",
      "Iteration 207, loss = 0.29186584\n",
      "Iteration 208, loss = 0.29033255\n",
      "Iteration 209, loss = 0.28877432\n",
      "Iteration 210, loss = 0.28731035\n",
      "Iteration 211, loss = 0.28578409\n",
      "Iteration 212, loss = 0.28426438\n",
      "Iteration 213, loss = 0.28277486\n",
      "Iteration 214, loss = 0.28127331\n",
      "Iteration 215, loss = 0.27973762\n",
      "Iteration 216, loss = 0.27846291\n",
      "Iteration 217, loss = 0.27699479\n",
      "Iteration 218, loss = 0.27567348\n",
      "Iteration 219, loss = 0.27418769\n",
      "Iteration 220, loss = 0.27279333\n",
      "Iteration 221, loss = 0.27142371\n",
      "Iteration 222, loss = 0.27006682\n",
      "Iteration 223, loss = 0.26872004\n",
      "Iteration 224, loss = 0.26745219\n",
      "Iteration 225, loss = 0.26608878\n",
      "Iteration 226, loss = 0.26488889\n",
      "Iteration 227, loss = 0.26367234\n",
      "Iteration 228, loss = 0.26225898\n",
      "Iteration 229, loss = 0.26092287\n",
      "Iteration 230, loss = 0.25961413\n",
      "Iteration 231, loss = 0.25833754\n",
      "Iteration 232, loss = 0.25711891\n",
      "Iteration 233, loss = 0.25590522\n",
      "Iteration 234, loss = 0.25468212\n",
      "Iteration 235, loss = 0.25351376\n",
      "Iteration 236, loss = 0.25233048\n",
      "Iteration 237, loss = 0.25116164\n",
      "Iteration 238, loss = 0.24988637\n",
      "Iteration 239, loss = 0.24873612\n",
      "Iteration 240, loss = 0.24764173\n",
      "Iteration 241, loss = 0.24651857\n",
      "Iteration 242, loss = 0.24540513\n",
      "Iteration 243, loss = 0.24425476\n",
      "Iteration 244, loss = 0.24329607\n",
      "Iteration 245, loss = 0.24209542\n",
      "Iteration 246, loss = 0.24094291\n",
      "Iteration 247, loss = 0.23983640\n",
      "Iteration 248, loss = 0.23880312\n",
      "Iteration 249, loss = 0.23769170\n",
      "Iteration 250, loss = 0.23665010\n",
      "Iteration 251, loss = 0.23560144\n",
      "Iteration 252, loss = 0.23463415\n",
      "Iteration 253, loss = 0.23364817\n",
      "Iteration 254, loss = 0.23252935\n",
      "Iteration 255, loss = 0.23153353\n",
      "Iteration 256, loss = 0.23050701\n",
      "Iteration 257, loss = 0.22952894\n",
      "Iteration 258, loss = 0.22855640\n",
      "Iteration 259, loss = 0.22755635\n",
      "Iteration 260, loss = 0.22664801\n",
      "Iteration 261, loss = 0.22558865\n",
      "Iteration 262, loss = 0.22465215\n",
      "Iteration 263, loss = 0.22368319\n",
      "Iteration 264, loss = 0.22271501\n",
      "Iteration 265, loss = 0.22178190\n",
      "Iteration 266, loss = 0.22084989\n",
      "Iteration 267, loss = 0.21990435\n",
      "Iteration 268, loss = 0.21898264\n",
      "Iteration 269, loss = 0.21801157\n",
      "Iteration 270, loss = 0.21715418\n",
      "Iteration 271, loss = 0.21625456\n",
      "Iteration 272, loss = 0.21533819\n",
      "Iteration 273, loss = 0.21448055\n",
      "Iteration 274, loss = 0.21350565\n",
      "Iteration 275, loss = 0.21275580\n",
      "Iteration 276, loss = 0.21194978\n",
      "Iteration 277, loss = 0.21101952\n",
      "Iteration 278, loss = 0.21016009\n",
      "Iteration 279, loss = 0.20926426\n",
      "Iteration 280, loss = 0.20847483\n",
      "Iteration 281, loss = 0.20759677\n",
      "Iteration 282, loss = 0.20684247\n",
      "Iteration 283, loss = 0.20600438\n",
      "Iteration 284, loss = 0.20516724\n",
      "Iteration 285, loss = 0.20441909\n",
      "Iteration 286, loss = 0.20363185\n",
      "Iteration 287, loss = 0.20281708\n",
      "Iteration 288, loss = 0.20206056\n",
      "Iteration 289, loss = 0.20121988\n",
      "Iteration 290, loss = 0.20045574\n",
      "Iteration 291, loss = 0.19971615\n",
      "Iteration 292, loss = 0.19893257\n",
      "Iteration 293, loss = 0.19814782\n",
      "Iteration 294, loss = 0.19745651\n",
      "Iteration 295, loss = 0.19669650\n",
      "Iteration 296, loss = 0.19593820\n",
      "Iteration 297, loss = 0.19525092\n",
      "Iteration 298, loss = 0.19448820\n",
      "Iteration 299, loss = 0.19379596\n",
      "Iteration 300, loss = 0.19308590\n",
      "Iteration 301, loss = 0.19233535\n",
      "Iteration 302, loss = 0.19158656\n",
      "Iteration 303, loss = 0.19087961\n",
      "Iteration 304, loss = 0.19021762\n",
      "Iteration 305, loss = 0.18957062\n",
      "Iteration 306, loss = 0.18877657\n",
      "Iteration 307, loss = 0.18805732\n",
      "Iteration 308, loss = 0.18733981\n",
      "Iteration 309, loss = 0.18672267\n",
      "Iteration 310, loss = 0.18601496\n",
      "Iteration 311, loss = 0.18538910\n",
      "Iteration 312, loss = 0.18469805\n",
      "Iteration 313, loss = 0.18409832\n",
      "Iteration 314, loss = 0.18338992\n",
      "Iteration 315, loss = 0.18275865\n",
      "Iteration 316, loss = 0.18210359\n",
      "Iteration 317, loss = 0.18150678\n",
      "Iteration 318, loss = 0.18084748\n",
      "Iteration 319, loss = 0.18026120\n",
      "Iteration 320, loss = 0.17959508\n",
      "Iteration 321, loss = 0.17895159\n",
      "Iteration 322, loss = 0.17835333\n",
      "Iteration 323, loss = 0.17778115\n",
      "Iteration 324, loss = 0.17717052\n",
      "Iteration 325, loss = 0.17651671\n",
      "Iteration 326, loss = 0.17594876\n",
      "Iteration 327, loss = 0.17536645\n",
      "Iteration 328, loss = 0.17475471\n",
      "Iteration 329, loss = 0.17415890\n",
      "Iteration 330, loss = 0.17354740\n",
      "Iteration 331, loss = 0.17296322\n",
      "Iteration 332, loss = 0.17239217\n",
      "Iteration 333, loss = 0.17180431\n",
      "Iteration 334, loss = 0.17126761\n",
      "Iteration 335, loss = 0.17065688\n",
      "Iteration 336, loss = 0.17009613\n",
      "Iteration 337, loss = 0.16954514\n",
      "Iteration 338, loss = 0.16898531\n",
      "Iteration 339, loss = 0.16851045\n",
      "Iteration 340, loss = 0.16789906\n",
      "Iteration 341, loss = 0.16735807\n",
      "Iteration 342, loss = 0.16676462\n",
      "Iteration 343, loss = 0.16625765\n",
      "Iteration 344, loss = 0.16570379\n",
      "Iteration 345, loss = 0.16519634\n",
      "Iteration 346, loss = 0.16468487\n",
      "Iteration 347, loss = 0.16416801\n",
      "Iteration 348, loss = 0.16362865\n",
      "Iteration 349, loss = 0.16310791\n",
      "Iteration 350, loss = 0.16258486\n",
      "Iteration 351, loss = 0.16207839\n",
      "Iteration 352, loss = 0.16161472\n",
      "Iteration 353, loss = 0.16108290\n",
      "Iteration 354, loss = 0.16060455\n",
      "Iteration 355, loss = 0.16007721\n",
      "Iteration 356, loss = 0.15959700\n",
      "Iteration 357, loss = 0.15911214\n",
      "Iteration 358, loss = 0.15865329\n",
      "Iteration 359, loss = 0.15813708\n",
      "Iteration 360, loss = 0.15768535\n",
      "Iteration 361, loss = 0.15718677\n",
      "Iteration 362, loss = 0.15674782\n",
      "Iteration 363, loss = 0.15623615\n",
      "Iteration 364, loss = 0.15576873\n",
      "Iteration 365, loss = 0.15527647\n",
      "Iteration 366, loss = 0.15478340\n",
      "Iteration 367, loss = 0.15435751\n",
      "Iteration 368, loss = 0.15390147\n",
      "Iteration 369, loss = 0.15342645\n",
      "Iteration 370, loss = 0.15298683\n",
      "Iteration 371, loss = 0.15252964\n",
      "Iteration 372, loss = 0.15208716\n",
      "Iteration 373, loss = 0.15163702\n",
      "Iteration 374, loss = 0.15118897\n",
      "Iteration 375, loss = 0.15075210\n",
      "Iteration 376, loss = 0.15029824\n",
      "Iteration 377, loss = 0.14988531\n",
      "Iteration 378, loss = 0.14945047\n",
      "Iteration 379, loss = 0.14901130\n",
      "Iteration 380, loss = 0.14859882\n",
      "Iteration 381, loss = 0.14815931\n",
      "Iteration 382, loss = 0.14773723\n",
      "Iteration 383, loss = 0.14729667\n",
      "Iteration 384, loss = 0.14686829\n",
      "Iteration 385, loss = 0.14643725\n",
      "Iteration 386, loss = 0.14601814\n",
      "Iteration 387, loss = 0.14559710\n",
      "Iteration 388, loss = 0.14518844\n",
      "Iteration 389, loss = 0.14478266\n",
      "Iteration 390, loss = 0.14440589\n",
      "Iteration 391, loss = 0.14397074\n",
      "Iteration 392, loss = 0.14358425\n",
      "Iteration 393, loss = 0.14318387\n",
      "Iteration 394, loss = 0.14277151\n",
      "Iteration 395, loss = 0.14236208\n",
      "Iteration 396, loss = 0.14200935\n",
      "Iteration 397, loss = 0.14158275\n",
      "Iteration 398, loss = 0.14119697\n",
      "Iteration 399, loss = 0.14080053\n",
      "Iteration 400, loss = 0.14044779\n",
      "Iteration 401, loss = 0.14006046\n",
      "Iteration 402, loss = 0.13968839\n",
      "Iteration 403, loss = 0.13926921\n",
      "Iteration 404, loss = 0.13889722\n",
      "Iteration 405, loss = 0.13852680\n",
      "Iteration 406, loss = 0.13816347\n",
      "Iteration 407, loss = 0.13779593\n",
      "Iteration 408, loss = 0.13741045\n",
      "Iteration 409, loss = 0.13706931\n",
      "Iteration 410, loss = 0.13670054\n",
      "Iteration 411, loss = 0.13639028\n",
      "Iteration 412, loss = 0.13599775\n",
      "Iteration 413, loss = 0.13565265\n",
      "Iteration 414, loss = 0.13529179\n",
      "Iteration 415, loss = 0.13493222\n",
      "Iteration 416, loss = 0.13460507\n",
      "Iteration 417, loss = 0.13422731\n",
      "Iteration 418, loss = 0.13390783\n",
      "Iteration 419, loss = 0.13353300\n",
      "Iteration 420, loss = 0.13320147\n",
      "Iteration 421, loss = 0.13283060\n",
      "Iteration 422, loss = 0.13248578\n",
      "Iteration 423, loss = 0.13219523\n",
      "Iteration 424, loss = 0.13183809\n",
      "Iteration 425, loss = 0.13149641\n",
      "Iteration 426, loss = 0.13117971\n",
      "Iteration 427, loss = 0.13086062\n",
      "Iteration 428, loss = 0.13053315\n",
      "Iteration 429, loss = 0.13019405\n",
      "Iteration 430, loss = 0.12987608\n",
      "Iteration 431, loss = 0.12953670\n",
      "Iteration 432, loss = 0.12919428\n",
      "Iteration 433, loss = 0.12888857\n",
      "Iteration 434, loss = 0.12861025\n",
      "Iteration 435, loss = 0.12828785\n",
      "Iteration 436, loss = 0.12793605\n",
      "Iteration 437, loss = 0.12761820\n",
      "Iteration 438, loss = 0.12729024\n",
      "Iteration 439, loss = 0.12698929\n",
      "Iteration 440, loss = 0.12669858\n",
      "Iteration 441, loss = 0.12642128\n",
      "Iteration 442, loss = 0.12606813\n",
      "Iteration 443, loss = 0.12575310\n",
      "Iteration 444, loss = 0.12541395\n",
      "Iteration 445, loss = 0.12509893\n",
      "Iteration 446, loss = 0.12480897\n",
      "Iteration 447, loss = 0.12450518\n",
      "Iteration 448, loss = 0.12421051\n",
      "Iteration 449, loss = 0.12389666\n",
      "Iteration 450, loss = 0.12364672\n",
      "Iteration 451, loss = 0.12332286\n",
      "Iteration 452, loss = 0.12301987\n",
      "Iteration 453, loss = 0.12273411\n",
      "Iteration 454, loss = 0.12247640\n",
      "Iteration 455, loss = 0.12221596\n",
      "Iteration 456, loss = 0.12187448\n",
      "Iteration 457, loss = 0.12160960\n",
      "Iteration 458, loss = 0.12131565\n",
      "Iteration 459, loss = 0.12102351\n",
      "Iteration 460, loss = 0.12072910\n",
      "Iteration 461, loss = 0.12048667\n",
      "Iteration 462, loss = 0.12019392\n",
      "Iteration 463, loss = 0.11988980\n",
      "Iteration 464, loss = 0.11963275\n",
      "Iteration 465, loss = 0.11937263\n",
      "Iteration 466, loss = 0.11910119\n",
      "Iteration 467, loss = 0.11881573\n",
      "Iteration 468, loss = 0.11854612\n",
      "Iteration 469, loss = 0.11829209\n",
      "Iteration 470, loss = 0.11802107\n",
      "Iteration 471, loss = 0.11774635\n",
      "Iteration 472, loss = 0.11749083\n",
      "Iteration 473, loss = 0.11722343\n",
      "Iteration 474, loss = 0.11694788\n",
      "Iteration 475, loss = 0.11670013\n",
      "Iteration 476, loss = 0.11644600\n",
      "Iteration 477, loss = 0.11623753\n",
      "Iteration 478, loss = 0.11593371\n",
      "Iteration 479, loss = 0.11572657\n",
      "Iteration 480, loss = 0.11545254\n",
      "Iteration 481, loss = 0.11519605\n",
      "Iteration 482, loss = 0.11496018\n",
      "Iteration 483, loss = 0.11468717\n",
      "Iteration 484, loss = 0.11445896\n",
      "Iteration 485, loss = 0.11421961\n",
      "Iteration 486, loss = 0.11396503\n",
      "Iteration 487, loss = 0.11372869\n",
      "Iteration 488, loss = 0.11348063\n",
      "Iteration 489, loss = 0.11323979\n",
      "Iteration 490, loss = 0.11297867\n",
      "Iteration 491, loss = 0.11276950\n",
      "Iteration 492, loss = 0.11249324\n",
      "Iteration 493, loss = 0.11225400\n",
      "Iteration 494, loss = 0.11204033\n",
      "Iteration 495, loss = 0.11179159\n",
      "Iteration 496, loss = 0.11155482\n",
      "Iteration 497, loss = 0.11127211\n",
      "Iteration 498, loss = 0.11102907\n",
      "Iteration 499, loss = 0.11080284\n",
      "Iteration 500, loss = 0.11059023\n",
      "Iteration 501, loss = 0.11037299\n",
      "Iteration 502, loss = 0.11014601\n",
      "Iteration 503, loss = 0.10992285\n",
      "Iteration 504, loss = 0.10970705\n",
      "Iteration 505, loss = 0.10947277\n",
      "Iteration 506, loss = 0.10927358\n",
      "Iteration 507, loss = 0.10903584\n",
      "Iteration 508, loss = 0.10883550\n",
      "Iteration 509, loss = 0.10860251\n",
      "Iteration 510, loss = 0.10841805\n",
      "Iteration 511, loss = 0.10820661\n",
      "Iteration 512, loss = 0.10799115\n",
      "Iteration 513, loss = 0.10775940\n",
      "Iteration 514, loss = 0.10753255\n",
      "Iteration 515, loss = 0.10731771\n",
      "Iteration 516, loss = 0.10710399\n",
      "Iteration 517, loss = 0.10690563\n",
      "Iteration 518, loss = 0.10669377\n",
      "Iteration 519, loss = 0.10649809\n",
      "Iteration 520, loss = 0.10628383\n",
      "Iteration 521, loss = 0.10608129\n",
      "Iteration 522, loss = 0.10592319\n",
      "Iteration 523, loss = 0.10571004\n",
      "Iteration 524, loss = 0.10552028\n",
      "Iteration 525, loss = 0.10529144\n",
      "Iteration 526, loss = 0.10508871\n",
      "Iteration 527, loss = 0.10488241\n",
      "Iteration 528, loss = 0.10469858\n",
      "Iteration 529, loss = 0.10448435\n",
      "Iteration 530, loss = 0.10428449\n",
      "Iteration 531, loss = 0.10410250\n",
      "Iteration 532, loss = 0.10389371\n",
      "Iteration 533, loss = 0.10371560\n",
      "Iteration 534, loss = 0.10352525\n",
      "Iteration 535, loss = 0.10333801\n",
      "Iteration 536, loss = 0.10316233\n",
      "Iteration 537, loss = 0.10295751\n",
      "Iteration 538, loss = 0.10278177\n",
      "Iteration 539, loss = 0.10258315\n",
      "Iteration 540, loss = 0.10238551\n",
      "Iteration 541, loss = 0.10220777\n",
      "Iteration 542, loss = 0.10200595\n",
      "Iteration 543, loss = 0.10181534\n",
      "Iteration 544, loss = 0.10163995\n",
      "Iteration 545, loss = 0.10142442\n",
      "Iteration 546, loss = 0.10125844\n",
      "Iteration 547, loss = 0.10102741\n",
      "Iteration 548, loss = 0.10085017\n",
      "Iteration 549, loss = 0.10069768\n",
      "Iteration 550, loss = 0.10049594\n",
      "Iteration 551, loss = 0.10030190\n",
      "Iteration 552, loss = 0.10011868\n",
      "Iteration 553, loss = 0.09993668\n",
      "Iteration 554, loss = 0.09976720\n",
      "Iteration 555, loss = 0.09960241\n",
      "Iteration 556, loss = 0.09941824\n",
      "Iteration 557, loss = 0.09924504\n",
      "Iteration 558, loss = 0.09907210\n",
      "Iteration 559, loss = 0.09890041\n",
      "Iteration 560, loss = 0.09872293\n",
      "Iteration 561, loss = 0.09856128\n",
      "Iteration 562, loss = 0.09838995\n",
      "Iteration 563, loss = 0.09820268\n",
      "Iteration 564, loss = 0.09804589\n",
      "Iteration 565, loss = 0.09787937\n",
      "Iteration 566, loss = 0.09771464\n",
      "Iteration 567, loss = 0.09754177\n",
      "Iteration 568, loss = 0.09737319\n",
      "Iteration 569, loss = 0.09719618\n",
      "Iteration 570, loss = 0.09701684\n",
      "Iteration 571, loss = 0.09684702\n",
      "Iteration 572, loss = 0.09668640\n",
      "Iteration 573, loss = 0.09653728\n",
      "Iteration 574, loss = 0.09637416\n",
      "Iteration 575, loss = 0.09620968\n",
      "Iteration 576, loss = 0.09605823\n",
      "Iteration 577, loss = 0.09590449\n",
      "Iteration 578, loss = 0.09574557\n",
      "Iteration 579, loss = 0.09556937\n",
      "Iteration 580, loss = 0.09541225\n",
      "Iteration 581, loss = 0.09523525\n",
      "Iteration 582, loss = 0.09510042\n",
      "Iteration 583, loss = 0.09495284\n",
      "Iteration 584, loss = 0.09478999\n",
      "Iteration 585, loss = 0.09463759\n",
      "Iteration 586, loss = 0.09448370\n",
      "Iteration 587, loss = 0.09433707\n",
      "Iteration 588, loss = 0.09419259\n",
      "Iteration 589, loss = 0.09402015\n",
      "Iteration 590, loss = 0.09388958\n",
      "Iteration 591, loss = 0.09372070\n",
      "Iteration 592, loss = 0.09357859\n",
      "Iteration 593, loss = 0.09341591\n",
      "Iteration 594, loss = 0.09328400\n",
      "Iteration 595, loss = 0.09312466\n",
      "Iteration 596, loss = 0.09297296\n",
      "Iteration 597, loss = 0.09282383\n",
      "Iteration 598, loss = 0.09267733\n",
      "Iteration 599, loss = 0.09253373\n",
      "Iteration 600, loss = 0.09238703\n",
      "Iteration 601, loss = 0.09224121\n",
      "Iteration 602, loss = 0.09210301\n",
      "Iteration 603, loss = 0.09196331\n",
      "Iteration 604, loss = 0.09181424\n",
      "Iteration 605, loss = 0.09165757\n",
      "Iteration 606, loss = 0.09151795\n",
      "Iteration 607, loss = 0.09138617\n",
      "Iteration 608, loss = 0.09124701\n",
      "Iteration 609, loss = 0.09111571\n",
      "Iteration 610, loss = 0.09094831\n",
      "Iteration 611, loss = 0.09080442\n",
      "Iteration 612, loss = 0.09067589\n",
      "Iteration 613, loss = 0.09053462\n",
      "Iteration 614, loss = 0.09040093\n",
      "Iteration 615, loss = 0.09027160\n",
      "Iteration 616, loss = 0.09013276\n",
      "Iteration 617, loss = 0.09000084\n",
      "Iteration 618, loss = 0.08986451\n",
      "Iteration 619, loss = 0.08972964\n",
      "Iteration 620, loss = 0.08961546\n",
      "Iteration 621, loss = 0.08946374\n",
      "Iteration 622, loss = 0.08931207\n",
      "Iteration 623, loss = 0.08920563\n",
      "Iteration 624, loss = 0.08907230\n",
      "Iteration 625, loss = 0.08895133\n",
      "Iteration 626, loss = 0.08880621\n",
      "Iteration 627, loss = 0.08867300\n",
      "Iteration 628, loss = 0.08854620\n",
      "Iteration 629, loss = 0.08842364\n",
      "Iteration 630, loss = 0.08828644\n",
      "Iteration 631, loss = 0.08816453\n",
      "Iteration 632, loss = 0.08804215\n",
      "Iteration 633, loss = 0.08792475\n",
      "Iteration 634, loss = 0.08778361\n",
      "Iteration 635, loss = 0.08764468\n",
      "Iteration 636, loss = 0.08751534\n",
      "Iteration 637, loss = 0.08740170\n",
      "Iteration 638, loss = 0.08727351\n",
      "Iteration 639, loss = 0.08715640\n",
      "Iteration 640, loss = 0.08703619\n",
      "Iteration 641, loss = 0.08693072\n",
      "Iteration 642, loss = 0.08679607\n",
      "Iteration 643, loss = 0.08667764\n",
      "Iteration 644, loss = 0.08653921\n",
      "Iteration 645, loss = 0.08643062\n",
      "Iteration 646, loss = 0.08629571\n",
      "Iteration 647, loss = 0.08618649\n",
      "Iteration 648, loss = 0.08606174\n",
      "Iteration 649, loss = 0.08594814\n",
      "Iteration 650, loss = 0.08585273\n",
      "Iteration 651, loss = 0.08574562\n",
      "Iteration 652, loss = 0.08560462\n",
      "Iteration 653, loss = 0.08549018\n",
      "Iteration 654, loss = 0.08537725\n",
      "Iteration 655, loss = 0.08525922\n",
      "Iteration 656, loss = 0.08515294\n",
      "Iteration 657, loss = 0.08503534\n",
      "Iteration 658, loss = 0.08492561\n",
      "Iteration 659, loss = 0.08481544\n",
      "Iteration 660, loss = 0.08469963\n",
      "Iteration 661, loss = 0.08458011\n",
      "Iteration 662, loss = 0.08447502\n",
      "Iteration 663, loss = 0.08433994\n",
      "Iteration 664, loss = 0.08422976\n",
      "Iteration 665, loss = 0.08411437\n",
      "Iteration 666, loss = 0.08403274\n",
      "Iteration 667, loss = 0.08391366\n",
      "Iteration 668, loss = 0.08379418\n",
      "Iteration 669, loss = 0.08368164\n",
      "Iteration 670, loss = 0.08356542\n",
      "Iteration 671, loss = 0.08348220\n",
      "Iteration 672, loss = 0.08334635\n",
      "Iteration 673, loss = 0.08324618\n",
      "Iteration 674, loss = 0.08313972\n",
      "Iteration 675, loss = 0.08304252\n",
      "Iteration 676, loss = 0.08293443\n",
      "Iteration 677, loss = 0.08283398\n",
      "Iteration 678, loss = 0.08272946\n",
      "Iteration 679, loss = 0.08264260\n",
      "Iteration 680, loss = 0.08253897\n",
      "Iteration 681, loss = 0.08242993\n",
      "Iteration 682, loss = 0.08231941\n",
      "Iteration 683, loss = 0.08220728\n",
      "Iteration 684, loss = 0.08210429\n",
      "Iteration 685, loss = 0.08200788\n",
      "Iteration 686, loss = 0.08190284\n",
      "Iteration 687, loss = 0.08180489\n",
      "Iteration 688, loss = 0.08169884\n",
      "Iteration 689, loss = 0.08159304\n",
      "Iteration 690, loss = 0.08149179\n",
      "Iteration 691, loss = 0.08139851\n",
      "Iteration 692, loss = 0.08132018\n",
      "Iteration 693, loss = 0.08119506\n",
      "Iteration 694, loss = 0.08110594\n",
      "Iteration 695, loss = 0.08101862\n",
      "Iteration 696, loss = 0.08091330\n",
      "Iteration 697, loss = 0.08081222\n",
      "Iteration 698, loss = 0.08071710\n",
      "Iteration 699, loss = 0.08061451\n",
      "Iteration 700, loss = 0.08051680\n",
      "Iteration 701, loss = 0.08042215\n",
      "Iteration 702, loss = 0.08033260\n",
      "Iteration 703, loss = 0.08023167\n",
      "Iteration 704, loss = 0.08012993\n",
      "Iteration 705, loss = 0.08002990\n",
      "Iteration 706, loss = 0.07992649\n",
      "Iteration 707, loss = 0.07983074\n",
      "Iteration 708, loss = 0.07972753\n",
      "Iteration 709, loss = 0.07964397\n",
      "Iteration 710, loss = 0.07956028\n",
      "Iteration 711, loss = 0.07947028\n",
      "Iteration 712, loss = 0.07938407\n",
      "Iteration 713, loss = 0.07929945\n",
      "Iteration 714, loss = 0.07919324\n",
      "Iteration 715, loss = 0.07910595\n",
      "Iteration 716, loss = 0.07901656\n",
      "Iteration 717, loss = 0.07892872\n",
      "Iteration 718, loss = 0.07884077\n",
      "Iteration 719, loss = 0.07874669\n",
      "Iteration 720, loss = 0.07866586\n",
      "Iteration 721, loss = 0.07857545\n",
      "Iteration 722, loss = 0.07849420\n",
      "Iteration 723, loss = 0.07840328\n",
      "Iteration 724, loss = 0.07832085\n",
      "Iteration 725, loss = 0.07822875\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.36578117\n",
      "Iteration 2, loss = 2.35323055\n",
      "Iteration 3, loss = 2.33623955\n",
      "Iteration 4, loss = 2.31822247\n",
      "Iteration 5, loss = 2.30059912\n",
      "Iteration 6, loss = 2.28313379\n",
      "Iteration 7, loss = 2.26604910\n",
      "Iteration 8, loss = 2.24883007\n",
      "Iteration 9, loss = 2.23125437\n",
      "Iteration 10, loss = 2.21334517\n",
      "Iteration 11, loss = 2.19494803\n",
      "Iteration 12, loss = 2.17612130\n",
      "Iteration 13, loss = 2.15687922\n",
      "Iteration 14, loss = 2.13704583\n",
      "Iteration 15, loss = 2.11657080\n",
      "Iteration 16, loss = 2.09577191\n",
      "Iteration 17, loss = 2.07399814\n",
      "Iteration 18, loss = 2.05187031\n",
      "Iteration 19, loss = 2.02929125\n",
      "Iteration 20, loss = 2.00608859\n",
      "Iteration 21, loss = 1.98247816\n",
      "Iteration 22, loss = 1.95856658\n",
      "Iteration 23, loss = 1.93401868\n",
      "Iteration 24, loss = 1.90901245\n",
      "Iteration 25, loss = 1.88388709\n",
      "Iteration 26, loss = 1.85802334\n",
      "Iteration 27, loss = 1.83192494\n",
      "Iteration 28, loss = 1.80585839\n",
      "Iteration 29, loss = 1.77960675\n",
      "Iteration 30, loss = 1.75334131\n",
      "Iteration 31, loss = 1.72687659\n",
      "Iteration 32, loss = 1.70026357\n",
      "Iteration 33, loss = 1.67391623\n",
      "Iteration 34, loss = 1.64750620\n",
      "Iteration 35, loss = 1.62123740\n",
      "Iteration 36, loss = 1.59478909\n",
      "Iteration 37, loss = 1.56895491\n",
      "Iteration 38, loss = 1.54317912\n",
      "Iteration 39, loss = 1.51736269\n",
      "Iteration 40, loss = 1.49197402\n",
      "Iteration 41, loss = 1.46690281\n",
      "Iteration 42, loss = 1.44252645\n",
      "Iteration 43, loss = 1.41854459\n",
      "Iteration 44, loss = 1.39494837\n",
      "Iteration 45, loss = 1.37181429\n",
      "Iteration 46, loss = 1.34938417\n",
      "Iteration 47, loss = 1.32725802\n",
      "Iteration 48, loss = 1.30538698\n",
      "Iteration 49, loss = 1.28464305\n",
      "Iteration 50, loss = 1.26367682\n",
      "Iteration 51, loss = 1.24399277\n",
      "Iteration 52, loss = 1.22425587\n",
      "Iteration 53, loss = 1.20470869\n",
      "Iteration 54, loss = 1.18571631\n",
      "Iteration 55, loss = 1.16724167\n",
      "Iteration 56, loss = 1.14918010\n",
      "Iteration 57, loss = 1.13170706\n",
      "Iteration 58, loss = 1.11446069\n",
      "Iteration 59, loss = 1.09773945\n",
      "Iteration 60, loss = 1.08115232\n",
      "Iteration 61, loss = 1.06516211\n",
      "Iteration 62, loss = 1.04948398\n",
      "Iteration 63, loss = 1.03449137\n",
      "Iteration 64, loss = 1.01968366\n",
      "Iteration 65, loss = 1.00518033\n",
      "Iteration 66, loss = 0.99115053\n",
      "Iteration 67, loss = 0.97724670\n",
      "Iteration 68, loss = 0.96374985\n",
      "Iteration 69, loss = 0.95079637\n",
      "Iteration 70, loss = 0.93793367\n",
      "Iteration 71, loss = 0.92540909\n",
      "Iteration 72, loss = 0.91314325\n",
      "Iteration 73, loss = 0.90117292\n",
      "Iteration 74, loss = 0.88927696\n",
      "Iteration 75, loss = 0.87800453\n",
      "Iteration 76, loss = 0.86677488\n",
      "Iteration 77, loss = 0.85577878\n",
      "Iteration 78, loss = 0.84513721\n",
      "Iteration 79, loss = 0.83499677\n",
      "Iteration 80, loss = 0.82457425\n",
      "Iteration 81, loss = 0.81452332\n",
      "Iteration 82, loss = 0.80466731\n",
      "Iteration 83, loss = 0.79514906\n",
      "Iteration 84, loss = 0.78579991\n",
      "Iteration 85, loss = 0.77656040\n",
      "Iteration 86, loss = 0.76767024\n",
      "Iteration 87, loss = 0.75885594\n",
      "Iteration 88, loss = 0.75026608\n",
      "Iteration 89, loss = 0.74172356\n",
      "Iteration 90, loss = 0.73352597\n",
      "Iteration 91, loss = 0.72544635\n",
      "Iteration 92, loss = 0.71746375\n",
      "Iteration 93, loss = 0.70969128\n",
      "Iteration 94, loss = 0.70231749\n",
      "Iteration 95, loss = 0.69446225\n",
      "Iteration 96, loss = 0.68712252\n",
      "Iteration 97, loss = 0.67995407\n",
      "Iteration 98, loss = 0.67293772\n",
      "Iteration 99, loss = 0.66601376\n",
      "Iteration 100, loss = 0.65916628\n",
      "Iteration 101, loss = 0.65249348\n",
      "Iteration 102, loss = 0.64591150\n",
      "Iteration 103, loss = 0.63946121\n",
      "Iteration 104, loss = 0.63328962\n",
      "Iteration 105, loss = 0.62714393\n",
      "Iteration 106, loss = 0.62104528\n",
      "Iteration 107, loss = 0.61500549\n",
      "Iteration 108, loss = 0.60926753\n",
      "Iteration 109, loss = 0.60344488\n",
      "Iteration 110, loss = 0.59784250\n",
      "Iteration 111, loss = 0.59222309\n",
      "Iteration 112, loss = 0.58671046\n",
      "Iteration 113, loss = 0.58133043\n",
      "Iteration 114, loss = 0.57603567\n",
      "Iteration 115, loss = 0.57089181\n",
      "Iteration 116, loss = 0.56573570\n",
      "Iteration 117, loss = 0.56066131\n",
      "Iteration 118, loss = 0.55569205\n",
      "Iteration 119, loss = 0.55083225\n",
      "Iteration 120, loss = 0.54600765\n",
      "Iteration 121, loss = 0.54143761\n",
      "Iteration 122, loss = 0.53660430\n",
      "Iteration 123, loss = 0.53205344\n",
      "Iteration 124, loss = 0.52737840\n",
      "Iteration 125, loss = 0.52307445\n",
      "Iteration 126, loss = 0.51883391\n",
      "Iteration 127, loss = 0.51439685\n",
      "Iteration 128, loss = 0.50989788\n",
      "Iteration 129, loss = 0.50572192\n",
      "Iteration 130, loss = 0.50158121\n",
      "Iteration 131, loss = 0.49753454\n",
      "Iteration 132, loss = 0.49355638\n",
      "Iteration 133, loss = 0.48961357\n",
      "Iteration 134, loss = 0.48580876\n",
      "Iteration 135, loss = 0.48202633\n",
      "Iteration 136, loss = 0.47819449\n",
      "Iteration 137, loss = 0.47439779\n",
      "Iteration 138, loss = 0.47084484\n",
      "Iteration 139, loss = 0.46735063\n",
      "Iteration 140, loss = 0.46375500\n",
      "Iteration 141, loss = 0.46029695\n",
      "Iteration 142, loss = 0.45686682\n",
      "Iteration 143, loss = 0.45325143\n",
      "Iteration 144, loss = 0.44983354\n",
      "Iteration 145, loss = 0.44674273\n",
      "Iteration 146, loss = 0.44338440\n",
      "Iteration 147, loss = 0.44009963\n",
      "Iteration 148, loss = 0.43710182\n",
      "Iteration 149, loss = 0.43400055\n",
      "Iteration 150, loss = 0.43074567\n",
      "Iteration 151, loss = 0.42763273\n",
      "Iteration 152, loss = 0.42464617\n",
      "Iteration 153, loss = 0.42164053\n",
      "Iteration 154, loss = 0.41871506\n",
      "Iteration 155, loss = 0.41591106\n",
      "Iteration 156, loss = 0.41295463\n",
      "Iteration 157, loss = 0.41017378\n",
      "Iteration 158, loss = 0.40732365\n",
      "Iteration 159, loss = 0.40439862\n",
      "Iteration 160, loss = 0.40175300\n",
      "Iteration 161, loss = 0.39912521\n",
      "Iteration 162, loss = 0.39645228\n",
      "Iteration 163, loss = 0.39384400\n",
      "Iteration 164, loss = 0.39118880\n",
      "Iteration 165, loss = 0.38870434\n",
      "Iteration 166, loss = 0.38616152\n",
      "Iteration 167, loss = 0.38366436\n",
      "Iteration 168, loss = 0.38109983\n",
      "Iteration 169, loss = 0.37877702\n",
      "Iteration 170, loss = 0.37648174\n",
      "Iteration 171, loss = 0.37394003\n",
      "Iteration 172, loss = 0.37163180\n",
      "Iteration 173, loss = 0.36929220\n",
      "Iteration 174, loss = 0.36687788\n",
      "Iteration 175, loss = 0.36466067\n",
      "Iteration 176, loss = 0.36240894\n",
      "Iteration 177, loss = 0.36012659\n",
      "Iteration 178, loss = 0.35788498\n",
      "Iteration 179, loss = 0.35572077\n",
      "Iteration 180, loss = 0.35353230\n",
      "Iteration 181, loss = 0.35136758\n",
      "Iteration 182, loss = 0.34918102\n",
      "Iteration 183, loss = 0.34717244\n",
      "Iteration 184, loss = 0.34501387\n",
      "Iteration 185, loss = 0.34287219\n",
      "Iteration 186, loss = 0.34086288\n",
      "Iteration 187, loss = 0.33881576\n",
      "Iteration 188, loss = 0.33685548\n",
      "Iteration 189, loss = 0.33490462\n",
      "Iteration 190, loss = 0.33302119\n",
      "Iteration 191, loss = 0.33106754\n",
      "Iteration 192, loss = 0.32912209\n",
      "Iteration 193, loss = 0.32730781\n",
      "Iteration 194, loss = 0.32538151\n",
      "Iteration 195, loss = 0.32348307\n",
      "Iteration 196, loss = 0.32167769\n",
      "Iteration 197, loss = 0.31993167\n",
      "Iteration 198, loss = 0.31814594\n",
      "Iteration 199, loss = 0.31636960\n",
      "Iteration 200, loss = 0.31440524\n",
      "Iteration 201, loss = 0.31272267\n",
      "Iteration 202, loss = 0.31105285\n",
      "Iteration 203, loss = 0.30931611\n",
      "Iteration 204, loss = 0.30761763\n",
      "Iteration 205, loss = 0.30600629\n",
      "Iteration 206, loss = 0.30434342\n",
      "Iteration 207, loss = 0.30278838\n",
      "Iteration 208, loss = 0.30126402\n",
      "Iteration 209, loss = 0.29971407\n",
      "Iteration 210, loss = 0.29811376\n",
      "Iteration 211, loss = 0.29655240\n",
      "Iteration 212, loss = 0.29493593\n",
      "Iteration 213, loss = 0.29341795\n",
      "Iteration 214, loss = 0.29181509\n",
      "Iteration 215, loss = 0.29025741\n",
      "Iteration 216, loss = 0.28888855\n",
      "Iteration 217, loss = 0.28739774\n",
      "Iteration 218, loss = 0.28594969\n",
      "Iteration 219, loss = 0.28443345\n",
      "Iteration 220, loss = 0.28302995\n",
      "Iteration 221, loss = 0.28159731\n",
      "Iteration 222, loss = 0.28023371\n",
      "Iteration 223, loss = 0.27880039\n",
      "Iteration 224, loss = 0.27744986\n",
      "Iteration 225, loss = 0.27602423\n",
      "Iteration 226, loss = 0.27477150\n",
      "Iteration 227, loss = 0.27342756\n",
      "Iteration 228, loss = 0.27208029\n",
      "Iteration 229, loss = 0.27065767\n",
      "Iteration 230, loss = 0.26932070\n",
      "Iteration 231, loss = 0.26799882\n",
      "Iteration 232, loss = 0.26672531\n",
      "Iteration 233, loss = 0.26547313\n",
      "Iteration 234, loss = 0.26419121\n",
      "Iteration 235, loss = 0.26303299\n",
      "Iteration 236, loss = 0.26171180\n",
      "Iteration 237, loss = 0.26054149\n",
      "Iteration 238, loss = 0.25925784\n",
      "Iteration 239, loss = 0.25805837\n",
      "Iteration 240, loss = 0.25686046\n",
      "Iteration 241, loss = 0.25571561\n",
      "Iteration 242, loss = 0.25461701\n",
      "Iteration 243, loss = 0.25338815\n",
      "Iteration 244, loss = 0.25235040\n",
      "Iteration 245, loss = 0.25115842\n",
      "Iteration 246, loss = 0.25003953\n",
      "Iteration 247, loss = 0.24881406\n",
      "Iteration 248, loss = 0.24768435\n",
      "Iteration 249, loss = 0.24660842\n",
      "Iteration 250, loss = 0.24549571\n",
      "Iteration 251, loss = 0.24439474\n",
      "Iteration 252, loss = 0.24339757\n",
      "Iteration 253, loss = 0.24232240\n",
      "Iteration 254, loss = 0.24117168\n",
      "Iteration 255, loss = 0.24012112\n",
      "Iteration 256, loss = 0.23908241\n",
      "Iteration 257, loss = 0.23808880\n",
      "Iteration 258, loss = 0.23707593\n",
      "Iteration 259, loss = 0.23601779\n",
      "Iteration 260, loss = 0.23504710\n",
      "Iteration 261, loss = 0.23406038\n",
      "Iteration 262, loss = 0.23305003\n",
      "Iteration 263, loss = 0.23204818\n",
      "Iteration 264, loss = 0.23107226\n",
      "Iteration 265, loss = 0.23010357\n",
      "Iteration 266, loss = 0.22908423\n",
      "Iteration 267, loss = 0.22817479\n",
      "Iteration 268, loss = 0.22723877\n",
      "Iteration 269, loss = 0.22622396\n",
      "Iteration 270, loss = 0.22534034\n",
      "Iteration 271, loss = 0.22440868\n",
      "Iteration 272, loss = 0.22350418\n",
      "Iteration 273, loss = 0.22263075\n",
      "Iteration 274, loss = 0.22156883\n",
      "Iteration 275, loss = 0.22073598\n",
      "Iteration 276, loss = 0.21990954\n",
      "Iteration 277, loss = 0.21894572\n",
      "Iteration 278, loss = 0.21806489\n",
      "Iteration 279, loss = 0.21718162\n",
      "Iteration 280, loss = 0.21636679\n",
      "Iteration 281, loss = 0.21543043\n",
      "Iteration 282, loss = 0.21462518\n",
      "Iteration 283, loss = 0.21377828\n",
      "Iteration 284, loss = 0.21288679\n",
      "Iteration 285, loss = 0.21211997\n",
      "Iteration 286, loss = 0.21127425\n",
      "Iteration 287, loss = 0.21043068\n",
      "Iteration 288, loss = 0.20966450\n",
      "Iteration 289, loss = 0.20878777\n",
      "Iteration 290, loss = 0.20799620\n",
      "Iteration 291, loss = 0.20720593\n",
      "Iteration 292, loss = 0.20641774\n",
      "Iteration 293, loss = 0.20565851\n",
      "Iteration 294, loss = 0.20493189\n",
      "Iteration 295, loss = 0.20413193\n",
      "Iteration 296, loss = 0.20333594\n",
      "Iteration 297, loss = 0.20259594\n",
      "Iteration 298, loss = 0.20183151\n",
      "Iteration 299, loss = 0.20111322\n",
      "Iteration 300, loss = 0.20047895\n",
      "Iteration 301, loss = 0.19965213\n",
      "Iteration 302, loss = 0.19887462\n",
      "Iteration 303, loss = 0.19818743\n",
      "Iteration 304, loss = 0.19745651\n",
      "Iteration 305, loss = 0.19677806\n",
      "Iteration 306, loss = 0.19597235\n",
      "Iteration 307, loss = 0.19523668\n",
      "Iteration 308, loss = 0.19446126\n",
      "Iteration 309, loss = 0.19381454\n",
      "Iteration 310, loss = 0.19309251\n",
      "Iteration 311, loss = 0.19241998\n",
      "Iteration 312, loss = 0.19170641\n",
      "Iteration 313, loss = 0.19110722\n",
      "Iteration 314, loss = 0.19040248\n",
      "Iteration 315, loss = 0.18973563\n",
      "Iteration 316, loss = 0.18907416\n",
      "Iteration 317, loss = 0.18846846\n",
      "Iteration 318, loss = 0.18774082\n",
      "Iteration 319, loss = 0.18708371\n",
      "Iteration 320, loss = 0.18639980\n",
      "Iteration 321, loss = 0.18572502\n",
      "Iteration 322, loss = 0.18513105\n",
      "Iteration 323, loss = 0.18448978\n",
      "Iteration 324, loss = 0.18388613\n",
      "Iteration 325, loss = 0.18323139\n",
      "Iteration 326, loss = 0.18259557\n",
      "Iteration 327, loss = 0.18199275\n",
      "Iteration 328, loss = 0.18139372\n",
      "Iteration 329, loss = 0.18076576\n",
      "Iteration 330, loss = 0.18013035\n",
      "Iteration 331, loss = 0.17952961\n",
      "Iteration 332, loss = 0.17894831\n",
      "Iteration 333, loss = 0.17833042\n",
      "Iteration 334, loss = 0.17777044\n",
      "Iteration 335, loss = 0.17716668\n",
      "Iteration 336, loss = 0.17654993\n",
      "Iteration 337, loss = 0.17594986\n",
      "Iteration 338, loss = 0.17535342\n",
      "Iteration 339, loss = 0.17488012\n",
      "Iteration 340, loss = 0.17427867\n",
      "Iteration 341, loss = 0.17371483\n",
      "Iteration 342, loss = 0.17309179\n",
      "Iteration 343, loss = 0.17254152\n",
      "Iteration 344, loss = 0.17198629\n",
      "Iteration 345, loss = 0.17145877\n",
      "Iteration 346, loss = 0.17092817\n",
      "Iteration 347, loss = 0.17037437\n",
      "Iteration 348, loss = 0.16983462\n",
      "Iteration 349, loss = 0.16928682\n",
      "Iteration 350, loss = 0.16871374\n",
      "Iteration 351, loss = 0.16819473\n",
      "Iteration 352, loss = 0.16770355\n",
      "Iteration 353, loss = 0.16716266\n",
      "Iteration 354, loss = 0.16665157\n",
      "Iteration 355, loss = 0.16610080\n",
      "Iteration 356, loss = 0.16560829\n",
      "Iteration 357, loss = 0.16511001\n",
      "Iteration 358, loss = 0.16462745\n",
      "Iteration 359, loss = 0.16411013\n",
      "Iteration 360, loss = 0.16365208\n",
      "Iteration 361, loss = 0.16312685\n",
      "Iteration 362, loss = 0.16267285\n",
      "Iteration 363, loss = 0.16211741\n",
      "Iteration 364, loss = 0.16167641\n",
      "Iteration 365, loss = 0.16118328\n",
      "Iteration 366, loss = 0.16069038\n",
      "Iteration 367, loss = 0.16021394\n",
      "Iteration 368, loss = 0.15973232\n",
      "Iteration 369, loss = 0.15923935\n",
      "Iteration 370, loss = 0.15877357\n",
      "Iteration 371, loss = 0.15831370\n",
      "Iteration 372, loss = 0.15785645\n",
      "Iteration 373, loss = 0.15741799\n",
      "Iteration 374, loss = 0.15695388\n",
      "Iteration 375, loss = 0.15648953\n",
      "Iteration 376, loss = 0.15600703\n",
      "Iteration 377, loss = 0.15554914\n",
      "Iteration 378, loss = 0.15510715\n",
      "Iteration 379, loss = 0.15465469\n",
      "Iteration 380, loss = 0.15423476\n",
      "Iteration 381, loss = 0.15377639\n",
      "Iteration 382, loss = 0.15332500\n",
      "Iteration 383, loss = 0.15287648\n",
      "Iteration 384, loss = 0.15244112\n",
      "Iteration 385, loss = 0.15197117\n",
      "Iteration 386, loss = 0.15154747\n",
      "Iteration 387, loss = 0.15112093\n",
      "Iteration 388, loss = 0.15068118\n",
      "Iteration 389, loss = 0.15025421\n",
      "Iteration 390, loss = 0.14985179\n",
      "Iteration 391, loss = 0.14941719\n",
      "Iteration 392, loss = 0.14900698\n",
      "Iteration 393, loss = 0.14858484\n",
      "Iteration 394, loss = 0.14814371\n",
      "Iteration 395, loss = 0.14772098\n",
      "Iteration 396, loss = 0.14735311\n",
      "Iteration 397, loss = 0.14692460\n",
      "Iteration 398, loss = 0.14652247\n",
      "Iteration 399, loss = 0.14610296\n",
      "Iteration 400, loss = 0.14574964\n",
      "Iteration 401, loss = 0.14532913\n",
      "Iteration 402, loss = 0.14493634\n",
      "Iteration 403, loss = 0.14452216\n",
      "Iteration 404, loss = 0.14414247\n",
      "Iteration 405, loss = 0.14378697\n",
      "Iteration 406, loss = 0.14338195\n",
      "Iteration 407, loss = 0.14300721\n",
      "Iteration 408, loss = 0.14259118\n",
      "Iteration 409, loss = 0.14223815\n",
      "Iteration 410, loss = 0.14186930\n",
      "Iteration 411, loss = 0.14151962\n",
      "Iteration 412, loss = 0.14108183\n",
      "Iteration 413, loss = 0.14072556\n",
      "Iteration 414, loss = 0.14035106\n",
      "Iteration 415, loss = 0.13999096\n",
      "Iteration 416, loss = 0.13962619\n",
      "Iteration 417, loss = 0.13924809\n",
      "Iteration 418, loss = 0.13889321\n",
      "Iteration 419, loss = 0.13850317\n",
      "Iteration 420, loss = 0.13818014\n",
      "Iteration 421, loss = 0.13779065\n",
      "Iteration 422, loss = 0.13743337\n",
      "Iteration 423, loss = 0.13711521\n",
      "Iteration 424, loss = 0.13673482\n",
      "Iteration 425, loss = 0.13637346\n",
      "Iteration 426, loss = 0.13602358\n",
      "Iteration 427, loss = 0.13569700\n",
      "Iteration 428, loss = 0.13536672\n",
      "Iteration 429, loss = 0.13502349\n",
      "Iteration 430, loss = 0.13468488\n",
      "Iteration 431, loss = 0.13434078\n",
      "Iteration 432, loss = 0.13400097\n",
      "Iteration 433, loss = 0.13369017\n",
      "Iteration 434, loss = 0.13339720\n",
      "Iteration 435, loss = 0.13302678\n",
      "Iteration 436, loss = 0.13267126\n",
      "Iteration 437, loss = 0.13235518\n",
      "Iteration 438, loss = 0.13201945\n",
      "Iteration 439, loss = 0.13168440\n",
      "Iteration 440, loss = 0.13137967\n",
      "Iteration 441, loss = 0.13105447\n",
      "Iteration 442, loss = 0.13071240\n",
      "Iteration 443, loss = 0.13040031\n",
      "Iteration 444, loss = 0.13007388\n",
      "Iteration 445, loss = 0.12976814\n",
      "Iteration 446, loss = 0.12945634\n",
      "Iteration 447, loss = 0.12915741\n",
      "Iteration 448, loss = 0.12881774\n",
      "Iteration 449, loss = 0.12849385\n",
      "Iteration 450, loss = 0.12821309\n",
      "Iteration 451, loss = 0.12789105\n",
      "Iteration 452, loss = 0.12761304\n",
      "Iteration 453, loss = 0.12728480\n",
      "Iteration 454, loss = 0.12701657\n",
      "Iteration 455, loss = 0.12672917\n",
      "Iteration 456, loss = 0.12639438\n",
      "Iteration 457, loss = 0.12612074\n",
      "Iteration 458, loss = 0.12582234\n",
      "Iteration 459, loss = 0.12551727\n",
      "Iteration 460, loss = 0.12520992\n",
      "Iteration 461, loss = 0.12494475\n",
      "Iteration 462, loss = 0.12464466\n",
      "Iteration 463, loss = 0.12433692\n",
      "Iteration 464, loss = 0.12404637\n",
      "Iteration 465, loss = 0.12375260\n",
      "Iteration 466, loss = 0.12347532\n",
      "Iteration 467, loss = 0.12321023\n",
      "Iteration 468, loss = 0.12293525\n",
      "Iteration 469, loss = 0.12263284\n",
      "Iteration 470, loss = 0.12237167\n",
      "Iteration 471, loss = 0.12207547\n",
      "Iteration 472, loss = 0.12180140\n",
      "Iteration 473, loss = 0.12153189\n",
      "Iteration 474, loss = 0.12124410\n",
      "Iteration 475, loss = 0.12097888\n",
      "Iteration 476, loss = 0.12072010\n",
      "Iteration 477, loss = 0.12048714\n",
      "Iteration 478, loss = 0.12020410\n",
      "Iteration 479, loss = 0.11996309\n",
      "Iteration 480, loss = 0.11968699\n",
      "Iteration 481, loss = 0.11942424\n",
      "Iteration 482, loss = 0.11918014\n",
      "Iteration 483, loss = 0.11889061\n",
      "Iteration 484, loss = 0.11865017\n",
      "Iteration 485, loss = 0.11840067\n",
      "Iteration 486, loss = 0.11811567\n",
      "Iteration 487, loss = 0.11786868\n",
      "Iteration 488, loss = 0.11761048\n",
      "Iteration 489, loss = 0.11735742\n",
      "Iteration 490, loss = 0.11712208\n",
      "Iteration 491, loss = 0.11687724\n",
      "Iteration 492, loss = 0.11658118\n",
      "Iteration 493, loss = 0.11632771\n",
      "Iteration 494, loss = 0.11610163\n",
      "Iteration 495, loss = 0.11583342\n",
      "Iteration 496, loss = 0.11558330\n",
      "Iteration 497, loss = 0.11530195\n",
      "Iteration 498, loss = 0.11506073\n",
      "Iteration 499, loss = 0.11481146\n",
      "Iteration 500, loss = 0.11457756\n",
      "Iteration 501, loss = 0.11434547\n",
      "Iteration 502, loss = 0.11411533\n",
      "Iteration 503, loss = 0.11389906\n",
      "Iteration 504, loss = 0.11368242\n",
      "Iteration 505, loss = 0.11342413\n",
      "Iteration 506, loss = 0.11323664\n",
      "Iteration 507, loss = 0.11297026\n",
      "Iteration 508, loss = 0.11275362\n",
      "Iteration 509, loss = 0.11251532\n",
      "Iteration 510, loss = 0.11231248\n",
      "Iteration 511, loss = 0.11207286\n",
      "Iteration 512, loss = 0.11184486\n",
      "Iteration 513, loss = 0.11159268\n",
      "Iteration 514, loss = 0.11137625\n",
      "Iteration 515, loss = 0.11115686\n",
      "Iteration 516, loss = 0.11092824\n",
      "Iteration 517, loss = 0.11070930\n",
      "Iteration 518, loss = 0.11049238\n",
      "Iteration 519, loss = 0.11028953\n",
      "Iteration 520, loss = 0.11006162\n",
      "Iteration 521, loss = 0.10984730\n",
      "Iteration 522, loss = 0.10969445\n",
      "Iteration 523, loss = 0.10943240\n",
      "Iteration 524, loss = 0.10923490\n",
      "Iteration 525, loss = 0.10902211\n",
      "Iteration 526, loss = 0.10881659\n",
      "Iteration 527, loss = 0.10859200\n",
      "Iteration 528, loss = 0.10839097\n",
      "Iteration 529, loss = 0.10818068\n",
      "Iteration 530, loss = 0.10798835\n",
      "Iteration 531, loss = 0.10779058\n",
      "Iteration 532, loss = 0.10759033\n",
      "Iteration 533, loss = 0.10737887\n",
      "Iteration 534, loss = 0.10717780\n",
      "Iteration 535, loss = 0.10699027\n",
      "Iteration 536, loss = 0.10676814\n",
      "Iteration 537, loss = 0.10658313\n",
      "Iteration 538, loss = 0.10638704\n",
      "Iteration 539, loss = 0.10619456\n",
      "Iteration 540, loss = 0.10599840\n",
      "Iteration 541, loss = 0.10582942\n",
      "Iteration 542, loss = 0.10561152\n",
      "Iteration 543, loss = 0.10540021\n",
      "Iteration 544, loss = 0.10521464\n",
      "Iteration 545, loss = 0.10499992\n",
      "Iteration 546, loss = 0.10480386\n",
      "Iteration 547, loss = 0.10461230\n",
      "Iteration 548, loss = 0.10439476\n",
      "Iteration 549, loss = 0.10421976\n",
      "Iteration 550, loss = 0.10402962\n",
      "Iteration 551, loss = 0.10379304\n",
      "Iteration 552, loss = 0.10362590\n",
      "Iteration 553, loss = 0.10340369\n",
      "Iteration 554, loss = 0.10323726\n",
      "Iteration 555, loss = 0.10307757\n",
      "Iteration 556, loss = 0.10287067\n",
      "Iteration 557, loss = 0.10268481\n",
      "Iteration 558, loss = 0.10249972\n",
      "Iteration 559, loss = 0.10231972\n",
      "Iteration 560, loss = 0.10213094\n",
      "Iteration 561, loss = 0.10195941\n",
      "Iteration 562, loss = 0.10178202\n",
      "Iteration 563, loss = 0.10160349\n",
      "Iteration 564, loss = 0.10143742\n",
      "Iteration 565, loss = 0.10126478\n",
      "Iteration 566, loss = 0.10107820\n",
      "Iteration 567, loss = 0.10090840\n",
      "Iteration 568, loss = 0.10075991\n",
      "Iteration 569, loss = 0.10055469\n",
      "Iteration 570, loss = 0.10036267\n",
      "Iteration 571, loss = 0.10019336\n",
      "Iteration 572, loss = 0.10003590\n",
      "Iteration 573, loss = 0.09987764\n",
      "Iteration 574, loss = 0.09969985\n",
      "Iteration 575, loss = 0.09953381\n",
      "Iteration 576, loss = 0.09936190\n",
      "Iteration 577, loss = 0.09920516\n",
      "Iteration 578, loss = 0.09904000\n",
      "Iteration 579, loss = 0.09885346\n",
      "Iteration 580, loss = 0.09868044\n",
      "Iteration 581, loss = 0.09851164\n",
      "Iteration 582, loss = 0.09835057\n",
      "Iteration 583, loss = 0.09818497\n",
      "Iteration 584, loss = 0.09802082\n",
      "Iteration 585, loss = 0.09785678\n",
      "Iteration 586, loss = 0.09770739\n",
      "Iteration 587, loss = 0.09754500\n",
      "Iteration 588, loss = 0.09740546\n",
      "Iteration 589, loss = 0.09723689\n",
      "Iteration 590, loss = 0.09708258\n",
      "Iteration 591, loss = 0.09689688\n",
      "Iteration 592, loss = 0.09675669\n",
      "Iteration 593, loss = 0.09659319\n",
      "Iteration 594, loss = 0.09644831\n",
      "Iteration 595, loss = 0.09628182\n",
      "Iteration 596, loss = 0.09612009\n",
      "Iteration 597, loss = 0.09597691\n",
      "Iteration 598, loss = 0.09580810\n",
      "Iteration 599, loss = 0.09565854\n",
      "Iteration 600, loss = 0.09551747\n",
      "Iteration 601, loss = 0.09535260\n",
      "Iteration 602, loss = 0.09520619\n",
      "Iteration 603, loss = 0.09505308\n",
      "Iteration 604, loss = 0.09491308\n",
      "Iteration 605, loss = 0.09474136\n",
      "Iteration 606, loss = 0.09459514\n",
      "Iteration 607, loss = 0.09446547\n",
      "Iteration 608, loss = 0.09432417\n",
      "Iteration 609, loss = 0.09419213\n",
      "Iteration 610, loss = 0.09401101\n",
      "Iteration 611, loss = 0.09387177\n",
      "Iteration 612, loss = 0.09371955\n",
      "Iteration 613, loss = 0.09357748\n",
      "Iteration 614, loss = 0.09343875\n",
      "Iteration 615, loss = 0.09329358\n",
      "Iteration 616, loss = 0.09315710\n",
      "Iteration 617, loss = 0.09303076\n",
      "Iteration 618, loss = 0.09288712\n",
      "Iteration 619, loss = 0.09274209\n",
      "Iteration 620, loss = 0.09263260\n",
      "Iteration 621, loss = 0.09246833\n",
      "Iteration 622, loss = 0.09232129\n",
      "Iteration 623, loss = 0.09221036\n",
      "Iteration 624, loss = 0.09207078\n",
      "Iteration 625, loss = 0.09194021\n",
      "Iteration 626, loss = 0.09177362\n",
      "Iteration 627, loss = 0.09163042\n",
      "Iteration 628, loss = 0.09150211\n",
      "Iteration 629, loss = 0.09136751\n",
      "Iteration 630, loss = 0.09121577\n",
      "Iteration 631, loss = 0.09108257\n",
      "Iteration 632, loss = 0.09096674\n",
      "Iteration 633, loss = 0.09084086\n",
      "Iteration 634, loss = 0.09069486\n",
      "Iteration 635, loss = 0.09054721\n",
      "Iteration 636, loss = 0.09041716\n",
      "Iteration 637, loss = 0.09029084\n",
      "Iteration 638, loss = 0.09016289\n",
      "Iteration 639, loss = 0.09003747\n",
      "Iteration 640, loss = 0.08990506\n",
      "Iteration 641, loss = 0.08979383\n",
      "Iteration 642, loss = 0.08965764\n",
      "Iteration 643, loss = 0.08952881\n",
      "Iteration 644, loss = 0.08937993\n",
      "Iteration 645, loss = 0.08927823\n",
      "Iteration 646, loss = 0.08913443\n",
      "Iteration 647, loss = 0.08901359\n",
      "Iteration 648, loss = 0.08888698\n",
      "Iteration 649, loss = 0.08876696\n",
      "Iteration 650, loss = 0.08866069\n",
      "Iteration 651, loss = 0.08854015\n",
      "Iteration 652, loss = 0.08841686\n",
      "Iteration 653, loss = 0.08829154\n",
      "Iteration 654, loss = 0.08817176\n",
      "Iteration 655, loss = 0.08806256\n",
      "Iteration 656, loss = 0.08794281\n",
      "Iteration 657, loss = 0.08781186\n",
      "Iteration 658, loss = 0.08769885\n",
      "Iteration 659, loss = 0.08756502\n",
      "Iteration 660, loss = 0.08745552\n",
      "Iteration 661, loss = 0.08732989\n",
      "Iteration 662, loss = 0.08722893\n",
      "Iteration 663, loss = 0.08708258\n",
      "Iteration 664, loss = 0.08696353\n",
      "Iteration 665, loss = 0.08684961\n",
      "Iteration 666, loss = 0.08675951\n",
      "Iteration 667, loss = 0.08664289\n",
      "Iteration 668, loss = 0.08651172\n",
      "Iteration 669, loss = 0.08638848\n",
      "Iteration 670, loss = 0.08626212\n",
      "Iteration 671, loss = 0.08616844\n",
      "Iteration 672, loss = 0.08602535\n",
      "Iteration 673, loss = 0.08591553\n",
      "Iteration 674, loss = 0.08580719\n",
      "Iteration 675, loss = 0.08570088\n",
      "Iteration 676, loss = 0.08558913\n",
      "Iteration 677, loss = 0.08549199\n",
      "Iteration 678, loss = 0.08538450\n",
      "Iteration 679, loss = 0.08528228\n",
      "Iteration 680, loss = 0.08516546\n",
      "Iteration 681, loss = 0.08504688\n",
      "Iteration 682, loss = 0.08493346\n",
      "Iteration 683, loss = 0.08482110\n",
      "Iteration 684, loss = 0.08471343\n",
      "Iteration 685, loss = 0.08461269\n",
      "Iteration 686, loss = 0.08450079\n",
      "Iteration 687, loss = 0.08439558\n",
      "Iteration 688, loss = 0.08429149\n",
      "Iteration 689, loss = 0.08417732\n",
      "Iteration 690, loss = 0.08407261\n",
      "Iteration 691, loss = 0.08397450\n",
      "Iteration 692, loss = 0.08388603\n",
      "Iteration 693, loss = 0.08376191\n",
      "Iteration 694, loss = 0.08365786\n",
      "Iteration 695, loss = 0.08356715\n",
      "Iteration 696, loss = 0.08345777\n",
      "Iteration 697, loss = 0.08336246\n",
      "Iteration 698, loss = 0.08326220\n",
      "Iteration 699, loss = 0.08314994\n",
      "Iteration 700, loss = 0.08304479\n",
      "Iteration 701, loss = 0.08294459\n",
      "Iteration 702, loss = 0.08285120\n",
      "Iteration 703, loss = 0.08274778\n",
      "Iteration 704, loss = 0.08264527\n",
      "Iteration 705, loss = 0.08253587\n",
      "Iteration 706, loss = 0.08243601\n",
      "Iteration 707, loss = 0.08233477\n",
      "Iteration 708, loss = 0.08222502\n",
      "Iteration 709, loss = 0.08213539\n",
      "Iteration 710, loss = 0.08204185\n",
      "Iteration 711, loss = 0.08194944\n",
      "Iteration 712, loss = 0.08185828\n",
      "Iteration 713, loss = 0.08176899\n",
      "Iteration 714, loss = 0.08165921\n",
      "Iteration 715, loss = 0.08156721\n",
      "Iteration 716, loss = 0.08147075\n",
      "Iteration 717, loss = 0.08138050\n",
      "Iteration 718, loss = 0.08129464\n",
      "Iteration 719, loss = 0.08118547\n",
      "Iteration 720, loss = 0.08109456\n",
      "Iteration 721, loss = 0.08099763\n",
      "Iteration 722, loss = 0.08091508\n",
      "Iteration 723, loss = 0.08081204\n",
      "Iteration 724, loss = 0.08072386\n",
      "Iteration 725, loss = 0.08063020\n",
      "Iteration 726, loss = 0.08053714\n",
      "Iteration 727, loss = 0.08045158\n",
      "Iteration 728, loss = 0.08036618\n",
      "Iteration 729, loss = 0.08027242\n",
      "Iteration 730, loss = 0.08017686\n",
      "Iteration 731, loss = 0.08007943\n",
      "Iteration 732, loss = 0.07998658\n",
      "Iteration 733, loss = 0.07990452\n",
      "Iteration 734, loss = 0.07981991\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.36279286\n",
      "Iteration 2, loss = 2.34965967\n",
      "Iteration 3, loss = 2.33228596\n",
      "Iteration 4, loss = 2.31379939\n",
      "Iteration 5, loss = 2.29589758\n",
      "Iteration 6, loss = 2.27807432\n",
      "Iteration 7, loss = 2.26050414\n",
      "Iteration 8, loss = 2.24254795\n",
      "Iteration 9, loss = 2.22421293\n",
      "Iteration 10, loss = 2.20525848\n",
      "Iteration 11, loss = 2.18535001\n",
      "Iteration 12, loss = 2.16481572\n",
      "Iteration 13, loss = 2.14407824\n",
      "Iteration 14, loss = 2.12286606\n",
      "Iteration 15, loss = 2.10109273\n",
      "Iteration 16, loss = 2.07897065\n",
      "Iteration 17, loss = 2.05591120\n",
      "Iteration 18, loss = 2.03218756\n",
      "Iteration 19, loss = 2.00796350\n",
      "Iteration 20, loss = 1.98307570\n",
      "Iteration 21, loss = 1.95808445\n",
      "Iteration 22, loss = 1.93289603\n",
      "Iteration 23, loss = 1.90762537\n",
      "Iteration 24, loss = 1.88169479\n",
      "Iteration 25, loss = 1.85558746\n",
      "Iteration 26, loss = 1.82894461\n",
      "Iteration 27, loss = 1.80202229\n",
      "Iteration 28, loss = 1.77506831\n",
      "Iteration 29, loss = 1.74778184\n",
      "Iteration 30, loss = 1.72093555\n",
      "Iteration 31, loss = 1.69386773\n",
      "Iteration 32, loss = 1.66665629\n",
      "Iteration 33, loss = 1.63981437\n",
      "Iteration 34, loss = 1.61278752\n",
      "Iteration 35, loss = 1.58601092\n",
      "Iteration 36, loss = 1.55925945\n",
      "Iteration 37, loss = 1.53260204\n",
      "Iteration 38, loss = 1.50631626\n",
      "Iteration 39, loss = 1.48010408\n",
      "Iteration 40, loss = 1.45417482\n",
      "Iteration 41, loss = 1.42852858\n",
      "Iteration 42, loss = 1.40354675\n",
      "Iteration 43, loss = 1.37879477\n",
      "Iteration 44, loss = 1.35475479\n",
      "Iteration 45, loss = 1.33088205\n",
      "Iteration 46, loss = 1.30763275\n",
      "Iteration 47, loss = 1.28495395\n",
      "Iteration 48, loss = 1.26238998\n",
      "Iteration 49, loss = 1.24064754\n",
      "Iteration 50, loss = 1.21932569\n",
      "Iteration 51, loss = 1.19868488\n",
      "Iteration 52, loss = 1.17827434\n",
      "Iteration 53, loss = 1.15838501\n",
      "Iteration 54, loss = 1.13904914\n",
      "Iteration 55, loss = 1.12037917\n",
      "Iteration 56, loss = 1.10181999\n",
      "Iteration 57, loss = 1.08377862\n",
      "Iteration 58, loss = 1.06638086\n",
      "Iteration 59, loss = 1.04924652\n",
      "Iteration 60, loss = 1.03240916\n",
      "Iteration 61, loss = 1.01621664\n",
      "Iteration 62, loss = 1.00010720\n",
      "Iteration 63, loss = 0.98463166\n",
      "Iteration 64, loss = 0.96955403\n",
      "Iteration 65, loss = 0.95482815\n",
      "Iteration 66, loss = 0.94042094\n",
      "Iteration 67, loss = 0.92655182\n",
      "Iteration 68, loss = 0.91278809\n",
      "Iteration 69, loss = 0.89959542\n",
      "Iteration 70, loss = 0.88656015\n",
      "Iteration 71, loss = 0.87390236\n",
      "Iteration 72, loss = 0.86132308\n",
      "Iteration 73, loss = 0.84927169\n",
      "Iteration 74, loss = 0.83748662\n",
      "Iteration 75, loss = 0.82623716\n",
      "Iteration 76, loss = 0.81485156\n",
      "Iteration 77, loss = 0.80387913\n",
      "Iteration 78, loss = 0.79319814\n",
      "Iteration 79, loss = 0.78270894\n",
      "Iteration 80, loss = 0.77237891\n",
      "Iteration 81, loss = 0.76243180\n",
      "Iteration 82, loss = 0.75258534\n",
      "Iteration 83, loss = 0.74307037\n",
      "Iteration 84, loss = 0.73384981\n",
      "Iteration 85, loss = 0.72459763\n",
      "Iteration 86, loss = 0.71568868\n",
      "Iteration 87, loss = 0.70692441\n",
      "Iteration 88, loss = 0.69828498\n",
      "Iteration 89, loss = 0.68973906\n",
      "Iteration 90, loss = 0.68167228\n",
      "Iteration 91, loss = 0.67363948\n",
      "Iteration 92, loss = 0.66582273\n",
      "Iteration 93, loss = 0.65803037\n",
      "Iteration 94, loss = 0.65065256\n",
      "Iteration 95, loss = 0.64308820\n",
      "Iteration 96, loss = 0.63599423\n",
      "Iteration 97, loss = 0.62898752\n",
      "Iteration 98, loss = 0.62203236\n",
      "Iteration 99, loss = 0.61524425\n",
      "Iteration 100, loss = 0.60856331\n",
      "Iteration 101, loss = 0.60195918\n",
      "Iteration 102, loss = 0.59546610\n",
      "Iteration 103, loss = 0.58910741\n",
      "Iteration 104, loss = 0.58302347\n",
      "Iteration 105, loss = 0.57698130\n",
      "Iteration 106, loss = 0.57104037\n",
      "Iteration 107, loss = 0.56512892\n",
      "Iteration 108, loss = 0.55952928\n",
      "Iteration 109, loss = 0.55384627\n",
      "Iteration 110, loss = 0.54840453\n",
      "Iteration 111, loss = 0.54280967\n",
      "Iteration 112, loss = 0.53751091\n",
      "Iteration 113, loss = 0.53235553\n",
      "Iteration 114, loss = 0.52728312\n",
      "Iteration 115, loss = 0.52226102\n",
      "Iteration 116, loss = 0.51736172\n",
      "Iteration 117, loss = 0.51242555\n",
      "Iteration 118, loss = 0.50773928\n",
      "Iteration 119, loss = 0.50309876\n",
      "Iteration 120, loss = 0.49847342\n",
      "Iteration 121, loss = 0.49393932\n",
      "Iteration 122, loss = 0.48935643\n",
      "Iteration 123, loss = 0.48490903\n",
      "Iteration 124, loss = 0.48060434\n",
      "Iteration 125, loss = 0.47640242\n",
      "Iteration 126, loss = 0.47231334\n",
      "Iteration 127, loss = 0.46814122\n",
      "Iteration 128, loss = 0.46396525\n",
      "Iteration 129, loss = 0.46006959\n",
      "Iteration 130, loss = 0.45617046\n",
      "Iteration 131, loss = 0.45231201\n",
      "Iteration 132, loss = 0.44845271\n",
      "Iteration 133, loss = 0.44473266\n",
      "Iteration 134, loss = 0.44110511\n",
      "Iteration 135, loss = 0.43760911\n",
      "Iteration 136, loss = 0.43409400\n",
      "Iteration 137, loss = 0.43049578\n",
      "Iteration 138, loss = 0.42706539\n",
      "Iteration 139, loss = 0.42375221\n",
      "Iteration 140, loss = 0.42045015\n",
      "Iteration 141, loss = 0.41719185\n",
      "Iteration 142, loss = 0.41392894\n",
      "Iteration 143, loss = 0.41059724\n",
      "Iteration 144, loss = 0.40746515\n",
      "Iteration 145, loss = 0.40434962\n",
      "Iteration 146, loss = 0.40125468\n",
      "Iteration 147, loss = 0.39819563\n",
      "Iteration 148, loss = 0.39524340\n",
      "Iteration 149, loss = 0.39237687\n",
      "Iteration 150, loss = 0.38932557\n",
      "Iteration 151, loss = 0.38650113\n",
      "Iteration 152, loss = 0.38366651\n",
      "Iteration 153, loss = 0.38082205\n",
      "Iteration 154, loss = 0.37808175\n",
      "Iteration 155, loss = 0.37544365\n",
      "Iteration 156, loss = 0.37282340\n",
      "Iteration 157, loss = 0.37009484\n",
      "Iteration 158, loss = 0.36743714\n",
      "Iteration 159, loss = 0.36483474\n",
      "Iteration 160, loss = 0.36238236\n",
      "Iteration 161, loss = 0.35988041\n",
      "Iteration 162, loss = 0.35730786\n",
      "Iteration 163, loss = 0.35483291\n",
      "Iteration 164, loss = 0.35245772\n",
      "Iteration 165, loss = 0.35017836\n",
      "Iteration 166, loss = 0.34782701\n",
      "Iteration 167, loss = 0.34541919\n",
      "Iteration 168, loss = 0.34310599\n",
      "Iteration 169, loss = 0.34093871\n",
      "Iteration 170, loss = 0.33867448\n",
      "Iteration 171, loss = 0.33645026\n",
      "Iteration 172, loss = 0.33430401\n",
      "Iteration 173, loss = 0.33212313\n",
      "Iteration 174, loss = 0.32992777\n",
      "Iteration 175, loss = 0.32784632\n",
      "Iteration 176, loss = 0.32580985\n",
      "Iteration 177, loss = 0.32368830\n",
      "Iteration 178, loss = 0.32158101\n",
      "Iteration 179, loss = 0.31964603\n",
      "Iteration 180, loss = 0.31769270\n",
      "Iteration 181, loss = 0.31562516\n",
      "Iteration 182, loss = 0.31363056\n",
      "Iteration 183, loss = 0.31173601\n",
      "Iteration 184, loss = 0.30974786\n",
      "Iteration 185, loss = 0.30781526\n",
      "Iteration 186, loss = 0.30592416\n",
      "Iteration 187, loss = 0.30404739\n",
      "Iteration 188, loss = 0.30224820\n",
      "Iteration 189, loss = 0.30056068\n",
      "Iteration 190, loss = 0.29876037\n",
      "Iteration 191, loss = 0.29690604\n",
      "Iteration 192, loss = 0.29513340\n",
      "Iteration 193, loss = 0.29341841\n",
      "Iteration 194, loss = 0.29163091\n",
      "Iteration 195, loss = 0.28994211\n",
      "Iteration 196, loss = 0.28834221\n",
      "Iteration 197, loss = 0.28663057\n",
      "Iteration 198, loss = 0.28498317\n",
      "Iteration 199, loss = 0.28334922\n",
      "Iteration 200, loss = 0.28172501\n",
      "Iteration 201, loss = 0.28017819\n",
      "Iteration 202, loss = 0.27866108\n",
      "Iteration 203, loss = 0.27707705\n",
      "Iteration 204, loss = 0.27547623\n",
      "Iteration 205, loss = 0.27392551\n",
      "Iteration 206, loss = 0.27244933\n",
      "Iteration 207, loss = 0.27105450\n",
      "Iteration 208, loss = 0.26956031\n",
      "Iteration 209, loss = 0.26812403\n",
      "Iteration 210, loss = 0.26666217\n",
      "Iteration 211, loss = 0.26526993\n",
      "Iteration 212, loss = 0.26384939\n",
      "Iteration 213, loss = 0.26247325\n",
      "Iteration 214, loss = 0.26110039\n",
      "Iteration 215, loss = 0.25974229\n",
      "Iteration 216, loss = 0.25843643\n",
      "Iteration 217, loss = 0.25701087\n",
      "Iteration 218, loss = 0.25570556\n",
      "Iteration 219, loss = 0.25433132\n",
      "Iteration 220, loss = 0.25306425\n",
      "Iteration 221, loss = 0.25179835\n",
      "Iteration 222, loss = 0.25053576\n",
      "Iteration 223, loss = 0.24930474\n",
      "Iteration 224, loss = 0.24809149\n",
      "Iteration 225, loss = 0.24687627\n",
      "Iteration 226, loss = 0.24557277\n",
      "Iteration 227, loss = 0.24433496\n",
      "Iteration 228, loss = 0.24309590\n",
      "Iteration 229, loss = 0.24184926\n",
      "Iteration 230, loss = 0.24066142\n",
      "Iteration 231, loss = 0.23950957\n",
      "Iteration 232, loss = 0.23831375\n",
      "Iteration 233, loss = 0.23718376\n",
      "Iteration 234, loss = 0.23598575\n",
      "Iteration 235, loss = 0.23487697\n",
      "Iteration 236, loss = 0.23375538\n",
      "Iteration 237, loss = 0.23265229\n",
      "Iteration 238, loss = 0.23150682\n",
      "Iteration 239, loss = 0.23044995\n",
      "Iteration 240, loss = 0.22935095\n",
      "Iteration 241, loss = 0.22833275\n",
      "Iteration 242, loss = 0.22727710\n",
      "Iteration 243, loss = 0.22618066\n",
      "Iteration 244, loss = 0.22515820\n",
      "Iteration 245, loss = 0.22416669\n",
      "Iteration 246, loss = 0.22317236\n",
      "Iteration 247, loss = 0.22208737\n",
      "Iteration 248, loss = 0.22107709\n",
      "Iteration 249, loss = 0.22001660\n",
      "Iteration 250, loss = 0.21898989\n",
      "Iteration 251, loss = 0.21798097\n",
      "Iteration 252, loss = 0.21710520\n",
      "Iteration 253, loss = 0.21605761\n",
      "Iteration 254, loss = 0.21506315\n",
      "Iteration 255, loss = 0.21409623\n",
      "Iteration 256, loss = 0.21317528\n",
      "Iteration 257, loss = 0.21223239\n",
      "Iteration 258, loss = 0.21133015\n",
      "Iteration 259, loss = 0.21037136\n",
      "Iteration 260, loss = 0.20944951\n",
      "Iteration 261, loss = 0.20860471\n",
      "Iteration 262, loss = 0.20770952\n",
      "Iteration 263, loss = 0.20682589\n",
      "Iteration 264, loss = 0.20589983\n",
      "Iteration 265, loss = 0.20500821\n",
      "Iteration 266, loss = 0.20410704\n",
      "Iteration 267, loss = 0.20331351\n",
      "Iteration 268, loss = 0.20241302\n",
      "Iteration 269, loss = 0.20153585\n",
      "Iteration 270, loss = 0.20070761\n",
      "Iteration 271, loss = 0.19986531\n",
      "Iteration 272, loss = 0.19903877\n",
      "Iteration 273, loss = 0.19821415\n",
      "Iteration 274, loss = 0.19735691\n",
      "Iteration 275, loss = 0.19659224\n",
      "Iteration 276, loss = 0.19579107\n",
      "Iteration 277, loss = 0.19492637\n",
      "Iteration 278, loss = 0.19412241\n",
      "Iteration 279, loss = 0.19332649\n",
      "Iteration 280, loss = 0.19253016\n",
      "Iteration 281, loss = 0.19176714\n",
      "Iteration 282, loss = 0.19097226\n",
      "Iteration 283, loss = 0.19019637\n",
      "Iteration 284, loss = 0.18940391\n",
      "Iteration 285, loss = 0.18870703\n",
      "Iteration 286, loss = 0.18791117\n",
      "Iteration 287, loss = 0.18711625\n",
      "Iteration 288, loss = 0.18639655\n",
      "Iteration 289, loss = 0.18562293\n",
      "Iteration 290, loss = 0.18489095\n",
      "Iteration 291, loss = 0.18413688\n",
      "Iteration 292, loss = 0.18346710\n",
      "Iteration 293, loss = 0.18278828\n",
      "Iteration 294, loss = 0.18215178\n",
      "Iteration 295, loss = 0.18144321\n",
      "Iteration 296, loss = 0.18069059\n",
      "Iteration 297, loss = 0.18001603\n",
      "Iteration 298, loss = 0.17933111\n",
      "Iteration 299, loss = 0.17861359\n",
      "Iteration 300, loss = 0.17796233\n",
      "Iteration 301, loss = 0.17725919\n",
      "Iteration 302, loss = 0.17662149\n",
      "Iteration 303, loss = 0.17597431\n",
      "Iteration 304, loss = 0.17536494\n",
      "Iteration 305, loss = 0.17472810\n",
      "Iteration 306, loss = 0.17402759\n",
      "Iteration 307, loss = 0.17335536\n",
      "Iteration 308, loss = 0.17266487\n",
      "Iteration 309, loss = 0.17206083\n",
      "Iteration 310, loss = 0.17137237\n",
      "Iteration 311, loss = 0.17077884\n",
      "Iteration 312, loss = 0.17015782\n",
      "Iteration 313, loss = 0.16956031\n",
      "Iteration 314, loss = 0.16891463\n",
      "Iteration 315, loss = 0.16835958\n",
      "Iteration 316, loss = 0.16774445\n",
      "Iteration 317, loss = 0.16722985\n",
      "Iteration 318, loss = 0.16658524\n",
      "Iteration 319, loss = 0.16598043\n",
      "Iteration 320, loss = 0.16537754\n",
      "Iteration 321, loss = 0.16480283\n",
      "Iteration 322, loss = 0.16430676\n",
      "Iteration 323, loss = 0.16371099\n",
      "Iteration 324, loss = 0.16314994\n",
      "Iteration 325, loss = 0.16254481\n",
      "Iteration 326, loss = 0.16199711\n",
      "Iteration 327, loss = 0.16145803\n",
      "Iteration 328, loss = 0.16088060\n",
      "Iteration 329, loss = 0.16037005\n",
      "Iteration 330, loss = 0.15981761\n",
      "Iteration 331, loss = 0.15928634\n",
      "Iteration 332, loss = 0.15875902\n",
      "Iteration 333, loss = 0.15822598\n",
      "Iteration 334, loss = 0.15768760\n",
      "Iteration 335, loss = 0.15716867\n",
      "Iteration 336, loss = 0.15661079\n",
      "Iteration 337, loss = 0.15608281\n",
      "Iteration 338, loss = 0.15553211\n",
      "Iteration 339, loss = 0.15507406\n",
      "Iteration 340, loss = 0.15452198\n",
      "Iteration 341, loss = 0.15399560\n",
      "Iteration 342, loss = 0.15347361\n",
      "Iteration 343, loss = 0.15301337\n",
      "Iteration 344, loss = 0.15249068\n",
      "Iteration 345, loss = 0.15201623\n",
      "Iteration 346, loss = 0.15154236\n",
      "Iteration 347, loss = 0.15104601\n",
      "Iteration 348, loss = 0.15058290\n",
      "Iteration 349, loss = 0.15012961\n",
      "Iteration 350, loss = 0.14961246\n",
      "Iteration 351, loss = 0.14913463\n",
      "Iteration 352, loss = 0.14865204\n",
      "Iteration 353, loss = 0.14818061\n",
      "Iteration 354, loss = 0.14775326\n",
      "Iteration 355, loss = 0.14725853\n",
      "Iteration 356, loss = 0.14683927\n",
      "Iteration 357, loss = 0.14637234\n",
      "Iteration 358, loss = 0.14590841\n",
      "Iteration 359, loss = 0.14544853\n",
      "Iteration 360, loss = 0.14503367\n",
      "Iteration 361, loss = 0.14456528\n",
      "Iteration 362, loss = 0.14416167\n",
      "Iteration 363, loss = 0.14368322\n",
      "Iteration 364, loss = 0.14326363\n",
      "Iteration 365, loss = 0.14281558\n",
      "Iteration 366, loss = 0.14240017\n",
      "Iteration 367, loss = 0.14199442\n",
      "Iteration 368, loss = 0.14154942\n",
      "Iteration 369, loss = 0.14110786\n",
      "Iteration 370, loss = 0.14069318\n",
      "Iteration 371, loss = 0.14027974\n",
      "Iteration 372, loss = 0.13985901\n",
      "Iteration 373, loss = 0.13947932\n",
      "Iteration 374, loss = 0.13905587\n",
      "Iteration 375, loss = 0.13863885\n",
      "Iteration 376, loss = 0.13823256\n",
      "Iteration 377, loss = 0.13780853\n",
      "Iteration 378, loss = 0.13741430\n",
      "Iteration 379, loss = 0.13700855\n",
      "Iteration 380, loss = 0.13664706\n",
      "Iteration 381, loss = 0.13624613\n",
      "Iteration 382, loss = 0.13583877\n",
      "Iteration 383, loss = 0.13546402\n",
      "Iteration 384, loss = 0.13506643\n",
      "Iteration 385, loss = 0.13464567\n",
      "Iteration 386, loss = 0.13425032\n",
      "Iteration 387, loss = 0.13389084\n",
      "Iteration 388, loss = 0.13348867\n",
      "Iteration 389, loss = 0.13311333\n",
      "Iteration 390, loss = 0.13272442\n",
      "Iteration 391, loss = 0.13235556\n",
      "Iteration 392, loss = 0.13199902\n",
      "Iteration 393, loss = 0.13165835\n",
      "Iteration 394, loss = 0.13126697\n",
      "Iteration 395, loss = 0.13088818\n",
      "Iteration 396, loss = 0.13054146\n",
      "Iteration 397, loss = 0.13018817\n",
      "Iteration 398, loss = 0.12985019\n",
      "Iteration 399, loss = 0.12947578\n",
      "Iteration 400, loss = 0.12916472\n",
      "Iteration 401, loss = 0.12880423\n",
      "Iteration 402, loss = 0.12846048\n",
      "Iteration 403, loss = 0.12810578\n",
      "Iteration 404, loss = 0.12778213\n",
      "Iteration 405, loss = 0.12740564\n",
      "Iteration 406, loss = 0.12706026\n",
      "Iteration 407, loss = 0.12673352\n",
      "Iteration 408, loss = 0.12640259\n",
      "Iteration 409, loss = 0.12608907\n",
      "Iteration 410, loss = 0.12573284\n",
      "Iteration 411, loss = 0.12545291\n",
      "Iteration 412, loss = 0.12509821\n",
      "Iteration 413, loss = 0.12479078\n",
      "Iteration 414, loss = 0.12443414\n",
      "Iteration 415, loss = 0.12413513\n",
      "Iteration 416, loss = 0.12379150\n",
      "Iteration 417, loss = 0.12346781\n",
      "Iteration 418, loss = 0.12315626\n",
      "Iteration 419, loss = 0.12283125\n",
      "Iteration 420, loss = 0.12253640\n",
      "Iteration 421, loss = 0.12221341\n",
      "Iteration 422, loss = 0.12190894\n",
      "Iteration 423, loss = 0.12161113\n",
      "Iteration 424, loss = 0.12128315\n",
      "Iteration 425, loss = 0.12099383\n",
      "Iteration 426, loss = 0.12068010\n",
      "Iteration 427, loss = 0.12040438\n",
      "Iteration 428, loss = 0.12010710\n",
      "Iteration 429, loss = 0.11982456\n",
      "Iteration 430, loss = 0.11950724\n",
      "Iteration 431, loss = 0.11922199\n",
      "Iteration 432, loss = 0.11893650\n",
      "Iteration 433, loss = 0.11860860\n",
      "Iteration 434, loss = 0.11838268\n",
      "Iteration 435, loss = 0.11808929\n",
      "Iteration 436, loss = 0.11777220\n",
      "Iteration 437, loss = 0.11749513\n",
      "Iteration 438, loss = 0.11719566\n",
      "Iteration 439, loss = 0.11687889\n",
      "Iteration 440, loss = 0.11663363\n",
      "Iteration 441, loss = 0.11635111\n",
      "Iteration 442, loss = 0.11607500\n",
      "Iteration 443, loss = 0.11577514\n",
      "Iteration 444, loss = 0.11547969\n",
      "Iteration 445, loss = 0.11523101\n",
      "Iteration 446, loss = 0.11496936\n",
      "Iteration 447, loss = 0.11470282\n",
      "Iteration 448, loss = 0.11441482\n",
      "Iteration 449, loss = 0.11415106\n",
      "Iteration 450, loss = 0.11390988\n",
      "Iteration 451, loss = 0.11362185\n",
      "Iteration 452, loss = 0.11339319\n",
      "Iteration 453, loss = 0.11310771\n",
      "Iteration 454, loss = 0.11285190\n",
      "Iteration 455, loss = 0.11258384\n",
      "Iteration 456, loss = 0.11229754\n",
      "Iteration 457, loss = 0.11205666\n",
      "Iteration 458, loss = 0.11180250\n",
      "Iteration 459, loss = 0.11153315\n",
      "Iteration 460, loss = 0.11128446\n",
      "Iteration 461, loss = 0.11104719\n",
      "Iteration 462, loss = 0.11078565\n",
      "Iteration 463, loss = 0.11051916\n",
      "Iteration 464, loss = 0.11027095\n",
      "Iteration 465, loss = 0.11003161\n",
      "Iteration 466, loss = 0.10977482\n",
      "Iteration 467, loss = 0.10955282\n",
      "Iteration 468, loss = 0.10932546\n",
      "Iteration 469, loss = 0.10907476\n",
      "Iteration 470, loss = 0.10882742\n",
      "Iteration 471, loss = 0.10858242\n",
      "Iteration 472, loss = 0.10836762\n",
      "Iteration 473, loss = 0.10812939\n",
      "Iteration 474, loss = 0.10787377\n",
      "Iteration 475, loss = 0.10764069\n",
      "Iteration 476, loss = 0.10741841\n",
      "Iteration 477, loss = 0.10719158\n",
      "Iteration 478, loss = 0.10696974\n",
      "Iteration 479, loss = 0.10675077\n",
      "Iteration 480, loss = 0.10651959\n",
      "Iteration 481, loss = 0.10629121\n",
      "Iteration 482, loss = 0.10607855\n",
      "Iteration 483, loss = 0.10585823\n",
      "Iteration 484, loss = 0.10563430\n",
      "Iteration 485, loss = 0.10540568\n",
      "Iteration 486, loss = 0.10520278\n",
      "Iteration 487, loss = 0.10499575\n",
      "Iteration 488, loss = 0.10476720\n",
      "Iteration 489, loss = 0.10454915\n",
      "Iteration 490, loss = 0.10430972\n",
      "Iteration 491, loss = 0.10411069\n",
      "Iteration 492, loss = 0.10387202\n",
      "Iteration 493, loss = 0.10366063\n",
      "Iteration 494, loss = 0.10344144\n",
      "Iteration 495, loss = 0.10321527\n",
      "Iteration 496, loss = 0.10305313\n",
      "Iteration 497, loss = 0.10281626\n",
      "Iteration 498, loss = 0.10259871\n",
      "Iteration 499, loss = 0.10236380\n",
      "Iteration 500, loss = 0.10214849\n",
      "Iteration 501, loss = 0.10193263\n",
      "Iteration 502, loss = 0.10172244\n",
      "Iteration 503, loss = 0.10151815\n",
      "Iteration 504, loss = 0.10133664\n",
      "Iteration 505, loss = 0.10111963\n",
      "Iteration 506, loss = 0.10093689\n",
      "Iteration 507, loss = 0.10073299\n",
      "Iteration 508, loss = 0.10053648\n",
      "Iteration 509, loss = 0.10032721\n",
      "Iteration 510, loss = 0.10015106\n",
      "Iteration 511, loss = 0.09994046\n",
      "Iteration 512, loss = 0.09975736\n",
      "Iteration 513, loss = 0.09956816\n",
      "Iteration 514, loss = 0.09938063\n",
      "Iteration 515, loss = 0.09918166\n",
      "Iteration 516, loss = 0.09898684\n",
      "Iteration 517, loss = 0.09879790\n",
      "Iteration 518, loss = 0.09861144\n",
      "Iteration 519, loss = 0.09842358\n",
      "Iteration 520, loss = 0.09824064\n",
      "Iteration 521, loss = 0.09806321\n",
      "Iteration 522, loss = 0.09789101\n",
      "Iteration 523, loss = 0.09769387\n",
      "Iteration 524, loss = 0.09750843\n",
      "Iteration 525, loss = 0.09733520\n",
      "Iteration 526, loss = 0.09717389\n",
      "Iteration 527, loss = 0.09698100\n",
      "Iteration 528, loss = 0.09681123\n",
      "Iteration 529, loss = 0.09663859\n",
      "Iteration 530, loss = 0.09645788\n",
      "Iteration 531, loss = 0.09629700\n",
      "Iteration 532, loss = 0.09610870\n",
      "Iteration 533, loss = 0.09594102\n",
      "Iteration 534, loss = 0.09577063\n",
      "Iteration 535, loss = 0.09559585\n",
      "Iteration 536, loss = 0.09542521\n",
      "Iteration 537, loss = 0.09526723\n",
      "Iteration 538, loss = 0.09510778\n",
      "Iteration 539, loss = 0.09493051\n",
      "Iteration 540, loss = 0.09476693\n",
      "Iteration 541, loss = 0.09462178\n",
      "Iteration 542, loss = 0.09443043\n",
      "Iteration 543, loss = 0.09424664\n",
      "Iteration 544, loss = 0.09407906\n",
      "Iteration 545, loss = 0.09391908\n",
      "Iteration 546, loss = 0.09374582\n",
      "Iteration 547, loss = 0.09360092\n",
      "Iteration 548, loss = 0.09342365\n",
      "Iteration 549, loss = 0.09326753\n",
      "Iteration 550, loss = 0.09310808\n",
      "Iteration 551, loss = 0.09294980\n",
      "Iteration 552, loss = 0.09280123\n",
      "Iteration 553, loss = 0.09260629\n",
      "Iteration 554, loss = 0.09244727\n",
      "Iteration 555, loss = 0.09230730\n",
      "Iteration 556, loss = 0.09215921\n",
      "Iteration 557, loss = 0.09199982\n",
      "Iteration 558, loss = 0.09184656\n",
      "Iteration 559, loss = 0.09169498\n",
      "Iteration 560, loss = 0.09154103\n",
      "Iteration 561, loss = 0.09141130\n",
      "Iteration 562, loss = 0.09124797\n",
      "Iteration 563, loss = 0.09109937\n",
      "Iteration 564, loss = 0.09095869\n",
      "Iteration 565, loss = 0.09080405\n",
      "Iteration 566, loss = 0.09065539\n",
      "Iteration 567, loss = 0.09051287\n",
      "Iteration 568, loss = 0.09038069\n",
      "Iteration 569, loss = 0.09021415\n",
      "Iteration 570, loss = 0.09006075\n",
      "Iteration 571, loss = 0.08992934\n",
      "Iteration 572, loss = 0.08978646\n",
      "Iteration 573, loss = 0.08965271\n",
      "Iteration 574, loss = 0.08951830\n",
      "Iteration 575, loss = 0.08938779\n",
      "Iteration 576, loss = 0.08923982\n",
      "Iteration 577, loss = 0.08909547\n",
      "Iteration 578, loss = 0.08894892\n",
      "Iteration 579, loss = 0.08879806\n",
      "Iteration 580, loss = 0.08865091\n",
      "Iteration 581, loss = 0.08851181\n",
      "Iteration 582, loss = 0.08837513\n",
      "Iteration 583, loss = 0.08823246\n",
      "Iteration 584, loss = 0.08810098\n",
      "Iteration 585, loss = 0.08795053\n",
      "Iteration 586, loss = 0.08782484\n",
      "Iteration 587, loss = 0.08770007\n",
      "Iteration 588, loss = 0.08756203\n",
      "Iteration 589, loss = 0.08742140\n",
      "Iteration 590, loss = 0.08728594\n",
      "Iteration 591, loss = 0.08714512\n",
      "Iteration 592, loss = 0.08703143\n",
      "Iteration 593, loss = 0.08688815\n",
      "Iteration 594, loss = 0.08676511\n",
      "Iteration 595, loss = 0.08661803\n",
      "Iteration 596, loss = 0.08648855\n",
      "Iteration 597, loss = 0.08636521\n",
      "Iteration 598, loss = 0.08622212\n",
      "Iteration 599, loss = 0.08610403\n",
      "Iteration 600, loss = 0.08596811\n",
      "Iteration 601, loss = 0.08583466\n",
      "Iteration 602, loss = 0.08570216\n",
      "Iteration 603, loss = 0.08557417\n",
      "Iteration 604, loss = 0.08545933\n",
      "Iteration 605, loss = 0.08531308\n",
      "Iteration 606, loss = 0.08519950\n",
      "Iteration 607, loss = 0.08508223\n",
      "Iteration 608, loss = 0.08495315\n",
      "Iteration 609, loss = 0.08482351\n",
      "Iteration 610, loss = 0.08467455\n",
      "Iteration 611, loss = 0.08455516\n",
      "Iteration 612, loss = 0.08443614\n",
      "Iteration 613, loss = 0.08430849\n",
      "Iteration 614, loss = 0.08420421\n",
      "Iteration 615, loss = 0.08408585\n",
      "Iteration 616, loss = 0.08396500\n",
      "Iteration 617, loss = 0.08384379\n",
      "Iteration 618, loss = 0.08373666\n",
      "Iteration 619, loss = 0.08361242\n",
      "Iteration 620, loss = 0.08350402\n",
      "Iteration 621, loss = 0.08337179\n",
      "Iteration 622, loss = 0.08326077\n",
      "Iteration 623, loss = 0.08316179\n",
      "Iteration 624, loss = 0.08304509\n",
      "Iteration 625, loss = 0.08293818\n",
      "Iteration 626, loss = 0.08281283\n",
      "Iteration 627, loss = 0.08270341\n",
      "Iteration 628, loss = 0.08259616\n",
      "Iteration 629, loss = 0.08247896\n",
      "Iteration 630, loss = 0.08235668\n",
      "Iteration 631, loss = 0.08224640\n",
      "Iteration 632, loss = 0.08213814\n",
      "Iteration 633, loss = 0.08202671\n",
      "Iteration 634, loss = 0.08190230\n",
      "Iteration 635, loss = 0.08179765\n",
      "Iteration 636, loss = 0.08169840\n",
      "Iteration 637, loss = 0.08158189\n",
      "Iteration 638, loss = 0.08147227\n",
      "Iteration 639, loss = 0.08136185\n",
      "Iteration 640, loss = 0.08126001\n",
      "Iteration 641, loss = 0.08114670\n",
      "Iteration 642, loss = 0.08104628\n",
      "Iteration 643, loss = 0.08093537\n",
      "Iteration 644, loss = 0.08083350\n",
      "Iteration 645, loss = 0.08072346\n",
      "Iteration 646, loss = 0.08062585\n",
      "Iteration 647, loss = 0.08052519\n",
      "Iteration 648, loss = 0.08043098\n",
      "Iteration 649, loss = 0.08033044\n",
      "Iteration 650, loss = 0.08022602\n",
      "Iteration 651, loss = 0.08012201\n",
      "Iteration 652, loss = 0.08002216\n",
      "Iteration 653, loss = 0.07991577\n",
      "Iteration 654, loss = 0.07981656\n",
      "Iteration 655, loss = 0.07971950\n",
      "Iteration 656, loss = 0.07962325\n",
      "Iteration 657, loss = 0.07952405\n",
      "Iteration 658, loss = 0.07942941\n",
      "Iteration 659, loss = 0.07933268\n",
      "Iteration 660, loss = 0.07923738\n",
      "Iteration 661, loss = 0.07914593\n",
      "Iteration 662, loss = 0.07904210\n",
      "Iteration 663, loss = 0.07893748\n",
      "Iteration 664, loss = 0.07883533\n",
      "Iteration 665, loss = 0.07873910\n",
      "Iteration 666, loss = 0.07864703\n",
      "Iteration 667, loss = 0.07855778\n",
      "Iteration 668, loss = 0.07845098\n",
      "Iteration 669, loss = 0.07834029\n",
      "Iteration 670, loss = 0.07824543\n",
      "Iteration 671, loss = 0.07816416\n",
      "Iteration 672, loss = 0.07804646\n",
      "Iteration 673, loss = 0.07796880\n",
      "Iteration 674, loss = 0.07787331\n",
      "Iteration 675, loss = 0.07777283\n",
      "Iteration 676, loss = 0.07769124\n",
      "Iteration 677, loss = 0.07760776\n",
      "Iteration 678, loss = 0.07751419\n",
      "Iteration 679, loss = 0.07742984\n",
      "Iteration 680, loss = 0.07734056\n",
      "Iteration 681, loss = 0.07722979\n",
      "Iteration 682, loss = 0.07714601\n",
      "Iteration 683, loss = 0.07704925\n",
      "Iteration 684, loss = 0.07695751\n",
      "Iteration 685, loss = 0.07687641\n",
      "Iteration 686, loss = 0.07678644\n",
      "Iteration 687, loss = 0.07669864\n",
      "Iteration 688, loss = 0.07660993\n",
      "Iteration 689, loss = 0.07652146\n",
      "Iteration 690, loss = 0.07643032\n",
      "Iteration 691, loss = 0.07634882\n",
      "Iteration 692, loss = 0.07625440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.29122821\n",
      "Iteration 2, loss = 1.91944596\n",
      "Iteration 3, loss = 1.52292996\n",
      "Iteration 4, loss = 1.15574232\n",
      "Iteration 5, loss = 0.79522140\n",
      "Iteration 6, loss = 0.56947466\n",
      "Iteration 7, loss = 0.42239643\n",
      "Iteration 8, loss = 0.33049464\n",
      "Iteration 9, loss = 0.27715652\n",
      "Iteration 10, loss = 0.24510772\n",
      "Iteration 11, loss = 0.27738421\n",
      "Iteration 12, loss = 0.30493764\n",
      "Iteration 13, loss = 0.32443570\n",
      "Iteration 14, loss = 0.29971565\n",
      "Iteration 15, loss = 0.27656531\n",
      "Iteration 16, loss = 0.28468413\n",
      "Iteration 17, loss = 0.27202772\n",
      "Iteration 18, loss = 0.27587576\n",
      "Iteration 19, loss = 0.27858239\n",
      "Iteration 20, loss = 0.26370272\n",
      "Iteration 21, loss = 0.24315384\n",
      "Iteration 22, loss = 0.25422091\n",
      "Iteration 23, loss = 0.26420636\n",
      "Iteration 24, loss = 0.27651945\n",
      "Iteration 25, loss = 0.25200990\n",
      "Iteration 26, loss = 0.29483769\n",
      "Iteration 27, loss = 0.31871241\n",
      "Iteration 28, loss = 0.35834625\n",
      "Iteration 29, loss = 0.37156757\n",
      "Iteration 30, loss = 0.37219731\n",
      "Iteration 31, loss = 0.36895865\n",
      "Iteration 32, loss = 0.33676488\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.79203360\n",
      "Iteration 2, loss = 1.59198103\n",
      "Iteration 3, loss = 1.13783883\n",
      "Iteration 4, loss = 0.84855293\n",
      "Iteration 5, loss = 0.63162228\n",
      "Iteration 6, loss = 0.49121344\n",
      "Iteration 7, loss = 0.38525406\n",
      "Iteration 8, loss = 0.30444129\n",
      "Iteration 9, loss = 0.24200778\n",
      "Iteration 10, loss = 0.19376687\n",
      "Iteration 11, loss = 0.15703544\n",
      "Iteration 12, loss = 0.13863574\n",
      "Iteration 13, loss = 0.13584370\n",
      "Iteration 14, loss = 0.13835274\n",
      "Iteration 15, loss = 0.17552523\n",
      "Iteration 16, loss = 0.25666750\n",
      "Iteration 17, loss = 0.40438355\n",
      "Iteration 18, loss = 0.53936152\n",
      "Iteration 19, loss = 0.53439681\n",
      "Iteration 20, loss = 0.54331552\n",
      "Iteration 21, loss = 0.54567684\n",
      "Iteration 22, loss = 0.52590865\n",
      "Iteration 23, loss = 0.50447295\n",
      "Iteration 24, loss = 0.51549818\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.74950016\n",
      "Iteration 2, loss = 1.69523963\n",
      "Iteration 3, loss = 1.27942587\n",
      "Iteration 4, loss = 0.89914708\n",
      "Iteration 5, loss = 0.62867141\n",
      "Iteration 6, loss = 0.49913676\n",
      "Iteration 7, loss = 0.40460468\n",
      "Iteration 8, loss = 0.31051910\n",
      "Iteration 9, loss = 0.24501160\n",
      "Iteration 10, loss = 0.19690456\n",
      "Iteration 11, loss = 0.17658400\n",
      "Iteration 12, loss = 0.17490243\n",
      "Iteration 13, loss = 0.18637056\n",
      "Iteration 14, loss = 0.19581925\n",
      "Iteration 15, loss = 0.23839895\n",
      "Iteration 16, loss = 0.25586689\n",
      "Iteration 17, loss = 0.27062611\n",
      "Iteration 18, loss = 0.23694010\n",
      "Iteration 19, loss = 0.21670041\n",
      "Iteration 20, loss = 0.21330985\n",
      "Iteration 21, loss = 0.20977795\n",
      "Iteration 22, loss = 0.31209571\n",
      "Iteration 23, loss = 0.38232725\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.13794480\n",
      "Iteration 2, loss = 2.17915830\n",
      "Iteration 3, loss = 1.62491058\n",
      "Iteration 4, loss = 1.07505163\n",
      "Iteration 5, loss = 0.71010784\n",
      "Iteration 6, loss = 0.55051386\n",
      "Iteration 7, loss = 0.43775606\n",
      "Iteration 8, loss = 0.35996872\n",
      "Iteration 9, loss = 0.30512887\n",
      "Iteration 10, loss = 0.25328517\n",
      "Iteration 11, loss = 0.25959107\n",
      "Iteration 12, loss = 0.28190847\n",
      "Iteration 13, loss = 0.28669761\n",
      "Iteration 14, loss = 0.30868821\n",
      "Iteration 15, loss = 0.32820678\n",
      "Iteration 16, loss = 0.37865322\n",
      "Iteration 17, loss = 0.47492170\n",
      "Iteration 18, loss = 0.45128030\n",
      "Iteration 19, loss = 0.41004931\n",
      "Iteration 20, loss = 0.37150519\n",
      "Iteration 21, loss = 0.29589822\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.20983705\n",
      "Iteration 2, loss = 2.13639234\n",
      "Iteration 3, loss = 1.38637754\n",
      "Iteration 4, loss = 1.01089607\n",
      "Iteration 5, loss = 0.75980483\n",
      "Iteration 6, loss = 0.57029620\n",
      "Iteration 7, loss = 0.43404089\n",
      "Iteration 8, loss = 0.35509749\n",
      "Iteration 9, loss = 0.30610492\n",
      "Iteration 10, loss = 0.26825424\n",
      "Iteration 11, loss = 0.26224762\n",
      "Iteration 12, loss = 0.25044635\n",
      "Iteration 13, loss = 0.23669956\n",
      "Iteration 14, loss = 0.20705890\n",
      "Iteration 15, loss = 0.19805265\n",
      "Iteration 16, loss = 0.16779883\n",
      "Iteration 17, loss = 0.15097321\n",
      "Iteration 18, loss = 0.18177693\n",
      "Iteration 19, loss = 0.21114057\n",
      "Iteration 20, loss = 0.30934011\n",
      "Iteration 21, loss = 0.38232899\n",
      "Iteration 22, loss = 0.40566866\n",
      "Iteration 23, loss = 0.36228436\n",
      "Iteration 24, loss = 0.32387955\n",
      "Iteration 25, loss = 0.34342478\n",
      "Iteration 26, loss = 0.33889663\n",
      "Iteration 27, loss = 0.30464560\n",
      "Iteration 28, loss = 0.28656538\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12358014\n",
      "Iteration 2, loss = 1.08398408\n",
      "Iteration 3, loss = 0.44090416\n",
      "Iteration 4, loss = 0.25823392\n",
      "Iteration 5, loss = 0.16490876\n",
      "Iteration 6, loss = 0.12123459\n",
      "Iteration 7, loss = 0.09965722\n",
      "Iteration 8, loss = 0.08619233\n",
      "Iteration 9, loss = 0.07663494\n",
      "Iteration 10, loss = 0.07149093\n",
      "Iteration 11, loss = 0.06845466\n",
      "Iteration 12, loss = 0.06663139\n",
      "Iteration 13, loss = 0.06422789\n",
      "Iteration 14, loss = 0.06318874\n",
      "Iteration 15, loss = 0.06241054\n",
      "Iteration 16, loss = 0.06126728\n",
      "Iteration 17, loss = 0.06046259\n",
      "Iteration 18, loss = 0.05983854\n",
      "Iteration 19, loss = 0.05922648\n",
      "Iteration 20, loss = 0.05867436\n",
      "Iteration 21, loss = 0.05833250\n",
      "Iteration 22, loss = 0.05784374\n",
      "Iteration 23, loss = 0.05728724\n",
      "Iteration 24, loss = 0.05692853\n",
      "Iteration 25, loss = 0.05651499\n",
      "Iteration 26, loss = 0.05618081\n",
      "Iteration 27, loss = 0.05581548\n",
      "Iteration 28, loss = 0.05544116\n",
      "Iteration 29, loss = 0.05512611\n",
      "Iteration 30, loss = 0.05481160\n",
      "Iteration 31, loss = 0.05451285\n",
      "Iteration 32, loss = 0.05430544\n",
      "Iteration 33, loss = 0.05396523\n",
      "Iteration 34, loss = 0.05378075\n",
      "Iteration 35, loss = 0.05342684\n",
      "Iteration 36, loss = 0.05320433\n",
      "Iteration 37, loss = 0.05296302\n",
      "Iteration 38, loss = 0.05266784\n",
      "Iteration 39, loss = 0.05249600\n",
      "Iteration 40, loss = 0.05220221\n",
      "Iteration 41, loss = 0.05196249\n",
      "Iteration 42, loss = 0.05171279\n",
      "Iteration 43, loss = 0.05155371\n",
      "Iteration 44, loss = 0.05126239\n",
      "Iteration 45, loss = 0.05108800\n",
      "Iteration 46, loss = 0.05089426\n",
      "Iteration 47, loss = 0.05066555\n",
      "Iteration 48, loss = 0.05046554\n",
      "Iteration 49, loss = 0.05025582\n",
      "Iteration 50, loss = 0.05007170\n",
      "Iteration 51, loss = 0.04987054\n",
      "Iteration 52, loss = 0.04973726\n",
      "Iteration 53, loss = 0.04955295\n",
      "Iteration 54, loss = 0.04937064\n",
      "Iteration 55, loss = 0.04919669\n",
      "Iteration 56, loss = 0.04897254\n",
      "Iteration 57, loss = 0.04884563\n",
      "Iteration 58, loss = 0.04863390\n",
      "Iteration 59, loss = 0.04846972\n",
      "Iteration 60, loss = 0.04830000\n",
      "Iteration 61, loss = 0.04815852\n",
      "Iteration 62, loss = 0.04803019\n",
      "Iteration 63, loss = 0.04787358\n",
      "Iteration 64, loss = 0.04770332\n",
      "Iteration 65, loss = 0.04751347\n",
      "Iteration 66, loss = 0.04737360\n",
      "Iteration 67, loss = 0.04723136\n",
      "Iteration 68, loss = 0.04709411\n",
      "Iteration 69, loss = 0.04699944\n",
      "Iteration 70, loss = 0.04680414\n",
      "Iteration 71, loss = 0.04672865\n",
      "Iteration 72, loss = 0.04655005\n",
      "Iteration 73, loss = 0.04642345\n",
      "Iteration 74, loss = 0.04627082\n",
      "Iteration 75, loss = 0.04614104\n",
      "Iteration 76, loss = 0.04603034\n",
      "Iteration 77, loss = 0.04589412\n",
      "Iteration 78, loss = 0.04579369\n",
      "Iteration 79, loss = 0.04567206\n",
      "Iteration 80, loss = 0.04553932\n",
      "Iteration 81, loss = 0.04539479\n",
      "Iteration 82, loss = 0.04529279\n",
      "Iteration 83, loss = 0.04516057\n",
      "Iteration 84, loss = 0.04505089\n",
      "Iteration 85, loss = 0.04495157\n",
      "Iteration 86, loss = 0.04483111\n",
      "Iteration 87, loss = 0.04472295\n",
      "Iteration 88, loss = 0.04462297\n",
      "Iteration 89, loss = 0.04452908\n",
      "Iteration 90, loss = 0.04441734\n",
      "Iteration 91, loss = 0.04433049\n",
      "Iteration 92, loss = 0.04426462\n",
      "Iteration 93, loss = 0.04413104\n",
      "Iteration 94, loss = 0.04403098\n",
      "Iteration 95, loss = 0.04392214\n",
      "Iteration 96, loss = 0.04383454\n",
      "Iteration 97, loss = 0.04373569\n",
      "Iteration 98, loss = 0.04363188\n",
      "Iteration 99, loss = 0.04352410\n",
      "Iteration 100, loss = 0.04344468\n",
      "Iteration 101, loss = 0.04334766\n",
      "Iteration 102, loss = 0.04328237\n",
      "Iteration 103, loss = 0.04318787\n",
      "Iteration 104, loss = 0.04306922\n",
      "Iteration 105, loss = 0.04301451\n",
      "Iteration 106, loss = 0.04298862\n",
      "Iteration 107, loss = 0.04284931\n",
      "Iteration 108, loss = 0.04278389\n",
      "Iteration 109, loss = 0.04268724\n",
      "Iteration 110, loss = 0.04260267\n",
      "Iteration 111, loss = 0.04250852\n",
      "Iteration 112, loss = 0.04242943\n",
      "Iteration 113, loss = 0.04236271\n",
      "Iteration 114, loss = 0.04230929\n",
      "Iteration 115, loss = 0.04220702\n",
      "Iteration 116, loss = 0.04215387\n",
      "Iteration 117, loss = 0.04208516\n",
      "Iteration 118, loss = 0.04199463\n",
      "Iteration 119, loss = 0.04190498\n",
      "Iteration 120, loss = 0.04183661\n",
      "Iteration 121, loss = 0.04178243\n",
      "Iteration 122, loss = 0.04172523\n",
      "Iteration 123, loss = 0.04165637\n",
      "Iteration 124, loss = 0.04158368\n",
      "Iteration 125, loss = 0.04152226\n",
      "Iteration 126, loss = 0.04145032\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11123874\n",
      "Iteration 2, loss = 1.03693042\n",
      "Iteration 3, loss = 0.42091245\n",
      "Iteration 4, loss = 0.25765329\n",
      "Iteration 5, loss = 0.16263896\n",
      "Iteration 6, loss = 0.12376819\n",
      "Iteration 7, loss = 0.10335624\n",
      "Iteration 8, loss = 0.08810170\n",
      "Iteration 9, loss = 0.07868749\n",
      "Iteration 10, loss = 0.07287381\n",
      "Iteration 11, loss = 0.06992676\n",
      "Iteration 12, loss = 0.06717906\n",
      "Iteration 13, loss = 0.06451070\n",
      "Iteration 14, loss = 0.06358635\n",
      "Iteration 15, loss = 0.06207647\n",
      "Iteration 16, loss = 0.06092650\n",
      "Iteration 17, loss = 0.06016942\n",
      "Iteration 18, loss = 0.05938815\n",
      "Iteration 19, loss = 0.05889296\n",
      "Iteration 20, loss = 0.05830434\n",
      "Iteration 21, loss = 0.05782230\n",
      "Iteration 22, loss = 0.05733620\n",
      "Iteration 23, loss = 0.05679432\n",
      "Iteration 24, loss = 0.05650755\n",
      "Iteration 25, loss = 0.05605488\n",
      "Iteration 26, loss = 0.05564496\n",
      "Iteration 27, loss = 0.05528259\n",
      "Iteration 28, loss = 0.05493533\n",
      "Iteration 29, loss = 0.05458940\n",
      "Iteration 30, loss = 0.05428809\n",
      "Iteration 31, loss = 0.05398942\n",
      "Iteration 32, loss = 0.05373173\n",
      "Iteration 33, loss = 0.05346735\n",
      "Iteration 34, loss = 0.05320075\n",
      "Iteration 35, loss = 0.05288344\n",
      "Iteration 36, loss = 0.05266506\n",
      "Iteration 37, loss = 0.05252681\n",
      "Iteration 38, loss = 0.05209940\n",
      "Iteration 39, loss = 0.05199001\n",
      "Iteration 40, loss = 0.05163630\n",
      "Iteration 41, loss = 0.05140880\n",
      "Iteration 42, loss = 0.05121863\n",
      "Iteration 43, loss = 0.05101225\n",
      "Iteration 44, loss = 0.05073636\n",
      "Iteration 45, loss = 0.05053659\n",
      "Iteration 46, loss = 0.05035217\n",
      "Iteration 47, loss = 0.05017131\n",
      "Iteration 48, loss = 0.04992255\n",
      "Iteration 49, loss = 0.04975447\n",
      "Iteration 50, loss = 0.04958088\n",
      "Iteration 51, loss = 0.04935447\n",
      "Iteration 52, loss = 0.04918797\n",
      "Iteration 53, loss = 0.04901933\n",
      "Iteration 54, loss = 0.04882903\n",
      "Iteration 55, loss = 0.04869415\n",
      "Iteration 56, loss = 0.04846723\n",
      "Iteration 57, loss = 0.04840121\n",
      "Iteration 58, loss = 0.04816165\n",
      "Iteration 59, loss = 0.04794554\n",
      "Iteration 60, loss = 0.04778353\n",
      "Iteration 61, loss = 0.04760944\n",
      "Iteration 62, loss = 0.04747974\n",
      "Iteration 63, loss = 0.04732656\n",
      "Iteration 64, loss = 0.04716285\n",
      "Iteration 65, loss = 0.04699227\n",
      "Iteration 66, loss = 0.04689207\n",
      "Iteration 67, loss = 0.04670616\n",
      "Iteration 68, loss = 0.04657585\n",
      "Iteration 69, loss = 0.04643403\n",
      "Iteration 70, loss = 0.04630907\n",
      "Iteration 71, loss = 0.04618128\n",
      "Iteration 72, loss = 0.04604390\n",
      "Iteration 73, loss = 0.04589655\n",
      "Iteration 74, loss = 0.04576684\n",
      "Iteration 75, loss = 0.04562482\n",
      "Iteration 76, loss = 0.04549445\n",
      "Iteration 77, loss = 0.04537526\n",
      "Iteration 78, loss = 0.04526592\n",
      "Iteration 79, loss = 0.04512054\n",
      "Iteration 80, loss = 0.04498642\n",
      "Iteration 81, loss = 0.04488084\n",
      "Iteration 82, loss = 0.04477220\n",
      "Iteration 83, loss = 0.04462861\n",
      "Iteration 84, loss = 0.04453754\n",
      "Iteration 85, loss = 0.04440136\n",
      "Iteration 86, loss = 0.04432600\n",
      "Iteration 87, loss = 0.04420154\n",
      "Iteration 88, loss = 0.04408784\n",
      "Iteration 89, loss = 0.04399822\n",
      "Iteration 90, loss = 0.04389351\n",
      "Iteration 91, loss = 0.04380199\n",
      "Iteration 92, loss = 0.04371265\n",
      "Iteration 93, loss = 0.04358872\n",
      "Iteration 94, loss = 0.04348518\n",
      "Iteration 95, loss = 0.04337732\n",
      "Iteration 96, loss = 0.04332885\n",
      "Iteration 97, loss = 0.04318445\n",
      "Iteration 98, loss = 0.04314668\n",
      "Iteration 99, loss = 0.04300705\n",
      "Iteration 100, loss = 0.04292355\n",
      "Iteration 101, loss = 0.04282681\n",
      "Iteration 102, loss = 0.04275305\n",
      "Iteration 103, loss = 0.04264088\n",
      "Iteration 104, loss = 0.04255761\n",
      "Iteration 105, loss = 0.04246822\n",
      "Iteration 106, loss = 0.04248754\n",
      "Iteration 107, loss = 0.04232696\n",
      "Iteration 108, loss = 0.04225157\n",
      "Iteration 109, loss = 0.04213008\n",
      "Iteration 110, loss = 0.04207511\n",
      "Iteration 111, loss = 0.04198691\n",
      "Iteration 112, loss = 0.04191658\n",
      "Iteration 113, loss = 0.04181862\n",
      "Iteration 114, loss = 0.04177522\n",
      "Iteration 115, loss = 0.04165861\n",
      "Iteration 116, loss = 0.04165776\n",
      "Iteration 117, loss = 0.04158530\n",
      "Iteration 118, loss = 0.04148074\n",
      "Iteration 119, loss = 0.04139619\n",
      "Iteration 120, loss = 0.04130426\n",
      "Iteration 121, loss = 0.04126493\n",
      "Iteration 122, loss = 0.04120573\n",
      "Iteration 123, loss = 0.04112606\n",
      "Iteration 124, loss = 0.04104322\n",
      "Iteration 125, loss = 0.04097407\n",
      "Iteration 126, loss = 0.04093245\n",
      "Iteration 127, loss = 0.04086204\n",
      "Iteration 128, loss = 0.04078624\n",
      "Iteration 129, loss = 0.04075502\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10183503\n",
      "Iteration 2, loss = 1.03909471\n",
      "Iteration 3, loss = 0.42634853\n",
      "Iteration 4, loss = 0.25707746\n",
      "Iteration 5, loss = 0.16991662\n",
      "Iteration 6, loss = 0.12875558\n",
      "Iteration 7, loss = 0.10540074\n",
      "Iteration 8, loss = 0.08574737\n",
      "Iteration 9, loss = 0.07789040\n",
      "Iteration 10, loss = 0.07477141\n",
      "Iteration 11, loss = 0.06934581\n",
      "Iteration 12, loss = 0.06692913\n",
      "Iteration 13, loss = 0.06491744\n",
      "Iteration 14, loss = 0.06368496\n",
      "Iteration 15, loss = 0.06233016\n",
      "Iteration 16, loss = 0.06147147\n",
      "Iteration 17, loss = 0.06069300\n",
      "Iteration 18, loss = 0.06004560\n",
      "Iteration 19, loss = 0.05953368\n",
      "Iteration 20, loss = 0.05911729\n",
      "Iteration 21, loss = 0.05840987\n",
      "Iteration 22, loss = 0.05822100\n",
      "Iteration 23, loss = 0.05759686\n",
      "Iteration 24, loss = 0.05711953\n",
      "Iteration 25, loss = 0.05670282\n",
      "Iteration 26, loss = 0.05641808\n",
      "Iteration 27, loss = 0.05598058\n",
      "Iteration 28, loss = 0.05564537\n",
      "Iteration 29, loss = 0.05531671\n",
      "Iteration 30, loss = 0.05509340\n",
      "Iteration 31, loss = 0.05477267\n",
      "Iteration 32, loss = 0.05446065\n",
      "Iteration 33, loss = 0.05418604\n",
      "Iteration 34, loss = 0.05394545\n",
      "Iteration 35, loss = 0.05364799\n",
      "Iteration 36, loss = 0.05337132\n",
      "Iteration 37, loss = 0.05312396\n",
      "Iteration 38, loss = 0.05285325\n",
      "Iteration 39, loss = 0.05262067\n",
      "Iteration 40, loss = 0.05241757\n",
      "Iteration 41, loss = 0.05225760\n",
      "Iteration 42, loss = 0.05194293\n",
      "Iteration 43, loss = 0.05173333\n",
      "Iteration 44, loss = 0.05149665\n",
      "Iteration 45, loss = 0.05128971\n",
      "Iteration 46, loss = 0.05109950\n",
      "Iteration 47, loss = 0.05090772\n",
      "Iteration 48, loss = 0.05070056\n",
      "Iteration 49, loss = 0.05047011\n",
      "Iteration 50, loss = 0.05034168\n",
      "Iteration 51, loss = 0.05016839\n",
      "Iteration 52, loss = 0.04999411\n",
      "Iteration 53, loss = 0.04976051\n",
      "Iteration 54, loss = 0.04956858\n",
      "Iteration 55, loss = 0.04944371\n",
      "Iteration 56, loss = 0.04918408\n",
      "Iteration 57, loss = 0.04906115\n",
      "Iteration 58, loss = 0.04886980\n",
      "Iteration 59, loss = 0.04869496\n",
      "Iteration 60, loss = 0.04855758\n",
      "Iteration 61, loss = 0.04838082\n",
      "Iteration 62, loss = 0.04822180\n",
      "Iteration 63, loss = 0.04806920\n",
      "Iteration 64, loss = 0.04791637\n",
      "Iteration 65, loss = 0.04776899\n",
      "Iteration 66, loss = 0.04763079\n",
      "Iteration 67, loss = 0.04748314\n",
      "Iteration 68, loss = 0.04731734\n",
      "Iteration 69, loss = 0.04721873\n",
      "Iteration 70, loss = 0.04704407\n",
      "Iteration 71, loss = 0.04693705\n",
      "Iteration 72, loss = 0.04680130\n",
      "Iteration 73, loss = 0.04665850\n",
      "Iteration 74, loss = 0.04655330\n",
      "Iteration 75, loss = 0.04637104\n",
      "Iteration 76, loss = 0.04624326\n",
      "Iteration 77, loss = 0.04612778\n",
      "Iteration 78, loss = 0.04601854\n",
      "Iteration 79, loss = 0.04588663\n",
      "Iteration 80, loss = 0.04579710\n",
      "Iteration 81, loss = 0.04563869\n",
      "Iteration 82, loss = 0.04552726\n",
      "Iteration 83, loss = 0.04543770\n",
      "Iteration 84, loss = 0.04528631\n",
      "Iteration 85, loss = 0.04519991\n",
      "Iteration 86, loss = 0.04505396\n",
      "Iteration 87, loss = 0.04496159\n",
      "Iteration 88, loss = 0.04483918\n",
      "Iteration 89, loss = 0.04474399\n",
      "Iteration 90, loss = 0.04465522\n",
      "Iteration 91, loss = 0.04452219\n",
      "Iteration 92, loss = 0.04443016\n",
      "Iteration 93, loss = 0.04434033\n",
      "Iteration 94, loss = 0.04425844\n",
      "Iteration 95, loss = 0.04414880\n",
      "Iteration 96, loss = 0.04404017\n",
      "Iteration 97, loss = 0.04396656\n",
      "Iteration 98, loss = 0.04384416\n",
      "Iteration 99, loss = 0.04376460\n",
      "Iteration 100, loss = 0.04366655\n",
      "Iteration 101, loss = 0.04359095\n",
      "Iteration 102, loss = 0.04349641\n",
      "Iteration 103, loss = 0.04340351\n",
      "Iteration 104, loss = 0.04332237\n",
      "Iteration 105, loss = 0.04324516\n",
      "Iteration 106, loss = 0.04312594\n",
      "Iteration 107, loss = 0.04304731\n",
      "Iteration 108, loss = 0.04299083\n",
      "Iteration 109, loss = 0.04292990\n",
      "Iteration 110, loss = 0.04285746\n",
      "Iteration 111, loss = 0.04273529\n",
      "Iteration 112, loss = 0.04268822\n",
      "Iteration 113, loss = 0.04260359\n",
      "Iteration 114, loss = 0.04252266\n",
      "Iteration 115, loss = 0.04244671\n",
      "Iteration 116, loss = 0.04237333\n",
      "Iteration 117, loss = 0.04229625\n",
      "Iteration 118, loss = 0.04220804\n",
      "Iteration 119, loss = 0.04214145\n",
      "Iteration 120, loss = 0.04208942\n",
      "Iteration 121, loss = 0.04199862\n",
      "Iteration 122, loss = 0.04192134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11118439\n",
      "Iteration 2, loss = 1.07660368\n",
      "Iteration 3, loss = 0.44935538\n",
      "Iteration 4, loss = 0.25544349\n",
      "Iteration 5, loss = 0.17266657\n",
      "Iteration 6, loss = 0.13070255\n",
      "Iteration 7, loss = 0.10808402\n",
      "Iteration 8, loss = 0.08696627\n",
      "Iteration 9, loss = 0.07922193\n",
      "Iteration 10, loss = 0.07461197\n",
      "Iteration 11, loss = 0.07032315\n",
      "Iteration 12, loss = 0.06804657\n",
      "Iteration 13, loss = 0.06594812\n",
      "Iteration 14, loss = 0.06443031\n",
      "Iteration 15, loss = 0.06320621\n",
      "Iteration 16, loss = 0.06233793\n",
      "Iteration 17, loss = 0.06158600\n",
      "Iteration 18, loss = 0.06092164\n",
      "Iteration 19, loss = 0.06033710\n",
      "Iteration 20, loss = 0.06001122\n",
      "Iteration 21, loss = 0.05923445\n",
      "Iteration 22, loss = 0.05889819\n",
      "Iteration 23, loss = 0.05835281\n",
      "Iteration 24, loss = 0.05790429\n",
      "Iteration 25, loss = 0.05757158\n",
      "Iteration 26, loss = 0.05717118\n",
      "Iteration 27, loss = 0.05674601\n",
      "Iteration 28, loss = 0.05641338\n",
      "Iteration 29, loss = 0.05609448\n",
      "Iteration 30, loss = 0.05588893\n",
      "Iteration 31, loss = 0.05557496\n",
      "Iteration 32, loss = 0.05523830\n",
      "Iteration 33, loss = 0.05490795\n",
      "Iteration 34, loss = 0.05461566\n",
      "Iteration 35, loss = 0.05445289\n",
      "Iteration 36, loss = 0.05411311\n",
      "Iteration 37, loss = 0.05385469\n",
      "Iteration 38, loss = 0.05359806\n",
      "Iteration 39, loss = 0.05337822\n",
      "Iteration 40, loss = 0.05322675\n",
      "Iteration 41, loss = 0.05300664\n",
      "Iteration 42, loss = 0.05266580\n",
      "Iteration 43, loss = 0.05247283\n",
      "Iteration 44, loss = 0.05222891\n",
      "Iteration 45, loss = 0.05203199\n",
      "Iteration 46, loss = 0.05182952\n",
      "Iteration 47, loss = 0.05161222\n",
      "Iteration 48, loss = 0.05143656\n",
      "Iteration 49, loss = 0.05121586\n",
      "Iteration 50, loss = 0.05106400\n",
      "Iteration 51, loss = 0.05087534\n",
      "Iteration 52, loss = 0.05073882\n",
      "Iteration 53, loss = 0.05049177\n",
      "Iteration 54, loss = 0.05029202\n",
      "Iteration 55, loss = 0.05018528\n",
      "Iteration 56, loss = 0.04992983\n",
      "Iteration 57, loss = 0.04978872\n",
      "Iteration 58, loss = 0.04960807\n",
      "Iteration 59, loss = 0.04946850\n",
      "Iteration 60, loss = 0.04936345\n",
      "Iteration 61, loss = 0.04916316\n",
      "Iteration 62, loss = 0.04896718\n",
      "Iteration 63, loss = 0.04880652\n",
      "Iteration 64, loss = 0.04866379\n",
      "Iteration 65, loss = 0.04853056\n",
      "Iteration 66, loss = 0.04834884\n",
      "Iteration 67, loss = 0.04823993\n",
      "Iteration 68, loss = 0.04806499\n",
      "Iteration 69, loss = 0.04794244\n",
      "Iteration 70, loss = 0.04783983\n",
      "Iteration 71, loss = 0.04767118\n",
      "Iteration 72, loss = 0.04754110\n",
      "Iteration 73, loss = 0.04742191\n",
      "Iteration 74, loss = 0.04728332\n",
      "Iteration 75, loss = 0.04714394\n",
      "Iteration 76, loss = 0.04699391\n",
      "Iteration 77, loss = 0.04691458\n",
      "Iteration 78, loss = 0.04678620\n",
      "Iteration 79, loss = 0.04663223\n",
      "Iteration 80, loss = 0.04654568\n",
      "Iteration 81, loss = 0.04640430\n",
      "Iteration 82, loss = 0.04627242\n",
      "Iteration 83, loss = 0.04615658\n",
      "Iteration 84, loss = 0.04605135\n",
      "Iteration 85, loss = 0.04594418\n",
      "Iteration 86, loss = 0.04584597\n",
      "Iteration 87, loss = 0.04571371\n",
      "Iteration 88, loss = 0.04559893\n",
      "Iteration 89, loss = 0.04550534\n",
      "Iteration 90, loss = 0.04538484\n",
      "Iteration 91, loss = 0.04527594\n",
      "Iteration 92, loss = 0.04518891\n",
      "Iteration 93, loss = 0.04510695\n",
      "Iteration 94, loss = 0.04500845\n",
      "Iteration 95, loss = 0.04489330\n",
      "Iteration 96, loss = 0.04482264\n",
      "Iteration 97, loss = 0.04469094\n",
      "Iteration 98, loss = 0.04460646\n",
      "Iteration 99, loss = 0.04454159\n",
      "Iteration 100, loss = 0.04441859\n",
      "Iteration 101, loss = 0.04437155\n",
      "Iteration 102, loss = 0.04423639\n",
      "Iteration 103, loss = 0.04413904\n",
      "Iteration 104, loss = 0.04406161\n",
      "Iteration 105, loss = 0.04399813\n",
      "Iteration 106, loss = 0.04386997\n",
      "Iteration 107, loss = 0.04382823\n",
      "Iteration 108, loss = 0.04375865\n",
      "Iteration 109, loss = 0.04368437\n",
      "Iteration 110, loss = 0.04358583\n",
      "Iteration 111, loss = 0.04348989\n",
      "Iteration 112, loss = 0.04345897\n",
      "Iteration 113, loss = 0.04335731\n",
      "Iteration 114, loss = 0.04328057\n",
      "Iteration 115, loss = 0.04321297\n",
      "Iteration 116, loss = 0.04312007\n",
      "Iteration 117, loss = 0.04307400\n",
      "Iteration 118, loss = 0.04296122\n",
      "Iteration 119, loss = 0.04291668\n",
      "Iteration 120, loss = 0.04284480\n",
      "Iteration 121, loss = 0.04276638\n",
      "Iteration 122, loss = 0.04270147\n",
      "Iteration 123, loss = 0.04260094\n",
      "Iteration 124, loss = 0.04254662\n",
      "Iteration 125, loss = 0.04250265\n",
      "Iteration 126, loss = 0.04243008\n",
      "Iteration 127, loss = 0.04242107\n",
      "Iteration 128, loss = 0.04229154\n",
      "Iteration 129, loss = 0.04223451\n",
      "Iteration 130, loss = 0.04214723\n",
      "Iteration 131, loss = 0.04212318\n",
      "Iteration 132, loss = 0.04205508\n",
      "Iteration 133, loss = 0.04200388\n",
      "Iteration 134, loss = 0.04193119\n",
      "Iteration 135, loss = 0.04186091\n",
      "Iteration 136, loss = 0.04179881\n",
      "Iteration 137, loss = 0.04175181\n",
      "Iteration 138, loss = 0.04172582\n",
      "Iteration 139, loss = 0.04164870\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10735749\n",
      "Iteration 2, loss = 1.04370255\n",
      "Iteration 3, loss = 0.40258112\n",
      "Iteration 4, loss = 0.21291259\n",
      "Iteration 5, loss = 0.14197084\n",
      "Iteration 6, loss = 0.11078786\n",
      "Iteration 7, loss = 0.09187130\n",
      "Iteration 8, loss = 0.07978195\n",
      "Iteration 9, loss = 0.07282824\n",
      "Iteration 10, loss = 0.06951715\n",
      "Iteration 11, loss = 0.06579434\n",
      "Iteration 12, loss = 0.06392381\n",
      "Iteration 13, loss = 0.06236284\n",
      "Iteration 14, loss = 0.06142216\n",
      "Iteration 15, loss = 0.06043130\n",
      "Iteration 16, loss = 0.05969355\n",
      "Iteration 17, loss = 0.05898796\n",
      "Iteration 18, loss = 0.05835434\n",
      "Iteration 19, loss = 0.05789155\n",
      "Iteration 20, loss = 0.05756659\n",
      "Iteration 21, loss = 0.05684123\n",
      "Iteration 22, loss = 0.05640661\n",
      "Iteration 23, loss = 0.05609046\n",
      "Iteration 24, loss = 0.05564288\n",
      "Iteration 25, loss = 0.05529345\n",
      "Iteration 26, loss = 0.05494369\n",
      "Iteration 27, loss = 0.05451423\n",
      "Iteration 28, loss = 0.05420663\n",
      "Iteration 29, loss = 0.05389532\n",
      "Iteration 30, loss = 0.05363072\n",
      "Iteration 31, loss = 0.05334161\n",
      "Iteration 32, loss = 0.05301366\n",
      "Iteration 33, loss = 0.05278928\n",
      "Iteration 34, loss = 0.05252624\n",
      "Iteration 35, loss = 0.05226650\n",
      "Iteration 36, loss = 0.05206088\n",
      "Iteration 37, loss = 0.05178225\n",
      "Iteration 38, loss = 0.05152678\n",
      "Iteration 39, loss = 0.05130981\n",
      "Iteration 40, loss = 0.05113127\n",
      "Iteration 41, loss = 0.05088300\n",
      "Iteration 42, loss = 0.05063519\n",
      "Iteration 43, loss = 0.05045759\n",
      "Iteration 44, loss = 0.05020854\n",
      "Iteration 45, loss = 0.04996547\n",
      "Iteration 46, loss = 0.04981120\n",
      "Iteration 47, loss = 0.04956034\n",
      "Iteration 48, loss = 0.04937610\n",
      "Iteration 49, loss = 0.04916056\n",
      "Iteration 50, loss = 0.04900992\n",
      "Iteration 51, loss = 0.04883146\n",
      "Iteration 52, loss = 0.04864154\n",
      "Iteration 53, loss = 0.04842706\n",
      "Iteration 54, loss = 0.04825305\n",
      "Iteration 55, loss = 0.04811453\n",
      "Iteration 56, loss = 0.04791508\n",
      "Iteration 57, loss = 0.04777859\n",
      "Iteration 58, loss = 0.04757951\n",
      "Iteration 59, loss = 0.04742098\n",
      "Iteration 60, loss = 0.04729828\n",
      "Iteration 61, loss = 0.04709979\n",
      "Iteration 62, loss = 0.04694291\n",
      "Iteration 63, loss = 0.04678205\n",
      "Iteration 64, loss = 0.04664351\n",
      "Iteration 65, loss = 0.04650021\n",
      "Iteration 66, loss = 0.04631565\n",
      "Iteration 67, loss = 0.04618248\n",
      "Iteration 68, loss = 0.04603858\n",
      "Iteration 69, loss = 0.04590465\n",
      "Iteration 70, loss = 0.04583355\n",
      "Iteration 71, loss = 0.04563685\n",
      "Iteration 72, loss = 0.04549878\n",
      "Iteration 73, loss = 0.04535888\n",
      "Iteration 74, loss = 0.04527501\n",
      "Iteration 75, loss = 0.04509031\n",
      "Iteration 76, loss = 0.04497659\n",
      "Iteration 77, loss = 0.04485520\n",
      "Iteration 78, loss = 0.04473232\n",
      "Iteration 79, loss = 0.04458493\n",
      "Iteration 80, loss = 0.04447263\n",
      "Iteration 81, loss = 0.04436719\n",
      "Iteration 82, loss = 0.04424697\n",
      "Iteration 83, loss = 0.04413121\n",
      "Iteration 84, loss = 0.04403364\n",
      "Iteration 85, loss = 0.04390234\n",
      "Iteration 86, loss = 0.04382815\n",
      "Iteration 87, loss = 0.04369000\n",
      "Iteration 88, loss = 0.04357378\n",
      "Iteration 89, loss = 0.04346656\n",
      "Iteration 90, loss = 0.04336355\n",
      "Iteration 91, loss = 0.04325934\n",
      "Iteration 92, loss = 0.04315553\n",
      "Iteration 93, loss = 0.04306259\n",
      "Iteration 94, loss = 0.04296480\n",
      "Iteration 95, loss = 0.04285588\n",
      "Iteration 96, loss = 0.04279165\n",
      "Iteration 97, loss = 0.04265205\n",
      "Iteration 98, loss = 0.04261164\n",
      "Iteration 99, loss = 0.04250084\n",
      "Iteration 100, loss = 0.04238222\n",
      "Iteration 101, loss = 0.04231104\n",
      "Iteration 102, loss = 0.04220389\n",
      "Iteration 103, loss = 0.04211497\n",
      "Iteration 104, loss = 0.04203981\n",
      "Iteration 105, loss = 0.04196974\n",
      "Iteration 106, loss = 0.04187770\n",
      "Iteration 107, loss = 0.04179795\n",
      "Iteration 108, loss = 0.04172751\n",
      "Iteration 109, loss = 0.04164605\n",
      "Iteration 110, loss = 0.04155587\n",
      "Iteration 111, loss = 0.04148643\n",
      "Iteration 112, loss = 0.04141381\n",
      "Iteration 113, loss = 0.04131936\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.70026810\n",
      "Iteration 2, loss = 0.42214736\n",
      "Iteration 3, loss = 0.17215181\n",
      "Iteration 4, loss = 0.11848734\n",
      "Iteration 5, loss = 0.09633863\n",
      "Iteration 6, loss = 0.08730021\n",
      "Iteration 7, loss = 0.08091416\n",
      "Iteration 8, loss = 0.07529897\n",
      "Iteration 9, loss = 0.07028716\n",
      "Iteration 10, loss = 0.06586116\n",
      "Iteration 11, loss = 0.06225841\n",
      "Iteration 12, loss = 0.05939289\n",
      "Iteration 13, loss = 0.05682902\n",
      "Iteration 14, loss = 0.05491239\n",
      "Iteration 15, loss = 0.05334285\n",
      "Iteration 16, loss = 0.05174975\n",
      "Iteration 17, loss = 0.05018479\n",
      "Iteration 18, loss = 0.04895257\n",
      "Iteration 19, loss = 0.04789591\n",
      "Iteration 20, loss = 0.04690633\n",
      "Iteration 21, loss = 0.04603571\n",
      "Iteration 22, loss = 0.04557228\n",
      "Iteration 23, loss = 0.04457014\n",
      "Iteration 24, loss = 0.04394638\n",
      "Iteration 25, loss = 0.04325257\n",
      "Iteration 26, loss = 0.04313139\n",
      "Iteration 27, loss = 0.04244140\n",
      "Iteration 28, loss = 0.04215973\n",
      "Iteration 29, loss = 0.04179783\n",
      "Iteration 30, loss = 0.04158900\n",
      "Iteration 31, loss = 0.04110632\n",
      "Iteration 32, loss = 0.04099719\n",
      "Iteration 33, loss = 0.04111292\n",
      "Iteration 34, loss = 0.04101472\n",
      "Iteration 35, loss = 0.04067651\n",
      "Iteration 36, loss = 0.04036091\n",
      "Iteration 37, loss = 0.04044055\n",
      "Iteration 38, loss = 0.04038700\n",
      "Iteration 39, loss = 0.04007883\n",
      "Iteration 40, loss = 0.03999602\n",
      "Iteration 41, loss = 0.03983551\n",
      "Iteration 42, loss = 0.03961544\n",
      "Iteration 43, loss = 0.03984748\n",
      "Iteration 44, loss = 0.03988070\n",
      "Iteration 45, loss = 0.03986874\n",
      "Iteration 46, loss = 0.03987830\n",
      "Iteration 47, loss = 0.03977002\n",
      "Iteration 48, loss = 0.03937788\n",
      "Iteration 49, loss = 0.03915961\n",
      "Iteration 50, loss = 0.03880273\n",
      "Iteration 51, loss = 0.03872712\n",
      "Iteration 52, loss = 0.03873036\n",
      "Iteration 53, loss = 0.04022125\n",
      "Iteration 54, loss = 0.04082692\n",
      "Iteration 55, loss = 0.04022433\n",
      "Iteration 56, loss = 0.04002398\n",
      "Iteration 57, loss = 0.03968245\n",
      "Iteration 58, loss = 0.03926449\n",
      "Iteration 59, loss = 0.03860464\n",
      "Iteration 60, loss = 0.03830159\n",
      "Iteration 61, loss = 0.03839868\n",
      "Iteration 62, loss = 0.03894145\n",
      "Iteration 63, loss = 0.03927945\n",
      "Iteration 64, loss = 0.03921286\n",
      "Iteration 65, loss = 0.03896182\n",
      "Iteration 66, loss = 0.03839910\n",
      "Iteration 67, loss = 0.03838958\n",
      "Iteration 68, loss = 0.03919072\n",
      "Iteration 69, loss = 0.03924512\n",
      "Iteration 70, loss = 0.03892098\n",
      "Iteration 71, loss = 0.03917517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.70838215\n",
      "Iteration 2, loss = 0.46107013\n",
      "Iteration 3, loss = 0.18450372\n",
      "Iteration 4, loss = 0.12322807\n",
      "Iteration 5, loss = 0.10074674\n",
      "Iteration 6, loss = 0.08933716\n",
      "Iteration 7, loss = 0.08290113\n",
      "Iteration 8, loss = 0.07681053\n",
      "Iteration 9, loss = 0.07158942\n",
      "Iteration 10, loss = 0.06694675\n",
      "Iteration 11, loss = 0.06316821\n",
      "Iteration 12, loss = 0.06013553\n",
      "Iteration 13, loss = 0.05730482\n",
      "Iteration 14, loss = 0.05525054\n",
      "Iteration 15, loss = 0.05354756\n",
      "Iteration 16, loss = 0.05191847\n",
      "Iteration 17, loss = 0.05028370\n",
      "Iteration 18, loss = 0.04899616\n",
      "Iteration 19, loss = 0.04806870\n",
      "Iteration 20, loss = 0.04692327\n",
      "Iteration 21, loss = 0.04595128\n",
      "Iteration 22, loss = 0.04541528\n",
      "Iteration 23, loss = 0.04444668\n",
      "Iteration 24, loss = 0.04360371\n",
      "Iteration 25, loss = 0.04301928\n",
      "Iteration 26, loss = 0.04276013\n",
      "Iteration 27, loss = 0.04226620\n",
      "Iteration 28, loss = 0.04187177\n",
      "Iteration 29, loss = 0.04156541\n",
      "Iteration 30, loss = 0.04117272\n",
      "Iteration 31, loss = 0.04081369\n",
      "Iteration 32, loss = 0.04050559\n",
      "Iteration 33, loss = 0.04048682\n",
      "Iteration 34, loss = 0.04038289\n",
      "Iteration 35, loss = 0.04037914\n",
      "Iteration 36, loss = 0.03996544\n",
      "Iteration 37, loss = 0.04063757\n",
      "Iteration 38, loss = 0.04033357\n",
      "Iteration 39, loss = 0.04111483\n",
      "Iteration 40, loss = 0.04149576\n",
      "Iteration 41, loss = 0.04051826\n",
      "Iteration 42, loss = 0.04002466\n",
      "Iteration 43, loss = 0.04054840\n",
      "Iteration 44, loss = 0.04007972\n",
      "Iteration 45, loss = 0.03922470\n",
      "Iteration 46, loss = 0.03886624\n",
      "Iteration 47, loss = 0.03914075\n",
      "Iteration 48, loss = 0.03883478\n",
      "Iteration 49, loss = 0.03861395\n",
      "Iteration 50, loss = 0.03851646\n",
      "Iteration 51, loss = 0.03847793\n",
      "Iteration 52, loss = 0.03815457\n",
      "Iteration 53, loss = 0.03837778\n",
      "Iteration 54, loss = 0.03865736\n",
      "Iteration 55, loss = 0.03826539\n",
      "Iteration 56, loss = 0.03877722\n",
      "Iteration 57, loss = 0.03933484\n",
      "Iteration 58, loss = 0.03941188\n",
      "Iteration 59, loss = 0.03901434\n",
      "Iteration 60, loss = 0.03867723\n",
      "Iteration 61, loss = 0.03832103\n",
      "Iteration 62, loss = 0.03808136\n",
      "Iteration 63, loss = 0.03822814\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.64344649\n",
      "Iteration 2, loss = 0.38182375\n",
      "Iteration 3, loss = 0.16302100\n",
      "Iteration 4, loss = 0.11695815\n",
      "Iteration 5, loss = 0.09688365\n",
      "Iteration 6, loss = 0.08912792\n",
      "Iteration 7, loss = 0.08376348\n",
      "Iteration 8, loss = 0.07750493\n",
      "Iteration 9, loss = 0.07190370\n",
      "Iteration 10, loss = 0.06725981\n",
      "Iteration 11, loss = 0.06313450\n",
      "Iteration 12, loss = 0.05989303\n",
      "Iteration 13, loss = 0.05721266\n",
      "Iteration 14, loss = 0.05503190\n",
      "Iteration 15, loss = 0.05284621\n",
      "Iteration 16, loss = 0.05127735\n",
      "Iteration 17, loss = 0.04990557\n",
      "Iteration 18, loss = 0.04857872\n",
      "Iteration 19, loss = 0.04748250\n",
      "Iteration 20, loss = 0.04694694\n",
      "Iteration 21, loss = 0.04609728\n",
      "Iteration 22, loss = 0.04561384\n",
      "Iteration 23, loss = 0.04494524\n",
      "Iteration 24, loss = 0.04412445\n",
      "Iteration 25, loss = 0.04344404\n",
      "Iteration 26, loss = 0.04321843\n",
      "Iteration 27, loss = 0.04239434\n",
      "Iteration 28, loss = 0.04201354\n",
      "Iteration 29, loss = 0.04174660\n",
      "Iteration 30, loss = 0.04156710\n",
      "Iteration 31, loss = 0.04184255\n",
      "Iteration 32, loss = 0.04175324\n",
      "Iteration 33, loss = 0.04148035\n",
      "Iteration 34, loss = 0.04070934\n",
      "Iteration 35, loss = 0.04048627\n",
      "Iteration 36, loss = 0.04059879\n",
      "Iteration 37, loss = 0.04036320\n",
      "Iteration 38, loss = 0.03992207\n",
      "Iteration 39, loss = 0.03959650\n",
      "Iteration 40, loss = 0.03952771\n",
      "Iteration 41, loss = 0.03997890\n",
      "Iteration 42, loss = 0.03996338\n",
      "Iteration 43, loss = 0.04034957\n",
      "Iteration 44, loss = 0.03961041\n",
      "Iteration 45, loss = 0.03921309\n",
      "Iteration 46, loss = 0.03914598\n",
      "Iteration 47, loss = 0.03925088\n",
      "Iteration 48, loss = 0.03919199\n",
      "Iteration 49, loss = 0.03894902\n",
      "Iteration 50, loss = 0.03901894\n",
      "Iteration 51, loss = 0.04002858\n",
      "Iteration 52, loss = 0.03974722\n",
      "Iteration 53, loss = 0.03982564\n",
      "Iteration 54, loss = 0.03929066\n",
      "Iteration 55, loss = 0.04012618\n",
      "Iteration 56, loss = 0.04023548\n",
      "Iteration 57, loss = 0.03920692\n",
      "Iteration 58, loss = 0.03909129\n",
      "Iteration 59, loss = 0.03846509\n",
      "Iteration 60, loss = 0.03869683\n",
      "Iteration 61, loss = 0.03842096\n",
      "Iteration 62, loss = 0.03844518\n",
      "Iteration 63, loss = 0.03863223\n",
      "Iteration 64, loss = 0.03858304\n",
      "Iteration 65, loss = 0.03824540\n",
      "Iteration 66, loss = 0.03837370\n",
      "Iteration 67, loss = 0.03820268\n",
      "Iteration 68, loss = 0.03820107\n",
      "Iteration 69, loss = 0.03837274\n",
      "Iteration 70, loss = 0.03872009\n",
      "Iteration 71, loss = 0.03883348\n",
      "Iteration 72, loss = 0.03977186\n",
      "Iteration 73, loss = 0.03902609\n",
      "Iteration 74, loss = 0.03910095\n",
      "Iteration 75, loss = 0.04006420\n",
      "Iteration 76, loss = 0.04165944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66741362\n",
      "Iteration 2, loss = 0.42309052\n",
      "Iteration 3, loss = 0.17905042\n",
      "Iteration 4, loss = 0.11942904\n",
      "Iteration 5, loss = 0.09708466\n",
      "Iteration 6, loss = 0.09030354\n",
      "Iteration 7, loss = 0.08459244\n",
      "Iteration 8, loss = 0.07864441\n",
      "Iteration 9, loss = 0.07327783\n",
      "Iteration 10, loss = 0.06871729\n",
      "Iteration 11, loss = 0.06492308\n",
      "Iteration 12, loss = 0.06155687\n",
      "Iteration 13, loss = 0.05867333\n",
      "Iteration 14, loss = 0.05639448\n",
      "Iteration 15, loss = 0.05427301\n",
      "Iteration 16, loss = 0.05252283\n",
      "Iteration 17, loss = 0.05105759\n",
      "Iteration 18, loss = 0.04987977\n",
      "Iteration 19, loss = 0.04885738\n",
      "Iteration 20, loss = 0.04803289\n",
      "Iteration 21, loss = 0.04712473\n",
      "Iteration 22, loss = 0.04658056\n",
      "Iteration 23, loss = 0.04593009\n",
      "Iteration 24, loss = 0.04532344\n",
      "Iteration 25, loss = 0.04465964\n",
      "Iteration 26, loss = 0.04433149\n",
      "Iteration 27, loss = 0.04351361\n",
      "Iteration 28, loss = 0.04300077\n",
      "Iteration 29, loss = 0.04274131\n",
      "Iteration 30, loss = 0.04280413\n",
      "Iteration 31, loss = 0.04287125\n",
      "Iteration 32, loss = 0.04226298\n",
      "Iteration 33, loss = 0.04217010\n",
      "Iteration 34, loss = 0.04135487\n",
      "Iteration 35, loss = 0.04189308\n",
      "Iteration 36, loss = 0.04208635\n",
      "Iteration 37, loss = 0.04139163\n",
      "Iteration 38, loss = 0.04090131\n",
      "Iteration 39, loss = 0.04055267\n",
      "Iteration 40, loss = 0.04071818\n",
      "Iteration 41, loss = 0.04156675\n",
      "Iteration 42, loss = 0.04118849\n",
      "Iteration 43, loss = 0.04167629\n",
      "Iteration 44, loss = 0.04090636\n",
      "Iteration 45, loss = 0.04045492\n",
      "Iteration 46, loss = 0.04025986\n",
      "Iteration 47, loss = 0.04017978\n",
      "Iteration 48, loss = 0.04018839\n",
      "Iteration 49, loss = 0.03964471\n",
      "Iteration 50, loss = 0.03958833\n",
      "Iteration 51, loss = 0.03999395\n",
      "Iteration 52, loss = 0.03971147\n",
      "Iteration 53, loss = 0.04010988\n",
      "Iteration 54, loss = 0.03974059\n",
      "Iteration 55, loss = 0.04088596\n",
      "Iteration 56, loss = 0.04040836\n",
      "Iteration 57, loss = 0.04015235\n",
      "Iteration 58, loss = 0.03974257\n",
      "Iteration 59, loss = 0.03930894\n",
      "Iteration 60, loss = 0.03941170\n",
      "Iteration 61, loss = 0.03943056\n",
      "Iteration 62, loss = 0.03945137\n",
      "Iteration 63, loss = 0.03943477\n",
      "Iteration 64, loss = 0.03948949\n",
      "Iteration 65, loss = 0.03943446\n",
      "Iteration 66, loss = 0.03962701\n",
      "Iteration 67, loss = 0.03945524\n",
      "Iteration 68, loss = 0.03907583\n",
      "Iteration 69, loss = 0.03932407\n",
      "Iteration 70, loss = 0.03937550\n",
      "Iteration 71, loss = 0.03976461\n",
      "Iteration 72, loss = 0.04056568\n",
      "Iteration 73, loss = 0.04117951\n",
      "Iteration 74, loss = 0.04128094\n",
      "Iteration 75, loss = 0.04416367\n",
      "Iteration 76, loss = 0.04722113\n",
      "Iteration 77, loss = 0.05563891\n",
      "Iteration 78, loss = 0.07875854\n",
      "Iteration 79, loss = 0.06943301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.63139266\n",
      "Iteration 2, loss = 0.37817612\n",
      "Iteration 3, loss = 0.15619560\n",
      "Iteration 4, loss = 0.11120365\n",
      "Iteration 5, loss = 0.09658340\n",
      "Iteration 6, loss = 0.08618653\n",
      "Iteration 7, loss = 0.08015284\n",
      "Iteration 8, loss = 0.07415428\n",
      "Iteration 9, loss = 0.06914036\n",
      "Iteration 10, loss = 0.06465290\n",
      "Iteration 11, loss = 0.06099080\n",
      "Iteration 12, loss = 0.05800397\n",
      "Iteration 13, loss = 0.05551997\n",
      "Iteration 14, loss = 0.05349942\n",
      "Iteration 15, loss = 0.05158637\n",
      "Iteration 16, loss = 0.05010620\n",
      "Iteration 17, loss = 0.04875876\n",
      "Iteration 18, loss = 0.04759008\n",
      "Iteration 19, loss = 0.04641674\n",
      "Iteration 20, loss = 0.04562359\n",
      "Iteration 21, loss = 0.04463367\n",
      "Iteration 22, loss = 0.04397387\n",
      "Iteration 23, loss = 0.04341486\n",
      "Iteration 24, loss = 0.04301480\n",
      "Iteration 25, loss = 0.04269348\n",
      "Iteration 26, loss = 0.04204061\n",
      "Iteration 27, loss = 0.04133574\n",
      "Iteration 28, loss = 0.04083134\n",
      "Iteration 29, loss = 0.04051073\n",
      "Iteration 30, loss = 0.03980175\n",
      "Iteration 31, loss = 0.03991203\n",
      "Iteration 32, loss = 0.03954604\n",
      "Iteration 33, loss = 0.03933870\n",
      "Iteration 34, loss = 0.03909983\n",
      "Iteration 35, loss = 0.03909614\n",
      "Iteration 36, loss = 0.03930247\n",
      "Iteration 37, loss = 0.03926746\n",
      "Iteration 38, loss = 0.03935332\n",
      "Iteration 39, loss = 0.03919732\n",
      "Iteration 40, loss = 0.03934420\n",
      "Iteration 41, loss = 0.03973885\n",
      "Iteration 42, loss = 0.03861904\n",
      "Iteration 43, loss = 0.03849603\n",
      "Iteration 44, loss = 0.03874877\n",
      "Iteration 45, loss = 0.03840869\n",
      "Iteration 46, loss = 0.03814613\n",
      "Iteration 47, loss = 0.03773443\n",
      "Iteration 48, loss = 0.03778131\n",
      "Iteration 49, loss = 0.03775821\n",
      "Iteration 50, loss = 0.03757119\n",
      "Iteration 51, loss = 0.03781840\n",
      "Iteration 52, loss = 0.03728297\n",
      "Iteration 53, loss = 0.03723700\n",
      "Iteration 54, loss = 0.03733940\n",
      "Iteration 55, loss = 0.03728825\n",
      "Iteration 56, loss = 0.03735127\n",
      "Iteration 57, loss = 0.03747697\n",
      "Iteration 58, loss = 0.03748002\n",
      "Iteration 59, loss = 0.03735218\n",
      "Iteration 60, loss = 0.03847254\n",
      "Iteration 61, loss = 0.03793141\n",
      "Iteration 62, loss = 0.03809628\n",
      "Iteration 63, loss = 0.03843235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33609348\n",
      "Iteration 2, loss = 2.18905805\n",
      "Iteration 3, loss = 2.02521471\n",
      "Iteration 4, loss = 1.81772491\n",
      "Iteration 5, loss = 1.59332686\n",
      "Iteration 6, loss = 1.36461859\n",
      "Iteration 7, loss = 1.15841995\n",
      "Iteration 8, loss = 0.98100968\n",
      "Iteration 9, loss = 0.83406785\n",
      "Iteration 10, loss = 0.71778857\n",
      "Iteration 11, loss = 0.62832565\n",
      "Iteration 12, loss = 0.55868891\n",
      "Iteration 13, loss = 0.49996375\n",
      "Iteration 14, loss = 0.45415097\n",
      "Iteration 15, loss = 0.41714127\n",
      "Iteration 16, loss = 0.38301279\n",
      "Iteration 17, loss = 0.35622547\n",
      "Iteration 18, loss = 0.33256292\n",
      "Iteration 19, loss = 0.31144373\n",
      "Iteration 20, loss = 0.29377062\n",
      "Iteration 21, loss = 0.27785388\n",
      "Iteration 22, loss = 0.26417134\n",
      "Iteration 23, loss = 0.24993492\n",
      "Iteration 24, loss = 0.23864747\n",
      "Iteration 25, loss = 0.22865708\n",
      "Iteration 26, loss = 0.21842967\n",
      "Iteration 27, loss = 0.21001208\n",
      "Iteration 28, loss = 0.20102596\n",
      "Iteration 29, loss = 0.19396888\n",
      "Iteration 30, loss = 0.18673534\n",
      "Iteration 31, loss = 0.18040193\n",
      "Iteration 32, loss = 0.17490227\n",
      "Iteration 33, loss = 0.16873238\n",
      "Iteration 34, loss = 0.16455248\n",
      "Iteration 35, loss = 0.15921996\n",
      "Iteration 36, loss = 0.15526162\n",
      "Iteration 37, loss = 0.15057212\n",
      "Iteration 38, loss = 0.14651625\n",
      "Iteration 39, loss = 0.14264144\n",
      "Iteration 40, loss = 0.13896146\n",
      "Iteration 41, loss = 0.13528485\n",
      "Iteration 42, loss = 0.13225033\n",
      "Iteration 43, loss = 0.12964814\n",
      "Iteration 44, loss = 0.12632384\n",
      "Iteration 45, loss = 0.12363159\n",
      "Iteration 46, loss = 0.12117077\n",
      "Iteration 47, loss = 0.11857498\n",
      "Iteration 48, loss = 0.11627314\n",
      "Iteration 49, loss = 0.11393116\n",
      "Iteration 50, loss = 0.11189799\n",
      "Iteration 51, loss = 0.10986875\n",
      "Iteration 52, loss = 0.10818174\n",
      "Iteration 53, loss = 0.10647796\n",
      "Iteration 54, loss = 0.10461463\n",
      "Iteration 55, loss = 0.10312552\n",
      "Iteration 56, loss = 0.10115620\n",
      "Iteration 57, loss = 0.10004047\n",
      "Iteration 58, loss = 0.09828428\n",
      "Iteration 59, loss = 0.09694313\n",
      "Iteration 60, loss = 0.09554161\n",
      "Iteration 61, loss = 0.09434467\n",
      "Iteration 62, loss = 0.09349094\n",
      "Iteration 63, loss = 0.09219278\n",
      "Iteration 64, loss = 0.09105788\n",
      "Iteration 65, loss = 0.08985922\n",
      "Iteration 66, loss = 0.08888436\n",
      "Iteration 67, loss = 0.08788949\n",
      "Iteration 68, loss = 0.08699085\n",
      "Iteration 69, loss = 0.08622516\n",
      "Iteration 70, loss = 0.08519058\n",
      "Iteration 71, loss = 0.08436009\n",
      "Iteration 72, loss = 0.08368049\n",
      "Iteration 73, loss = 0.08275255\n",
      "Iteration 74, loss = 0.08211718\n",
      "Iteration 75, loss = 0.08136202\n",
      "Iteration 76, loss = 0.08064301\n",
      "Iteration 77, loss = 0.08003440\n",
      "Iteration 78, loss = 0.07934760\n",
      "Iteration 79, loss = 0.07882540\n",
      "Iteration 80, loss = 0.07820524\n",
      "Iteration 81, loss = 0.07760846\n",
      "Iteration 82, loss = 0.07704566\n",
      "Iteration 83, loss = 0.07650716\n",
      "Iteration 84, loss = 0.07601698\n",
      "Iteration 85, loss = 0.07551213\n",
      "Iteration 86, loss = 0.07504181\n",
      "Iteration 87, loss = 0.07459043\n",
      "Iteration 88, loss = 0.07416934\n",
      "Iteration 89, loss = 0.07370940\n",
      "Iteration 90, loss = 0.07331822\n",
      "Iteration 91, loss = 0.07292164\n",
      "Iteration 92, loss = 0.07255143\n",
      "Iteration 93, loss = 0.07212788\n",
      "Iteration 94, loss = 0.07176893\n",
      "Iteration 95, loss = 0.07135350\n",
      "Iteration 96, loss = 0.07104898\n",
      "Iteration 97, loss = 0.07064970\n",
      "Iteration 98, loss = 0.07033025\n",
      "Iteration 99, loss = 0.06998212\n",
      "Iteration 100, loss = 0.06966868\n",
      "Iteration 101, loss = 0.06933953\n",
      "Iteration 102, loss = 0.06915958\n",
      "Iteration 103, loss = 0.06879974\n",
      "Iteration 104, loss = 0.06853032\n",
      "Iteration 105, loss = 0.06828644\n",
      "Iteration 106, loss = 0.06812328\n",
      "Iteration 107, loss = 0.06777152\n",
      "Iteration 108, loss = 0.06751179\n",
      "Iteration 109, loss = 0.06725536\n",
      "Iteration 110, loss = 0.06702668\n",
      "Iteration 111, loss = 0.06678822\n",
      "Iteration 112, loss = 0.06656065\n",
      "Iteration 113, loss = 0.06632467\n",
      "Iteration 114, loss = 0.06613992\n",
      "Iteration 115, loss = 0.06586847\n",
      "Iteration 116, loss = 0.06569381\n",
      "Iteration 117, loss = 0.06548196\n",
      "Iteration 118, loss = 0.06525130\n",
      "Iteration 119, loss = 0.06505341\n",
      "Iteration 120, loss = 0.06487084\n",
      "Iteration 121, loss = 0.06468171\n",
      "Iteration 122, loss = 0.06451872\n",
      "Iteration 123, loss = 0.06432384\n",
      "Iteration 124, loss = 0.06415480\n",
      "Iteration 125, loss = 0.06399009\n",
      "Iteration 126, loss = 0.06383610\n",
      "Iteration 127, loss = 0.06370408\n",
      "Iteration 128, loss = 0.06351457\n",
      "Iteration 129, loss = 0.06338823\n",
      "Iteration 130, loss = 0.06322561\n",
      "Iteration 131, loss = 0.06308686\n",
      "Iteration 132, loss = 0.06291958\n",
      "Iteration 133, loss = 0.06277670\n",
      "Iteration 134, loss = 0.06265442\n",
      "Iteration 135, loss = 0.06250349\n",
      "Iteration 136, loss = 0.06236803\n",
      "Iteration 137, loss = 0.06221783\n",
      "Iteration 138, loss = 0.06209389\n",
      "Iteration 139, loss = 0.06197945\n",
      "Iteration 140, loss = 0.06186916\n",
      "Iteration 141, loss = 0.06172956\n",
      "Iteration 142, loss = 0.06159881\n",
      "Iteration 143, loss = 0.06149277\n",
      "Iteration 144, loss = 0.06138147\n",
      "Iteration 145, loss = 0.06125450\n",
      "Iteration 146, loss = 0.06113568\n",
      "Iteration 147, loss = 0.06103380\n",
      "Iteration 148, loss = 0.06093408\n",
      "Iteration 149, loss = 0.06084263\n",
      "Iteration 150, loss = 0.06075817\n",
      "Iteration 151, loss = 0.06063608\n",
      "Iteration 152, loss = 0.06051826\n",
      "Iteration 153, loss = 0.06040053\n",
      "Iteration 154, loss = 0.06031774\n",
      "Iteration 155, loss = 0.06021292\n",
      "Iteration 156, loss = 0.06013226\n",
      "Iteration 157, loss = 0.06003658\n",
      "Iteration 158, loss = 0.05992227\n",
      "Iteration 159, loss = 0.05982961\n",
      "Iteration 160, loss = 0.05973950\n",
      "Iteration 161, loss = 0.05964873\n",
      "Iteration 162, loss = 0.05956323\n",
      "Iteration 163, loss = 0.05948176\n",
      "Iteration 164, loss = 0.05941124\n",
      "Iteration 165, loss = 0.05931920\n",
      "Iteration 166, loss = 0.05923761\n",
      "Iteration 167, loss = 0.05915267\n",
      "Iteration 168, loss = 0.05907197\n",
      "Iteration 169, loss = 0.05899717\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33262858\n",
      "Iteration 2, loss = 2.18106124\n",
      "Iteration 3, loss = 2.00898377\n",
      "Iteration 4, loss = 1.79688091\n",
      "Iteration 5, loss = 1.55959891\n",
      "Iteration 6, loss = 1.32166641\n",
      "Iteration 7, loss = 1.11288733\n",
      "Iteration 8, loss = 0.94037408\n",
      "Iteration 9, loss = 0.80057329\n",
      "Iteration 10, loss = 0.69059294\n",
      "Iteration 11, loss = 0.60758911\n",
      "Iteration 12, loss = 0.54077134\n",
      "Iteration 13, loss = 0.48459320\n",
      "Iteration 14, loss = 0.44250426\n",
      "Iteration 15, loss = 0.40510609\n",
      "Iteration 16, loss = 0.37357114\n",
      "Iteration 17, loss = 0.34761159\n",
      "Iteration 18, loss = 0.32587116\n",
      "Iteration 19, loss = 0.30538930\n",
      "Iteration 20, loss = 0.28787718\n",
      "Iteration 21, loss = 0.27366037\n",
      "Iteration 22, loss = 0.25914630\n",
      "Iteration 23, loss = 0.24576678\n",
      "Iteration 24, loss = 0.23477949\n",
      "Iteration 25, loss = 0.22507958\n",
      "Iteration 26, loss = 0.21499139\n",
      "Iteration 27, loss = 0.20657006\n",
      "Iteration 28, loss = 0.19841961\n",
      "Iteration 29, loss = 0.19154452\n",
      "Iteration 30, loss = 0.18418813\n",
      "Iteration 31, loss = 0.17827728\n",
      "Iteration 32, loss = 0.17313658\n",
      "Iteration 33, loss = 0.16725891\n",
      "Iteration 34, loss = 0.16271006\n",
      "Iteration 35, loss = 0.15785105\n",
      "Iteration 36, loss = 0.15362112\n",
      "Iteration 37, loss = 0.14983376\n",
      "Iteration 38, loss = 0.14550622\n",
      "Iteration 39, loss = 0.14212676\n",
      "Iteration 40, loss = 0.13837225\n",
      "Iteration 41, loss = 0.13508855\n",
      "Iteration 42, loss = 0.13202018\n",
      "Iteration 43, loss = 0.12923771\n",
      "Iteration 44, loss = 0.12632773\n",
      "Iteration 45, loss = 0.12348907\n",
      "Iteration 46, loss = 0.12122059\n",
      "Iteration 47, loss = 0.11888019\n",
      "Iteration 48, loss = 0.11642194\n",
      "Iteration 49, loss = 0.11434671\n",
      "Iteration 50, loss = 0.11230780\n",
      "Iteration 51, loss = 0.11029983\n",
      "Iteration 52, loss = 0.10852147\n",
      "Iteration 53, loss = 0.10678849\n",
      "Iteration 54, loss = 0.10515899\n",
      "Iteration 55, loss = 0.10355139\n",
      "Iteration 56, loss = 0.10176489\n",
      "Iteration 57, loss = 0.10083587\n",
      "Iteration 58, loss = 0.09893247\n",
      "Iteration 59, loss = 0.09741701\n",
      "Iteration 60, loss = 0.09601144\n",
      "Iteration 61, loss = 0.09466941\n",
      "Iteration 62, loss = 0.09376575\n",
      "Iteration 63, loss = 0.09247624\n",
      "Iteration 64, loss = 0.09131446\n",
      "Iteration 65, loss = 0.09017540\n",
      "Iteration 66, loss = 0.08920364\n",
      "Iteration 67, loss = 0.08821368\n",
      "Iteration 68, loss = 0.08729468\n",
      "Iteration 69, loss = 0.08639248\n",
      "Iteration 70, loss = 0.08546264\n",
      "Iteration 71, loss = 0.08459040\n",
      "Iteration 72, loss = 0.08383538\n",
      "Iteration 73, loss = 0.08294671\n",
      "Iteration 74, loss = 0.08228510\n",
      "Iteration 75, loss = 0.08150547\n",
      "Iteration 76, loss = 0.08077507\n",
      "Iteration 77, loss = 0.08008480\n",
      "Iteration 78, loss = 0.07941591\n",
      "Iteration 79, loss = 0.07879692\n",
      "Iteration 80, loss = 0.07817625\n",
      "Iteration 81, loss = 0.07759399\n",
      "Iteration 82, loss = 0.07701354\n",
      "Iteration 83, loss = 0.07646912\n",
      "Iteration 84, loss = 0.07598212\n",
      "Iteration 85, loss = 0.07545354\n",
      "Iteration 86, loss = 0.07498246\n",
      "Iteration 87, loss = 0.07460449\n",
      "Iteration 88, loss = 0.07406183\n",
      "Iteration 89, loss = 0.07356996\n",
      "Iteration 90, loss = 0.07318877\n",
      "Iteration 91, loss = 0.07278342\n",
      "Iteration 92, loss = 0.07232704\n",
      "Iteration 93, loss = 0.07193743\n",
      "Iteration 94, loss = 0.07156802\n",
      "Iteration 95, loss = 0.07115183\n",
      "Iteration 96, loss = 0.07080531\n",
      "Iteration 97, loss = 0.07038105\n",
      "Iteration 98, loss = 0.07015960\n",
      "Iteration 99, loss = 0.06975541\n",
      "Iteration 100, loss = 0.06938377\n",
      "Iteration 101, loss = 0.06899847\n",
      "Iteration 102, loss = 0.06879780\n",
      "Iteration 103, loss = 0.06846405\n",
      "Iteration 104, loss = 0.06818185\n",
      "Iteration 105, loss = 0.06792448\n",
      "Iteration 106, loss = 0.06779292\n",
      "Iteration 107, loss = 0.06744544\n",
      "Iteration 108, loss = 0.06716908\n",
      "Iteration 109, loss = 0.06685917\n",
      "Iteration 110, loss = 0.06660209\n",
      "Iteration 111, loss = 0.06638251\n",
      "Iteration 112, loss = 0.06618243\n",
      "Iteration 113, loss = 0.06591220\n",
      "Iteration 114, loss = 0.06575019\n",
      "Iteration 115, loss = 0.06543331\n",
      "Iteration 116, loss = 0.06533453\n",
      "Iteration 117, loss = 0.06508491\n",
      "Iteration 118, loss = 0.06484206\n",
      "Iteration 119, loss = 0.06462661\n",
      "Iteration 120, loss = 0.06442875\n",
      "Iteration 121, loss = 0.06423041\n",
      "Iteration 122, loss = 0.06407982\n",
      "Iteration 123, loss = 0.06392043\n",
      "Iteration 124, loss = 0.06369548\n",
      "Iteration 125, loss = 0.06354539\n",
      "Iteration 126, loss = 0.06336988\n",
      "Iteration 127, loss = 0.06320470\n",
      "Iteration 128, loss = 0.06302168\n",
      "Iteration 129, loss = 0.06287795\n",
      "Iteration 130, loss = 0.06271897\n",
      "Iteration 131, loss = 0.06257615\n",
      "Iteration 132, loss = 0.06243327\n",
      "Iteration 133, loss = 0.06228908\n",
      "Iteration 134, loss = 0.06216573\n",
      "Iteration 135, loss = 0.06201812\n",
      "Iteration 136, loss = 0.06187995\n",
      "Iteration 137, loss = 0.06174259\n",
      "Iteration 138, loss = 0.06161147\n",
      "Iteration 139, loss = 0.06148309\n",
      "Iteration 140, loss = 0.06137513\n",
      "Iteration 141, loss = 0.06123011\n",
      "Iteration 142, loss = 0.06112585\n",
      "Iteration 143, loss = 0.06100842\n",
      "Iteration 144, loss = 0.06088193\n",
      "Iteration 145, loss = 0.06076935\n",
      "Iteration 146, loss = 0.06065016\n",
      "Iteration 147, loss = 0.06053433\n",
      "Iteration 148, loss = 0.06043507\n",
      "Iteration 149, loss = 0.06033710\n",
      "Iteration 150, loss = 0.06021952\n",
      "Iteration 151, loss = 0.06013500\n",
      "Iteration 152, loss = 0.06001675\n",
      "Iteration 153, loss = 0.05990049\n",
      "Iteration 154, loss = 0.05979689\n",
      "Iteration 155, loss = 0.05972675\n",
      "Iteration 156, loss = 0.05964099\n",
      "Iteration 157, loss = 0.05951268\n",
      "Iteration 158, loss = 0.05940635\n",
      "Iteration 159, loss = 0.05932348\n",
      "Iteration 160, loss = 0.05922654\n",
      "Iteration 161, loss = 0.05913499\n",
      "Iteration 162, loss = 0.05904658\n",
      "Iteration 163, loss = 0.05896805\n",
      "Iteration 164, loss = 0.05888960\n",
      "Iteration 165, loss = 0.05878032\n",
      "Iteration 166, loss = 0.05873071\n",
      "Iteration 167, loss = 0.05863576\n",
      "Iteration 168, loss = 0.05855765\n",
      "Iteration 169, loss = 0.05847936\n",
      "Iteration 170, loss = 0.05839536\n",
      "Iteration 171, loss = 0.05831193\n",
      "Iteration 172, loss = 0.05824478\n",
      "Iteration 173, loss = 0.05816955\n",
      "Iteration 174, loss = 0.05807799\n",
      "Iteration 175, loss = 0.05800949\n",
      "Iteration 176, loss = 0.05792780\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32955721\n",
      "Iteration 2, loss = 2.18204607\n",
      "Iteration 3, loss = 2.00807126\n",
      "Iteration 4, loss = 1.79516401\n",
      "Iteration 5, loss = 1.56486321\n",
      "Iteration 6, loss = 1.33895475\n",
      "Iteration 7, loss = 1.13385164\n",
      "Iteration 8, loss = 0.96323607\n",
      "Iteration 9, loss = 0.82284244\n",
      "Iteration 10, loss = 0.71173982\n",
      "Iteration 11, loss = 0.62423887\n",
      "Iteration 12, loss = 0.55647521\n",
      "Iteration 13, loss = 0.50157272\n",
      "Iteration 14, loss = 0.45473956\n",
      "Iteration 15, loss = 0.41663306\n",
      "Iteration 16, loss = 0.38504577\n",
      "Iteration 17, loss = 0.35801778\n",
      "Iteration 18, loss = 0.33478667\n",
      "Iteration 19, loss = 0.31434012\n",
      "Iteration 20, loss = 0.29704157\n",
      "Iteration 21, loss = 0.28107474\n",
      "Iteration 22, loss = 0.26667451\n",
      "Iteration 23, loss = 0.25431099\n",
      "Iteration 24, loss = 0.24196100\n",
      "Iteration 25, loss = 0.23102216\n",
      "Iteration 26, loss = 0.22211233\n",
      "Iteration 27, loss = 0.21256697\n",
      "Iteration 28, loss = 0.20451229\n",
      "Iteration 29, loss = 0.19677216\n",
      "Iteration 30, loss = 0.19027378\n",
      "Iteration 31, loss = 0.18414920\n",
      "Iteration 32, loss = 0.17770213\n",
      "Iteration 33, loss = 0.17243580\n",
      "Iteration 34, loss = 0.16704340\n",
      "Iteration 35, loss = 0.16235499\n",
      "Iteration 36, loss = 0.15757321\n",
      "Iteration 37, loss = 0.15307702\n",
      "Iteration 38, loss = 0.14865389\n",
      "Iteration 39, loss = 0.14519817\n",
      "Iteration 40, loss = 0.14164344\n",
      "Iteration 41, loss = 0.13862330\n",
      "Iteration 42, loss = 0.13509812\n",
      "Iteration 43, loss = 0.13162168\n",
      "Iteration 44, loss = 0.12855743\n",
      "Iteration 45, loss = 0.12591137\n",
      "Iteration 46, loss = 0.12354350\n",
      "Iteration 47, loss = 0.12087687\n",
      "Iteration 48, loss = 0.11835189\n",
      "Iteration 49, loss = 0.11594848\n",
      "Iteration 50, loss = 0.11434529\n",
      "Iteration 51, loss = 0.11181965\n",
      "Iteration 52, loss = 0.11019283\n",
      "Iteration 53, loss = 0.10808472\n",
      "Iteration 54, loss = 0.10645860\n",
      "Iteration 55, loss = 0.10458329\n",
      "Iteration 56, loss = 0.10285886\n",
      "Iteration 57, loss = 0.10155761\n",
      "Iteration 58, loss = 0.10006920\n",
      "Iteration 59, loss = 0.09865077\n",
      "Iteration 60, loss = 0.09738023\n",
      "Iteration 61, loss = 0.09590352\n",
      "Iteration 62, loss = 0.09479766\n",
      "Iteration 63, loss = 0.09346654\n",
      "Iteration 64, loss = 0.09247171\n",
      "Iteration 65, loss = 0.09132734\n",
      "Iteration 66, loss = 0.09034763\n",
      "Iteration 67, loss = 0.08925970\n",
      "Iteration 68, loss = 0.08828682\n",
      "Iteration 69, loss = 0.08747193\n",
      "Iteration 70, loss = 0.08644163\n",
      "Iteration 71, loss = 0.08572107\n",
      "Iteration 72, loss = 0.08481000\n",
      "Iteration 73, loss = 0.08399172\n",
      "Iteration 74, loss = 0.08334111\n",
      "Iteration 75, loss = 0.08246807\n",
      "Iteration 76, loss = 0.08168319\n",
      "Iteration 77, loss = 0.08094130\n",
      "Iteration 78, loss = 0.08047009\n",
      "Iteration 79, loss = 0.07971859\n",
      "Iteration 80, loss = 0.07913254\n",
      "Iteration 81, loss = 0.07843390\n",
      "Iteration 82, loss = 0.07785773\n",
      "Iteration 83, loss = 0.07736390\n",
      "Iteration 84, loss = 0.07676477\n",
      "Iteration 85, loss = 0.07630636\n",
      "Iteration 86, loss = 0.07577628\n",
      "Iteration 87, loss = 0.07527994\n",
      "Iteration 88, loss = 0.07482224\n",
      "Iteration 89, loss = 0.07437940\n",
      "Iteration 90, loss = 0.07391111\n",
      "Iteration 91, loss = 0.07350672\n",
      "Iteration 92, loss = 0.07310848\n",
      "Iteration 93, loss = 0.07271017\n",
      "Iteration 94, loss = 0.07235801\n",
      "Iteration 95, loss = 0.07198897\n",
      "Iteration 96, loss = 0.07161164\n",
      "Iteration 97, loss = 0.07127129\n",
      "Iteration 98, loss = 0.07090574\n",
      "Iteration 99, loss = 0.07057400\n",
      "Iteration 100, loss = 0.07028350\n",
      "Iteration 101, loss = 0.06995549\n",
      "Iteration 102, loss = 0.06970113\n",
      "Iteration 103, loss = 0.06934652\n",
      "Iteration 104, loss = 0.06908038\n",
      "Iteration 105, loss = 0.06882230\n",
      "Iteration 106, loss = 0.06851939\n",
      "Iteration 107, loss = 0.06824647\n",
      "Iteration 108, loss = 0.06801427\n",
      "Iteration 109, loss = 0.06779090\n",
      "Iteration 110, loss = 0.06751061\n",
      "Iteration 111, loss = 0.06727793\n",
      "Iteration 112, loss = 0.06704610\n",
      "Iteration 113, loss = 0.06678320\n",
      "Iteration 114, loss = 0.06656140\n",
      "Iteration 115, loss = 0.06633967\n",
      "Iteration 116, loss = 0.06611777\n",
      "Iteration 117, loss = 0.06592599\n",
      "Iteration 118, loss = 0.06572956\n",
      "Iteration 119, loss = 0.06551296\n",
      "Iteration 120, loss = 0.06532077\n",
      "Iteration 121, loss = 0.06510631\n",
      "Iteration 122, loss = 0.06492627\n",
      "Iteration 123, loss = 0.06474602\n",
      "Iteration 124, loss = 0.06457622\n",
      "Iteration 125, loss = 0.06441215\n",
      "Iteration 126, loss = 0.06424101\n",
      "Iteration 127, loss = 0.06411393\n",
      "Iteration 128, loss = 0.06392110\n",
      "Iteration 129, loss = 0.06374701\n",
      "Iteration 130, loss = 0.06357126\n",
      "Iteration 131, loss = 0.06343115\n",
      "Iteration 132, loss = 0.06328155\n",
      "Iteration 133, loss = 0.06313612\n",
      "Iteration 134, loss = 0.06300356\n",
      "Iteration 135, loss = 0.06283643\n",
      "Iteration 136, loss = 0.06269159\n",
      "Iteration 137, loss = 0.06255052\n",
      "Iteration 138, loss = 0.06242487\n",
      "Iteration 139, loss = 0.06230150\n",
      "Iteration 140, loss = 0.06218378\n",
      "Iteration 141, loss = 0.06203823\n",
      "Iteration 142, loss = 0.06191667\n",
      "Iteration 143, loss = 0.06178915\n",
      "Iteration 144, loss = 0.06169237\n",
      "Iteration 145, loss = 0.06154662\n",
      "Iteration 146, loss = 0.06143877\n",
      "Iteration 147, loss = 0.06132125\n",
      "Iteration 148, loss = 0.06122725\n",
      "Iteration 149, loss = 0.06111091\n",
      "Iteration 150, loss = 0.06102355\n",
      "Iteration 151, loss = 0.06091057\n",
      "Iteration 152, loss = 0.06081011\n",
      "Iteration 153, loss = 0.06069660\n",
      "Iteration 154, loss = 0.06058864\n",
      "Iteration 155, loss = 0.06047968\n",
      "Iteration 156, loss = 0.06037650\n",
      "Iteration 157, loss = 0.06027979\n",
      "Iteration 158, loss = 0.06018772\n",
      "Iteration 159, loss = 0.06009696\n",
      "Iteration 160, loss = 0.06000390\n",
      "Iteration 161, loss = 0.05990839\n",
      "Iteration 162, loss = 0.05982914\n",
      "Iteration 163, loss = 0.05972569\n",
      "Iteration 164, loss = 0.05963640\n",
      "Iteration 165, loss = 0.05955112\n",
      "Iteration 166, loss = 0.05945916\n",
      "Iteration 167, loss = 0.05938785\n",
      "Iteration 168, loss = 0.05930039\n",
      "Iteration 169, loss = 0.05922060\n",
      "Iteration 170, loss = 0.05912861\n",
      "Iteration 171, loss = 0.05907322\n",
      "Iteration 172, loss = 0.05898448\n",
      "Iteration 173, loss = 0.05891381\n",
      "Iteration 174, loss = 0.05883002\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32908319\n",
      "Iteration 2, loss = 2.18226997\n",
      "Iteration 3, loss = 2.01392733\n",
      "Iteration 4, loss = 1.81079370\n",
      "Iteration 5, loss = 1.58553257\n",
      "Iteration 6, loss = 1.36365075\n",
      "Iteration 7, loss = 1.15947897\n",
      "Iteration 8, loss = 0.98898993\n",
      "Iteration 9, loss = 0.84834126\n",
      "Iteration 10, loss = 0.73774316\n",
      "Iteration 11, loss = 0.64910706\n",
      "Iteration 12, loss = 0.57805197\n",
      "Iteration 13, loss = 0.52019268\n",
      "Iteration 14, loss = 0.47273308\n",
      "Iteration 15, loss = 0.43239837\n",
      "Iteration 16, loss = 0.39815620\n",
      "Iteration 17, loss = 0.36998804\n",
      "Iteration 18, loss = 0.34553642\n",
      "Iteration 19, loss = 0.32409794\n",
      "Iteration 20, loss = 0.30662316\n",
      "Iteration 21, loss = 0.28964261\n",
      "Iteration 22, loss = 0.27405583\n",
      "Iteration 23, loss = 0.26051255\n",
      "Iteration 24, loss = 0.24897844\n",
      "Iteration 25, loss = 0.23712633\n",
      "Iteration 26, loss = 0.22757374\n",
      "Iteration 27, loss = 0.21754961\n",
      "Iteration 28, loss = 0.20880483\n",
      "Iteration 29, loss = 0.20110349\n",
      "Iteration 30, loss = 0.19440258\n",
      "Iteration 31, loss = 0.18788316\n",
      "Iteration 32, loss = 0.18138214\n",
      "Iteration 33, loss = 0.17543353\n",
      "Iteration 34, loss = 0.16951246\n",
      "Iteration 35, loss = 0.16551980\n",
      "Iteration 36, loss = 0.16034715\n",
      "Iteration 37, loss = 0.15572276\n",
      "Iteration 38, loss = 0.15120888\n",
      "Iteration 39, loss = 0.14762774\n",
      "Iteration 40, loss = 0.14426039\n",
      "Iteration 41, loss = 0.14091208\n",
      "Iteration 42, loss = 0.13683483\n",
      "Iteration 43, loss = 0.13373335\n",
      "Iteration 44, loss = 0.13056851\n",
      "Iteration 45, loss = 0.12794319\n",
      "Iteration 46, loss = 0.12543459\n",
      "Iteration 47, loss = 0.12258737\n",
      "Iteration 48, loss = 0.12007780\n",
      "Iteration 49, loss = 0.11779941\n",
      "Iteration 50, loss = 0.11593245\n",
      "Iteration 51, loss = 0.11357153\n",
      "Iteration 52, loss = 0.11195471\n",
      "Iteration 53, loss = 0.10977416\n",
      "Iteration 54, loss = 0.10818733\n",
      "Iteration 55, loss = 0.10628376\n",
      "Iteration 56, loss = 0.10453578\n",
      "Iteration 57, loss = 0.10297597\n",
      "Iteration 58, loss = 0.10163236\n",
      "Iteration 59, loss = 0.10020578\n",
      "Iteration 60, loss = 0.09898780\n",
      "Iteration 61, loss = 0.09741652\n",
      "Iteration 62, loss = 0.09623234\n",
      "Iteration 63, loss = 0.09489948\n",
      "Iteration 64, loss = 0.09379297\n",
      "Iteration 65, loss = 0.09266919\n",
      "Iteration 66, loss = 0.09167200\n",
      "Iteration 67, loss = 0.09059982\n",
      "Iteration 68, loss = 0.08977499\n",
      "Iteration 69, loss = 0.08879185\n",
      "Iteration 70, loss = 0.08785942\n",
      "Iteration 71, loss = 0.08702150\n",
      "Iteration 72, loss = 0.08605910\n",
      "Iteration 73, loss = 0.08539149\n",
      "Iteration 74, loss = 0.08467687\n",
      "Iteration 75, loss = 0.08386578\n",
      "Iteration 76, loss = 0.08306479\n",
      "Iteration 77, loss = 0.08235406\n",
      "Iteration 78, loss = 0.08179024\n",
      "Iteration 79, loss = 0.08103611\n",
      "Iteration 80, loss = 0.08047444\n",
      "Iteration 81, loss = 0.07976315\n",
      "Iteration 82, loss = 0.07912892\n",
      "Iteration 83, loss = 0.07861180\n",
      "Iteration 84, loss = 0.07804817\n",
      "Iteration 85, loss = 0.07755870\n",
      "Iteration 86, loss = 0.07705167\n",
      "Iteration 87, loss = 0.07653832\n",
      "Iteration 88, loss = 0.07614821\n",
      "Iteration 89, loss = 0.07563313\n",
      "Iteration 90, loss = 0.07515550\n",
      "Iteration 91, loss = 0.07475560\n",
      "Iteration 92, loss = 0.07436959\n",
      "Iteration 93, loss = 0.07395252\n",
      "Iteration 94, loss = 0.07356183\n",
      "Iteration 95, loss = 0.07319931\n",
      "Iteration 96, loss = 0.07280383\n",
      "Iteration 97, loss = 0.07244392\n",
      "Iteration 98, loss = 0.07207219\n",
      "Iteration 99, loss = 0.07184662\n",
      "Iteration 100, loss = 0.07146587\n",
      "Iteration 101, loss = 0.07113696\n",
      "Iteration 102, loss = 0.07087751\n",
      "Iteration 103, loss = 0.07050697\n",
      "Iteration 104, loss = 0.07020830\n",
      "Iteration 105, loss = 0.06996951\n",
      "Iteration 106, loss = 0.06966426\n",
      "Iteration 107, loss = 0.06940020\n",
      "Iteration 108, loss = 0.06915861\n",
      "Iteration 109, loss = 0.06891997\n",
      "Iteration 110, loss = 0.06861334\n",
      "Iteration 111, loss = 0.06838852\n",
      "Iteration 112, loss = 0.06815226\n",
      "Iteration 113, loss = 0.06789663\n",
      "Iteration 114, loss = 0.06766933\n",
      "Iteration 115, loss = 0.06745428\n",
      "Iteration 116, loss = 0.06721042\n",
      "Iteration 117, loss = 0.06702576\n",
      "Iteration 118, loss = 0.06680687\n",
      "Iteration 119, loss = 0.06660776\n",
      "Iteration 120, loss = 0.06640841\n",
      "Iteration 121, loss = 0.06618761\n",
      "Iteration 122, loss = 0.06602384\n",
      "Iteration 123, loss = 0.06583502\n",
      "Iteration 124, loss = 0.06565029\n",
      "Iteration 125, loss = 0.06549376\n",
      "Iteration 126, loss = 0.06530905\n",
      "Iteration 127, loss = 0.06518783\n",
      "Iteration 128, loss = 0.06496672\n",
      "Iteration 129, loss = 0.06478962\n",
      "Iteration 130, loss = 0.06461938\n",
      "Iteration 131, loss = 0.06447167\n",
      "Iteration 132, loss = 0.06433259\n",
      "Iteration 133, loss = 0.06417580\n",
      "Iteration 134, loss = 0.06403419\n",
      "Iteration 135, loss = 0.06386699\n",
      "Iteration 136, loss = 0.06372306\n",
      "Iteration 137, loss = 0.06357289\n",
      "Iteration 138, loss = 0.06344629\n",
      "Iteration 139, loss = 0.06330665\n",
      "Iteration 140, loss = 0.06317994\n",
      "Iteration 141, loss = 0.06303639\n",
      "Iteration 142, loss = 0.06292211\n",
      "Iteration 143, loss = 0.06280088\n",
      "Iteration 144, loss = 0.06271213\n",
      "Iteration 145, loss = 0.06253145\n",
      "Iteration 146, loss = 0.06243846\n",
      "Iteration 147, loss = 0.06230186\n",
      "Iteration 148, loss = 0.06221029\n",
      "Iteration 149, loss = 0.06208132\n",
      "Iteration 150, loss = 0.06198864\n",
      "Iteration 151, loss = 0.06188612\n",
      "Iteration 152, loss = 0.06177583\n",
      "Iteration 153, loss = 0.06165017\n",
      "Iteration 154, loss = 0.06155524\n",
      "Iteration 155, loss = 0.06143683\n",
      "Iteration 156, loss = 0.06133021\n",
      "Iteration 157, loss = 0.06123732\n",
      "Iteration 158, loss = 0.06113811\n",
      "Iteration 159, loss = 0.06104042\n",
      "Iteration 160, loss = 0.06096403\n",
      "Iteration 161, loss = 0.06085942\n",
      "Iteration 162, loss = 0.06076531\n",
      "Iteration 163, loss = 0.06067971\n",
      "Iteration 164, loss = 0.06058305\n",
      "Iteration 165, loss = 0.06050271\n",
      "Iteration 166, loss = 0.06040741\n",
      "Iteration 167, loss = 0.06032533\n",
      "Iteration 168, loss = 0.06022918\n",
      "Iteration 169, loss = 0.06015946\n",
      "Iteration 170, loss = 0.06007056\n",
      "Iteration 171, loss = 0.06000531\n",
      "Iteration 172, loss = 0.05990568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33194759\n",
      "Iteration 2, loss = 2.18291531\n",
      "Iteration 3, loss = 2.00742693\n",
      "Iteration 4, loss = 1.79794151\n",
      "Iteration 5, loss = 1.56217263\n",
      "Iteration 6, loss = 1.33069594\n",
      "Iteration 7, loss = 1.11884919\n",
      "Iteration 8, loss = 0.94368483\n",
      "Iteration 9, loss = 0.80104327\n",
      "Iteration 10, loss = 0.68910465\n",
      "Iteration 11, loss = 0.60093255\n",
      "Iteration 12, loss = 0.53079661\n",
      "Iteration 13, loss = 0.47393754\n",
      "Iteration 14, loss = 0.43120034\n",
      "Iteration 15, loss = 0.39049152\n",
      "Iteration 16, loss = 0.35796939\n",
      "Iteration 17, loss = 0.33138876\n",
      "Iteration 18, loss = 0.30847115\n",
      "Iteration 19, loss = 0.28870325\n",
      "Iteration 20, loss = 0.27232027\n",
      "Iteration 21, loss = 0.25582418\n",
      "Iteration 22, loss = 0.24197719\n",
      "Iteration 23, loss = 0.22998556\n",
      "Iteration 24, loss = 0.21965433\n",
      "Iteration 25, loss = 0.20877210\n",
      "Iteration 26, loss = 0.20022259\n",
      "Iteration 27, loss = 0.19097196\n",
      "Iteration 28, loss = 0.18332481\n",
      "Iteration 29, loss = 0.17672547\n",
      "Iteration 30, loss = 0.17063596\n",
      "Iteration 31, loss = 0.16480072\n",
      "Iteration 32, loss = 0.15913355\n",
      "Iteration 33, loss = 0.15429069\n",
      "Iteration 34, loss = 0.14979795\n",
      "Iteration 35, loss = 0.14551474\n",
      "Iteration 36, loss = 0.14185242\n",
      "Iteration 37, loss = 0.13781960\n",
      "Iteration 38, loss = 0.13406362\n",
      "Iteration 39, loss = 0.13090135\n",
      "Iteration 40, loss = 0.12758532\n",
      "Iteration 41, loss = 0.12463709\n",
      "Iteration 42, loss = 0.12140788\n",
      "Iteration 43, loss = 0.11933986\n",
      "Iteration 44, loss = 0.11631925\n",
      "Iteration 45, loss = 0.11398031\n",
      "Iteration 46, loss = 0.11166806\n",
      "Iteration 47, loss = 0.10936860\n",
      "Iteration 48, loss = 0.10742357\n",
      "Iteration 49, loss = 0.10552756\n",
      "Iteration 50, loss = 0.10383832\n",
      "Iteration 51, loss = 0.10223042\n",
      "Iteration 52, loss = 0.10060037\n",
      "Iteration 53, loss = 0.09884849\n",
      "Iteration 54, loss = 0.09733576\n",
      "Iteration 55, loss = 0.09609124\n",
      "Iteration 56, loss = 0.09461532\n",
      "Iteration 57, loss = 0.09340055\n",
      "Iteration 58, loss = 0.09218955\n",
      "Iteration 59, loss = 0.09103880\n",
      "Iteration 60, loss = 0.09000232\n",
      "Iteration 61, loss = 0.08879639\n",
      "Iteration 62, loss = 0.08769460\n",
      "Iteration 63, loss = 0.08668739\n",
      "Iteration 64, loss = 0.08589316\n",
      "Iteration 65, loss = 0.08490988\n",
      "Iteration 66, loss = 0.08387557\n",
      "Iteration 67, loss = 0.08308955\n",
      "Iteration 68, loss = 0.08240533\n",
      "Iteration 69, loss = 0.08158979\n",
      "Iteration 70, loss = 0.08093288\n",
      "Iteration 71, loss = 0.08013465\n",
      "Iteration 72, loss = 0.07940009\n",
      "Iteration 73, loss = 0.07877235\n",
      "Iteration 74, loss = 0.07821097\n",
      "Iteration 75, loss = 0.07750946\n",
      "Iteration 76, loss = 0.07688457\n",
      "Iteration 77, loss = 0.07626137\n",
      "Iteration 78, loss = 0.07579462\n",
      "Iteration 79, loss = 0.07525763\n",
      "Iteration 80, loss = 0.07471116\n",
      "Iteration 81, loss = 0.07418702\n",
      "Iteration 82, loss = 0.07373143\n",
      "Iteration 83, loss = 0.07327247\n",
      "Iteration 84, loss = 0.07280899\n",
      "Iteration 85, loss = 0.07235681\n",
      "Iteration 86, loss = 0.07198777\n",
      "Iteration 87, loss = 0.07156166\n",
      "Iteration 88, loss = 0.07117509\n",
      "Iteration 89, loss = 0.07075719\n",
      "Iteration 90, loss = 0.07035585\n",
      "Iteration 91, loss = 0.07001660\n",
      "Iteration 92, loss = 0.06966170\n",
      "Iteration 93, loss = 0.06929722\n",
      "Iteration 94, loss = 0.06899057\n",
      "Iteration 95, loss = 0.06867165\n",
      "Iteration 96, loss = 0.06841007\n",
      "Iteration 97, loss = 0.06803549\n",
      "Iteration 98, loss = 0.06776664\n",
      "Iteration 99, loss = 0.06755271\n",
      "Iteration 100, loss = 0.06723658\n",
      "Iteration 101, loss = 0.06693849\n",
      "Iteration 102, loss = 0.06667669\n",
      "Iteration 103, loss = 0.06642463\n",
      "Iteration 104, loss = 0.06617570\n",
      "Iteration 105, loss = 0.06595482\n",
      "Iteration 106, loss = 0.06571817\n",
      "Iteration 107, loss = 0.06548620\n",
      "Iteration 108, loss = 0.06528286\n",
      "Iteration 109, loss = 0.06507613\n",
      "Iteration 110, loss = 0.06482205\n",
      "Iteration 111, loss = 0.06462885\n",
      "Iteration 112, loss = 0.06443427\n",
      "Iteration 113, loss = 0.06420209\n",
      "Iteration 114, loss = 0.06400404\n",
      "Iteration 115, loss = 0.06381991\n",
      "Iteration 116, loss = 0.06363334\n",
      "Iteration 117, loss = 0.06345070\n",
      "Iteration 118, loss = 0.06329676\n",
      "Iteration 119, loss = 0.06311615\n",
      "Iteration 120, loss = 0.06295587\n",
      "Iteration 121, loss = 0.06278596\n",
      "Iteration 122, loss = 0.06262206\n",
      "Iteration 123, loss = 0.06246119\n",
      "Iteration 124, loss = 0.06230885\n",
      "Iteration 125, loss = 0.06216515\n",
      "Iteration 126, loss = 0.06200480\n",
      "Iteration 127, loss = 0.06187550\n",
      "Iteration 128, loss = 0.06171233\n",
      "Iteration 129, loss = 0.06158275\n",
      "Iteration 130, loss = 0.06143570\n",
      "Iteration 131, loss = 0.06129324\n",
      "Iteration 132, loss = 0.06117497\n",
      "Iteration 133, loss = 0.06104459\n",
      "Iteration 134, loss = 0.06091002\n",
      "Iteration 135, loss = 0.06077930\n",
      "Iteration 136, loss = 0.06064729\n",
      "Iteration 137, loss = 0.06052800\n",
      "Iteration 138, loss = 0.06042031\n",
      "Iteration 139, loss = 0.06030258\n",
      "Iteration 140, loss = 0.06018582\n",
      "Iteration 141, loss = 0.06006847\n",
      "Iteration 142, loss = 0.05996276\n",
      "Iteration 143, loss = 0.05984919\n",
      "Iteration 144, loss = 0.05977054\n",
      "Iteration 145, loss = 0.05962552\n",
      "Iteration 146, loss = 0.05953723\n",
      "Iteration 147, loss = 0.05942482\n",
      "Iteration 148, loss = 0.05932030\n",
      "Iteration 149, loss = 0.05922168\n",
      "Iteration 150, loss = 0.05912892\n",
      "Iteration 151, loss = 0.05903777\n",
      "Iteration 152, loss = 0.05894620\n",
      "Iteration 153, loss = 0.05885226\n",
      "Iteration 154, loss = 0.05875403\n",
      "Iteration 155, loss = 0.05867034\n",
      "Iteration 156, loss = 0.05856656\n",
      "Iteration 157, loss = 0.05849368\n",
      "Iteration 158, loss = 0.05841848\n",
      "Iteration 159, loss = 0.05832577\n",
      "Iteration 160, loss = 0.05825397\n",
      "Iteration 161, loss = 0.05816012\n",
      "Iteration 162, loss = 0.05807875\n",
      "Iteration 163, loss = 0.05799633\n",
      "Iteration 164, loss = 0.05791548\n",
      "Iteration 165, loss = 0.05783801\n",
      "Iteration 166, loss = 0.05775466\n",
      "Iteration 167, loss = 0.05767336\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28214371\n",
      "Iteration 2, loss = 2.01567275\n",
      "Iteration 3, loss = 1.74389109\n",
      "Iteration 4, loss = 1.44957913\n",
      "Iteration 5, loss = 1.17958947\n",
      "Iteration 6, loss = 0.95144080\n",
      "Iteration 7, loss = 0.77107850\n",
      "Iteration 8, loss = 0.63281309\n",
      "Iteration 9, loss = 0.52882673\n",
      "Iteration 10, loss = 0.44717998\n",
      "Iteration 11, loss = 0.38693908\n",
      "Iteration 12, loss = 0.33900404\n",
      "Iteration 13, loss = 0.29962257\n",
      "Iteration 14, loss = 0.26820188\n",
      "Iteration 15, loss = 0.24270184\n",
      "Iteration 16, loss = 0.22064324\n",
      "Iteration 17, loss = 0.20255282\n",
      "Iteration 18, loss = 0.18686870\n",
      "Iteration 19, loss = 0.17310761\n",
      "Iteration 20, loss = 0.16176686\n",
      "Iteration 21, loss = 0.15159544\n",
      "Iteration 22, loss = 0.14300043\n",
      "Iteration 23, loss = 0.13484123\n",
      "Iteration 24, loss = 0.12766707\n",
      "Iteration 25, loss = 0.12197174\n",
      "Iteration 26, loss = 0.11640472\n",
      "Iteration 27, loss = 0.11163539\n",
      "Iteration 28, loss = 0.10706393\n",
      "Iteration 29, loss = 0.10323902\n",
      "Iteration 30, loss = 0.09964244\n",
      "Iteration 31, loss = 0.09655384\n",
      "Iteration 32, loss = 0.09386106\n",
      "Iteration 33, loss = 0.09112619\n",
      "Iteration 34, loss = 0.08894672\n",
      "Iteration 35, loss = 0.08690635\n",
      "Iteration 36, loss = 0.08489090\n",
      "Iteration 37, loss = 0.08296759\n",
      "Iteration 38, loss = 0.08121538\n",
      "Iteration 39, loss = 0.07956360\n",
      "Iteration 40, loss = 0.07817112\n",
      "Iteration 41, loss = 0.07676517\n",
      "Iteration 42, loss = 0.07556742\n",
      "Iteration 43, loss = 0.07444284\n",
      "Iteration 44, loss = 0.07336145\n",
      "Iteration 45, loss = 0.07232078\n",
      "Iteration 46, loss = 0.07140234\n",
      "Iteration 47, loss = 0.07050298\n",
      "Iteration 48, loss = 0.06966339\n",
      "Iteration 49, loss = 0.06883441\n",
      "Iteration 50, loss = 0.06809501\n",
      "Iteration 51, loss = 0.06740065\n",
      "Iteration 52, loss = 0.06673304\n",
      "Iteration 53, loss = 0.06610011\n",
      "Iteration 54, loss = 0.06545681\n",
      "Iteration 55, loss = 0.06487256\n",
      "Iteration 56, loss = 0.06426907\n",
      "Iteration 57, loss = 0.06375639\n",
      "Iteration 58, loss = 0.06320237\n",
      "Iteration 59, loss = 0.06268655\n",
      "Iteration 60, loss = 0.06217731\n",
      "Iteration 61, loss = 0.06173137\n",
      "Iteration 62, loss = 0.06130889\n",
      "Iteration 63, loss = 0.06086186\n",
      "Iteration 64, loss = 0.06042981\n",
      "Iteration 65, loss = 0.05998461\n",
      "Iteration 66, loss = 0.05960849\n",
      "Iteration 67, loss = 0.05920721\n",
      "Iteration 68, loss = 0.05881210\n",
      "Iteration 69, loss = 0.05846263\n",
      "Iteration 70, loss = 0.05807675\n",
      "Iteration 71, loss = 0.05773683\n",
      "Iteration 72, loss = 0.05742587\n",
      "Iteration 73, loss = 0.05706076\n",
      "Iteration 74, loss = 0.05675594\n",
      "Iteration 75, loss = 0.05642927\n",
      "Iteration 76, loss = 0.05612185\n",
      "Iteration 77, loss = 0.05582751\n",
      "Iteration 78, loss = 0.05550757\n",
      "Iteration 79, loss = 0.05524814\n",
      "Iteration 80, loss = 0.05496223\n",
      "Iteration 81, loss = 0.05466918\n",
      "Iteration 82, loss = 0.05439375\n",
      "Iteration 83, loss = 0.05413555\n",
      "Iteration 84, loss = 0.05387528\n",
      "Iteration 85, loss = 0.05362132\n",
      "Iteration 86, loss = 0.05337166\n",
      "Iteration 87, loss = 0.05313485\n",
      "Iteration 88, loss = 0.05289750\n",
      "Iteration 89, loss = 0.05265915\n",
      "Iteration 90, loss = 0.05244286\n",
      "Iteration 91, loss = 0.05221121\n",
      "Iteration 92, loss = 0.05200335\n",
      "Iteration 93, loss = 0.05177012\n",
      "Iteration 94, loss = 0.05155435\n",
      "Iteration 95, loss = 0.05132568\n",
      "Iteration 96, loss = 0.05113340\n",
      "Iteration 97, loss = 0.05091301\n",
      "Iteration 98, loss = 0.05071388\n",
      "Iteration 99, loss = 0.05050376\n",
      "Iteration 100, loss = 0.05031327\n",
      "Iteration 101, loss = 0.05010742\n",
      "Iteration 102, loss = 0.04996848\n",
      "Iteration 103, loss = 0.04974956\n",
      "Iteration 104, loss = 0.04956276\n",
      "Iteration 105, loss = 0.04939105\n",
      "Iteration 106, loss = 0.04924095\n",
      "Iteration 107, loss = 0.04903984\n",
      "Iteration 108, loss = 0.04886009\n",
      "Iteration 109, loss = 0.04868524\n",
      "Iteration 110, loss = 0.04852352\n",
      "Iteration 111, loss = 0.04835380\n",
      "Iteration 112, loss = 0.04819765\n",
      "Iteration 113, loss = 0.04802988\n",
      "Iteration 114, loss = 0.04787532\n",
      "Iteration 115, loss = 0.04770489\n",
      "Iteration 116, loss = 0.04756751\n",
      "Iteration 117, loss = 0.04742133\n",
      "Iteration 118, loss = 0.04726181\n",
      "Iteration 119, loss = 0.04711768\n",
      "Iteration 120, loss = 0.04696787\n",
      "Iteration 121, loss = 0.04683151\n",
      "Iteration 122, loss = 0.04669383\n",
      "Iteration 123, loss = 0.04654631\n",
      "Iteration 124, loss = 0.04641371\n",
      "Iteration 125, loss = 0.04628231\n",
      "Iteration 126, loss = 0.04615878\n",
      "Iteration 127, loss = 0.04604453\n",
      "Iteration 128, loss = 0.04589870\n",
      "Iteration 129, loss = 0.04579801\n",
      "Iteration 130, loss = 0.04566887\n",
      "Iteration 131, loss = 0.04553768\n",
      "Iteration 132, loss = 0.04541661\n",
      "Iteration 133, loss = 0.04530482\n",
      "Iteration 134, loss = 0.04518968\n",
      "Iteration 135, loss = 0.04506917\n",
      "Iteration 136, loss = 0.04494537\n",
      "Iteration 137, loss = 0.04482967\n",
      "Iteration 138, loss = 0.04473258\n",
      "Iteration 139, loss = 0.04462445\n",
      "Iteration 140, loss = 0.04452024\n",
      "Iteration 141, loss = 0.04440669\n",
      "Iteration 142, loss = 0.04429654\n",
      "Iteration 143, loss = 0.04420783\n",
      "Iteration 144, loss = 0.04411828\n",
      "Iteration 145, loss = 0.04400312\n",
      "Iteration 146, loss = 0.04389860\n",
      "Iteration 147, loss = 0.04381511\n",
      "Iteration 148, loss = 0.04372990\n",
      "Iteration 149, loss = 0.04366176\n",
      "Iteration 150, loss = 0.04357547\n",
      "Iteration 151, loss = 0.04345339\n",
      "Iteration 152, loss = 0.04336769\n",
      "Iteration 153, loss = 0.04327609\n",
      "Iteration 154, loss = 0.04320761\n",
      "Iteration 155, loss = 0.04311116\n",
      "Iteration 156, loss = 0.04305094\n",
      "Iteration 157, loss = 0.04293957\n",
      "Iteration 158, loss = 0.04285688\n",
      "Iteration 159, loss = 0.04277200\n",
      "Iteration 160, loss = 0.04267831\n",
      "Iteration 161, loss = 0.04261187\n",
      "Iteration 162, loss = 0.04252126\n",
      "Iteration 163, loss = 0.04244580\n",
      "Iteration 164, loss = 0.04240654\n",
      "Iteration 165, loss = 0.04232597\n",
      "Iteration 166, loss = 0.04225134\n",
      "Iteration 167, loss = 0.04218195\n",
      "Iteration 168, loss = 0.04210655\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28019680\n",
      "Iteration 2, loss = 2.01539987\n",
      "Iteration 3, loss = 1.74641223\n",
      "Iteration 4, loss = 1.45455042\n",
      "Iteration 5, loss = 1.18213876\n",
      "Iteration 6, loss = 0.94812362\n",
      "Iteration 7, loss = 0.76437842\n",
      "Iteration 8, loss = 0.62542145\n",
      "Iteration 9, loss = 0.51989469\n",
      "Iteration 10, loss = 0.44070307\n",
      "Iteration 11, loss = 0.38020405\n",
      "Iteration 12, loss = 0.33304532\n",
      "Iteration 13, loss = 0.29450367\n",
      "Iteration 14, loss = 0.26360843\n",
      "Iteration 15, loss = 0.23828736\n",
      "Iteration 16, loss = 0.21666380\n",
      "Iteration 17, loss = 0.19891437\n",
      "Iteration 18, loss = 0.18352394\n",
      "Iteration 19, loss = 0.17000888\n",
      "Iteration 20, loss = 0.15858289\n",
      "Iteration 21, loss = 0.14903536\n",
      "Iteration 22, loss = 0.14065901\n",
      "Iteration 23, loss = 0.13252825\n",
      "Iteration 24, loss = 0.12565110\n",
      "Iteration 25, loss = 0.11974979\n",
      "Iteration 26, loss = 0.11433720\n",
      "Iteration 27, loss = 0.10955953\n",
      "Iteration 28, loss = 0.10518002\n",
      "Iteration 29, loss = 0.10143013\n",
      "Iteration 30, loss = 0.09781190\n",
      "Iteration 31, loss = 0.09481682\n",
      "Iteration 32, loss = 0.09216412\n",
      "Iteration 33, loss = 0.08953688\n",
      "Iteration 34, loss = 0.08725431\n",
      "Iteration 35, loss = 0.08518746\n",
      "Iteration 36, loss = 0.08326311\n",
      "Iteration 37, loss = 0.08136451\n",
      "Iteration 38, loss = 0.07976212\n",
      "Iteration 39, loss = 0.07810975\n",
      "Iteration 40, loss = 0.07680377\n",
      "Iteration 41, loss = 0.07543785\n",
      "Iteration 42, loss = 0.07421322\n",
      "Iteration 43, loss = 0.07312528\n",
      "Iteration 44, loss = 0.07203886\n",
      "Iteration 45, loss = 0.07101008\n",
      "Iteration 46, loss = 0.07011961\n",
      "Iteration 47, loss = 0.06925742\n",
      "Iteration 48, loss = 0.06842499\n",
      "Iteration 49, loss = 0.06763912\n",
      "Iteration 50, loss = 0.06690555\n",
      "Iteration 51, loss = 0.06620707\n",
      "Iteration 52, loss = 0.06552429\n",
      "Iteration 53, loss = 0.06489012\n",
      "Iteration 54, loss = 0.06432663\n",
      "Iteration 55, loss = 0.06372811\n",
      "Iteration 56, loss = 0.06311755\n",
      "Iteration 57, loss = 0.06265394\n",
      "Iteration 58, loss = 0.06210746\n",
      "Iteration 59, loss = 0.06157248\n",
      "Iteration 60, loss = 0.06107898\n",
      "Iteration 61, loss = 0.06062430\n",
      "Iteration 62, loss = 0.06021763\n",
      "Iteration 63, loss = 0.05976955\n",
      "Iteration 64, loss = 0.05935305\n",
      "Iteration 65, loss = 0.05893184\n",
      "Iteration 66, loss = 0.05856364\n",
      "Iteration 67, loss = 0.05818822\n",
      "Iteration 68, loss = 0.05780216\n",
      "Iteration 69, loss = 0.05743235\n",
      "Iteration 70, loss = 0.05708090\n",
      "Iteration 71, loss = 0.05673635\n",
      "Iteration 72, loss = 0.05642225\n",
      "Iteration 73, loss = 0.05607254\n",
      "Iteration 74, loss = 0.05576779\n",
      "Iteration 75, loss = 0.05545674\n",
      "Iteration 76, loss = 0.05515020\n",
      "Iteration 77, loss = 0.05486576\n",
      "Iteration 78, loss = 0.05455919\n",
      "Iteration 79, loss = 0.05427275\n",
      "Iteration 80, loss = 0.05399111\n",
      "Iteration 81, loss = 0.05371396\n",
      "Iteration 82, loss = 0.05345002\n",
      "Iteration 83, loss = 0.05319667\n",
      "Iteration 84, loss = 0.05294439\n",
      "Iteration 85, loss = 0.05268467\n",
      "Iteration 86, loss = 0.05243852\n",
      "Iteration 87, loss = 0.05221549\n",
      "Iteration 88, loss = 0.05196808\n",
      "Iteration 89, loss = 0.05172799\n",
      "Iteration 90, loss = 0.05151650\n",
      "Iteration 91, loss = 0.05129718\n",
      "Iteration 92, loss = 0.05106716\n",
      "Iteration 93, loss = 0.05084978\n",
      "Iteration 94, loss = 0.05064729\n",
      "Iteration 95, loss = 0.05042579\n",
      "Iteration 96, loss = 0.05022924\n",
      "Iteration 97, loss = 0.05001447\n",
      "Iteration 98, loss = 0.04984437\n",
      "Iteration 99, loss = 0.04963129\n",
      "Iteration 100, loss = 0.04943335\n",
      "Iteration 101, loss = 0.04922083\n",
      "Iteration 102, loss = 0.04908281\n",
      "Iteration 103, loss = 0.04887506\n",
      "Iteration 104, loss = 0.04869732\n",
      "Iteration 105, loss = 0.04852490\n",
      "Iteration 106, loss = 0.04838833\n",
      "Iteration 107, loss = 0.04818055\n",
      "Iteration 108, loss = 0.04800216\n",
      "Iteration 109, loss = 0.04782197\n",
      "Iteration 110, loss = 0.04765182\n",
      "Iteration 111, loss = 0.04749176\n",
      "Iteration 112, loss = 0.04734793\n",
      "Iteration 113, loss = 0.04717514\n",
      "Iteration 114, loss = 0.04704148\n",
      "Iteration 115, loss = 0.04685073\n",
      "Iteration 116, loss = 0.04675850\n",
      "Iteration 117, loss = 0.04660457\n",
      "Iteration 118, loss = 0.04644463\n",
      "Iteration 119, loss = 0.04629918\n",
      "Iteration 120, loss = 0.04614745\n",
      "Iteration 121, loss = 0.04599586\n",
      "Iteration 122, loss = 0.04588604\n",
      "Iteration 123, loss = 0.04575667\n",
      "Iteration 124, loss = 0.04559217\n",
      "Iteration 125, loss = 0.04548209\n",
      "Iteration 126, loss = 0.04535382\n",
      "Iteration 127, loss = 0.04522377\n",
      "Iteration 128, loss = 0.04509396\n",
      "Iteration 129, loss = 0.04498947\n",
      "Iteration 130, loss = 0.04486421\n",
      "Iteration 131, loss = 0.04474321\n",
      "Iteration 132, loss = 0.04463565\n",
      "Iteration 133, loss = 0.04451812\n",
      "Iteration 134, loss = 0.04440212\n",
      "Iteration 135, loss = 0.04430071\n",
      "Iteration 136, loss = 0.04417603\n",
      "Iteration 137, loss = 0.04405537\n",
      "Iteration 138, loss = 0.04395321\n",
      "Iteration 139, loss = 0.04384740\n",
      "Iteration 140, loss = 0.04376303\n",
      "Iteration 141, loss = 0.04364007\n",
      "Iteration 142, loss = 0.04354111\n",
      "Iteration 143, loss = 0.04345285\n",
      "Iteration 144, loss = 0.04336291\n",
      "Iteration 145, loss = 0.04325198\n",
      "Iteration 146, loss = 0.04314620\n",
      "Iteration 147, loss = 0.04305596\n",
      "Iteration 148, loss = 0.04298228\n",
      "Iteration 149, loss = 0.04290873\n",
      "Iteration 150, loss = 0.04278947\n",
      "Iteration 151, loss = 0.04270668\n",
      "Iteration 152, loss = 0.04261845\n",
      "Iteration 153, loss = 0.04252745\n",
      "Iteration 154, loss = 0.04244287\n",
      "Iteration 155, loss = 0.04238632\n",
      "Iteration 156, loss = 0.04229211\n",
      "Iteration 157, loss = 0.04217437\n",
      "Iteration 158, loss = 0.04211049\n",
      "Iteration 159, loss = 0.04204298\n",
      "Iteration 160, loss = 0.04195462\n",
      "Iteration 161, loss = 0.04187658\n",
      "Iteration 162, loss = 0.04178556\n",
      "Iteration 163, loss = 0.04171279\n",
      "Iteration 164, loss = 0.04166214\n",
      "Iteration 165, loss = 0.04155671\n",
      "Iteration 166, loss = 0.04153186\n",
      "Iteration 167, loss = 0.04144288\n",
      "Iteration 168, loss = 0.04137283\n",
      "Iteration 169, loss = 0.04130914\n",
      "Iteration 170, loss = 0.04122971\n",
      "Iteration 171, loss = 0.04116129\n",
      "Iteration 172, loss = 0.04110756\n",
      "Iteration 173, loss = 0.04103748\n",
      "Iteration 174, loss = 0.04095401\n",
      "Iteration 175, loss = 0.04090832\n",
      "Iteration 176, loss = 0.04083834\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27252664\n",
      "Iteration 2, loss = 2.00559570\n",
      "Iteration 3, loss = 1.73074742\n",
      "Iteration 4, loss = 1.43811128\n",
      "Iteration 5, loss = 1.17159154\n",
      "Iteration 6, loss = 0.94508216\n",
      "Iteration 7, loss = 0.76660620\n",
      "Iteration 8, loss = 0.62950552\n",
      "Iteration 9, loss = 0.52530057\n",
      "Iteration 10, loss = 0.44600740\n",
      "Iteration 11, loss = 0.38492282\n",
      "Iteration 12, loss = 0.33671817\n",
      "Iteration 13, loss = 0.29870425\n",
      "Iteration 14, loss = 0.26654123\n",
      "Iteration 15, loss = 0.24107128\n",
      "Iteration 16, loss = 0.21959426\n",
      "Iteration 17, loss = 0.20132772\n",
      "Iteration 18, loss = 0.18553187\n",
      "Iteration 19, loss = 0.17222838\n",
      "Iteration 20, loss = 0.16066630\n",
      "Iteration 21, loss = 0.15053927\n",
      "Iteration 22, loss = 0.14153187\n",
      "Iteration 23, loss = 0.13413378\n",
      "Iteration 24, loss = 0.12705403\n",
      "Iteration 25, loss = 0.12087866\n",
      "Iteration 26, loss = 0.11553869\n",
      "Iteration 27, loss = 0.11083409\n",
      "Iteration 28, loss = 0.10647016\n",
      "Iteration 29, loss = 0.10251508\n",
      "Iteration 30, loss = 0.09917072\n",
      "Iteration 31, loss = 0.09609325\n",
      "Iteration 32, loss = 0.09332410\n",
      "Iteration 33, loss = 0.09076659\n",
      "Iteration 34, loss = 0.08840554\n",
      "Iteration 35, loss = 0.08638310\n",
      "Iteration 36, loss = 0.08424928\n",
      "Iteration 37, loss = 0.08249569\n",
      "Iteration 38, loss = 0.08087411\n",
      "Iteration 39, loss = 0.07933386\n",
      "Iteration 40, loss = 0.07795999\n",
      "Iteration 41, loss = 0.07667127\n",
      "Iteration 42, loss = 0.07545392\n",
      "Iteration 43, loss = 0.07426215\n",
      "Iteration 44, loss = 0.07316484\n",
      "Iteration 45, loss = 0.07218991\n",
      "Iteration 46, loss = 0.07126223\n",
      "Iteration 47, loss = 0.07038356\n",
      "Iteration 48, loss = 0.06953517\n",
      "Iteration 49, loss = 0.06873406\n",
      "Iteration 50, loss = 0.06804881\n",
      "Iteration 51, loss = 0.06728338\n",
      "Iteration 52, loss = 0.06664260\n",
      "Iteration 53, loss = 0.06600151\n",
      "Iteration 54, loss = 0.06536545\n",
      "Iteration 55, loss = 0.06474715\n",
      "Iteration 56, loss = 0.06417033\n",
      "Iteration 57, loss = 0.06371720\n",
      "Iteration 58, loss = 0.06315213\n",
      "Iteration 59, loss = 0.06265368\n",
      "Iteration 60, loss = 0.06215170\n",
      "Iteration 61, loss = 0.06166779\n",
      "Iteration 62, loss = 0.06123226\n",
      "Iteration 63, loss = 0.06078069\n",
      "Iteration 64, loss = 0.06035755\n",
      "Iteration 65, loss = 0.05995463\n",
      "Iteration 66, loss = 0.05957696\n",
      "Iteration 67, loss = 0.05916799\n",
      "Iteration 68, loss = 0.05878675\n",
      "Iteration 69, loss = 0.05844340\n",
      "Iteration 70, loss = 0.05807630\n",
      "Iteration 71, loss = 0.05774763\n",
      "Iteration 72, loss = 0.05740432\n",
      "Iteration 73, loss = 0.05706738\n",
      "Iteration 74, loss = 0.05677291\n",
      "Iteration 75, loss = 0.05642067\n",
      "Iteration 76, loss = 0.05609149\n",
      "Iteration 77, loss = 0.05579153\n",
      "Iteration 78, loss = 0.05551460\n",
      "Iteration 79, loss = 0.05521991\n",
      "Iteration 80, loss = 0.05496145\n",
      "Iteration 81, loss = 0.05465622\n",
      "Iteration 82, loss = 0.05439028\n",
      "Iteration 83, loss = 0.05415204\n",
      "Iteration 84, loss = 0.05386835\n",
      "Iteration 85, loss = 0.05362328\n",
      "Iteration 86, loss = 0.05336151\n",
      "Iteration 87, loss = 0.05310886\n",
      "Iteration 88, loss = 0.05287975\n",
      "Iteration 89, loss = 0.05265021\n",
      "Iteration 90, loss = 0.05242568\n",
      "Iteration 91, loss = 0.05218512\n",
      "Iteration 92, loss = 0.05196574\n",
      "Iteration 93, loss = 0.05174897\n",
      "Iteration 94, loss = 0.05154573\n",
      "Iteration 95, loss = 0.05133124\n",
      "Iteration 96, loss = 0.05111953\n",
      "Iteration 97, loss = 0.05090909\n",
      "Iteration 98, loss = 0.05070614\n",
      "Iteration 99, loss = 0.05050226\n",
      "Iteration 100, loss = 0.05032033\n",
      "Iteration 101, loss = 0.05012161\n",
      "Iteration 102, loss = 0.04996113\n",
      "Iteration 103, loss = 0.04976011\n",
      "Iteration 104, loss = 0.04957194\n",
      "Iteration 105, loss = 0.04940545\n",
      "Iteration 106, loss = 0.04921176\n",
      "Iteration 107, loss = 0.04903372\n",
      "Iteration 108, loss = 0.04887208\n",
      "Iteration 109, loss = 0.04871593\n",
      "Iteration 110, loss = 0.04853497\n",
      "Iteration 111, loss = 0.04837983\n",
      "Iteration 112, loss = 0.04822264\n",
      "Iteration 113, loss = 0.04805126\n",
      "Iteration 114, loss = 0.04789204\n",
      "Iteration 115, loss = 0.04774227\n",
      "Iteration 116, loss = 0.04757953\n",
      "Iteration 117, loss = 0.04744408\n",
      "Iteration 118, loss = 0.04729822\n",
      "Iteration 119, loss = 0.04714498\n",
      "Iteration 120, loss = 0.04701145\n",
      "Iteration 121, loss = 0.04686884\n",
      "Iteration 122, loss = 0.04672270\n",
      "Iteration 123, loss = 0.04658007\n",
      "Iteration 124, loss = 0.04644330\n",
      "Iteration 125, loss = 0.04631442\n",
      "Iteration 126, loss = 0.04618947\n",
      "Iteration 127, loss = 0.04609060\n",
      "Iteration 128, loss = 0.04593371\n",
      "Iteration 129, loss = 0.04580199\n",
      "Iteration 130, loss = 0.04567103\n",
      "Iteration 131, loss = 0.04557004\n",
      "Iteration 132, loss = 0.04543600\n",
      "Iteration 133, loss = 0.04533785\n",
      "Iteration 134, loss = 0.04522119\n",
      "Iteration 135, loss = 0.04507725\n",
      "Iteration 136, loss = 0.04496751\n",
      "Iteration 137, loss = 0.04485501\n",
      "Iteration 138, loss = 0.04475748\n",
      "Iteration 139, loss = 0.04465607\n",
      "Iteration 140, loss = 0.04455600\n",
      "Iteration 141, loss = 0.04443476\n",
      "Iteration 142, loss = 0.04432615\n",
      "Iteration 143, loss = 0.04422088\n",
      "Iteration 144, loss = 0.04414315\n",
      "Iteration 145, loss = 0.04401736\n",
      "Iteration 146, loss = 0.04393894\n",
      "Iteration 147, loss = 0.04383668\n",
      "Iteration 148, loss = 0.04376648\n",
      "Iteration 149, loss = 0.04365871\n",
      "Iteration 150, loss = 0.04358635\n",
      "Iteration 151, loss = 0.04349744\n",
      "Iteration 152, loss = 0.04339557\n",
      "Iteration 153, loss = 0.04329648\n",
      "Iteration 154, loss = 0.04321103\n",
      "Iteration 155, loss = 0.04311638\n",
      "Iteration 156, loss = 0.04302207\n",
      "Iteration 157, loss = 0.04294587\n",
      "Iteration 158, loss = 0.04286679\n",
      "Iteration 159, loss = 0.04278151\n",
      "Iteration 160, loss = 0.04270249\n",
      "Iteration 161, loss = 0.04262002\n",
      "Iteration 162, loss = 0.04254742\n",
      "Iteration 163, loss = 0.04247091\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27259437\n",
      "Iteration 2, loss = 2.00616329\n",
      "Iteration 3, loss = 1.73548471\n",
      "Iteration 4, loss = 1.45081756\n",
      "Iteration 5, loss = 1.18598265\n",
      "Iteration 6, loss = 0.95948051\n",
      "Iteration 7, loss = 0.77905019\n",
      "Iteration 8, loss = 0.63873181\n",
      "Iteration 9, loss = 0.53233860\n",
      "Iteration 10, loss = 0.45182868\n",
      "Iteration 11, loss = 0.38937646\n",
      "Iteration 12, loss = 0.34016788\n",
      "Iteration 13, loss = 0.30094187\n",
      "Iteration 14, loss = 0.26865019\n",
      "Iteration 15, loss = 0.24314936\n",
      "Iteration 16, loss = 0.22104417\n",
      "Iteration 17, loss = 0.20247379\n",
      "Iteration 18, loss = 0.18666112\n",
      "Iteration 19, loss = 0.17333897\n",
      "Iteration 20, loss = 0.16190222\n",
      "Iteration 21, loss = 0.15189225\n",
      "Iteration 22, loss = 0.14276061\n",
      "Iteration 23, loss = 0.13499401\n",
      "Iteration 24, loss = 0.12827151\n",
      "Iteration 25, loss = 0.12195157\n",
      "Iteration 26, loss = 0.11649487\n",
      "Iteration 27, loss = 0.11171595\n",
      "Iteration 28, loss = 0.10726444\n",
      "Iteration 29, loss = 0.10336041\n",
      "Iteration 30, loss = 0.10000618\n",
      "Iteration 31, loss = 0.09681658\n",
      "Iteration 32, loss = 0.09404762\n",
      "Iteration 33, loss = 0.09147016\n",
      "Iteration 34, loss = 0.08902045\n",
      "Iteration 35, loss = 0.08710788\n",
      "Iteration 36, loss = 0.08503358\n",
      "Iteration 37, loss = 0.08325885\n",
      "Iteration 38, loss = 0.08157951\n",
      "Iteration 39, loss = 0.08006347\n",
      "Iteration 40, loss = 0.07875497\n",
      "Iteration 41, loss = 0.07740193\n",
      "Iteration 42, loss = 0.07608671\n",
      "Iteration 43, loss = 0.07494373\n",
      "Iteration 44, loss = 0.07381006\n",
      "Iteration 45, loss = 0.07285369\n",
      "Iteration 46, loss = 0.07191228\n",
      "Iteration 47, loss = 0.07098466\n",
      "Iteration 48, loss = 0.07015190\n",
      "Iteration 49, loss = 0.06936682\n",
      "Iteration 50, loss = 0.06865627\n",
      "Iteration 51, loss = 0.06789946\n",
      "Iteration 52, loss = 0.06729375\n",
      "Iteration 53, loss = 0.06659842\n",
      "Iteration 54, loss = 0.06598146\n",
      "Iteration 55, loss = 0.06536638\n",
      "Iteration 56, loss = 0.06479421\n",
      "Iteration 57, loss = 0.06427379\n",
      "Iteration 58, loss = 0.06374191\n",
      "Iteration 59, loss = 0.06327922\n",
      "Iteration 60, loss = 0.06276674\n",
      "Iteration 61, loss = 0.06227747\n",
      "Iteration 62, loss = 0.06181970\n",
      "Iteration 63, loss = 0.06137065\n",
      "Iteration 64, loss = 0.06093668\n",
      "Iteration 65, loss = 0.06053833\n",
      "Iteration 66, loss = 0.06016747\n",
      "Iteration 67, loss = 0.05974795\n",
      "Iteration 68, loss = 0.05940164\n",
      "Iteration 69, loss = 0.05902586\n",
      "Iteration 70, loss = 0.05867094\n",
      "Iteration 71, loss = 0.05832190\n",
      "Iteration 72, loss = 0.05796456\n",
      "Iteration 73, loss = 0.05766038\n",
      "Iteration 74, loss = 0.05735363\n",
      "Iteration 75, loss = 0.05701885\n",
      "Iteration 76, loss = 0.05669303\n",
      "Iteration 77, loss = 0.05638002\n",
      "Iteration 78, loss = 0.05610667\n",
      "Iteration 79, loss = 0.05580371\n",
      "Iteration 80, loss = 0.05554842\n",
      "Iteration 81, loss = 0.05525713\n",
      "Iteration 82, loss = 0.05497665\n",
      "Iteration 83, loss = 0.05472667\n",
      "Iteration 84, loss = 0.05446680\n",
      "Iteration 85, loss = 0.05421502\n",
      "Iteration 86, loss = 0.05396180\n",
      "Iteration 87, loss = 0.05370978\n",
      "Iteration 88, loss = 0.05349484\n",
      "Iteration 89, loss = 0.05325564\n",
      "Iteration 90, loss = 0.05301521\n",
      "Iteration 91, loss = 0.05279188\n",
      "Iteration 92, loss = 0.05258210\n",
      "Iteration 93, loss = 0.05236654\n",
      "Iteration 94, loss = 0.05215767\n",
      "Iteration 95, loss = 0.05194667\n",
      "Iteration 96, loss = 0.05174236\n",
      "Iteration 97, loss = 0.05152890\n",
      "Iteration 98, loss = 0.05132050\n",
      "Iteration 99, loss = 0.05115052\n",
      "Iteration 100, loss = 0.05094273\n",
      "Iteration 101, loss = 0.05075262\n",
      "Iteration 102, loss = 0.05058592\n",
      "Iteration 103, loss = 0.05038260\n",
      "Iteration 104, loss = 0.05019007\n",
      "Iteration 105, loss = 0.05003654\n",
      "Iteration 106, loss = 0.04983468\n",
      "Iteration 107, loss = 0.04967483\n",
      "Iteration 108, loss = 0.04952127\n",
      "Iteration 109, loss = 0.04934463\n",
      "Iteration 110, loss = 0.04915909\n",
      "Iteration 111, loss = 0.04901394\n",
      "Iteration 112, loss = 0.04885928\n",
      "Iteration 113, loss = 0.04869479\n",
      "Iteration 114, loss = 0.04852959\n",
      "Iteration 115, loss = 0.04838608\n",
      "Iteration 116, loss = 0.04822280\n",
      "Iteration 117, loss = 0.04809495\n",
      "Iteration 118, loss = 0.04793673\n",
      "Iteration 119, loss = 0.04780564\n",
      "Iteration 120, loss = 0.04766111\n",
      "Iteration 121, loss = 0.04751506\n",
      "Iteration 122, loss = 0.04739169\n",
      "Iteration 123, loss = 0.04723974\n",
      "Iteration 124, loss = 0.04709922\n",
      "Iteration 125, loss = 0.04697276\n",
      "Iteration 126, loss = 0.04684736\n",
      "Iteration 127, loss = 0.04675767\n",
      "Iteration 128, loss = 0.04658897\n",
      "Iteration 129, loss = 0.04646137\n",
      "Iteration 130, loss = 0.04632887\n",
      "Iteration 131, loss = 0.04621812\n",
      "Iteration 132, loss = 0.04610617\n",
      "Iteration 133, loss = 0.04599489\n",
      "Iteration 134, loss = 0.04587589\n",
      "Iteration 135, loss = 0.04574355\n",
      "Iteration 136, loss = 0.04563061\n",
      "Iteration 137, loss = 0.04551225\n",
      "Iteration 138, loss = 0.04542612\n",
      "Iteration 139, loss = 0.04531287\n",
      "Iteration 140, loss = 0.04520922\n",
      "Iteration 141, loss = 0.04509602\n",
      "Iteration 142, loss = 0.04499469\n",
      "Iteration 143, loss = 0.04491318\n",
      "Iteration 144, loss = 0.04482593\n",
      "Iteration 145, loss = 0.04467657\n",
      "Iteration 146, loss = 0.04460584\n",
      "Iteration 147, loss = 0.04450586\n",
      "Iteration 148, loss = 0.04443411\n",
      "Iteration 149, loss = 0.04432477\n",
      "Iteration 150, loss = 0.04425543\n",
      "Iteration 151, loss = 0.04417542\n",
      "Iteration 152, loss = 0.04406864\n",
      "Iteration 153, loss = 0.04397299\n",
      "Iteration 154, loss = 0.04389425\n",
      "Iteration 155, loss = 0.04379091\n",
      "Iteration 156, loss = 0.04369049\n",
      "Iteration 157, loss = 0.04362495\n",
      "Iteration 158, loss = 0.04353395\n",
      "Iteration 159, loss = 0.04345899\n",
      "Iteration 160, loss = 0.04339993\n",
      "Iteration 161, loss = 0.04330160\n",
      "Iteration 162, loss = 0.04322690\n",
      "Iteration 163, loss = 0.04317308\n",
      "Iteration 164, loss = 0.04307858\n",
      "Iteration 165, loss = 0.04302640\n",
      "Iteration 166, loss = 0.04293250\n",
      "Iteration 167, loss = 0.04289228\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27525268\n",
      "Iteration 2, loss = 2.00708674\n",
      "Iteration 3, loss = 1.72962322\n",
      "Iteration 4, loss = 1.43842606\n",
      "Iteration 5, loss = 1.16579539\n",
      "Iteration 6, loss = 0.93402202\n",
      "Iteration 7, loss = 0.74798767\n",
      "Iteration 8, loss = 0.60568869\n",
      "Iteration 9, loss = 0.49833996\n",
      "Iteration 10, loss = 0.41825819\n",
      "Iteration 11, loss = 0.35620485\n",
      "Iteration 12, loss = 0.30850590\n",
      "Iteration 13, loss = 0.27067078\n",
      "Iteration 14, loss = 0.24251165\n",
      "Iteration 15, loss = 0.21758723\n",
      "Iteration 16, loss = 0.19724385\n",
      "Iteration 17, loss = 0.18037185\n",
      "Iteration 18, loss = 0.16630449\n",
      "Iteration 19, loss = 0.15427690\n",
      "Iteration 20, loss = 0.14417709\n",
      "Iteration 21, loss = 0.13545063\n",
      "Iteration 22, loss = 0.12741146\n",
      "Iteration 23, loss = 0.12056701\n",
      "Iteration 24, loss = 0.11513096\n",
      "Iteration 25, loss = 0.10970404\n",
      "Iteration 26, loss = 0.10501940\n",
      "Iteration 27, loss = 0.10081780\n",
      "Iteration 28, loss = 0.09711834\n",
      "Iteration 29, loss = 0.09398996\n",
      "Iteration 30, loss = 0.09124568\n",
      "Iteration 31, loss = 0.08863102\n",
      "Iteration 32, loss = 0.08630940\n",
      "Iteration 33, loss = 0.08426572\n",
      "Iteration 34, loss = 0.08230867\n",
      "Iteration 35, loss = 0.08059378\n",
      "Iteration 36, loss = 0.07897354\n",
      "Iteration 37, loss = 0.07743995\n",
      "Iteration 38, loss = 0.07599160\n",
      "Iteration 39, loss = 0.07470030\n",
      "Iteration 40, loss = 0.07350274\n",
      "Iteration 41, loss = 0.07232349\n",
      "Iteration 42, loss = 0.07122364\n",
      "Iteration 43, loss = 0.07032095\n",
      "Iteration 44, loss = 0.06932706\n",
      "Iteration 45, loss = 0.06845097\n",
      "Iteration 46, loss = 0.06760464\n",
      "Iteration 47, loss = 0.06678619\n",
      "Iteration 48, loss = 0.06606653\n",
      "Iteration 49, loss = 0.06536474\n",
      "Iteration 50, loss = 0.06473354\n",
      "Iteration 51, loss = 0.06409582\n",
      "Iteration 52, loss = 0.06350749\n",
      "Iteration 53, loss = 0.06290011\n",
      "Iteration 54, loss = 0.06232818\n",
      "Iteration 55, loss = 0.06181601\n",
      "Iteration 56, loss = 0.06129424\n",
      "Iteration 57, loss = 0.06081890\n",
      "Iteration 58, loss = 0.06032350\n",
      "Iteration 59, loss = 0.05986949\n",
      "Iteration 60, loss = 0.05942356\n",
      "Iteration 61, loss = 0.05899317\n",
      "Iteration 62, loss = 0.05855242\n",
      "Iteration 63, loss = 0.05815945\n",
      "Iteration 64, loss = 0.05777191\n",
      "Iteration 65, loss = 0.05738639\n",
      "Iteration 66, loss = 0.05699805\n",
      "Iteration 67, loss = 0.05663648\n",
      "Iteration 68, loss = 0.05631080\n",
      "Iteration 69, loss = 0.05595676\n",
      "Iteration 70, loss = 0.05565600\n",
      "Iteration 71, loss = 0.05530773\n",
      "Iteration 72, loss = 0.05496974\n",
      "Iteration 73, loss = 0.05467510\n",
      "Iteration 74, loss = 0.05438026\n",
      "Iteration 75, loss = 0.05406424\n",
      "Iteration 76, loss = 0.05377314\n",
      "Iteration 77, loss = 0.05348561\n",
      "Iteration 78, loss = 0.05322842\n",
      "Iteration 79, loss = 0.05294999\n",
      "Iteration 80, loss = 0.05268631\n",
      "Iteration 81, loss = 0.05242173\n",
      "Iteration 82, loss = 0.05216600\n",
      "Iteration 83, loss = 0.05192646\n",
      "Iteration 84, loss = 0.05166962\n",
      "Iteration 85, loss = 0.05142155\n",
      "Iteration 86, loss = 0.05119041\n",
      "Iteration 87, loss = 0.05096225\n",
      "Iteration 88, loss = 0.05074264\n",
      "Iteration 89, loss = 0.05051381\n",
      "Iteration 90, loss = 0.05028875\n",
      "Iteration 91, loss = 0.05008141\n",
      "Iteration 92, loss = 0.04987048\n",
      "Iteration 93, loss = 0.04966156\n",
      "Iteration 94, loss = 0.04946248\n",
      "Iteration 95, loss = 0.04925505\n",
      "Iteration 96, loss = 0.04906864\n",
      "Iteration 97, loss = 0.04884363\n",
      "Iteration 98, loss = 0.04868000\n",
      "Iteration 99, loss = 0.04850692\n",
      "Iteration 100, loss = 0.04829987\n",
      "Iteration 101, loss = 0.04811880\n",
      "Iteration 102, loss = 0.04794131\n",
      "Iteration 103, loss = 0.04776450\n",
      "Iteration 104, loss = 0.04758410\n",
      "Iteration 105, loss = 0.04742440\n",
      "Iteration 106, loss = 0.04725338\n",
      "Iteration 107, loss = 0.04709343\n",
      "Iteration 108, loss = 0.04694488\n",
      "Iteration 109, loss = 0.04677451\n",
      "Iteration 110, loss = 0.04660683\n",
      "Iteration 111, loss = 0.04645602\n",
      "Iteration 112, loss = 0.04630847\n",
      "Iteration 113, loss = 0.04614374\n",
      "Iteration 114, loss = 0.04599251\n",
      "Iteration 115, loss = 0.04585136\n",
      "Iteration 116, loss = 0.04570012\n",
      "Iteration 117, loss = 0.04556600\n",
      "Iteration 118, loss = 0.04543534\n",
      "Iteration 119, loss = 0.04529773\n",
      "Iteration 120, loss = 0.04517270\n",
      "Iteration 121, loss = 0.04503819\n",
      "Iteration 122, loss = 0.04489740\n",
      "Iteration 123, loss = 0.04476312\n",
      "Iteration 124, loss = 0.04463749\n",
      "Iteration 125, loss = 0.04451351\n",
      "Iteration 126, loss = 0.04439310\n",
      "Iteration 127, loss = 0.04428481\n",
      "Iteration 128, loss = 0.04413991\n",
      "Iteration 129, loss = 0.04402954\n",
      "Iteration 130, loss = 0.04389970\n",
      "Iteration 131, loss = 0.04378579\n",
      "Iteration 132, loss = 0.04369447\n",
      "Iteration 133, loss = 0.04358357\n",
      "Iteration 134, loss = 0.04346901\n",
      "Iteration 135, loss = 0.04334549\n",
      "Iteration 136, loss = 0.04323779\n",
      "Iteration 137, loss = 0.04312991\n",
      "Iteration 138, loss = 0.04303470\n",
      "Iteration 139, loss = 0.04292735\n",
      "Iteration 140, loss = 0.04282759\n",
      "Iteration 141, loss = 0.04271847\n",
      "Iteration 142, loss = 0.04263029\n",
      "Iteration 143, loss = 0.04253885\n",
      "Iteration 144, loss = 0.04243831\n",
      "Iteration 145, loss = 0.04231343\n",
      "Iteration 146, loss = 0.04226525\n",
      "Iteration 147, loss = 0.04216166\n",
      "Iteration 148, loss = 0.04205603\n",
      "Iteration 149, loss = 0.04197248\n",
      "Iteration 150, loss = 0.04188881\n",
      "Iteration 151, loss = 0.04180363\n",
      "Iteration 152, loss = 0.04171795\n",
      "Iteration 153, loss = 0.04164271\n",
      "Iteration 154, loss = 0.04155548\n",
      "Iteration 155, loss = 0.04147271\n",
      "Iteration 156, loss = 0.04137074\n",
      "Iteration 157, loss = 0.04131000\n",
      "Iteration 158, loss = 0.04126118\n",
      "Iteration 159, loss = 0.04115580\n",
      "Iteration 160, loss = 0.04109801\n",
      "Iteration 161, loss = 0.04101809\n",
      "Iteration 162, loss = 0.04095324\n",
      "Iteration 163, loss = 0.04087737\n",
      "Iteration 164, loss = 0.04081085\n",
      "Iteration 165, loss = 0.04072794\n",
      "Iteration 166, loss = 0.04064623\n",
      "Iteration 167, loss = 0.04058753\n",
      "Iteration 168, loss = 0.04051824\n",
      "Iteration 169, loss = 0.04045884\n",
      "Iteration 170, loss = 0.04040716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.37226039\n",
      "Iteration 2, loss = 2.34988006\n",
      "Iteration 3, loss = 2.32275724\n",
      "Iteration 4, loss = 2.29511813\n",
      "Iteration 5, loss = 2.27012884\n",
      "Iteration 6, loss = 2.24606160\n",
      "Iteration 7, loss = 2.22323138\n",
      "Iteration 8, loss = 2.20067502\n",
      "Iteration 9, loss = 2.17776908\n",
      "Iteration 10, loss = 2.15494770\n",
      "Iteration 11, loss = 2.13200217\n",
      "Iteration 12, loss = 2.10883953\n",
      "Iteration 13, loss = 2.08562694\n",
      "Iteration 14, loss = 2.06213275\n",
      "Iteration 15, loss = 2.03785186\n",
      "Iteration 16, loss = 2.01352793\n",
      "Iteration 17, loss = 1.98888652\n",
      "Iteration 18, loss = 1.96373961\n",
      "Iteration 19, loss = 1.93816112\n",
      "Iteration 20, loss = 1.91250433\n",
      "Iteration 21, loss = 1.88593667\n",
      "Iteration 22, loss = 1.85952316\n",
      "Iteration 23, loss = 1.83272541\n",
      "Iteration 24, loss = 1.80598625\n",
      "Iteration 25, loss = 1.77901487\n",
      "Iteration 26, loss = 1.75178991\n",
      "Iteration 27, loss = 1.72446929\n",
      "Iteration 28, loss = 1.69796563\n",
      "Iteration 29, loss = 1.67086009\n",
      "Iteration 30, loss = 1.64377618\n",
      "Iteration 31, loss = 1.61656817\n",
      "Iteration 32, loss = 1.58979453\n",
      "Iteration 33, loss = 1.56317994\n",
      "Iteration 34, loss = 1.53656977\n",
      "Iteration 35, loss = 1.51044606\n",
      "Iteration 36, loss = 1.48440879\n",
      "Iteration 37, loss = 1.45878145\n",
      "Iteration 38, loss = 1.43365165\n",
      "Iteration 39, loss = 1.40909708\n",
      "Iteration 40, loss = 1.38463737\n",
      "Iteration 41, loss = 1.36060756\n",
      "Iteration 42, loss = 1.33719533\n",
      "Iteration 43, loss = 1.31383344\n",
      "Iteration 44, loss = 1.29126237\n",
      "Iteration 45, loss = 1.26909698\n",
      "Iteration 46, loss = 1.24730125\n",
      "Iteration 47, loss = 1.22596355\n",
      "Iteration 48, loss = 1.20502142\n",
      "Iteration 49, loss = 1.18456554\n",
      "Iteration 50, loss = 1.16419108\n",
      "Iteration 51, loss = 1.14468995\n",
      "Iteration 52, loss = 1.12548989\n",
      "Iteration 53, loss = 1.10659687\n",
      "Iteration 54, loss = 1.08799803\n",
      "Iteration 55, loss = 1.07002870\n",
      "Iteration 56, loss = 1.05257490\n",
      "Iteration 57, loss = 1.03545916\n",
      "Iteration 58, loss = 1.01878479\n",
      "Iteration 59, loss = 1.00244370\n",
      "Iteration 60, loss = 0.98620181\n",
      "Iteration 61, loss = 0.97039106\n",
      "Iteration 62, loss = 0.95502712\n",
      "Iteration 63, loss = 0.93991525\n",
      "Iteration 64, loss = 0.92542123\n",
      "Iteration 65, loss = 0.91089549\n",
      "Iteration 66, loss = 0.89699547\n",
      "Iteration 67, loss = 0.88294887\n",
      "Iteration 68, loss = 0.86959441\n",
      "Iteration 69, loss = 0.85631796\n",
      "Iteration 70, loss = 0.84361770\n",
      "Iteration 71, loss = 0.83115763\n",
      "Iteration 72, loss = 0.81911744\n",
      "Iteration 73, loss = 0.80716270\n",
      "Iteration 74, loss = 0.79554275\n",
      "Iteration 75, loss = 0.78434741\n",
      "Iteration 76, loss = 0.77327496\n",
      "Iteration 77, loss = 0.76251180\n",
      "Iteration 78, loss = 0.75183439\n",
      "Iteration 79, loss = 0.74171980\n",
      "Iteration 80, loss = 0.73146919\n",
      "Iteration 81, loss = 0.72170442\n",
      "Iteration 82, loss = 0.71203613\n",
      "Iteration 83, loss = 0.70254163\n",
      "Iteration 84, loss = 0.69347572\n",
      "Iteration 85, loss = 0.68453046\n",
      "Iteration 86, loss = 0.67560141\n",
      "Iteration 87, loss = 0.66714513\n",
      "Iteration 88, loss = 0.65877507\n",
      "Iteration 89, loss = 0.65050016\n",
      "Iteration 90, loss = 0.64256638\n",
      "Iteration 91, loss = 0.63469183\n",
      "Iteration 92, loss = 0.62707093\n",
      "Iteration 93, loss = 0.61965659\n",
      "Iteration 94, loss = 0.61247623\n",
      "Iteration 95, loss = 0.60518527\n",
      "Iteration 96, loss = 0.59816660\n",
      "Iteration 97, loss = 0.59136977\n",
      "Iteration 98, loss = 0.58472961\n",
      "Iteration 99, loss = 0.57819084\n",
      "Iteration 100, loss = 0.57181564\n",
      "Iteration 101, loss = 0.56543597\n",
      "Iteration 102, loss = 0.55933632\n",
      "Iteration 103, loss = 0.55312071\n",
      "Iteration 104, loss = 0.54717385\n",
      "Iteration 105, loss = 0.54139633\n",
      "Iteration 106, loss = 0.53586689\n",
      "Iteration 107, loss = 0.53029450\n",
      "Iteration 108, loss = 0.52484990\n",
      "Iteration 109, loss = 0.51934101\n",
      "Iteration 110, loss = 0.51411180\n",
      "Iteration 111, loss = 0.50884406\n",
      "Iteration 112, loss = 0.50382250\n",
      "Iteration 113, loss = 0.49878534\n",
      "Iteration 114, loss = 0.49385782\n",
      "Iteration 115, loss = 0.48903588\n",
      "Iteration 116, loss = 0.48440687\n",
      "Iteration 117, loss = 0.47981359\n",
      "Iteration 118, loss = 0.47526435\n",
      "Iteration 119, loss = 0.47081729\n",
      "Iteration 120, loss = 0.46647186\n",
      "Iteration 121, loss = 0.46214572\n",
      "Iteration 122, loss = 0.45786884\n",
      "Iteration 123, loss = 0.45371180\n",
      "Iteration 124, loss = 0.44967320\n",
      "Iteration 125, loss = 0.44566500\n",
      "Iteration 126, loss = 0.44181719\n",
      "Iteration 127, loss = 0.43798992\n",
      "Iteration 128, loss = 0.43427764\n",
      "Iteration 129, loss = 0.43070150\n",
      "Iteration 130, loss = 0.42697524\n",
      "Iteration 131, loss = 0.42347748\n",
      "Iteration 132, loss = 0.41980468\n",
      "Iteration 133, loss = 0.41627710\n",
      "Iteration 134, loss = 0.41290495\n",
      "Iteration 135, loss = 0.40959619\n",
      "Iteration 136, loss = 0.40623956\n",
      "Iteration 137, loss = 0.40280193\n",
      "Iteration 138, loss = 0.39958978\n",
      "Iteration 139, loss = 0.39632180\n",
      "Iteration 140, loss = 0.39326954\n",
      "Iteration 141, loss = 0.39016488\n",
      "Iteration 142, loss = 0.38721310\n",
      "Iteration 143, loss = 0.38419469\n",
      "Iteration 144, loss = 0.38128140\n",
      "Iteration 145, loss = 0.37836977\n",
      "Iteration 146, loss = 0.37553760\n",
      "Iteration 147, loss = 0.37274818\n",
      "Iteration 148, loss = 0.36998474\n",
      "Iteration 149, loss = 0.36740866\n",
      "Iteration 150, loss = 0.36477448\n",
      "Iteration 151, loss = 0.36207633\n",
      "Iteration 152, loss = 0.35945222\n",
      "Iteration 153, loss = 0.35684352\n",
      "Iteration 154, loss = 0.35440760\n",
      "Iteration 155, loss = 0.35189045\n",
      "Iteration 156, loss = 0.34940311\n",
      "Iteration 157, loss = 0.34696727\n",
      "Iteration 158, loss = 0.34454710\n",
      "Iteration 159, loss = 0.34222956\n",
      "Iteration 160, loss = 0.33980241\n",
      "Iteration 161, loss = 0.33754910\n",
      "Iteration 162, loss = 0.33529587\n",
      "Iteration 163, loss = 0.33309815\n",
      "Iteration 164, loss = 0.33096209\n",
      "Iteration 165, loss = 0.32877907\n",
      "Iteration 166, loss = 0.32667526\n",
      "Iteration 167, loss = 0.32456289\n",
      "Iteration 168, loss = 0.32247080\n",
      "Iteration 169, loss = 0.32043125\n",
      "Iteration 170, loss = 0.31843215\n",
      "Iteration 171, loss = 0.31635811\n",
      "Iteration 172, loss = 0.31434505\n",
      "Iteration 173, loss = 0.31248345\n",
      "Iteration 174, loss = 0.31046912\n",
      "Iteration 175, loss = 0.30853414\n",
      "Iteration 176, loss = 0.30669190\n",
      "Iteration 177, loss = 0.30473958\n",
      "Iteration 178, loss = 0.30290687\n",
      "Iteration 179, loss = 0.30108162\n",
      "Iteration 180, loss = 0.29928341\n",
      "Iteration 181, loss = 0.29749785\n",
      "Iteration 182, loss = 0.29574523\n",
      "Iteration 183, loss = 0.29406308\n",
      "Iteration 184, loss = 0.29238221\n",
      "Iteration 185, loss = 0.29064619\n",
      "Iteration 186, loss = 0.28901512\n",
      "Iteration 187, loss = 0.28731488\n",
      "Iteration 188, loss = 0.28562850\n",
      "Iteration 189, loss = 0.28398959\n",
      "Iteration 190, loss = 0.28236620\n",
      "Iteration 191, loss = 0.28074118\n",
      "Iteration 192, loss = 0.27922684\n",
      "Iteration 193, loss = 0.27766992\n",
      "Iteration 194, loss = 0.27607777\n",
      "Iteration 195, loss = 0.27461981\n",
      "Iteration 196, loss = 0.27307862\n",
      "Iteration 197, loss = 0.27169473\n",
      "Iteration 198, loss = 0.27035602\n",
      "Iteration 199, loss = 0.26887293\n",
      "Iteration 200, loss = 0.26738084\n",
      "Iteration 201, loss = 0.26595948\n",
      "Iteration 202, loss = 0.26456063\n",
      "Iteration 203, loss = 0.26317976\n",
      "Iteration 204, loss = 0.26186608\n",
      "Iteration 205, loss = 0.26052466\n",
      "Iteration 206, loss = 0.25908791\n",
      "Iteration 207, loss = 0.25781674\n",
      "Iteration 208, loss = 0.25647179\n",
      "Iteration 209, loss = 0.25516923\n",
      "Iteration 210, loss = 0.25388852\n",
      "Iteration 211, loss = 0.25263647\n",
      "Iteration 212, loss = 0.25139238\n",
      "Iteration 213, loss = 0.25013666\n",
      "Iteration 214, loss = 0.24890186\n",
      "Iteration 215, loss = 0.24765522\n",
      "Iteration 216, loss = 0.24648785\n",
      "Iteration 217, loss = 0.24537732\n",
      "Iteration 218, loss = 0.24417091\n",
      "Iteration 219, loss = 0.24295846\n",
      "Iteration 220, loss = 0.24179689\n",
      "Iteration 221, loss = 0.24072548\n",
      "Iteration 222, loss = 0.23958399\n",
      "Iteration 223, loss = 0.23842826\n",
      "Iteration 224, loss = 0.23729380\n",
      "Iteration 225, loss = 0.23627489\n",
      "Iteration 226, loss = 0.23512191\n",
      "Iteration 227, loss = 0.23403004\n",
      "Iteration 228, loss = 0.23301719\n",
      "Iteration 229, loss = 0.23194877\n",
      "Iteration 230, loss = 0.23083063\n",
      "Iteration 231, loss = 0.22986518\n",
      "Iteration 232, loss = 0.22880679\n",
      "Iteration 233, loss = 0.22778425\n",
      "Iteration 234, loss = 0.22671473\n",
      "Iteration 235, loss = 0.22568964\n",
      "Iteration 236, loss = 0.22481253\n",
      "Iteration 237, loss = 0.22380082\n",
      "Iteration 238, loss = 0.22285417\n",
      "Iteration 239, loss = 0.22188260\n",
      "Iteration 240, loss = 0.22091256\n",
      "Iteration 241, loss = 0.21997322\n",
      "Iteration 242, loss = 0.21897565\n",
      "Iteration 243, loss = 0.21798441\n",
      "Iteration 244, loss = 0.21708406\n",
      "Iteration 245, loss = 0.21617093\n",
      "Iteration 246, loss = 0.21516662\n",
      "Iteration 247, loss = 0.21427597\n",
      "Iteration 248, loss = 0.21336752\n",
      "Iteration 249, loss = 0.21246221\n",
      "Iteration 250, loss = 0.21162521\n",
      "Iteration 251, loss = 0.21075593\n",
      "Iteration 252, loss = 0.20984077\n",
      "Iteration 253, loss = 0.20894719\n",
      "Iteration 254, loss = 0.20811074\n",
      "Iteration 255, loss = 0.20727003\n",
      "Iteration 256, loss = 0.20642675\n",
      "Iteration 257, loss = 0.20556066\n",
      "Iteration 258, loss = 0.20473987\n",
      "Iteration 259, loss = 0.20391689\n",
      "Iteration 260, loss = 0.20309764\n",
      "Iteration 261, loss = 0.20231762\n",
      "Iteration 262, loss = 0.20152298\n",
      "Iteration 263, loss = 0.20065935\n",
      "Iteration 264, loss = 0.19990325\n",
      "Iteration 265, loss = 0.19913778\n",
      "Iteration 266, loss = 0.19837830\n",
      "Iteration 267, loss = 0.19764339\n",
      "Iteration 268, loss = 0.19689869\n",
      "Iteration 269, loss = 0.19618471\n",
      "Iteration 270, loss = 0.19545502\n",
      "Iteration 271, loss = 0.19470919\n",
      "Iteration 272, loss = 0.19398630\n",
      "Iteration 273, loss = 0.19323763\n",
      "Iteration 274, loss = 0.19253557\n",
      "Iteration 275, loss = 0.19184327\n",
      "Iteration 276, loss = 0.19113754\n",
      "Iteration 277, loss = 0.19045385\n",
      "Iteration 278, loss = 0.18970130\n",
      "Iteration 279, loss = 0.18903910\n",
      "Iteration 280, loss = 0.18829007\n",
      "Iteration 281, loss = 0.18760503\n",
      "Iteration 282, loss = 0.18695440\n",
      "Iteration 283, loss = 0.18633536\n",
      "Iteration 284, loss = 0.18560920\n",
      "Iteration 285, loss = 0.18493188\n",
      "Iteration 286, loss = 0.18431212\n",
      "Iteration 287, loss = 0.18367166\n",
      "Iteration 288, loss = 0.18303136\n",
      "Iteration 289, loss = 0.18238789\n",
      "Iteration 290, loss = 0.18177852\n",
      "Iteration 291, loss = 0.18115238\n",
      "Iteration 292, loss = 0.18054163\n",
      "Iteration 293, loss = 0.17990165\n",
      "Iteration 294, loss = 0.17933476\n",
      "Iteration 295, loss = 0.17870723\n",
      "Iteration 296, loss = 0.17808243\n",
      "Iteration 297, loss = 0.17746245\n",
      "Iteration 298, loss = 0.17685489\n",
      "Iteration 299, loss = 0.17634378\n",
      "Iteration 300, loss = 0.17574157\n",
      "Iteration 301, loss = 0.17509633\n",
      "Iteration 302, loss = 0.17451658\n",
      "Iteration 303, loss = 0.17391769\n",
      "Iteration 304, loss = 0.17334237\n",
      "Iteration 305, loss = 0.17277648\n",
      "Iteration 306, loss = 0.17223357\n",
      "Iteration 307, loss = 0.17166477\n",
      "Iteration 308, loss = 0.17113447\n",
      "Iteration 309, loss = 0.17054494\n",
      "Iteration 310, loss = 0.16996936\n",
      "Iteration 311, loss = 0.16940344\n",
      "Iteration 312, loss = 0.16887204\n",
      "Iteration 313, loss = 0.16832505\n",
      "Iteration 314, loss = 0.16781240\n",
      "Iteration 315, loss = 0.16727712\n",
      "Iteration 316, loss = 0.16672521\n",
      "Iteration 317, loss = 0.16616393\n",
      "Iteration 318, loss = 0.16566374\n",
      "Iteration 319, loss = 0.16515239\n",
      "Iteration 320, loss = 0.16463325\n",
      "Iteration 321, loss = 0.16413843\n",
      "Iteration 322, loss = 0.16360965\n",
      "Iteration 323, loss = 0.16311249\n",
      "Iteration 324, loss = 0.16262066\n",
      "Iteration 325, loss = 0.16209818\n",
      "Iteration 326, loss = 0.16159979\n",
      "Iteration 327, loss = 0.16110303\n",
      "Iteration 328, loss = 0.16059797\n",
      "Iteration 329, loss = 0.16013447\n",
      "Iteration 330, loss = 0.15963518\n",
      "Iteration 331, loss = 0.15910813\n",
      "Iteration 332, loss = 0.15860412\n",
      "Iteration 333, loss = 0.15813658\n",
      "Iteration 334, loss = 0.15765186\n",
      "Iteration 335, loss = 0.15715790\n",
      "Iteration 336, loss = 0.15670061\n",
      "Iteration 337, loss = 0.15624364\n",
      "Iteration 338, loss = 0.15582235\n",
      "Iteration 339, loss = 0.15534867\n",
      "Iteration 340, loss = 0.15490185\n",
      "Iteration 341, loss = 0.15446720\n",
      "Iteration 342, loss = 0.15400540\n",
      "Iteration 343, loss = 0.15354663\n",
      "Iteration 344, loss = 0.15311859\n",
      "Iteration 345, loss = 0.15265655\n",
      "Iteration 346, loss = 0.15223148\n",
      "Iteration 347, loss = 0.15180312\n",
      "Iteration 348, loss = 0.15135505\n",
      "Iteration 349, loss = 0.15094327\n",
      "Iteration 350, loss = 0.15053471\n",
      "Iteration 351, loss = 0.15008736\n",
      "Iteration 352, loss = 0.14968697\n",
      "Iteration 353, loss = 0.14928611\n",
      "Iteration 354, loss = 0.14888763\n",
      "Iteration 355, loss = 0.14846422\n",
      "Iteration 356, loss = 0.14805335\n",
      "Iteration 357, loss = 0.14761369\n",
      "Iteration 358, loss = 0.14720412\n",
      "Iteration 359, loss = 0.14678657\n",
      "Iteration 360, loss = 0.14640333\n",
      "Iteration 361, loss = 0.14599910\n",
      "Iteration 362, loss = 0.14561466\n",
      "Iteration 363, loss = 0.14520709\n",
      "Iteration 364, loss = 0.14485186\n",
      "Iteration 365, loss = 0.14447238\n",
      "Iteration 366, loss = 0.14409570\n",
      "Iteration 367, loss = 0.14368614\n",
      "Iteration 368, loss = 0.14336153\n",
      "Iteration 369, loss = 0.14298241\n",
      "Iteration 370, loss = 0.14260718\n",
      "Iteration 371, loss = 0.14226031\n",
      "Iteration 372, loss = 0.14186719\n",
      "Iteration 373, loss = 0.14153380\n",
      "Iteration 374, loss = 0.14116023\n",
      "Iteration 375, loss = 0.14083351\n",
      "Iteration 376, loss = 0.14044534\n",
      "Iteration 377, loss = 0.14010251\n",
      "Iteration 378, loss = 0.13975939\n",
      "Iteration 379, loss = 0.13939864\n",
      "Iteration 380, loss = 0.13905931\n",
      "Iteration 381, loss = 0.13873719\n",
      "Iteration 382, loss = 0.13834781\n",
      "Iteration 383, loss = 0.13800993\n",
      "Iteration 384, loss = 0.13765944\n",
      "Iteration 385, loss = 0.13730266\n",
      "Iteration 386, loss = 0.13696461\n",
      "Iteration 387, loss = 0.13661660\n",
      "Iteration 388, loss = 0.13629293\n",
      "Iteration 389, loss = 0.13596154\n",
      "Iteration 390, loss = 0.13564501\n",
      "Iteration 391, loss = 0.13530091\n",
      "Iteration 392, loss = 0.13503104\n",
      "Iteration 393, loss = 0.13467147\n",
      "Iteration 394, loss = 0.13436326\n",
      "Iteration 395, loss = 0.13399204\n",
      "Iteration 396, loss = 0.13365848\n",
      "Iteration 397, loss = 0.13337569\n",
      "Iteration 398, loss = 0.13303392\n",
      "Iteration 399, loss = 0.13270759\n",
      "Iteration 400, loss = 0.13242162\n",
      "Iteration 401, loss = 0.13211189\n",
      "Iteration 402, loss = 0.13178955\n",
      "Iteration 403, loss = 0.13148883\n",
      "Iteration 404, loss = 0.13117240\n",
      "Iteration 405, loss = 0.13084161\n",
      "Iteration 406, loss = 0.13055605\n",
      "Iteration 407, loss = 0.13029895\n",
      "Iteration 408, loss = 0.12996781\n",
      "Iteration 409, loss = 0.12962547\n",
      "Iteration 410, loss = 0.12929906\n",
      "Iteration 411, loss = 0.12901259\n",
      "Iteration 412, loss = 0.12869667\n",
      "Iteration 413, loss = 0.12843404\n",
      "Iteration 414, loss = 0.12814365\n",
      "Iteration 415, loss = 0.12782537\n",
      "Iteration 416, loss = 0.12753387\n",
      "Iteration 417, loss = 0.12723904\n",
      "Iteration 418, loss = 0.12697678\n",
      "Iteration 419, loss = 0.12665835\n",
      "Iteration 420, loss = 0.12635914\n",
      "Iteration 421, loss = 0.12607417\n",
      "Iteration 422, loss = 0.12578776\n",
      "Iteration 423, loss = 0.12552103\n",
      "Iteration 424, loss = 0.12526623\n",
      "Iteration 425, loss = 0.12500658\n",
      "Iteration 426, loss = 0.12471439\n",
      "Iteration 427, loss = 0.12445047\n",
      "Iteration 428, loss = 0.12416774\n",
      "Iteration 429, loss = 0.12392635\n",
      "Iteration 430, loss = 0.12364357\n",
      "Iteration 431, loss = 0.12339037\n",
      "Iteration 432, loss = 0.12314592\n",
      "Iteration 433, loss = 0.12284505\n",
      "Iteration 434, loss = 0.12258707\n",
      "Iteration 435, loss = 0.12233238\n",
      "Iteration 436, loss = 0.12208250\n",
      "Iteration 437, loss = 0.12182452\n",
      "Iteration 438, loss = 0.12159068\n",
      "Iteration 439, loss = 0.12130915\n",
      "Iteration 440, loss = 0.12106829\n",
      "Iteration 441, loss = 0.12083504\n",
      "Iteration 442, loss = 0.12059385\n",
      "Iteration 443, loss = 0.12033536\n",
      "Iteration 444, loss = 0.12009642\n",
      "Iteration 445, loss = 0.11985173\n",
      "Iteration 446, loss = 0.11962199\n",
      "Iteration 447, loss = 0.11934947\n",
      "Iteration 448, loss = 0.11912241\n",
      "Iteration 449, loss = 0.11887345\n",
      "Iteration 450, loss = 0.11865893\n",
      "Iteration 451, loss = 0.11839608\n",
      "Iteration 452, loss = 0.11815322\n",
      "Iteration 453, loss = 0.11795998\n",
      "Iteration 454, loss = 0.11768472\n",
      "Iteration 455, loss = 0.11744983\n",
      "Iteration 456, loss = 0.11721237\n",
      "Iteration 457, loss = 0.11698247\n",
      "Iteration 458, loss = 0.11676329\n",
      "Iteration 459, loss = 0.11653699\n",
      "Iteration 460, loss = 0.11631284\n",
      "Iteration 461, loss = 0.11611680\n",
      "Iteration 462, loss = 0.11586536\n",
      "Iteration 463, loss = 0.11564330\n",
      "Iteration 464, loss = 0.11544297\n",
      "Iteration 465, loss = 0.11520982\n",
      "Iteration 466, loss = 0.11500279\n",
      "Iteration 467, loss = 0.11477449\n",
      "Iteration 468, loss = 0.11456714\n",
      "Iteration 469, loss = 0.11435252\n",
      "Iteration 470, loss = 0.11413448\n",
      "Iteration 471, loss = 0.11391821\n",
      "Iteration 472, loss = 0.11371830\n",
      "Iteration 473, loss = 0.11350531\n",
      "Iteration 474, loss = 0.11329446\n",
      "Iteration 475, loss = 0.11313122\n",
      "Iteration 476, loss = 0.11287114\n",
      "Iteration 477, loss = 0.11267102\n",
      "Iteration 478, loss = 0.11246070\n",
      "Iteration 479, loss = 0.11225220\n",
      "Iteration 480, loss = 0.11205933\n",
      "Iteration 481, loss = 0.11186733\n",
      "Iteration 482, loss = 0.11164382\n",
      "Iteration 483, loss = 0.11144755\n",
      "Iteration 484, loss = 0.11125907\n",
      "Iteration 485, loss = 0.11104189\n",
      "Iteration 486, loss = 0.11086182\n",
      "Iteration 487, loss = 0.11065971\n",
      "Iteration 488, loss = 0.11044235\n",
      "Iteration 489, loss = 0.11025618\n",
      "Iteration 490, loss = 0.11006439\n",
      "Iteration 491, loss = 0.10986981\n",
      "Iteration 492, loss = 0.10967298\n",
      "Iteration 493, loss = 0.10947255\n",
      "Iteration 494, loss = 0.10928684\n",
      "Iteration 495, loss = 0.10909314\n",
      "Iteration 496, loss = 0.10892441\n",
      "Iteration 497, loss = 0.10870323\n",
      "Iteration 498, loss = 0.10852858\n",
      "Iteration 499, loss = 0.10834578\n",
      "Iteration 500, loss = 0.10816685\n",
      "Iteration 501, loss = 0.10799573\n",
      "Iteration 502, loss = 0.10780393\n",
      "Iteration 503, loss = 0.10762657\n",
      "Iteration 504, loss = 0.10742693\n",
      "Iteration 505, loss = 0.10725971\n",
      "Iteration 506, loss = 0.10708913\n",
      "Iteration 507, loss = 0.10690376\n",
      "Iteration 508, loss = 0.10670734\n",
      "Iteration 509, loss = 0.10654078\n",
      "Iteration 510, loss = 0.10637812\n",
      "Iteration 511, loss = 0.10619263\n",
      "Iteration 512, loss = 0.10602512\n",
      "Iteration 513, loss = 0.10584905\n",
      "Iteration 514, loss = 0.10567757\n",
      "Iteration 515, loss = 0.10548415\n",
      "Iteration 516, loss = 0.10531412\n",
      "Iteration 517, loss = 0.10514327\n",
      "Iteration 518, loss = 0.10496725\n",
      "Iteration 519, loss = 0.10481393\n",
      "Iteration 520, loss = 0.10464387\n",
      "Iteration 521, loss = 0.10448011\n",
      "Iteration 522, loss = 0.10432915\n",
      "Iteration 523, loss = 0.10417123\n",
      "Iteration 524, loss = 0.10399328\n",
      "Iteration 525, loss = 0.10383719\n",
      "Iteration 526, loss = 0.10369535\n",
      "Iteration 527, loss = 0.10353011\n",
      "Iteration 528, loss = 0.10335638\n",
      "Iteration 529, loss = 0.10317468\n",
      "Iteration 530, loss = 0.10303120\n",
      "Iteration 531, loss = 0.10284162\n",
      "Iteration 532, loss = 0.10268719\n",
      "Iteration 533, loss = 0.10252387\n",
      "Iteration 534, loss = 0.10238320\n",
      "Iteration 535, loss = 0.10221075\n",
      "Iteration 536, loss = 0.10206567\n",
      "Iteration 537, loss = 0.10190630\n",
      "Iteration 538, loss = 0.10175885\n",
      "Iteration 539, loss = 0.10160842\n",
      "Iteration 540, loss = 0.10146715\n",
      "Iteration 541, loss = 0.10132640\n",
      "Iteration 542, loss = 0.10115780\n",
      "Iteration 543, loss = 0.10101315\n",
      "Iteration 544, loss = 0.10084196\n",
      "Iteration 545, loss = 0.10070394\n",
      "Iteration 546, loss = 0.10057047\n",
      "Iteration 547, loss = 0.10040225\n",
      "Iteration 548, loss = 0.10026826\n",
      "Iteration 549, loss = 0.10010865\n",
      "Iteration 550, loss = 0.09995636\n",
      "Iteration 551, loss = 0.09979837\n",
      "Iteration 552, loss = 0.09964630\n",
      "Iteration 553, loss = 0.09950505\n",
      "Iteration 554, loss = 0.09935465\n",
      "Iteration 555, loss = 0.09920145\n",
      "Iteration 556, loss = 0.09905703\n",
      "Iteration 557, loss = 0.09891490\n",
      "Iteration 558, loss = 0.09877418\n",
      "Iteration 559, loss = 0.09863853\n",
      "Iteration 560, loss = 0.09850096\n",
      "Iteration 561, loss = 0.09835777\n",
      "Iteration 562, loss = 0.09821640\n",
      "Iteration 563, loss = 0.09808597\n",
      "Iteration 564, loss = 0.09795496\n",
      "Iteration 565, loss = 0.09782853\n",
      "Iteration 566, loss = 0.09766534\n",
      "Iteration 567, loss = 0.09751651\n",
      "Iteration 568, loss = 0.09738258\n",
      "Iteration 569, loss = 0.09723750\n",
      "Iteration 570, loss = 0.09710140\n",
      "Iteration 571, loss = 0.09697879\n",
      "Iteration 572, loss = 0.09683706\n",
      "Iteration 573, loss = 0.09670740\n",
      "Iteration 574, loss = 0.09657002\n",
      "Iteration 575, loss = 0.09643807\n",
      "Iteration 576, loss = 0.09630672\n",
      "Iteration 577, loss = 0.09618768\n",
      "Iteration 578, loss = 0.09605764\n",
      "Iteration 579, loss = 0.09593386\n",
      "Iteration 580, loss = 0.09579658\n",
      "Iteration 581, loss = 0.09567085\n",
      "Iteration 582, loss = 0.09554974\n",
      "Iteration 583, loss = 0.09542591\n",
      "Iteration 584, loss = 0.09529017\n",
      "Iteration 585, loss = 0.09517373\n",
      "Iteration 586, loss = 0.09506051\n",
      "Iteration 587, loss = 0.09492595\n",
      "Iteration 588, loss = 0.09480004\n",
      "Iteration 589, loss = 0.09469476\n",
      "Iteration 590, loss = 0.09455835\n",
      "Iteration 591, loss = 0.09444620\n",
      "Iteration 592, loss = 0.09433637\n",
      "Iteration 593, loss = 0.09419991\n",
      "Iteration 594, loss = 0.09408423\n",
      "Iteration 595, loss = 0.09395852\n",
      "Iteration 596, loss = 0.09383033\n",
      "Iteration 597, loss = 0.09369745\n",
      "Iteration 598, loss = 0.09358370\n",
      "Iteration 599, loss = 0.09346624\n",
      "Iteration 600, loss = 0.09334720\n",
      "Iteration 601, loss = 0.09323368\n",
      "Iteration 602, loss = 0.09312429\n",
      "Iteration 603, loss = 0.09300391\n",
      "Iteration 604, loss = 0.09289298\n",
      "Iteration 605, loss = 0.09277893\n",
      "Iteration 606, loss = 0.09267978\n",
      "Iteration 607, loss = 0.09255957\n",
      "Iteration 608, loss = 0.09243705\n",
      "Iteration 609, loss = 0.09232225\n",
      "Iteration 610, loss = 0.09220867\n",
      "Iteration 611, loss = 0.09209850\n",
      "Iteration 612, loss = 0.09199042\n",
      "Iteration 613, loss = 0.09187470\n",
      "Iteration 614, loss = 0.09177039\n",
      "Iteration 615, loss = 0.09167033\n",
      "Iteration 616, loss = 0.09155521\n",
      "Iteration 617, loss = 0.09144249\n",
      "Iteration 618, loss = 0.09132947\n",
      "Iteration 619, loss = 0.09124021\n",
      "Iteration 620, loss = 0.09111743\n",
      "Iteration 621, loss = 0.09101259\n",
      "Iteration 622, loss = 0.09091274\n",
      "Iteration 623, loss = 0.09080468\n",
      "Iteration 624, loss = 0.09069529\n",
      "Iteration 625, loss = 0.09058393\n",
      "Iteration 626, loss = 0.09048365\n",
      "Iteration 627, loss = 0.09037790\n",
      "Iteration 628, loss = 0.09027451\n",
      "Iteration 629, loss = 0.09017101\n",
      "Iteration 630, loss = 0.09006992\n",
      "Iteration 631, loss = 0.08996767\n",
      "Iteration 632, loss = 0.08986615\n",
      "Iteration 633, loss = 0.08977300\n",
      "Iteration 634, loss = 0.08966375\n",
      "Iteration 635, loss = 0.08956192\n",
      "Iteration 636, loss = 0.08946525\n",
      "Iteration 637, loss = 0.08937179\n",
      "Iteration 638, loss = 0.08927550\n",
      "Iteration 639, loss = 0.08918207\n",
      "Iteration 640, loss = 0.08908079\n",
      "Iteration 641, loss = 0.08899373\n",
      "Iteration 642, loss = 0.08888914\n",
      "Iteration 643, loss = 0.08878535\n",
      "Iteration 644, loss = 0.08868839\n",
      "Iteration 645, loss = 0.08857050\n",
      "Iteration 646, loss = 0.08848011\n",
      "Iteration 647, loss = 0.08839053\n",
      "Iteration 648, loss = 0.08829763\n",
      "Iteration 649, loss = 0.08819054\n",
      "Iteration 650, loss = 0.08809506\n",
      "Iteration 651, loss = 0.08799839\n",
      "Iteration 652, loss = 0.08790890\n",
      "Iteration 653, loss = 0.08781375\n",
      "Iteration 654, loss = 0.08771390\n",
      "Iteration 655, loss = 0.08762388\n",
      "Iteration 656, loss = 0.08753393\n",
      "Iteration 657, loss = 0.08744053\n",
      "Iteration 658, loss = 0.08734953\n",
      "Iteration 659, loss = 0.08725063\n",
      "Iteration 660, loss = 0.08716755\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.37015101\n",
      "Iteration 2, loss = 2.34714439\n",
      "Iteration 3, loss = 2.31857775\n",
      "Iteration 4, loss = 2.28991579\n",
      "Iteration 5, loss = 2.26321596\n",
      "Iteration 6, loss = 2.23807555\n",
      "Iteration 7, loss = 2.21417239\n",
      "Iteration 8, loss = 2.19049506\n",
      "Iteration 9, loss = 2.16693590\n",
      "Iteration 10, loss = 2.14389127\n",
      "Iteration 11, loss = 2.12057634\n",
      "Iteration 12, loss = 2.09688137\n",
      "Iteration 13, loss = 2.07311174\n",
      "Iteration 14, loss = 2.04902858\n",
      "Iteration 15, loss = 2.02436047\n",
      "Iteration 16, loss = 1.99946653\n",
      "Iteration 17, loss = 1.97441518\n",
      "Iteration 18, loss = 1.94889998\n",
      "Iteration 19, loss = 1.92304606\n",
      "Iteration 20, loss = 1.89679327\n",
      "Iteration 21, loss = 1.86955221\n",
      "Iteration 22, loss = 1.84256049\n",
      "Iteration 23, loss = 1.81506270\n",
      "Iteration 24, loss = 1.78754942\n",
      "Iteration 25, loss = 1.75996925\n",
      "Iteration 26, loss = 1.73188316\n",
      "Iteration 27, loss = 1.70382178\n",
      "Iteration 28, loss = 1.67613600\n",
      "Iteration 29, loss = 1.64773890\n",
      "Iteration 30, loss = 1.61992301\n",
      "Iteration 31, loss = 1.59198199\n",
      "Iteration 32, loss = 1.56413956\n",
      "Iteration 33, loss = 1.53700741\n",
      "Iteration 34, loss = 1.50969667\n",
      "Iteration 35, loss = 1.48310939\n",
      "Iteration 36, loss = 1.45632590\n",
      "Iteration 37, loss = 1.42996480\n",
      "Iteration 38, loss = 1.40426542\n",
      "Iteration 39, loss = 1.37903592\n",
      "Iteration 40, loss = 1.35403247\n",
      "Iteration 41, loss = 1.32963033\n",
      "Iteration 42, loss = 1.30587608\n",
      "Iteration 43, loss = 1.28205825\n",
      "Iteration 44, loss = 1.25931295\n",
      "Iteration 45, loss = 1.23686943\n",
      "Iteration 46, loss = 1.21498217\n",
      "Iteration 47, loss = 1.19335966\n",
      "Iteration 48, loss = 1.17206669\n",
      "Iteration 49, loss = 1.15156437\n",
      "Iteration 50, loss = 1.13110382\n",
      "Iteration 51, loss = 1.11135980\n",
      "Iteration 52, loss = 1.09210033\n",
      "Iteration 53, loss = 1.07311556\n",
      "Iteration 54, loss = 1.05457169\n",
      "Iteration 55, loss = 1.03666232\n",
      "Iteration 56, loss = 1.01908064\n",
      "Iteration 57, loss = 1.00219744\n",
      "Iteration 58, loss = 0.98550683\n",
      "Iteration 59, loss = 0.96927700\n",
      "Iteration 60, loss = 0.95337238\n",
      "Iteration 61, loss = 0.93785187\n",
      "Iteration 62, loss = 0.92288128\n",
      "Iteration 63, loss = 0.90814399\n",
      "Iteration 64, loss = 0.89387196\n",
      "Iteration 65, loss = 0.87975085\n",
      "Iteration 66, loss = 0.86613402\n",
      "Iteration 67, loss = 0.85266869\n",
      "Iteration 68, loss = 0.83981091\n",
      "Iteration 69, loss = 0.82693505\n",
      "Iteration 70, loss = 0.81460906\n",
      "Iteration 71, loss = 0.80252505\n",
      "Iteration 72, loss = 0.79106943\n",
      "Iteration 73, loss = 0.77953836\n",
      "Iteration 74, loss = 0.76842088\n",
      "Iteration 75, loss = 0.75752756\n",
      "Iteration 76, loss = 0.74702419\n",
      "Iteration 77, loss = 0.73662789\n",
      "Iteration 78, loss = 0.72649241\n",
      "Iteration 79, loss = 0.71659861\n",
      "Iteration 80, loss = 0.70688507\n",
      "Iteration 81, loss = 0.69744163\n",
      "Iteration 82, loss = 0.68818283\n",
      "Iteration 83, loss = 0.67920945\n",
      "Iteration 84, loss = 0.67053644\n",
      "Iteration 85, loss = 0.66192928\n",
      "Iteration 86, loss = 0.65352954\n",
      "Iteration 87, loss = 0.64539305\n",
      "Iteration 88, loss = 0.63746595\n",
      "Iteration 89, loss = 0.62964479\n",
      "Iteration 90, loss = 0.62201072\n",
      "Iteration 91, loss = 0.61454134\n",
      "Iteration 92, loss = 0.60714378\n",
      "Iteration 93, loss = 0.60019608\n",
      "Iteration 94, loss = 0.59322018\n",
      "Iteration 95, loss = 0.58638656\n",
      "Iteration 96, loss = 0.57973161\n",
      "Iteration 97, loss = 0.57321517\n",
      "Iteration 98, loss = 0.56699364\n",
      "Iteration 99, loss = 0.56073293\n",
      "Iteration 100, loss = 0.55463200\n",
      "Iteration 101, loss = 0.54852900\n",
      "Iteration 102, loss = 0.54265790\n",
      "Iteration 103, loss = 0.53687890\n",
      "Iteration 104, loss = 0.53119293\n",
      "Iteration 105, loss = 0.52560750\n",
      "Iteration 106, loss = 0.52040461\n",
      "Iteration 107, loss = 0.51497855\n",
      "Iteration 108, loss = 0.50979068\n",
      "Iteration 109, loss = 0.50450257\n",
      "Iteration 110, loss = 0.49930015\n",
      "Iteration 111, loss = 0.49429440\n",
      "Iteration 112, loss = 0.48950265\n",
      "Iteration 113, loss = 0.48463176\n",
      "Iteration 114, loss = 0.47998168\n",
      "Iteration 115, loss = 0.47542401\n",
      "Iteration 116, loss = 0.47101021\n",
      "Iteration 117, loss = 0.46664842\n",
      "Iteration 118, loss = 0.46226933\n",
      "Iteration 119, loss = 0.45802784\n",
      "Iteration 120, loss = 0.45378406\n",
      "Iteration 121, loss = 0.44960331\n",
      "Iteration 122, loss = 0.44567486\n",
      "Iteration 123, loss = 0.44176378\n",
      "Iteration 124, loss = 0.43782182\n",
      "Iteration 125, loss = 0.43395161\n",
      "Iteration 126, loss = 0.43024773\n",
      "Iteration 127, loss = 0.42659176\n",
      "Iteration 128, loss = 0.42300021\n",
      "Iteration 129, loss = 0.41954705\n",
      "Iteration 130, loss = 0.41600487\n",
      "Iteration 131, loss = 0.41260041\n",
      "Iteration 132, loss = 0.40912359\n",
      "Iteration 133, loss = 0.40577338\n",
      "Iteration 134, loss = 0.40247480\n",
      "Iteration 135, loss = 0.39923526\n",
      "Iteration 136, loss = 0.39606233\n",
      "Iteration 137, loss = 0.39282268\n",
      "Iteration 138, loss = 0.38971004\n",
      "Iteration 139, loss = 0.38663494\n",
      "Iteration 140, loss = 0.38365175\n",
      "Iteration 141, loss = 0.38059410\n",
      "Iteration 142, loss = 0.37785284\n",
      "Iteration 143, loss = 0.37495929\n",
      "Iteration 144, loss = 0.37211536\n",
      "Iteration 145, loss = 0.36936523\n",
      "Iteration 146, loss = 0.36661166\n",
      "Iteration 147, loss = 0.36393560\n",
      "Iteration 148, loss = 0.36129294\n",
      "Iteration 149, loss = 0.35867543\n",
      "Iteration 150, loss = 0.35612392\n",
      "Iteration 151, loss = 0.35373405\n",
      "Iteration 152, loss = 0.35116720\n",
      "Iteration 153, loss = 0.34868616\n",
      "Iteration 154, loss = 0.34626196\n",
      "Iteration 155, loss = 0.34392850\n",
      "Iteration 156, loss = 0.34150645\n",
      "Iteration 157, loss = 0.33904359\n",
      "Iteration 158, loss = 0.33668763\n",
      "Iteration 159, loss = 0.33452287\n",
      "Iteration 160, loss = 0.33217780\n",
      "Iteration 161, loss = 0.32995043\n",
      "Iteration 162, loss = 0.32782438\n",
      "Iteration 163, loss = 0.32570044\n",
      "Iteration 164, loss = 0.32362057\n",
      "Iteration 165, loss = 0.32145633\n",
      "Iteration 166, loss = 0.31947439\n",
      "Iteration 167, loss = 0.31739983\n",
      "Iteration 168, loss = 0.31543180\n",
      "Iteration 169, loss = 0.31343704\n",
      "Iteration 170, loss = 0.31152318\n",
      "Iteration 171, loss = 0.30954924\n",
      "Iteration 172, loss = 0.30766470\n",
      "Iteration 173, loss = 0.30583557\n",
      "Iteration 174, loss = 0.30391475\n",
      "Iteration 175, loss = 0.30214656\n",
      "Iteration 176, loss = 0.30025944\n",
      "Iteration 177, loss = 0.29840776\n",
      "Iteration 178, loss = 0.29668633\n",
      "Iteration 179, loss = 0.29496054\n",
      "Iteration 180, loss = 0.29318222\n",
      "Iteration 181, loss = 0.29146572\n",
      "Iteration 182, loss = 0.28980272\n",
      "Iteration 183, loss = 0.28809230\n",
      "Iteration 184, loss = 0.28638747\n",
      "Iteration 185, loss = 0.28475954\n",
      "Iteration 186, loss = 0.28320532\n",
      "Iteration 187, loss = 0.28166359\n",
      "Iteration 188, loss = 0.28000938\n",
      "Iteration 189, loss = 0.27837216\n",
      "Iteration 190, loss = 0.27680217\n",
      "Iteration 191, loss = 0.27526121\n",
      "Iteration 192, loss = 0.27377511\n",
      "Iteration 193, loss = 0.27233090\n",
      "Iteration 194, loss = 0.27079423\n",
      "Iteration 195, loss = 0.26934467\n",
      "Iteration 196, loss = 0.26791013\n",
      "Iteration 197, loss = 0.26656095\n",
      "Iteration 198, loss = 0.26520033\n",
      "Iteration 199, loss = 0.26376631\n",
      "Iteration 200, loss = 0.26230357\n",
      "Iteration 201, loss = 0.26099962\n",
      "Iteration 202, loss = 0.25967690\n",
      "Iteration 203, loss = 0.25833535\n",
      "Iteration 204, loss = 0.25706445\n",
      "Iteration 205, loss = 0.25570142\n",
      "Iteration 206, loss = 0.25437138\n",
      "Iteration 207, loss = 0.25305297\n",
      "Iteration 208, loss = 0.25179557\n",
      "Iteration 209, loss = 0.25052068\n",
      "Iteration 210, loss = 0.24928760\n",
      "Iteration 211, loss = 0.24804542\n",
      "Iteration 212, loss = 0.24684154\n",
      "Iteration 213, loss = 0.24562091\n",
      "Iteration 214, loss = 0.24444706\n",
      "Iteration 215, loss = 0.24323215\n",
      "Iteration 216, loss = 0.24205896\n",
      "Iteration 217, loss = 0.24101655\n",
      "Iteration 218, loss = 0.23978879\n",
      "Iteration 219, loss = 0.23869709\n",
      "Iteration 220, loss = 0.23752479\n",
      "Iteration 221, loss = 0.23646573\n",
      "Iteration 222, loss = 0.23532788\n",
      "Iteration 223, loss = 0.23425354\n",
      "Iteration 224, loss = 0.23314925\n",
      "Iteration 225, loss = 0.23214094\n",
      "Iteration 226, loss = 0.23103642\n",
      "Iteration 227, loss = 0.22998689\n",
      "Iteration 228, loss = 0.22904205\n",
      "Iteration 229, loss = 0.22805690\n",
      "Iteration 230, loss = 0.22697677\n",
      "Iteration 231, loss = 0.22596803\n",
      "Iteration 232, loss = 0.22495992\n",
      "Iteration 233, loss = 0.22394975\n",
      "Iteration 234, loss = 0.22302723\n",
      "Iteration 235, loss = 0.22200981\n",
      "Iteration 236, loss = 0.22107481\n",
      "Iteration 237, loss = 0.22009665\n",
      "Iteration 238, loss = 0.21913727\n",
      "Iteration 239, loss = 0.21822722\n",
      "Iteration 240, loss = 0.21730057\n",
      "Iteration 241, loss = 0.21634692\n",
      "Iteration 242, loss = 0.21543733\n",
      "Iteration 243, loss = 0.21447087\n",
      "Iteration 244, loss = 0.21361226\n",
      "Iteration 245, loss = 0.21269204\n",
      "Iteration 246, loss = 0.21180851\n",
      "Iteration 247, loss = 0.21092132\n",
      "Iteration 248, loss = 0.21012451\n",
      "Iteration 249, loss = 0.20924704\n",
      "Iteration 250, loss = 0.20841225\n",
      "Iteration 251, loss = 0.20752280\n",
      "Iteration 252, loss = 0.20667547\n",
      "Iteration 253, loss = 0.20575390\n",
      "Iteration 254, loss = 0.20489855\n",
      "Iteration 255, loss = 0.20412499\n",
      "Iteration 256, loss = 0.20328043\n",
      "Iteration 257, loss = 0.20243312\n",
      "Iteration 258, loss = 0.20166583\n",
      "Iteration 259, loss = 0.20085723\n",
      "Iteration 260, loss = 0.20004114\n",
      "Iteration 261, loss = 0.19929299\n",
      "Iteration 262, loss = 0.19851547\n",
      "Iteration 263, loss = 0.19775201\n",
      "Iteration 264, loss = 0.19698161\n",
      "Iteration 265, loss = 0.19623478\n",
      "Iteration 266, loss = 0.19549448\n",
      "Iteration 267, loss = 0.19477458\n",
      "Iteration 268, loss = 0.19406742\n",
      "Iteration 269, loss = 0.19335955\n",
      "Iteration 270, loss = 0.19266805\n",
      "Iteration 271, loss = 0.19195016\n",
      "Iteration 272, loss = 0.19133187\n",
      "Iteration 273, loss = 0.19054069\n",
      "Iteration 274, loss = 0.18987413\n",
      "Iteration 275, loss = 0.18916471\n",
      "Iteration 276, loss = 0.18850866\n",
      "Iteration 277, loss = 0.18779818\n",
      "Iteration 278, loss = 0.18714924\n",
      "Iteration 279, loss = 0.18649921\n",
      "Iteration 280, loss = 0.18578747\n",
      "Iteration 281, loss = 0.18510812\n",
      "Iteration 282, loss = 0.18446705\n",
      "Iteration 283, loss = 0.18386548\n",
      "Iteration 284, loss = 0.18317562\n",
      "Iteration 285, loss = 0.18255992\n",
      "Iteration 286, loss = 0.18195266\n",
      "Iteration 287, loss = 0.18132323\n",
      "Iteration 288, loss = 0.18071878\n",
      "Iteration 289, loss = 0.18007892\n",
      "Iteration 290, loss = 0.17944085\n",
      "Iteration 291, loss = 0.17882592\n",
      "Iteration 292, loss = 0.17827141\n",
      "Iteration 293, loss = 0.17764183\n",
      "Iteration 294, loss = 0.17705410\n",
      "Iteration 295, loss = 0.17645703\n",
      "Iteration 296, loss = 0.17587984\n",
      "Iteration 297, loss = 0.17529109\n",
      "Iteration 298, loss = 0.17471048\n",
      "Iteration 299, loss = 0.17414555\n",
      "Iteration 300, loss = 0.17357993\n",
      "Iteration 301, loss = 0.17302373\n",
      "Iteration 302, loss = 0.17243319\n",
      "Iteration 303, loss = 0.17186960\n",
      "Iteration 304, loss = 0.17134434\n",
      "Iteration 305, loss = 0.17077147\n",
      "Iteration 306, loss = 0.17026398\n",
      "Iteration 307, loss = 0.16970709\n",
      "Iteration 308, loss = 0.16922311\n",
      "Iteration 309, loss = 0.16860868\n",
      "Iteration 310, loss = 0.16803414\n",
      "Iteration 311, loss = 0.16748809\n",
      "Iteration 312, loss = 0.16693952\n",
      "Iteration 313, loss = 0.16645681\n",
      "Iteration 314, loss = 0.16601261\n",
      "Iteration 315, loss = 0.16544674\n",
      "Iteration 316, loss = 0.16491489\n",
      "Iteration 317, loss = 0.16438426\n",
      "Iteration 318, loss = 0.16391220\n",
      "Iteration 319, loss = 0.16338943\n",
      "Iteration 320, loss = 0.16288660\n",
      "Iteration 321, loss = 0.16241093\n",
      "Iteration 322, loss = 0.16190184\n",
      "Iteration 323, loss = 0.16141993\n",
      "Iteration 324, loss = 0.16090768\n",
      "Iteration 325, loss = 0.16042484\n",
      "Iteration 326, loss = 0.15992472\n",
      "Iteration 327, loss = 0.15944746\n",
      "Iteration 328, loss = 0.15897105\n",
      "Iteration 329, loss = 0.15851919\n",
      "Iteration 330, loss = 0.15806899\n",
      "Iteration 331, loss = 0.15754877\n",
      "Iteration 332, loss = 0.15707838\n",
      "Iteration 333, loss = 0.15659420\n",
      "Iteration 334, loss = 0.15612431\n",
      "Iteration 335, loss = 0.15565919\n",
      "Iteration 336, loss = 0.15520729\n",
      "Iteration 337, loss = 0.15476922\n",
      "Iteration 338, loss = 0.15432373\n",
      "Iteration 339, loss = 0.15387210\n",
      "Iteration 340, loss = 0.15343293\n",
      "Iteration 341, loss = 0.15300654\n",
      "Iteration 342, loss = 0.15254129\n",
      "Iteration 343, loss = 0.15212422\n",
      "Iteration 344, loss = 0.15169625\n",
      "Iteration 345, loss = 0.15126535\n",
      "Iteration 346, loss = 0.15084625\n",
      "Iteration 347, loss = 0.15044189\n",
      "Iteration 348, loss = 0.14998213\n",
      "Iteration 349, loss = 0.14955094\n",
      "Iteration 350, loss = 0.14916785\n",
      "Iteration 351, loss = 0.14874623\n",
      "Iteration 352, loss = 0.14835181\n",
      "Iteration 353, loss = 0.14797754\n",
      "Iteration 354, loss = 0.14756368\n",
      "Iteration 355, loss = 0.14715929\n",
      "Iteration 356, loss = 0.14674192\n",
      "Iteration 357, loss = 0.14632334\n",
      "Iteration 358, loss = 0.14592244\n",
      "Iteration 359, loss = 0.14551476\n",
      "Iteration 360, loss = 0.14514616\n",
      "Iteration 361, loss = 0.14474803\n",
      "Iteration 362, loss = 0.14436929\n",
      "Iteration 363, loss = 0.14402336\n",
      "Iteration 364, loss = 0.14362402\n",
      "Iteration 365, loss = 0.14328586\n",
      "Iteration 366, loss = 0.14287760\n",
      "Iteration 367, loss = 0.14251175\n",
      "Iteration 368, loss = 0.14217051\n",
      "Iteration 369, loss = 0.14180120\n",
      "Iteration 370, loss = 0.14145266\n",
      "Iteration 371, loss = 0.14109633\n",
      "Iteration 372, loss = 0.14073982\n",
      "Iteration 373, loss = 0.14038344\n",
      "Iteration 374, loss = 0.14004918\n",
      "Iteration 375, loss = 0.13970387\n",
      "Iteration 376, loss = 0.13935652\n",
      "Iteration 377, loss = 0.13900361\n",
      "Iteration 378, loss = 0.13867479\n",
      "Iteration 379, loss = 0.13830657\n",
      "Iteration 380, loss = 0.13797612\n",
      "Iteration 381, loss = 0.13761747\n",
      "Iteration 382, loss = 0.13726157\n",
      "Iteration 383, loss = 0.13693754\n",
      "Iteration 384, loss = 0.13658748\n",
      "Iteration 385, loss = 0.13627628\n",
      "Iteration 386, loss = 0.13592719\n",
      "Iteration 387, loss = 0.13560409\n",
      "Iteration 388, loss = 0.13526553\n",
      "Iteration 389, loss = 0.13494314\n",
      "Iteration 390, loss = 0.13463988\n",
      "Iteration 391, loss = 0.13429521\n",
      "Iteration 392, loss = 0.13401760\n",
      "Iteration 393, loss = 0.13366313\n",
      "Iteration 394, loss = 0.13335202\n",
      "Iteration 395, loss = 0.13300851\n",
      "Iteration 396, loss = 0.13272850\n",
      "Iteration 397, loss = 0.13241680\n",
      "Iteration 398, loss = 0.13213597\n",
      "Iteration 399, loss = 0.13180409\n",
      "Iteration 400, loss = 0.13150003\n",
      "Iteration 401, loss = 0.13118732\n",
      "Iteration 402, loss = 0.13087347\n",
      "Iteration 403, loss = 0.13059921\n",
      "Iteration 404, loss = 0.13028605\n",
      "Iteration 405, loss = 0.12996620\n",
      "Iteration 406, loss = 0.12970599\n",
      "Iteration 407, loss = 0.12942284\n",
      "Iteration 408, loss = 0.12912276\n",
      "Iteration 409, loss = 0.12880690\n",
      "Iteration 410, loss = 0.12847434\n",
      "Iteration 411, loss = 0.12821491\n",
      "Iteration 412, loss = 0.12789344\n",
      "Iteration 413, loss = 0.12762531\n",
      "Iteration 414, loss = 0.12734506\n",
      "Iteration 415, loss = 0.12702363\n",
      "Iteration 416, loss = 0.12675115\n",
      "Iteration 417, loss = 0.12646401\n",
      "Iteration 418, loss = 0.12619172\n",
      "Iteration 419, loss = 0.12591635\n",
      "Iteration 420, loss = 0.12564046\n",
      "Iteration 421, loss = 0.12534158\n",
      "Iteration 422, loss = 0.12507465\n",
      "Iteration 423, loss = 0.12481019\n",
      "Iteration 424, loss = 0.12456766\n",
      "Iteration 425, loss = 0.12429797\n",
      "Iteration 426, loss = 0.12402330\n",
      "Iteration 427, loss = 0.12375592\n",
      "Iteration 428, loss = 0.12346594\n",
      "Iteration 429, loss = 0.12322789\n",
      "Iteration 430, loss = 0.12295484\n",
      "Iteration 431, loss = 0.12269389\n",
      "Iteration 432, loss = 0.12245761\n",
      "Iteration 433, loss = 0.12217095\n",
      "Iteration 434, loss = 0.12192167\n",
      "Iteration 435, loss = 0.12166134\n",
      "Iteration 436, loss = 0.12141560\n",
      "Iteration 437, loss = 0.12117532\n",
      "Iteration 438, loss = 0.12095356\n",
      "Iteration 439, loss = 0.12066741\n",
      "Iteration 440, loss = 0.12041636\n",
      "Iteration 441, loss = 0.12019071\n",
      "Iteration 442, loss = 0.11995267\n",
      "Iteration 443, loss = 0.11968631\n",
      "Iteration 444, loss = 0.11944787\n",
      "Iteration 445, loss = 0.11920617\n",
      "Iteration 446, loss = 0.11899053\n",
      "Iteration 447, loss = 0.11872886\n",
      "Iteration 448, loss = 0.11849000\n",
      "Iteration 449, loss = 0.11825936\n",
      "Iteration 450, loss = 0.11804513\n",
      "Iteration 451, loss = 0.11780093\n",
      "Iteration 452, loss = 0.11755207\n",
      "Iteration 453, loss = 0.11735540\n",
      "Iteration 454, loss = 0.11710346\n",
      "Iteration 455, loss = 0.11684273\n",
      "Iteration 456, loss = 0.11661545\n",
      "Iteration 457, loss = 0.11641791\n",
      "Iteration 458, loss = 0.11618301\n",
      "Iteration 459, loss = 0.11594910\n",
      "Iteration 460, loss = 0.11572813\n",
      "Iteration 461, loss = 0.11553913\n",
      "Iteration 462, loss = 0.11529146\n",
      "Iteration 463, loss = 0.11508114\n",
      "Iteration 464, loss = 0.11487662\n",
      "Iteration 465, loss = 0.11465689\n",
      "Iteration 466, loss = 0.11447922\n",
      "Iteration 467, loss = 0.11422702\n",
      "Iteration 468, loss = 0.11402206\n",
      "Iteration 469, loss = 0.11381510\n",
      "Iteration 470, loss = 0.11359876\n",
      "Iteration 471, loss = 0.11340364\n",
      "Iteration 472, loss = 0.11318744\n",
      "Iteration 473, loss = 0.11297645\n",
      "Iteration 474, loss = 0.11279082\n",
      "Iteration 475, loss = 0.11261582\n",
      "Iteration 476, loss = 0.11235455\n",
      "Iteration 477, loss = 0.11216247\n",
      "Iteration 478, loss = 0.11195787\n",
      "Iteration 479, loss = 0.11175165\n",
      "Iteration 480, loss = 0.11155921\n",
      "Iteration 481, loss = 0.11134675\n",
      "Iteration 482, loss = 0.11114727\n",
      "Iteration 483, loss = 0.11094293\n",
      "Iteration 484, loss = 0.11076390\n",
      "Iteration 485, loss = 0.11054328\n",
      "Iteration 486, loss = 0.11036735\n",
      "Iteration 487, loss = 0.11018222\n",
      "Iteration 488, loss = 0.10997654\n",
      "Iteration 489, loss = 0.10979188\n",
      "Iteration 490, loss = 0.10957184\n",
      "Iteration 491, loss = 0.10938945\n",
      "Iteration 492, loss = 0.10919885\n",
      "Iteration 493, loss = 0.10899557\n",
      "Iteration 494, loss = 0.10881021\n",
      "Iteration 495, loss = 0.10861780\n",
      "Iteration 496, loss = 0.10845120\n",
      "Iteration 497, loss = 0.10824555\n",
      "Iteration 498, loss = 0.10806683\n",
      "Iteration 499, loss = 0.10788410\n",
      "Iteration 500, loss = 0.10770355\n",
      "Iteration 501, loss = 0.10754103\n",
      "Iteration 502, loss = 0.10734308\n",
      "Iteration 503, loss = 0.10717066\n",
      "Iteration 504, loss = 0.10699062\n",
      "Iteration 505, loss = 0.10681625\n",
      "Iteration 506, loss = 0.10663181\n",
      "Iteration 507, loss = 0.10645011\n",
      "Iteration 508, loss = 0.10625086\n",
      "Iteration 509, loss = 0.10608031\n",
      "Iteration 510, loss = 0.10591941\n",
      "Iteration 511, loss = 0.10574185\n",
      "Iteration 512, loss = 0.10555867\n",
      "Iteration 513, loss = 0.10539704\n",
      "Iteration 514, loss = 0.10522283\n",
      "Iteration 515, loss = 0.10504318\n",
      "Iteration 516, loss = 0.10487397\n",
      "Iteration 517, loss = 0.10469777\n",
      "Iteration 518, loss = 0.10452620\n",
      "Iteration 519, loss = 0.10436852\n",
      "Iteration 520, loss = 0.10419391\n",
      "Iteration 521, loss = 0.10404681\n",
      "Iteration 522, loss = 0.10388007\n",
      "Iteration 523, loss = 0.10371444\n",
      "Iteration 524, loss = 0.10355050\n",
      "Iteration 525, loss = 0.10339805\n",
      "Iteration 526, loss = 0.10325344\n",
      "Iteration 527, loss = 0.10309040\n",
      "Iteration 528, loss = 0.10293294\n",
      "Iteration 529, loss = 0.10277284\n",
      "Iteration 530, loss = 0.10261751\n",
      "Iteration 531, loss = 0.10243660\n",
      "Iteration 532, loss = 0.10228092\n",
      "Iteration 533, loss = 0.10213107\n",
      "Iteration 534, loss = 0.10198002\n",
      "Iteration 535, loss = 0.10181284\n",
      "Iteration 536, loss = 0.10166873\n",
      "Iteration 537, loss = 0.10150349\n",
      "Iteration 538, loss = 0.10136239\n",
      "Iteration 539, loss = 0.10121176\n",
      "Iteration 540, loss = 0.10106166\n",
      "Iteration 541, loss = 0.10091516\n",
      "Iteration 542, loss = 0.10075734\n",
      "Iteration 543, loss = 0.10061017\n",
      "Iteration 544, loss = 0.10044999\n",
      "Iteration 545, loss = 0.10031699\n",
      "Iteration 546, loss = 0.10018167\n",
      "Iteration 547, loss = 0.10001972\n",
      "Iteration 548, loss = 0.09988690\n",
      "Iteration 549, loss = 0.09973438\n",
      "Iteration 550, loss = 0.09956799\n",
      "Iteration 551, loss = 0.09942249\n",
      "Iteration 552, loss = 0.09926910\n",
      "Iteration 553, loss = 0.09912328\n",
      "Iteration 554, loss = 0.09896752\n",
      "Iteration 555, loss = 0.09881362\n",
      "Iteration 556, loss = 0.09867351\n",
      "Iteration 557, loss = 0.09853717\n",
      "Iteration 558, loss = 0.09840787\n",
      "Iteration 559, loss = 0.09825796\n",
      "Iteration 560, loss = 0.09812684\n",
      "Iteration 561, loss = 0.09797744\n",
      "Iteration 562, loss = 0.09783870\n",
      "Iteration 563, loss = 0.09770610\n",
      "Iteration 564, loss = 0.09758345\n",
      "Iteration 565, loss = 0.09744177\n",
      "Iteration 566, loss = 0.09728779\n",
      "Iteration 567, loss = 0.09715037\n",
      "Iteration 568, loss = 0.09701484\n",
      "Iteration 569, loss = 0.09687247\n",
      "Iteration 570, loss = 0.09673891\n",
      "Iteration 571, loss = 0.09661174\n",
      "Iteration 572, loss = 0.09647551\n",
      "Iteration 573, loss = 0.09634820\n",
      "Iteration 574, loss = 0.09621524\n",
      "Iteration 575, loss = 0.09607952\n",
      "Iteration 576, loss = 0.09595149\n",
      "Iteration 577, loss = 0.09582566\n",
      "Iteration 578, loss = 0.09569794\n",
      "Iteration 579, loss = 0.09557971\n",
      "Iteration 580, loss = 0.09544074\n",
      "Iteration 581, loss = 0.09531514\n",
      "Iteration 582, loss = 0.09518690\n",
      "Iteration 583, loss = 0.09505625\n",
      "Iteration 584, loss = 0.09492974\n",
      "Iteration 585, loss = 0.09482876\n",
      "Iteration 586, loss = 0.09469278\n",
      "Iteration 587, loss = 0.09456644\n",
      "Iteration 588, loss = 0.09444021\n",
      "Iteration 589, loss = 0.09432520\n",
      "Iteration 590, loss = 0.09419454\n",
      "Iteration 591, loss = 0.09408302\n",
      "Iteration 592, loss = 0.09396715\n",
      "Iteration 593, loss = 0.09382393\n",
      "Iteration 594, loss = 0.09371771\n",
      "Iteration 595, loss = 0.09358949\n",
      "Iteration 596, loss = 0.09345538\n",
      "Iteration 597, loss = 0.09333837\n",
      "Iteration 598, loss = 0.09322615\n",
      "Iteration 599, loss = 0.09309774\n",
      "Iteration 600, loss = 0.09296997\n",
      "Iteration 601, loss = 0.09285287\n",
      "Iteration 602, loss = 0.09274469\n",
      "Iteration 603, loss = 0.09262798\n",
      "Iteration 604, loss = 0.09250800\n",
      "Iteration 605, loss = 0.09239515\n",
      "Iteration 606, loss = 0.09230323\n",
      "Iteration 607, loss = 0.09218695\n",
      "Iteration 608, loss = 0.09206098\n",
      "Iteration 609, loss = 0.09194860\n",
      "Iteration 610, loss = 0.09182747\n",
      "Iteration 611, loss = 0.09172660\n",
      "Iteration 612, loss = 0.09162394\n",
      "Iteration 613, loss = 0.09150175\n",
      "Iteration 614, loss = 0.09140799\n",
      "Iteration 615, loss = 0.09129968\n",
      "Iteration 616, loss = 0.09118139\n",
      "Iteration 617, loss = 0.09107210\n",
      "Iteration 618, loss = 0.09095615\n",
      "Iteration 619, loss = 0.09085594\n",
      "Iteration 620, loss = 0.09074078\n",
      "Iteration 621, loss = 0.09064619\n",
      "Iteration 622, loss = 0.09053820\n",
      "Iteration 623, loss = 0.09042739\n",
      "Iteration 624, loss = 0.09032265\n",
      "Iteration 625, loss = 0.09021542\n",
      "Iteration 626, loss = 0.09011496\n",
      "Iteration 627, loss = 0.09000238\n",
      "Iteration 628, loss = 0.08989329\n",
      "Iteration 629, loss = 0.08979682\n",
      "Iteration 630, loss = 0.08969791\n",
      "Iteration 631, loss = 0.08959843\n",
      "Iteration 632, loss = 0.08949434\n",
      "Iteration 633, loss = 0.08939763\n",
      "Iteration 634, loss = 0.08929529\n",
      "Iteration 635, loss = 0.08919130\n",
      "Iteration 636, loss = 0.08908944\n",
      "Iteration 637, loss = 0.08900816\n",
      "Iteration 638, loss = 0.08889856\n",
      "Iteration 639, loss = 0.08880317\n",
      "Iteration 640, loss = 0.08870903\n",
      "Iteration 641, loss = 0.08863480\n",
      "Iteration 642, loss = 0.08851661\n",
      "Iteration 643, loss = 0.08841290\n",
      "Iteration 644, loss = 0.08831487\n",
      "Iteration 645, loss = 0.08819461\n",
      "Iteration 646, loss = 0.08811015\n",
      "Iteration 647, loss = 0.08802120\n",
      "Iteration 648, loss = 0.08793033\n",
      "Iteration 649, loss = 0.08782326\n",
      "Iteration 650, loss = 0.08773084\n",
      "Iteration 651, loss = 0.08763004\n",
      "Iteration 652, loss = 0.08753972\n",
      "Iteration 653, loss = 0.08744871\n",
      "Iteration 654, loss = 0.08734543\n",
      "Iteration 655, loss = 0.08725462\n",
      "Iteration 656, loss = 0.08715879\n",
      "Iteration 657, loss = 0.08707466\n",
      "Iteration 658, loss = 0.08697503\n",
      "Iteration 659, loss = 0.08688524\n",
      "Iteration 660, loss = 0.08680301\n",
      "Iteration 661, loss = 0.08670524\n",
      "Iteration 662, loss = 0.08661663\n",
      "Iteration 663, loss = 0.08652614\n",
      "Iteration 664, loss = 0.08642213\n",
      "Iteration 665, loss = 0.08635486\n",
      "Iteration 666, loss = 0.08625259\n",
      "Iteration 667, loss = 0.08615445\n",
      "Iteration 668, loss = 0.08607291\n",
      "Iteration 669, loss = 0.08598928\n",
      "Iteration 670, loss = 0.08590138\n",
      "Iteration 671, loss = 0.08581509\n",
      "Iteration 672, loss = 0.08571892\n",
      "Iteration 673, loss = 0.08563529\n",
      "Iteration 674, loss = 0.08555141\n",
      "Iteration 675, loss = 0.08546671\n",
      "Iteration 676, loss = 0.08538157\n",
      "Iteration 677, loss = 0.08529684\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.36639183\n",
      "Iteration 2, loss = 2.34375384\n",
      "Iteration 3, loss = 2.31480648\n",
      "Iteration 4, loss = 2.28647854\n",
      "Iteration 5, loss = 2.26092362\n",
      "Iteration 6, loss = 2.23619861\n",
      "Iteration 7, loss = 2.21280464\n",
      "Iteration 8, loss = 2.18937895\n",
      "Iteration 9, loss = 2.16584092\n",
      "Iteration 10, loss = 2.14235554\n",
      "Iteration 11, loss = 2.11874027\n",
      "Iteration 12, loss = 2.09478572\n",
      "Iteration 13, loss = 2.07139107\n",
      "Iteration 14, loss = 2.04735452\n",
      "Iteration 15, loss = 2.02292998\n",
      "Iteration 16, loss = 1.99849579\n",
      "Iteration 17, loss = 1.97288254\n",
      "Iteration 18, loss = 1.94759074\n",
      "Iteration 19, loss = 1.92160865\n",
      "Iteration 20, loss = 1.89553916\n",
      "Iteration 21, loss = 1.86902517\n",
      "Iteration 22, loss = 1.84242653\n",
      "Iteration 23, loss = 1.81565390\n",
      "Iteration 24, loss = 1.78842095\n",
      "Iteration 25, loss = 1.76111430\n",
      "Iteration 26, loss = 1.73378275\n",
      "Iteration 27, loss = 1.70633974\n",
      "Iteration 28, loss = 1.67893408\n",
      "Iteration 29, loss = 1.65119461\n",
      "Iteration 30, loss = 1.62389120\n",
      "Iteration 31, loss = 1.59697857\n",
      "Iteration 32, loss = 1.57003106\n",
      "Iteration 33, loss = 1.54343450\n",
      "Iteration 34, loss = 1.51699939\n",
      "Iteration 35, loss = 1.49121710\n",
      "Iteration 36, loss = 1.46564919\n",
      "Iteration 37, loss = 1.44060488\n",
      "Iteration 38, loss = 1.41570368\n",
      "Iteration 39, loss = 1.39092804\n",
      "Iteration 40, loss = 1.36706271\n",
      "Iteration 41, loss = 1.34357018\n",
      "Iteration 42, loss = 1.32047441\n",
      "Iteration 43, loss = 1.29760572\n",
      "Iteration 44, loss = 1.27536601\n",
      "Iteration 45, loss = 1.25304306\n",
      "Iteration 46, loss = 1.23147924\n",
      "Iteration 47, loss = 1.21046761\n",
      "Iteration 48, loss = 1.18963893\n",
      "Iteration 49, loss = 1.16906781\n",
      "Iteration 50, loss = 1.14923486\n",
      "Iteration 51, loss = 1.12970407\n",
      "Iteration 52, loss = 1.11052406\n",
      "Iteration 53, loss = 1.09172945\n",
      "Iteration 54, loss = 1.07354109\n",
      "Iteration 55, loss = 1.05555189\n",
      "Iteration 56, loss = 1.03799846\n",
      "Iteration 57, loss = 1.02124182\n",
      "Iteration 58, loss = 1.00413358\n",
      "Iteration 59, loss = 0.98761848\n",
      "Iteration 60, loss = 0.97156264\n",
      "Iteration 61, loss = 0.95565159\n",
      "Iteration 62, loss = 0.94055862\n",
      "Iteration 63, loss = 0.92544812\n",
      "Iteration 64, loss = 0.91079398\n",
      "Iteration 65, loss = 0.89649607\n",
      "Iteration 66, loss = 0.88278118\n",
      "Iteration 67, loss = 0.86896799\n",
      "Iteration 68, loss = 0.85560549\n",
      "Iteration 69, loss = 0.84270686\n",
      "Iteration 70, loss = 0.83012376\n",
      "Iteration 71, loss = 0.81804064\n",
      "Iteration 72, loss = 0.80597326\n",
      "Iteration 73, loss = 0.79439406\n",
      "Iteration 74, loss = 0.78320899\n",
      "Iteration 75, loss = 0.77199044\n",
      "Iteration 76, loss = 0.76116590\n",
      "Iteration 77, loss = 0.75059378\n",
      "Iteration 78, loss = 0.74045473\n",
      "Iteration 79, loss = 0.73049105\n",
      "Iteration 80, loss = 0.72094873\n",
      "Iteration 81, loss = 0.71132048\n",
      "Iteration 82, loss = 0.70191342\n",
      "Iteration 83, loss = 0.69289613\n",
      "Iteration 84, loss = 0.68388629\n",
      "Iteration 85, loss = 0.67515973\n",
      "Iteration 86, loss = 0.66662432\n",
      "Iteration 87, loss = 0.65845078\n",
      "Iteration 88, loss = 0.65021653\n",
      "Iteration 89, loss = 0.64244258\n",
      "Iteration 90, loss = 0.63491602\n",
      "Iteration 91, loss = 0.62718072\n",
      "Iteration 92, loss = 0.61983035\n",
      "Iteration 93, loss = 0.61272597\n",
      "Iteration 94, loss = 0.60557972\n",
      "Iteration 95, loss = 0.59859522\n",
      "Iteration 96, loss = 0.59175878\n",
      "Iteration 97, loss = 0.58515239\n",
      "Iteration 98, loss = 0.57861884\n",
      "Iteration 99, loss = 0.57233499\n",
      "Iteration 100, loss = 0.56621965\n",
      "Iteration 101, loss = 0.56015738\n",
      "Iteration 102, loss = 0.55428263\n",
      "Iteration 103, loss = 0.54839143\n",
      "Iteration 104, loss = 0.54253962\n",
      "Iteration 105, loss = 0.53691791\n",
      "Iteration 106, loss = 0.53143248\n",
      "Iteration 107, loss = 0.52603779\n",
      "Iteration 108, loss = 0.52080143\n",
      "Iteration 109, loss = 0.51582837\n",
      "Iteration 110, loss = 0.51072005\n",
      "Iteration 111, loss = 0.50563579\n",
      "Iteration 112, loss = 0.50092925\n",
      "Iteration 113, loss = 0.49605574\n",
      "Iteration 114, loss = 0.49138346\n",
      "Iteration 115, loss = 0.48681503\n",
      "Iteration 116, loss = 0.48227429\n",
      "Iteration 117, loss = 0.47774200\n",
      "Iteration 118, loss = 0.47348043\n",
      "Iteration 119, loss = 0.46908133\n",
      "Iteration 120, loss = 0.46492639\n",
      "Iteration 121, loss = 0.46072121\n",
      "Iteration 122, loss = 0.45664940\n",
      "Iteration 123, loss = 0.45265491\n",
      "Iteration 124, loss = 0.44876680\n",
      "Iteration 125, loss = 0.44499863\n",
      "Iteration 126, loss = 0.44118316\n",
      "Iteration 127, loss = 0.43749194\n",
      "Iteration 128, loss = 0.43373038\n",
      "Iteration 129, loss = 0.43019650\n",
      "Iteration 130, loss = 0.42661423\n",
      "Iteration 131, loss = 0.42321016\n",
      "Iteration 132, loss = 0.41976593\n",
      "Iteration 133, loss = 0.41637500\n",
      "Iteration 134, loss = 0.41310785\n",
      "Iteration 135, loss = 0.40980314\n",
      "Iteration 136, loss = 0.40666207\n",
      "Iteration 137, loss = 0.40344790\n",
      "Iteration 138, loss = 0.40038461\n",
      "Iteration 139, loss = 0.39737906\n",
      "Iteration 140, loss = 0.39428319\n",
      "Iteration 141, loss = 0.39131201\n",
      "Iteration 142, loss = 0.38843137\n",
      "Iteration 143, loss = 0.38546324\n",
      "Iteration 144, loss = 0.38268638\n",
      "Iteration 145, loss = 0.37986801\n",
      "Iteration 146, loss = 0.37713905\n",
      "Iteration 147, loss = 0.37442791\n",
      "Iteration 148, loss = 0.37190260\n",
      "Iteration 149, loss = 0.36927798\n",
      "Iteration 150, loss = 0.36666937\n",
      "Iteration 151, loss = 0.36417709\n",
      "Iteration 152, loss = 0.36175305\n",
      "Iteration 153, loss = 0.35922962\n",
      "Iteration 154, loss = 0.35669752\n",
      "Iteration 155, loss = 0.35418160\n",
      "Iteration 156, loss = 0.35172415\n",
      "Iteration 157, loss = 0.34932386\n",
      "Iteration 158, loss = 0.34700555\n",
      "Iteration 159, loss = 0.34474509\n",
      "Iteration 160, loss = 0.34251715\n",
      "Iteration 161, loss = 0.34024458\n",
      "Iteration 162, loss = 0.33808955\n",
      "Iteration 163, loss = 0.33590563\n",
      "Iteration 164, loss = 0.33379493\n",
      "Iteration 165, loss = 0.33171589\n",
      "Iteration 166, loss = 0.32958749\n",
      "Iteration 167, loss = 0.32762548\n",
      "Iteration 168, loss = 0.32555043\n",
      "Iteration 169, loss = 0.32362443\n",
      "Iteration 170, loss = 0.32155128\n",
      "Iteration 171, loss = 0.31969190\n",
      "Iteration 172, loss = 0.31768742\n",
      "Iteration 173, loss = 0.31580499\n",
      "Iteration 174, loss = 0.31381234\n",
      "Iteration 175, loss = 0.31195786\n",
      "Iteration 176, loss = 0.31011040\n",
      "Iteration 177, loss = 0.30828330\n",
      "Iteration 178, loss = 0.30648593\n",
      "Iteration 179, loss = 0.30465738\n",
      "Iteration 180, loss = 0.30287188\n",
      "Iteration 181, loss = 0.30117031\n",
      "Iteration 182, loss = 0.29953673\n",
      "Iteration 183, loss = 0.29783657\n",
      "Iteration 184, loss = 0.29622164\n",
      "Iteration 185, loss = 0.29460913\n",
      "Iteration 186, loss = 0.29291445\n",
      "Iteration 187, loss = 0.29120640\n",
      "Iteration 188, loss = 0.28960108\n",
      "Iteration 189, loss = 0.28802061\n",
      "Iteration 190, loss = 0.28649745\n",
      "Iteration 191, loss = 0.28494901\n",
      "Iteration 192, loss = 0.28339435\n",
      "Iteration 193, loss = 0.28186634\n",
      "Iteration 194, loss = 0.28041441\n",
      "Iteration 195, loss = 0.27892411\n",
      "Iteration 196, loss = 0.27749056\n",
      "Iteration 197, loss = 0.27606489\n",
      "Iteration 198, loss = 0.27478095\n",
      "Iteration 199, loss = 0.27317607\n",
      "Iteration 200, loss = 0.27175685\n",
      "Iteration 201, loss = 0.27038060\n",
      "Iteration 202, loss = 0.26902747\n",
      "Iteration 203, loss = 0.26772183\n",
      "Iteration 204, loss = 0.26634986\n",
      "Iteration 205, loss = 0.26501115\n",
      "Iteration 206, loss = 0.26383435\n",
      "Iteration 207, loss = 0.26247502\n",
      "Iteration 208, loss = 0.26111363\n",
      "Iteration 209, loss = 0.25987772\n",
      "Iteration 210, loss = 0.25860367\n",
      "Iteration 211, loss = 0.25733915\n",
      "Iteration 212, loss = 0.25603369\n",
      "Iteration 213, loss = 0.25487601\n",
      "Iteration 214, loss = 0.25366669\n",
      "Iteration 215, loss = 0.25239770\n",
      "Iteration 216, loss = 0.25122562\n",
      "Iteration 217, loss = 0.24996886\n",
      "Iteration 218, loss = 0.24874236\n",
      "Iteration 219, loss = 0.24756502\n",
      "Iteration 220, loss = 0.24642773\n",
      "Iteration 221, loss = 0.24528476\n",
      "Iteration 222, loss = 0.24417725\n",
      "Iteration 223, loss = 0.24304236\n",
      "Iteration 224, loss = 0.24191073\n",
      "Iteration 225, loss = 0.24079957\n",
      "Iteration 226, loss = 0.23968932\n",
      "Iteration 227, loss = 0.23863523\n",
      "Iteration 228, loss = 0.23757188\n",
      "Iteration 229, loss = 0.23652391\n",
      "Iteration 230, loss = 0.23548062\n",
      "Iteration 231, loss = 0.23438681\n",
      "Iteration 232, loss = 0.23334379\n",
      "Iteration 233, loss = 0.23232790\n",
      "Iteration 234, loss = 0.23131138\n",
      "Iteration 235, loss = 0.23029930\n",
      "Iteration 236, loss = 0.22933061\n",
      "Iteration 237, loss = 0.22835943\n",
      "Iteration 238, loss = 0.22743648\n",
      "Iteration 239, loss = 0.22648392\n",
      "Iteration 240, loss = 0.22554710\n",
      "Iteration 241, loss = 0.22456314\n",
      "Iteration 242, loss = 0.22359926\n",
      "Iteration 243, loss = 0.22267580\n",
      "Iteration 244, loss = 0.22166836\n",
      "Iteration 245, loss = 0.22074320\n",
      "Iteration 246, loss = 0.21981443\n",
      "Iteration 247, loss = 0.21890512\n",
      "Iteration 248, loss = 0.21800777\n",
      "Iteration 249, loss = 0.21707616\n",
      "Iteration 250, loss = 0.21624817\n",
      "Iteration 251, loss = 0.21538179\n",
      "Iteration 252, loss = 0.21453490\n",
      "Iteration 253, loss = 0.21367460\n",
      "Iteration 254, loss = 0.21283087\n",
      "Iteration 255, loss = 0.21202599\n",
      "Iteration 256, loss = 0.21115523\n",
      "Iteration 257, loss = 0.21031310\n",
      "Iteration 258, loss = 0.20946494\n",
      "Iteration 259, loss = 0.20865372\n",
      "Iteration 260, loss = 0.20785149\n",
      "Iteration 261, loss = 0.20700822\n",
      "Iteration 262, loss = 0.20622731\n",
      "Iteration 263, loss = 0.20544381\n",
      "Iteration 264, loss = 0.20465718\n",
      "Iteration 265, loss = 0.20390986\n",
      "Iteration 266, loss = 0.20311239\n",
      "Iteration 267, loss = 0.20232783\n",
      "Iteration 268, loss = 0.20159722\n",
      "Iteration 269, loss = 0.20084529\n",
      "Iteration 270, loss = 0.20008796\n",
      "Iteration 271, loss = 0.19932284\n",
      "Iteration 272, loss = 0.19857587\n",
      "Iteration 273, loss = 0.19784498\n",
      "Iteration 274, loss = 0.19714716\n",
      "Iteration 275, loss = 0.19639309\n",
      "Iteration 276, loss = 0.19568360\n",
      "Iteration 277, loss = 0.19500753\n",
      "Iteration 278, loss = 0.19429190\n",
      "Iteration 279, loss = 0.19360375\n",
      "Iteration 280, loss = 0.19292710\n",
      "Iteration 281, loss = 0.19224186\n",
      "Iteration 282, loss = 0.19153028\n",
      "Iteration 283, loss = 0.19084101\n",
      "Iteration 284, loss = 0.19021218\n",
      "Iteration 285, loss = 0.18954661\n",
      "Iteration 286, loss = 0.18891421\n",
      "Iteration 287, loss = 0.18830367\n",
      "Iteration 288, loss = 0.18758389\n",
      "Iteration 289, loss = 0.18698061\n",
      "Iteration 290, loss = 0.18629815\n",
      "Iteration 291, loss = 0.18565154\n",
      "Iteration 292, loss = 0.18502185\n",
      "Iteration 293, loss = 0.18442098\n",
      "Iteration 294, loss = 0.18379980\n",
      "Iteration 295, loss = 0.18319778\n",
      "Iteration 296, loss = 0.18257791\n",
      "Iteration 297, loss = 0.18196538\n",
      "Iteration 298, loss = 0.18133134\n",
      "Iteration 299, loss = 0.18074063\n",
      "Iteration 300, loss = 0.18016305\n",
      "Iteration 301, loss = 0.17957427\n",
      "Iteration 302, loss = 0.17898152\n",
      "Iteration 303, loss = 0.17838463\n",
      "Iteration 304, loss = 0.17783206\n",
      "Iteration 305, loss = 0.17722681\n",
      "Iteration 306, loss = 0.17665571\n",
      "Iteration 307, loss = 0.17610090\n",
      "Iteration 308, loss = 0.17555441\n",
      "Iteration 309, loss = 0.17497564\n",
      "Iteration 310, loss = 0.17443471\n",
      "Iteration 311, loss = 0.17384411\n",
      "Iteration 312, loss = 0.17331140\n",
      "Iteration 313, loss = 0.17278579\n",
      "Iteration 314, loss = 0.17223315\n",
      "Iteration 315, loss = 0.17168182\n",
      "Iteration 316, loss = 0.17115263\n",
      "Iteration 317, loss = 0.17058235\n",
      "Iteration 318, loss = 0.17010917\n",
      "Iteration 319, loss = 0.16954045\n",
      "Iteration 320, loss = 0.16900487\n",
      "Iteration 321, loss = 0.16850041\n",
      "Iteration 322, loss = 0.16796472\n",
      "Iteration 323, loss = 0.16746028\n",
      "Iteration 324, loss = 0.16699453\n",
      "Iteration 325, loss = 0.16647998\n",
      "Iteration 326, loss = 0.16599615\n",
      "Iteration 327, loss = 0.16548544\n",
      "Iteration 328, loss = 0.16496656\n",
      "Iteration 329, loss = 0.16447959\n",
      "Iteration 330, loss = 0.16401387\n",
      "Iteration 331, loss = 0.16350561\n",
      "Iteration 332, loss = 0.16302329\n",
      "Iteration 333, loss = 0.16255352\n",
      "Iteration 334, loss = 0.16208242\n",
      "Iteration 335, loss = 0.16163750\n",
      "Iteration 336, loss = 0.16120695\n",
      "Iteration 337, loss = 0.16069083\n",
      "Iteration 338, loss = 0.16023828\n",
      "Iteration 339, loss = 0.15980977\n",
      "Iteration 340, loss = 0.15937607\n",
      "Iteration 341, loss = 0.15891395\n",
      "Iteration 342, loss = 0.15843359\n",
      "Iteration 343, loss = 0.15799169\n",
      "Iteration 344, loss = 0.15755091\n",
      "Iteration 345, loss = 0.15710345\n",
      "Iteration 346, loss = 0.15673216\n",
      "Iteration 347, loss = 0.15624136\n",
      "Iteration 348, loss = 0.15582464\n",
      "Iteration 349, loss = 0.15540056\n",
      "Iteration 350, loss = 0.15496844\n",
      "Iteration 351, loss = 0.15451594\n",
      "Iteration 352, loss = 0.15410565\n",
      "Iteration 353, loss = 0.15368650\n",
      "Iteration 354, loss = 0.15330386\n",
      "Iteration 355, loss = 0.15287809\n",
      "Iteration 356, loss = 0.15247485\n",
      "Iteration 357, loss = 0.15211366\n",
      "Iteration 358, loss = 0.15165748\n",
      "Iteration 359, loss = 0.15126711\n",
      "Iteration 360, loss = 0.15086692\n",
      "Iteration 361, loss = 0.15048530\n",
      "Iteration 362, loss = 0.15006097\n",
      "Iteration 363, loss = 0.14963202\n",
      "Iteration 364, loss = 0.14922850\n",
      "Iteration 365, loss = 0.14885689\n",
      "Iteration 366, loss = 0.14845484\n",
      "Iteration 367, loss = 0.14805754\n",
      "Iteration 368, loss = 0.14767441\n",
      "Iteration 369, loss = 0.14730237\n",
      "Iteration 370, loss = 0.14692482\n",
      "Iteration 371, loss = 0.14653199\n",
      "Iteration 372, loss = 0.14619784\n",
      "Iteration 373, loss = 0.14581432\n",
      "Iteration 374, loss = 0.14541008\n",
      "Iteration 375, loss = 0.14503928\n",
      "Iteration 376, loss = 0.14466927\n",
      "Iteration 377, loss = 0.14431813\n",
      "Iteration 378, loss = 0.14394391\n",
      "Iteration 379, loss = 0.14354192\n",
      "Iteration 380, loss = 0.14324019\n",
      "Iteration 381, loss = 0.14285933\n",
      "Iteration 382, loss = 0.14251126\n",
      "Iteration 383, loss = 0.14214865\n",
      "Iteration 384, loss = 0.14178580\n",
      "Iteration 385, loss = 0.14140470\n",
      "Iteration 386, loss = 0.14107840\n",
      "Iteration 387, loss = 0.14070874\n",
      "Iteration 388, loss = 0.14039115\n",
      "Iteration 389, loss = 0.14003207\n",
      "Iteration 390, loss = 0.13967187\n",
      "Iteration 391, loss = 0.13934766\n",
      "Iteration 392, loss = 0.13898817\n",
      "Iteration 393, loss = 0.13866577\n",
      "Iteration 394, loss = 0.13832778\n",
      "Iteration 395, loss = 0.13800445\n",
      "Iteration 396, loss = 0.13766718\n",
      "Iteration 397, loss = 0.13731540\n",
      "Iteration 398, loss = 0.13699847\n",
      "Iteration 399, loss = 0.13667728\n",
      "Iteration 400, loss = 0.13636470\n",
      "Iteration 401, loss = 0.13600786\n",
      "Iteration 402, loss = 0.13570552\n",
      "Iteration 403, loss = 0.13539379\n",
      "Iteration 404, loss = 0.13508124\n",
      "Iteration 405, loss = 0.13476000\n",
      "Iteration 406, loss = 0.13444537\n",
      "Iteration 407, loss = 0.13416155\n",
      "Iteration 408, loss = 0.13383316\n",
      "Iteration 409, loss = 0.13350585\n",
      "Iteration 410, loss = 0.13320849\n",
      "Iteration 411, loss = 0.13289497\n",
      "Iteration 412, loss = 0.13260836\n",
      "Iteration 413, loss = 0.13230747\n",
      "Iteration 414, loss = 0.13200757\n",
      "Iteration 415, loss = 0.13171930\n",
      "Iteration 416, loss = 0.13140195\n",
      "Iteration 417, loss = 0.13109652\n",
      "Iteration 418, loss = 0.13080364\n",
      "Iteration 419, loss = 0.13050949\n",
      "Iteration 420, loss = 0.13025015\n",
      "Iteration 421, loss = 0.12995746\n",
      "Iteration 422, loss = 0.12968078\n",
      "Iteration 423, loss = 0.12939860\n",
      "Iteration 424, loss = 0.12910506\n",
      "Iteration 425, loss = 0.12880287\n",
      "Iteration 426, loss = 0.12851156\n",
      "Iteration 427, loss = 0.12824350\n",
      "Iteration 428, loss = 0.12799160\n",
      "Iteration 429, loss = 0.12767548\n",
      "Iteration 430, loss = 0.12739601\n",
      "Iteration 431, loss = 0.12709581\n",
      "Iteration 432, loss = 0.12682354\n",
      "Iteration 433, loss = 0.12653205\n",
      "Iteration 434, loss = 0.12628332\n",
      "Iteration 435, loss = 0.12601516\n",
      "Iteration 436, loss = 0.12574933\n",
      "Iteration 437, loss = 0.12550881\n",
      "Iteration 438, loss = 0.12525207\n",
      "Iteration 439, loss = 0.12497887\n",
      "Iteration 440, loss = 0.12473759\n",
      "Iteration 441, loss = 0.12449220\n",
      "Iteration 442, loss = 0.12423618\n",
      "Iteration 443, loss = 0.12399052\n",
      "Iteration 444, loss = 0.12371071\n",
      "Iteration 445, loss = 0.12345479\n",
      "Iteration 446, loss = 0.12318247\n",
      "Iteration 447, loss = 0.12297466\n",
      "Iteration 448, loss = 0.12269690\n",
      "Iteration 449, loss = 0.12242184\n",
      "Iteration 450, loss = 0.12219414\n",
      "Iteration 451, loss = 0.12191211\n",
      "Iteration 452, loss = 0.12167526\n",
      "Iteration 453, loss = 0.12141744\n",
      "Iteration 454, loss = 0.12118169\n",
      "Iteration 455, loss = 0.12097678\n",
      "Iteration 456, loss = 0.12071054\n",
      "Iteration 457, loss = 0.12047831\n",
      "Iteration 458, loss = 0.12021494\n",
      "Iteration 459, loss = 0.11999529\n",
      "Iteration 460, loss = 0.11974770\n",
      "Iteration 461, loss = 0.11952615\n",
      "Iteration 462, loss = 0.11929645\n",
      "Iteration 463, loss = 0.11909069\n",
      "Iteration 464, loss = 0.11881471\n",
      "Iteration 465, loss = 0.11858167\n",
      "Iteration 466, loss = 0.11834266\n",
      "Iteration 467, loss = 0.11809981\n",
      "Iteration 468, loss = 0.11789789\n",
      "Iteration 469, loss = 0.11765798\n",
      "Iteration 470, loss = 0.11745386\n",
      "Iteration 471, loss = 0.11720746\n",
      "Iteration 472, loss = 0.11699035\n",
      "Iteration 473, loss = 0.11675216\n",
      "Iteration 474, loss = 0.11653326\n",
      "Iteration 475, loss = 0.11632522\n",
      "Iteration 476, loss = 0.11608873\n",
      "Iteration 477, loss = 0.11588237\n",
      "Iteration 478, loss = 0.11567173\n",
      "Iteration 479, loss = 0.11544871\n",
      "Iteration 480, loss = 0.11526728\n",
      "Iteration 481, loss = 0.11504951\n",
      "Iteration 482, loss = 0.11482219\n",
      "Iteration 483, loss = 0.11460379\n",
      "Iteration 484, loss = 0.11437659\n",
      "Iteration 485, loss = 0.11416053\n",
      "Iteration 486, loss = 0.11397063\n",
      "Iteration 487, loss = 0.11374273\n",
      "Iteration 488, loss = 0.11356735\n",
      "Iteration 489, loss = 0.11332163\n",
      "Iteration 490, loss = 0.11313558\n",
      "Iteration 491, loss = 0.11292368\n",
      "Iteration 492, loss = 0.11274110\n",
      "Iteration 493, loss = 0.11251667\n",
      "Iteration 494, loss = 0.11231076\n",
      "Iteration 495, loss = 0.11211679\n",
      "Iteration 496, loss = 0.11191186\n",
      "Iteration 497, loss = 0.11171319\n",
      "Iteration 498, loss = 0.11152837\n",
      "Iteration 499, loss = 0.11135594\n",
      "Iteration 500, loss = 0.11114247\n",
      "Iteration 501, loss = 0.11094443\n",
      "Iteration 502, loss = 0.11076739\n",
      "Iteration 503, loss = 0.11055973\n",
      "Iteration 504, loss = 0.11035275\n",
      "Iteration 505, loss = 0.11015861\n",
      "Iteration 506, loss = 0.10997225\n",
      "Iteration 507, loss = 0.10977322\n",
      "Iteration 508, loss = 0.10959919\n",
      "Iteration 509, loss = 0.10941442\n",
      "Iteration 510, loss = 0.10922434\n",
      "Iteration 511, loss = 0.10904991\n",
      "Iteration 512, loss = 0.10888068\n",
      "Iteration 513, loss = 0.10868382\n",
      "Iteration 514, loss = 0.10851188\n",
      "Iteration 515, loss = 0.10831486\n",
      "Iteration 516, loss = 0.10815947\n",
      "Iteration 517, loss = 0.10796453\n",
      "Iteration 518, loss = 0.10778770\n",
      "Iteration 519, loss = 0.10761533\n",
      "Iteration 520, loss = 0.10743685\n",
      "Iteration 521, loss = 0.10728378\n",
      "Iteration 522, loss = 0.10711351\n",
      "Iteration 523, loss = 0.10692420\n",
      "Iteration 524, loss = 0.10673066\n",
      "Iteration 525, loss = 0.10656377\n",
      "Iteration 526, loss = 0.10636769\n",
      "Iteration 527, loss = 0.10621207\n",
      "Iteration 528, loss = 0.10603992\n",
      "Iteration 529, loss = 0.10587811\n",
      "Iteration 530, loss = 0.10569915\n",
      "Iteration 531, loss = 0.10552111\n",
      "Iteration 532, loss = 0.10533682\n",
      "Iteration 533, loss = 0.10517545\n",
      "Iteration 534, loss = 0.10503365\n",
      "Iteration 535, loss = 0.10486528\n",
      "Iteration 536, loss = 0.10469013\n",
      "Iteration 537, loss = 0.10452696\n",
      "Iteration 538, loss = 0.10433677\n",
      "Iteration 539, loss = 0.10418806\n",
      "Iteration 540, loss = 0.10402993\n",
      "Iteration 541, loss = 0.10389508\n",
      "Iteration 542, loss = 0.10370472\n",
      "Iteration 543, loss = 0.10354168\n",
      "Iteration 544, loss = 0.10338590\n",
      "Iteration 545, loss = 0.10323098\n",
      "Iteration 546, loss = 0.10307655\n",
      "Iteration 547, loss = 0.10290173\n",
      "Iteration 548, loss = 0.10275675\n",
      "Iteration 549, loss = 0.10259848\n",
      "Iteration 550, loss = 0.10244293\n",
      "Iteration 551, loss = 0.10227875\n",
      "Iteration 552, loss = 0.10213392\n",
      "Iteration 553, loss = 0.10198639\n",
      "Iteration 554, loss = 0.10183956\n",
      "Iteration 555, loss = 0.10169015\n",
      "Iteration 556, loss = 0.10153408\n",
      "Iteration 557, loss = 0.10137439\n",
      "Iteration 558, loss = 0.10121698\n",
      "Iteration 559, loss = 0.10108565\n",
      "Iteration 560, loss = 0.10094586\n",
      "Iteration 561, loss = 0.10078524\n",
      "Iteration 562, loss = 0.10064502\n",
      "Iteration 563, loss = 0.10050062\n",
      "Iteration 564, loss = 0.10035388\n",
      "Iteration 565, loss = 0.10021121\n",
      "Iteration 566, loss = 0.10005671\n",
      "Iteration 567, loss = 0.09991261\n",
      "Iteration 568, loss = 0.09976838\n",
      "Iteration 569, loss = 0.09963258\n",
      "Iteration 570, loss = 0.09949227\n",
      "Iteration 571, loss = 0.09935411\n",
      "Iteration 572, loss = 0.09923064\n",
      "Iteration 573, loss = 0.09907133\n",
      "Iteration 574, loss = 0.09894260\n",
      "Iteration 575, loss = 0.09881076\n",
      "Iteration 576, loss = 0.09867393\n",
      "Iteration 577, loss = 0.09853290\n",
      "Iteration 578, loss = 0.09841201\n",
      "Iteration 579, loss = 0.09827878\n",
      "Iteration 580, loss = 0.09814424\n",
      "Iteration 581, loss = 0.09802181\n",
      "Iteration 582, loss = 0.09785929\n",
      "Iteration 583, loss = 0.09771246\n",
      "Iteration 584, loss = 0.09758568\n",
      "Iteration 585, loss = 0.09743856\n",
      "Iteration 586, loss = 0.09731736\n",
      "Iteration 587, loss = 0.09718204\n",
      "Iteration 588, loss = 0.09705789\n",
      "Iteration 589, loss = 0.09692526\n",
      "Iteration 590, loss = 0.09680872\n",
      "Iteration 591, loss = 0.09665506\n",
      "Iteration 592, loss = 0.09654038\n",
      "Iteration 593, loss = 0.09642058\n",
      "Iteration 594, loss = 0.09629592\n",
      "Iteration 595, loss = 0.09615810\n",
      "Iteration 596, loss = 0.09603554\n",
      "Iteration 597, loss = 0.09589593\n",
      "Iteration 598, loss = 0.09578406\n",
      "Iteration 599, loss = 0.09566048\n",
      "Iteration 600, loss = 0.09554433\n",
      "Iteration 601, loss = 0.09542311\n",
      "Iteration 602, loss = 0.09529434\n",
      "Iteration 603, loss = 0.09516260\n",
      "Iteration 604, loss = 0.09504628\n",
      "Iteration 605, loss = 0.09490866\n",
      "Iteration 606, loss = 0.09481027\n",
      "Iteration 607, loss = 0.09468439\n",
      "Iteration 608, loss = 0.09457170\n",
      "Iteration 609, loss = 0.09444816\n",
      "Iteration 610, loss = 0.09432755\n",
      "Iteration 611, loss = 0.09420878\n",
      "Iteration 612, loss = 0.09409019\n",
      "Iteration 613, loss = 0.09398722\n",
      "Iteration 614, loss = 0.09385766\n",
      "Iteration 615, loss = 0.09374938\n",
      "Iteration 616, loss = 0.09363301\n",
      "Iteration 617, loss = 0.09352580\n",
      "Iteration 618, loss = 0.09340262\n",
      "Iteration 619, loss = 0.09330357\n",
      "Iteration 620, loss = 0.09317660\n",
      "Iteration 621, loss = 0.09305363\n",
      "Iteration 622, loss = 0.09294370\n",
      "Iteration 623, loss = 0.09284595\n",
      "Iteration 624, loss = 0.09272069\n",
      "Iteration 625, loss = 0.09261336\n",
      "Iteration 626, loss = 0.09248588\n",
      "Iteration 627, loss = 0.09237485\n",
      "Iteration 628, loss = 0.09226046\n",
      "Iteration 629, loss = 0.09214101\n",
      "Iteration 630, loss = 0.09203396\n",
      "Iteration 631, loss = 0.09191352\n",
      "Iteration 632, loss = 0.09181082\n",
      "Iteration 633, loss = 0.09170583\n",
      "Iteration 634, loss = 0.09158992\n",
      "Iteration 635, loss = 0.09148231\n",
      "Iteration 636, loss = 0.09137600\n",
      "Iteration 637, loss = 0.09126880\n",
      "Iteration 638, loss = 0.09114214\n",
      "Iteration 639, loss = 0.09102576\n",
      "Iteration 640, loss = 0.09092317\n",
      "Iteration 641, loss = 0.09081377\n",
      "Iteration 642, loss = 0.09071007\n",
      "Iteration 643, loss = 0.09060209\n",
      "Iteration 644, loss = 0.09049715\n",
      "Iteration 645, loss = 0.09038605\n",
      "Iteration 646, loss = 0.09029352\n",
      "Iteration 647, loss = 0.09018749\n",
      "Iteration 648, loss = 0.09008535\n",
      "Iteration 649, loss = 0.08998578\n",
      "Iteration 650, loss = 0.08988711\n",
      "Iteration 651, loss = 0.08979757\n",
      "Iteration 652, loss = 0.08969548\n",
      "Iteration 653, loss = 0.08960612\n",
      "Iteration 654, loss = 0.08949411\n",
      "Iteration 655, loss = 0.08940150\n",
      "Iteration 656, loss = 0.08929824\n",
      "Iteration 657, loss = 0.08919836\n",
      "Iteration 658, loss = 0.08909827\n",
      "Iteration 659, loss = 0.08899784\n",
      "Iteration 660, loss = 0.08890816\n",
      "Iteration 661, loss = 0.08880727\n",
      "Iteration 662, loss = 0.08871932\n",
      "Iteration 663, loss = 0.08862039\n",
      "Iteration 664, loss = 0.08852280\n",
      "Iteration 665, loss = 0.08843617\n",
      "Iteration 666, loss = 0.08833759\n",
      "Iteration 667, loss = 0.08824317\n",
      "Iteration 668, loss = 0.08812802\n",
      "Iteration 669, loss = 0.08805837\n",
      "Iteration 670, loss = 0.08795369\n",
      "Iteration 671, loss = 0.08787115\n",
      "Iteration 672, loss = 0.08775874\n",
      "Iteration 673, loss = 0.08767156\n",
      "Iteration 674, loss = 0.08757456\n",
      "Iteration 675, loss = 0.08747408\n",
      "Iteration 676, loss = 0.08739138\n",
      "Iteration 677, loss = 0.08729477\n",
      "Iteration 678, loss = 0.08719808\n",
      "Iteration 679, loss = 0.08712082\n",
      "Iteration 680, loss = 0.08701915\n",
      "Iteration 681, loss = 0.08693560\n",
      "Iteration 682, loss = 0.08683086\n",
      "Iteration 683, loss = 0.08674435\n",
      "Iteration 684, loss = 0.08664998\n",
      "Iteration 685, loss = 0.08655982\n",
      "Iteration 686, loss = 0.08647788\n",
      "Iteration 687, loss = 0.08638721\n",
      "Iteration 688, loss = 0.08630156\n",
      "Iteration 689, loss = 0.08621127\n",
      "Iteration 690, loss = 0.08611429\n",
      "Iteration 691, loss = 0.08603314\n",
      "Iteration 692, loss = 0.08594457\n",
      "Iteration 693, loss = 0.08586594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.36490142\n",
      "Iteration 2, loss = 2.34261153\n",
      "Iteration 3, loss = 2.31470552\n",
      "Iteration 4, loss = 2.28659369\n",
      "Iteration 5, loss = 2.26149734\n",
      "Iteration 6, loss = 2.23701220\n",
      "Iteration 7, loss = 2.21408154\n",
      "Iteration 8, loss = 2.19094821\n",
      "Iteration 9, loss = 2.16769138\n",
      "Iteration 10, loss = 2.14461440\n",
      "Iteration 11, loss = 2.12112000\n",
      "Iteration 12, loss = 2.09762455\n",
      "Iteration 13, loss = 2.07456606\n",
      "Iteration 14, loss = 2.05134217\n",
      "Iteration 15, loss = 2.02765755\n",
      "Iteration 16, loss = 2.00381366\n",
      "Iteration 17, loss = 1.97955394\n",
      "Iteration 18, loss = 1.95512566\n",
      "Iteration 19, loss = 1.93017754\n",
      "Iteration 20, loss = 1.90508387\n",
      "Iteration 21, loss = 1.87948778\n",
      "Iteration 22, loss = 1.85385982\n",
      "Iteration 23, loss = 1.82806243\n",
      "Iteration 24, loss = 1.80218060\n",
      "Iteration 25, loss = 1.77615299\n",
      "Iteration 26, loss = 1.74995482\n",
      "Iteration 27, loss = 1.72377326\n",
      "Iteration 28, loss = 1.69754247\n",
      "Iteration 29, loss = 1.67096223\n",
      "Iteration 30, loss = 1.64463066\n",
      "Iteration 31, loss = 1.61835551\n",
      "Iteration 32, loss = 1.59196790\n",
      "Iteration 33, loss = 1.56587272\n",
      "Iteration 34, loss = 1.53979908\n",
      "Iteration 35, loss = 1.51451330\n",
      "Iteration 36, loss = 1.48922208\n",
      "Iteration 37, loss = 1.46449941\n",
      "Iteration 38, loss = 1.43986258\n",
      "Iteration 39, loss = 1.41570331\n",
      "Iteration 40, loss = 1.39188907\n",
      "Iteration 41, loss = 1.36862409\n",
      "Iteration 42, loss = 1.34570047\n",
      "Iteration 43, loss = 1.32320463\n",
      "Iteration 44, loss = 1.30099702\n",
      "Iteration 45, loss = 1.27893036\n",
      "Iteration 46, loss = 1.25770228\n",
      "Iteration 47, loss = 1.23657914\n",
      "Iteration 48, loss = 1.21582804\n",
      "Iteration 49, loss = 1.19525671\n",
      "Iteration 50, loss = 1.17533483\n",
      "Iteration 51, loss = 1.15592073\n",
      "Iteration 52, loss = 1.13672924\n",
      "Iteration 53, loss = 1.11778310\n",
      "Iteration 54, loss = 1.09955695\n",
      "Iteration 55, loss = 1.08177791\n",
      "Iteration 56, loss = 1.06409012\n",
      "Iteration 57, loss = 1.04724942\n",
      "Iteration 58, loss = 1.03063216\n",
      "Iteration 59, loss = 1.01436143\n",
      "Iteration 60, loss = 0.99832630\n",
      "Iteration 61, loss = 0.98263992\n",
      "Iteration 62, loss = 0.96747790\n",
      "Iteration 63, loss = 0.95257211\n",
      "Iteration 64, loss = 0.93797840\n",
      "Iteration 65, loss = 0.92386517\n",
      "Iteration 66, loss = 0.91011284\n",
      "Iteration 67, loss = 0.89648346\n",
      "Iteration 68, loss = 0.88321197\n",
      "Iteration 69, loss = 0.87029628\n",
      "Iteration 70, loss = 0.85771172\n",
      "Iteration 71, loss = 0.84555058\n",
      "Iteration 72, loss = 0.83334777\n",
      "Iteration 73, loss = 0.82158380\n",
      "Iteration 74, loss = 0.81001986\n",
      "Iteration 75, loss = 0.79883738\n",
      "Iteration 76, loss = 0.78774833\n",
      "Iteration 77, loss = 0.77698555\n",
      "Iteration 78, loss = 0.76658880\n",
      "Iteration 79, loss = 0.75636335\n",
      "Iteration 80, loss = 0.74665035\n",
      "Iteration 81, loss = 0.73689524\n",
      "Iteration 82, loss = 0.72717929\n",
      "Iteration 83, loss = 0.71793766\n",
      "Iteration 84, loss = 0.70871355\n",
      "Iteration 85, loss = 0.69966040\n",
      "Iteration 86, loss = 0.69096566\n",
      "Iteration 87, loss = 0.68239803\n",
      "Iteration 88, loss = 0.67405904\n",
      "Iteration 89, loss = 0.66595578\n",
      "Iteration 90, loss = 0.65794081\n",
      "Iteration 91, loss = 0.65007440\n",
      "Iteration 92, loss = 0.64238093\n",
      "Iteration 93, loss = 0.63500499\n",
      "Iteration 94, loss = 0.62747776\n",
      "Iteration 95, loss = 0.62022819\n",
      "Iteration 96, loss = 0.61318166\n",
      "Iteration 97, loss = 0.60626381\n",
      "Iteration 98, loss = 0.59950291\n",
      "Iteration 99, loss = 0.59301759\n",
      "Iteration 100, loss = 0.58657479\n",
      "Iteration 101, loss = 0.58030332\n",
      "Iteration 102, loss = 0.57413532\n",
      "Iteration 103, loss = 0.56798395\n",
      "Iteration 104, loss = 0.56194554\n",
      "Iteration 105, loss = 0.55618584\n",
      "Iteration 106, loss = 0.55030537\n",
      "Iteration 107, loss = 0.54464367\n",
      "Iteration 108, loss = 0.53909374\n",
      "Iteration 109, loss = 0.53376654\n",
      "Iteration 110, loss = 0.52846880\n",
      "Iteration 111, loss = 0.52321439\n",
      "Iteration 112, loss = 0.51815712\n",
      "Iteration 113, loss = 0.51306800\n",
      "Iteration 114, loss = 0.50820120\n",
      "Iteration 115, loss = 0.50345201\n",
      "Iteration 116, loss = 0.49856746\n",
      "Iteration 117, loss = 0.49396201\n",
      "Iteration 118, loss = 0.48930288\n",
      "Iteration 119, loss = 0.48474092\n",
      "Iteration 120, loss = 0.48035997\n",
      "Iteration 121, loss = 0.47592563\n",
      "Iteration 122, loss = 0.47163695\n",
      "Iteration 123, loss = 0.46742361\n",
      "Iteration 124, loss = 0.46333411\n",
      "Iteration 125, loss = 0.45933284\n",
      "Iteration 126, loss = 0.45536242\n",
      "Iteration 127, loss = 0.45132585\n",
      "Iteration 128, loss = 0.44737628\n",
      "Iteration 129, loss = 0.44363370\n",
      "Iteration 130, loss = 0.43987575\n",
      "Iteration 131, loss = 0.43631968\n",
      "Iteration 132, loss = 0.43275111\n",
      "Iteration 133, loss = 0.42914114\n",
      "Iteration 134, loss = 0.42562512\n",
      "Iteration 135, loss = 0.42216203\n",
      "Iteration 136, loss = 0.41876610\n",
      "Iteration 137, loss = 0.41539022\n",
      "Iteration 138, loss = 0.41213956\n",
      "Iteration 139, loss = 0.40896616\n",
      "Iteration 140, loss = 0.40579095\n",
      "Iteration 141, loss = 0.40274966\n",
      "Iteration 142, loss = 0.39967507\n",
      "Iteration 143, loss = 0.39657680\n",
      "Iteration 144, loss = 0.39369571\n",
      "Iteration 145, loss = 0.39064713\n",
      "Iteration 146, loss = 0.38782279\n",
      "Iteration 147, loss = 0.38490578\n",
      "Iteration 148, loss = 0.38221030\n",
      "Iteration 149, loss = 0.37939985\n",
      "Iteration 150, loss = 0.37674152\n",
      "Iteration 151, loss = 0.37419988\n",
      "Iteration 152, loss = 0.37155979\n",
      "Iteration 153, loss = 0.36888692\n",
      "Iteration 154, loss = 0.36625104\n",
      "Iteration 155, loss = 0.36366024\n",
      "Iteration 156, loss = 0.36101777\n",
      "Iteration 157, loss = 0.35860098\n",
      "Iteration 158, loss = 0.35616300\n",
      "Iteration 159, loss = 0.35377206\n",
      "Iteration 160, loss = 0.35147041\n",
      "Iteration 161, loss = 0.34903949\n",
      "Iteration 162, loss = 0.34677067\n",
      "Iteration 163, loss = 0.34449296\n",
      "Iteration 164, loss = 0.34228000\n",
      "Iteration 165, loss = 0.34012162\n",
      "Iteration 166, loss = 0.33792542\n",
      "Iteration 167, loss = 0.33583427\n",
      "Iteration 168, loss = 0.33363323\n",
      "Iteration 169, loss = 0.33162069\n",
      "Iteration 170, loss = 0.32940378\n",
      "Iteration 171, loss = 0.32744078\n",
      "Iteration 172, loss = 0.32535261\n",
      "Iteration 173, loss = 0.32341573\n",
      "Iteration 174, loss = 0.32133812\n",
      "Iteration 175, loss = 0.31942024\n",
      "Iteration 176, loss = 0.31744762\n",
      "Iteration 177, loss = 0.31561490\n",
      "Iteration 178, loss = 0.31380432\n",
      "Iteration 179, loss = 0.31181993\n",
      "Iteration 180, loss = 0.30999980\n",
      "Iteration 181, loss = 0.30818407\n",
      "Iteration 182, loss = 0.30650026\n",
      "Iteration 183, loss = 0.30471609\n",
      "Iteration 184, loss = 0.30294644\n",
      "Iteration 185, loss = 0.30124663\n",
      "Iteration 186, loss = 0.29951447\n",
      "Iteration 187, loss = 0.29782496\n",
      "Iteration 188, loss = 0.29609819\n",
      "Iteration 189, loss = 0.29442760\n",
      "Iteration 190, loss = 0.29281086\n",
      "Iteration 191, loss = 0.29122618\n",
      "Iteration 192, loss = 0.28962764\n",
      "Iteration 193, loss = 0.28802329\n",
      "Iteration 194, loss = 0.28646681\n",
      "Iteration 195, loss = 0.28490608\n",
      "Iteration 196, loss = 0.28344601\n",
      "Iteration 197, loss = 0.28195822\n",
      "Iteration 198, loss = 0.28046553\n",
      "Iteration 199, loss = 0.27890758\n",
      "Iteration 200, loss = 0.27748005\n",
      "Iteration 201, loss = 0.27602099\n",
      "Iteration 202, loss = 0.27464404\n",
      "Iteration 203, loss = 0.27322617\n",
      "Iteration 204, loss = 0.27182515\n",
      "Iteration 205, loss = 0.27041157\n",
      "Iteration 206, loss = 0.26915266\n",
      "Iteration 207, loss = 0.26777315\n",
      "Iteration 208, loss = 0.26639577\n",
      "Iteration 209, loss = 0.26512837\n",
      "Iteration 210, loss = 0.26378429\n",
      "Iteration 211, loss = 0.26256003\n",
      "Iteration 212, loss = 0.26123314\n",
      "Iteration 213, loss = 0.26007114\n",
      "Iteration 214, loss = 0.25875545\n",
      "Iteration 215, loss = 0.25740874\n",
      "Iteration 216, loss = 0.25624702\n",
      "Iteration 217, loss = 0.25492366\n",
      "Iteration 218, loss = 0.25369259\n",
      "Iteration 219, loss = 0.25249354\n",
      "Iteration 220, loss = 0.25130861\n",
      "Iteration 221, loss = 0.25016137\n",
      "Iteration 222, loss = 0.24891035\n",
      "Iteration 223, loss = 0.24775901\n",
      "Iteration 224, loss = 0.24659275\n",
      "Iteration 225, loss = 0.24546107\n",
      "Iteration 226, loss = 0.24437058\n",
      "Iteration 227, loss = 0.24325504\n",
      "Iteration 228, loss = 0.24212057\n",
      "Iteration 229, loss = 0.24107975\n",
      "Iteration 230, loss = 0.24001283\n",
      "Iteration 231, loss = 0.23889675\n",
      "Iteration 232, loss = 0.23785627\n",
      "Iteration 233, loss = 0.23677956\n",
      "Iteration 234, loss = 0.23572607\n",
      "Iteration 235, loss = 0.23469122\n",
      "Iteration 236, loss = 0.23366278\n",
      "Iteration 237, loss = 0.23264058\n",
      "Iteration 238, loss = 0.23168791\n",
      "Iteration 239, loss = 0.23072826\n",
      "Iteration 240, loss = 0.22973824\n",
      "Iteration 241, loss = 0.22871787\n",
      "Iteration 242, loss = 0.22779123\n",
      "Iteration 243, loss = 0.22676207\n",
      "Iteration 244, loss = 0.22580652\n",
      "Iteration 245, loss = 0.22482345\n",
      "Iteration 246, loss = 0.22387485\n",
      "Iteration 247, loss = 0.22293680\n",
      "Iteration 248, loss = 0.22198748\n",
      "Iteration 249, loss = 0.22103793\n",
      "Iteration 250, loss = 0.22015873\n",
      "Iteration 251, loss = 0.21921975\n",
      "Iteration 252, loss = 0.21842235\n",
      "Iteration 253, loss = 0.21751936\n",
      "Iteration 254, loss = 0.21663487\n",
      "Iteration 255, loss = 0.21576102\n",
      "Iteration 256, loss = 0.21489875\n",
      "Iteration 257, loss = 0.21406389\n",
      "Iteration 258, loss = 0.21316009\n",
      "Iteration 259, loss = 0.21231034\n",
      "Iteration 260, loss = 0.21149787\n",
      "Iteration 261, loss = 0.21066073\n",
      "Iteration 262, loss = 0.20981194\n",
      "Iteration 263, loss = 0.20899314\n",
      "Iteration 264, loss = 0.20818772\n",
      "Iteration 265, loss = 0.20742409\n",
      "Iteration 266, loss = 0.20660937\n",
      "Iteration 267, loss = 0.20584361\n",
      "Iteration 268, loss = 0.20506044\n",
      "Iteration 269, loss = 0.20426676\n",
      "Iteration 270, loss = 0.20350721\n",
      "Iteration 271, loss = 0.20274062\n",
      "Iteration 272, loss = 0.20196212\n",
      "Iteration 273, loss = 0.20123364\n",
      "Iteration 274, loss = 0.20055398\n",
      "Iteration 275, loss = 0.19979004\n",
      "Iteration 276, loss = 0.19904022\n",
      "Iteration 277, loss = 0.19832678\n",
      "Iteration 278, loss = 0.19757335\n",
      "Iteration 279, loss = 0.19685812\n",
      "Iteration 280, loss = 0.19616022\n",
      "Iteration 281, loss = 0.19547836\n",
      "Iteration 282, loss = 0.19474206\n",
      "Iteration 283, loss = 0.19405225\n",
      "Iteration 284, loss = 0.19339939\n",
      "Iteration 285, loss = 0.19272123\n",
      "Iteration 286, loss = 0.19204176\n",
      "Iteration 287, loss = 0.19142063\n",
      "Iteration 288, loss = 0.19072067\n",
      "Iteration 289, loss = 0.19004170\n",
      "Iteration 290, loss = 0.18938752\n",
      "Iteration 291, loss = 0.18872156\n",
      "Iteration 292, loss = 0.18810407\n",
      "Iteration 293, loss = 0.18746822\n",
      "Iteration 294, loss = 0.18683287\n",
      "Iteration 295, loss = 0.18620085\n",
      "Iteration 296, loss = 0.18554949\n",
      "Iteration 297, loss = 0.18488132\n",
      "Iteration 298, loss = 0.18425798\n",
      "Iteration 299, loss = 0.18363955\n",
      "Iteration 300, loss = 0.18304381\n",
      "Iteration 301, loss = 0.18244344\n",
      "Iteration 302, loss = 0.18186035\n",
      "Iteration 303, loss = 0.18123743\n",
      "Iteration 304, loss = 0.18066269\n",
      "Iteration 305, loss = 0.18006131\n",
      "Iteration 306, loss = 0.17951624\n",
      "Iteration 307, loss = 0.17889457\n",
      "Iteration 308, loss = 0.17834054\n",
      "Iteration 309, loss = 0.17775971\n",
      "Iteration 310, loss = 0.17721707\n",
      "Iteration 311, loss = 0.17657398\n",
      "Iteration 312, loss = 0.17600643\n",
      "Iteration 313, loss = 0.17545915\n",
      "Iteration 314, loss = 0.17489657\n",
      "Iteration 315, loss = 0.17434082\n",
      "Iteration 316, loss = 0.17379287\n",
      "Iteration 317, loss = 0.17324812\n",
      "Iteration 318, loss = 0.17271687\n",
      "Iteration 319, loss = 0.17215297\n",
      "Iteration 320, loss = 0.17162433\n",
      "Iteration 321, loss = 0.17110914\n",
      "Iteration 322, loss = 0.17055971\n",
      "Iteration 323, loss = 0.17006742\n",
      "Iteration 324, loss = 0.16955463\n",
      "Iteration 325, loss = 0.16904544\n",
      "Iteration 326, loss = 0.16852645\n",
      "Iteration 327, loss = 0.16800568\n",
      "Iteration 328, loss = 0.16749713\n",
      "Iteration 329, loss = 0.16698311\n",
      "Iteration 330, loss = 0.16653380\n",
      "Iteration 331, loss = 0.16599757\n",
      "Iteration 332, loss = 0.16552143\n",
      "Iteration 333, loss = 0.16503711\n",
      "Iteration 334, loss = 0.16453608\n",
      "Iteration 335, loss = 0.16408969\n",
      "Iteration 336, loss = 0.16360382\n",
      "Iteration 337, loss = 0.16308057\n",
      "Iteration 338, loss = 0.16261642\n",
      "Iteration 339, loss = 0.16217092\n",
      "Iteration 340, loss = 0.16173464\n",
      "Iteration 341, loss = 0.16123618\n",
      "Iteration 342, loss = 0.16077380\n",
      "Iteration 343, loss = 0.16032621\n",
      "Iteration 344, loss = 0.15986453\n",
      "Iteration 345, loss = 0.15942288\n",
      "Iteration 346, loss = 0.15900610\n",
      "Iteration 347, loss = 0.15854879\n",
      "Iteration 348, loss = 0.15812542\n",
      "Iteration 349, loss = 0.15768199\n",
      "Iteration 350, loss = 0.15724370\n",
      "Iteration 351, loss = 0.15676535\n",
      "Iteration 352, loss = 0.15634649\n",
      "Iteration 353, loss = 0.15591858\n",
      "Iteration 354, loss = 0.15550229\n",
      "Iteration 355, loss = 0.15507389\n",
      "Iteration 356, loss = 0.15466721\n",
      "Iteration 357, loss = 0.15426649\n",
      "Iteration 358, loss = 0.15382778\n",
      "Iteration 359, loss = 0.15342585\n",
      "Iteration 360, loss = 0.15301756\n",
      "Iteration 361, loss = 0.15259836\n",
      "Iteration 362, loss = 0.15216461\n",
      "Iteration 363, loss = 0.15173297\n",
      "Iteration 364, loss = 0.15133209\n",
      "Iteration 365, loss = 0.15095305\n",
      "Iteration 366, loss = 0.15052683\n",
      "Iteration 367, loss = 0.15012400\n",
      "Iteration 368, loss = 0.14972397\n",
      "Iteration 369, loss = 0.14936370\n",
      "Iteration 370, loss = 0.14898086\n",
      "Iteration 371, loss = 0.14857200\n",
      "Iteration 372, loss = 0.14819281\n",
      "Iteration 373, loss = 0.14780673\n",
      "Iteration 374, loss = 0.14741562\n",
      "Iteration 375, loss = 0.14702331\n",
      "Iteration 376, loss = 0.14666111\n",
      "Iteration 377, loss = 0.14629204\n",
      "Iteration 378, loss = 0.14593730\n",
      "Iteration 379, loss = 0.14553308\n",
      "Iteration 380, loss = 0.14515724\n",
      "Iteration 381, loss = 0.14480016\n",
      "Iteration 382, loss = 0.14442238\n",
      "Iteration 383, loss = 0.14404684\n",
      "Iteration 384, loss = 0.14367282\n",
      "Iteration 385, loss = 0.14331920\n",
      "Iteration 386, loss = 0.14297069\n",
      "Iteration 387, loss = 0.14259378\n",
      "Iteration 388, loss = 0.14228026\n",
      "Iteration 389, loss = 0.14187502\n",
      "Iteration 390, loss = 0.14155398\n",
      "Iteration 391, loss = 0.14119187\n",
      "Iteration 392, loss = 0.14081986\n",
      "Iteration 393, loss = 0.14047635\n",
      "Iteration 394, loss = 0.14014259\n",
      "Iteration 395, loss = 0.13981218\n",
      "Iteration 396, loss = 0.13947591\n",
      "Iteration 397, loss = 0.13912557\n",
      "Iteration 398, loss = 0.13879581\n",
      "Iteration 399, loss = 0.13849610\n",
      "Iteration 400, loss = 0.13814387\n",
      "Iteration 401, loss = 0.13778049\n",
      "Iteration 402, loss = 0.13744310\n",
      "Iteration 403, loss = 0.13712676\n",
      "Iteration 404, loss = 0.13678945\n",
      "Iteration 405, loss = 0.13649375\n",
      "Iteration 406, loss = 0.13617626\n",
      "Iteration 407, loss = 0.13586957\n",
      "Iteration 408, loss = 0.13552727\n",
      "Iteration 409, loss = 0.13521087\n",
      "Iteration 410, loss = 0.13489477\n",
      "Iteration 411, loss = 0.13456090\n",
      "Iteration 412, loss = 0.13427359\n",
      "Iteration 413, loss = 0.13395619\n",
      "Iteration 414, loss = 0.13366085\n",
      "Iteration 415, loss = 0.13335219\n",
      "Iteration 416, loss = 0.13302961\n",
      "Iteration 417, loss = 0.13274391\n",
      "Iteration 418, loss = 0.13244676\n",
      "Iteration 419, loss = 0.13214321\n",
      "Iteration 420, loss = 0.13186747\n",
      "Iteration 421, loss = 0.13155305\n",
      "Iteration 422, loss = 0.13127494\n",
      "Iteration 423, loss = 0.13096967\n",
      "Iteration 424, loss = 0.13067103\n",
      "Iteration 425, loss = 0.13037936\n",
      "Iteration 426, loss = 0.13007643\n",
      "Iteration 427, loss = 0.12981514\n",
      "Iteration 428, loss = 0.12956069\n",
      "Iteration 429, loss = 0.12924732\n",
      "Iteration 430, loss = 0.12895551\n",
      "Iteration 431, loss = 0.12865751\n",
      "Iteration 432, loss = 0.12839837\n",
      "Iteration 433, loss = 0.12808200\n",
      "Iteration 434, loss = 0.12782480\n",
      "Iteration 435, loss = 0.12754258\n",
      "Iteration 436, loss = 0.12727436\n",
      "Iteration 437, loss = 0.12702437\n",
      "Iteration 438, loss = 0.12677690\n",
      "Iteration 439, loss = 0.12650567\n",
      "Iteration 440, loss = 0.12624997\n",
      "Iteration 441, loss = 0.12600276\n",
      "Iteration 442, loss = 0.12572723\n",
      "Iteration 443, loss = 0.12549240\n",
      "Iteration 444, loss = 0.12522221\n",
      "Iteration 445, loss = 0.12497619\n",
      "Iteration 446, loss = 0.12466606\n",
      "Iteration 447, loss = 0.12444273\n",
      "Iteration 448, loss = 0.12417640\n",
      "Iteration 449, loss = 0.12389361\n",
      "Iteration 450, loss = 0.12366212\n",
      "Iteration 451, loss = 0.12340691\n",
      "Iteration 452, loss = 0.12314609\n",
      "Iteration 453, loss = 0.12289041\n",
      "Iteration 454, loss = 0.12265752\n",
      "Iteration 455, loss = 0.12242233\n",
      "Iteration 456, loss = 0.12217554\n",
      "Iteration 457, loss = 0.12193198\n",
      "Iteration 458, loss = 0.12163957\n",
      "Iteration 459, loss = 0.12143621\n",
      "Iteration 460, loss = 0.12116610\n",
      "Iteration 461, loss = 0.12091229\n",
      "Iteration 462, loss = 0.12069112\n",
      "Iteration 463, loss = 0.12047977\n",
      "Iteration 464, loss = 0.12020586\n",
      "Iteration 465, loss = 0.11997885\n",
      "Iteration 466, loss = 0.11974757\n",
      "Iteration 467, loss = 0.11951553\n",
      "Iteration 468, loss = 0.11927857\n",
      "Iteration 469, loss = 0.11903869\n",
      "Iteration 470, loss = 0.11881768\n",
      "Iteration 471, loss = 0.11856538\n",
      "Iteration 472, loss = 0.11833586\n",
      "Iteration 473, loss = 0.11812394\n",
      "Iteration 474, loss = 0.11787856\n",
      "Iteration 475, loss = 0.11767622\n",
      "Iteration 476, loss = 0.11743906\n",
      "Iteration 477, loss = 0.11723748\n",
      "Iteration 478, loss = 0.11701166\n",
      "Iteration 479, loss = 0.11680673\n",
      "Iteration 480, loss = 0.11662379\n",
      "Iteration 481, loss = 0.11639163\n",
      "Iteration 482, loss = 0.11617268\n",
      "Iteration 483, loss = 0.11595451\n",
      "Iteration 484, loss = 0.11572107\n",
      "Iteration 485, loss = 0.11551362\n",
      "Iteration 486, loss = 0.11531273\n",
      "Iteration 487, loss = 0.11509622\n",
      "Iteration 488, loss = 0.11490057\n",
      "Iteration 489, loss = 0.11468554\n",
      "Iteration 490, loss = 0.11447466\n",
      "Iteration 491, loss = 0.11426556\n",
      "Iteration 492, loss = 0.11406760\n",
      "Iteration 493, loss = 0.11387356\n",
      "Iteration 494, loss = 0.11366511\n",
      "Iteration 495, loss = 0.11346095\n",
      "Iteration 496, loss = 0.11324895\n",
      "Iteration 497, loss = 0.11303975\n",
      "Iteration 498, loss = 0.11285863\n",
      "Iteration 499, loss = 0.11269167\n",
      "Iteration 500, loss = 0.11247694\n",
      "Iteration 501, loss = 0.11227099\n",
      "Iteration 502, loss = 0.11209115\n",
      "Iteration 503, loss = 0.11188478\n",
      "Iteration 504, loss = 0.11166625\n",
      "Iteration 505, loss = 0.11147868\n",
      "Iteration 506, loss = 0.11129180\n",
      "Iteration 507, loss = 0.11108800\n",
      "Iteration 508, loss = 0.11090263\n",
      "Iteration 509, loss = 0.11071631\n",
      "Iteration 510, loss = 0.11051926\n",
      "Iteration 511, loss = 0.11035361\n",
      "Iteration 512, loss = 0.11016897\n",
      "Iteration 513, loss = 0.10998294\n",
      "Iteration 514, loss = 0.10981281\n",
      "Iteration 515, loss = 0.10960927\n",
      "Iteration 516, loss = 0.10945752\n",
      "Iteration 517, loss = 0.10925450\n",
      "Iteration 518, loss = 0.10907594\n",
      "Iteration 519, loss = 0.10889823\n",
      "Iteration 520, loss = 0.10872457\n",
      "Iteration 521, loss = 0.10857274\n",
      "Iteration 522, loss = 0.10841120\n",
      "Iteration 523, loss = 0.10820035\n",
      "Iteration 524, loss = 0.10801874\n",
      "Iteration 525, loss = 0.10785226\n",
      "Iteration 526, loss = 0.10764816\n",
      "Iteration 527, loss = 0.10748982\n",
      "Iteration 528, loss = 0.10731897\n",
      "Iteration 529, loss = 0.10717643\n",
      "Iteration 530, loss = 0.10699183\n",
      "Iteration 531, loss = 0.10681783\n",
      "Iteration 532, loss = 0.10663277\n",
      "Iteration 533, loss = 0.10646212\n",
      "Iteration 534, loss = 0.10631403\n",
      "Iteration 535, loss = 0.10614343\n",
      "Iteration 536, loss = 0.10596232\n",
      "Iteration 537, loss = 0.10580202\n",
      "Iteration 538, loss = 0.10561751\n",
      "Iteration 539, loss = 0.10547031\n",
      "Iteration 540, loss = 0.10531006\n",
      "Iteration 541, loss = 0.10515038\n",
      "Iteration 542, loss = 0.10499391\n",
      "Iteration 543, loss = 0.10482453\n",
      "Iteration 544, loss = 0.10465793\n",
      "Iteration 545, loss = 0.10449517\n",
      "Iteration 546, loss = 0.10434737\n",
      "Iteration 547, loss = 0.10417860\n",
      "Iteration 548, loss = 0.10403072\n",
      "Iteration 549, loss = 0.10388078\n",
      "Iteration 550, loss = 0.10372244\n",
      "Iteration 551, loss = 0.10356627\n",
      "Iteration 552, loss = 0.10340415\n",
      "Iteration 553, loss = 0.10326111\n",
      "Iteration 554, loss = 0.10311485\n",
      "Iteration 555, loss = 0.10296421\n",
      "Iteration 556, loss = 0.10279806\n",
      "Iteration 557, loss = 0.10263732\n",
      "Iteration 558, loss = 0.10249048\n",
      "Iteration 559, loss = 0.10234957\n",
      "Iteration 560, loss = 0.10221312\n",
      "Iteration 561, loss = 0.10203847\n",
      "Iteration 562, loss = 0.10190185\n",
      "Iteration 563, loss = 0.10175479\n",
      "Iteration 564, loss = 0.10161692\n",
      "Iteration 565, loss = 0.10146516\n",
      "Iteration 566, loss = 0.10130314\n",
      "Iteration 567, loss = 0.10115596\n",
      "Iteration 568, loss = 0.10100675\n",
      "Iteration 569, loss = 0.10087887\n",
      "Iteration 570, loss = 0.10073701\n",
      "Iteration 571, loss = 0.10061198\n",
      "Iteration 572, loss = 0.10046216\n",
      "Iteration 573, loss = 0.10030132\n",
      "Iteration 574, loss = 0.10016619\n",
      "Iteration 575, loss = 0.10003470\n",
      "Iteration 576, loss = 0.09989890\n",
      "Iteration 577, loss = 0.09975557\n",
      "Iteration 578, loss = 0.09963134\n",
      "Iteration 579, loss = 0.09949746\n",
      "Iteration 580, loss = 0.09935261\n",
      "Iteration 581, loss = 0.09924399\n",
      "Iteration 582, loss = 0.09908090\n",
      "Iteration 583, loss = 0.09893278\n",
      "Iteration 584, loss = 0.09881614\n",
      "Iteration 585, loss = 0.09865925\n",
      "Iteration 586, loss = 0.09853236\n",
      "Iteration 587, loss = 0.09839813\n",
      "Iteration 588, loss = 0.09827094\n",
      "Iteration 589, loss = 0.09814639\n",
      "Iteration 590, loss = 0.09802550\n",
      "Iteration 591, loss = 0.09787844\n",
      "Iteration 592, loss = 0.09776311\n",
      "Iteration 593, loss = 0.09763532\n",
      "Iteration 594, loss = 0.09751869\n",
      "Iteration 595, loss = 0.09738285\n",
      "Iteration 596, loss = 0.09725813\n",
      "Iteration 597, loss = 0.09712925\n",
      "Iteration 598, loss = 0.09701915\n",
      "Iteration 599, loss = 0.09688454\n",
      "Iteration 600, loss = 0.09676670\n",
      "Iteration 601, loss = 0.09664497\n",
      "Iteration 602, loss = 0.09652410\n",
      "Iteration 603, loss = 0.09638634\n",
      "Iteration 604, loss = 0.09626879\n",
      "Iteration 605, loss = 0.09613579\n",
      "Iteration 606, loss = 0.09602730\n",
      "Iteration 607, loss = 0.09591654\n",
      "Iteration 608, loss = 0.09578952\n",
      "Iteration 609, loss = 0.09566658\n",
      "Iteration 610, loss = 0.09555352\n",
      "Iteration 611, loss = 0.09543920\n",
      "Iteration 612, loss = 0.09532259\n",
      "Iteration 613, loss = 0.09520664\n",
      "Iteration 614, loss = 0.09507787\n",
      "Iteration 615, loss = 0.09496868\n",
      "Iteration 616, loss = 0.09484972\n",
      "Iteration 617, loss = 0.09473784\n",
      "Iteration 618, loss = 0.09460920\n",
      "Iteration 619, loss = 0.09451170\n",
      "Iteration 620, loss = 0.09438248\n",
      "Iteration 621, loss = 0.09426994\n",
      "Iteration 622, loss = 0.09416520\n",
      "Iteration 623, loss = 0.09405828\n",
      "Iteration 624, loss = 0.09393305\n",
      "Iteration 625, loss = 0.09382379\n",
      "Iteration 626, loss = 0.09370655\n",
      "Iteration 627, loss = 0.09357958\n",
      "Iteration 628, loss = 0.09347792\n",
      "Iteration 629, loss = 0.09335461\n",
      "Iteration 630, loss = 0.09324543\n",
      "Iteration 631, loss = 0.09312280\n",
      "Iteration 632, loss = 0.09301659\n",
      "Iteration 633, loss = 0.09291532\n",
      "Iteration 634, loss = 0.09279491\n",
      "Iteration 635, loss = 0.09268673\n",
      "Iteration 636, loss = 0.09258530\n",
      "Iteration 637, loss = 0.09249047\n",
      "Iteration 638, loss = 0.09235306\n",
      "Iteration 639, loss = 0.09224180\n",
      "Iteration 640, loss = 0.09213435\n",
      "Iteration 641, loss = 0.09203325\n",
      "Iteration 642, loss = 0.09193378\n",
      "Iteration 643, loss = 0.09182467\n",
      "Iteration 644, loss = 0.09172336\n",
      "Iteration 645, loss = 0.09161119\n",
      "Iteration 646, loss = 0.09150832\n",
      "Iteration 647, loss = 0.09140377\n",
      "Iteration 648, loss = 0.09130372\n",
      "Iteration 649, loss = 0.09119956\n",
      "Iteration 650, loss = 0.09110487\n",
      "Iteration 651, loss = 0.09101206\n",
      "Iteration 652, loss = 0.09090506\n",
      "Iteration 653, loss = 0.09081289\n",
      "Iteration 654, loss = 0.09070954\n",
      "Iteration 655, loss = 0.09061463\n",
      "Iteration 656, loss = 0.09050903\n",
      "Iteration 657, loss = 0.09041420\n",
      "Iteration 658, loss = 0.09031319\n",
      "Iteration 659, loss = 0.09021425\n",
      "Iteration 660, loss = 0.09011729\n",
      "Iteration 661, loss = 0.09002313\n",
      "Iteration 662, loss = 0.08993242\n",
      "Iteration 663, loss = 0.08984006\n",
      "Iteration 664, loss = 0.08973463\n",
      "Iteration 665, loss = 0.08965062\n",
      "Iteration 666, loss = 0.08955215\n",
      "Iteration 667, loss = 0.08945653\n",
      "Iteration 668, loss = 0.08934570\n",
      "Iteration 669, loss = 0.08926361\n",
      "Iteration 670, loss = 0.08916254\n",
      "Iteration 671, loss = 0.08908664\n",
      "Iteration 672, loss = 0.08897185\n",
      "Iteration 673, loss = 0.08888744\n",
      "Iteration 674, loss = 0.08877951\n",
      "Iteration 675, loss = 0.08869261\n",
      "Iteration 676, loss = 0.08860124\n",
      "Iteration 677, loss = 0.08849147\n",
      "Iteration 678, loss = 0.08840450\n",
      "Iteration 679, loss = 0.08831772\n",
      "Iteration 680, loss = 0.08821284\n",
      "Iteration 681, loss = 0.08812749\n",
      "Iteration 682, loss = 0.08802677\n",
      "Iteration 683, loss = 0.08793697\n",
      "Iteration 684, loss = 0.08784859\n",
      "Iteration 685, loss = 0.08774946\n",
      "Iteration 686, loss = 0.08767146\n",
      "Iteration 687, loss = 0.08758438\n",
      "Iteration 688, loss = 0.08748709\n",
      "Iteration 689, loss = 0.08740024\n",
      "Iteration 690, loss = 0.08730358\n",
      "Iteration 691, loss = 0.08721813\n",
      "Iteration 692, loss = 0.08713151\n",
      "Iteration 693, loss = 0.08705417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.36792629\n",
      "Iteration 2, loss = 2.34573054\n",
      "Iteration 3, loss = 2.31729690\n",
      "Iteration 4, loss = 2.28887417\n",
      "Iteration 5, loss = 2.26262775\n",
      "Iteration 6, loss = 2.23734607\n",
      "Iteration 7, loss = 2.21355937\n",
      "Iteration 8, loss = 2.18950425\n",
      "Iteration 9, loss = 2.16563441\n",
      "Iteration 10, loss = 2.14230079\n",
      "Iteration 11, loss = 2.11861660\n",
      "Iteration 12, loss = 2.09461983\n",
      "Iteration 13, loss = 2.07092139\n",
      "Iteration 14, loss = 2.04693102\n",
      "Iteration 15, loss = 2.02266041\n",
      "Iteration 16, loss = 1.99811824\n",
      "Iteration 17, loss = 1.97323087\n",
      "Iteration 18, loss = 1.94815416\n",
      "Iteration 19, loss = 1.92259745\n",
      "Iteration 20, loss = 1.89670123\n",
      "Iteration 21, loss = 1.87030711\n",
      "Iteration 22, loss = 1.84367104\n",
      "Iteration 23, loss = 1.81687725\n",
      "Iteration 24, loss = 1.79021350\n",
      "Iteration 25, loss = 1.76321346\n",
      "Iteration 26, loss = 1.73601463\n",
      "Iteration 27, loss = 1.70826658\n",
      "Iteration 28, loss = 1.68052046\n",
      "Iteration 29, loss = 1.65257563\n",
      "Iteration 30, loss = 1.62455753\n",
      "Iteration 31, loss = 1.59667773\n",
      "Iteration 32, loss = 1.56881098\n",
      "Iteration 33, loss = 1.54122603\n",
      "Iteration 34, loss = 1.51375891\n",
      "Iteration 35, loss = 1.48683720\n",
      "Iteration 36, loss = 1.46021027\n",
      "Iteration 37, loss = 1.43406456\n",
      "Iteration 38, loss = 1.40825225\n",
      "Iteration 39, loss = 1.38302334\n",
      "Iteration 40, loss = 1.35818937\n",
      "Iteration 41, loss = 1.33351871\n",
      "Iteration 42, loss = 1.30975259\n",
      "Iteration 43, loss = 1.28623369\n",
      "Iteration 44, loss = 1.26300027\n",
      "Iteration 45, loss = 1.23989651\n",
      "Iteration 46, loss = 1.21760603\n",
      "Iteration 47, loss = 1.19551011\n",
      "Iteration 48, loss = 1.17403749\n",
      "Iteration 49, loss = 1.15296699\n",
      "Iteration 50, loss = 1.13252273\n",
      "Iteration 51, loss = 1.11233851\n",
      "Iteration 52, loss = 1.09265006\n",
      "Iteration 53, loss = 1.07343195\n",
      "Iteration 54, loss = 1.05474876\n",
      "Iteration 55, loss = 1.03652059\n",
      "Iteration 56, loss = 1.01852416\n",
      "Iteration 57, loss = 1.00119467\n",
      "Iteration 58, loss = 0.98420507\n",
      "Iteration 59, loss = 0.96747086\n",
      "Iteration 60, loss = 0.95120285\n",
      "Iteration 61, loss = 0.93544917\n",
      "Iteration 62, loss = 0.92025136\n",
      "Iteration 63, loss = 0.90514061\n",
      "Iteration 64, loss = 0.89060473\n",
      "Iteration 65, loss = 0.87635574\n",
      "Iteration 66, loss = 0.86238851\n",
      "Iteration 67, loss = 0.84885978\n",
      "Iteration 68, loss = 0.83569039\n",
      "Iteration 69, loss = 0.82272340\n",
      "Iteration 70, loss = 0.81016831\n",
      "Iteration 71, loss = 0.79790077\n",
      "Iteration 72, loss = 0.78585225\n",
      "Iteration 73, loss = 0.77406246\n",
      "Iteration 74, loss = 0.76264307\n",
      "Iteration 75, loss = 0.75145012\n",
      "Iteration 76, loss = 0.74044888\n",
      "Iteration 77, loss = 0.72959148\n",
      "Iteration 78, loss = 0.71937426\n",
      "Iteration 79, loss = 0.70924767\n",
      "Iteration 80, loss = 0.69952032\n",
      "Iteration 81, loss = 0.68984621\n",
      "Iteration 82, loss = 0.68034402\n",
      "Iteration 83, loss = 0.67114114\n",
      "Iteration 84, loss = 0.66217918\n",
      "Iteration 85, loss = 0.65323953\n",
      "Iteration 86, loss = 0.64469467\n",
      "Iteration 87, loss = 0.63629967\n",
      "Iteration 88, loss = 0.62812044\n",
      "Iteration 89, loss = 0.62026107\n",
      "Iteration 90, loss = 0.61234616\n",
      "Iteration 91, loss = 0.60462001\n",
      "Iteration 92, loss = 0.59708585\n",
      "Iteration 93, loss = 0.58984285\n",
      "Iteration 94, loss = 0.58247186\n",
      "Iteration 95, loss = 0.57544103\n",
      "Iteration 96, loss = 0.56847477\n",
      "Iteration 97, loss = 0.56170930\n",
      "Iteration 98, loss = 0.55522830\n",
      "Iteration 99, loss = 0.54888876\n",
      "Iteration 100, loss = 0.54246544\n",
      "Iteration 101, loss = 0.53638415\n",
      "Iteration 102, loss = 0.53022272\n",
      "Iteration 103, loss = 0.52434930\n",
      "Iteration 104, loss = 0.51849414\n",
      "Iteration 105, loss = 0.51283059\n",
      "Iteration 106, loss = 0.50716477\n",
      "Iteration 107, loss = 0.50158799\n",
      "Iteration 108, loss = 0.49623884\n",
      "Iteration 109, loss = 0.49099016\n",
      "Iteration 110, loss = 0.48584380\n",
      "Iteration 111, loss = 0.48081327\n",
      "Iteration 112, loss = 0.47594151\n",
      "Iteration 113, loss = 0.47103574\n",
      "Iteration 114, loss = 0.46624466\n",
      "Iteration 115, loss = 0.46156046\n",
      "Iteration 116, loss = 0.45688272\n",
      "Iteration 117, loss = 0.45244245\n",
      "Iteration 118, loss = 0.44801463\n",
      "Iteration 119, loss = 0.44366664\n",
      "Iteration 120, loss = 0.43943644\n",
      "Iteration 121, loss = 0.43519085\n",
      "Iteration 122, loss = 0.43112619\n",
      "Iteration 123, loss = 0.42705410\n",
      "Iteration 124, loss = 0.42299903\n",
      "Iteration 125, loss = 0.41915043\n",
      "Iteration 126, loss = 0.41532221\n",
      "Iteration 127, loss = 0.41155351\n",
      "Iteration 128, loss = 0.40779665\n",
      "Iteration 129, loss = 0.40417840\n",
      "Iteration 130, loss = 0.40055247\n",
      "Iteration 131, loss = 0.39710049\n",
      "Iteration 132, loss = 0.39378293\n",
      "Iteration 133, loss = 0.39026954\n",
      "Iteration 134, loss = 0.38694009\n",
      "Iteration 135, loss = 0.38369759\n",
      "Iteration 136, loss = 0.38047642\n",
      "Iteration 137, loss = 0.37722594\n",
      "Iteration 138, loss = 0.37410500\n",
      "Iteration 139, loss = 0.37108383\n",
      "Iteration 140, loss = 0.36806107\n",
      "Iteration 141, loss = 0.36520129\n",
      "Iteration 142, loss = 0.36230865\n",
      "Iteration 143, loss = 0.35934368\n",
      "Iteration 144, loss = 0.35659363\n",
      "Iteration 145, loss = 0.35372823\n",
      "Iteration 146, loss = 0.35106785\n",
      "Iteration 147, loss = 0.34831839\n",
      "Iteration 148, loss = 0.34567633\n",
      "Iteration 149, loss = 0.34307383\n",
      "Iteration 150, loss = 0.34055009\n",
      "Iteration 151, loss = 0.33803605\n",
      "Iteration 152, loss = 0.33552711\n",
      "Iteration 153, loss = 0.33304526\n",
      "Iteration 154, loss = 0.33054357\n",
      "Iteration 155, loss = 0.32816591\n",
      "Iteration 156, loss = 0.32571765\n",
      "Iteration 157, loss = 0.32345733\n",
      "Iteration 158, loss = 0.32114681\n",
      "Iteration 159, loss = 0.31891426\n",
      "Iteration 160, loss = 0.31678382\n",
      "Iteration 161, loss = 0.31452031\n",
      "Iteration 162, loss = 0.31229208\n",
      "Iteration 163, loss = 0.31016565\n",
      "Iteration 164, loss = 0.30807379\n",
      "Iteration 165, loss = 0.30601338\n",
      "Iteration 166, loss = 0.30397234\n",
      "Iteration 167, loss = 0.30188054\n",
      "Iteration 168, loss = 0.29988832\n",
      "Iteration 169, loss = 0.29794761\n",
      "Iteration 170, loss = 0.29599309\n",
      "Iteration 171, loss = 0.29408487\n",
      "Iteration 172, loss = 0.29214802\n",
      "Iteration 173, loss = 0.29034690\n",
      "Iteration 174, loss = 0.28840242\n",
      "Iteration 175, loss = 0.28673633\n",
      "Iteration 176, loss = 0.28480545\n",
      "Iteration 177, loss = 0.28301658\n",
      "Iteration 178, loss = 0.28137784\n",
      "Iteration 179, loss = 0.27951482\n",
      "Iteration 180, loss = 0.27782155\n",
      "Iteration 181, loss = 0.27611412\n",
      "Iteration 182, loss = 0.27451694\n",
      "Iteration 183, loss = 0.27279466\n",
      "Iteration 184, loss = 0.27111687\n",
      "Iteration 185, loss = 0.26957832\n",
      "Iteration 186, loss = 0.26799940\n",
      "Iteration 187, loss = 0.26646160\n",
      "Iteration 188, loss = 0.26490253\n",
      "Iteration 189, loss = 0.26335602\n",
      "Iteration 190, loss = 0.26186514\n",
      "Iteration 191, loss = 0.26035092\n",
      "Iteration 192, loss = 0.25889388\n",
      "Iteration 193, loss = 0.25738955\n",
      "Iteration 194, loss = 0.25596388\n",
      "Iteration 195, loss = 0.25452038\n",
      "Iteration 196, loss = 0.25309045\n",
      "Iteration 197, loss = 0.25170299\n",
      "Iteration 198, loss = 0.25030223\n",
      "Iteration 199, loss = 0.24887097\n",
      "Iteration 200, loss = 0.24750686\n",
      "Iteration 201, loss = 0.24615665\n",
      "Iteration 202, loss = 0.24494498\n",
      "Iteration 203, loss = 0.24360027\n",
      "Iteration 204, loss = 0.24229843\n",
      "Iteration 205, loss = 0.24097917\n",
      "Iteration 206, loss = 0.23973225\n",
      "Iteration 207, loss = 0.23847176\n",
      "Iteration 208, loss = 0.23725901\n",
      "Iteration 209, loss = 0.23610294\n",
      "Iteration 210, loss = 0.23488770\n",
      "Iteration 211, loss = 0.23371576\n",
      "Iteration 212, loss = 0.23245355\n",
      "Iteration 213, loss = 0.23128752\n",
      "Iteration 214, loss = 0.23010309\n",
      "Iteration 215, loss = 0.22892007\n",
      "Iteration 216, loss = 0.22781967\n",
      "Iteration 217, loss = 0.22663081\n",
      "Iteration 218, loss = 0.22552259\n",
      "Iteration 219, loss = 0.22442285\n",
      "Iteration 220, loss = 0.22334173\n",
      "Iteration 221, loss = 0.22229258\n",
      "Iteration 222, loss = 0.22114027\n",
      "Iteration 223, loss = 0.22004328\n",
      "Iteration 224, loss = 0.21899591\n",
      "Iteration 225, loss = 0.21798183\n",
      "Iteration 226, loss = 0.21697770\n",
      "Iteration 227, loss = 0.21592400\n",
      "Iteration 228, loss = 0.21491061\n",
      "Iteration 229, loss = 0.21392229\n",
      "Iteration 230, loss = 0.21291102\n",
      "Iteration 231, loss = 0.21190093\n",
      "Iteration 232, loss = 0.21095931\n",
      "Iteration 233, loss = 0.20999968\n",
      "Iteration 234, loss = 0.20907559\n",
      "Iteration 235, loss = 0.20813349\n",
      "Iteration 236, loss = 0.20718685\n",
      "Iteration 237, loss = 0.20624940\n",
      "Iteration 238, loss = 0.20532791\n",
      "Iteration 239, loss = 0.20443968\n",
      "Iteration 240, loss = 0.20350506\n",
      "Iteration 241, loss = 0.20266175\n",
      "Iteration 242, loss = 0.20174524\n",
      "Iteration 243, loss = 0.20079156\n",
      "Iteration 244, loss = 0.19996391\n",
      "Iteration 245, loss = 0.19908855\n",
      "Iteration 246, loss = 0.19821260\n",
      "Iteration 247, loss = 0.19740193\n",
      "Iteration 248, loss = 0.19651371\n",
      "Iteration 249, loss = 0.19569588\n",
      "Iteration 250, loss = 0.19486977\n",
      "Iteration 251, loss = 0.19405700\n",
      "Iteration 252, loss = 0.19328363\n",
      "Iteration 253, loss = 0.19248615\n",
      "Iteration 254, loss = 0.19168096\n",
      "Iteration 255, loss = 0.19089871\n",
      "Iteration 256, loss = 0.19008624\n",
      "Iteration 257, loss = 0.18933832\n",
      "Iteration 258, loss = 0.18861622\n",
      "Iteration 259, loss = 0.18781825\n",
      "Iteration 260, loss = 0.18705682\n",
      "Iteration 261, loss = 0.18631074\n",
      "Iteration 262, loss = 0.18552406\n",
      "Iteration 263, loss = 0.18480974\n",
      "Iteration 264, loss = 0.18406563\n",
      "Iteration 265, loss = 0.18338705\n",
      "Iteration 266, loss = 0.18265850\n",
      "Iteration 267, loss = 0.18198955\n",
      "Iteration 268, loss = 0.18125238\n",
      "Iteration 269, loss = 0.18053905\n",
      "Iteration 270, loss = 0.17985618\n",
      "Iteration 271, loss = 0.17912860\n",
      "Iteration 272, loss = 0.17845442\n",
      "Iteration 273, loss = 0.17777832\n",
      "Iteration 274, loss = 0.17710247\n",
      "Iteration 275, loss = 0.17643560\n",
      "Iteration 276, loss = 0.17582417\n",
      "Iteration 277, loss = 0.17512698\n",
      "Iteration 278, loss = 0.17449599\n",
      "Iteration 279, loss = 0.17383859\n",
      "Iteration 280, loss = 0.17317470\n",
      "Iteration 281, loss = 0.17258004\n",
      "Iteration 282, loss = 0.17193840\n",
      "Iteration 283, loss = 0.17133814\n",
      "Iteration 284, loss = 0.17073347\n",
      "Iteration 285, loss = 0.17011720\n",
      "Iteration 286, loss = 0.16951577\n",
      "Iteration 287, loss = 0.16895235\n",
      "Iteration 288, loss = 0.16834242\n",
      "Iteration 289, loss = 0.16771487\n",
      "Iteration 290, loss = 0.16712832\n",
      "Iteration 291, loss = 0.16655062\n",
      "Iteration 292, loss = 0.16599412\n",
      "Iteration 293, loss = 0.16542972\n",
      "Iteration 294, loss = 0.16482063\n",
      "Iteration 295, loss = 0.16427319\n",
      "Iteration 296, loss = 0.16371714\n",
      "Iteration 297, loss = 0.16315242\n",
      "Iteration 298, loss = 0.16261379\n",
      "Iteration 299, loss = 0.16207069\n",
      "Iteration 300, loss = 0.16154464\n",
      "Iteration 301, loss = 0.16097827\n",
      "Iteration 302, loss = 0.16045364\n",
      "Iteration 303, loss = 0.15992299\n",
      "Iteration 304, loss = 0.15939709\n",
      "Iteration 305, loss = 0.15891986\n",
      "Iteration 306, loss = 0.15836751\n",
      "Iteration 307, loss = 0.15782785\n",
      "Iteration 308, loss = 0.15732807\n",
      "Iteration 309, loss = 0.15681126\n",
      "Iteration 310, loss = 0.15630244\n",
      "Iteration 311, loss = 0.15580940\n",
      "Iteration 312, loss = 0.15529054\n",
      "Iteration 313, loss = 0.15481375\n",
      "Iteration 314, loss = 0.15429223\n",
      "Iteration 315, loss = 0.15381357\n",
      "Iteration 316, loss = 0.15334686\n",
      "Iteration 317, loss = 0.15283806\n",
      "Iteration 318, loss = 0.15237752\n",
      "Iteration 319, loss = 0.15188794\n",
      "Iteration 320, loss = 0.15142342\n",
      "Iteration 321, loss = 0.15097460\n",
      "Iteration 322, loss = 0.15048798\n",
      "Iteration 323, loss = 0.15005494\n",
      "Iteration 324, loss = 0.14957599\n",
      "Iteration 325, loss = 0.14913692\n",
      "Iteration 326, loss = 0.14871336\n",
      "Iteration 327, loss = 0.14826376\n",
      "Iteration 328, loss = 0.14780783\n",
      "Iteration 329, loss = 0.14737146\n",
      "Iteration 330, loss = 0.14699111\n",
      "Iteration 331, loss = 0.14653294\n",
      "Iteration 332, loss = 0.14611997\n",
      "Iteration 333, loss = 0.14566380\n",
      "Iteration 334, loss = 0.14522093\n",
      "Iteration 335, loss = 0.14482559\n",
      "Iteration 336, loss = 0.14439990\n",
      "Iteration 337, loss = 0.14392024\n",
      "Iteration 338, loss = 0.14359469\n",
      "Iteration 339, loss = 0.14314153\n",
      "Iteration 340, loss = 0.14271069\n",
      "Iteration 341, loss = 0.14227803\n",
      "Iteration 342, loss = 0.14185880\n",
      "Iteration 343, loss = 0.14148428\n",
      "Iteration 344, loss = 0.14105665\n",
      "Iteration 345, loss = 0.14066976\n",
      "Iteration 346, loss = 0.14027463\n",
      "Iteration 347, loss = 0.13989558\n",
      "Iteration 348, loss = 0.13950479\n",
      "Iteration 349, loss = 0.13913563\n",
      "Iteration 350, loss = 0.13875420\n",
      "Iteration 351, loss = 0.13837863\n",
      "Iteration 352, loss = 0.13800440\n",
      "Iteration 353, loss = 0.13762485\n",
      "Iteration 354, loss = 0.13723777\n",
      "Iteration 355, loss = 0.13690215\n",
      "Iteration 356, loss = 0.13654985\n",
      "Iteration 357, loss = 0.13617819\n",
      "Iteration 358, loss = 0.13580533\n",
      "Iteration 359, loss = 0.13544847\n",
      "Iteration 360, loss = 0.13511281\n",
      "Iteration 361, loss = 0.13478110\n",
      "Iteration 362, loss = 0.13440174\n",
      "Iteration 363, loss = 0.13403532\n",
      "Iteration 364, loss = 0.13368176\n",
      "Iteration 365, loss = 0.13334456\n",
      "Iteration 366, loss = 0.13298623\n",
      "Iteration 367, loss = 0.13264470\n",
      "Iteration 368, loss = 0.13228212\n",
      "Iteration 369, loss = 0.13196750\n",
      "Iteration 370, loss = 0.13162970\n",
      "Iteration 371, loss = 0.13128462\n",
      "Iteration 372, loss = 0.13096680\n",
      "Iteration 373, loss = 0.13062200\n",
      "Iteration 374, loss = 0.13034104\n",
      "Iteration 375, loss = 0.12998167\n",
      "Iteration 376, loss = 0.12965115\n",
      "Iteration 377, loss = 0.12932290\n",
      "Iteration 378, loss = 0.12900847\n",
      "Iteration 379, loss = 0.12867967\n",
      "Iteration 380, loss = 0.12835775\n",
      "Iteration 381, loss = 0.12804057\n",
      "Iteration 382, loss = 0.12772498\n",
      "Iteration 383, loss = 0.12739448\n",
      "Iteration 384, loss = 0.12708781\n",
      "Iteration 385, loss = 0.12678217\n",
      "Iteration 386, loss = 0.12646727\n",
      "Iteration 387, loss = 0.12617736\n",
      "Iteration 388, loss = 0.12587509\n",
      "Iteration 389, loss = 0.12556399\n",
      "Iteration 390, loss = 0.12526983\n",
      "Iteration 391, loss = 0.12499634\n",
      "Iteration 392, loss = 0.12470462\n",
      "Iteration 393, loss = 0.12440625\n",
      "Iteration 394, loss = 0.12410590\n",
      "Iteration 395, loss = 0.12383317\n",
      "Iteration 396, loss = 0.12354177\n",
      "Iteration 397, loss = 0.12323630\n",
      "Iteration 398, loss = 0.12296213\n",
      "Iteration 399, loss = 0.12272181\n",
      "Iteration 400, loss = 0.12241372\n",
      "Iteration 401, loss = 0.12211998\n",
      "Iteration 402, loss = 0.12183241\n",
      "Iteration 403, loss = 0.12155169\n",
      "Iteration 404, loss = 0.12126308\n",
      "Iteration 405, loss = 0.12099900\n",
      "Iteration 406, loss = 0.12072766\n",
      "Iteration 407, loss = 0.12046073\n",
      "Iteration 408, loss = 0.12017962\n",
      "Iteration 409, loss = 0.11993217\n",
      "Iteration 410, loss = 0.11964222\n",
      "Iteration 411, loss = 0.11938572\n",
      "Iteration 412, loss = 0.11912891\n",
      "Iteration 413, loss = 0.11886803\n",
      "Iteration 414, loss = 0.11860466\n",
      "Iteration 415, loss = 0.11834505\n",
      "Iteration 416, loss = 0.11807659\n",
      "Iteration 417, loss = 0.11780770\n",
      "Iteration 418, loss = 0.11756440\n",
      "Iteration 419, loss = 0.11731522\n",
      "Iteration 420, loss = 0.11707780\n",
      "Iteration 421, loss = 0.11683396\n",
      "Iteration 422, loss = 0.11659293\n",
      "Iteration 423, loss = 0.11634002\n",
      "Iteration 424, loss = 0.11608491\n",
      "Iteration 425, loss = 0.11582507\n",
      "Iteration 426, loss = 0.11555982\n",
      "Iteration 427, loss = 0.11533413\n",
      "Iteration 428, loss = 0.11511849\n",
      "Iteration 429, loss = 0.11486436\n",
      "Iteration 430, loss = 0.11461608\n",
      "Iteration 431, loss = 0.11436888\n",
      "Iteration 432, loss = 0.11414244\n",
      "Iteration 433, loss = 0.11393600\n",
      "Iteration 434, loss = 0.11369561\n",
      "Iteration 435, loss = 0.11345101\n",
      "Iteration 436, loss = 0.11321624\n",
      "Iteration 437, loss = 0.11298538\n",
      "Iteration 438, loss = 0.11275408\n",
      "Iteration 439, loss = 0.11253733\n",
      "Iteration 440, loss = 0.11231205\n",
      "Iteration 441, loss = 0.11209744\n",
      "Iteration 442, loss = 0.11188621\n",
      "Iteration 443, loss = 0.11167462\n",
      "Iteration 444, loss = 0.11145299\n",
      "Iteration 445, loss = 0.11123940\n",
      "Iteration 446, loss = 0.11100838\n",
      "Iteration 447, loss = 0.11078982\n",
      "Iteration 448, loss = 0.11058186\n",
      "Iteration 449, loss = 0.11035393\n",
      "Iteration 450, loss = 0.11015538\n",
      "Iteration 451, loss = 0.10996158\n",
      "Iteration 452, loss = 0.10975234\n",
      "Iteration 453, loss = 0.10953687\n",
      "Iteration 454, loss = 0.10932389\n",
      "Iteration 455, loss = 0.10914173\n",
      "Iteration 456, loss = 0.10892098\n",
      "Iteration 457, loss = 0.10870323\n",
      "Iteration 458, loss = 0.10848475\n",
      "Iteration 459, loss = 0.10828586\n",
      "Iteration 460, loss = 0.10807705\n",
      "Iteration 461, loss = 0.10786007\n",
      "Iteration 462, loss = 0.10767663\n",
      "Iteration 463, loss = 0.10750612\n",
      "Iteration 464, loss = 0.10729353\n",
      "Iteration 465, loss = 0.10709896\n",
      "Iteration 466, loss = 0.10691172\n",
      "Iteration 467, loss = 0.10673119\n",
      "Iteration 468, loss = 0.10653057\n",
      "Iteration 469, loss = 0.10632998\n",
      "Iteration 470, loss = 0.10613368\n",
      "Iteration 471, loss = 0.10593651\n",
      "Iteration 472, loss = 0.10574514\n",
      "Iteration 473, loss = 0.10555633\n",
      "Iteration 474, loss = 0.10536706\n",
      "Iteration 475, loss = 0.10519196\n",
      "Iteration 476, loss = 0.10499972\n",
      "Iteration 477, loss = 0.10480894\n",
      "Iteration 478, loss = 0.10464129\n",
      "Iteration 479, loss = 0.10444818\n",
      "Iteration 480, loss = 0.10428164\n",
      "Iteration 481, loss = 0.10409692\n",
      "Iteration 482, loss = 0.10390918\n",
      "Iteration 483, loss = 0.10373490\n",
      "Iteration 484, loss = 0.10354954\n",
      "Iteration 485, loss = 0.10338063\n",
      "Iteration 486, loss = 0.10320407\n",
      "Iteration 487, loss = 0.10303266\n",
      "Iteration 488, loss = 0.10286242\n",
      "Iteration 489, loss = 0.10270261\n",
      "Iteration 490, loss = 0.10252158\n",
      "Iteration 491, loss = 0.10235558\n",
      "Iteration 492, loss = 0.10219477\n",
      "Iteration 493, loss = 0.10202718\n",
      "Iteration 494, loss = 0.10186348\n",
      "Iteration 495, loss = 0.10171100\n",
      "Iteration 496, loss = 0.10152890\n",
      "Iteration 497, loss = 0.10134883\n",
      "Iteration 498, loss = 0.10118942\n",
      "Iteration 499, loss = 0.10103891\n",
      "Iteration 500, loss = 0.10087250\n",
      "Iteration 501, loss = 0.10069645\n",
      "Iteration 502, loss = 0.10054730\n",
      "Iteration 503, loss = 0.10038763\n",
      "Iteration 504, loss = 0.10021256\n",
      "Iteration 505, loss = 0.10006828\n",
      "Iteration 506, loss = 0.09991129\n",
      "Iteration 507, loss = 0.09974625\n",
      "Iteration 508, loss = 0.09958887\n",
      "Iteration 509, loss = 0.09943853\n",
      "Iteration 510, loss = 0.09928214\n",
      "Iteration 511, loss = 0.09913980\n",
      "Iteration 512, loss = 0.09898517\n",
      "Iteration 513, loss = 0.09883441\n",
      "Iteration 514, loss = 0.09868438\n",
      "Iteration 515, loss = 0.09852548\n",
      "Iteration 516, loss = 0.09839806\n",
      "Iteration 517, loss = 0.09823266\n",
      "Iteration 518, loss = 0.09808357\n",
      "Iteration 519, loss = 0.09793713\n",
      "Iteration 520, loss = 0.09779328\n",
      "Iteration 521, loss = 0.09764390\n",
      "Iteration 522, loss = 0.09750782\n",
      "Iteration 523, loss = 0.09736498\n",
      "Iteration 524, loss = 0.09720842\n",
      "Iteration 525, loss = 0.09706548\n",
      "Iteration 526, loss = 0.09693088\n",
      "Iteration 527, loss = 0.09679572\n",
      "Iteration 528, loss = 0.09664852\n",
      "Iteration 529, loss = 0.09651874\n",
      "Iteration 530, loss = 0.09637853\n",
      "Iteration 531, loss = 0.09624571\n",
      "Iteration 532, loss = 0.09610625\n",
      "Iteration 533, loss = 0.09595787\n",
      "Iteration 534, loss = 0.09581787\n",
      "Iteration 535, loss = 0.09567269\n",
      "Iteration 536, loss = 0.09551908\n",
      "Iteration 537, loss = 0.09539523\n",
      "Iteration 538, loss = 0.09525952\n",
      "Iteration 539, loss = 0.09512133\n",
      "Iteration 540, loss = 0.09498761\n",
      "Iteration 541, loss = 0.09485399\n",
      "Iteration 542, loss = 0.09473542\n",
      "Iteration 543, loss = 0.09459505\n",
      "Iteration 544, loss = 0.09445516\n",
      "Iteration 545, loss = 0.09433001\n",
      "Iteration 546, loss = 0.09420397\n",
      "Iteration 547, loss = 0.09407917\n",
      "Iteration 548, loss = 0.09394136\n",
      "Iteration 549, loss = 0.09381602\n",
      "Iteration 550, loss = 0.09369292\n",
      "Iteration 551, loss = 0.09357108\n",
      "Iteration 552, loss = 0.09343837\n",
      "Iteration 553, loss = 0.09331740\n",
      "Iteration 554, loss = 0.09320162\n",
      "Iteration 555, loss = 0.09307180\n",
      "Iteration 556, loss = 0.09293810\n",
      "Iteration 557, loss = 0.09282077\n",
      "Iteration 558, loss = 0.09270886\n",
      "Iteration 559, loss = 0.09258085\n",
      "Iteration 560, loss = 0.09245037\n",
      "Iteration 561, loss = 0.09233032\n",
      "Iteration 562, loss = 0.09221256\n",
      "Iteration 563, loss = 0.09209464\n",
      "Iteration 564, loss = 0.09198068\n",
      "Iteration 565, loss = 0.09186358\n",
      "Iteration 566, loss = 0.09172047\n",
      "Iteration 567, loss = 0.09158794\n",
      "Iteration 568, loss = 0.09147339\n",
      "Iteration 569, loss = 0.09135204\n",
      "Iteration 570, loss = 0.09123522\n",
      "Iteration 571, loss = 0.09112040\n",
      "Iteration 572, loss = 0.09101471\n",
      "Iteration 573, loss = 0.09088259\n",
      "Iteration 574, loss = 0.09076854\n",
      "Iteration 575, loss = 0.09065264\n",
      "Iteration 576, loss = 0.09055063\n",
      "Iteration 577, loss = 0.09043572\n",
      "Iteration 578, loss = 0.09032931\n",
      "Iteration 579, loss = 0.09020814\n",
      "Iteration 580, loss = 0.09008992\n",
      "Iteration 581, loss = 0.08998495\n",
      "Iteration 582, loss = 0.08987239\n",
      "Iteration 583, loss = 0.08976212\n",
      "Iteration 584, loss = 0.08965245\n",
      "Iteration 585, loss = 0.08954152\n",
      "Iteration 586, loss = 0.08943993\n",
      "Iteration 587, loss = 0.08932289\n",
      "Iteration 588, loss = 0.08921797\n",
      "Iteration 589, loss = 0.08911658\n",
      "Iteration 590, loss = 0.08901430\n",
      "Iteration 591, loss = 0.08890705\n",
      "Iteration 592, loss = 0.08880351\n",
      "Iteration 593, loss = 0.08870035\n",
      "Iteration 594, loss = 0.08859610\n",
      "Iteration 595, loss = 0.08849378\n",
      "Iteration 596, loss = 0.08838414\n",
      "Iteration 597, loss = 0.08828284\n",
      "Iteration 598, loss = 0.08818420\n",
      "Iteration 599, loss = 0.08807767\n",
      "Iteration 600, loss = 0.08798109\n",
      "Iteration 601, loss = 0.08788048\n",
      "Iteration 602, loss = 0.08777540\n",
      "Iteration 603, loss = 0.08766714\n",
      "Iteration 604, loss = 0.08756805\n",
      "Iteration 605, loss = 0.08747980\n",
      "Iteration 606, loss = 0.08738537\n",
      "Iteration 607, loss = 0.08728063\n",
      "Iteration 608, loss = 0.08718418\n",
      "Iteration 609, loss = 0.08708209\n",
      "Iteration 610, loss = 0.08699179\n",
      "Iteration 611, loss = 0.08689031\n",
      "Iteration 612, loss = 0.08679077\n",
      "Iteration 613, loss = 0.08670226\n",
      "Iteration 614, loss = 0.08659768\n",
      "Iteration 615, loss = 0.08651076\n",
      "Iteration 616, loss = 0.08640996\n",
      "Iteration 617, loss = 0.08631288\n",
      "Iteration 618, loss = 0.08622377\n",
      "Iteration 619, loss = 0.08612655\n",
      "Iteration 620, loss = 0.08602893\n",
      "Iteration 621, loss = 0.08592826\n",
      "Iteration 622, loss = 0.08584462\n",
      "Iteration 623, loss = 0.08575278\n",
      "Iteration 624, loss = 0.08564680\n",
      "Iteration 625, loss = 0.08556671\n",
      "Iteration 626, loss = 0.08548108\n",
      "Iteration 627, loss = 0.08537602\n",
      "Iteration 628, loss = 0.08528589\n",
      "Iteration 629, loss = 0.08518502\n",
      "Iteration 630, loss = 0.08508968\n",
      "Iteration 631, loss = 0.08499455\n",
      "Iteration 632, loss = 0.08490368\n",
      "Iteration 633, loss = 0.08481714\n",
      "Iteration 634, loss = 0.08473451\n",
      "Iteration 635, loss = 0.08463485\n",
      "Iteration 636, loss = 0.08454345\n",
      "Iteration 637, loss = 0.08446908\n",
      "Iteration 638, loss = 0.08436153\n",
      "Iteration 639, loss = 0.08428201\n",
      "Iteration 640, loss = 0.08419027\n",
      "Iteration 641, loss = 0.08410869\n",
      "Iteration 642, loss = 0.08402111\n",
      "Iteration 643, loss = 0.08393649\n",
      "Iteration 644, loss = 0.08385594\n",
      "Iteration 645, loss = 0.08376125\n",
      "Iteration 646, loss = 0.08368327\n",
      "Iteration 647, loss = 0.08359038\n",
      "Iteration 648, loss = 0.08351420\n",
      "Iteration 649, loss = 0.08342731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.64651528\n",
      "Iteration 2, loss = 1.93834535\n",
      "Iteration 3, loss = 1.29923682\n",
      "Iteration 4, loss = 0.90581060\n",
      "Iteration 5, loss = 0.63921866\n",
      "Iteration 6, loss = 0.48435029\n",
      "Iteration 7, loss = 0.38232411\n",
      "Iteration 8, loss = 0.29316997\n",
      "Iteration 9, loss = 0.23018119\n",
      "Iteration 10, loss = 0.19611539\n",
      "Iteration 11, loss = 0.18383332\n",
      "Iteration 12, loss = 0.19682026\n",
      "Iteration 13, loss = 0.23011295\n",
      "Iteration 14, loss = 0.29657361\n",
      "Iteration 15, loss = 0.46049904\n",
      "Iteration 16, loss = 0.59309421\n",
      "Iteration 17, loss = 0.63254063\n",
      "Iteration 18, loss = 0.61367928\n",
      "Iteration 19, loss = 0.57229850\n",
      "Iteration 20, loss = 0.46165762\n",
      "Iteration 21, loss = 0.42213894\n",
      "Iteration 22, loss = 0.34831025\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.12448841\n",
      "Iteration 2, loss = 2.07258784\n",
      "Iteration 3, loss = 1.32998470\n",
      "Iteration 4, loss = 0.87583990\n",
      "Iteration 5, loss = 0.64653869\n",
      "Iteration 6, loss = 0.50816062\n",
      "Iteration 7, loss = 0.42290903\n",
      "Iteration 8, loss = 0.37208156\n",
      "Iteration 9, loss = 0.33993381\n",
      "Iteration 10, loss = 0.31590084\n",
      "Iteration 11, loss = 0.28362461\n",
      "Iteration 12, loss = 0.44528988\n",
      "Iteration 13, loss = 0.41269216\n",
      "Iteration 14, loss = 0.43109899\n",
      "Iteration 15, loss = 0.48629438\n",
      "Iteration 16, loss = 0.45325718\n",
      "Iteration 17, loss = 0.44721763\n",
      "Iteration 18, loss = 0.38018353\n",
      "Iteration 19, loss = 0.38318774\n",
      "Iteration 20, loss = 0.36790214\n",
      "Iteration 21, loss = 0.40050726\n",
      "Iteration 22, loss = 0.34436775\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.60780097\n",
      "Iteration 2, loss = 1.97145364\n",
      "Iteration 3, loss = 1.28021608\n",
      "Iteration 4, loss = 0.96558801\n",
      "Iteration 5, loss = 0.73554216\n",
      "Iteration 6, loss = 0.56218034\n",
      "Iteration 7, loss = 0.45917669\n",
      "Iteration 8, loss = 0.38068738\n",
      "Iteration 9, loss = 0.33515358\n",
      "Iteration 10, loss = 0.32847368\n",
      "Iteration 11, loss = 0.32679042\n",
      "Iteration 12, loss = 0.30587065\n",
      "Iteration 13, loss = 0.27288814\n",
      "Iteration 14, loss = 0.23912071\n",
      "Iteration 15, loss = 0.21639441\n",
      "Iteration 16, loss = 0.24349965\n",
      "Iteration 17, loss = 0.28758683\n",
      "Iteration 18, loss = 0.37278867\n",
      "Iteration 19, loss = 0.44950089\n",
      "Iteration 20, loss = 0.49101262\n",
      "Iteration 21, loss = 0.53119859\n",
      "Iteration 22, loss = 0.47934483\n",
      "Iteration 23, loss = 0.45335465\n",
      "Iteration 24, loss = 0.45777024\n",
      "Iteration 25, loss = 0.40189348\n",
      "Iteration 26, loss = 0.39206688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.89924480\n",
      "Iteration 2, loss = 2.33770018\n",
      "Iteration 3, loss = 1.62537525\n",
      "Iteration 4, loss = 1.14251477\n",
      "Iteration 5, loss = 0.79836604\n",
      "Iteration 6, loss = 0.60614269\n",
      "Iteration 7, loss = 0.47000672\n",
      "Iteration 8, loss = 0.36071439\n",
      "Iteration 9, loss = 0.29389681\n",
      "Iteration 10, loss = 0.24075050\n",
      "Iteration 11, loss = 0.20609764\n",
      "Iteration 12, loss = 0.19061133\n",
      "Iteration 13, loss = 0.20526885\n",
      "Iteration 14, loss = 0.22510366\n",
      "Iteration 15, loss = 0.23014198\n",
      "Iteration 16, loss = 0.22570107\n",
      "Iteration 17, loss = 0.23883205\n",
      "Iteration 18, loss = 0.26466683\n",
      "Iteration 19, loss = 0.29410493\n",
      "Iteration 20, loss = 0.36982614\n",
      "Iteration 21, loss = 0.36132976\n",
      "Iteration 22, loss = 0.39020200\n",
      "Iteration 23, loss = 0.48282113\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.53789705\n",
      "Iteration 2, loss = 2.26459395\n",
      "Iteration 3, loss = 1.76870389\n",
      "Iteration 4, loss = 1.35131702\n",
      "Iteration 5, loss = 1.03282715\n",
      "Iteration 6, loss = 0.71234852\n",
      "Iteration 7, loss = 0.50453471\n",
      "Iteration 8, loss = 0.40493692\n",
      "Iteration 9, loss = 0.34204337\n",
      "Iteration 10, loss = 0.28661781\n",
      "Iteration 11, loss = 0.30631397\n",
      "Iteration 12, loss = 0.36965053\n",
      "Iteration 13, loss = 0.40662669\n",
      "Iteration 14, loss = 0.46017007\n",
      "Iteration 15, loss = 0.42166819\n",
      "Iteration 16, loss = 0.39158652\n",
      "Iteration 17, loss = 0.39598063\n",
      "Iteration 18, loss = 0.38526031\n",
      "Iteration 19, loss = 0.36482025\n",
      "Iteration 20, loss = 0.31504002\n",
      "Iteration 21, loss = 0.27260830\n",
      "Iteration 22, loss = 0.24793012\n",
      "Iteration 23, loss = 0.21519425\n",
      "Iteration 24, loss = 0.19364905\n",
      "Iteration 25, loss = 0.16896105\n",
      "Iteration 26, loss = 0.17233000\n",
      "Iteration 27, loss = 0.15285382\n",
      "Iteration 28, loss = 0.14422573\n",
      "Iteration 29, loss = 0.15363558\n",
      "Iteration 30, loss = 0.15827435\n",
      "Iteration 31, loss = 0.16236009\n",
      "Iteration 32, loss = 0.15950172\n",
      "Iteration 33, loss = 0.17471305\n",
      "Iteration 34, loss = 0.18266827\n",
      "Iteration 35, loss = 0.31126630\n",
      "Iteration 36, loss = 0.46954836\n",
      "Iteration 37, loss = 0.56779009\n",
      "Iteration 38, loss = 0.58594890\n",
      "Iteration 39, loss = 0.68016580\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19594124\n",
      "Iteration 2, loss = 1.17524368\n",
      "Iteration 3, loss = 0.46902277\n",
      "Iteration 4, loss = 0.25847202\n",
      "Iteration 5, loss = 0.18136180\n",
      "Iteration 6, loss = 0.13519478\n",
      "Iteration 7, loss = 0.11232162\n",
      "Iteration 8, loss = 0.09578145\n",
      "Iteration 9, loss = 0.08732862\n",
      "Iteration 10, loss = 0.08123207\n",
      "Iteration 11, loss = 0.07804302\n",
      "Iteration 12, loss = 0.07573802\n",
      "Iteration 13, loss = 0.07378882\n",
      "Iteration 14, loss = 0.07270402\n",
      "Iteration 15, loss = 0.07145567\n",
      "Iteration 16, loss = 0.07065080\n",
      "Iteration 17, loss = 0.06919611\n",
      "Iteration 18, loss = 0.06839898\n",
      "Iteration 19, loss = 0.06766982\n",
      "Iteration 20, loss = 0.06710476\n",
      "Iteration 21, loss = 0.06657936\n",
      "Iteration 22, loss = 0.06589388\n",
      "Iteration 23, loss = 0.06542451\n",
      "Iteration 24, loss = 0.06481423\n",
      "Iteration 25, loss = 0.06430619\n",
      "Iteration 26, loss = 0.06386891\n",
      "Iteration 27, loss = 0.06338152\n",
      "Iteration 28, loss = 0.06306925\n",
      "Iteration 29, loss = 0.06257927\n",
      "Iteration 30, loss = 0.06223428\n",
      "Iteration 31, loss = 0.06177835\n",
      "Iteration 32, loss = 0.06141302\n",
      "Iteration 33, loss = 0.06101091\n",
      "Iteration 34, loss = 0.06066866\n",
      "Iteration 35, loss = 0.06035467\n",
      "Iteration 36, loss = 0.05999831\n",
      "Iteration 37, loss = 0.05960734\n",
      "Iteration 38, loss = 0.05931684\n",
      "Iteration 39, loss = 0.05899450\n",
      "Iteration 40, loss = 0.05869476\n",
      "Iteration 41, loss = 0.05838474\n",
      "Iteration 42, loss = 0.05803110\n",
      "Iteration 43, loss = 0.05778855\n",
      "Iteration 44, loss = 0.05747736\n",
      "Iteration 45, loss = 0.05717012\n",
      "Iteration 46, loss = 0.05689417\n",
      "Iteration 47, loss = 0.05662355\n",
      "Iteration 48, loss = 0.05630662\n",
      "Iteration 49, loss = 0.05610602\n",
      "Iteration 50, loss = 0.05578841\n",
      "Iteration 51, loss = 0.05552538\n",
      "Iteration 52, loss = 0.05531053\n",
      "Iteration 53, loss = 0.05505362\n",
      "Iteration 54, loss = 0.05478170\n",
      "Iteration 55, loss = 0.05454526\n",
      "Iteration 56, loss = 0.05432806\n",
      "Iteration 57, loss = 0.05412365\n",
      "Iteration 58, loss = 0.05382566\n",
      "Iteration 59, loss = 0.05361145\n",
      "Iteration 60, loss = 0.05339873\n",
      "Iteration 61, loss = 0.05320721\n",
      "Iteration 62, loss = 0.05295423\n",
      "Iteration 63, loss = 0.05275037\n",
      "Iteration 64, loss = 0.05252540\n",
      "Iteration 65, loss = 0.05234719\n",
      "Iteration 66, loss = 0.05211515\n",
      "Iteration 67, loss = 0.05192201\n",
      "Iteration 68, loss = 0.05173690\n",
      "Iteration 69, loss = 0.05153709\n",
      "Iteration 70, loss = 0.05132797\n",
      "Iteration 71, loss = 0.05114409\n",
      "Iteration 72, loss = 0.05091207\n",
      "Iteration 73, loss = 0.05078955\n",
      "Iteration 74, loss = 0.05055628\n",
      "Iteration 75, loss = 0.05037937\n",
      "Iteration 76, loss = 0.05021488\n",
      "Iteration 77, loss = 0.05004309\n",
      "Iteration 78, loss = 0.04992141\n",
      "Iteration 79, loss = 0.04967876\n",
      "Iteration 80, loss = 0.04955776\n",
      "Iteration 81, loss = 0.04934544\n",
      "Iteration 82, loss = 0.04919792\n",
      "Iteration 83, loss = 0.04902650\n",
      "Iteration 84, loss = 0.04885326\n",
      "Iteration 85, loss = 0.04870761\n",
      "Iteration 86, loss = 0.04853977\n",
      "Iteration 87, loss = 0.04841915\n",
      "Iteration 88, loss = 0.04823832\n",
      "Iteration 89, loss = 0.04809846\n",
      "Iteration 90, loss = 0.04794733\n",
      "Iteration 91, loss = 0.04781432\n",
      "Iteration 92, loss = 0.04769306\n",
      "Iteration 93, loss = 0.04754660\n",
      "Iteration 94, loss = 0.04737217\n",
      "Iteration 95, loss = 0.04727065\n",
      "Iteration 96, loss = 0.04710572\n",
      "Iteration 97, loss = 0.04697855\n",
      "Iteration 98, loss = 0.04684106\n",
      "Iteration 99, loss = 0.04675603\n",
      "Iteration 100, loss = 0.04657239\n",
      "Iteration 101, loss = 0.04645576\n",
      "Iteration 102, loss = 0.04633816\n",
      "Iteration 103, loss = 0.04621108\n",
      "Iteration 104, loss = 0.04607842\n",
      "Iteration 105, loss = 0.04597344\n",
      "Iteration 106, loss = 0.04584102\n",
      "Iteration 107, loss = 0.04571625\n",
      "Iteration 108, loss = 0.04561250\n",
      "Iteration 109, loss = 0.04550136\n",
      "Iteration 110, loss = 0.04538000\n",
      "Iteration 111, loss = 0.04529140\n",
      "Iteration 112, loss = 0.04517922\n",
      "Iteration 113, loss = 0.04506702\n",
      "Iteration 114, loss = 0.04494638\n",
      "Iteration 115, loss = 0.04483223\n",
      "Iteration 116, loss = 0.04472606\n",
      "Iteration 117, loss = 0.04463281\n",
      "Iteration 118, loss = 0.04459736\n",
      "Iteration 119, loss = 0.04445857\n",
      "Iteration 120, loss = 0.04434056\n",
      "Iteration 121, loss = 0.04423774\n",
      "Iteration 122, loss = 0.04415815\n",
      "Iteration 123, loss = 0.04411210\n",
      "Iteration 124, loss = 0.04394473\n",
      "Iteration 125, loss = 0.04386146\n",
      "Iteration 126, loss = 0.04379846\n",
      "Iteration 127, loss = 0.04368020\n",
      "Iteration 128, loss = 0.04358899\n",
      "Iteration 129, loss = 0.04349261\n",
      "Iteration 130, loss = 0.04343114\n",
      "Iteration 131, loss = 0.04332677\n",
      "Iteration 132, loss = 0.04323536\n",
      "Iteration 133, loss = 0.04316752\n",
      "Iteration 134, loss = 0.04311571\n",
      "Iteration 135, loss = 0.04299211\n",
      "Iteration 136, loss = 0.04290981\n",
      "Iteration 137, loss = 0.04286881\n",
      "Iteration 138, loss = 0.04277150\n",
      "Iteration 139, loss = 0.04270155\n",
      "Iteration 140, loss = 0.04262023\n",
      "Iteration 141, loss = 0.04250083\n",
      "Iteration 142, loss = 0.04245343\n",
      "Iteration 143, loss = 0.04236749\n",
      "Iteration 144, loss = 0.04227944\n",
      "Iteration 145, loss = 0.04223293\n",
      "Iteration 146, loss = 0.04216063\n",
      "Iteration 147, loss = 0.04209615\n",
      "Iteration 148, loss = 0.04200732\n",
      "Iteration 149, loss = 0.04194301\n",
      "Iteration 150, loss = 0.04188348\n",
      "Iteration 151, loss = 0.04181597\n",
      "Iteration 152, loss = 0.04176645\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18270185\n",
      "Iteration 2, loss = 1.13323583\n",
      "Iteration 3, loss = 0.44066636\n",
      "Iteration 4, loss = 0.24820437\n",
      "Iteration 5, loss = 0.18874179\n",
      "Iteration 6, loss = 0.13948920\n",
      "Iteration 7, loss = 0.11403706\n",
      "Iteration 8, loss = 0.09830639\n",
      "Iteration 9, loss = 0.08888178\n",
      "Iteration 10, loss = 0.08290253\n",
      "Iteration 11, loss = 0.07836568\n",
      "Iteration 12, loss = 0.07544046\n",
      "Iteration 13, loss = 0.07345898\n",
      "Iteration 14, loss = 0.07231444\n",
      "Iteration 15, loss = 0.07082117\n",
      "Iteration 16, loss = 0.06981204\n",
      "Iteration 17, loss = 0.06843990\n",
      "Iteration 18, loss = 0.06771382\n",
      "Iteration 19, loss = 0.06695286\n",
      "Iteration 20, loss = 0.06637888\n",
      "Iteration 21, loss = 0.06571950\n",
      "Iteration 22, loss = 0.06519249\n",
      "Iteration 23, loss = 0.06470882\n",
      "Iteration 24, loss = 0.06419700\n",
      "Iteration 25, loss = 0.06371507\n",
      "Iteration 26, loss = 0.06325791\n",
      "Iteration 27, loss = 0.06279053\n",
      "Iteration 28, loss = 0.06242419\n",
      "Iteration 29, loss = 0.06192687\n",
      "Iteration 30, loss = 0.06154135\n",
      "Iteration 31, loss = 0.06112315\n",
      "Iteration 32, loss = 0.06077187\n",
      "Iteration 33, loss = 0.06045170\n",
      "Iteration 34, loss = 0.06006287\n",
      "Iteration 35, loss = 0.05976775\n",
      "Iteration 36, loss = 0.05936498\n",
      "Iteration 37, loss = 0.05903821\n",
      "Iteration 38, loss = 0.05875645\n",
      "Iteration 39, loss = 0.05840318\n",
      "Iteration 40, loss = 0.05809500\n",
      "Iteration 41, loss = 0.05777414\n",
      "Iteration 42, loss = 0.05747273\n",
      "Iteration 43, loss = 0.05720745\n",
      "Iteration 44, loss = 0.05688924\n",
      "Iteration 45, loss = 0.05661462\n",
      "Iteration 46, loss = 0.05637876\n",
      "Iteration 47, loss = 0.05606749\n",
      "Iteration 48, loss = 0.05576922\n",
      "Iteration 49, loss = 0.05548340\n",
      "Iteration 50, loss = 0.05519842\n",
      "Iteration 51, loss = 0.05495288\n",
      "Iteration 52, loss = 0.05472943\n",
      "Iteration 53, loss = 0.05446491\n",
      "Iteration 54, loss = 0.05420378\n",
      "Iteration 55, loss = 0.05401339\n",
      "Iteration 56, loss = 0.05373995\n",
      "Iteration 57, loss = 0.05354223\n",
      "Iteration 58, loss = 0.05328486\n",
      "Iteration 59, loss = 0.05302547\n",
      "Iteration 60, loss = 0.05284524\n",
      "Iteration 61, loss = 0.05261223\n",
      "Iteration 62, loss = 0.05237315\n",
      "Iteration 63, loss = 0.05225203\n",
      "Iteration 64, loss = 0.05195969\n",
      "Iteration 65, loss = 0.05177936\n",
      "Iteration 66, loss = 0.05155252\n",
      "Iteration 67, loss = 0.05133426\n",
      "Iteration 68, loss = 0.05122910\n",
      "Iteration 69, loss = 0.05096021\n",
      "Iteration 70, loss = 0.05076720\n",
      "Iteration 71, loss = 0.05055547\n",
      "Iteration 72, loss = 0.05034445\n",
      "Iteration 73, loss = 0.05018102\n",
      "Iteration 74, loss = 0.05001204\n",
      "Iteration 75, loss = 0.04984297\n",
      "Iteration 76, loss = 0.04966172\n",
      "Iteration 77, loss = 0.04945800\n",
      "Iteration 78, loss = 0.04930197\n",
      "Iteration 79, loss = 0.04911920\n",
      "Iteration 80, loss = 0.04897026\n",
      "Iteration 81, loss = 0.04878704\n",
      "Iteration 82, loss = 0.04865410\n",
      "Iteration 83, loss = 0.04846057\n",
      "Iteration 84, loss = 0.04831835\n",
      "Iteration 85, loss = 0.04814549\n",
      "Iteration 86, loss = 0.04802418\n",
      "Iteration 87, loss = 0.04785291\n",
      "Iteration 88, loss = 0.04767757\n",
      "Iteration 89, loss = 0.04753424\n",
      "Iteration 90, loss = 0.04740829\n",
      "Iteration 91, loss = 0.04725640\n",
      "Iteration 92, loss = 0.04710585\n",
      "Iteration 93, loss = 0.04697600\n",
      "Iteration 94, loss = 0.04679924\n",
      "Iteration 95, loss = 0.04667029\n",
      "Iteration 96, loss = 0.04652006\n",
      "Iteration 97, loss = 0.04641173\n",
      "Iteration 98, loss = 0.04628847\n",
      "Iteration 99, loss = 0.04616587\n",
      "Iteration 100, loss = 0.04599790\n",
      "Iteration 101, loss = 0.04587326\n",
      "Iteration 102, loss = 0.04580342\n",
      "Iteration 103, loss = 0.04566402\n",
      "Iteration 104, loss = 0.04552177\n",
      "Iteration 105, loss = 0.04540912\n",
      "Iteration 106, loss = 0.04528350\n",
      "Iteration 107, loss = 0.04514820\n",
      "Iteration 108, loss = 0.04503290\n",
      "Iteration 109, loss = 0.04493097\n",
      "Iteration 110, loss = 0.04481599\n",
      "Iteration 111, loss = 0.04472367\n",
      "Iteration 112, loss = 0.04462906\n",
      "Iteration 113, loss = 0.04447636\n",
      "Iteration 114, loss = 0.04437461\n",
      "Iteration 115, loss = 0.04427024\n",
      "Iteration 116, loss = 0.04416911\n",
      "Iteration 117, loss = 0.04406219\n",
      "Iteration 118, loss = 0.04403331\n",
      "Iteration 119, loss = 0.04390588\n",
      "Iteration 120, loss = 0.04378108\n",
      "Iteration 121, loss = 0.04364994\n",
      "Iteration 122, loss = 0.04357796\n",
      "Iteration 123, loss = 0.04356152\n",
      "Iteration 124, loss = 0.04338266\n",
      "Iteration 125, loss = 0.04329177\n",
      "Iteration 126, loss = 0.04319843\n",
      "Iteration 127, loss = 0.04310626\n",
      "Iteration 128, loss = 0.04304401\n",
      "Iteration 129, loss = 0.04294877\n",
      "Iteration 130, loss = 0.04285823\n",
      "Iteration 131, loss = 0.04276384\n",
      "Iteration 132, loss = 0.04266608\n",
      "Iteration 133, loss = 0.04259198\n",
      "Iteration 134, loss = 0.04252052\n",
      "Iteration 135, loss = 0.04243120\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17421760\n",
      "Iteration 2, loss = 1.10656787\n",
      "Iteration 3, loss = 0.43381853\n",
      "Iteration 4, loss = 0.25512466\n",
      "Iteration 5, loss = 0.17916471\n",
      "Iteration 6, loss = 0.14760627\n",
      "Iteration 7, loss = 0.11247452\n",
      "Iteration 8, loss = 0.09957705\n",
      "Iteration 9, loss = 0.08774623\n",
      "Iteration 10, loss = 0.08238080\n",
      "Iteration 11, loss = 0.07779734\n",
      "Iteration 12, loss = 0.07556442\n",
      "Iteration 13, loss = 0.07337240\n",
      "Iteration 14, loss = 0.07206003\n",
      "Iteration 15, loss = 0.07103377\n",
      "Iteration 16, loss = 0.06990617\n",
      "Iteration 17, loss = 0.06903758\n",
      "Iteration 18, loss = 0.06829461\n",
      "Iteration 19, loss = 0.06763212\n",
      "Iteration 20, loss = 0.06727189\n",
      "Iteration 21, loss = 0.06640576\n",
      "Iteration 22, loss = 0.06582622\n",
      "Iteration 23, loss = 0.06527443\n",
      "Iteration 24, loss = 0.06490905\n",
      "Iteration 25, loss = 0.06437124\n",
      "Iteration 26, loss = 0.06380940\n",
      "Iteration 27, loss = 0.06333121\n",
      "Iteration 28, loss = 0.06300920\n",
      "Iteration 29, loss = 0.06256210\n",
      "Iteration 30, loss = 0.06210851\n",
      "Iteration 31, loss = 0.06180820\n",
      "Iteration 32, loss = 0.06137876\n",
      "Iteration 33, loss = 0.06100662\n",
      "Iteration 34, loss = 0.06066635\n",
      "Iteration 35, loss = 0.06033815\n",
      "Iteration 36, loss = 0.05997761\n",
      "Iteration 37, loss = 0.05964740\n",
      "Iteration 38, loss = 0.05932283\n",
      "Iteration 39, loss = 0.05905996\n",
      "Iteration 40, loss = 0.05871670\n",
      "Iteration 41, loss = 0.05841485\n",
      "Iteration 42, loss = 0.05811359\n",
      "Iteration 43, loss = 0.05787321\n",
      "Iteration 44, loss = 0.05759643\n",
      "Iteration 45, loss = 0.05727223\n",
      "Iteration 46, loss = 0.05712148\n",
      "Iteration 47, loss = 0.05671517\n",
      "Iteration 48, loss = 0.05644012\n",
      "Iteration 49, loss = 0.05618597\n",
      "Iteration 50, loss = 0.05594736\n",
      "Iteration 51, loss = 0.05567720\n",
      "Iteration 52, loss = 0.05538623\n",
      "Iteration 53, loss = 0.05521677\n",
      "Iteration 54, loss = 0.05492751\n",
      "Iteration 55, loss = 0.05474006\n",
      "Iteration 56, loss = 0.05449725\n",
      "Iteration 57, loss = 0.05422440\n",
      "Iteration 58, loss = 0.05396242\n",
      "Iteration 59, loss = 0.05374856\n",
      "Iteration 60, loss = 0.05350089\n",
      "Iteration 61, loss = 0.05332151\n",
      "Iteration 62, loss = 0.05310167\n",
      "Iteration 63, loss = 0.05288906\n",
      "Iteration 64, loss = 0.05269067\n",
      "Iteration 65, loss = 0.05245145\n",
      "Iteration 66, loss = 0.05231286\n",
      "Iteration 67, loss = 0.05208335\n",
      "Iteration 68, loss = 0.05185358\n",
      "Iteration 69, loss = 0.05166758\n",
      "Iteration 70, loss = 0.05150855\n",
      "Iteration 71, loss = 0.05133861\n",
      "Iteration 72, loss = 0.05113954\n",
      "Iteration 73, loss = 0.05101654\n",
      "Iteration 74, loss = 0.05071543\n",
      "Iteration 75, loss = 0.05053419\n",
      "Iteration 76, loss = 0.05040131\n",
      "Iteration 77, loss = 0.05023858\n",
      "Iteration 78, loss = 0.05005226\n",
      "Iteration 79, loss = 0.04985751\n",
      "Iteration 80, loss = 0.04969492\n",
      "Iteration 81, loss = 0.04951616\n",
      "Iteration 82, loss = 0.04935105\n",
      "Iteration 83, loss = 0.04921095\n",
      "Iteration 84, loss = 0.04905651\n",
      "Iteration 85, loss = 0.04891235\n",
      "Iteration 86, loss = 0.04873840\n",
      "Iteration 87, loss = 0.04858847\n",
      "Iteration 88, loss = 0.04843347\n",
      "Iteration 89, loss = 0.04828924\n",
      "Iteration 90, loss = 0.04811340\n",
      "Iteration 91, loss = 0.04798716\n",
      "Iteration 92, loss = 0.04786954\n",
      "Iteration 93, loss = 0.04772092\n",
      "Iteration 94, loss = 0.04755920\n",
      "Iteration 95, loss = 0.04745534\n",
      "Iteration 96, loss = 0.04731039\n",
      "Iteration 97, loss = 0.04716462\n",
      "Iteration 98, loss = 0.04702492\n",
      "Iteration 99, loss = 0.04692502\n",
      "Iteration 100, loss = 0.04676100\n",
      "Iteration 101, loss = 0.04667711\n",
      "Iteration 102, loss = 0.04662016\n",
      "Iteration 103, loss = 0.04641221\n",
      "Iteration 104, loss = 0.04628922\n",
      "Iteration 105, loss = 0.04614945\n",
      "Iteration 106, loss = 0.04603615\n",
      "Iteration 107, loss = 0.04595426\n",
      "Iteration 108, loss = 0.04580870\n",
      "Iteration 109, loss = 0.04567411\n",
      "Iteration 110, loss = 0.04558773\n",
      "Iteration 111, loss = 0.04547035\n",
      "Iteration 112, loss = 0.04536830\n",
      "Iteration 113, loss = 0.04525138\n",
      "Iteration 114, loss = 0.04515286\n",
      "Iteration 115, loss = 0.04504048\n",
      "Iteration 116, loss = 0.04493001\n",
      "Iteration 117, loss = 0.04482957\n",
      "Iteration 118, loss = 0.04474830\n",
      "Iteration 119, loss = 0.04467521\n",
      "Iteration 120, loss = 0.04453058\n",
      "Iteration 121, loss = 0.04442916\n",
      "Iteration 122, loss = 0.04431440\n",
      "Iteration 123, loss = 0.04424502\n",
      "Iteration 124, loss = 0.04416514\n",
      "Iteration 125, loss = 0.04405491\n",
      "Iteration 126, loss = 0.04399499\n",
      "Iteration 127, loss = 0.04386872\n",
      "Iteration 128, loss = 0.04377695\n",
      "Iteration 129, loss = 0.04369064\n",
      "Iteration 130, loss = 0.04360591\n",
      "Iteration 131, loss = 0.04353575\n",
      "Iteration 132, loss = 0.04343059\n",
      "Iteration 133, loss = 0.04335262\n",
      "Iteration 134, loss = 0.04327099\n",
      "Iteration 135, loss = 0.04322070\n",
      "Iteration 136, loss = 0.04311010\n",
      "Iteration 137, loss = 0.04302732\n",
      "Iteration 138, loss = 0.04294920\n",
      "Iteration 139, loss = 0.04287709\n",
      "Iteration 140, loss = 0.04282140\n",
      "Iteration 141, loss = 0.04272765\n",
      "Iteration 142, loss = 0.04267723\n",
      "Iteration 143, loss = 0.04257220\n",
      "Iteration 144, loss = 0.04247399\n",
      "Iteration 145, loss = 0.04243348\n",
      "Iteration 146, loss = 0.04235138\n",
      "Iteration 147, loss = 0.04228367\n",
      "Iteration 148, loss = 0.04219616\n",
      "Iteration 149, loss = 0.04215098\n",
      "Iteration 150, loss = 0.04206230\n",
      "Iteration 151, loss = 0.04200651\n",
      "Iteration 152, loss = 0.04193878\n",
      "Iteration 153, loss = 0.04188336\n",
      "Iteration 154, loss = 0.04179477\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17831047\n",
      "Iteration 2, loss = 1.14021164\n",
      "Iteration 3, loss = 0.46249801\n",
      "Iteration 4, loss = 0.27456667\n",
      "Iteration 5, loss = 0.19197973\n",
      "Iteration 6, loss = 0.15837462\n",
      "Iteration 7, loss = 0.11845459\n",
      "Iteration 8, loss = 0.10156470\n",
      "Iteration 9, loss = 0.09051586\n",
      "Iteration 10, loss = 0.08540939\n",
      "Iteration 11, loss = 0.07985673\n",
      "Iteration 12, loss = 0.07712934\n",
      "Iteration 13, loss = 0.07491658\n",
      "Iteration 14, loss = 0.07338754\n",
      "Iteration 15, loss = 0.07232968\n",
      "Iteration 16, loss = 0.07087975\n",
      "Iteration 17, loss = 0.07013555\n",
      "Iteration 18, loss = 0.06924510\n",
      "Iteration 19, loss = 0.06858544\n",
      "Iteration 20, loss = 0.06811084\n",
      "Iteration 21, loss = 0.06731437\n",
      "Iteration 22, loss = 0.06686814\n",
      "Iteration 23, loss = 0.06631596\n",
      "Iteration 24, loss = 0.06581163\n",
      "Iteration 25, loss = 0.06516874\n",
      "Iteration 26, loss = 0.06474147\n",
      "Iteration 27, loss = 0.06422107\n",
      "Iteration 28, loss = 0.06386849\n",
      "Iteration 29, loss = 0.06345268\n",
      "Iteration 30, loss = 0.06295379\n",
      "Iteration 31, loss = 0.06264486\n",
      "Iteration 32, loss = 0.06225965\n",
      "Iteration 33, loss = 0.06185345\n",
      "Iteration 34, loss = 0.06152232\n",
      "Iteration 35, loss = 0.06117192\n",
      "Iteration 36, loss = 0.06081546\n",
      "Iteration 37, loss = 0.06047376\n",
      "Iteration 38, loss = 0.06017529\n",
      "Iteration 39, loss = 0.05986834\n",
      "Iteration 40, loss = 0.05955896\n",
      "Iteration 41, loss = 0.05923431\n",
      "Iteration 42, loss = 0.05892046\n",
      "Iteration 43, loss = 0.05869733\n",
      "Iteration 44, loss = 0.05839957\n",
      "Iteration 45, loss = 0.05808896\n",
      "Iteration 46, loss = 0.05786175\n",
      "Iteration 47, loss = 0.05753928\n",
      "Iteration 48, loss = 0.05728119\n",
      "Iteration 49, loss = 0.05700831\n",
      "Iteration 50, loss = 0.05672552\n",
      "Iteration 51, loss = 0.05648746\n",
      "Iteration 52, loss = 0.05621340\n",
      "Iteration 53, loss = 0.05598944\n",
      "Iteration 54, loss = 0.05577952\n",
      "Iteration 55, loss = 0.05555411\n",
      "Iteration 56, loss = 0.05528174\n",
      "Iteration 57, loss = 0.05500882\n",
      "Iteration 58, loss = 0.05478163\n",
      "Iteration 59, loss = 0.05455801\n",
      "Iteration 60, loss = 0.05430784\n",
      "Iteration 61, loss = 0.05411389\n",
      "Iteration 62, loss = 0.05388078\n",
      "Iteration 63, loss = 0.05367620\n",
      "Iteration 64, loss = 0.05345707\n",
      "Iteration 65, loss = 0.05325398\n",
      "Iteration 66, loss = 0.05311623\n",
      "Iteration 67, loss = 0.05287750\n",
      "Iteration 68, loss = 0.05264459\n",
      "Iteration 69, loss = 0.05246410\n",
      "Iteration 70, loss = 0.05226035\n",
      "Iteration 71, loss = 0.05210149\n",
      "Iteration 72, loss = 0.05192074\n",
      "Iteration 73, loss = 0.05180728\n",
      "Iteration 74, loss = 0.05151437\n",
      "Iteration 75, loss = 0.05131981\n",
      "Iteration 76, loss = 0.05117036\n",
      "Iteration 77, loss = 0.05100678\n",
      "Iteration 78, loss = 0.05082064\n",
      "Iteration 79, loss = 0.05066475\n",
      "Iteration 80, loss = 0.05046664\n",
      "Iteration 81, loss = 0.05028595\n",
      "Iteration 82, loss = 0.05015436\n",
      "Iteration 83, loss = 0.04998443\n",
      "Iteration 84, loss = 0.04985668\n",
      "Iteration 85, loss = 0.04969743\n",
      "Iteration 86, loss = 0.04948843\n",
      "Iteration 87, loss = 0.04936237\n",
      "Iteration 88, loss = 0.04917006\n",
      "Iteration 89, loss = 0.04904566\n",
      "Iteration 90, loss = 0.04888292\n",
      "Iteration 91, loss = 0.04874463\n",
      "Iteration 92, loss = 0.04862067\n",
      "Iteration 93, loss = 0.04845128\n",
      "Iteration 94, loss = 0.04832650\n",
      "Iteration 95, loss = 0.04820231\n",
      "Iteration 96, loss = 0.04807471\n",
      "Iteration 97, loss = 0.04791290\n",
      "Iteration 98, loss = 0.04778262\n",
      "Iteration 99, loss = 0.04767278\n",
      "Iteration 100, loss = 0.04752348\n",
      "Iteration 101, loss = 0.04740658\n",
      "Iteration 102, loss = 0.04732412\n",
      "Iteration 103, loss = 0.04717340\n",
      "Iteration 104, loss = 0.04702803\n",
      "Iteration 105, loss = 0.04689546\n",
      "Iteration 106, loss = 0.04679829\n",
      "Iteration 107, loss = 0.04668012\n",
      "Iteration 108, loss = 0.04656635\n",
      "Iteration 109, loss = 0.04645652\n",
      "Iteration 110, loss = 0.04634132\n",
      "Iteration 111, loss = 0.04621913\n",
      "Iteration 112, loss = 0.04614579\n",
      "Iteration 113, loss = 0.04605094\n",
      "Iteration 114, loss = 0.04597602\n",
      "Iteration 115, loss = 0.04579194\n",
      "Iteration 116, loss = 0.04570942\n",
      "Iteration 117, loss = 0.04558428\n",
      "Iteration 118, loss = 0.04548132\n",
      "Iteration 119, loss = 0.04542128\n",
      "Iteration 120, loss = 0.04528333\n",
      "Iteration 121, loss = 0.04516536\n",
      "Iteration 122, loss = 0.04507403\n",
      "Iteration 123, loss = 0.04499954\n",
      "Iteration 124, loss = 0.04490258\n",
      "Iteration 125, loss = 0.04479725\n",
      "Iteration 126, loss = 0.04476801\n",
      "Iteration 127, loss = 0.04464271\n",
      "Iteration 128, loss = 0.04452617\n",
      "Iteration 129, loss = 0.04447078\n",
      "Iteration 130, loss = 0.04435992\n",
      "Iteration 131, loss = 0.04430817\n",
      "Iteration 132, loss = 0.04417924\n",
      "Iteration 133, loss = 0.04410848\n",
      "Iteration 134, loss = 0.04405858\n",
      "Iteration 135, loss = 0.04395314\n",
      "Iteration 136, loss = 0.04387685\n",
      "Iteration 137, loss = 0.04377574\n",
      "Iteration 138, loss = 0.04375027\n",
      "Iteration 139, loss = 0.04363030\n",
      "Iteration 140, loss = 0.04357448\n",
      "Iteration 141, loss = 0.04347905\n",
      "Iteration 142, loss = 0.04340200\n",
      "Iteration 143, loss = 0.04334204\n",
      "Iteration 144, loss = 0.04324329\n",
      "Iteration 145, loss = 0.04322982\n",
      "Iteration 146, loss = 0.04315290\n",
      "Iteration 147, loss = 0.04304900\n",
      "Iteration 148, loss = 0.04297465\n",
      "Iteration 149, loss = 0.04289075\n",
      "Iteration 150, loss = 0.04281287\n",
      "Iteration 151, loss = 0.04276684\n",
      "Iteration 152, loss = 0.04268536\n",
      "Iteration 153, loss = 0.04262978\n",
      "Iteration 154, loss = 0.04254067\n",
      "Iteration 155, loss = 0.04249431\n",
      "Iteration 156, loss = 0.04247768\n",
      "Iteration 157, loss = 0.04235549\n",
      "Iteration 158, loss = 0.04232864\n",
      "Iteration 159, loss = 0.04225920\n",
      "Iteration 160, loss = 0.04220416\n",
      "Iteration 161, loss = 0.04212122\n",
      "Iteration 162, loss = 0.04205498\n",
      "Iteration 163, loss = 0.04205014\n",
      "Iteration 164, loss = 0.04196590\n",
      "Iteration 165, loss = 0.04192758\n",
      "Iteration 166, loss = 0.04185812\n",
      "Iteration 167, loss = 0.04179532\n",
      "Iteration 168, loss = 0.04173749\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17039032\n",
      "Iteration 2, loss = 1.08948965\n",
      "Iteration 3, loss = 0.40059988\n",
      "Iteration 4, loss = 0.22344484\n",
      "Iteration 5, loss = 0.15610671\n",
      "Iteration 6, loss = 0.13071930\n",
      "Iteration 7, loss = 0.10098977\n",
      "Iteration 8, loss = 0.09129523\n",
      "Iteration 9, loss = 0.08280521\n",
      "Iteration 10, loss = 0.07833274\n",
      "Iteration 11, loss = 0.07516071\n",
      "Iteration 12, loss = 0.07310205\n",
      "Iteration 13, loss = 0.07167279\n",
      "Iteration 14, loss = 0.07043168\n",
      "Iteration 15, loss = 0.06949116\n",
      "Iteration 16, loss = 0.06828438\n",
      "Iteration 17, loss = 0.06761867\n",
      "Iteration 18, loss = 0.06683727\n",
      "Iteration 19, loss = 0.06622937\n",
      "Iteration 20, loss = 0.06584909\n",
      "Iteration 21, loss = 0.06510407\n",
      "Iteration 22, loss = 0.06489269\n",
      "Iteration 23, loss = 0.06405949\n",
      "Iteration 24, loss = 0.06357096\n",
      "Iteration 25, loss = 0.06303553\n",
      "Iteration 26, loss = 0.06263585\n",
      "Iteration 27, loss = 0.06215677\n",
      "Iteration 28, loss = 0.06179312\n",
      "Iteration 29, loss = 0.06133749\n",
      "Iteration 30, loss = 0.06098486\n",
      "Iteration 31, loss = 0.06061676\n",
      "Iteration 32, loss = 0.06027154\n",
      "Iteration 33, loss = 0.05985189\n",
      "Iteration 34, loss = 0.05956344\n",
      "Iteration 35, loss = 0.05918232\n",
      "Iteration 36, loss = 0.05883533\n",
      "Iteration 37, loss = 0.05853157\n",
      "Iteration 38, loss = 0.05821828\n",
      "Iteration 39, loss = 0.05791091\n",
      "Iteration 40, loss = 0.05757298\n",
      "Iteration 41, loss = 0.05728651\n",
      "Iteration 42, loss = 0.05698723\n",
      "Iteration 43, loss = 0.05667617\n",
      "Iteration 44, loss = 0.05644004\n",
      "Iteration 45, loss = 0.05612313\n",
      "Iteration 46, loss = 0.05588084\n",
      "Iteration 47, loss = 0.05559573\n",
      "Iteration 48, loss = 0.05530396\n",
      "Iteration 49, loss = 0.05505468\n",
      "Iteration 50, loss = 0.05480543\n",
      "Iteration 51, loss = 0.05454931\n",
      "Iteration 52, loss = 0.05427156\n",
      "Iteration 53, loss = 0.05407865\n",
      "Iteration 54, loss = 0.05382323\n",
      "Iteration 55, loss = 0.05355987\n",
      "Iteration 56, loss = 0.05335276\n",
      "Iteration 57, loss = 0.05305304\n",
      "Iteration 58, loss = 0.05282299\n",
      "Iteration 59, loss = 0.05261986\n",
      "Iteration 60, loss = 0.05237657\n",
      "Iteration 61, loss = 0.05214201\n",
      "Iteration 62, loss = 0.05195198\n",
      "Iteration 63, loss = 0.05181722\n",
      "Iteration 64, loss = 0.05151069\n",
      "Iteration 65, loss = 0.05132335\n",
      "Iteration 66, loss = 0.05112495\n",
      "Iteration 67, loss = 0.05093771\n",
      "Iteration 68, loss = 0.05071262\n",
      "Iteration 69, loss = 0.05053496\n",
      "Iteration 70, loss = 0.05030897\n",
      "Iteration 71, loss = 0.05013537\n",
      "Iteration 72, loss = 0.04998508\n",
      "Iteration 73, loss = 0.04981976\n",
      "Iteration 74, loss = 0.04956943\n",
      "Iteration 75, loss = 0.04940212\n",
      "Iteration 76, loss = 0.04922871\n",
      "Iteration 77, loss = 0.04912736\n",
      "Iteration 78, loss = 0.04884785\n",
      "Iteration 79, loss = 0.04872607\n",
      "Iteration 80, loss = 0.04854944\n",
      "Iteration 81, loss = 0.04834109\n",
      "Iteration 82, loss = 0.04821709\n",
      "Iteration 83, loss = 0.04802620\n",
      "Iteration 84, loss = 0.04790053\n",
      "Iteration 85, loss = 0.04773061\n",
      "Iteration 86, loss = 0.04757478\n",
      "Iteration 87, loss = 0.04740851\n",
      "Iteration 88, loss = 0.04724477\n",
      "Iteration 89, loss = 0.04709220\n",
      "Iteration 90, loss = 0.04694690\n",
      "Iteration 91, loss = 0.04680017\n",
      "Iteration 92, loss = 0.04665460\n",
      "Iteration 93, loss = 0.04652813\n",
      "Iteration 94, loss = 0.04637907\n",
      "Iteration 95, loss = 0.04626016\n",
      "Iteration 96, loss = 0.04612468\n",
      "Iteration 97, loss = 0.04598253\n",
      "Iteration 98, loss = 0.04585025\n",
      "Iteration 99, loss = 0.04571974\n",
      "Iteration 100, loss = 0.04558764\n",
      "Iteration 101, loss = 0.04545800\n",
      "Iteration 102, loss = 0.04532238\n",
      "Iteration 103, loss = 0.04520043\n",
      "Iteration 104, loss = 0.04508503\n",
      "Iteration 105, loss = 0.04496557\n",
      "Iteration 106, loss = 0.04485691\n",
      "Iteration 107, loss = 0.04474695\n",
      "Iteration 108, loss = 0.04460788\n",
      "Iteration 109, loss = 0.04451384\n",
      "Iteration 110, loss = 0.04438299\n",
      "Iteration 111, loss = 0.04427317\n",
      "Iteration 112, loss = 0.04416415\n",
      "Iteration 113, loss = 0.04410263\n",
      "Iteration 114, loss = 0.04399573\n",
      "Iteration 115, loss = 0.04387645\n",
      "Iteration 116, loss = 0.04375128\n",
      "Iteration 117, loss = 0.04365611\n",
      "Iteration 118, loss = 0.04353306\n",
      "Iteration 119, loss = 0.04345858\n",
      "Iteration 120, loss = 0.04332284\n",
      "Iteration 121, loss = 0.04321304\n",
      "Iteration 122, loss = 0.04312416\n",
      "Iteration 123, loss = 0.04303670\n",
      "Iteration 124, loss = 0.04296591\n",
      "Iteration 125, loss = 0.04287312\n",
      "Iteration 126, loss = 0.04280646\n",
      "Iteration 127, loss = 0.04269046\n",
      "Iteration 128, loss = 0.04258511\n",
      "Iteration 129, loss = 0.04251148\n",
      "Iteration 130, loss = 0.04240490\n",
      "Iteration 131, loss = 0.04235056\n",
      "Iteration 132, loss = 0.04223816\n",
      "Iteration 133, loss = 0.04215702\n",
      "Iteration 134, loss = 0.04206733\n",
      "Iteration 135, loss = 0.04199090\n",
      "Iteration 136, loss = 0.04193402\n",
      "Iteration 137, loss = 0.04182495\n",
      "Iteration 138, loss = 0.04177738\n",
      "Iteration 139, loss = 0.04168498\n",
      "Iteration 140, loss = 0.04163734\n",
      "Iteration 141, loss = 0.04153333\n",
      "Iteration 142, loss = 0.04146210\n",
      "Iteration 143, loss = 0.04136693\n",
      "Iteration 144, loss = 0.04130653\n",
      "Iteration 145, loss = 0.04124704\n",
      "Iteration 146, loss = 0.04116954\n",
      "Iteration 147, loss = 0.04107838\n",
      "Iteration 148, loss = 0.04099458\n",
      "Iteration 149, loss = 0.04093651\n",
      "Iteration 150, loss = 0.04086319\n",
      "Iteration 151, loss = 0.04079973\n",
      "Iteration 152, loss = 0.04071876\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.77687624\n",
      "Iteration 2, loss = 0.42944624\n",
      "Iteration 3, loss = 0.17927238\n",
      "Iteration 4, loss = 0.12696805\n",
      "Iteration 5, loss = 0.10898843\n",
      "Iteration 6, loss = 0.09770309\n",
      "Iteration 7, loss = 0.08913951\n",
      "Iteration 8, loss = 0.08125473\n",
      "Iteration 9, loss = 0.07419309\n",
      "Iteration 10, loss = 0.06849303\n",
      "Iteration 11, loss = 0.06406864\n",
      "Iteration 12, loss = 0.06058485\n",
      "Iteration 13, loss = 0.05787246\n",
      "Iteration 14, loss = 0.05556543\n",
      "Iteration 15, loss = 0.05366050\n",
      "Iteration 16, loss = 0.05230811\n",
      "Iteration 17, loss = 0.05023753\n",
      "Iteration 18, loss = 0.04874340\n",
      "Iteration 19, loss = 0.04729256\n",
      "Iteration 20, loss = 0.04657217\n",
      "Iteration 21, loss = 0.04636646\n",
      "Iteration 22, loss = 0.04569697\n",
      "Iteration 23, loss = 0.04451029\n",
      "Iteration 24, loss = 0.04466671\n",
      "Iteration 25, loss = 0.04416620\n",
      "Iteration 26, loss = 0.04339976\n",
      "Iteration 27, loss = 0.04288423\n",
      "Iteration 28, loss = 0.04236299\n",
      "Iteration 29, loss = 0.04204274\n",
      "Iteration 30, loss = 0.04121419\n",
      "Iteration 31, loss = 0.04132513\n",
      "Iteration 32, loss = 0.04153162\n",
      "Iteration 33, loss = 0.04093395\n",
      "Iteration 34, loss = 0.04046627\n",
      "Iteration 35, loss = 0.04033250\n",
      "Iteration 36, loss = 0.04020487\n",
      "Iteration 37, loss = 0.04024942\n",
      "Iteration 38, loss = 0.04004812\n",
      "Iteration 39, loss = 0.03975976\n",
      "Iteration 40, loss = 0.03952657\n",
      "Iteration 41, loss = 0.03993655\n",
      "Iteration 42, loss = 0.04024307\n",
      "Iteration 43, loss = 0.04042435\n",
      "Iteration 44, loss = 0.04021916\n",
      "Iteration 45, loss = 0.04039570\n",
      "Iteration 46, loss = 0.04056489\n",
      "Iteration 47, loss = 0.04072708\n",
      "Iteration 48, loss = 0.04151151\n",
      "Iteration 49, loss = 0.04093627\n",
      "Iteration 50, loss = 0.04092780\n",
      "Iteration 51, loss = 0.04074835\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.72743726\n",
      "Iteration 2, loss = 0.40519860\n",
      "Iteration 3, loss = 0.17646482\n",
      "Iteration 4, loss = 0.12022570\n",
      "Iteration 5, loss = 0.10195170\n",
      "Iteration 6, loss = 0.09384763\n",
      "Iteration 7, loss = 0.08572993\n",
      "Iteration 8, loss = 0.07818086\n",
      "Iteration 9, loss = 0.07173854\n",
      "Iteration 10, loss = 0.06630603\n",
      "Iteration 11, loss = 0.06219419\n",
      "Iteration 12, loss = 0.05882212\n",
      "Iteration 13, loss = 0.05654305\n",
      "Iteration 14, loss = 0.05426154\n",
      "Iteration 15, loss = 0.05233812\n",
      "Iteration 16, loss = 0.05057073\n",
      "Iteration 17, loss = 0.04878283\n",
      "Iteration 18, loss = 0.04708528\n",
      "Iteration 19, loss = 0.04628499\n",
      "Iteration 20, loss = 0.04533496\n",
      "Iteration 21, loss = 0.04452334\n",
      "Iteration 22, loss = 0.04417643\n",
      "Iteration 23, loss = 0.04379722\n",
      "Iteration 24, loss = 0.04354082\n",
      "Iteration 25, loss = 0.04291783\n",
      "Iteration 26, loss = 0.04244124\n",
      "Iteration 27, loss = 0.04196631\n",
      "Iteration 28, loss = 0.04136907\n",
      "Iteration 29, loss = 0.04074230\n",
      "Iteration 30, loss = 0.04056786\n",
      "Iteration 31, loss = 0.04027207\n",
      "Iteration 32, loss = 0.03987221\n",
      "Iteration 33, loss = 0.04029096\n",
      "Iteration 34, loss = 0.03996447\n",
      "Iteration 35, loss = 0.03984330\n",
      "Iteration 36, loss = 0.03941318\n",
      "Iteration 37, loss = 0.03926068\n",
      "Iteration 38, loss = 0.03952303\n",
      "Iteration 39, loss = 0.03920116\n",
      "Iteration 40, loss = 0.03942057\n",
      "Iteration 41, loss = 0.03959214\n",
      "Iteration 42, loss = 0.03958671\n",
      "Iteration 43, loss = 0.03981723\n",
      "Iteration 44, loss = 0.03984660\n",
      "Iteration 45, loss = 0.03982531\n",
      "Iteration 46, loss = 0.04248257\n",
      "Iteration 47, loss = 0.04343571\n",
      "Iteration 48, loss = 0.04394042\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.67035208\n",
      "Iteration 2, loss = 0.35050367\n",
      "Iteration 3, loss = 0.16703982\n",
      "Iteration 4, loss = 0.12031723\n",
      "Iteration 5, loss = 0.10587223\n",
      "Iteration 6, loss = 0.09746705\n",
      "Iteration 7, loss = 0.08930834\n",
      "Iteration 8, loss = 0.08097606\n",
      "Iteration 9, loss = 0.07427469\n",
      "Iteration 10, loss = 0.06856553\n",
      "Iteration 11, loss = 0.06393875\n",
      "Iteration 12, loss = 0.06043008\n",
      "Iteration 13, loss = 0.05726787\n",
      "Iteration 14, loss = 0.05485689\n",
      "Iteration 15, loss = 0.05302866\n",
      "Iteration 16, loss = 0.05125872\n",
      "Iteration 17, loss = 0.04973378\n",
      "Iteration 18, loss = 0.04842712\n",
      "Iteration 19, loss = 0.04727147\n",
      "Iteration 20, loss = 0.04699073\n",
      "Iteration 21, loss = 0.04613158\n",
      "Iteration 22, loss = 0.04539031\n",
      "Iteration 23, loss = 0.04465696\n",
      "Iteration 24, loss = 0.04397531\n",
      "Iteration 25, loss = 0.04361979\n",
      "Iteration 26, loss = 0.04299159\n",
      "Iteration 27, loss = 0.04278392\n",
      "Iteration 28, loss = 0.04228375\n",
      "Iteration 29, loss = 0.04217627\n",
      "Iteration 30, loss = 0.04179166\n",
      "Iteration 31, loss = 0.04128193\n",
      "Iteration 32, loss = 0.04101163\n",
      "Iteration 33, loss = 0.04096563\n",
      "Iteration 34, loss = 0.04119198\n",
      "Iteration 35, loss = 0.04080359\n",
      "Iteration 36, loss = 0.04098690\n",
      "Iteration 37, loss = 0.04078051\n",
      "Iteration 38, loss = 0.04003151\n",
      "Iteration 39, loss = 0.03992089\n",
      "Iteration 40, loss = 0.03981604\n",
      "Iteration 41, loss = 0.03933030\n",
      "Iteration 42, loss = 0.03940113\n",
      "Iteration 43, loss = 0.03965462\n",
      "Iteration 44, loss = 0.04060181\n",
      "Iteration 45, loss = 0.04109028\n",
      "Iteration 46, loss = 0.04200193\n",
      "Iteration 47, loss = 0.04136249\n",
      "Iteration 48, loss = 0.04072833\n",
      "Iteration 49, loss = 0.04067846\n",
      "Iteration 50, loss = 0.04079770\n",
      "Iteration 51, loss = 0.04042468\n",
      "Iteration 52, loss = 0.04015685\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65042157\n",
      "Iteration 2, loss = 0.35787819\n",
      "Iteration 3, loss = 0.17625309\n",
      "Iteration 4, loss = 0.12681669\n",
      "Iteration 5, loss = 0.10812676\n",
      "Iteration 6, loss = 0.09962493\n",
      "Iteration 7, loss = 0.09107874\n",
      "Iteration 8, loss = 0.08265115\n",
      "Iteration 9, loss = 0.07560555\n",
      "Iteration 10, loss = 0.06979882\n",
      "Iteration 11, loss = 0.06531947\n",
      "Iteration 12, loss = 0.06147727\n",
      "Iteration 13, loss = 0.05875039\n",
      "Iteration 14, loss = 0.05624332\n",
      "Iteration 15, loss = 0.05439971\n",
      "Iteration 16, loss = 0.05251770\n",
      "Iteration 17, loss = 0.05105982\n",
      "Iteration 18, loss = 0.04951400\n",
      "Iteration 19, loss = 0.04856490\n",
      "Iteration 20, loss = 0.04832057\n",
      "Iteration 21, loss = 0.04720217\n",
      "Iteration 22, loss = 0.04634909\n",
      "Iteration 23, loss = 0.04587031\n",
      "Iteration 24, loss = 0.04544244\n",
      "Iteration 25, loss = 0.04485299\n",
      "Iteration 26, loss = 0.04466374\n",
      "Iteration 27, loss = 0.04378010\n",
      "Iteration 28, loss = 0.04301096\n",
      "Iteration 29, loss = 0.04275031\n",
      "Iteration 30, loss = 0.04287516\n",
      "Iteration 31, loss = 0.04199324\n",
      "Iteration 32, loss = 0.04184374\n",
      "Iteration 33, loss = 0.04173363\n",
      "Iteration 34, loss = 0.04184774\n",
      "Iteration 35, loss = 0.04134530\n",
      "Iteration 36, loss = 0.04170921\n",
      "Iteration 37, loss = 0.04150859\n",
      "Iteration 38, loss = 0.04103713\n",
      "Iteration 39, loss = 0.04106182\n",
      "Iteration 40, loss = 0.04082216\n",
      "Iteration 41, loss = 0.04058889\n",
      "Iteration 42, loss = 0.04035184\n",
      "Iteration 43, loss = 0.04098402\n",
      "Iteration 44, loss = 0.04188964\n",
      "Iteration 45, loss = 0.04151213\n",
      "Iteration 46, loss = 0.04212054\n",
      "Iteration 47, loss = 0.04164475\n",
      "Iteration 48, loss = 0.04098447\n",
      "Iteration 49, loss = 0.04074961\n",
      "Iteration 50, loss = 0.04051070\n",
      "Iteration 51, loss = 0.04030320\n",
      "Iteration 52, loss = 0.04045972\n",
      "Iteration 53, loss = 0.04014802\n",
      "Iteration 54, loss = 0.04052649\n",
      "Iteration 55, loss = 0.04089450\n",
      "Iteration 56, loss = 0.04104153\n",
      "Iteration 57, loss = 0.04165643\n",
      "Iteration 58, loss = 0.04200097\n",
      "Iteration 59, loss = 0.04227997\n",
      "Iteration 60, loss = 0.04086552\n",
      "Iteration 61, loss = 0.04039527\n",
      "Iteration 62, loss = 0.04033835\n",
      "Iteration 63, loss = 0.03998581\n",
      "Iteration 64, loss = 0.04078364\n",
      "Iteration 65, loss = 0.04169778\n",
      "Iteration 66, loss = 0.04160033\n",
      "Iteration 67, loss = 0.04194767\n",
      "Iteration 68, loss = 0.04178796\n",
      "Iteration 69, loss = 0.04147385\n",
      "Iteration 70, loss = 0.04140987\n",
      "Iteration 71, loss = 0.04087241\n",
      "Iteration 72, loss = 0.04228196\n",
      "Iteration 73, loss = 0.04350263\n",
      "Iteration 74, loss = 0.04373655\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.60245101\n",
      "Iteration 2, loss = 0.31821903\n",
      "Iteration 3, loss = 0.16053166\n",
      "Iteration 4, loss = 0.12583684\n",
      "Iteration 5, loss = 0.10747163\n",
      "Iteration 6, loss = 0.10045592\n",
      "Iteration 7, loss = 0.09083749\n",
      "Iteration 8, loss = 0.08299443\n",
      "Iteration 9, loss = 0.07423606\n",
      "Iteration 10, loss = 0.06824830\n",
      "Iteration 11, loss = 0.06309411\n",
      "Iteration 12, loss = 0.05896507\n",
      "Iteration 13, loss = 0.05582292\n",
      "Iteration 14, loss = 0.05348270\n",
      "Iteration 15, loss = 0.05156342\n",
      "Iteration 16, loss = 0.04989975\n",
      "Iteration 17, loss = 0.04819030\n",
      "Iteration 18, loss = 0.04688567\n",
      "Iteration 19, loss = 0.04605023\n",
      "Iteration 20, loss = 0.04559071\n",
      "Iteration 21, loss = 0.04459933\n",
      "Iteration 22, loss = 0.04386890\n",
      "Iteration 23, loss = 0.04345928\n",
      "Iteration 24, loss = 0.04257151\n",
      "Iteration 25, loss = 0.04205993\n",
      "Iteration 26, loss = 0.04178520\n",
      "Iteration 27, loss = 0.04113021\n",
      "Iteration 28, loss = 0.04075521\n",
      "Iteration 29, loss = 0.04032193\n",
      "Iteration 30, loss = 0.03995599\n",
      "Iteration 31, loss = 0.03971626\n",
      "Iteration 32, loss = 0.03974571\n",
      "Iteration 33, loss = 0.03970123\n",
      "Iteration 34, loss = 0.03954512\n",
      "Iteration 35, loss = 0.03915226\n",
      "Iteration 36, loss = 0.03945850\n",
      "Iteration 37, loss = 0.03911803\n",
      "Iteration 38, loss = 0.03896205\n",
      "Iteration 39, loss = 0.03891723\n",
      "Iteration 40, loss = 0.03906526\n",
      "Iteration 41, loss = 0.03910697\n",
      "Iteration 42, loss = 0.03896549\n",
      "Iteration 43, loss = 0.03871010\n",
      "Iteration 44, loss = 0.03895979\n",
      "Iteration 45, loss = 0.03866398\n",
      "Iteration 46, loss = 0.03853033\n",
      "Iteration 47, loss = 0.03841816\n",
      "Iteration 48, loss = 0.03840455\n",
      "Iteration 49, loss = 0.03823189\n",
      "Iteration 50, loss = 0.03858068\n",
      "Iteration 51, loss = 0.03769774\n",
      "Iteration 52, loss = 0.03761223\n",
      "Iteration 53, loss = 0.03778117\n",
      "Iteration 54, loss = 0.03802045\n",
      "Iteration 55, loss = 0.03867575\n",
      "Iteration 56, loss = 0.03956541\n",
      "Iteration 57, loss = 0.04271222\n",
      "Iteration 58, loss = 0.04686249\n",
      "Iteration 59, loss = 0.04763028\n",
      "Iteration 60, loss = 0.05625348\n",
      "Iteration 61, loss = 0.05999036\n",
      "Iteration 62, loss = 0.06139640\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39258382\n",
      "Iteration 2, loss = 2.23584755\n",
      "Iteration 3, loss = 2.09562637\n",
      "Iteration 4, loss = 1.90948488\n",
      "Iteration 5, loss = 1.68278705\n",
      "Iteration 6, loss = 1.44912102\n",
      "Iteration 7, loss = 1.22970912\n",
      "Iteration 8, loss = 1.04265348\n",
      "Iteration 9, loss = 0.88545989\n",
      "Iteration 10, loss = 0.75611789\n",
      "Iteration 11, loss = 0.66244153\n",
      "Iteration 12, loss = 0.58288884\n",
      "Iteration 13, loss = 0.52199816\n",
      "Iteration 14, loss = 0.47682976\n",
      "Iteration 15, loss = 0.43516522\n",
      "Iteration 16, loss = 0.40082268\n",
      "Iteration 17, loss = 0.37294321\n",
      "Iteration 18, loss = 0.34712963\n",
      "Iteration 19, loss = 0.32610694\n",
      "Iteration 20, loss = 0.30803227\n",
      "Iteration 21, loss = 0.29256504\n",
      "Iteration 22, loss = 0.27623666\n",
      "Iteration 23, loss = 0.26337192\n",
      "Iteration 24, loss = 0.25037219\n",
      "Iteration 25, loss = 0.23964690\n",
      "Iteration 26, loss = 0.22975608\n",
      "Iteration 27, loss = 0.22024091\n",
      "Iteration 28, loss = 0.21305280\n",
      "Iteration 29, loss = 0.20478706\n",
      "Iteration 30, loss = 0.19794630\n",
      "Iteration 31, loss = 0.19086686\n",
      "Iteration 32, loss = 0.18486968\n",
      "Iteration 33, loss = 0.17949086\n",
      "Iteration 34, loss = 0.17378755\n",
      "Iteration 35, loss = 0.16897994\n",
      "Iteration 36, loss = 0.16402576\n",
      "Iteration 37, loss = 0.15939735\n",
      "Iteration 38, loss = 0.15565346\n",
      "Iteration 39, loss = 0.15147708\n",
      "Iteration 40, loss = 0.14807659\n",
      "Iteration 41, loss = 0.14456638\n",
      "Iteration 42, loss = 0.14137913\n",
      "Iteration 43, loss = 0.13831648\n",
      "Iteration 44, loss = 0.13532830\n",
      "Iteration 45, loss = 0.13253784\n",
      "Iteration 46, loss = 0.13005011\n",
      "Iteration 47, loss = 0.12743017\n",
      "Iteration 48, loss = 0.12496444\n",
      "Iteration 49, loss = 0.12288197\n",
      "Iteration 50, loss = 0.12071791\n",
      "Iteration 51, loss = 0.11871203\n",
      "Iteration 52, loss = 0.11672505\n",
      "Iteration 53, loss = 0.11510181\n",
      "Iteration 54, loss = 0.11315825\n",
      "Iteration 55, loss = 0.11156871\n",
      "Iteration 56, loss = 0.11004158\n",
      "Iteration 57, loss = 0.10887385\n",
      "Iteration 58, loss = 0.10715842\n",
      "Iteration 59, loss = 0.10562092\n",
      "Iteration 60, loss = 0.10440972\n",
      "Iteration 61, loss = 0.10318611\n",
      "Iteration 62, loss = 0.10183108\n",
      "Iteration 63, loss = 0.10078324\n",
      "Iteration 64, loss = 0.09966459\n",
      "Iteration 65, loss = 0.09867435\n",
      "Iteration 66, loss = 0.09765323\n",
      "Iteration 67, loss = 0.09664972\n",
      "Iteration 68, loss = 0.09577366\n",
      "Iteration 69, loss = 0.09488337\n",
      "Iteration 70, loss = 0.09398127\n",
      "Iteration 71, loss = 0.09317317\n",
      "Iteration 72, loss = 0.09229357\n",
      "Iteration 73, loss = 0.09164173\n",
      "Iteration 74, loss = 0.09073419\n",
      "Iteration 75, loss = 0.09009438\n",
      "Iteration 76, loss = 0.08946632\n",
      "Iteration 77, loss = 0.08877173\n",
      "Iteration 78, loss = 0.08812092\n",
      "Iteration 79, loss = 0.08748183\n",
      "Iteration 80, loss = 0.08689957\n",
      "Iteration 81, loss = 0.08639732\n",
      "Iteration 82, loss = 0.08577730\n",
      "Iteration 83, loss = 0.08528854\n",
      "Iteration 84, loss = 0.08476266\n",
      "Iteration 85, loss = 0.08427089\n",
      "Iteration 86, loss = 0.08378497\n",
      "Iteration 87, loss = 0.08333608\n",
      "Iteration 88, loss = 0.08288399\n",
      "Iteration 89, loss = 0.08248035\n",
      "Iteration 90, loss = 0.08204267\n",
      "Iteration 91, loss = 0.08162090\n",
      "Iteration 92, loss = 0.08128014\n",
      "Iteration 93, loss = 0.08085048\n",
      "Iteration 94, loss = 0.08046487\n",
      "Iteration 95, loss = 0.08010604\n",
      "Iteration 96, loss = 0.07972079\n",
      "Iteration 97, loss = 0.07940740\n",
      "Iteration 98, loss = 0.07908001\n",
      "Iteration 99, loss = 0.07879922\n",
      "Iteration 100, loss = 0.07841800\n",
      "Iteration 101, loss = 0.07809280\n",
      "Iteration 102, loss = 0.07784178\n",
      "Iteration 103, loss = 0.07751859\n",
      "Iteration 104, loss = 0.07724898\n",
      "Iteration 105, loss = 0.07695986\n",
      "Iteration 106, loss = 0.07668362\n",
      "Iteration 107, loss = 0.07641883\n",
      "Iteration 108, loss = 0.07619832\n",
      "Iteration 109, loss = 0.07597328\n",
      "Iteration 110, loss = 0.07569781\n",
      "Iteration 111, loss = 0.07548913\n",
      "Iteration 112, loss = 0.07526507\n",
      "Iteration 113, loss = 0.07504126\n",
      "Iteration 114, loss = 0.07478463\n",
      "Iteration 115, loss = 0.07455940\n",
      "Iteration 116, loss = 0.07435302\n",
      "Iteration 117, loss = 0.07414803\n",
      "Iteration 118, loss = 0.07398782\n",
      "Iteration 119, loss = 0.07378580\n",
      "Iteration 120, loss = 0.07357959\n",
      "Iteration 121, loss = 0.07340643\n",
      "Iteration 122, loss = 0.07318207\n",
      "Iteration 123, loss = 0.07305789\n",
      "Iteration 124, loss = 0.07287974\n",
      "Iteration 125, loss = 0.07267705\n",
      "Iteration 126, loss = 0.07250103\n",
      "Iteration 127, loss = 0.07233127\n",
      "Iteration 128, loss = 0.07215364\n",
      "Iteration 129, loss = 0.07198655\n",
      "Iteration 130, loss = 0.07183995\n",
      "Iteration 131, loss = 0.07168651\n",
      "Iteration 132, loss = 0.07152904\n",
      "Iteration 133, loss = 0.07137440\n",
      "Iteration 134, loss = 0.07124846\n",
      "Iteration 135, loss = 0.07108979\n",
      "Iteration 136, loss = 0.07094754\n",
      "Iteration 137, loss = 0.07081591\n",
      "Iteration 138, loss = 0.07066601\n",
      "Iteration 139, loss = 0.07050372\n",
      "Iteration 140, loss = 0.07040622\n",
      "Iteration 141, loss = 0.07022121\n",
      "Iteration 142, loss = 0.07012427\n",
      "Iteration 143, loss = 0.06998220\n",
      "Iteration 144, loss = 0.06984102\n",
      "Iteration 145, loss = 0.06971217\n",
      "Iteration 146, loss = 0.06961259\n",
      "Iteration 147, loss = 0.06948761\n",
      "Iteration 148, loss = 0.06936108\n",
      "Iteration 149, loss = 0.06925435\n",
      "Iteration 150, loss = 0.06914547\n",
      "Iteration 151, loss = 0.06902216\n",
      "Iteration 152, loss = 0.06891463\n",
      "Iteration 153, loss = 0.06880523\n",
      "Iteration 154, loss = 0.06868885\n",
      "Iteration 155, loss = 0.06859034\n",
      "Iteration 156, loss = 0.06848757\n",
      "Iteration 157, loss = 0.06838484\n",
      "Iteration 158, loss = 0.06827112\n",
      "Iteration 159, loss = 0.06821405\n",
      "Iteration 160, loss = 0.06806863\n",
      "Iteration 161, loss = 0.06796859\n",
      "Iteration 162, loss = 0.06788424\n",
      "Iteration 163, loss = 0.06780208\n",
      "Iteration 164, loss = 0.06769019\n",
      "Iteration 165, loss = 0.06757414\n",
      "Iteration 166, loss = 0.06749215\n",
      "Iteration 167, loss = 0.06741488\n",
      "Iteration 168, loss = 0.06731033\n",
      "Iteration 169, loss = 0.06722919\n",
      "Iteration 170, loss = 0.06714428\n",
      "Iteration 171, loss = 0.06705599\n",
      "Iteration 172, loss = 0.06697920\n",
      "Iteration 173, loss = 0.06689144\n",
      "Iteration 174, loss = 0.06679409\n",
      "Iteration 175, loss = 0.06671680\n",
      "Iteration 176, loss = 0.06663515\n",
      "Iteration 177, loss = 0.06655956\n",
      "Iteration 178, loss = 0.06649934\n",
      "Iteration 179, loss = 0.06640113\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39133892\n",
      "Iteration 2, loss = 2.23165487\n",
      "Iteration 3, loss = 2.08485871\n",
      "Iteration 4, loss = 1.89086835\n",
      "Iteration 5, loss = 1.65939381\n",
      "Iteration 6, loss = 1.41758827\n",
      "Iteration 7, loss = 1.19610265\n",
      "Iteration 8, loss = 1.00725935\n",
      "Iteration 9, loss = 0.85365406\n",
      "Iteration 10, loss = 0.72680215\n",
      "Iteration 11, loss = 0.63350928\n",
      "Iteration 12, loss = 0.55914051\n",
      "Iteration 13, loss = 0.50105003\n",
      "Iteration 14, loss = 0.45808834\n",
      "Iteration 15, loss = 0.41676428\n",
      "Iteration 16, loss = 0.38434536\n",
      "Iteration 17, loss = 0.35726844\n",
      "Iteration 18, loss = 0.33392006\n",
      "Iteration 19, loss = 0.31338001\n",
      "Iteration 20, loss = 0.29579896\n",
      "Iteration 21, loss = 0.27957878\n",
      "Iteration 22, loss = 0.26534712\n",
      "Iteration 23, loss = 0.25329865\n",
      "Iteration 24, loss = 0.24193674\n",
      "Iteration 25, loss = 0.23205995\n",
      "Iteration 26, loss = 0.22249250\n",
      "Iteration 27, loss = 0.21389091\n",
      "Iteration 28, loss = 0.20619666\n",
      "Iteration 29, loss = 0.19916388\n",
      "Iteration 30, loss = 0.19236208\n",
      "Iteration 31, loss = 0.18618043\n",
      "Iteration 32, loss = 0.18068325\n",
      "Iteration 33, loss = 0.17545398\n",
      "Iteration 34, loss = 0.17046877\n",
      "Iteration 35, loss = 0.16586457\n",
      "Iteration 36, loss = 0.16147282\n",
      "Iteration 37, loss = 0.15722044\n",
      "Iteration 38, loss = 0.15370168\n",
      "Iteration 39, loss = 0.14982183\n",
      "Iteration 40, loss = 0.14626468\n",
      "Iteration 41, loss = 0.14283044\n",
      "Iteration 42, loss = 0.14015398\n",
      "Iteration 43, loss = 0.13725042\n",
      "Iteration 44, loss = 0.13419409\n",
      "Iteration 45, loss = 0.13190363\n",
      "Iteration 46, loss = 0.12969861\n",
      "Iteration 47, loss = 0.12661976\n",
      "Iteration 48, loss = 0.12460890\n",
      "Iteration 49, loss = 0.12244153\n",
      "Iteration 50, loss = 0.12023421\n",
      "Iteration 51, loss = 0.11845785\n",
      "Iteration 52, loss = 0.11658403\n",
      "Iteration 53, loss = 0.11483052\n",
      "Iteration 54, loss = 0.11300031\n",
      "Iteration 55, loss = 0.11171025\n",
      "Iteration 56, loss = 0.11007548\n",
      "Iteration 57, loss = 0.10881730\n",
      "Iteration 58, loss = 0.10741817\n",
      "Iteration 59, loss = 0.10568452\n",
      "Iteration 60, loss = 0.10444415\n",
      "Iteration 61, loss = 0.10313618\n",
      "Iteration 62, loss = 0.10196843\n",
      "Iteration 63, loss = 0.10127170\n",
      "Iteration 64, loss = 0.09992358\n",
      "Iteration 65, loss = 0.09860286\n",
      "Iteration 66, loss = 0.09768230\n",
      "Iteration 67, loss = 0.09659029\n",
      "Iteration 68, loss = 0.09568775\n",
      "Iteration 69, loss = 0.09470767\n",
      "Iteration 70, loss = 0.09391289\n",
      "Iteration 71, loss = 0.09311186\n",
      "Iteration 72, loss = 0.09216148\n",
      "Iteration 73, loss = 0.09139245\n",
      "Iteration 74, loss = 0.09059601\n",
      "Iteration 75, loss = 0.09001917\n",
      "Iteration 76, loss = 0.08933322\n",
      "Iteration 77, loss = 0.08857667\n",
      "Iteration 78, loss = 0.08788328\n",
      "Iteration 79, loss = 0.08727542\n",
      "Iteration 80, loss = 0.08676536\n",
      "Iteration 81, loss = 0.08614514\n",
      "Iteration 82, loss = 0.08560523\n",
      "Iteration 83, loss = 0.08511995\n",
      "Iteration 84, loss = 0.08454943\n",
      "Iteration 85, loss = 0.08405780\n",
      "Iteration 86, loss = 0.08363663\n",
      "Iteration 87, loss = 0.08315430\n",
      "Iteration 88, loss = 0.08262979\n",
      "Iteration 89, loss = 0.08225953\n",
      "Iteration 90, loss = 0.08176313\n",
      "Iteration 91, loss = 0.08142037\n",
      "Iteration 92, loss = 0.08093370\n",
      "Iteration 93, loss = 0.08063404\n",
      "Iteration 94, loss = 0.08014300\n",
      "Iteration 95, loss = 0.07980055\n",
      "Iteration 96, loss = 0.07941032\n",
      "Iteration 97, loss = 0.07902754\n",
      "Iteration 98, loss = 0.07872510\n",
      "Iteration 99, loss = 0.07844599\n",
      "Iteration 100, loss = 0.07807104\n",
      "Iteration 101, loss = 0.07776225\n",
      "Iteration 102, loss = 0.07750349\n",
      "Iteration 103, loss = 0.07716290\n",
      "Iteration 104, loss = 0.07691230\n",
      "Iteration 105, loss = 0.07658842\n",
      "Iteration 106, loss = 0.07630542\n",
      "Iteration 107, loss = 0.07605452\n",
      "Iteration 108, loss = 0.07579951\n",
      "Iteration 109, loss = 0.07558669\n",
      "Iteration 110, loss = 0.07531332\n",
      "Iteration 111, loss = 0.07509736\n",
      "Iteration 112, loss = 0.07489784\n",
      "Iteration 113, loss = 0.07464962\n",
      "Iteration 114, loss = 0.07439543\n",
      "Iteration 115, loss = 0.07417543\n",
      "Iteration 116, loss = 0.07396842\n",
      "Iteration 117, loss = 0.07376088\n",
      "Iteration 118, loss = 0.07361746\n",
      "Iteration 119, loss = 0.07339578\n",
      "Iteration 120, loss = 0.07319799\n",
      "Iteration 121, loss = 0.07300253\n",
      "Iteration 122, loss = 0.07280430\n",
      "Iteration 123, loss = 0.07263746\n",
      "Iteration 124, loss = 0.07246206\n",
      "Iteration 125, loss = 0.07226923\n",
      "Iteration 126, loss = 0.07208702\n",
      "Iteration 127, loss = 0.07191779\n",
      "Iteration 128, loss = 0.07175860\n",
      "Iteration 129, loss = 0.07160850\n",
      "Iteration 130, loss = 0.07142275\n",
      "Iteration 131, loss = 0.07126837\n",
      "Iteration 132, loss = 0.07111663\n",
      "Iteration 133, loss = 0.07095667\n",
      "Iteration 134, loss = 0.07082422\n",
      "Iteration 135, loss = 0.07067405\n",
      "Iteration 136, loss = 0.07051268\n",
      "Iteration 137, loss = 0.07038550\n",
      "Iteration 138, loss = 0.07023685\n",
      "Iteration 139, loss = 0.07009079\n",
      "Iteration 140, loss = 0.06997309\n",
      "Iteration 141, loss = 0.06980886\n",
      "Iteration 142, loss = 0.06971191\n",
      "Iteration 143, loss = 0.06955178\n",
      "Iteration 144, loss = 0.06942691\n",
      "Iteration 145, loss = 0.06929174\n",
      "Iteration 146, loss = 0.06917576\n",
      "Iteration 147, loss = 0.06906614\n",
      "Iteration 148, loss = 0.06894233\n",
      "Iteration 149, loss = 0.06882360\n",
      "Iteration 150, loss = 0.06870639\n",
      "Iteration 151, loss = 0.06858849\n",
      "Iteration 152, loss = 0.06849093\n",
      "Iteration 153, loss = 0.06837863\n",
      "Iteration 154, loss = 0.06825334\n",
      "Iteration 155, loss = 0.06815570\n",
      "Iteration 156, loss = 0.06804971\n",
      "Iteration 157, loss = 0.06794402\n",
      "Iteration 158, loss = 0.06784844\n",
      "Iteration 159, loss = 0.06776973\n",
      "Iteration 160, loss = 0.06764178\n",
      "Iteration 161, loss = 0.06754211\n",
      "Iteration 162, loss = 0.06746637\n",
      "Iteration 163, loss = 0.06736485\n",
      "Iteration 164, loss = 0.06724697\n",
      "Iteration 165, loss = 0.06716004\n",
      "Iteration 166, loss = 0.06707386\n",
      "Iteration 167, loss = 0.06699583\n",
      "Iteration 168, loss = 0.06687942\n",
      "Iteration 169, loss = 0.06679286\n",
      "Iteration 170, loss = 0.06670320\n",
      "Iteration 171, loss = 0.06660337\n",
      "Iteration 172, loss = 0.06653181\n",
      "Iteration 173, loss = 0.06644556\n",
      "Iteration 174, loss = 0.06635134\n",
      "Iteration 175, loss = 0.06626934\n",
      "Iteration 176, loss = 0.06618730\n",
      "Iteration 177, loss = 0.06612066\n",
      "Iteration 178, loss = 0.06603739\n",
      "Iteration 179, loss = 0.06594976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38923122\n",
      "Iteration 2, loss = 2.22832059\n",
      "Iteration 3, loss = 2.07698192\n",
      "Iteration 4, loss = 1.88076335\n",
      "Iteration 5, loss = 1.64783003\n",
      "Iteration 6, loss = 1.41395472\n",
      "Iteration 7, loss = 1.19332252\n",
      "Iteration 8, loss = 1.00477733\n",
      "Iteration 9, loss = 0.85014701\n",
      "Iteration 10, loss = 0.72939631\n",
      "Iteration 11, loss = 0.63514081\n",
      "Iteration 12, loss = 0.56277351\n",
      "Iteration 13, loss = 0.50616521\n",
      "Iteration 14, loss = 0.45945872\n",
      "Iteration 15, loss = 0.42274789\n",
      "Iteration 16, loss = 0.39069795\n",
      "Iteration 17, loss = 0.36388210\n",
      "Iteration 18, loss = 0.34022165\n",
      "Iteration 19, loss = 0.32097713\n",
      "Iteration 20, loss = 0.30460884\n",
      "Iteration 21, loss = 0.28822161\n",
      "Iteration 22, loss = 0.27295080\n",
      "Iteration 23, loss = 0.26051251\n",
      "Iteration 24, loss = 0.24958389\n",
      "Iteration 25, loss = 0.23821170\n",
      "Iteration 26, loss = 0.22852437\n",
      "Iteration 27, loss = 0.21900971\n",
      "Iteration 28, loss = 0.21155604\n",
      "Iteration 29, loss = 0.20342435\n",
      "Iteration 30, loss = 0.19640529\n",
      "Iteration 31, loss = 0.19045636\n",
      "Iteration 32, loss = 0.18407390\n",
      "Iteration 33, loss = 0.17832101\n",
      "Iteration 34, loss = 0.17346349\n",
      "Iteration 35, loss = 0.16863310\n",
      "Iteration 36, loss = 0.16420193\n",
      "Iteration 37, loss = 0.15983134\n",
      "Iteration 38, loss = 0.15567856\n",
      "Iteration 39, loss = 0.15236904\n",
      "Iteration 40, loss = 0.14862280\n",
      "Iteration 41, loss = 0.14531938\n",
      "Iteration 42, loss = 0.14205030\n",
      "Iteration 43, loss = 0.13938678\n",
      "Iteration 44, loss = 0.13656646\n",
      "Iteration 45, loss = 0.13351716\n",
      "Iteration 46, loss = 0.13090033\n",
      "Iteration 47, loss = 0.12843902\n",
      "Iteration 48, loss = 0.12570134\n",
      "Iteration 49, loss = 0.12356689\n",
      "Iteration 50, loss = 0.12163568\n",
      "Iteration 51, loss = 0.11951638\n",
      "Iteration 52, loss = 0.11759196\n",
      "Iteration 53, loss = 0.11600639\n",
      "Iteration 54, loss = 0.11413213\n",
      "Iteration 55, loss = 0.11263382\n",
      "Iteration 56, loss = 0.11101845\n",
      "Iteration 57, loss = 0.10931539\n",
      "Iteration 58, loss = 0.10770225\n",
      "Iteration 59, loss = 0.10607513\n",
      "Iteration 60, loss = 0.10477211\n",
      "Iteration 61, loss = 0.10350974\n",
      "Iteration 62, loss = 0.10239094\n",
      "Iteration 63, loss = 0.10118433\n",
      "Iteration 64, loss = 0.10020759\n",
      "Iteration 65, loss = 0.09915068\n",
      "Iteration 66, loss = 0.09818902\n",
      "Iteration 67, loss = 0.09708892\n",
      "Iteration 68, loss = 0.09622454\n",
      "Iteration 69, loss = 0.09530465\n",
      "Iteration 70, loss = 0.09453111\n",
      "Iteration 71, loss = 0.09379592\n",
      "Iteration 72, loss = 0.09301948\n",
      "Iteration 73, loss = 0.09213169\n",
      "Iteration 74, loss = 0.09129256\n",
      "Iteration 75, loss = 0.09052409\n",
      "Iteration 76, loss = 0.08986062\n",
      "Iteration 77, loss = 0.08927341\n",
      "Iteration 78, loss = 0.08852736\n",
      "Iteration 79, loss = 0.08791622\n",
      "Iteration 80, loss = 0.08728127\n",
      "Iteration 81, loss = 0.08671752\n",
      "Iteration 82, loss = 0.08616821\n",
      "Iteration 83, loss = 0.08566859\n",
      "Iteration 84, loss = 0.08511441\n",
      "Iteration 85, loss = 0.08473666\n",
      "Iteration 86, loss = 0.08415450\n",
      "Iteration 87, loss = 0.08365247\n",
      "Iteration 88, loss = 0.08316855\n",
      "Iteration 89, loss = 0.08271637\n",
      "Iteration 90, loss = 0.08232295\n",
      "Iteration 91, loss = 0.08193170\n",
      "Iteration 92, loss = 0.08153981\n",
      "Iteration 93, loss = 0.08110604\n",
      "Iteration 94, loss = 0.08071939\n",
      "Iteration 95, loss = 0.08037571\n",
      "Iteration 96, loss = 0.08002912\n",
      "Iteration 97, loss = 0.07970739\n",
      "Iteration 98, loss = 0.07933630\n",
      "Iteration 99, loss = 0.07900244\n",
      "Iteration 100, loss = 0.07868060\n",
      "Iteration 101, loss = 0.07838089\n",
      "Iteration 102, loss = 0.07815129\n",
      "Iteration 103, loss = 0.07777657\n",
      "Iteration 104, loss = 0.07746179\n",
      "Iteration 105, loss = 0.07724780\n",
      "Iteration 106, loss = 0.07698100\n",
      "Iteration 107, loss = 0.07667139\n",
      "Iteration 108, loss = 0.07642327\n",
      "Iteration 109, loss = 0.07617128\n",
      "Iteration 110, loss = 0.07593094\n",
      "Iteration 111, loss = 0.07566968\n",
      "Iteration 112, loss = 0.07552015\n",
      "Iteration 113, loss = 0.07526313\n",
      "Iteration 114, loss = 0.07502045\n",
      "Iteration 115, loss = 0.07480468\n",
      "Iteration 116, loss = 0.07459535\n",
      "Iteration 117, loss = 0.07437794\n",
      "Iteration 118, loss = 0.07417446\n",
      "Iteration 119, loss = 0.07399136\n",
      "Iteration 120, loss = 0.07377734\n",
      "Iteration 121, loss = 0.07355867\n",
      "Iteration 122, loss = 0.07336936\n",
      "Iteration 123, loss = 0.07322013\n",
      "Iteration 124, loss = 0.07303478\n",
      "Iteration 125, loss = 0.07284810\n",
      "Iteration 126, loss = 0.07269671\n",
      "Iteration 127, loss = 0.07252100\n",
      "Iteration 128, loss = 0.07236388\n",
      "Iteration 129, loss = 0.07218407\n",
      "Iteration 130, loss = 0.07202084\n",
      "Iteration 131, loss = 0.07186646\n",
      "Iteration 132, loss = 0.07173119\n",
      "Iteration 133, loss = 0.07159158\n",
      "Iteration 134, loss = 0.07143073\n",
      "Iteration 135, loss = 0.07126726\n",
      "Iteration 136, loss = 0.07112699\n",
      "Iteration 137, loss = 0.07098909\n",
      "Iteration 138, loss = 0.07082204\n",
      "Iteration 139, loss = 0.07070712\n",
      "Iteration 140, loss = 0.07056183\n",
      "Iteration 141, loss = 0.07043101\n",
      "Iteration 142, loss = 0.07032059\n",
      "Iteration 143, loss = 0.07016250\n",
      "Iteration 144, loss = 0.07004328\n",
      "Iteration 145, loss = 0.06991545\n",
      "Iteration 146, loss = 0.06979337\n",
      "Iteration 147, loss = 0.06966936\n",
      "Iteration 148, loss = 0.06953353\n",
      "Iteration 149, loss = 0.06943258\n",
      "Iteration 150, loss = 0.06931436\n",
      "Iteration 151, loss = 0.06920525\n",
      "Iteration 152, loss = 0.06909599\n",
      "Iteration 153, loss = 0.06898520\n",
      "Iteration 154, loss = 0.06887264\n",
      "Iteration 155, loss = 0.06876356\n",
      "Iteration 156, loss = 0.06866547\n",
      "Iteration 157, loss = 0.06856484\n",
      "Iteration 158, loss = 0.06846456\n",
      "Iteration 159, loss = 0.06835910\n",
      "Iteration 160, loss = 0.06826111\n",
      "Iteration 161, loss = 0.06817596\n",
      "Iteration 162, loss = 0.06806479\n",
      "Iteration 163, loss = 0.06797736\n",
      "Iteration 164, loss = 0.06786411\n",
      "Iteration 165, loss = 0.06776853\n",
      "Iteration 166, loss = 0.06769070\n",
      "Iteration 167, loss = 0.06758824\n",
      "Iteration 168, loss = 0.06750025\n",
      "Iteration 169, loss = 0.06740359\n",
      "Iteration 170, loss = 0.06732209\n",
      "Iteration 171, loss = 0.06724515\n",
      "Iteration 172, loss = 0.06714773\n",
      "Iteration 173, loss = 0.06706411\n",
      "Iteration 174, loss = 0.06698008\n",
      "Iteration 175, loss = 0.06689970\n",
      "Iteration 176, loss = 0.06681536\n",
      "Iteration 177, loss = 0.06674451\n",
      "Iteration 178, loss = 0.06666357\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39339924\n",
      "Iteration 2, loss = 2.23511832\n",
      "Iteration 3, loss = 2.09284900\n",
      "Iteration 4, loss = 1.89969497\n",
      "Iteration 5, loss = 1.67196510\n",
      "Iteration 6, loss = 1.44157647\n",
      "Iteration 7, loss = 1.22502185\n",
      "Iteration 8, loss = 1.03766162\n",
      "Iteration 9, loss = 0.88662105\n",
      "Iteration 10, loss = 0.76614874\n",
      "Iteration 11, loss = 0.67032733\n",
      "Iteration 12, loss = 0.59499387\n",
      "Iteration 13, loss = 0.53668810\n",
      "Iteration 14, loss = 0.48644348\n",
      "Iteration 15, loss = 0.44795508\n",
      "Iteration 16, loss = 0.41292286\n",
      "Iteration 17, loss = 0.38588585\n",
      "Iteration 18, loss = 0.35964137\n",
      "Iteration 19, loss = 0.33859201\n",
      "Iteration 20, loss = 0.32040200\n",
      "Iteration 21, loss = 0.30336602\n",
      "Iteration 22, loss = 0.28841620\n",
      "Iteration 23, loss = 0.27468512\n",
      "Iteration 24, loss = 0.26276201\n",
      "Iteration 25, loss = 0.25092515\n",
      "Iteration 26, loss = 0.24028127\n",
      "Iteration 27, loss = 0.23062985\n",
      "Iteration 28, loss = 0.22226565\n",
      "Iteration 29, loss = 0.21436329\n",
      "Iteration 30, loss = 0.20609352\n",
      "Iteration 31, loss = 0.19964723\n",
      "Iteration 32, loss = 0.19319413\n",
      "Iteration 33, loss = 0.18712032\n",
      "Iteration 34, loss = 0.18176922\n",
      "Iteration 35, loss = 0.17674059\n",
      "Iteration 36, loss = 0.17211144\n",
      "Iteration 37, loss = 0.16724289\n",
      "Iteration 38, loss = 0.16281407\n",
      "Iteration 39, loss = 0.15915504\n",
      "Iteration 40, loss = 0.15530816\n",
      "Iteration 41, loss = 0.15173123\n",
      "Iteration 42, loss = 0.14830818\n",
      "Iteration 43, loss = 0.14554309\n",
      "Iteration 44, loss = 0.14241780\n",
      "Iteration 45, loss = 0.13926569\n",
      "Iteration 46, loss = 0.13629159\n",
      "Iteration 47, loss = 0.13393433\n",
      "Iteration 48, loss = 0.13096676\n",
      "Iteration 49, loss = 0.12871761\n",
      "Iteration 50, loss = 0.12646659\n",
      "Iteration 51, loss = 0.12439902\n",
      "Iteration 52, loss = 0.12229597\n",
      "Iteration 53, loss = 0.12059474\n",
      "Iteration 54, loss = 0.11874527\n",
      "Iteration 55, loss = 0.11676198\n",
      "Iteration 56, loss = 0.11518369\n",
      "Iteration 57, loss = 0.11315515\n",
      "Iteration 58, loss = 0.11174161\n",
      "Iteration 59, loss = 0.10986748\n",
      "Iteration 60, loss = 0.10849397\n",
      "Iteration 61, loss = 0.10712183\n",
      "Iteration 62, loss = 0.10580887\n",
      "Iteration 63, loss = 0.10462318\n",
      "Iteration 64, loss = 0.10343853\n",
      "Iteration 65, loss = 0.10230632\n",
      "Iteration 66, loss = 0.10143564\n",
      "Iteration 67, loss = 0.10016801\n",
      "Iteration 68, loss = 0.09915392\n",
      "Iteration 69, loss = 0.09815690\n",
      "Iteration 70, loss = 0.09718920\n",
      "Iteration 71, loss = 0.09644277\n",
      "Iteration 72, loss = 0.09565011\n",
      "Iteration 73, loss = 0.09475051\n",
      "Iteration 74, loss = 0.09384574\n",
      "Iteration 75, loss = 0.09296574\n",
      "Iteration 76, loss = 0.09228521\n",
      "Iteration 77, loss = 0.09161422\n",
      "Iteration 78, loss = 0.09076370\n",
      "Iteration 79, loss = 0.09019835\n",
      "Iteration 80, loss = 0.08954407\n",
      "Iteration 81, loss = 0.08883169\n",
      "Iteration 82, loss = 0.08830519\n",
      "Iteration 83, loss = 0.08770644\n",
      "Iteration 84, loss = 0.08715112\n",
      "Iteration 85, loss = 0.08670934\n",
      "Iteration 86, loss = 0.08604756\n",
      "Iteration 87, loss = 0.08550656\n",
      "Iteration 88, loss = 0.08501340\n",
      "Iteration 89, loss = 0.08451049\n",
      "Iteration 90, loss = 0.08411268\n",
      "Iteration 91, loss = 0.08368633\n",
      "Iteration 92, loss = 0.08327895\n",
      "Iteration 93, loss = 0.08279092\n",
      "Iteration 94, loss = 0.08240862\n",
      "Iteration 95, loss = 0.08203354\n",
      "Iteration 96, loss = 0.08167010\n",
      "Iteration 97, loss = 0.08132538\n",
      "Iteration 98, loss = 0.08092644\n",
      "Iteration 99, loss = 0.08058891\n",
      "Iteration 100, loss = 0.08024789\n",
      "Iteration 101, loss = 0.07995123\n",
      "Iteration 102, loss = 0.07967113\n",
      "Iteration 103, loss = 0.07929122\n",
      "Iteration 104, loss = 0.07897956\n",
      "Iteration 105, loss = 0.07869371\n",
      "Iteration 106, loss = 0.07842857\n",
      "Iteration 107, loss = 0.07813455\n",
      "Iteration 108, loss = 0.07787079\n",
      "Iteration 109, loss = 0.07760999\n",
      "Iteration 110, loss = 0.07732224\n",
      "Iteration 111, loss = 0.07704317\n",
      "Iteration 112, loss = 0.07691010\n",
      "Iteration 113, loss = 0.07661375\n",
      "Iteration 114, loss = 0.07639198\n",
      "Iteration 115, loss = 0.07613124\n",
      "Iteration 116, loss = 0.07594100\n",
      "Iteration 117, loss = 0.07570266\n",
      "Iteration 118, loss = 0.07545181\n",
      "Iteration 119, loss = 0.07528116\n",
      "Iteration 120, loss = 0.07504945\n",
      "Iteration 121, loss = 0.07482653\n",
      "Iteration 122, loss = 0.07462693\n",
      "Iteration 123, loss = 0.07446822\n",
      "Iteration 124, loss = 0.07427046\n",
      "Iteration 125, loss = 0.07407427\n",
      "Iteration 126, loss = 0.07393714\n",
      "Iteration 127, loss = 0.07375224\n",
      "Iteration 128, loss = 0.07357550\n",
      "Iteration 129, loss = 0.07338967\n",
      "Iteration 130, loss = 0.07321751\n",
      "Iteration 131, loss = 0.07306210\n",
      "Iteration 132, loss = 0.07289518\n",
      "Iteration 133, loss = 0.07277028\n",
      "Iteration 134, loss = 0.07260287\n",
      "Iteration 135, loss = 0.07242418\n",
      "Iteration 136, loss = 0.07227803\n",
      "Iteration 137, loss = 0.07214256\n",
      "Iteration 138, loss = 0.07196570\n",
      "Iteration 139, loss = 0.07184832\n",
      "Iteration 140, loss = 0.07169986\n",
      "Iteration 141, loss = 0.07155166\n",
      "Iteration 142, loss = 0.07142094\n",
      "Iteration 143, loss = 0.07127767\n",
      "Iteration 144, loss = 0.07114835\n",
      "Iteration 145, loss = 0.07101155\n",
      "Iteration 146, loss = 0.07090419\n",
      "Iteration 147, loss = 0.07076469\n",
      "Iteration 148, loss = 0.07062090\n",
      "Iteration 149, loss = 0.07050905\n",
      "Iteration 150, loss = 0.07038512\n",
      "Iteration 151, loss = 0.07026754\n",
      "Iteration 152, loss = 0.07015218\n",
      "Iteration 153, loss = 0.07003931\n",
      "Iteration 154, loss = 0.06992746\n",
      "Iteration 155, loss = 0.06981892\n",
      "Iteration 156, loss = 0.06973170\n",
      "Iteration 157, loss = 0.06959816\n",
      "Iteration 158, loss = 0.06949415\n",
      "Iteration 159, loss = 0.06939072\n",
      "Iteration 160, loss = 0.06929781\n",
      "Iteration 161, loss = 0.06917860\n",
      "Iteration 162, loss = 0.06906668\n",
      "Iteration 163, loss = 0.06897942\n",
      "Iteration 164, loss = 0.06887901\n",
      "Iteration 165, loss = 0.06876833\n",
      "Iteration 166, loss = 0.06867264\n",
      "Iteration 167, loss = 0.06858892\n",
      "Iteration 168, loss = 0.06849810\n",
      "Iteration 169, loss = 0.06839772\n",
      "Iteration 170, loss = 0.06830004\n",
      "Iteration 171, loss = 0.06821650\n",
      "Iteration 172, loss = 0.06812112\n",
      "Iteration 173, loss = 0.06804165\n",
      "Iteration 174, loss = 0.06795073\n",
      "Iteration 175, loss = 0.06786494\n",
      "Iteration 176, loss = 0.06777545\n",
      "Iteration 177, loss = 0.06770060\n",
      "Iteration 178, loss = 0.06761600\n",
      "Iteration 179, loss = 0.06753618\n",
      "Iteration 180, loss = 0.06746399\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38999864\n",
      "Iteration 2, loss = 2.22786884\n",
      "Iteration 3, loss = 2.08150250\n",
      "Iteration 4, loss = 1.88819334\n",
      "Iteration 5, loss = 1.65119188\n",
      "Iteration 6, loss = 1.40895096\n",
      "Iteration 7, loss = 1.18289520\n",
      "Iteration 8, loss = 0.98993156\n",
      "Iteration 9, loss = 0.83005411\n",
      "Iteration 10, loss = 0.70584272\n",
      "Iteration 11, loss = 0.61081930\n",
      "Iteration 12, loss = 0.53467444\n",
      "Iteration 13, loss = 0.47689482\n",
      "Iteration 14, loss = 0.43024160\n",
      "Iteration 15, loss = 0.39314999\n",
      "Iteration 16, loss = 0.36166685\n",
      "Iteration 17, loss = 0.33707618\n",
      "Iteration 18, loss = 0.31401501\n",
      "Iteration 19, loss = 0.29488061\n",
      "Iteration 20, loss = 0.27894725\n",
      "Iteration 21, loss = 0.26410501\n",
      "Iteration 22, loss = 0.25199850\n",
      "Iteration 23, loss = 0.23893973\n",
      "Iteration 24, loss = 0.22834874\n",
      "Iteration 25, loss = 0.21762293\n",
      "Iteration 26, loss = 0.20921475\n",
      "Iteration 27, loss = 0.20111265\n",
      "Iteration 28, loss = 0.19366457\n",
      "Iteration 29, loss = 0.18632359\n",
      "Iteration 30, loss = 0.18030172\n",
      "Iteration 31, loss = 0.17439324\n",
      "Iteration 32, loss = 0.16900985\n",
      "Iteration 33, loss = 0.16390266\n",
      "Iteration 34, loss = 0.15969687\n",
      "Iteration 35, loss = 0.15500197\n",
      "Iteration 36, loss = 0.15088591\n",
      "Iteration 37, loss = 0.14712008\n",
      "Iteration 38, loss = 0.14331449\n",
      "Iteration 39, loss = 0.14027197\n",
      "Iteration 40, loss = 0.13683914\n",
      "Iteration 41, loss = 0.13397510\n",
      "Iteration 42, loss = 0.13118227\n",
      "Iteration 43, loss = 0.12861828\n",
      "Iteration 44, loss = 0.12599891\n",
      "Iteration 45, loss = 0.12375893\n",
      "Iteration 46, loss = 0.12150190\n",
      "Iteration 47, loss = 0.11937839\n",
      "Iteration 48, loss = 0.11714856\n",
      "Iteration 49, loss = 0.11542077\n",
      "Iteration 50, loss = 0.11359934\n",
      "Iteration 51, loss = 0.11190698\n",
      "Iteration 52, loss = 0.11010084\n",
      "Iteration 53, loss = 0.10863744\n",
      "Iteration 54, loss = 0.10709922\n",
      "Iteration 55, loss = 0.10544266\n",
      "Iteration 56, loss = 0.10425904\n",
      "Iteration 57, loss = 0.10284283\n",
      "Iteration 58, loss = 0.10136535\n",
      "Iteration 59, loss = 0.10012386\n",
      "Iteration 60, loss = 0.09894449\n",
      "Iteration 61, loss = 0.09779671\n",
      "Iteration 62, loss = 0.09683356\n",
      "Iteration 63, loss = 0.09609786\n",
      "Iteration 64, loss = 0.09495746\n",
      "Iteration 65, loss = 0.09393684\n",
      "Iteration 66, loss = 0.09321061\n",
      "Iteration 67, loss = 0.09239954\n",
      "Iteration 68, loss = 0.09148889\n",
      "Iteration 69, loss = 0.09069400\n",
      "Iteration 70, loss = 0.08997400\n",
      "Iteration 71, loss = 0.08925615\n",
      "Iteration 72, loss = 0.08863806\n",
      "Iteration 73, loss = 0.08795009\n",
      "Iteration 74, loss = 0.08719267\n",
      "Iteration 75, loss = 0.08657059\n",
      "Iteration 76, loss = 0.08605452\n",
      "Iteration 77, loss = 0.08554878\n",
      "Iteration 78, loss = 0.08482702\n",
      "Iteration 79, loss = 0.08428194\n",
      "Iteration 80, loss = 0.08377586\n",
      "Iteration 81, loss = 0.08321303\n",
      "Iteration 82, loss = 0.08278076\n",
      "Iteration 83, loss = 0.08228585\n",
      "Iteration 84, loss = 0.08185847\n",
      "Iteration 85, loss = 0.08137314\n",
      "Iteration 86, loss = 0.08098029\n",
      "Iteration 87, loss = 0.08052704\n",
      "Iteration 88, loss = 0.08010809\n",
      "Iteration 89, loss = 0.07972668\n",
      "Iteration 90, loss = 0.07936656\n",
      "Iteration 91, loss = 0.07900665\n",
      "Iteration 92, loss = 0.07865870\n",
      "Iteration 93, loss = 0.07831959\n",
      "Iteration 94, loss = 0.07800215\n",
      "Iteration 95, loss = 0.07767347\n",
      "Iteration 96, loss = 0.07736360\n",
      "Iteration 97, loss = 0.07707339\n",
      "Iteration 98, loss = 0.07675321\n",
      "Iteration 99, loss = 0.07643553\n",
      "Iteration 100, loss = 0.07618223\n",
      "Iteration 101, loss = 0.07590935\n",
      "Iteration 102, loss = 0.07565327\n",
      "Iteration 103, loss = 0.07535087\n",
      "Iteration 104, loss = 0.07510899\n",
      "Iteration 105, loss = 0.07485147\n",
      "Iteration 106, loss = 0.07461255\n",
      "Iteration 107, loss = 0.07439289\n",
      "Iteration 108, loss = 0.07418041\n",
      "Iteration 109, loss = 0.07396097\n",
      "Iteration 110, loss = 0.07371333\n",
      "Iteration 111, loss = 0.07347574\n",
      "Iteration 112, loss = 0.07328835\n",
      "Iteration 113, loss = 0.07311849\n",
      "Iteration 114, loss = 0.07292164\n",
      "Iteration 115, loss = 0.07276054\n",
      "Iteration 116, loss = 0.07250469\n",
      "Iteration 117, loss = 0.07231021\n",
      "Iteration 118, loss = 0.07213013\n",
      "Iteration 119, loss = 0.07198215\n",
      "Iteration 120, loss = 0.07177761\n",
      "Iteration 121, loss = 0.07158463\n",
      "Iteration 122, loss = 0.07141815\n",
      "Iteration 123, loss = 0.07125275\n",
      "Iteration 124, loss = 0.07110986\n",
      "Iteration 125, loss = 0.07094783\n",
      "Iteration 126, loss = 0.07081817\n",
      "Iteration 127, loss = 0.07064575\n",
      "Iteration 128, loss = 0.07047655\n",
      "Iteration 129, loss = 0.07032873\n",
      "Iteration 130, loss = 0.07018116\n",
      "Iteration 131, loss = 0.07005439\n",
      "Iteration 132, loss = 0.06989805\n",
      "Iteration 133, loss = 0.06976808\n",
      "Iteration 134, loss = 0.06962979\n",
      "Iteration 135, loss = 0.06949154\n",
      "Iteration 136, loss = 0.06937154\n",
      "Iteration 137, loss = 0.06923833\n",
      "Iteration 138, loss = 0.06908484\n",
      "Iteration 139, loss = 0.06898489\n",
      "Iteration 140, loss = 0.06886081\n",
      "Iteration 141, loss = 0.06872738\n",
      "Iteration 142, loss = 0.06859783\n",
      "Iteration 143, loss = 0.06849549\n",
      "Iteration 144, loss = 0.06837155\n",
      "Iteration 145, loss = 0.06823755\n",
      "Iteration 146, loss = 0.06813593\n",
      "Iteration 147, loss = 0.06801878\n",
      "Iteration 148, loss = 0.06790158\n",
      "Iteration 149, loss = 0.06779856\n",
      "Iteration 150, loss = 0.06768970\n",
      "Iteration 151, loss = 0.06758470\n",
      "Iteration 152, loss = 0.06747347\n",
      "Iteration 153, loss = 0.06738220\n",
      "Iteration 154, loss = 0.06728091\n",
      "Iteration 155, loss = 0.06717566\n",
      "Iteration 156, loss = 0.06707850\n",
      "Iteration 157, loss = 0.06698482\n",
      "Iteration 158, loss = 0.06690140\n",
      "Iteration 159, loss = 0.06680408\n",
      "Iteration 160, loss = 0.06671204\n",
      "Iteration 161, loss = 0.06660886\n",
      "Iteration 162, loss = 0.06651889\n",
      "Iteration 163, loss = 0.06643634\n",
      "Iteration 164, loss = 0.06634410\n",
      "Iteration 165, loss = 0.06625356\n",
      "Iteration 166, loss = 0.06618352\n",
      "Iteration 167, loss = 0.06609090\n",
      "Iteration 168, loss = 0.06601220\n",
      "Iteration 169, loss = 0.06593664\n",
      "Iteration 170, loss = 0.06584923\n",
      "Iteration 171, loss = 0.06574854\n",
      "Iteration 172, loss = 0.06565807\n",
      "Iteration 173, loss = 0.06557746\n",
      "Iteration 174, loss = 0.06551448\n",
      "Iteration 175, loss = 0.06542475\n",
      "Iteration 176, loss = 0.06534310\n",
      "Iteration 177, loss = 0.06526909\n",
      "Iteration 178, loss = 0.06519607\n",
      "Iteration 179, loss = 0.06512165\n",
      "Iteration 180, loss = 0.06504630\n",
      "Iteration 181, loss = 0.06497490\n",
      "Iteration 182, loss = 0.06491753\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32066133\n",
      "Iteration 2, loss = 2.01829952\n",
      "Iteration 3, loss = 1.72255624\n",
      "Iteration 4, loss = 1.39274232\n",
      "Iteration 5, loss = 1.09101892\n",
      "Iteration 6, loss = 0.84744101\n",
      "Iteration 7, loss = 0.66161763\n",
      "Iteration 8, loss = 0.52873232\n",
      "Iteration 9, loss = 0.43159581\n",
      "Iteration 10, loss = 0.36056811\n",
      "Iteration 11, loss = 0.31004800\n",
      "Iteration 12, loss = 0.26987011\n",
      "Iteration 13, loss = 0.23830346\n",
      "Iteration 14, loss = 0.21506921\n",
      "Iteration 15, loss = 0.19373307\n",
      "Iteration 16, loss = 0.17672392\n",
      "Iteration 17, loss = 0.16334483\n",
      "Iteration 18, loss = 0.15105439\n",
      "Iteration 19, loss = 0.14137270\n",
      "Iteration 20, loss = 0.13270482\n",
      "Iteration 21, loss = 0.12576585\n",
      "Iteration 22, loss = 0.11893127\n",
      "Iteration 23, loss = 0.11352297\n",
      "Iteration 24, loss = 0.10853537\n",
      "Iteration 25, loss = 0.10460380\n",
      "Iteration 26, loss = 0.10088185\n",
      "Iteration 27, loss = 0.09744906\n",
      "Iteration 28, loss = 0.09467771\n",
      "Iteration 29, loss = 0.09198804\n",
      "Iteration 30, loss = 0.08961961\n",
      "Iteration 31, loss = 0.08751335\n",
      "Iteration 32, loss = 0.08551263\n",
      "Iteration 33, loss = 0.08387120\n",
      "Iteration 34, loss = 0.08211769\n",
      "Iteration 35, loss = 0.08066601\n",
      "Iteration 36, loss = 0.07925929\n",
      "Iteration 37, loss = 0.07792767\n",
      "Iteration 38, loss = 0.07677796\n",
      "Iteration 39, loss = 0.07564030\n",
      "Iteration 40, loss = 0.07463836\n",
      "Iteration 41, loss = 0.07360021\n",
      "Iteration 42, loss = 0.07266065\n",
      "Iteration 43, loss = 0.07177792\n",
      "Iteration 44, loss = 0.07091331\n",
      "Iteration 45, loss = 0.07011984\n",
      "Iteration 46, loss = 0.06932942\n",
      "Iteration 47, loss = 0.06859446\n",
      "Iteration 48, loss = 0.06790617\n",
      "Iteration 49, loss = 0.06721771\n",
      "Iteration 50, loss = 0.06659136\n",
      "Iteration 51, loss = 0.06595560\n",
      "Iteration 52, loss = 0.06535498\n",
      "Iteration 53, loss = 0.06483223\n",
      "Iteration 54, loss = 0.06424083\n",
      "Iteration 55, loss = 0.06370555\n",
      "Iteration 56, loss = 0.06318196\n",
      "Iteration 57, loss = 0.06270414\n",
      "Iteration 58, loss = 0.06218558\n",
      "Iteration 59, loss = 0.06171509\n",
      "Iteration 60, loss = 0.06127216\n",
      "Iteration 61, loss = 0.06081130\n",
      "Iteration 62, loss = 0.06035404\n",
      "Iteration 63, loss = 0.05994492\n",
      "Iteration 64, loss = 0.05951325\n",
      "Iteration 65, loss = 0.05909850\n",
      "Iteration 66, loss = 0.05872020\n",
      "Iteration 67, loss = 0.05832712\n",
      "Iteration 68, loss = 0.05796000\n",
      "Iteration 69, loss = 0.05758913\n",
      "Iteration 70, loss = 0.05719366\n",
      "Iteration 71, loss = 0.05682355\n",
      "Iteration 72, loss = 0.05648337\n",
      "Iteration 73, loss = 0.05615524\n",
      "Iteration 74, loss = 0.05577671\n",
      "Iteration 75, loss = 0.05545881\n",
      "Iteration 76, loss = 0.05514732\n",
      "Iteration 77, loss = 0.05484077\n",
      "Iteration 78, loss = 0.05451503\n",
      "Iteration 79, loss = 0.05420953\n",
      "Iteration 80, loss = 0.05391158\n",
      "Iteration 81, loss = 0.05363511\n",
      "Iteration 82, loss = 0.05331686\n",
      "Iteration 83, loss = 0.05306408\n",
      "Iteration 84, loss = 0.05276468\n",
      "Iteration 85, loss = 0.05249805\n",
      "Iteration 86, loss = 0.05223281\n",
      "Iteration 87, loss = 0.05196947\n",
      "Iteration 88, loss = 0.05170304\n",
      "Iteration 89, loss = 0.05145968\n",
      "Iteration 90, loss = 0.05121153\n",
      "Iteration 91, loss = 0.05096882\n",
      "Iteration 92, loss = 0.05074941\n",
      "Iteration 93, loss = 0.05048959\n",
      "Iteration 94, loss = 0.05025440\n",
      "Iteration 95, loss = 0.05004658\n",
      "Iteration 96, loss = 0.04980980\n",
      "Iteration 97, loss = 0.04960263\n",
      "Iteration 98, loss = 0.04937469\n",
      "Iteration 99, loss = 0.04919073\n",
      "Iteration 100, loss = 0.04895888\n",
      "Iteration 101, loss = 0.04875260\n",
      "Iteration 102, loss = 0.04856337\n",
      "Iteration 103, loss = 0.04836126\n",
      "Iteration 104, loss = 0.04817366\n",
      "Iteration 105, loss = 0.04798165\n",
      "Iteration 106, loss = 0.04779557\n",
      "Iteration 107, loss = 0.04760530\n",
      "Iteration 108, loss = 0.04743664\n",
      "Iteration 109, loss = 0.04726476\n",
      "Iteration 110, loss = 0.04707454\n",
      "Iteration 111, loss = 0.04693185\n",
      "Iteration 112, loss = 0.04677609\n",
      "Iteration 113, loss = 0.04658726\n",
      "Iteration 114, loss = 0.04641560\n",
      "Iteration 115, loss = 0.04626215\n",
      "Iteration 116, loss = 0.04610422\n",
      "Iteration 117, loss = 0.04594642\n",
      "Iteration 118, loss = 0.04584327\n",
      "Iteration 119, loss = 0.04569034\n",
      "Iteration 120, loss = 0.04552558\n",
      "Iteration 121, loss = 0.04538382\n",
      "Iteration 122, loss = 0.04522073\n",
      "Iteration 123, loss = 0.04516233\n",
      "Iteration 124, loss = 0.04496945\n",
      "Iteration 125, loss = 0.04483973\n",
      "Iteration 126, loss = 0.04471538\n",
      "Iteration 127, loss = 0.04456929\n",
      "Iteration 128, loss = 0.04443622\n",
      "Iteration 129, loss = 0.04431162\n",
      "Iteration 130, loss = 0.04419619\n",
      "Iteration 131, loss = 0.04407589\n",
      "Iteration 132, loss = 0.04394445\n",
      "Iteration 133, loss = 0.04382822\n",
      "Iteration 134, loss = 0.04375863\n",
      "Iteration 135, loss = 0.04362220\n",
      "Iteration 136, loss = 0.04350889\n",
      "Iteration 137, loss = 0.04341894\n",
      "Iteration 138, loss = 0.04330357\n",
      "Iteration 139, loss = 0.04319654\n",
      "Iteration 140, loss = 0.04309945\n",
      "Iteration 141, loss = 0.04295210\n",
      "Iteration 142, loss = 0.04289991\n",
      "Iteration 143, loss = 0.04277030\n",
      "Iteration 144, loss = 0.04267656\n",
      "Iteration 145, loss = 0.04258261\n",
      "Iteration 146, loss = 0.04250590\n",
      "Iteration 147, loss = 0.04241169\n",
      "Iteration 148, loss = 0.04232164\n",
      "Iteration 149, loss = 0.04222379\n",
      "Iteration 150, loss = 0.04214743\n",
      "Iteration 151, loss = 0.04205629\n",
      "Iteration 152, loss = 0.04198446\n",
      "Iteration 153, loss = 0.04189316\n",
      "Iteration 154, loss = 0.04180887\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31619237\n",
      "Iteration 2, loss = 2.01145526\n",
      "Iteration 3, loss = 1.71579666\n",
      "Iteration 4, loss = 1.38671715\n",
      "Iteration 5, loss = 1.08771276\n",
      "Iteration 6, loss = 0.83978514\n",
      "Iteration 7, loss = 0.65664997\n",
      "Iteration 8, loss = 0.52569982\n",
      "Iteration 9, loss = 0.43261943\n",
      "Iteration 10, loss = 0.36152185\n",
      "Iteration 11, loss = 0.31061731\n",
      "Iteration 12, loss = 0.27034905\n",
      "Iteration 13, loss = 0.23960748\n",
      "Iteration 14, loss = 0.21643893\n",
      "Iteration 15, loss = 0.19423233\n",
      "Iteration 16, loss = 0.17671636\n",
      "Iteration 17, loss = 0.16298464\n",
      "Iteration 18, loss = 0.15092047\n",
      "Iteration 19, loss = 0.14093270\n",
      "Iteration 20, loss = 0.13211803\n",
      "Iteration 21, loss = 0.12463291\n",
      "Iteration 22, loss = 0.11801044\n",
      "Iteration 23, loss = 0.11244448\n",
      "Iteration 24, loss = 0.10765926\n",
      "Iteration 25, loss = 0.10352004\n",
      "Iteration 26, loss = 0.09986300\n",
      "Iteration 27, loss = 0.09644536\n",
      "Iteration 28, loss = 0.09340879\n",
      "Iteration 29, loss = 0.09088667\n",
      "Iteration 30, loss = 0.08849528\n",
      "Iteration 31, loss = 0.08640833\n",
      "Iteration 32, loss = 0.08441084\n",
      "Iteration 33, loss = 0.08274456\n",
      "Iteration 34, loss = 0.08110992\n",
      "Iteration 35, loss = 0.07958411\n",
      "Iteration 36, loss = 0.07826135\n",
      "Iteration 37, loss = 0.07696207\n",
      "Iteration 38, loss = 0.07573815\n",
      "Iteration 39, loss = 0.07463846\n",
      "Iteration 40, loss = 0.07360171\n",
      "Iteration 41, loss = 0.07254331\n",
      "Iteration 42, loss = 0.07165978\n",
      "Iteration 43, loss = 0.07077541\n",
      "Iteration 44, loss = 0.06988525\n",
      "Iteration 45, loss = 0.06913615\n",
      "Iteration 46, loss = 0.06843806\n",
      "Iteration 47, loss = 0.06757987\n",
      "Iteration 48, loss = 0.06693730\n",
      "Iteration 49, loss = 0.06627362\n",
      "Iteration 50, loss = 0.06558918\n",
      "Iteration 51, loss = 0.06502587\n",
      "Iteration 52, loss = 0.06441640\n",
      "Iteration 53, loss = 0.06388256\n",
      "Iteration 54, loss = 0.06332645\n",
      "Iteration 55, loss = 0.06280640\n",
      "Iteration 56, loss = 0.06228732\n",
      "Iteration 57, loss = 0.06175206\n",
      "Iteration 58, loss = 0.06129457\n",
      "Iteration 59, loss = 0.06082269\n",
      "Iteration 60, loss = 0.06036024\n",
      "Iteration 61, loss = 0.05990619\n",
      "Iteration 62, loss = 0.05948369\n",
      "Iteration 63, loss = 0.05908364\n",
      "Iteration 64, loss = 0.05863931\n",
      "Iteration 65, loss = 0.05821076\n",
      "Iteration 66, loss = 0.05784166\n",
      "Iteration 67, loss = 0.05744068\n",
      "Iteration 68, loss = 0.05707846\n",
      "Iteration 69, loss = 0.05669656\n",
      "Iteration 70, loss = 0.05632668\n",
      "Iteration 71, loss = 0.05598084\n",
      "Iteration 72, loss = 0.05562145\n",
      "Iteration 73, loss = 0.05528560\n",
      "Iteration 74, loss = 0.05493659\n",
      "Iteration 75, loss = 0.05464189\n",
      "Iteration 76, loss = 0.05432917\n",
      "Iteration 77, loss = 0.05399811\n",
      "Iteration 78, loss = 0.05367952\n",
      "Iteration 79, loss = 0.05338176\n",
      "Iteration 80, loss = 0.05309176\n",
      "Iteration 81, loss = 0.05281586\n",
      "Iteration 82, loss = 0.05251174\n",
      "Iteration 83, loss = 0.05225256\n",
      "Iteration 84, loss = 0.05197762\n",
      "Iteration 85, loss = 0.05170130\n",
      "Iteration 86, loss = 0.05145157\n",
      "Iteration 87, loss = 0.05118750\n",
      "Iteration 88, loss = 0.05091622\n",
      "Iteration 89, loss = 0.05068739\n",
      "Iteration 90, loss = 0.05043834\n",
      "Iteration 91, loss = 0.05021140\n",
      "Iteration 92, loss = 0.04994649\n",
      "Iteration 93, loss = 0.04974247\n",
      "Iteration 94, loss = 0.04948874\n",
      "Iteration 95, loss = 0.04927969\n",
      "Iteration 96, loss = 0.04904538\n",
      "Iteration 97, loss = 0.04881720\n",
      "Iteration 98, loss = 0.04863599\n",
      "Iteration 99, loss = 0.04842319\n",
      "Iteration 100, loss = 0.04819665\n",
      "Iteration 101, loss = 0.04800311\n",
      "Iteration 102, loss = 0.04782868\n",
      "Iteration 103, loss = 0.04761129\n",
      "Iteration 104, loss = 0.04744724\n",
      "Iteration 105, loss = 0.04724307\n",
      "Iteration 106, loss = 0.04705350\n",
      "Iteration 107, loss = 0.04686783\n",
      "Iteration 108, loss = 0.04668909\n",
      "Iteration 109, loss = 0.04652770\n",
      "Iteration 110, loss = 0.04634255\n",
      "Iteration 111, loss = 0.04620078\n",
      "Iteration 112, loss = 0.04605588\n",
      "Iteration 113, loss = 0.04585846\n",
      "Iteration 114, loss = 0.04569299\n",
      "Iteration 115, loss = 0.04554636\n",
      "Iteration 116, loss = 0.04538309\n",
      "Iteration 117, loss = 0.04522406\n",
      "Iteration 118, loss = 0.04513376\n",
      "Iteration 119, loss = 0.04498633\n",
      "Iteration 120, loss = 0.04482170\n",
      "Iteration 121, loss = 0.04465984\n",
      "Iteration 122, loss = 0.04451350\n",
      "Iteration 123, loss = 0.04442736\n",
      "Iteration 124, loss = 0.04427355\n",
      "Iteration 125, loss = 0.04414195\n",
      "Iteration 126, loss = 0.04399426\n",
      "Iteration 127, loss = 0.04388011\n",
      "Iteration 128, loss = 0.04374637\n",
      "Iteration 129, loss = 0.04365035\n",
      "Iteration 130, loss = 0.04350026\n",
      "Iteration 131, loss = 0.04338621\n",
      "Iteration 132, loss = 0.04325482\n",
      "Iteration 133, loss = 0.04313726\n",
      "Iteration 134, loss = 0.04304105\n",
      "Iteration 135, loss = 0.04292473\n",
      "Iteration 136, loss = 0.04280947\n",
      "Iteration 137, loss = 0.04271281\n",
      "Iteration 138, loss = 0.04260412\n",
      "Iteration 139, loss = 0.04251148\n",
      "Iteration 140, loss = 0.04240359\n",
      "Iteration 141, loss = 0.04228127\n",
      "Iteration 142, loss = 0.04222759\n",
      "Iteration 143, loss = 0.04209033\n",
      "Iteration 144, loss = 0.04202502\n",
      "Iteration 145, loss = 0.04190845\n",
      "Iteration 146, loss = 0.04182956\n",
      "Iteration 147, loss = 0.04175559\n",
      "Iteration 148, loss = 0.04164340\n",
      "Iteration 149, loss = 0.04154328\n",
      "Iteration 150, loss = 0.04146967\n",
      "Iteration 151, loss = 0.04139237\n",
      "Iteration 152, loss = 0.04132416\n",
      "Iteration 153, loss = 0.04123832\n",
      "Iteration 154, loss = 0.04114463\n",
      "Iteration 155, loss = 0.04108672\n",
      "Iteration 156, loss = 0.04101895\n",
      "Iteration 157, loss = 0.04094273\n",
      "Iteration 158, loss = 0.04088720\n",
      "Iteration 159, loss = 0.04078541\n",
      "Iteration 160, loss = 0.04071460\n",
      "Iteration 161, loss = 0.04066585\n",
      "Iteration 162, loss = 0.04059462\n",
      "Iteration 163, loss = 0.04049777\n",
      "Iteration 164, loss = 0.04044502\n",
      "Iteration 165, loss = 0.04039415\n",
      "Iteration 166, loss = 0.04030541\n",
      "Iteration 167, loss = 0.04024798\n",
      "Iteration 168, loss = 0.04016736\n",
      "Iteration 169, loss = 0.04011395\n",
      "Iteration 170, loss = 0.04004440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31159254\n",
      "Iteration 2, loss = 2.00534427\n",
      "Iteration 3, loss = 1.70367976\n",
      "Iteration 4, loss = 1.37050570\n",
      "Iteration 5, loss = 1.07194053\n",
      "Iteration 6, loss = 0.83540456\n",
      "Iteration 7, loss = 0.65457448\n",
      "Iteration 8, loss = 0.52297141\n",
      "Iteration 9, loss = 0.43023757\n",
      "Iteration 10, loss = 0.36091012\n",
      "Iteration 11, loss = 0.30944755\n",
      "Iteration 12, loss = 0.26957293\n",
      "Iteration 13, loss = 0.23848259\n",
      "Iteration 14, loss = 0.21267724\n",
      "Iteration 15, loss = 0.19182904\n",
      "Iteration 16, loss = 0.17471153\n",
      "Iteration 17, loss = 0.16047113\n",
      "Iteration 18, loss = 0.14901471\n",
      "Iteration 19, loss = 0.13864637\n",
      "Iteration 20, loss = 0.13036021\n",
      "Iteration 21, loss = 0.12324820\n",
      "Iteration 22, loss = 0.11630339\n",
      "Iteration 23, loss = 0.11091740\n",
      "Iteration 24, loss = 0.10640772\n",
      "Iteration 25, loss = 0.10203694\n",
      "Iteration 26, loss = 0.09840645\n",
      "Iteration 27, loss = 0.09513550\n",
      "Iteration 28, loss = 0.09257560\n",
      "Iteration 29, loss = 0.08995296\n",
      "Iteration 30, loss = 0.08770584\n",
      "Iteration 31, loss = 0.08572873\n",
      "Iteration 32, loss = 0.08385071\n",
      "Iteration 33, loss = 0.08215509\n",
      "Iteration 34, loss = 0.08067178\n",
      "Iteration 35, loss = 0.07926110\n",
      "Iteration 36, loss = 0.07796883\n",
      "Iteration 37, loss = 0.07673112\n",
      "Iteration 38, loss = 0.07556921\n",
      "Iteration 39, loss = 0.07456912\n",
      "Iteration 40, loss = 0.07355837\n",
      "Iteration 41, loss = 0.07263027\n",
      "Iteration 42, loss = 0.07172759\n",
      "Iteration 43, loss = 0.07090349\n",
      "Iteration 44, loss = 0.07011834\n",
      "Iteration 45, loss = 0.06932119\n",
      "Iteration 46, loss = 0.06860217\n",
      "Iteration 47, loss = 0.06789041\n",
      "Iteration 48, loss = 0.06718611\n",
      "Iteration 49, loss = 0.06653585\n",
      "Iteration 50, loss = 0.06593408\n",
      "Iteration 51, loss = 0.06532776\n",
      "Iteration 52, loss = 0.06474021\n",
      "Iteration 53, loss = 0.06420188\n",
      "Iteration 54, loss = 0.06366329\n",
      "Iteration 55, loss = 0.06318598\n",
      "Iteration 56, loss = 0.06266289\n",
      "Iteration 57, loss = 0.06215373\n",
      "Iteration 58, loss = 0.06167808\n",
      "Iteration 59, loss = 0.06118386\n",
      "Iteration 60, loss = 0.06072203\n",
      "Iteration 61, loss = 0.06030573\n",
      "Iteration 62, loss = 0.05989406\n",
      "Iteration 63, loss = 0.05945255\n",
      "Iteration 64, loss = 0.05904285\n",
      "Iteration 65, loss = 0.05866182\n",
      "Iteration 66, loss = 0.05827563\n",
      "Iteration 67, loss = 0.05788451\n",
      "Iteration 68, loss = 0.05749899\n",
      "Iteration 69, loss = 0.05715063\n",
      "Iteration 70, loss = 0.05681233\n",
      "Iteration 71, loss = 0.05647715\n",
      "Iteration 72, loss = 0.05612194\n",
      "Iteration 73, loss = 0.05580044\n",
      "Iteration 74, loss = 0.05544795\n",
      "Iteration 75, loss = 0.05509689\n",
      "Iteration 76, loss = 0.05478882\n",
      "Iteration 77, loss = 0.05449001\n",
      "Iteration 78, loss = 0.05419263\n",
      "Iteration 79, loss = 0.05389345\n",
      "Iteration 80, loss = 0.05357561\n",
      "Iteration 81, loss = 0.05330603\n",
      "Iteration 82, loss = 0.05300367\n",
      "Iteration 83, loss = 0.05274343\n",
      "Iteration 84, loss = 0.05246552\n",
      "Iteration 85, loss = 0.05222251\n",
      "Iteration 86, loss = 0.05194534\n",
      "Iteration 87, loss = 0.05169163\n",
      "Iteration 88, loss = 0.05142584\n",
      "Iteration 89, loss = 0.05117967\n",
      "Iteration 90, loss = 0.05093189\n",
      "Iteration 91, loss = 0.05070080\n",
      "Iteration 92, loss = 0.05048404\n",
      "Iteration 93, loss = 0.05024520\n",
      "Iteration 94, loss = 0.05000463\n",
      "Iteration 95, loss = 0.04981697\n",
      "Iteration 96, loss = 0.04959601\n",
      "Iteration 97, loss = 0.04937552\n",
      "Iteration 98, loss = 0.04916358\n",
      "Iteration 99, loss = 0.04897013\n",
      "Iteration 100, loss = 0.04875133\n",
      "Iteration 101, loss = 0.04855974\n",
      "Iteration 102, loss = 0.04843471\n",
      "Iteration 103, loss = 0.04817237\n",
      "Iteration 104, loss = 0.04797714\n",
      "Iteration 105, loss = 0.04781931\n",
      "Iteration 106, loss = 0.04762328\n",
      "Iteration 107, loss = 0.04744826\n",
      "Iteration 108, loss = 0.04727118\n",
      "Iteration 109, loss = 0.04706399\n",
      "Iteration 110, loss = 0.04691065\n",
      "Iteration 111, loss = 0.04674834\n",
      "Iteration 112, loss = 0.04660817\n",
      "Iteration 113, loss = 0.04641505\n",
      "Iteration 114, loss = 0.04626750\n",
      "Iteration 115, loss = 0.04612897\n",
      "Iteration 116, loss = 0.04594762\n",
      "Iteration 117, loss = 0.04581330\n",
      "Iteration 118, loss = 0.04568269\n",
      "Iteration 119, loss = 0.04554762\n",
      "Iteration 120, loss = 0.04538707\n",
      "Iteration 121, loss = 0.04523142\n",
      "Iteration 122, loss = 0.04508559\n",
      "Iteration 123, loss = 0.04496549\n",
      "Iteration 124, loss = 0.04484044\n",
      "Iteration 125, loss = 0.04469674\n",
      "Iteration 126, loss = 0.04459104\n",
      "Iteration 127, loss = 0.04445010\n",
      "Iteration 128, loss = 0.04432369\n",
      "Iteration 129, loss = 0.04421230\n",
      "Iteration 130, loss = 0.04408155\n",
      "Iteration 131, loss = 0.04396912\n",
      "Iteration 132, loss = 0.04386612\n",
      "Iteration 133, loss = 0.04374019\n",
      "Iteration 134, loss = 0.04364902\n",
      "Iteration 135, loss = 0.04352067\n",
      "Iteration 136, loss = 0.04341912\n",
      "Iteration 137, loss = 0.04332069\n",
      "Iteration 138, loss = 0.04319244\n",
      "Iteration 139, loss = 0.04311754\n",
      "Iteration 140, loss = 0.04301445\n",
      "Iteration 141, loss = 0.04292911\n",
      "Iteration 142, loss = 0.04282539\n",
      "Iteration 143, loss = 0.04271923\n",
      "Iteration 144, loss = 0.04261851\n",
      "Iteration 145, loss = 0.04252715\n",
      "Iteration 146, loss = 0.04245896\n",
      "Iteration 147, loss = 0.04234933\n",
      "Iteration 148, loss = 0.04225939\n",
      "Iteration 149, loss = 0.04218221\n",
      "Iteration 150, loss = 0.04209332\n",
      "Iteration 151, loss = 0.04202079\n",
      "Iteration 152, loss = 0.04193684\n",
      "Iteration 153, loss = 0.04186249\n",
      "Iteration 154, loss = 0.04176768\n",
      "Iteration 155, loss = 0.04169351\n",
      "Iteration 156, loss = 0.04164883\n",
      "Iteration 157, loss = 0.04157358\n",
      "Iteration 158, loss = 0.04148649\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31437141\n",
      "Iteration 2, loss = 2.01267417\n",
      "Iteration 3, loss = 1.71302571\n",
      "Iteration 4, loss = 1.38206733\n",
      "Iteration 5, loss = 1.08591814\n",
      "Iteration 6, loss = 0.84976175\n",
      "Iteration 7, loss = 0.67053139\n",
      "Iteration 8, loss = 0.53777876\n",
      "Iteration 9, loss = 0.44448843\n",
      "Iteration 10, loss = 0.37486377\n",
      "Iteration 11, loss = 0.32140738\n",
      "Iteration 12, loss = 0.28118480\n",
      "Iteration 13, loss = 0.24939276\n",
      "Iteration 14, loss = 0.22291901\n",
      "Iteration 15, loss = 0.20159381\n",
      "Iteration 16, loss = 0.18381701\n",
      "Iteration 17, loss = 0.16948039\n",
      "Iteration 18, loss = 0.15698435\n",
      "Iteration 19, loss = 0.14607332\n",
      "Iteration 20, loss = 0.13737965\n",
      "Iteration 21, loss = 0.12974200\n",
      "Iteration 22, loss = 0.12252369\n",
      "Iteration 23, loss = 0.11673781\n",
      "Iteration 24, loss = 0.11188827\n",
      "Iteration 25, loss = 0.10703644\n",
      "Iteration 26, loss = 0.10299263\n",
      "Iteration 27, loss = 0.09947227\n",
      "Iteration 28, loss = 0.09658924\n",
      "Iteration 29, loss = 0.09380893\n",
      "Iteration 30, loss = 0.09121251\n",
      "Iteration 31, loss = 0.08901209\n",
      "Iteration 32, loss = 0.08697428\n",
      "Iteration 33, loss = 0.08508047\n",
      "Iteration 34, loss = 0.08343678\n",
      "Iteration 35, loss = 0.08190913\n",
      "Iteration 36, loss = 0.08049102\n",
      "Iteration 37, loss = 0.07912164\n",
      "Iteration 38, loss = 0.07787211\n",
      "Iteration 39, loss = 0.07677120\n",
      "Iteration 40, loss = 0.07567487\n",
      "Iteration 41, loss = 0.07466171\n",
      "Iteration 42, loss = 0.07369737\n",
      "Iteration 43, loss = 0.07278414\n",
      "Iteration 44, loss = 0.07193724\n",
      "Iteration 45, loss = 0.07110584\n",
      "Iteration 46, loss = 0.07032307\n",
      "Iteration 47, loss = 0.06960677\n",
      "Iteration 48, loss = 0.06887659\n",
      "Iteration 49, loss = 0.06820251\n",
      "Iteration 50, loss = 0.06753886\n",
      "Iteration 51, loss = 0.06691361\n",
      "Iteration 52, loss = 0.06628944\n",
      "Iteration 53, loss = 0.06573021\n",
      "Iteration 54, loss = 0.06517547\n",
      "Iteration 55, loss = 0.06462266\n",
      "Iteration 56, loss = 0.06409236\n",
      "Iteration 57, loss = 0.06353898\n",
      "Iteration 58, loss = 0.06306507\n",
      "Iteration 59, loss = 0.06255710\n",
      "Iteration 60, loss = 0.06208424\n",
      "Iteration 61, loss = 0.06164518\n",
      "Iteration 62, loss = 0.06118939\n",
      "Iteration 63, loss = 0.06075487\n",
      "Iteration 64, loss = 0.06033090\n",
      "Iteration 65, loss = 0.05991865\n",
      "Iteration 66, loss = 0.05955028\n",
      "Iteration 67, loss = 0.05914549\n",
      "Iteration 68, loss = 0.05874131\n",
      "Iteration 69, loss = 0.05837153\n",
      "Iteration 70, loss = 0.05800597\n",
      "Iteration 71, loss = 0.05767380\n",
      "Iteration 72, loss = 0.05732410\n",
      "Iteration 73, loss = 0.05698471\n",
      "Iteration 74, loss = 0.05663252\n",
      "Iteration 75, loss = 0.05626963\n",
      "Iteration 76, loss = 0.05595499\n",
      "Iteration 77, loss = 0.05565100\n",
      "Iteration 78, loss = 0.05532414\n",
      "Iteration 79, loss = 0.05504221\n",
      "Iteration 80, loss = 0.05473195\n",
      "Iteration 81, loss = 0.05443405\n",
      "Iteration 82, loss = 0.05414592\n",
      "Iteration 83, loss = 0.05387832\n",
      "Iteration 84, loss = 0.05359460\n",
      "Iteration 85, loss = 0.05333009\n",
      "Iteration 86, loss = 0.05303890\n",
      "Iteration 87, loss = 0.05278320\n",
      "Iteration 88, loss = 0.05251243\n",
      "Iteration 89, loss = 0.05224545\n",
      "Iteration 90, loss = 0.05201264\n",
      "Iteration 91, loss = 0.05177742\n",
      "Iteration 92, loss = 0.05154754\n",
      "Iteration 93, loss = 0.05128667\n",
      "Iteration 94, loss = 0.05107356\n",
      "Iteration 95, loss = 0.05084769\n",
      "Iteration 96, loss = 0.05063932\n",
      "Iteration 97, loss = 0.05041247\n",
      "Iteration 98, loss = 0.05019350\n",
      "Iteration 99, loss = 0.04999146\n",
      "Iteration 100, loss = 0.04978163\n",
      "Iteration 101, loss = 0.04958242\n",
      "Iteration 102, loss = 0.04941699\n",
      "Iteration 103, loss = 0.04919146\n",
      "Iteration 104, loss = 0.04899252\n",
      "Iteration 105, loss = 0.04879904\n",
      "Iteration 106, loss = 0.04863558\n",
      "Iteration 107, loss = 0.04844381\n",
      "Iteration 108, loss = 0.04826923\n",
      "Iteration 109, loss = 0.04808597\n",
      "Iteration 110, loss = 0.04790980\n",
      "Iteration 111, loss = 0.04774446\n",
      "Iteration 112, loss = 0.04762168\n",
      "Iteration 113, loss = 0.04742584\n",
      "Iteration 114, loss = 0.04729452\n",
      "Iteration 115, loss = 0.04711699\n",
      "Iteration 116, loss = 0.04694554\n",
      "Iteration 117, loss = 0.04678891\n",
      "Iteration 118, loss = 0.04663220\n",
      "Iteration 119, loss = 0.04650669\n",
      "Iteration 120, loss = 0.04634782\n",
      "Iteration 121, loss = 0.04619085\n",
      "Iteration 122, loss = 0.04604435\n",
      "Iteration 123, loss = 0.04592038\n",
      "Iteration 124, loss = 0.04578257\n",
      "Iteration 125, loss = 0.04564107\n",
      "Iteration 126, loss = 0.04554594\n",
      "Iteration 127, loss = 0.04541703\n",
      "Iteration 128, loss = 0.04527308\n",
      "Iteration 129, loss = 0.04516843\n",
      "Iteration 130, loss = 0.04503383\n",
      "Iteration 131, loss = 0.04493094\n",
      "Iteration 132, loss = 0.04480300\n",
      "Iteration 133, loss = 0.04468244\n",
      "Iteration 134, loss = 0.04459229\n",
      "Iteration 135, loss = 0.04445784\n",
      "Iteration 136, loss = 0.04434724\n",
      "Iteration 137, loss = 0.04426301\n",
      "Iteration 138, loss = 0.04412158\n",
      "Iteration 139, loss = 0.04405125\n",
      "Iteration 140, loss = 0.04393957\n",
      "Iteration 141, loss = 0.04386390\n",
      "Iteration 142, loss = 0.04373647\n",
      "Iteration 143, loss = 0.04365046\n",
      "Iteration 144, loss = 0.04355373\n",
      "Iteration 145, loss = 0.04346077\n",
      "Iteration 146, loss = 0.04342492\n",
      "Iteration 147, loss = 0.04327055\n",
      "Iteration 148, loss = 0.04316758\n",
      "Iteration 149, loss = 0.04308088\n",
      "Iteration 150, loss = 0.04298775\n",
      "Iteration 151, loss = 0.04291495\n",
      "Iteration 152, loss = 0.04282628\n",
      "Iteration 153, loss = 0.04274792\n",
      "Iteration 154, loss = 0.04265492\n",
      "Iteration 155, loss = 0.04258449\n",
      "Iteration 156, loss = 0.04253565\n",
      "Iteration 157, loss = 0.04244107\n",
      "Iteration 158, loss = 0.04237986\n",
      "Iteration 159, loss = 0.04231513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30794163\n",
      "Iteration 2, loss = 1.99684189\n",
      "Iteration 3, loss = 1.68971343\n",
      "Iteration 4, loss = 1.35383271\n",
      "Iteration 5, loss = 1.04725278\n",
      "Iteration 6, loss = 0.80586284\n",
      "Iteration 7, loss = 0.62356070\n",
      "Iteration 8, loss = 0.49507535\n",
      "Iteration 9, loss = 0.40034001\n",
      "Iteration 10, loss = 0.33420141\n",
      "Iteration 11, loss = 0.28626425\n",
      "Iteration 12, loss = 0.24824089\n",
      "Iteration 13, loss = 0.22060711\n",
      "Iteration 14, loss = 0.19667115\n",
      "Iteration 15, loss = 0.17845467\n",
      "Iteration 16, loss = 0.16297419\n",
      "Iteration 17, loss = 0.15087635\n",
      "Iteration 18, loss = 0.14040052\n",
      "Iteration 19, loss = 0.13116909\n",
      "Iteration 20, loss = 0.12384663\n",
      "Iteration 21, loss = 0.11751191\n",
      "Iteration 22, loss = 0.11193532\n",
      "Iteration 23, loss = 0.10685123\n",
      "Iteration 24, loss = 0.10260564\n",
      "Iteration 25, loss = 0.09871885\n",
      "Iteration 26, loss = 0.09543014\n",
      "Iteration 27, loss = 0.09255339\n",
      "Iteration 28, loss = 0.08996866\n",
      "Iteration 29, loss = 0.08755393\n",
      "Iteration 30, loss = 0.08547675\n",
      "Iteration 31, loss = 0.08355749\n",
      "Iteration 32, loss = 0.08183165\n",
      "Iteration 33, loss = 0.08020832\n",
      "Iteration 34, loss = 0.07879690\n",
      "Iteration 35, loss = 0.07739984\n",
      "Iteration 36, loss = 0.07612348\n",
      "Iteration 37, loss = 0.07493172\n",
      "Iteration 38, loss = 0.07380394\n",
      "Iteration 39, loss = 0.07283688\n",
      "Iteration 40, loss = 0.07182975\n",
      "Iteration 41, loss = 0.07091145\n",
      "Iteration 42, loss = 0.07004916\n",
      "Iteration 43, loss = 0.06920541\n",
      "Iteration 44, loss = 0.06841407\n",
      "Iteration 45, loss = 0.06767522\n",
      "Iteration 46, loss = 0.06701645\n",
      "Iteration 47, loss = 0.06631116\n",
      "Iteration 48, loss = 0.06562597\n",
      "Iteration 49, loss = 0.06500318\n",
      "Iteration 50, loss = 0.06438561\n",
      "Iteration 51, loss = 0.06379369\n",
      "Iteration 52, loss = 0.06319968\n",
      "Iteration 53, loss = 0.06268804\n",
      "Iteration 54, loss = 0.06214111\n",
      "Iteration 55, loss = 0.06159145\n",
      "Iteration 56, loss = 0.06111937\n",
      "Iteration 57, loss = 0.06060883\n",
      "Iteration 58, loss = 0.06012785\n",
      "Iteration 59, loss = 0.05965300\n",
      "Iteration 60, loss = 0.05920894\n",
      "Iteration 61, loss = 0.05876898\n",
      "Iteration 62, loss = 0.05834297\n",
      "Iteration 63, loss = 0.05796172\n",
      "Iteration 64, loss = 0.05753318\n",
      "Iteration 65, loss = 0.05710595\n",
      "Iteration 66, loss = 0.05676287\n",
      "Iteration 67, loss = 0.05639302\n",
      "Iteration 68, loss = 0.05599080\n",
      "Iteration 69, loss = 0.05562171\n",
      "Iteration 70, loss = 0.05526326\n",
      "Iteration 71, loss = 0.05492093\n",
      "Iteration 72, loss = 0.05459531\n",
      "Iteration 73, loss = 0.05426734\n",
      "Iteration 74, loss = 0.05392354\n",
      "Iteration 75, loss = 0.05359936\n",
      "Iteration 76, loss = 0.05329571\n",
      "Iteration 77, loss = 0.05301847\n",
      "Iteration 78, loss = 0.05268833\n",
      "Iteration 79, loss = 0.05237725\n",
      "Iteration 80, loss = 0.05208580\n",
      "Iteration 81, loss = 0.05179380\n",
      "Iteration 82, loss = 0.05152842\n",
      "Iteration 83, loss = 0.05125624\n",
      "Iteration 84, loss = 0.05098978\n",
      "Iteration 85, loss = 0.05071398\n",
      "Iteration 86, loss = 0.05045663\n",
      "Iteration 87, loss = 0.05018377\n",
      "Iteration 88, loss = 0.04993984\n",
      "Iteration 89, loss = 0.04967941\n",
      "Iteration 90, loss = 0.04944576\n",
      "Iteration 91, loss = 0.04920413\n",
      "Iteration 92, loss = 0.04897149\n",
      "Iteration 93, loss = 0.04874549\n",
      "Iteration 94, loss = 0.04852805\n",
      "Iteration 95, loss = 0.04829665\n",
      "Iteration 96, loss = 0.04809886\n",
      "Iteration 97, loss = 0.04788059\n",
      "Iteration 98, loss = 0.04767265\n",
      "Iteration 99, loss = 0.04746381\n",
      "Iteration 100, loss = 0.04726348\n",
      "Iteration 101, loss = 0.04707372\n",
      "Iteration 102, loss = 0.04687020\n",
      "Iteration 103, loss = 0.04667116\n",
      "Iteration 104, loss = 0.04648970\n",
      "Iteration 105, loss = 0.04631150\n",
      "Iteration 106, loss = 0.04612998\n",
      "Iteration 107, loss = 0.04596413\n",
      "Iteration 108, loss = 0.04579044\n",
      "Iteration 109, loss = 0.04562553\n",
      "Iteration 110, loss = 0.04544012\n",
      "Iteration 111, loss = 0.04527701\n",
      "Iteration 112, loss = 0.04510866\n",
      "Iteration 113, loss = 0.04497422\n",
      "Iteration 114, loss = 0.04483284\n",
      "Iteration 115, loss = 0.04467190\n",
      "Iteration 116, loss = 0.04449934\n",
      "Iteration 117, loss = 0.04435806\n",
      "Iteration 118, loss = 0.04419446\n",
      "Iteration 119, loss = 0.04406638\n",
      "Iteration 120, loss = 0.04390939\n",
      "Iteration 121, loss = 0.04375176\n",
      "Iteration 122, loss = 0.04362707\n",
      "Iteration 123, loss = 0.04349697\n",
      "Iteration 124, loss = 0.04337235\n",
      "Iteration 125, loss = 0.04324902\n",
      "Iteration 126, loss = 0.04314363\n",
      "Iteration 127, loss = 0.04301058\n",
      "Iteration 128, loss = 0.04286912\n",
      "Iteration 129, loss = 0.04277157\n",
      "Iteration 130, loss = 0.04264658\n",
      "Iteration 131, loss = 0.04255853\n",
      "Iteration 132, loss = 0.04241757\n",
      "Iteration 133, loss = 0.04229561\n",
      "Iteration 134, loss = 0.04218873\n",
      "Iteration 135, loss = 0.04207891\n",
      "Iteration 136, loss = 0.04198071\n",
      "Iteration 137, loss = 0.04189000\n",
      "Iteration 138, loss = 0.04177563\n",
      "Iteration 139, loss = 0.04171225\n",
      "Iteration 140, loss = 0.04158566\n",
      "Iteration 141, loss = 0.04147949\n",
      "Iteration 142, loss = 0.04138101\n",
      "Iteration 143, loss = 0.04129385\n",
      "Iteration 144, loss = 0.04121143\n",
      "Iteration 145, loss = 0.04113440\n",
      "Iteration 146, loss = 0.04104549\n",
      "Iteration 147, loss = 0.04093475\n",
      "Iteration 148, loss = 0.04084148\n",
      "Iteration 149, loss = 0.04075992\n",
      "Iteration 150, loss = 0.04066948\n",
      "Iteration 151, loss = 0.04060096\n",
      "Iteration 152, loss = 0.04050517\n",
      "Iteration 153, loss = 0.04044655\n",
      "Iteration 154, loss = 0.04036185\n",
      "Iteration 155, loss = 0.04028344\n",
      "Iteration 156, loss = 0.04021043\n",
      "Iteration 157, loss = 0.04016449\n",
      "Iteration 158, loss = 0.04007812\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.44092794\n",
      "Iteration 2, loss = 2.40913490\n",
      "Iteration 3, loss = 2.37281960\n",
      "Iteration 4, loss = 2.33879019\n",
      "Iteration 5, loss = 2.31118292\n",
      "Iteration 6, loss = 2.28651149\n",
      "Iteration 7, loss = 2.26429741\n",
      "Iteration 8, loss = 2.24383782\n",
      "Iteration 9, loss = 2.22340568\n",
      "Iteration 10, loss = 2.20361317\n",
      "Iteration 11, loss = 2.18372604\n",
      "Iteration 12, loss = 2.16365274\n",
      "Iteration 13, loss = 2.14320322\n",
      "Iteration 14, loss = 2.12244754\n",
      "Iteration 15, loss = 2.10133119\n",
      "Iteration 16, loss = 2.07988001\n",
      "Iteration 17, loss = 2.05768416\n",
      "Iteration 18, loss = 2.03531583\n",
      "Iteration 19, loss = 2.01223246\n",
      "Iteration 20, loss = 1.98864749\n",
      "Iteration 21, loss = 1.96496560\n",
      "Iteration 22, loss = 1.94062473\n",
      "Iteration 23, loss = 1.91589289\n",
      "Iteration 24, loss = 1.89053730\n",
      "Iteration 25, loss = 1.86480172\n",
      "Iteration 26, loss = 1.83879263\n",
      "Iteration 27, loss = 1.81261122\n",
      "Iteration 28, loss = 1.78634886\n",
      "Iteration 29, loss = 1.75972948\n",
      "Iteration 30, loss = 1.73305845\n",
      "Iteration 31, loss = 1.70635473\n",
      "Iteration 32, loss = 1.67974177\n",
      "Iteration 33, loss = 1.65329628\n",
      "Iteration 34, loss = 1.62681510\n",
      "Iteration 35, loss = 1.60014738\n",
      "Iteration 36, loss = 1.57393323\n",
      "Iteration 37, loss = 1.54809352\n",
      "Iteration 38, loss = 1.52207654\n",
      "Iteration 39, loss = 1.49629149\n",
      "Iteration 40, loss = 1.47084669\n",
      "Iteration 41, loss = 1.44580399\n",
      "Iteration 42, loss = 1.42092539\n",
      "Iteration 43, loss = 1.39657314\n",
      "Iteration 44, loss = 1.37214336\n",
      "Iteration 45, loss = 1.34845569\n",
      "Iteration 46, loss = 1.32492210\n",
      "Iteration 47, loss = 1.30187897\n",
      "Iteration 48, loss = 1.27919871\n",
      "Iteration 49, loss = 1.25683477\n",
      "Iteration 50, loss = 1.23493778\n",
      "Iteration 51, loss = 1.21324324\n",
      "Iteration 52, loss = 1.19217798\n",
      "Iteration 53, loss = 1.17141923\n",
      "Iteration 54, loss = 1.15114701\n",
      "Iteration 55, loss = 1.13093270\n",
      "Iteration 56, loss = 1.11129101\n",
      "Iteration 57, loss = 1.09241861\n",
      "Iteration 58, loss = 1.07362155\n",
      "Iteration 59, loss = 1.05541209\n",
      "Iteration 60, loss = 1.03762483\n",
      "Iteration 61, loss = 1.02027288\n",
      "Iteration 62, loss = 1.00295874\n",
      "Iteration 63, loss = 0.98619384\n",
      "Iteration 64, loss = 0.96991959\n",
      "Iteration 65, loss = 0.95396398\n",
      "Iteration 66, loss = 0.93844104\n",
      "Iteration 67, loss = 0.92344717\n",
      "Iteration 68, loss = 0.90896737\n",
      "Iteration 69, loss = 0.89459955\n",
      "Iteration 70, loss = 0.88037535\n",
      "Iteration 71, loss = 0.86669895\n",
      "Iteration 72, loss = 0.85336590\n",
      "Iteration 73, loss = 0.84040409\n",
      "Iteration 74, loss = 0.82764727\n",
      "Iteration 75, loss = 0.81533105\n",
      "Iteration 76, loss = 0.80330473\n",
      "Iteration 77, loss = 0.79164379\n",
      "Iteration 78, loss = 0.78011943\n",
      "Iteration 79, loss = 0.76890040\n",
      "Iteration 80, loss = 0.75808603\n",
      "Iteration 81, loss = 0.74734655\n",
      "Iteration 82, loss = 0.73675382\n",
      "Iteration 83, loss = 0.72662896\n",
      "Iteration 84, loss = 0.71693456\n",
      "Iteration 85, loss = 0.70713366\n",
      "Iteration 86, loss = 0.69771604\n",
      "Iteration 87, loss = 0.68846494\n",
      "Iteration 88, loss = 0.67934546\n",
      "Iteration 89, loss = 0.67070380\n",
      "Iteration 90, loss = 0.66199338\n",
      "Iteration 91, loss = 0.65353901\n",
      "Iteration 92, loss = 0.64538893\n",
      "Iteration 93, loss = 0.63728210\n",
      "Iteration 94, loss = 0.62948392\n",
      "Iteration 95, loss = 0.62193098\n",
      "Iteration 96, loss = 0.61448510\n",
      "Iteration 97, loss = 0.60741013\n",
      "Iteration 98, loss = 0.60026491\n",
      "Iteration 99, loss = 0.59345182\n",
      "Iteration 100, loss = 0.58645604\n",
      "Iteration 101, loss = 0.57991665\n",
      "Iteration 102, loss = 0.57334748\n",
      "Iteration 103, loss = 0.56700934\n",
      "Iteration 104, loss = 0.56096332\n",
      "Iteration 105, loss = 0.55481595\n",
      "Iteration 106, loss = 0.54885246\n",
      "Iteration 107, loss = 0.54296061\n",
      "Iteration 108, loss = 0.53729912\n",
      "Iteration 109, loss = 0.53184529\n",
      "Iteration 110, loss = 0.52634730\n",
      "Iteration 111, loss = 0.52100706\n",
      "Iteration 112, loss = 0.51587517\n",
      "Iteration 113, loss = 0.51064950\n",
      "Iteration 114, loss = 0.50554720\n",
      "Iteration 115, loss = 0.50068657\n",
      "Iteration 116, loss = 0.49581792\n",
      "Iteration 117, loss = 0.49109995\n",
      "Iteration 118, loss = 0.48652135\n",
      "Iteration 119, loss = 0.48185771\n",
      "Iteration 120, loss = 0.47727894\n",
      "Iteration 121, loss = 0.47296229\n",
      "Iteration 122, loss = 0.46871209\n",
      "Iteration 123, loss = 0.46447324\n",
      "Iteration 124, loss = 0.46020598\n",
      "Iteration 125, loss = 0.45619017\n",
      "Iteration 126, loss = 0.45209060\n",
      "Iteration 127, loss = 0.44815303\n",
      "Iteration 128, loss = 0.44431606\n",
      "Iteration 129, loss = 0.44047999\n",
      "Iteration 130, loss = 0.43681514\n",
      "Iteration 131, loss = 0.43311490\n",
      "Iteration 132, loss = 0.42956621\n",
      "Iteration 133, loss = 0.42598893\n",
      "Iteration 134, loss = 0.42259100\n",
      "Iteration 135, loss = 0.41914131\n",
      "Iteration 136, loss = 0.41580411\n",
      "Iteration 137, loss = 0.41256380\n",
      "Iteration 138, loss = 0.40928467\n",
      "Iteration 139, loss = 0.40598710\n",
      "Iteration 140, loss = 0.40287265\n",
      "Iteration 141, loss = 0.39968208\n",
      "Iteration 142, loss = 0.39673705\n",
      "Iteration 143, loss = 0.39366905\n",
      "Iteration 144, loss = 0.39072088\n",
      "Iteration 145, loss = 0.38783145\n",
      "Iteration 146, loss = 0.38506250\n",
      "Iteration 147, loss = 0.38226155\n",
      "Iteration 148, loss = 0.37950644\n",
      "Iteration 149, loss = 0.37681397\n",
      "Iteration 150, loss = 0.37416912\n",
      "Iteration 151, loss = 0.37145793\n",
      "Iteration 152, loss = 0.36889435\n",
      "Iteration 153, loss = 0.36629209\n",
      "Iteration 154, loss = 0.36375479\n",
      "Iteration 155, loss = 0.36122581\n",
      "Iteration 156, loss = 0.35871349\n",
      "Iteration 157, loss = 0.35628364\n",
      "Iteration 158, loss = 0.35380172\n",
      "Iteration 159, loss = 0.35154337\n",
      "Iteration 160, loss = 0.34917732\n",
      "Iteration 161, loss = 0.34687984\n",
      "Iteration 162, loss = 0.34456422\n",
      "Iteration 163, loss = 0.34240236\n",
      "Iteration 164, loss = 0.34024206\n",
      "Iteration 165, loss = 0.33797034\n",
      "Iteration 166, loss = 0.33574779\n",
      "Iteration 167, loss = 0.33363707\n",
      "Iteration 168, loss = 0.33154806\n",
      "Iteration 169, loss = 0.32948622\n",
      "Iteration 170, loss = 0.32744777\n",
      "Iteration 171, loss = 0.32544623\n",
      "Iteration 172, loss = 0.32346263\n",
      "Iteration 173, loss = 0.32159997\n",
      "Iteration 174, loss = 0.31958169\n",
      "Iteration 175, loss = 0.31768396\n",
      "Iteration 176, loss = 0.31576166\n",
      "Iteration 177, loss = 0.31395196\n",
      "Iteration 178, loss = 0.31213803\n",
      "Iteration 179, loss = 0.31020964\n",
      "Iteration 180, loss = 0.30845816\n",
      "Iteration 181, loss = 0.30663630\n",
      "Iteration 182, loss = 0.30498378\n",
      "Iteration 183, loss = 0.30324606\n",
      "Iteration 184, loss = 0.30145364\n",
      "Iteration 185, loss = 0.29971694\n",
      "Iteration 186, loss = 0.29810906\n",
      "Iteration 187, loss = 0.29644327\n",
      "Iteration 188, loss = 0.29479464\n",
      "Iteration 189, loss = 0.29323525\n",
      "Iteration 190, loss = 0.29165881\n",
      "Iteration 191, loss = 0.28999414\n",
      "Iteration 192, loss = 0.28846140\n",
      "Iteration 193, loss = 0.28692910\n",
      "Iteration 194, loss = 0.28541921\n",
      "Iteration 195, loss = 0.28386517\n",
      "Iteration 196, loss = 0.28242593\n",
      "Iteration 197, loss = 0.28093910\n",
      "Iteration 198, loss = 0.27947607\n",
      "Iteration 199, loss = 0.27809160\n",
      "Iteration 200, loss = 0.27663797\n",
      "Iteration 201, loss = 0.27526050\n",
      "Iteration 202, loss = 0.27387900\n",
      "Iteration 203, loss = 0.27252018\n",
      "Iteration 204, loss = 0.27105803\n",
      "Iteration 205, loss = 0.26969308\n",
      "Iteration 206, loss = 0.26832735\n",
      "Iteration 207, loss = 0.26701992\n",
      "Iteration 208, loss = 0.26572181\n",
      "Iteration 209, loss = 0.26437798\n",
      "Iteration 210, loss = 0.26316686\n",
      "Iteration 211, loss = 0.26192700\n",
      "Iteration 212, loss = 0.26063168\n",
      "Iteration 213, loss = 0.25936511\n",
      "Iteration 214, loss = 0.25812338\n",
      "Iteration 215, loss = 0.25688542\n",
      "Iteration 216, loss = 0.25567799\n",
      "Iteration 217, loss = 0.25445767\n",
      "Iteration 218, loss = 0.25329895\n",
      "Iteration 219, loss = 0.25213214\n",
      "Iteration 220, loss = 0.25095484\n",
      "Iteration 221, loss = 0.24979876\n",
      "Iteration 222, loss = 0.24867611\n",
      "Iteration 223, loss = 0.24755616\n",
      "Iteration 224, loss = 0.24642517\n",
      "Iteration 225, loss = 0.24534844\n",
      "Iteration 226, loss = 0.24424424\n",
      "Iteration 227, loss = 0.24314326\n",
      "Iteration 228, loss = 0.24205845\n",
      "Iteration 229, loss = 0.24101334\n",
      "Iteration 230, loss = 0.24002265\n",
      "Iteration 231, loss = 0.23889873\n",
      "Iteration 232, loss = 0.23787145\n",
      "Iteration 233, loss = 0.23682565\n",
      "Iteration 234, loss = 0.23579550\n",
      "Iteration 235, loss = 0.23475999\n",
      "Iteration 236, loss = 0.23377265\n",
      "Iteration 237, loss = 0.23283151\n",
      "Iteration 238, loss = 0.23177745\n",
      "Iteration 239, loss = 0.23078631\n",
      "Iteration 240, loss = 0.22985827\n",
      "Iteration 241, loss = 0.22894553\n",
      "Iteration 242, loss = 0.22796141\n",
      "Iteration 243, loss = 0.22706371\n",
      "Iteration 244, loss = 0.22609808\n",
      "Iteration 245, loss = 0.22516479\n",
      "Iteration 246, loss = 0.22420685\n",
      "Iteration 247, loss = 0.22329843\n",
      "Iteration 248, loss = 0.22236838\n",
      "Iteration 249, loss = 0.22150144\n",
      "Iteration 250, loss = 0.22062895\n",
      "Iteration 251, loss = 0.21979553\n",
      "Iteration 252, loss = 0.21891708\n",
      "Iteration 253, loss = 0.21805775\n",
      "Iteration 254, loss = 0.21712710\n",
      "Iteration 255, loss = 0.21630769\n",
      "Iteration 256, loss = 0.21543177\n",
      "Iteration 257, loss = 0.21456036\n",
      "Iteration 258, loss = 0.21369728\n",
      "Iteration 259, loss = 0.21292597\n",
      "Iteration 260, loss = 0.21214597\n",
      "Iteration 261, loss = 0.21135071\n",
      "Iteration 262, loss = 0.21052073\n",
      "Iteration 263, loss = 0.20974164\n",
      "Iteration 264, loss = 0.20891876\n",
      "Iteration 265, loss = 0.20818417\n",
      "Iteration 266, loss = 0.20737635\n",
      "Iteration 267, loss = 0.20665972\n",
      "Iteration 268, loss = 0.20588117\n",
      "Iteration 269, loss = 0.20506261\n",
      "Iteration 270, loss = 0.20423219\n",
      "Iteration 271, loss = 0.20347119\n",
      "Iteration 272, loss = 0.20276255\n",
      "Iteration 273, loss = 0.20203531\n",
      "Iteration 274, loss = 0.20133699\n",
      "Iteration 275, loss = 0.20062472\n",
      "Iteration 276, loss = 0.19990697\n",
      "Iteration 277, loss = 0.19915365\n",
      "Iteration 278, loss = 0.19839796\n",
      "Iteration 279, loss = 0.19773710\n",
      "Iteration 280, loss = 0.19702576\n",
      "Iteration 281, loss = 0.19635993\n",
      "Iteration 282, loss = 0.19566827\n",
      "Iteration 283, loss = 0.19499293\n",
      "Iteration 284, loss = 0.19434183\n",
      "Iteration 285, loss = 0.19367537\n",
      "Iteration 286, loss = 0.19304496\n",
      "Iteration 287, loss = 0.19240548\n",
      "Iteration 288, loss = 0.19174251\n",
      "Iteration 289, loss = 0.19106762\n",
      "Iteration 290, loss = 0.19036995\n",
      "Iteration 291, loss = 0.18968961\n",
      "Iteration 292, loss = 0.18907829\n",
      "Iteration 293, loss = 0.18843386\n",
      "Iteration 294, loss = 0.18782696\n",
      "Iteration 295, loss = 0.18724289\n",
      "Iteration 296, loss = 0.18663774\n",
      "Iteration 297, loss = 0.18602045\n",
      "Iteration 298, loss = 0.18542014\n",
      "Iteration 299, loss = 0.18481337\n",
      "Iteration 300, loss = 0.18420572\n",
      "Iteration 301, loss = 0.18356129\n",
      "Iteration 302, loss = 0.18300671\n",
      "Iteration 303, loss = 0.18239355\n",
      "Iteration 304, loss = 0.18182524\n",
      "Iteration 305, loss = 0.18121556\n",
      "Iteration 306, loss = 0.18064799\n",
      "Iteration 307, loss = 0.18006772\n",
      "Iteration 308, loss = 0.17951208\n",
      "Iteration 309, loss = 0.17893829\n",
      "Iteration 310, loss = 0.17840804\n",
      "Iteration 311, loss = 0.17784096\n",
      "Iteration 312, loss = 0.17733097\n",
      "Iteration 313, loss = 0.17678621\n",
      "Iteration 314, loss = 0.17626711\n",
      "Iteration 315, loss = 0.17572845\n",
      "Iteration 316, loss = 0.17516993\n",
      "Iteration 317, loss = 0.17467951\n",
      "Iteration 318, loss = 0.17413568\n",
      "Iteration 319, loss = 0.17362964\n",
      "Iteration 320, loss = 0.17308790\n",
      "Iteration 321, loss = 0.17253172\n",
      "Iteration 322, loss = 0.17200558\n",
      "Iteration 323, loss = 0.17154190\n",
      "Iteration 324, loss = 0.17102188\n",
      "Iteration 325, loss = 0.17054758\n",
      "Iteration 326, loss = 0.17003866\n",
      "Iteration 327, loss = 0.16951587\n",
      "Iteration 328, loss = 0.16912182\n",
      "Iteration 329, loss = 0.16857885\n",
      "Iteration 330, loss = 0.16804226\n",
      "Iteration 331, loss = 0.16757540\n",
      "Iteration 332, loss = 0.16709328\n",
      "Iteration 333, loss = 0.16663462\n",
      "Iteration 334, loss = 0.16615880\n",
      "Iteration 335, loss = 0.16570008\n",
      "Iteration 336, loss = 0.16525279\n",
      "Iteration 337, loss = 0.16477668\n",
      "Iteration 338, loss = 0.16436641\n",
      "Iteration 339, loss = 0.16388632\n",
      "Iteration 340, loss = 0.16342817\n",
      "Iteration 341, loss = 0.16298697\n",
      "Iteration 342, loss = 0.16258186\n",
      "Iteration 343, loss = 0.16210631\n",
      "Iteration 344, loss = 0.16164620\n",
      "Iteration 345, loss = 0.16120430\n",
      "Iteration 346, loss = 0.16073427\n",
      "Iteration 347, loss = 0.16030676\n",
      "Iteration 348, loss = 0.15981707\n",
      "Iteration 349, loss = 0.15937406\n",
      "Iteration 350, loss = 0.15895260\n",
      "Iteration 351, loss = 0.15854370\n",
      "Iteration 352, loss = 0.15812238\n",
      "Iteration 353, loss = 0.15771756\n",
      "Iteration 354, loss = 0.15732056\n",
      "Iteration 355, loss = 0.15686057\n",
      "Iteration 356, loss = 0.15646456\n",
      "Iteration 357, loss = 0.15606745\n",
      "Iteration 358, loss = 0.15566370\n",
      "Iteration 359, loss = 0.15523619\n",
      "Iteration 360, loss = 0.15485177\n",
      "Iteration 361, loss = 0.15445159\n",
      "Iteration 362, loss = 0.15404161\n",
      "Iteration 363, loss = 0.15365104\n",
      "Iteration 364, loss = 0.15324957\n",
      "Iteration 365, loss = 0.15289651\n",
      "Iteration 366, loss = 0.15246607\n",
      "Iteration 367, loss = 0.15213033\n",
      "Iteration 368, loss = 0.15171949\n",
      "Iteration 369, loss = 0.15138023\n",
      "Iteration 370, loss = 0.15098625\n",
      "Iteration 371, loss = 0.15066715\n",
      "Iteration 372, loss = 0.15027279\n",
      "Iteration 373, loss = 0.14989685\n",
      "Iteration 374, loss = 0.14952770\n",
      "Iteration 375, loss = 0.14917123\n",
      "Iteration 376, loss = 0.14881147\n",
      "Iteration 377, loss = 0.14846043\n",
      "Iteration 378, loss = 0.14809216\n",
      "Iteration 379, loss = 0.14774063\n",
      "Iteration 380, loss = 0.14735944\n",
      "Iteration 381, loss = 0.14702679\n",
      "Iteration 382, loss = 0.14666071\n",
      "Iteration 383, loss = 0.14631693\n",
      "Iteration 384, loss = 0.14597418\n",
      "Iteration 385, loss = 0.14560773\n",
      "Iteration 386, loss = 0.14530688\n",
      "Iteration 387, loss = 0.14495201\n",
      "Iteration 388, loss = 0.14460041\n",
      "Iteration 389, loss = 0.14427034\n",
      "Iteration 390, loss = 0.14394627\n",
      "Iteration 391, loss = 0.14360725\n",
      "Iteration 392, loss = 0.14327462\n",
      "Iteration 393, loss = 0.14294274\n",
      "Iteration 394, loss = 0.14264300\n",
      "Iteration 395, loss = 0.14229829\n",
      "Iteration 396, loss = 0.14198445\n",
      "Iteration 397, loss = 0.14168725\n",
      "Iteration 398, loss = 0.14133779\n",
      "Iteration 399, loss = 0.14103531\n",
      "Iteration 400, loss = 0.14073098\n",
      "Iteration 401, loss = 0.14041362\n",
      "Iteration 402, loss = 0.14007165\n",
      "Iteration 403, loss = 0.13977935\n",
      "Iteration 404, loss = 0.13951453\n",
      "Iteration 405, loss = 0.13920040\n",
      "Iteration 406, loss = 0.13887930\n",
      "Iteration 407, loss = 0.13859129\n",
      "Iteration 408, loss = 0.13826620\n",
      "Iteration 409, loss = 0.13799684\n",
      "Iteration 410, loss = 0.13766104\n",
      "Iteration 411, loss = 0.13732850\n",
      "Iteration 412, loss = 0.13705130\n",
      "Iteration 413, loss = 0.13678936\n",
      "Iteration 414, loss = 0.13649508\n",
      "Iteration 415, loss = 0.13617866\n",
      "Iteration 416, loss = 0.13587790\n",
      "Iteration 417, loss = 0.13561326\n",
      "Iteration 418, loss = 0.13532621\n",
      "Iteration 419, loss = 0.13504783\n",
      "Iteration 420, loss = 0.13477060\n",
      "Iteration 421, loss = 0.13449022\n",
      "Iteration 422, loss = 0.13420888\n",
      "Iteration 423, loss = 0.13393765\n",
      "Iteration 424, loss = 0.13365815\n",
      "Iteration 425, loss = 0.13337287\n",
      "Iteration 426, loss = 0.13310748\n",
      "Iteration 427, loss = 0.13282860\n",
      "Iteration 428, loss = 0.13255495\n",
      "Iteration 429, loss = 0.13228068\n",
      "Iteration 430, loss = 0.13200539\n",
      "Iteration 431, loss = 0.13176334\n",
      "Iteration 432, loss = 0.13150003\n",
      "Iteration 433, loss = 0.13122607\n",
      "Iteration 434, loss = 0.13098197\n",
      "Iteration 435, loss = 0.13073448\n",
      "Iteration 436, loss = 0.13047448\n",
      "Iteration 437, loss = 0.13021371\n",
      "Iteration 438, loss = 0.12997100\n",
      "Iteration 439, loss = 0.12971243\n",
      "Iteration 440, loss = 0.12947931\n",
      "Iteration 441, loss = 0.12925268\n",
      "Iteration 442, loss = 0.12897475\n",
      "Iteration 443, loss = 0.12873182\n",
      "Iteration 444, loss = 0.12846545\n",
      "Iteration 445, loss = 0.12821292\n",
      "Iteration 446, loss = 0.12796770\n",
      "Iteration 447, loss = 0.12773228\n",
      "Iteration 448, loss = 0.12746246\n",
      "Iteration 449, loss = 0.12721454\n",
      "Iteration 450, loss = 0.12697260\n",
      "Iteration 451, loss = 0.12673485\n",
      "Iteration 452, loss = 0.12650977\n",
      "Iteration 453, loss = 0.12628469\n",
      "Iteration 454, loss = 0.12604247\n",
      "Iteration 455, loss = 0.12583440\n",
      "Iteration 456, loss = 0.12558348\n",
      "Iteration 457, loss = 0.12535867\n",
      "Iteration 458, loss = 0.12514041\n",
      "Iteration 459, loss = 0.12491814\n",
      "Iteration 460, loss = 0.12469365\n",
      "Iteration 461, loss = 0.12447440\n",
      "Iteration 462, loss = 0.12423990\n",
      "Iteration 463, loss = 0.12401869\n",
      "Iteration 464, loss = 0.12380381\n",
      "Iteration 465, loss = 0.12358380\n",
      "Iteration 466, loss = 0.12337143\n",
      "Iteration 467, loss = 0.12314776\n",
      "Iteration 468, loss = 0.12293006\n",
      "Iteration 469, loss = 0.12273023\n",
      "Iteration 470, loss = 0.12250914\n",
      "Iteration 471, loss = 0.12228617\n",
      "Iteration 472, loss = 0.12209207\n",
      "Iteration 473, loss = 0.12184835\n",
      "Iteration 474, loss = 0.12163941\n",
      "Iteration 475, loss = 0.12142998\n",
      "Iteration 476, loss = 0.12120743\n",
      "Iteration 477, loss = 0.12100540\n",
      "Iteration 478, loss = 0.12079890\n",
      "Iteration 479, loss = 0.12059540\n",
      "Iteration 480, loss = 0.12039766\n",
      "Iteration 481, loss = 0.12018432\n",
      "Iteration 482, loss = 0.11998240\n",
      "Iteration 483, loss = 0.11979442\n",
      "Iteration 484, loss = 0.11959639\n",
      "Iteration 485, loss = 0.11938739\n",
      "Iteration 486, loss = 0.11921850\n",
      "Iteration 487, loss = 0.11900480\n",
      "Iteration 488, loss = 0.11882425\n",
      "Iteration 489, loss = 0.11862463\n",
      "Iteration 490, loss = 0.11844599\n",
      "Iteration 491, loss = 0.11823846\n",
      "Iteration 492, loss = 0.11805208\n",
      "Iteration 493, loss = 0.11787420\n",
      "Iteration 494, loss = 0.11767136\n",
      "Iteration 495, loss = 0.11748484\n",
      "Iteration 496, loss = 0.11729307\n",
      "Iteration 497, loss = 0.11709932\n",
      "Iteration 498, loss = 0.11691158\n",
      "Iteration 499, loss = 0.11673394\n",
      "Iteration 500, loss = 0.11655234\n",
      "Iteration 501, loss = 0.11635917\n",
      "Iteration 502, loss = 0.11619714\n",
      "Iteration 503, loss = 0.11598429\n",
      "Iteration 504, loss = 0.11582435\n",
      "Iteration 505, loss = 0.11564831\n",
      "Iteration 506, loss = 0.11546544\n",
      "Iteration 507, loss = 0.11527285\n",
      "Iteration 508, loss = 0.11510040\n",
      "Iteration 509, loss = 0.11492600\n",
      "Iteration 510, loss = 0.11475504\n",
      "Iteration 511, loss = 0.11456942\n",
      "Iteration 512, loss = 0.11440135\n",
      "Iteration 513, loss = 0.11421498\n",
      "Iteration 514, loss = 0.11405549\n",
      "Iteration 515, loss = 0.11386583\n",
      "Iteration 516, loss = 0.11369776\n",
      "Iteration 517, loss = 0.11354217\n",
      "Iteration 518, loss = 0.11334731\n",
      "Iteration 519, loss = 0.11318157\n",
      "Iteration 520, loss = 0.11302476\n",
      "Iteration 521, loss = 0.11286006\n",
      "Iteration 522, loss = 0.11269598\n",
      "Iteration 523, loss = 0.11253489\n",
      "Iteration 524, loss = 0.11236362\n",
      "Iteration 525, loss = 0.11220505\n",
      "Iteration 526, loss = 0.11204915\n",
      "Iteration 527, loss = 0.11188860\n",
      "Iteration 528, loss = 0.11172688\n",
      "Iteration 529, loss = 0.11155377\n",
      "Iteration 530, loss = 0.11139797\n",
      "Iteration 531, loss = 0.11125316\n",
      "Iteration 532, loss = 0.11108176\n",
      "Iteration 533, loss = 0.11092638\n",
      "Iteration 534, loss = 0.11078781\n",
      "Iteration 535, loss = 0.11063305\n",
      "Iteration 536, loss = 0.11047018\n",
      "Iteration 537, loss = 0.11031946\n",
      "Iteration 538, loss = 0.11017263\n",
      "Iteration 539, loss = 0.10999974\n",
      "Iteration 540, loss = 0.10985436\n",
      "Iteration 541, loss = 0.10973227\n",
      "Iteration 542, loss = 0.10955607\n",
      "Iteration 543, loss = 0.10939963\n",
      "Iteration 544, loss = 0.10924820\n",
      "Iteration 545, loss = 0.10910519\n",
      "Iteration 546, loss = 0.10895887\n",
      "Iteration 547, loss = 0.10881443\n",
      "Iteration 548, loss = 0.10867426\n",
      "Iteration 549, loss = 0.10850084\n",
      "Iteration 550, loss = 0.10837016\n",
      "Iteration 551, loss = 0.10822211\n",
      "Iteration 552, loss = 0.10808891\n",
      "Iteration 553, loss = 0.10792754\n",
      "Iteration 554, loss = 0.10778756\n",
      "Iteration 555, loss = 0.10767048\n",
      "Iteration 556, loss = 0.10754221\n",
      "Iteration 557, loss = 0.10737235\n",
      "Iteration 558, loss = 0.10724048\n",
      "Iteration 559, loss = 0.10708597\n",
      "Iteration 560, loss = 0.10696321\n",
      "Iteration 561, loss = 0.10682160\n",
      "Iteration 562, loss = 0.10668332\n",
      "Iteration 563, loss = 0.10652908\n",
      "Iteration 564, loss = 0.10641130\n",
      "Iteration 565, loss = 0.10627208\n",
      "Iteration 566, loss = 0.10613678\n",
      "Iteration 567, loss = 0.10600865\n",
      "Iteration 568, loss = 0.10584839\n",
      "Iteration 569, loss = 0.10572105\n",
      "Iteration 570, loss = 0.10558925\n",
      "Iteration 571, loss = 0.10546096\n",
      "Iteration 572, loss = 0.10532351\n",
      "Iteration 573, loss = 0.10518977\n",
      "Iteration 574, loss = 0.10505903\n",
      "Iteration 575, loss = 0.10492170\n",
      "Iteration 576, loss = 0.10480390\n",
      "Iteration 577, loss = 0.10467548\n",
      "Iteration 578, loss = 0.10453949\n",
      "Iteration 579, loss = 0.10442041\n",
      "Iteration 580, loss = 0.10428604\n",
      "Iteration 581, loss = 0.10415394\n",
      "Iteration 582, loss = 0.10402458\n",
      "Iteration 583, loss = 0.10390422\n",
      "Iteration 584, loss = 0.10378663\n",
      "Iteration 585, loss = 0.10366651\n",
      "Iteration 586, loss = 0.10353960\n",
      "Iteration 587, loss = 0.10341251\n",
      "Iteration 588, loss = 0.10328887\n",
      "Iteration 589, loss = 0.10315929\n",
      "Iteration 590, loss = 0.10304253\n",
      "Iteration 591, loss = 0.10294918\n",
      "Iteration 592, loss = 0.10279407\n",
      "Iteration 593, loss = 0.10266574\n",
      "Iteration 594, loss = 0.10258015\n",
      "Iteration 595, loss = 0.10243561\n",
      "Iteration 596, loss = 0.10232262\n",
      "Iteration 597, loss = 0.10220532\n",
      "Iteration 598, loss = 0.10208378\n",
      "Iteration 599, loss = 0.10197595\n",
      "Iteration 600, loss = 0.10184292\n",
      "Iteration 601, loss = 0.10173068\n",
      "Iteration 602, loss = 0.10161555\n",
      "Iteration 603, loss = 0.10150219\n",
      "Iteration 604, loss = 0.10138533\n",
      "Iteration 605, loss = 0.10126801\n",
      "Iteration 606, loss = 0.10115959\n",
      "Iteration 607, loss = 0.10103746\n",
      "Iteration 608, loss = 0.10092517\n",
      "Iteration 609, loss = 0.10081883\n",
      "Iteration 610, loss = 0.10071301\n",
      "Iteration 611, loss = 0.10060352\n",
      "Iteration 612, loss = 0.10047696\n",
      "Iteration 613, loss = 0.10036691\n",
      "Iteration 614, loss = 0.10025193\n",
      "Iteration 615, loss = 0.10014317\n",
      "Iteration 616, loss = 0.10002622\n",
      "Iteration 617, loss = 0.09991292\n",
      "Iteration 618, loss = 0.09981283\n",
      "Iteration 619, loss = 0.09969240\n",
      "Iteration 620, loss = 0.09958251\n",
      "Iteration 621, loss = 0.09948543\n",
      "Iteration 622, loss = 0.09938410\n",
      "Iteration 623, loss = 0.09926231\n",
      "Iteration 624, loss = 0.09915873\n",
      "Iteration 625, loss = 0.09905852\n",
      "Iteration 626, loss = 0.09894546\n",
      "Iteration 627, loss = 0.09884261\n",
      "Iteration 628, loss = 0.09873541\n",
      "Iteration 629, loss = 0.09863909\n",
      "Iteration 630, loss = 0.09853541\n",
      "Iteration 631, loss = 0.09843413\n",
      "Iteration 632, loss = 0.09833624\n",
      "Iteration 633, loss = 0.09824589\n",
      "Iteration 634, loss = 0.09814193\n",
      "Iteration 635, loss = 0.09804402\n",
      "Iteration 636, loss = 0.09793177\n",
      "Iteration 637, loss = 0.09783242\n",
      "Iteration 638, loss = 0.09773280\n",
      "Iteration 639, loss = 0.09763429\n",
      "Iteration 640, loss = 0.09754445\n",
      "Iteration 641, loss = 0.09744313\n",
      "Iteration 642, loss = 0.09734128\n",
      "Iteration 643, loss = 0.09725386\n",
      "Iteration 644, loss = 0.09714299\n",
      "Iteration 645, loss = 0.09705001\n",
      "Iteration 646, loss = 0.09695444\n",
      "Iteration 647, loss = 0.09686115\n",
      "Iteration 648, loss = 0.09677109\n",
      "Iteration 649, loss = 0.09668718\n",
      "Iteration 650, loss = 0.09657677\n",
      "Iteration 651, loss = 0.09647783\n",
      "Iteration 652, loss = 0.09638646\n",
      "Iteration 653, loss = 0.09629446\n",
      "Iteration 654, loss = 0.09620121\n",
      "Iteration 655, loss = 0.09612273\n",
      "Iteration 656, loss = 0.09602891\n",
      "Iteration 657, loss = 0.09593680\n",
      "Iteration 658, loss = 0.09584079\n",
      "Iteration 659, loss = 0.09575823\n",
      "Iteration 660, loss = 0.09567639\n",
      "Iteration 661, loss = 0.09556157\n",
      "Iteration 662, loss = 0.09547818\n",
      "Iteration 663, loss = 0.09537519\n",
      "Iteration 664, loss = 0.09528782\n",
      "Iteration 665, loss = 0.09521035\n",
      "Iteration 666, loss = 0.09512485\n",
      "Iteration 667, loss = 0.09502604\n",
      "Iteration 668, loss = 0.09494185\n",
      "Iteration 669, loss = 0.09485736\n",
      "Iteration 670, loss = 0.09475732\n",
      "Iteration 671, loss = 0.09467437\n",
      "Iteration 672, loss = 0.09457767\n",
      "Iteration 673, loss = 0.09448881\n",
      "Iteration 674, loss = 0.09440507\n",
      "Iteration 675, loss = 0.09432175\n",
      "Iteration 676, loss = 0.09425374\n",
      "Iteration 677, loss = 0.09416380\n",
      "Iteration 678, loss = 0.09408013\n",
      "Iteration 679, loss = 0.09400003\n",
      "Iteration 680, loss = 0.09391526\n",
      "Iteration 681, loss = 0.09382636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.44100078\n",
      "Iteration 2, loss = 2.40887983\n",
      "Iteration 3, loss = 2.37168741\n",
      "Iteration 4, loss = 2.33783765\n",
      "Iteration 5, loss = 2.30967880\n",
      "Iteration 6, loss = 2.28472208\n",
      "Iteration 7, loss = 2.26216934\n",
      "Iteration 8, loss = 2.24133982\n",
      "Iteration 9, loss = 2.22056629\n",
      "Iteration 10, loss = 2.20028374\n",
      "Iteration 11, loss = 2.18003538\n",
      "Iteration 12, loss = 2.15915629\n",
      "Iteration 13, loss = 2.13818029\n",
      "Iteration 14, loss = 2.11675569\n",
      "Iteration 15, loss = 2.09515745\n",
      "Iteration 16, loss = 2.07280325\n",
      "Iteration 17, loss = 2.04992282\n",
      "Iteration 18, loss = 2.02688461\n",
      "Iteration 19, loss = 2.00338959\n",
      "Iteration 20, loss = 1.97900830\n",
      "Iteration 21, loss = 1.95444092\n",
      "Iteration 22, loss = 1.92927129\n",
      "Iteration 23, loss = 1.90377783\n",
      "Iteration 24, loss = 1.87797721\n",
      "Iteration 25, loss = 1.85187367\n",
      "Iteration 26, loss = 1.82539993\n",
      "Iteration 27, loss = 1.79870188\n",
      "Iteration 28, loss = 1.77182146\n",
      "Iteration 29, loss = 1.74461962\n",
      "Iteration 30, loss = 1.71744501\n",
      "Iteration 31, loss = 1.69003902\n",
      "Iteration 32, loss = 1.66258133\n",
      "Iteration 33, loss = 1.63538571\n",
      "Iteration 34, loss = 1.60809281\n",
      "Iteration 35, loss = 1.58055572\n",
      "Iteration 36, loss = 1.55359948\n",
      "Iteration 37, loss = 1.52683862\n",
      "Iteration 38, loss = 1.49988224\n",
      "Iteration 39, loss = 1.47329673\n",
      "Iteration 40, loss = 1.44714628\n",
      "Iteration 41, loss = 1.42124327\n",
      "Iteration 42, loss = 1.39559618\n",
      "Iteration 43, loss = 1.37021095\n",
      "Iteration 44, loss = 1.34532783\n",
      "Iteration 45, loss = 1.32104996\n",
      "Iteration 46, loss = 1.29737964\n",
      "Iteration 47, loss = 1.27395965\n",
      "Iteration 48, loss = 1.25081117\n",
      "Iteration 49, loss = 1.22798181\n",
      "Iteration 50, loss = 1.20566720\n",
      "Iteration 51, loss = 1.18374515\n",
      "Iteration 52, loss = 1.16217310\n",
      "Iteration 53, loss = 1.14127461\n",
      "Iteration 54, loss = 1.12100138\n",
      "Iteration 55, loss = 1.10067227\n",
      "Iteration 56, loss = 1.08101225\n",
      "Iteration 57, loss = 1.06182151\n",
      "Iteration 58, loss = 1.04302947\n",
      "Iteration 59, loss = 1.02487473\n",
      "Iteration 60, loss = 1.00708199\n",
      "Iteration 61, loss = 0.98970332\n",
      "Iteration 62, loss = 0.97248119\n",
      "Iteration 63, loss = 0.95580514\n",
      "Iteration 64, loss = 0.93958812\n",
      "Iteration 65, loss = 0.92372061\n",
      "Iteration 66, loss = 0.90834899\n",
      "Iteration 67, loss = 0.89317099\n",
      "Iteration 68, loss = 0.87877400\n",
      "Iteration 69, loss = 0.86456607\n",
      "Iteration 70, loss = 0.85048500\n",
      "Iteration 71, loss = 0.83711829\n",
      "Iteration 72, loss = 0.82389492\n",
      "Iteration 73, loss = 0.81116882\n",
      "Iteration 74, loss = 0.79861291\n",
      "Iteration 75, loss = 0.78651842\n",
      "Iteration 76, loss = 0.77472713\n",
      "Iteration 77, loss = 0.76317824\n",
      "Iteration 78, loss = 0.75191852\n",
      "Iteration 79, loss = 0.74118994\n",
      "Iteration 80, loss = 0.73046032\n",
      "Iteration 81, loss = 0.72017842\n",
      "Iteration 82, loss = 0.70985452\n",
      "Iteration 83, loss = 0.70005569\n",
      "Iteration 84, loss = 0.69058881\n",
      "Iteration 85, loss = 0.68108176\n",
      "Iteration 86, loss = 0.67197210\n",
      "Iteration 87, loss = 0.66309283\n",
      "Iteration 88, loss = 0.65434620\n",
      "Iteration 89, loss = 0.64597344\n",
      "Iteration 90, loss = 0.63766249\n",
      "Iteration 91, loss = 0.62967145\n",
      "Iteration 92, loss = 0.62178487\n",
      "Iteration 93, loss = 0.61407359\n",
      "Iteration 94, loss = 0.60654264\n",
      "Iteration 95, loss = 0.59932977\n",
      "Iteration 96, loss = 0.59216618\n",
      "Iteration 97, loss = 0.58518645\n",
      "Iteration 98, loss = 0.57839605\n",
      "Iteration 99, loss = 0.57168276\n",
      "Iteration 100, loss = 0.56513044\n",
      "Iteration 101, loss = 0.55876603\n",
      "Iteration 102, loss = 0.55248693\n",
      "Iteration 103, loss = 0.54642590\n",
      "Iteration 104, loss = 0.54049762\n",
      "Iteration 105, loss = 0.53453187\n",
      "Iteration 106, loss = 0.52874794\n",
      "Iteration 107, loss = 0.52310298\n",
      "Iteration 108, loss = 0.51755966\n",
      "Iteration 109, loss = 0.51227532\n",
      "Iteration 110, loss = 0.50688267\n",
      "Iteration 111, loss = 0.50171551\n",
      "Iteration 112, loss = 0.49684243\n",
      "Iteration 113, loss = 0.49182085\n",
      "Iteration 114, loss = 0.48687896\n",
      "Iteration 115, loss = 0.48215491\n",
      "Iteration 116, loss = 0.47738665\n",
      "Iteration 117, loss = 0.47278603\n",
      "Iteration 118, loss = 0.46841717\n",
      "Iteration 119, loss = 0.46400830\n",
      "Iteration 120, loss = 0.45955180\n",
      "Iteration 121, loss = 0.45541308\n",
      "Iteration 122, loss = 0.45140799\n",
      "Iteration 123, loss = 0.44721374\n",
      "Iteration 124, loss = 0.44318727\n",
      "Iteration 125, loss = 0.43920366\n",
      "Iteration 126, loss = 0.43522934\n",
      "Iteration 127, loss = 0.43139430\n",
      "Iteration 128, loss = 0.42775509\n",
      "Iteration 129, loss = 0.42416245\n",
      "Iteration 130, loss = 0.42052928\n",
      "Iteration 131, loss = 0.41697672\n",
      "Iteration 132, loss = 0.41352169\n",
      "Iteration 133, loss = 0.41002868\n",
      "Iteration 134, loss = 0.40674701\n",
      "Iteration 135, loss = 0.40343265\n",
      "Iteration 136, loss = 0.40019824\n",
      "Iteration 137, loss = 0.39707023\n",
      "Iteration 138, loss = 0.39394882\n",
      "Iteration 139, loss = 0.39077678\n",
      "Iteration 140, loss = 0.38768090\n",
      "Iteration 141, loss = 0.38467028\n",
      "Iteration 142, loss = 0.38185198\n",
      "Iteration 143, loss = 0.37891041\n",
      "Iteration 144, loss = 0.37616979\n",
      "Iteration 145, loss = 0.37330657\n",
      "Iteration 146, loss = 0.37053992\n",
      "Iteration 147, loss = 0.36794545\n",
      "Iteration 148, loss = 0.36526788\n",
      "Iteration 149, loss = 0.36260648\n",
      "Iteration 150, loss = 0.36003837\n",
      "Iteration 151, loss = 0.35743673\n",
      "Iteration 152, loss = 0.35500044\n",
      "Iteration 153, loss = 0.35251997\n",
      "Iteration 154, loss = 0.35012813\n",
      "Iteration 155, loss = 0.34767890\n",
      "Iteration 156, loss = 0.34534408\n",
      "Iteration 157, loss = 0.34294420\n",
      "Iteration 158, loss = 0.34067352\n",
      "Iteration 159, loss = 0.33856508\n",
      "Iteration 160, loss = 0.33628787\n",
      "Iteration 161, loss = 0.33412139\n",
      "Iteration 162, loss = 0.33197701\n",
      "Iteration 163, loss = 0.32982855\n",
      "Iteration 164, loss = 0.32771339\n",
      "Iteration 165, loss = 0.32560078\n",
      "Iteration 166, loss = 0.32347395\n",
      "Iteration 167, loss = 0.32148118\n",
      "Iteration 168, loss = 0.31939147\n",
      "Iteration 169, loss = 0.31743484\n",
      "Iteration 170, loss = 0.31548599\n",
      "Iteration 171, loss = 0.31349849\n",
      "Iteration 172, loss = 0.31166470\n",
      "Iteration 173, loss = 0.30985245\n",
      "Iteration 174, loss = 0.30790535\n",
      "Iteration 175, loss = 0.30616351\n",
      "Iteration 176, loss = 0.30428626\n",
      "Iteration 177, loss = 0.30258146\n",
      "Iteration 178, loss = 0.30083484\n",
      "Iteration 179, loss = 0.29904038\n",
      "Iteration 180, loss = 0.29733402\n",
      "Iteration 181, loss = 0.29561237\n",
      "Iteration 182, loss = 0.29403498\n",
      "Iteration 183, loss = 0.29237581\n",
      "Iteration 184, loss = 0.29070046\n",
      "Iteration 185, loss = 0.28907272\n",
      "Iteration 186, loss = 0.28750060\n",
      "Iteration 187, loss = 0.28597017\n",
      "Iteration 188, loss = 0.28442998\n",
      "Iteration 189, loss = 0.28293655\n",
      "Iteration 190, loss = 0.28150738\n",
      "Iteration 191, loss = 0.28001325\n",
      "Iteration 192, loss = 0.27845975\n",
      "Iteration 193, loss = 0.27703465\n",
      "Iteration 194, loss = 0.27561930\n",
      "Iteration 195, loss = 0.27415497\n",
      "Iteration 196, loss = 0.27282991\n",
      "Iteration 197, loss = 0.27137413\n",
      "Iteration 198, loss = 0.26999305\n",
      "Iteration 199, loss = 0.26861959\n",
      "Iteration 200, loss = 0.26724277\n",
      "Iteration 201, loss = 0.26593178\n",
      "Iteration 202, loss = 0.26461549\n",
      "Iteration 203, loss = 0.26338313\n",
      "Iteration 204, loss = 0.26198739\n",
      "Iteration 205, loss = 0.26064341\n",
      "Iteration 206, loss = 0.25934784\n",
      "Iteration 207, loss = 0.25816600\n",
      "Iteration 208, loss = 0.25690612\n",
      "Iteration 209, loss = 0.25567857\n",
      "Iteration 210, loss = 0.25450344\n",
      "Iteration 211, loss = 0.25333803\n",
      "Iteration 212, loss = 0.25208129\n",
      "Iteration 213, loss = 0.25090494\n",
      "Iteration 214, loss = 0.24974654\n",
      "Iteration 215, loss = 0.24861086\n",
      "Iteration 216, loss = 0.24744585\n",
      "Iteration 217, loss = 0.24635262\n",
      "Iteration 218, loss = 0.24518333\n",
      "Iteration 219, loss = 0.24413121\n",
      "Iteration 220, loss = 0.24302381\n",
      "Iteration 221, loss = 0.24191776\n",
      "Iteration 222, loss = 0.24081686\n",
      "Iteration 223, loss = 0.23979572\n",
      "Iteration 224, loss = 0.23877177\n",
      "Iteration 225, loss = 0.23776264\n",
      "Iteration 226, loss = 0.23671786\n",
      "Iteration 227, loss = 0.23565362\n",
      "Iteration 228, loss = 0.23465563\n",
      "Iteration 229, loss = 0.23363418\n",
      "Iteration 230, loss = 0.23268032\n",
      "Iteration 231, loss = 0.23161048\n",
      "Iteration 232, loss = 0.23066054\n",
      "Iteration 233, loss = 0.22967703\n",
      "Iteration 234, loss = 0.22875488\n",
      "Iteration 235, loss = 0.22775310\n",
      "Iteration 236, loss = 0.22682318\n",
      "Iteration 237, loss = 0.22591440\n",
      "Iteration 238, loss = 0.22501795\n",
      "Iteration 239, loss = 0.22407467\n",
      "Iteration 240, loss = 0.22318302\n",
      "Iteration 241, loss = 0.22233466\n",
      "Iteration 242, loss = 0.22137453\n",
      "Iteration 243, loss = 0.22050534\n",
      "Iteration 244, loss = 0.21965139\n",
      "Iteration 245, loss = 0.21875754\n",
      "Iteration 246, loss = 0.21791242\n",
      "Iteration 247, loss = 0.21703899\n",
      "Iteration 248, loss = 0.21621747\n",
      "Iteration 249, loss = 0.21545246\n",
      "Iteration 250, loss = 0.21454146\n",
      "Iteration 251, loss = 0.21373124\n",
      "Iteration 252, loss = 0.21295041\n",
      "Iteration 253, loss = 0.21212677\n",
      "Iteration 254, loss = 0.21129955\n",
      "Iteration 255, loss = 0.21048094\n",
      "Iteration 256, loss = 0.20967589\n",
      "Iteration 257, loss = 0.20889989\n",
      "Iteration 258, loss = 0.20805843\n",
      "Iteration 259, loss = 0.20731189\n",
      "Iteration 260, loss = 0.20659095\n",
      "Iteration 261, loss = 0.20584979\n",
      "Iteration 262, loss = 0.20512204\n",
      "Iteration 263, loss = 0.20432714\n",
      "Iteration 264, loss = 0.20358653\n",
      "Iteration 265, loss = 0.20292177\n",
      "Iteration 266, loss = 0.20212675\n",
      "Iteration 267, loss = 0.20145099\n",
      "Iteration 268, loss = 0.20074054\n",
      "Iteration 269, loss = 0.20001118\n",
      "Iteration 270, loss = 0.19925830\n",
      "Iteration 271, loss = 0.19858881\n",
      "Iteration 272, loss = 0.19790915\n",
      "Iteration 273, loss = 0.19721218\n",
      "Iteration 274, loss = 0.19654618\n",
      "Iteration 275, loss = 0.19587658\n",
      "Iteration 276, loss = 0.19530975\n",
      "Iteration 277, loss = 0.19456572\n",
      "Iteration 278, loss = 0.19385380\n",
      "Iteration 279, loss = 0.19328763\n",
      "Iteration 280, loss = 0.19259836\n",
      "Iteration 281, loss = 0.19196483\n",
      "Iteration 282, loss = 0.19132094\n",
      "Iteration 283, loss = 0.19071917\n",
      "Iteration 284, loss = 0.19008247\n",
      "Iteration 285, loss = 0.18944058\n",
      "Iteration 286, loss = 0.18889943\n",
      "Iteration 287, loss = 0.18831081\n",
      "Iteration 288, loss = 0.18764806\n",
      "Iteration 289, loss = 0.18703966\n",
      "Iteration 290, loss = 0.18642139\n",
      "Iteration 291, loss = 0.18581149\n",
      "Iteration 292, loss = 0.18520557\n",
      "Iteration 293, loss = 0.18459552\n",
      "Iteration 294, loss = 0.18404642\n",
      "Iteration 295, loss = 0.18348224\n",
      "Iteration 296, loss = 0.18289909\n",
      "Iteration 297, loss = 0.18230968\n",
      "Iteration 298, loss = 0.18176241\n",
      "Iteration 299, loss = 0.18118814\n",
      "Iteration 300, loss = 0.18063190\n",
      "Iteration 301, loss = 0.18005224\n",
      "Iteration 302, loss = 0.17955343\n",
      "Iteration 303, loss = 0.17895570\n",
      "Iteration 304, loss = 0.17843228\n",
      "Iteration 305, loss = 0.17786498\n",
      "Iteration 306, loss = 0.17731177\n",
      "Iteration 307, loss = 0.17676241\n",
      "Iteration 308, loss = 0.17624501\n",
      "Iteration 309, loss = 0.17569722\n",
      "Iteration 310, loss = 0.17520672\n",
      "Iteration 311, loss = 0.17468346\n",
      "Iteration 312, loss = 0.17416790\n",
      "Iteration 313, loss = 0.17371160\n",
      "Iteration 314, loss = 0.17319152\n",
      "Iteration 315, loss = 0.17270123\n",
      "Iteration 316, loss = 0.17216908\n",
      "Iteration 317, loss = 0.17170496\n",
      "Iteration 318, loss = 0.17118285\n",
      "Iteration 319, loss = 0.17068675\n",
      "Iteration 320, loss = 0.17021488\n",
      "Iteration 321, loss = 0.16970694\n",
      "Iteration 322, loss = 0.16922710\n",
      "Iteration 323, loss = 0.16875470\n",
      "Iteration 324, loss = 0.16827585\n",
      "Iteration 325, loss = 0.16779226\n",
      "Iteration 326, loss = 0.16732356\n",
      "Iteration 327, loss = 0.16682920\n",
      "Iteration 328, loss = 0.16641328\n",
      "Iteration 329, loss = 0.16594522\n",
      "Iteration 330, loss = 0.16547664\n",
      "Iteration 331, loss = 0.16503678\n",
      "Iteration 332, loss = 0.16458932\n",
      "Iteration 333, loss = 0.16413576\n",
      "Iteration 334, loss = 0.16371631\n",
      "Iteration 335, loss = 0.16328447\n",
      "Iteration 336, loss = 0.16286641\n",
      "Iteration 337, loss = 0.16243048\n",
      "Iteration 338, loss = 0.16204664\n",
      "Iteration 339, loss = 0.16156306\n",
      "Iteration 340, loss = 0.16116169\n",
      "Iteration 341, loss = 0.16073718\n",
      "Iteration 342, loss = 0.16034932\n",
      "Iteration 343, loss = 0.15993575\n",
      "Iteration 344, loss = 0.15951002\n",
      "Iteration 345, loss = 0.15908872\n",
      "Iteration 346, loss = 0.15863482\n",
      "Iteration 347, loss = 0.15823361\n",
      "Iteration 348, loss = 0.15777967\n",
      "Iteration 349, loss = 0.15737565\n",
      "Iteration 350, loss = 0.15700763\n",
      "Iteration 351, loss = 0.15663935\n",
      "Iteration 352, loss = 0.15619515\n",
      "Iteration 353, loss = 0.15581155\n",
      "Iteration 354, loss = 0.15542750\n",
      "Iteration 355, loss = 0.15499630\n",
      "Iteration 356, loss = 0.15465933\n",
      "Iteration 357, loss = 0.15425773\n",
      "Iteration 358, loss = 0.15387415\n",
      "Iteration 359, loss = 0.15347129\n",
      "Iteration 360, loss = 0.15310462\n",
      "Iteration 361, loss = 0.15271428\n",
      "Iteration 362, loss = 0.15231434\n",
      "Iteration 363, loss = 0.15200138\n",
      "Iteration 364, loss = 0.15159329\n",
      "Iteration 365, loss = 0.15127828\n",
      "Iteration 366, loss = 0.15087489\n",
      "Iteration 367, loss = 0.15053359\n",
      "Iteration 368, loss = 0.15015380\n",
      "Iteration 369, loss = 0.14980713\n",
      "Iteration 370, loss = 0.14944437\n",
      "Iteration 371, loss = 0.14913051\n",
      "Iteration 372, loss = 0.14876183\n",
      "Iteration 373, loss = 0.14841159\n",
      "Iteration 374, loss = 0.14805643\n",
      "Iteration 375, loss = 0.14772233\n",
      "Iteration 376, loss = 0.14739045\n",
      "Iteration 377, loss = 0.14712488\n",
      "Iteration 378, loss = 0.14673067\n",
      "Iteration 379, loss = 0.14637370\n",
      "Iteration 380, loss = 0.14599185\n",
      "Iteration 381, loss = 0.14566146\n",
      "Iteration 382, loss = 0.14533619\n",
      "Iteration 383, loss = 0.14497467\n",
      "Iteration 384, loss = 0.14464083\n",
      "Iteration 385, loss = 0.14426114\n",
      "Iteration 386, loss = 0.14397930\n",
      "Iteration 387, loss = 0.14363112\n",
      "Iteration 388, loss = 0.14332399\n",
      "Iteration 389, loss = 0.14298208\n",
      "Iteration 390, loss = 0.14269125\n",
      "Iteration 391, loss = 0.14235288\n",
      "Iteration 392, loss = 0.14204316\n",
      "Iteration 393, loss = 0.14172382\n",
      "Iteration 394, loss = 0.14143556\n",
      "Iteration 395, loss = 0.14111679\n",
      "Iteration 396, loss = 0.14080287\n",
      "Iteration 397, loss = 0.14053628\n",
      "Iteration 398, loss = 0.14021090\n",
      "Iteration 399, loss = 0.13991125\n",
      "Iteration 400, loss = 0.13964151\n",
      "Iteration 401, loss = 0.13930236\n",
      "Iteration 402, loss = 0.13899238\n",
      "Iteration 403, loss = 0.13870967\n",
      "Iteration 404, loss = 0.13842411\n",
      "Iteration 405, loss = 0.13812270\n",
      "Iteration 406, loss = 0.13783351\n",
      "Iteration 407, loss = 0.13755140\n",
      "Iteration 408, loss = 0.13725034\n",
      "Iteration 409, loss = 0.13693331\n",
      "Iteration 410, loss = 0.13665548\n",
      "Iteration 411, loss = 0.13634660\n",
      "Iteration 412, loss = 0.13608103\n",
      "Iteration 413, loss = 0.13582804\n",
      "Iteration 414, loss = 0.13559846\n",
      "Iteration 415, loss = 0.13528007\n",
      "Iteration 416, loss = 0.13498820\n",
      "Iteration 417, loss = 0.13475210\n",
      "Iteration 418, loss = 0.13446589\n",
      "Iteration 419, loss = 0.13420911\n",
      "Iteration 420, loss = 0.13392011\n",
      "Iteration 421, loss = 0.13365097\n",
      "Iteration 422, loss = 0.13340966\n",
      "Iteration 423, loss = 0.13313137\n",
      "Iteration 424, loss = 0.13286545\n",
      "Iteration 425, loss = 0.13259701\n",
      "Iteration 426, loss = 0.13232296\n",
      "Iteration 427, loss = 0.13206732\n",
      "Iteration 428, loss = 0.13181911\n",
      "Iteration 429, loss = 0.13155692\n",
      "Iteration 430, loss = 0.13128866\n",
      "Iteration 431, loss = 0.13107508\n",
      "Iteration 432, loss = 0.13081062\n",
      "Iteration 433, loss = 0.13053400\n",
      "Iteration 434, loss = 0.13028741\n",
      "Iteration 435, loss = 0.13008319\n",
      "Iteration 436, loss = 0.12980931\n",
      "Iteration 437, loss = 0.12955962\n",
      "Iteration 438, loss = 0.12933002\n",
      "Iteration 439, loss = 0.12907480\n",
      "Iteration 440, loss = 0.12885790\n",
      "Iteration 441, loss = 0.12865716\n",
      "Iteration 442, loss = 0.12836266\n",
      "Iteration 443, loss = 0.12812689\n",
      "Iteration 444, loss = 0.12787969\n",
      "Iteration 445, loss = 0.12764126\n",
      "Iteration 446, loss = 0.12740360\n",
      "Iteration 447, loss = 0.12717204\n",
      "Iteration 448, loss = 0.12690248\n",
      "Iteration 449, loss = 0.12666567\n",
      "Iteration 450, loss = 0.12643554\n",
      "Iteration 451, loss = 0.12618722\n",
      "Iteration 452, loss = 0.12597740\n",
      "Iteration 453, loss = 0.12576534\n",
      "Iteration 454, loss = 0.12552416\n",
      "Iteration 455, loss = 0.12533842\n",
      "Iteration 456, loss = 0.12511109\n",
      "Iteration 457, loss = 0.12486743\n",
      "Iteration 458, loss = 0.12465265\n",
      "Iteration 459, loss = 0.12446984\n",
      "Iteration 460, loss = 0.12423351\n",
      "Iteration 461, loss = 0.12401370\n",
      "Iteration 462, loss = 0.12379086\n",
      "Iteration 463, loss = 0.12357526\n",
      "Iteration 464, loss = 0.12335470\n",
      "Iteration 465, loss = 0.12313282\n",
      "Iteration 466, loss = 0.12293042\n",
      "Iteration 467, loss = 0.12269183\n",
      "Iteration 468, loss = 0.12250032\n",
      "Iteration 469, loss = 0.12230102\n",
      "Iteration 470, loss = 0.12206939\n",
      "Iteration 471, loss = 0.12185853\n",
      "Iteration 472, loss = 0.12167655\n",
      "Iteration 473, loss = 0.12146405\n",
      "Iteration 474, loss = 0.12124804\n",
      "Iteration 475, loss = 0.12101544\n",
      "Iteration 476, loss = 0.12081937\n",
      "Iteration 477, loss = 0.12061711\n",
      "Iteration 478, loss = 0.12042367\n",
      "Iteration 479, loss = 0.12023284\n",
      "Iteration 480, loss = 0.12002457\n",
      "Iteration 481, loss = 0.11982075\n",
      "Iteration 482, loss = 0.11961823\n",
      "Iteration 483, loss = 0.11946066\n",
      "Iteration 484, loss = 0.11926152\n",
      "Iteration 485, loss = 0.11905150\n",
      "Iteration 486, loss = 0.11887152\n",
      "Iteration 487, loss = 0.11867509\n",
      "Iteration 488, loss = 0.11850023\n",
      "Iteration 489, loss = 0.11830088\n",
      "Iteration 490, loss = 0.11811417\n",
      "Iteration 491, loss = 0.11792592\n",
      "Iteration 492, loss = 0.11773808\n",
      "Iteration 493, loss = 0.11754312\n",
      "Iteration 494, loss = 0.11734667\n",
      "Iteration 495, loss = 0.11716130\n",
      "Iteration 496, loss = 0.11698084\n",
      "Iteration 497, loss = 0.11680484\n",
      "Iteration 498, loss = 0.11661046\n",
      "Iteration 499, loss = 0.11643062\n",
      "Iteration 500, loss = 0.11626938\n",
      "Iteration 501, loss = 0.11607014\n",
      "Iteration 502, loss = 0.11589663\n",
      "Iteration 503, loss = 0.11573770\n",
      "Iteration 504, loss = 0.11557121\n",
      "Iteration 505, loss = 0.11537834\n",
      "Iteration 506, loss = 0.11518805\n",
      "Iteration 507, loss = 0.11499758\n",
      "Iteration 508, loss = 0.11482569\n",
      "Iteration 509, loss = 0.11465415\n",
      "Iteration 510, loss = 0.11447692\n",
      "Iteration 511, loss = 0.11431327\n",
      "Iteration 512, loss = 0.11414296\n",
      "Iteration 513, loss = 0.11396168\n",
      "Iteration 514, loss = 0.11380427\n",
      "Iteration 515, loss = 0.11363035\n",
      "Iteration 516, loss = 0.11346999\n",
      "Iteration 517, loss = 0.11330781\n",
      "Iteration 518, loss = 0.11313705\n",
      "Iteration 519, loss = 0.11297519\n",
      "Iteration 520, loss = 0.11280699\n",
      "Iteration 521, loss = 0.11264073\n",
      "Iteration 522, loss = 0.11248038\n",
      "Iteration 523, loss = 0.11232894\n",
      "Iteration 524, loss = 0.11215739\n",
      "Iteration 525, loss = 0.11200883\n",
      "Iteration 526, loss = 0.11184394\n",
      "Iteration 527, loss = 0.11169233\n",
      "Iteration 528, loss = 0.11153072\n",
      "Iteration 529, loss = 0.11135620\n",
      "Iteration 530, loss = 0.11120708\n",
      "Iteration 531, loss = 0.11106638\n",
      "Iteration 532, loss = 0.11091361\n",
      "Iteration 533, loss = 0.11075861\n",
      "Iteration 534, loss = 0.11060933\n",
      "Iteration 535, loss = 0.11046392\n",
      "Iteration 536, loss = 0.11030616\n",
      "Iteration 537, loss = 0.11014711\n",
      "Iteration 538, loss = 0.11000938\n",
      "Iteration 539, loss = 0.10984705\n",
      "Iteration 540, loss = 0.10969246\n",
      "Iteration 541, loss = 0.10955912\n",
      "Iteration 542, loss = 0.10939884\n",
      "Iteration 543, loss = 0.10925063\n",
      "Iteration 544, loss = 0.10911536\n",
      "Iteration 545, loss = 0.10896508\n",
      "Iteration 546, loss = 0.10882147\n",
      "Iteration 547, loss = 0.10866880\n",
      "Iteration 548, loss = 0.10852704\n",
      "Iteration 549, loss = 0.10835795\n",
      "Iteration 550, loss = 0.10822007\n",
      "Iteration 551, loss = 0.10807552\n",
      "Iteration 552, loss = 0.10795882\n",
      "Iteration 553, loss = 0.10780303\n",
      "Iteration 554, loss = 0.10764887\n",
      "Iteration 555, loss = 0.10752838\n",
      "Iteration 556, loss = 0.10739898\n",
      "Iteration 557, loss = 0.10723748\n",
      "Iteration 558, loss = 0.10712408\n",
      "Iteration 559, loss = 0.10695098\n",
      "Iteration 560, loss = 0.10682945\n",
      "Iteration 561, loss = 0.10667838\n",
      "Iteration 562, loss = 0.10655368\n",
      "Iteration 563, loss = 0.10640350\n",
      "Iteration 564, loss = 0.10627845\n",
      "Iteration 565, loss = 0.10614990\n",
      "Iteration 566, loss = 0.10600977\n",
      "Iteration 567, loss = 0.10588096\n",
      "Iteration 568, loss = 0.10574534\n",
      "Iteration 569, loss = 0.10561807\n",
      "Iteration 570, loss = 0.10549129\n",
      "Iteration 571, loss = 0.10536177\n",
      "Iteration 572, loss = 0.10520877\n",
      "Iteration 573, loss = 0.10508045\n",
      "Iteration 574, loss = 0.10494359\n",
      "Iteration 575, loss = 0.10481529\n",
      "Iteration 576, loss = 0.10469392\n",
      "Iteration 577, loss = 0.10456385\n",
      "Iteration 578, loss = 0.10443817\n",
      "Iteration 579, loss = 0.10432166\n",
      "Iteration 580, loss = 0.10416574\n",
      "Iteration 581, loss = 0.10404809\n",
      "Iteration 582, loss = 0.10392054\n",
      "Iteration 583, loss = 0.10381152\n",
      "Iteration 584, loss = 0.10369335\n",
      "Iteration 585, loss = 0.10356855\n",
      "Iteration 586, loss = 0.10344699\n",
      "Iteration 587, loss = 0.10331293\n",
      "Iteration 588, loss = 0.10319395\n",
      "Iteration 589, loss = 0.10307535\n",
      "Iteration 590, loss = 0.10296672\n",
      "Iteration 591, loss = 0.10286255\n",
      "Iteration 592, loss = 0.10271885\n",
      "Iteration 593, loss = 0.10258623\n",
      "Iteration 594, loss = 0.10246545\n",
      "Iteration 595, loss = 0.10233834\n",
      "Iteration 596, loss = 0.10222523\n",
      "Iteration 597, loss = 0.10210873\n",
      "Iteration 598, loss = 0.10199960\n",
      "Iteration 599, loss = 0.10188683\n",
      "Iteration 600, loss = 0.10175920\n",
      "Iteration 601, loss = 0.10164279\n",
      "Iteration 602, loss = 0.10153496\n",
      "Iteration 603, loss = 0.10141877\n",
      "Iteration 604, loss = 0.10130891\n",
      "Iteration 605, loss = 0.10119214\n",
      "Iteration 606, loss = 0.10107390\n",
      "Iteration 607, loss = 0.10095236\n",
      "Iteration 608, loss = 0.10083833\n",
      "Iteration 609, loss = 0.10073726\n",
      "Iteration 610, loss = 0.10062501\n",
      "Iteration 611, loss = 0.10052545\n",
      "Iteration 612, loss = 0.10039504\n",
      "Iteration 613, loss = 0.10029631\n",
      "Iteration 614, loss = 0.10017683\n",
      "Iteration 615, loss = 0.10007083\n",
      "Iteration 616, loss = 0.09995614\n",
      "Iteration 617, loss = 0.09984158\n",
      "Iteration 618, loss = 0.09972935\n",
      "Iteration 619, loss = 0.09961988\n",
      "Iteration 620, loss = 0.09951223\n",
      "Iteration 621, loss = 0.09940352\n",
      "Iteration 622, loss = 0.09930299\n",
      "Iteration 623, loss = 0.09919771\n",
      "Iteration 624, loss = 0.09909780\n",
      "Iteration 625, loss = 0.09898326\n",
      "Iteration 626, loss = 0.09887282\n",
      "Iteration 627, loss = 0.09876153\n",
      "Iteration 628, loss = 0.09866322\n",
      "Iteration 629, loss = 0.09856157\n",
      "Iteration 630, loss = 0.09844978\n",
      "Iteration 631, loss = 0.09834715\n",
      "Iteration 632, loss = 0.09824408\n",
      "Iteration 633, loss = 0.09815114\n",
      "Iteration 634, loss = 0.09805241\n",
      "Iteration 635, loss = 0.09795576\n",
      "Iteration 636, loss = 0.09784058\n",
      "Iteration 637, loss = 0.09774136\n",
      "Iteration 638, loss = 0.09765226\n",
      "Iteration 639, loss = 0.09754988\n",
      "Iteration 640, loss = 0.09745589\n",
      "Iteration 641, loss = 0.09735455\n",
      "Iteration 642, loss = 0.09726022\n",
      "Iteration 643, loss = 0.09716449\n",
      "Iteration 644, loss = 0.09704661\n",
      "Iteration 645, loss = 0.09695339\n",
      "Iteration 646, loss = 0.09685520\n",
      "Iteration 647, loss = 0.09676318\n",
      "Iteration 648, loss = 0.09668041\n",
      "Iteration 649, loss = 0.09658796\n",
      "Iteration 650, loss = 0.09648204\n",
      "Iteration 651, loss = 0.09638676\n",
      "Iteration 652, loss = 0.09629342\n",
      "Iteration 653, loss = 0.09619580\n",
      "Iteration 654, loss = 0.09610398\n",
      "Iteration 655, loss = 0.09601641\n",
      "Iteration 656, loss = 0.09592187\n",
      "Iteration 657, loss = 0.09583519\n",
      "Iteration 658, loss = 0.09573673\n",
      "Iteration 659, loss = 0.09564874\n",
      "Iteration 660, loss = 0.09557414\n",
      "Iteration 661, loss = 0.09547248\n",
      "Iteration 662, loss = 0.09537805\n",
      "Iteration 663, loss = 0.09527798\n",
      "Iteration 664, loss = 0.09519608\n",
      "Iteration 665, loss = 0.09511957\n",
      "Iteration 666, loss = 0.09503856\n",
      "Iteration 667, loss = 0.09492420\n",
      "Iteration 668, loss = 0.09483871\n",
      "Iteration 669, loss = 0.09475446\n",
      "Iteration 670, loss = 0.09465429\n",
      "Iteration 671, loss = 0.09457021\n",
      "Iteration 672, loss = 0.09447668\n",
      "Iteration 673, loss = 0.09439041\n",
      "Iteration 674, loss = 0.09430479\n",
      "Iteration 675, loss = 0.09421602\n",
      "Iteration 676, loss = 0.09413496\n",
      "Iteration 677, loss = 0.09404546\n",
      "Iteration 678, loss = 0.09396110\n",
      "Iteration 679, loss = 0.09387737\n",
      "Iteration 680, loss = 0.09380268\n",
      "Iteration 681, loss = 0.09370920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.43819554\n",
      "Iteration 2, loss = 2.40637303\n",
      "Iteration 3, loss = 2.36936623\n",
      "Iteration 4, loss = 2.33565955\n",
      "Iteration 5, loss = 2.30779161\n",
      "Iteration 6, loss = 2.28286620\n",
      "Iteration 7, loss = 2.26057449\n",
      "Iteration 8, loss = 2.23948676\n",
      "Iteration 9, loss = 2.21916386\n",
      "Iteration 10, loss = 2.19915808\n",
      "Iteration 11, loss = 2.17883846\n",
      "Iteration 12, loss = 2.15825948\n",
      "Iteration 13, loss = 2.13724384\n",
      "Iteration 14, loss = 2.11599243\n",
      "Iteration 15, loss = 2.09460405\n",
      "Iteration 16, loss = 2.07255466\n",
      "Iteration 17, loss = 2.04964311\n",
      "Iteration 18, loss = 2.02609119\n",
      "Iteration 19, loss = 2.00239800\n",
      "Iteration 20, loss = 1.97826738\n",
      "Iteration 21, loss = 1.95344089\n",
      "Iteration 22, loss = 1.92845295\n",
      "Iteration 23, loss = 1.90301520\n",
      "Iteration 24, loss = 1.87714099\n",
      "Iteration 25, loss = 1.85106942\n",
      "Iteration 26, loss = 1.82503879\n",
      "Iteration 27, loss = 1.79859872\n",
      "Iteration 28, loss = 1.77208028\n",
      "Iteration 29, loss = 1.74517670\n",
      "Iteration 30, loss = 1.71820153\n",
      "Iteration 31, loss = 1.69108769\n",
      "Iteration 32, loss = 1.66439724\n",
      "Iteration 33, loss = 1.63766883\n",
      "Iteration 34, loss = 1.61052626\n",
      "Iteration 35, loss = 1.58328882\n",
      "Iteration 36, loss = 1.55661431\n",
      "Iteration 37, loss = 1.53019795\n",
      "Iteration 38, loss = 1.50377230\n",
      "Iteration 39, loss = 1.47754632\n",
      "Iteration 40, loss = 1.45186375\n",
      "Iteration 41, loss = 1.42633638\n",
      "Iteration 42, loss = 1.40124704\n",
      "Iteration 43, loss = 1.37661379\n",
      "Iteration 44, loss = 1.35238772\n",
      "Iteration 45, loss = 1.32858682\n",
      "Iteration 46, loss = 1.30514475\n",
      "Iteration 47, loss = 1.28186579\n",
      "Iteration 48, loss = 1.25883097\n",
      "Iteration 49, loss = 1.23632972\n",
      "Iteration 50, loss = 1.21406210\n",
      "Iteration 51, loss = 1.19235052\n",
      "Iteration 52, loss = 1.17096176\n",
      "Iteration 53, loss = 1.15007286\n",
      "Iteration 54, loss = 1.12985444\n",
      "Iteration 55, loss = 1.10980541\n",
      "Iteration 56, loss = 1.09037954\n",
      "Iteration 57, loss = 1.07105337\n",
      "Iteration 58, loss = 1.05244366\n",
      "Iteration 59, loss = 1.03427739\n",
      "Iteration 60, loss = 1.01626808\n",
      "Iteration 61, loss = 0.99864134\n",
      "Iteration 62, loss = 0.98146521\n",
      "Iteration 63, loss = 0.96462359\n",
      "Iteration 64, loss = 0.94832080\n",
      "Iteration 65, loss = 0.93232865\n",
      "Iteration 66, loss = 0.91678420\n",
      "Iteration 67, loss = 0.90157712\n",
      "Iteration 68, loss = 0.88656695\n",
      "Iteration 69, loss = 0.87221165\n",
      "Iteration 70, loss = 0.85830598\n",
      "Iteration 71, loss = 0.84507011\n",
      "Iteration 72, loss = 0.83176062\n",
      "Iteration 73, loss = 0.81861681\n",
      "Iteration 74, loss = 0.80582026\n",
      "Iteration 75, loss = 0.79357516\n",
      "Iteration 76, loss = 0.78165494\n",
      "Iteration 77, loss = 0.77006160\n",
      "Iteration 78, loss = 0.75872799\n",
      "Iteration 79, loss = 0.74789204\n",
      "Iteration 80, loss = 0.73703241\n",
      "Iteration 81, loss = 0.72650714\n",
      "Iteration 82, loss = 0.71635256\n",
      "Iteration 83, loss = 0.70636982\n",
      "Iteration 84, loss = 0.69675725\n",
      "Iteration 85, loss = 0.68752920\n",
      "Iteration 86, loss = 0.67819962\n",
      "Iteration 87, loss = 0.66915414\n",
      "Iteration 88, loss = 0.66054817\n",
      "Iteration 89, loss = 0.65193983\n",
      "Iteration 90, loss = 0.64354364\n",
      "Iteration 91, loss = 0.63551367\n",
      "Iteration 92, loss = 0.62772888\n",
      "Iteration 93, loss = 0.61985419\n",
      "Iteration 94, loss = 0.61239885\n",
      "Iteration 95, loss = 0.60501293\n",
      "Iteration 96, loss = 0.59791938\n",
      "Iteration 97, loss = 0.59091119\n",
      "Iteration 98, loss = 0.58414816\n",
      "Iteration 99, loss = 0.57747026\n",
      "Iteration 100, loss = 0.57084589\n",
      "Iteration 101, loss = 0.56446753\n",
      "Iteration 102, loss = 0.55835105\n",
      "Iteration 103, loss = 0.55226645\n",
      "Iteration 104, loss = 0.54621411\n",
      "Iteration 105, loss = 0.54054995\n",
      "Iteration 106, loss = 0.53490247\n",
      "Iteration 107, loss = 0.52935725\n",
      "Iteration 108, loss = 0.52402300\n",
      "Iteration 109, loss = 0.51858759\n",
      "Iteration 110, loss = 0.51341507\n",
      "Iteration 111, loss = 0.50829011\n",
      "Iteration 112, loss = 0.50345200\n",
      "Iteration 113, loss = 0.49851920\n",
      "Iteration 114, loss = 0.49383193\n",
      "Iteration 115, loss = 0.48902163\n",
      "Iteration 116, loss = 0.48437700\n",
      "Iteration 117, loss = 0.47976555\n",
      "Iteration 118, loss = 0.47542904\n",
      "Iteration 119, loss = 0.47102373\n",
      "Iteration 120, loss = 0.46671425\n",
      "Iteration 121, loss = 0.46241912\n",
      "Iteration 122, loss = 0.45820096\n",
      "Iteration 123, loss = 0.45427908\n",
      "Iteration 124, loss = 0.45029865\n",
      "Iteration 125, loss = 0.44635333\n",
      "Iteration 126, loss = 0.44254047\n",
      "Iteration 127, loss = 0.43874379\n",
      "Iteration 128, loss = 0.43522226\n",
      "Iteration 129, loss = 0.43156822\n",
      "Iteration 130, loss = 0.42804865\n",
      "Iteration 131, loss = 0.42457789\n",
      "Iteration 132, loss = 0.42115060\n",
      "Iteration 133, loss = 0.41791425\n",
      "Iteration 134, loss = 0.41456030\n",
      "Iteration 135, loss = 0.41129287\n",
      "Iteration 136, loss = 0.40802408\n",
      "Iteration 137, loss = 0.40501309\n",
      "Iteration 138, loss = 0.40172125\n",
      "Iteration 139, loss = 0.39880556\n",
      "Iteration 140, loss = 0.39580047\n",
      "Iteration 141, loss = 0.39280927\n",
      "Iteration 142, loss = 0.39016810\n",
      "Iteration 143, loss = 0.38709973\n",
      "Iteration 144, loss = 0.38428734\n",
      "Iteration 145, loss = 0.38155622\n",
      "Iteration 146, loss = 0.37881730\n",
      "Iteration 147, loss = 0.37619249\n",
      "Iteration 148, loss = 0.37352606\n",
      "Iteration 149, loss = 0.37096034\n",
      "Iteration 150, loss = 0.36840146\n",
      "Iteration 151, loss = 0.36585075\n",
      "Iteration 152, loss = 0.36336697\n",
      "Iteration 153, loss = 0.36094098\n",
      "Iteration 154, loss = 0.35850817\n",
      "Iteration 155, loss = 0.35609089\n",
      "Iteration 156, loss = 0.35367956\n",
      "Iteration 157, loss = 0.35139081\n",
      "Iteration 158, loss = 0.34912367\n",
      "Iteration 159, loss = 0.34685577\n",
      "Iteration 160, loss = 0.34467058\n",
      "Iteration 161, loss = 0.34251135\n",
      "Iteration 162, loss = 0.34025932\n",
      "Iteration 163, loss = 0.33810031\n",
      "Iteration 164, loss = 0.33591650\n",
      "Iteration 165, loss = 0.33373623\n",
      "Iteration 166, loss = 0.33175419\n",
      "Iteration 167, loss = 0.32964167\n",
      "Iteration 168, loss = 0.32765930\n",
      "Iteration 169, loss = 0.32565596\n",
      "Iteration 170, loss = 0.32367454\n",
      "Iteration 171, loss = 0.32175723\n",
      "Iteration 172, loss = 0.31982087\n",
      "Iteration 173, loss = 0.31795348\n",
      "Iteration 174, loss = 0.31614172\n",
      "Iteration 175, loss = 0.31430577\n",
      "Iteration 176, loss = 0.31244008\n",
      "Iteration 177, loss = 0.31070332\n",
      "Iteration 178, loss = 0.30894531\n",
      "Iteration 179, loss = 0.30721121\n",
      "Iteration 180, loss = 0.30547077\n",
      "Iteration 181, loss = 0.30377644\n",
      "Iteration 182, loss = 0.30201906\n",
      "Iteration 183, loss = 0.30033415\n",
      "Iteration 184, loss = 0.29867057\n",
      "Iteration 185, loss = 0.29714118\n",
      "Iteration 186, loss = 0.29553237\n",
      "Iteration 187, loss = 0.29382730\n",
      "Iteration 188, loss = 0.29225204\n",
      "Iteration 189, loss = 0.29061844\n",
      "Iteration 190, loss = 0.28912152\n",
      "Iteration 191, loss = 0.28762520\n",
      "Iteration 192, loss = 0.28616353\n",
      "Iteration 193, loss = 0.28458913\n",
      "Iteration 194, loss = 0.28307507\n",
      "Iteration 195, loss = 0.28157459\n",
      "Iteration 196, loss = 0.28017575\n",
      "Iteration 197, loss = 0.27885170\n",
      "Iteration 198, loss = 0.27734532\n",
      "Iteration 199, loss = 0.27593907\n",
      "Iteration 200, loss = 0.27450579\n",
      "Iteration 201, loss = 0.27306795\n",
      "Iteration 202, loss = 0.27173727\n",
      "Iteration 203, loss = 0.27043399\n",
      "Iteration 204, loss = 0.26914035\n",
      "Iteration 205, loss = 0.26776424\n",
      "Iteration 206, loss = 0.26645922\n",
      "Iteration 207, loss = 0.26511888\n",
      "Iteration 208, loss = 0.26391739\n",
      "Iteration 209, loss = 0.26264054\n",
      "Iteration 210, loss = 0.26138348\n",
      "Iteration 211, loss = 0.26014389\n",
      "Iteration 212, loss = 0.25894341\n",
      "Iteration 213, loss = 0.25771103\n",
      "Iteration 214, loss = 0.25652389\n",
      "Iteration 215, loss = 0.25534597\n",
      "Iteration 216, loss = 0.25419452\n",
      "Iteration 217, loss = 0.25305072\n",
      "Iteration 218, loss = 0.25187157\n",
      "Iteration 219, loss = 0.25077905\n",
      "Iteration 220, loss = 0.24968680\n",
      "Iteration 221, loss = 0.24862809\n",
      "Iteration 222, loss = 0.24747028\n",
      "Iteration 223, loss = 0.24633790\n",
      "Iteration 224, loss = 0.24524310\n",
      "Iteration 225, loss = 0.24407999\n",
      "Iteration 226, loss = 0.24296271\n",
      "Iteration 227, loss = 0.24189504\n",
      "Iteration 228, loss = 0.24086154\n",
      "Iteration 229, loss = 0.23988206\n",
      "Iteration 230, loss = 0.23887943\n",
      "Iteration 231, loss = 0.23783137\n",
      "Iteration 232, loss = 0.23677152\n",
      "Iteration 233, loss = 0.23578020\n",
      "Iteration 234, loss = 0.23478242\n",
      "Iteration 235, loss = 0.23379833\n",
      "Iteration 236, loss = 0.23284069\n",
      "Iteration 237, loss = 0.23185372\n",
      "Iteration 238, loss = 0.23099707\n",
      "Iteration 239, loss = 0.22994237\n",
      "Iteration 240, loss = 0.22900340\n",
      "Iteration 241, loss = 0.22808316\n",
      "Iteration 242, loss = 0.22719588\n",
      "Iteration 243, loss = 0.22625838\n",
      "Iteration 244, loss = 0.22534214\n",
      "Iteration 245, loss = 0.22443232\n",
      "Iteration 246, loss = 0.22357965\n",
      "Iteration 247, loss = 0.22273913\n",
      "Iteration 248, loss = 0.22185965\n",
      "Iteration 249, loss = 0.22096846\n",
      "Iteration 250, loss = 0.22017038\n",
      "Iteration 251, loss = 0.21922055\n",
      "Iteration 252, loss = 0.21838027\n",
      "Iteration 253, loss = 0.21752692\n",
      "Iteration 254, loss = 0.21667927\n",
      "Iteration 255, loss = 0.21584066\n",
      "Iteration 256, loss = 0.21504595\n",
      "Iteration 257, loss = 0.21425716\n",
      "Iteration 258, loss = 0.21340922\n",
      "Iteration 259, loss = 0.21261970\n",
      "Iteration 260, loss = 0.21181324\n",
      "Iteration 261, loss = 0.21099334\n",
      "Iteration 262, loss = 0.21032602\n",
      "Iteration 263, loss = 0.20954067\n",
      "Iteration 264, loss = 0.20871591\n",
      "Iteration 265, loss = 0.20795122\n",
      "Iteration 266, loss = 0.20716401\n",
      "Iteration 267, loss = 0.20642015\n",
      "Iteration 268, loss = 0.20566600\n",
      "Iteration 269, loss = 0.20493889\n",
      "Iteration 270, loss = 0.20427213\n",
      "Iteration 271, loss = 0.20355453\n",
      "Iteration 272, loss = 0.20279885\n",
      "Iteration 273, loss = 0.20207907\n",
      "Iteration 274, loss = 0.20134964\n",
      "Iteration 275, loss = 0.20066076\n",
      "Iteration 276, loss = 0.19999156\n",
      "Iteration 277, loss = 0.19925627\n",
      "Iteration 278, loss = 0.19855376\n",
      "Iteration 279, loss = 0.19789071\n",
      "Iteration 280, loss = 0.19718670\n",
      "Iteration 281, loss = 0.19656328\n",
      "Iteration 282, loss = 0.19587568\n",
      "Iteration 283, loss = 0.19518929\n",
      "Iteration 284, loss = 0.19453038\n",
      "Iteration 285, loss = 0.19385637\n",
      "Iteration 286, loss = 0.19325361\n",
      "Iteration 287, loss = 0.19256515\n",
      "Iteration 288, loss = 0.19200146\n",
      "Iteration 289, loss = 0.19126377\n",
      "Iteration 290, loss = 0.19066355\n",
      "Iteration 291, loss = 0.18998330\n",
      "Iteration 292, loss = 0.18947959\n",
      "Iteration 293, loss = 0.18875362\n",
      "Iteration 294, loss = 0.18815638\n",
      "Iteration 295, loss = 0.18754508\n",
      "Iteration 296, loss = 0.18692731\n",
      "Iteration 297, loss = 0.18632926\n",
      "Iteration 298, loss = 0.18572628\n",
      "Iteration 299, loss = 0.18510355\n",
      "Iteration 300, loss = 0.18453153\n",
      "Iteration 301, loss = 0.18393647\n",
      "Iteration 302, loss = 0.18334682\n",
      "Iteration 303, loss = 0.18280096\n",
      "Iteration 304, loss = 0.18225598\n",
      "Iteration 305, loss = 0.18167210\n",
      "Iteration 306, loss = 0.18113465\n",
      "Iteration 307, loss = 0.18055785\n",
      "Iteration 308, loss = 0.18000607\n",
      "Iteration 309, loss = 0.17947057\n",
      "Iteration 310, loss = 0.17893700\n",
      "Iteration 311, loss = 0.17839614\n",
      "Iteration 312, loss = 0.17785303\n",
      "Iteration 313, loss = 0.17728750\n",
      "Iteration 314, loss = 0.17681557\n",
      "Iteration 315, loss = 0.17634125\n",
      "Iteration 316, loss = 0.17576796\n",
      "Iteration 317, loss = 0.17523710\n",
      "Iteration 318, loss = 0.17469915\n",
      "Iteration 319, loss = 0.17421010\n",
      "Iteration 320, loss = 0.17366430\n",
      "Iteration 321, loss = 0.17310773\n",
      "Iteration 322, loss = 0.17263754\n",
      "Iteration 323, loss = 0.17210245\n",
      "Iteration 324, loss = 0.17165458\n",
      "Iteration 325, loss = 0.17118317\n",
      "Iteration 326, loss = 0.17065086\n",
      "Iteration 327, loss = 0.17017880\n",
      "Iteration 328, loss = 0.16971219\n",
      "Iteration 329, loss = 0.16917051\n",
      "Iteration 330, loss = 0.16871077\n",
      "Iteration 331, loss = 0.16825254\n",
      "Iteration 332, loss = 0.16775295\n",
      "Iteration 333, loss = 0.16727019\n",
      "Iteration 334, loss = 0.16686070\n",
      "Iteration 335, loss = 0.16633193\n",
      "Iteration 336, loss = 0.16586505\n",
      "Iteration 337, loss = 0.16540146\n",
      "Iteration 338, loss = 0.16491738\n",
      "Iteration 339, loss = 0.16447463\n",
      "Iteration 340, loss = 0.16404218\n",
      "Iteration 341, loss = 0.16364590\n",
      "Iteration 342, loss = 0.16319887\n",
      "Iteration 343, loss = 0.16275300\n",
      "Iteration 344, loss = 0.16234473\n",
      "Iteration 345, loss = 0.16190609\n",
      "Iteration 346, loss = 0.16143988\n",
      "Iteration 347, loss = 0.16107264\n",
      "Iteration 348, loss = 0.16064070\n",
      "Iteration 349, loss = 0.16018757\n",
      "Iteration 350, loss = 0.15984372\n",
      "Iteration 351, loss = 0.15936429\n",
      "Iteration 352, loss = 0.15894649\n",
      "Iteration 353, loss = 0.15853150\n",
      "Iteration 354, loss = 0.15812692\n",
      "Iteration 355, loss = 0.15775727\n",
      "Iteration 356, loss = 0.15733542\n",
      "Iteration 357, loss = 0.15695051\n",
      "Iteration 358, loss = 0.15647898\n",
      "Iteration 359, loss = 0.15611507\n",
      "Iteration 360, loss = 0.15570506\n",
      "Iteration 361, loss = 0.15527766\n",
      "Iteration 362, loss = 0.15487069\n",
      "Iteration 363, loss = 0.15449563\n",
      "Iteration 364, loss = 0.15412471\n",
      "Iteration 365, loss = 0.15371442\n",
      "Iteration 366, loss = 0.15333813\n",
      "Iteration 367, loss = 0.15296775\n",
      "Iteration 368, loss = 0.15260418\n",
      "Iteration 369, loss = 0.15221800\n",
      "Iteration 370, loss = 0.15185066\n",
      "Iteration 371, loss = 0.15149481\n",
      "Iteration 372, loss = 0.15111632\n",
      "Iteration 373, loss = 0.15075746\n",
      "Iteration 374, loss = 0.15038382\n",
      "Iteration 375, loss = 0.14999837\n",
      "Iteration 376, loss = 0.14963323\n",
      "Iteration 377, loss = 0.14927053\n",
      "Iteration 378, loss = 0.14895887\n",
      "Iteration 379, loss = 0.14860117\n",
      "Iteration 380, loss = 0.14820819\n",
      "Iteration 381, loss = 0.14787450\n",
      "Iteration 382, loss = 0.14754317\n",
      "Iteration 383, loss = 0.14719025\n",
      "Iteration 384, loss = 0.14681985\n",
      "Iteration 385, loss = 0.14647173\n",
      "Iteration 386, loss = 0.14613892\n",
      "Iteration 387, loss = 0.14579115\n",
      "Iteration 388, loss = 0.14545204\n",
      "Iteration 389, loss = 0.14513331\n",
      "Iteration 390, loss = 0.14479195\n",
      "Iteration 391, loss = 0.14447125\n",
      "Iteration 392, loss = 0.14413346\n",
      "Iteration 393, loss = 0.14380628\n",
      "Iteration 394, loss = 0.14349287\n",
      "Iteration 395, loss = 0.14317117\n",
      "Iteration 396, loss = 0.14286343\n",
      "Iteration 397, loss = 0.14253579\n",
      "Iteration 398, loss = 0.14224214\n",
      "Iteration 399, loss = 0.14189510\n",
      "Iteration 400, loss = 0.14157980\n",
      "Iteration 401, loss = 0.14127230\n",
      "Iteration 402, loss = 0.14095568\n",
      "Iteration 403, loss = 0.14066436\n",
      "Iteration 404, loss = 0.14035232\n",
      "Iteration 405, loss = 0.14004841\n",
      "Iteration 406, loss = 0.13975665\n",
      "Iteration 407, loss = 0.13946902\n",
      "Iteration 408, loss = 0.13916598\n",
      "Iteration 409, loss = 0.13888133\n",
      "Iteration 410, loss = 0.13855767\n",
      "Iteration 411, loss = 0.13827346\n",
      "Iteration 412, loss = 0.13798906\n",
      "Iteration 413, loss = 0.13769206\n",
      "Iteration 414, loss = 0.13740232\n",
      "Iteration 415, loss = 0.13709766\n",
      "Iteration 416, loss = 0.13680468\n",
      "Iteration 417, loss = 0.13651350\n",
      "Iteration 418, loss = 0.13622307\n",
      "Iteration 419, loss = 0.13592786\n",
      "Iteration 420, loss = 0.13566767\n",
      "Iteration 421, loss = 0.13539321\n",
      "Iteration 422, loss = 0.13511084\n",
      "Iteration 423, loss = 0.13485463\n",
      "Iteration 424, loss = 0.13456411\n",
      "Iteration 425, loss = 0.13431237\n",
      "Iteration 426, loss = 0.13405613\n",
      "Iteration 427, loss = 0.13377785\n",
      "Iteration 428, loss = 0.13351482\n",
      "Iteration 429, loss = 0.13325182\n",
      "Iteration 430, loss = 0.13299377\n",
      "Iteration 431, loss = 0.13273247\n",
      "Iteration 432, loss = 0.13247870\n",
      "Iteration 433, loss = 0.13221384\n",
      "Iteration 434, loss = 0.13196148\n",
      "Iteration 435, loss = 0.13169456\n",
      "Iteration 436, loss = 0.13142772\n",
      "Iteration 437, loss = 0.13116840\n",
      "Iteration 438, loss = 0.13093425\n",
      "Iteration 439, loss = 0.13068174\n",
      "Iteration 440, loss = 0.13042249\n",
      "Iteration 441, loss = 0.13017350\n",
      "Iteration 442, loss = 0.12990464\n",
      "Iteration 443, loss = 0.12967256\n",
      "Iteration 444, loss = 0.12941200\n",
      "Iteration 445, loss = 0.12921318\n",
      "Iteration 446, loss = 0.12895720\n",
      "Iteration 447, loss = 0.12869551\n",
      "Iteration 448, loss = 0.12842345\n",
      "Iteration 449, loss = 0.12818913\n",
      "Iteration 450, loss = 0.12794682\n",
      "Iteration 451, loss = 0.12771206\n",
      "Iteration 452, loss = 0.12748872\n",
      "Iteration 453, loss = 0.12725005\n",
      "Iteration 454, loss = 0.12700476\n",
      "Iteration 455, loss = 0.12677699\n",
      "Iteration 456, loss = 0.12656295\n",
      "Iteration 457, loss = 0.12633164\n",
      "Iteration 458, loss = 0.12608455\n",
      "Iteration 459, loss = 0.12584496\n",
      "Iteration 460, loss = 0.12564144\n",
      "Iteration 461, loss = 0.12541683\n",
      "Iteration 462, loss = 0.12519255\n",
      "Iteration 463, loss = 0.12496100\n",
      "Iteration 464, loss = 0.12471465\n",
      "Iteration 465, loss = 0.12451900\n",
      "Iteration 466, loss = 0.12429499\n",
      "Iteration 467, loss = 0.12408091\n",
      "Iteration 468, loss = 0.12386089\n",
      "Iteration 469, loss = 0.12364528\n",
      "Iteration 470, loss = 0.12343031\n",
      "Iteration 471, loss = 0.12321250\n",
      "Iteration 472, loss = 0.12301216\n",
      "Iteration 473, loss = 0.12281165\n",
      "Iteration 474, loss = 0.12259522\n",
      "Iteration 475, loss = 0.12237340\n",
      "Iteration 476, loss = 0.12216166\n",
      "Iteration 477, loss = 0.12194702\n",
      "Iteration 478, loss = 0.12175903\n",
      "Iteration 479, loss = 0.12155132\n",
      "Iteration 480, loss = 0.12134483\n",
      "Iteration 481, loss = 0.12115472\n",
      "Iteration 482, loss = 0.12096218\n",
      "Iteration 483, loss = 0.12076045\n",
      "Iteration 484, loss = 0.12057933\n",
      "Iteration 485, loss = 0.12036202\n",
      "Iteration 486, loss = 0.12016272\n",
      "Iteration 487, loss = 0.11998106\n",
      "Iteration 488, loss = 0.11977872\n",
      "Iteration 489, loss = 0.11957778\n",
      "Iteration 490, loss = 0.11934901\n",
      "Iteration 491, loss = 0.11917918\n",
      "Iteration 492, loss = 0.11897973\n",
      "Iteration 493, loss = 0.11879393\n",
      "Iteration 494, loss = 0.11859995\n",
      "Iteration 495, loss = 0.11839578\n",
      "Iteration 496, loss = 0.11820965\n",
      "Iteration 497, loss = 0.11800437\n",
      "Iteration 498, loss = 0.11781285\n",
      "Iteration 499, loss = 0.11762287\n",
      "Iteration 500, loss = 0.11744222\n",
      "Iteration 501, loss = 0.11725270\n",
      "Iteration 502, loss = 0.11707619\n",
      "Iteration 503, loss = 0.11688497\n",
      "Iteration 504, loss = 0.11669273\n",
      "Iteration 505, loss = 0.11652490\n",
      "Iteration 506, loss = 0.11634536\n",
      "Iteration 507, loss = 0.11615047\n",
      "Iteration 508, loss = 0.11598867\n",
      "Iteration 509, loss = 0.11581845\n",
      "Iteration 510, loss = 0.11565379\n",
      "Iteration 511, loss = 0.11545669\n",
      "Iteration 512, loss = 0.11530946\n",
      "Iteration 513, loss = 0.11513720\n",
      "Iteration 514, loss = 0.11494227\n",
      "Iteration 515, loss = 0.11475853\n",
      "Iteration 516, loss = 0.11460909\n",
      "Iteration 517, loss = 0.11442939\n",
      "Iteration 518, loss = 0.11426042\n",
      "Iteration 519, loss = 0.11409001\n",
      "Iteration 520, loss = 0.11392107\n",
      "Iteration 521, loss = 0.11376663\n",
      "Iteration 522, loss = 0.11360722\n",
      "Iteration 523, loss = 0.11342465\n",
      "Iteration 524, loss = 0.11324988\n",
      "Iteration 525, loss = 0.11310005\n",
      "Iteration 526, loss = 0.11292028\n",
      "Iteration 527, loss = 0.11275188\n",
      "Iteration 528, loss = 0.11261074\n",
      "Iteration 529, loss = 0.11244706\n",
      "Iteration 530, loss = 0.11227395\n",
      "Iteration 531, loss = 0.11210128\n",
      "Iteration 532, loss = 0.11195251\n",
      "Iteration 533, loss = 0.11179316\n",
      "Iteration 534, loss = 0.11162113\n",
      "Iteration 535, loss = 0.11148812\n",
      "Iteration 536, loss = 0.11130080\n",
      "Iteration 537, loss = 0.11115060\n",
      "Iteration 538, loss = 0.11097606\n",
      "Iteration 539, loss = 0.11081261\n",
      "Iteration 540, loss = 0.11066885\n",
      "Iteration 541, loss = 0.11051888\n",
      "Iteration 542, loss = 0.11037460\n",
      "Iteration 543, loss = 0.11023098\n",
      "Iteration 544, loss = 0.11007676\n",
      "Iteration 545, loss = 0.10991584\n",
      "Iteration 546, loss = 0.10977006\n",
      "Iteration 547, loss = 0.10962065\n",
      "Iteration 548, loss = 0.10948411\n",
      "Iteration 549, loss = 0.10933298\n",
      "Iteration 550, loss = 0.10919021\n",
      "Iteration 551, loss = 0.10905897\n",
      "Iteration 552, loss = 0.10890500\n",
      "Iteration 553, loss = 0.10875506\n",
      "Iteration 554, loss = 0.10861882\n",
      "Iteration 555, loss = 0.10846284\n",
      "Iteration 556, loss = 0.10831298\n",
      "Iteration 557, loss = 0.10817044\n",
      "Iteration 558, loss = 0.10802825\n",
      "Iteration 559, loss = 0.10789962\n",
      "Iteration 560, loss = 0.10779185\n",
      "Iteration 561, loss = 0.10763223\n",
      "Iteration 562, loss = 0.10748807\n",
      "Iteration 563, loss = 0.10734325\n",
      "Iteration 564, loss = 0.10721014\n",
      "Iteration 565, loss = 0.10709511\n",
      "Iteration 566, loss = 0.10692380\n",
      "Iteration 567, loss = 0.10679912\n",
      "Iteration 568, loss = 0.10664879\n",
      "Iteration 569, loss = 0.10651112\n",
      "Iteration 570, loss = 0.10637895\n",
      "Iteration 571, loss = 0.10624374\n",
      "Iteration 572, loss = 0.10609245\n",
      "Iteration 573, loss = 0.10595863\n",
      "Iteration 574, loss = 0.10584181\n",
      "Iteration 575, loss = 0.10569885\n",
      "Iteration 576, loss = 0.10557914\n",
      "Iteration 577, loss = 0.10544217\n",
      "Iteration 578, loss = 0.10531278\n",
      "Iteration 579, loss = 0.10516852\n",
      "Iteration 580, loss = 0.10505870\n",
      "Iteration 581, loss = 0.10493722\n",
      "Iteration 582, loss = 0.10481019\n",
      "Iteration 583, loss = 0.10467395\n",
      "Iteration 584, loss = 0.10454734\n",
      "Iteration 585, loss = 0.10441685\n",
      "Iteration 586, loss = 0.10427788\n",
      "Iteration 587, loss = 0.10415923\n",
      "Iteration 588, loss = 0.10404155\n",
      "Iteration 589, loss = 0.10392561\n",
      "Iteration 590, loss = 0.10378179\n",
      "Iteration 591, loss = 0.10368420\n",
      "Iteration 592, loss = 0.10354505\n",
      "Iteration 593, loss = 0.10341739\n",
      "Iteration 594, loss = 0.10328869\n",
      "Iteration 595, loss = 0.10317288\n",
      "Iteration 596, loss = 0.10305134\n",
      "Iteration 597, loss = 0.10293672\n",
      "Iteration 598, loss = 0.10281741\n",
      "Iteration 599, loss = 0.10269121\n",
      "Iteration 600, loss = 0.10258820\n",
      "Iteration 601, loss = 0.10245330\n",
      "Iteration 602, loss = 0.10234536\n",
      "Iteration 603, loss = 0.10223102\n",
      "Iteration 604, loss = 0.10211131\n",
      "Iteration 605, loss = 0.10199382\n",
      "Iteration 606, loss = 0.10190217\n",
      "Iteration 607, loss = 0.10176257\n",
      "Iteration 608, loss = 0.10165336\n",
      "Iteration 609, loss = 0.10154442\n",
      "Iteration 610, loss = 0.10143153\n",
      "Iteration 611, loss = 0.10129443\n",
      "Iteration 612, loss = 0.10119187\n",
      "Iteration 613, loss = 0.10108819\n",
      "Iteration 614, loss = 0.10096849\n",
      "Iteration 615, loss = 0.10084133\n",
      "Iteration 616, loss = 0.10073163\n",
      "Iteration 617, loss = 0.10061753\n",
      "Iteration 618, loss = 0.10051754\n",
      "Iteration 619, loss = 0.10039769\n",
      "Iteration 620, loss = 0.10028722\n",
      "Iteration 621, loss = 0.10017859\n",
      "Iteration 622, loss = 0.10006732\n",
      "Iteration 623, loss = 0.09995528\n",
      "Iteration 624, loss = 0.09983588\n",
      "Iteration 625, loss = 0.09973283\n",
      "Iteration 626, loss = 0.09962801\n",
      "Iteration 627, loss = 0.09952071\n",
      "Iteration 628, loss = 0.09941015\n",
      "Iteration 629, loss = 0.09930979\n",
      "Iteration 630, loss = 0.09921580\n",
      "Iteration 631, loss = 0.09910166\n",
      "Iteration 632, loss = 0.09899592\n",
      "Iteration 633, loss = 0.09890665\n",
      "Iteration 634, loss = 0.09879049\n",
      "Iteration 635, loss = 0.09869617\n",
      "Iteration 636, loss = 0.09858750\n",
      "Iteration 637, loss = 0.09848585\n",
      "Iteration 638, loss = 0.09840594\n",
      "Iteration 639, loss = 0.09829496\n",
      "Iteration 640, loss = 0.09819365\n",
      "Iteration 641, loss = 0.09809224\n",
      "Iteration 642, loss = 0.09799480\n",
      "Iteration 643, loss = 0.09789713\n",
      "Iteration 644, loss = 0.09779722\n",
      "Iteration 645, loss = 0.09769264\n",
      "Iteration 646, loss = 0.09759668\n",
      "Iteration 647, loss = 0.09751110\n",
      "Iteration 648, loss = 0.09740834\n",
      "Iteration 649, loss = 0.09730009\n",
      "Iteration 650, loss = 0.09720851\n",
      "Iteration 651, loss = 0.09710144\n",
      "Iteration 652, loss = 0.09702646\n",
      "Iteration 653, loss = 0.09691314\n",
      "Iteration 654, loss = 0.09682789\n",
      "Iteration 655, loss = 0.09672522\n",
      "Iteration 656, loss = 0.09663128\n",
      "Iteration 657, loss = 0.09654014\n",
      "Iteration 658, loss = 0.09644499\n",
      "Iteration 659, loss = 0.09636073\n",
      "Iteration 660, loss = 0.09626210\n",
      "Iteration 661, loss = 0.09617064\n",
      "Iteration 662, loss = 0.09608066\n",
      "Iteration 663, loss = 0.09599712\n",
      "Iteration 664, loss = 0.09590161\n",
      "Iteration 665, loss = 0.09581743\n",
      "Iteration 666, loss = 0.09573123\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.44359765\n",
      "Iteration 2, loss = 2.41185396\n",
      "Iteration 3, loss = 2.37512516\n",
      "Iteration 4, loss = 2.34121779\n",
      "Iteration 5, loss = 2.31339434\n",
      "Iteration 6, loss = 2.28918389\n",
      "Iteration 7, loss = 2.26687805\n",
      "Iteration 8, loss = 2.24616431\n",
      "Iteration 9, loss = 2.22625512\n",
      "Iteration 10, loss = 2.20657155\n",
      "Iteration 11, loss = 2.18694039\n",
      "Iteration 12, loss = 2.16688904\n",
      "Iteration 13, loss = 2.14658419\n",
      "Iteration 14, loss = 2.12599089\n",
      "Iteration 15, loss = 2.10524896\n",
      "Iteration 16, loss = 2.08379722\n",
      "Iteration 17, loss = 2.06152296\n",
      "Iteration 18, loss = 2.03876395\n",
      "Iteration 19, loss = 2.01564162\n",
      "Iteration 20, loss = 1.99188769\n",
      "Iteration 21, loss = 1.96742655\n",
      "Iteration 22, loss = 1.94257254\n",
      "Iteration 23, loss = 1.91730357\n",
      "Iteration 24, loss = 1.89170495\n",
      "Iteration 25, loss = 1.86594098\n",
      "Iteration 26, loss = 1.84020363\n",
      "Iteration 27, loss = 1.81423391\n",
      "Iteration 28, loss = 1.78821145\n",
      "Iteration 29, loss = 1.76186177\n",
      "Iteration 30, loss = 1.73517980\n",
      "Iteration 31, loss = 1.70847723\n",
      "Iteration 32, loss = 1.68198782\n",
      "Iteration 33, loss = 1.65564174\n",
      "Iteration 34, loss = 1.62908790\n",
      "Iteration 35, loss = 1.60276364\n",
      "Iteration 36, loss = 1.57695069\n",
      "Iteration 37, loss = 1.55123177\n",
      "Iteration 38, loss = 1.52561581\n",
      "Iteration 39, loss = 1.49994575\n",
      "Iteration 40, loss = 1.47492027\n",
      "Iteration 41, loss = 1.44989619\n",
      "Iteration 42, loss = 1.42521955\n",
      "Iteration 43, loss = 1.40093125\n",
      "Iteration 44, loss = 1.37704125\n",
      "Iteration 45, loss = 1.35360575\n",
      "Iteration 46, loss = 1.33057781\n",
      "Iteration 47, loss = 1.30767406\n",
      "Iteration 48, loss = 1.28508890\n",
      "Iteration 49, loss = 1.26310188\n",
      "Iteration 50, loss = 1.24139193\n",
      "Iteration 51, loss = 1.22026590\n",
      "Iteration 52, loss = 1.19913000\n",
      "Iteration 53, loss = 1.17874344\n",
      "Iteration 54, loss = 1.15891593\n",
      "Iteration 55, loss = 1.13932273\n",
      "Iteration 56, loss = 1.11996916\n",
      "Iteration 57, loss = 1.10110831\n",
      "Iteration 58, loss = 1.08273018\n",
      "Iteration 59, loss = 1.06470593\n",
      "Iteration 60, loss = 1.04699761\n",
      "Iteration 61, loss = 1.02967071\n",
      "Iteration 62, loss = 1.01270436\n",
      "Iteration 63, loss = 0.99611270\n",
      "Iteration 64, loss = 0.97993125\n",
      "Iteration 65, loss = 0.96422629\n",
      "Iteration 66, loss = 0.94889893\n",
      "Iteration 67, loss = 0.93391162\n",
      "Iteration 68, loss = 0.91930948\n",
      "Iteration 69, loss = 0.90500650\n",
      "Iteration 70, loss = 0.89112330\n",
      "Iteration 71, loss = 0.87770127\n",
      "Iteration 72, loss = 0.86458989\n",
      "Iteration 73, loss = 0.85171298\n",
      "Iteration 74, loss = 0.83879986\n",
      "Iteration 75, loss = 0.82641113\n",
      "Iteration 76, loss = 0.81444900\n",
      "Iteration 77, loss = 0.80276713\n",
      "Iteration 78, loss = 0.79132140\n",
      "Iteration 79, loss = 0.78037166\n",
      "Iteration 80, loss = 0.76953585\n",
      "Iteration 81, loss = 0.75895346\n",
      "Iteration 82, loss = 0.74860181\n",
      "Iteration 83, loss = 0.73848831\n",
      "Iteration 84, loss = 0.72871851\n",
      "Iteration 85, loss = 0.71929754\n",
      "Iteration 86, loss = 0.70989818\n",
      "Iteration 87, loss = 0.70066214\n",
      "Iteration 88, loss = 0.69173806\n",
      "Iteration 89, loss = 0.68293493\n",
      "Iteration 90, loss = 0.67442782\n",
      "Iteration 91, loss = 0.66613666\n",
      "Iteration 92, loss = 0.65812306\n",
      "Iteration 93, loss = 0.65005375\n",
      "Iteration 94, loss = 0.64238617\n",
      "Iteration 95, loss = 0.63474102\n",
      "Iteration 96, loss = 0.62744503\n",
      "Iteration 97, loss = 0.62017004\n",
      "Iteration 98, loss = 0.61307141\n",
      "Iteration 99, loss = 0.60611467\n",
      "Iteration 100, loss = 0.59928192\n",
      "Iteration 101, loss = 0.59266485\n",
      "Iteration 102, loss = 0.58632009\n",
      "Iteration 103, loss = 0.57991880\n",
      "Iteration 104, loss = 0.57367700\n",
      "Iteration 105, loss = 0.56765900\n",
      "Iteration 106, loss = 0.56177106\n",
      "Iteration 107, loss = 0.55595308\n",
      "Iteration 108, loss = 0.55041592\n",
      "Iteration 109, loss = 0.54480055\n",
      "Iteration 110, loss = 0.53924457\n",
      "Iteration 111, loss = 0.53392466\n",
      "Iteration 112, loss = 0.52888983\n",
      "Iteration 113, loss = 0.52375623\n",
      "Iteration 114, loss = 0.51883672\n",
      "Iteration 115, loss = 0.51373147\n",
      "Iteration 116, loss = 0.50901930\n",
      "Iteration 117, loss = 0.50407148\n",
      "Iteration 118, loss = 0.49933949\n",
      "Iteration 119, loss = 0.49494837\n",
      "Iteration 120, loss = 0.49042396\n",
      "Iteration 121, loss = 0.48602336\n",
      "Iteration 122, loss = 0.48163608\n",
      "Iteration 123, loss = 0.47754157\n",
      "Iteration 124, loss = 0.47330918\n",
      "Iteration 125, loss = 0.46904474\n",
      "Iteration 126, loss = 0.46507841\n",
      "Iteration 127, loss = 0.46109905\n",
      "Iteration 128, loss = 0.45742000\n",
      "Iteration 129, loss = 0.45358529\n",
      "Iteration 130, loss = 0.44987620\n",
      "Iteration 131, loss = 0.44621069\n",
      "Iteration 132, loss = 0.44260499\n",
      "Iteration 133, loss = 0.43925818\n",
      "Iteration 134, loss = 0.43568245\n",
      "Iteration 135, loss = 0.43222345\n",
      "Iteration 136, loss = 0.42884284\n",
      "Iteration 137, loss = 0.42566703\n",
      "Iteration 138, loss = 0.42216544\n",
      "Iteration 139, loss = 0.41904273\n",
      "Iteration 140, loss = 0.41594506\n",
      "Iteration 141, loss = 0.41285466\n",
      "Iteration 142, loss = 0.40996995\n",
      "Iteration 143, loss = 0.40690509\n",
      "Iteration 144, loss = 0.40394209\n",
      "Iteration 145, loss = 0.40102771\n",
      "Iteration 146, loss = 0.39833989\n",
      "Iteration 147, loss = 0.39545618\n",
      "Iteration 148, loss = 0.39264957\n",
      "Iteration 149, loss = 0.38992594\n",
      "Iteration 150, loss = 0.38724620\n",
      "Iteration 151, loss = 0.38455932\n",
      "Iteration 152, loss = 0.38194553\n",
      "Iteration 153, loss = 0.37941673\n",
      "Iteration 154, loss = 0.37676939\n",
      "Iteration 155, loss = 0.37420640\n",
      "Iteration 156, loss = 0.37181912\n",
      "Iteration 157, loss = 0.36925342\n",
      "Iteration 158, loss = 0.36691044\n",
      "Iteration 159, loss = 0.36454423\n",
      "Iteration 160, loss = 0.36222686\n",
      "Iteration 161, loss = 0.35985119\n",
      "Iteration 162, loss = 0.35742031\n",
      "Iteration 163, loss = 0.35509245\n",
      "Iteration 164, loss = 0.35288905\n",
      "Iteration 165, loss = 0.35058369\n",
      "Iteration 166, loss = 0.34835011\n",
      "Iteration 167, loss = 0.34627475\n",
      "Iteration 168, loss = 0.34415630\n",
      "Iteration 169, loss = 0.34201182\n",
      "Iteration 170, loss = 0.33990912\n",
      "Iteration 171, loss = 0.33786600\n",
      "Iteration 172, loss = 0.33580342\n",
      "Iteration 173, loss = 0.33385663\n",
      "Iteration 174, loss = 0.33190988\n",
      "Iteration 175, loss = 0.32998677\n",
      "Iteration 176, loss = 0.32801213\n",
      "Iteration 177, loss = 0.32615286\n",
      "Iteration 178, loss = 0.32427011\n",
      "Iteration 179, loss = 0.32239889\n",
      "Iteration 180, loss = 0.32057088\n",
      "Iteration 181, loss = 0.31877083\n",
      "Iteration 182, loss = 0.31689291\n",
      "Iteration 183, loss = 0.31514760\n",
      "Iteration 184, loss = 0.31338410\n",
      "Iteration 185, loss = 0.31176582\n",
      "Iteration 186, loss = 0.31012007\n",
      "Iteration 187, loss = 0.30835730\n",
      "Iteration 188, loss = 0.30671039\n",
      "Iteration 189, loss = 0.30496777\n",
      "Iteration 190, loss = 0.30339478\n",
      "Iteration 191, loss = 0.30172485\n",
      "Iteration 192, loss = 0.30012176\n",
      "Iteration 193, loss = 0.29856612\n",
      "Iteration 194, loss = 0.29699306\n",
      "Iteration 195, loss = 0.29538065\n",
      "Iteration 196, loss = 0.29391521\n",
      "Iteration 197, loss = 0.29247893\n",
      "Iteration 198, loss = 0.29092568\n",
      "Iteration 199, loss = 0.28942785\n",
      "Iteration 200, loss = 0.28793485\n",
      "Iteration 201, loss = 0.28642779\n",
      "Iteration 202, loss = 0.28496982\n",
      "Iteration 203, loss = 0.28369364\n",
      "Iteration 204, loss = 0.28222515\n",
      "Iteration 205, loss = 0.28080964\n",
      "Iteration 206, loss = 0.27939019\n",
      "Iteration 207, loss = 0.27804254\n",
      "Iteration 208, loss = 0.27674353\n",
      "Iteration 209, loss = 0.27538240\n",
      "Iteration 210, loss = 0.27409630\n",
      "Iteration 211, loss = 0.27277226\n",
      "Iteration 212, loss = 0.27150475\n",
      "Iteration 213, loss = 0.27019581\n",
      "Iteration 214, loss = 0.26899033\n",
      "Iteration 215, loss = 0.26773744\n",
      "Iteration 216, loss = 0.26648967\n",
      "Iteration 217, loss = 0.26526558\n",
      "Iteration 218, loss = 0.26404744\n",
      "Iteration 219, loss = 0.26286506\n",
      "Iteration 220, loss = 0.26163693\n",
      "Iteration 221, loss = 0.26051309\n",
      "Iteration 222, loss = 0.25932725\n",
      "Iteration 223, loss = 0.25822347\n",
      "Iteration 224, loss = 0.25709455\n",
      "Iteration 225, loss = 0.25580756\n",
      "Iteration 226, loss = 0.25464286\n",
      "Iteration 227, loss = 0.25350180\n",
      "Iteration 228, loss = 0.25239041\n",
      "Iteration 229, loss = 0.25137020\n",
      "Iteration 230, loss = 0.25033526\n",
      "Iteration 231, loss = 0.24931646\n",
      "Iteration 232, loss = 0.24811620\n",
      "Iteration 233, loss = 0.24706137\n",
      "Iteration 234, loss = 0.24596658\n",
      "Iteration 235, loss = 0.24488488\n",
      "Iteration 236, loss = 0.24388438\n",
      "Iteration 237, loss = 0.24287241\n",
      "Iteration 238, loss = 0.24191701\n",
      "Iteration 239, loss = 0.24088995\n",
      "Iteration 240, loss = 0.23986996\n",
      "Iteration 241, loss = 0.23888348\n",
      "Iteration 242, loss = 0.23793770\n",
      "Iteration 243, loss = 0.23696438\n",
      "Iteration 244, loss = 0.23598573\n",
      "Iteration 245, loss = 0.23505324\n",
      "Iteration 246, loss = 0.23419425\n",
      "Iteration 247, loss = 0.23320175\n",
      "Iteration 248, loss = 0.23233159\n",
      "Iteration 249, loss = 0.23139048\n",
      "Iteration 250, loss = 0.23053880\n",
      "Iteration 251, loss = 0.22953855\n",
      "Iteration 252, loss = 0.22865479\n",
      "Iteration 253, loss = 0.22769751\n",
      "Iteration 254, loss = 0.22681553\n",
      "Iteration 255, loss = 0.22591105\n",
      "Iteration 256, loss = 0.22506094\n",
      "Iteration 257, loss = 0.22424943\n",
      "Iteration 258, loss = 0.22337501\n",
      "Iteration 259, loss = 0.22252095\n",
      "Iteration 260, loss = 0.22166746\n",
      "Iteration 261, loss = 0.22079160\n",
      "Iteration 262, loss = 0.22002088\n",
      "Iteration 263, loss = 0.21927105\n",
      "Iteration 264, loss = 0.21839990\n",
      "Iteration 265, loss = 0.21762962\n",
      "Iteration 266, loss = 0.21678677\n",
      "Iteration 267, loss = 0.21600842\n",
      "Iteration 268, loss = 0.21520121\n",
      "Iteration 269, loss = 0.21435919\n",
      "Iteration 270, loss = 0.21359932\n",
      "Iteration 271, loss = 0.21286014\n",
      "Iteration 272, loss = 0.21212755\n",
      "Iteration 273, loss = 0.21138180\n",
      "Iteration 274, loss = 0.21064724\n",
      "Iteration 275, loss = 0.20987061\n",
      "Iteration 276, loss = 0.20913004\n",
      "Iteration 277, loss = 0.20834349\n",
      "Iteration 278, loss = 0.20762431\n",
      "Iteration 279, loss = 0.20695392\n",
      "Iteration 280, loss = 0.20622648\n",
      "Iteration 281, loss = 0.20549866\n",
      "Iteration 282, loss = 0.20479980\n",
      "Iteration 283, loss = 0.20406326\n",
      "Iteration 284, loss = 0.20335879\n",
      "Iteration 285, loss = 0.20263437\n",
      "Iteration 286, loss = 0.20195783\n",
      "Iteration 287, loss = 0.20125267\n",
      "Iteration 288, loss = 0.20060323\n",
      "Iteration 289, loss = 0.19986997\n",
      "Iteration 290, loss = 0.19924070\n",
      "Iteration 291, loss = 0.19855653\n",
      "Iteration 292, loss = 0.19788497\n",
      "Iteration 293, loss = 0.19724532\n",
      "Iteration 294, loss = 0.19660820\n",
      "Iteration 295, loss = 0.19592220\n",
      "Iteration 296, loss = 0.19527870\n",
      "Iteration 297, loss = 0.19464761\n",
      "Iteration 298, loss = 0.19403114\n",
      "Iteration 299, loss = 0.19338843\n",
      "Iteration 300, loss = 0.19275620\n",
      "Iteration 301, loss = 0.19212713\n",
      "Iteration 302, loss = 0.19150298\n",
      "Iteration 303, loss = 0.19091912\n",
      "Iteration 304, loss = 0.19031150\n",
      "Iteration 305, loss = 0.18971773\n",
      "Iteration 306, loss = 0.18914230\n",
      "Iteration 307, loss = 0.18854742\n",
      "Iteration 308, loss = 0.18797854\n",
      "Iteration 309, loss = 0.18742105\n",
      "Iteration 310, loss = 0.18686131\n",
      "Iteration 311, loss = 0.18626665\n",
      "Iteration 312, loss = 0.18572281\n",
      "Iteration 313, loss = 0.18510964\n",
      "Iteration 314, loss = 0.18461426\n",
      "Iteration 315, loss = 0.18408315\n",
      "Iteration 316, loss = 0.18349586\n",
      "Iteration 317, loss = 0.18290841\n",
      "Iteration 318, loss = 0.18234446\n",
      "Iteration 319, loss = 0.18186732\n",
      "Iteration 320, loss = 0.18126505\n",
      "Iteration 321, loss = 0.18072673\n",
      "Iteration 322, loss = 0.18014965\n",
      "Iteration 323, loss = 0.17959773\n",
      "Iteration 324, loss = 0.17911242\n",
      "Iteration 325, loss = 0.17859907\n",
      "Iteration 326, loss = 0.17809910\n",
      "Iteration 327, loss = 0.17758549\n",
      "Iteration 328, loss = 0.17711377\n",
      "Iteration 329, loss = 0.17654349\n",
      "Iteration 330, loss = 0.17601429\n",
      "Iteration 331, loss = 0.17553393\n",
      "Iteration 332, loss = 0.17501551\n",
      "Iteration 333, loss = 0.17452367\n",
      "Iteration 334, loss = 0.17409752\n",
      "Iteration 335, loss = 0.17354991\n",
      "Iteration 336, loss = 0.17305272\n",
      "Iteration 337, loss = 0.17258308\n",
      "Iteration 338, loss = 0.17208149\n",
      "Iteration 339, loss = 0.17158799\n",
      "Iteration 340, loss = 0.17111910\n",
      "Iteration 341, loss = 0.17070899\n",
      "Iteration 342, loss = 0.17023943\n",
      "Iteration 343, loss = 0.16978401\n",
      "Iteration 344, loss = 0.16932112\n",
      "Iteration 345, loss = 0.16885812\n",
      "Iteration 346, loss = 0.16838454\n",
      "Iteration 347, loss = 0.16792164\n",
      "Iteration 348, loss = 0.16746289\n",
      "Iteration 349, loss = 0.16701165\n",
      "Iteration 350, loss = 0.16659916\n",
      "Iteration 351, loss = 0.16611690\n",
      "Iteration 352, loss = 0.16569093\n",
      "Iteration 353, loss = 0.16524897\n",
      "Iteration 354, loss = 0.16481791\n",
      "Iteration 355, loss = 0.16442515\n",
      "Iteration 356, loss = 0.16395216\n",
      "Iteration 357, loss = 0.16356119\n",
      "Iteration 358, loss = 0.16310994\n",
      "Iteration 359, loss = 0.16268652\n",
      "Iteration 360, loss = 0.16224900\n",
      "Iteration 361, loss = 0.16179497\n",
      "Iteration 362, loss = 0.16136587\n",
      "Iteration 363, loss = 0.16096893\n",
      "Iteration 364, loss = 0.16056362\n",
      "Iteration 365, loss = 0.16012837\n",
      "Iteration 366, loss = 0.15973949\n",
      "Iteration 367, loss = 0.15933915\n",
      "Iteration 368, loss = 0.15896962\n",
      "Iteration 369, loss = 0.15857003\n",
      "Iteration 370, loss = 0.15817118\n",
      "Iteration 371, loss = 0.15779669\n",
      "Iteration 372, loss = 0.15740796\n",
      "Iteration 373, loss = 0.15703049\n",
      "Iteration 374, loss = 0.15664136\n",
      "Iteration 375, loss = 0.15621347\n",
      "Iteration 376, loss = 0.15582851\n",
      "Iteration 377, loss = 0.15545816\n",
      "Iteration 378, loss = 0.15515178\n",
      "Iteration 379, loss = 0.15477410\n",
      "Iteration 380, loss = 0.15433685\n",
      "Iteration 381, loss = 0.15397663\n",
      "Iteration 382, loss = 0.15359288\n",
      "Iteration 383, loss = 0.15321758\n",
      "Iteration 384, loss = 0.15287642\n",
      "Iteration 385, loss = 0.15247428\n",
      "Iteration 386, loss = 0.15213153\n",
      "Iteration 387, loss = 0.15178154\n",
      "Iteration 388, loss = 0.15143895\n",
      "Iteration 389, loss = 0.15106998\n",
      "Iteration 390, loss = 0.15071327\n",
      "Iteration 391, loss = 0.15037672\n",
      "Iteration 392, loss = 0.15003464\n",
      "Iteration 393, loss = 0.14968516\n",
      "Iteration 394, loss = 0.14934937\n",
      "Iteration 395, loss = 0.14899847\n",
      "Iteration 396, loss = 0.14869530\n",
      "Iteration 397, loss = 0.14834968\n",
      "Iteration 398, loss = 0.14802054\n",
      "Iteration 399, loss = 0.14765219\n",
      "Iteration 400, loss = 0.14732651\n",
      "Iteration 401, loss = 0.14700287\n",
      "Iteration 402, loss = 0.14666853\n",
      "Iteration 403, loss = 0.14636765\n",
      "Iteration 404, loss = 0.14604946\n",
      "Iteration 405, loss = 0.14569293\n",
      "Iteration 406, loss = 0.14540007\n",
      "Iteration 407, loss = 0.14509006\n",
      "Iteration 408, loss = 0.14477925\n",
      "Iteration 409, loss = 0.14448103\n",
      "Iteration 410, loss = 0.14412410\n",
      "Iteration 411, loss = 0.14383809\n",
      "Iteration 412, loss = 0.14354068\n",
      "Iteration 413, loss = 0.14320917\n",
      "Iteration 414, loss = 0.14291384\n",
      "Iteration 415, loss = 0.14258942\n",
      "Iteration 416, loss = 0.14228189\n",
      "Iteration 417, loss = 0.14195797\n",
      "Iteration 418, loss = 0.14164523\n",
      "Iteration 419, loss = 0.14135229\n",
      "Iteration 420, loss = 0.14105987\n",
      "Iteration 421, loss = 0.14079929\n",
      "Iteration 422, loss = 0.14047502\n",
      "Iteration 423, loss = 0.14019960\n",
      "Iteration 424, loss = 0.13988926\n",
      "Iteration 425, loss = 0.13962932\n",
      "Iteration 426, loss = 0.13934231\n",
      "Iteration 427, loss = 0.13904315\n",
      "Iteration 428, loss = 0.13876678\n",
      "Iteration 429, loss = 0.13849142\n",
      "Iteration 430, loss = 0.13821360\n",
      "Iteration 431, loss = 0.13792224\n",
      "Iteration 432, loss = 0.13765840\n",
      "Iteration 433, loss = 0.13737376\n",
      "Iteration 434, loss = 0.13710006\n",
      "Iteration 435, loss = 0.13682301\n",
      "Iteration 436, loss = 0.13654086\n",
      "Iteration 437, loss = 0.13626193\n",
      "Iteration 438, loss = 0.13600763\n",
      "Iteration 439, loss = 0.13574661\n",
      "Iteration 440, loss = 0.13548877\n",
      "Iteration 441, loss = 0.13519012\n",
      "Iteration 442, loss = 0.13493187\n",
      "Iteration 443, loss = 0.13466805\n",
      "Iteration 444, loss = 0.13441101\n",
      "Iteration 445, loss = 0.13417046\n",
      "Iteration 446, loss = 0.13392648\n",
      "Iteration 447, loss = 0.13363872\n",
      "Iteration 448, loss = 0.13335689\n",
      "Iteration 449, loss = 0.13308007\n",
      "Iteration 450, loss = 0.13281396\n",
      "Iteration 451, loss = 0.13258959\n",
      "Iteration 452, loss = 0.13235011\n",
      "Iteration 453, loss = 0.13208864\n",
      "Iteration 454, loss = 0.13183742\n",
      "Iteration 455, loss = 0.13157463\n",
      "Iteration 456, loss = 0.13135125\n",
      "Iteration 457, loss = 0.13110661\n",
      "Iteration 458, loss = 0.13086234\n",
      "Iteration 459, loss = 0.13060158\n",
      "Iteration 460, loss = 0.13036764\n",
      "Iteration 461, loss = 0.13014682\n",
      "Iteration 462, loss = 0.12989714\n",
      "Iteration 463, loss = 0.12966178\n",
      "Iteration 464, loss = 0.12940027\n",
      "Iteration 465, loss = 0.12919041\n",
      "Iteration 466, loss = 0.12896496\n",
      "Iteration 467, loss = 0.12872206\n",
      "Iteration 468, loss = 0.12850357\n",
      "Iteration 469, loss = 0.12827519\n",
      "Iteration 470, loss = 0.12803025\n",
      "Iteration 471, loss = 0.12778577\n",
      "Iteration 472, loss = 0.12757554\n",
      "Iteration 473, loss = 0.12736378\n",
      "Iteration 474, loss = 0.12713716\n",
      "Iteration 475, loss = 0.12690510\n",
      "Iteration 476, loss = 0.12669081\n",
      "Iteration 477, loss = 0.12645414\n",
      "Iteration 478, loss = 0.12626213\n",
      "Iteration 479, loss = 0.12603422\n",
      "Iteration 480, loss = 0.12580853\n",
      "Iteration 481, loss = 0.12559906\n",
      "Iteration 482, loss = 0.12539650\n",
      "Iteration 483, loss = 0.12517584\n",
      "Iteration 484, loss = 0.12499948\n",
      "Iteration 485, loss = 0.12474340\n",
      "Iteration 486, loss = 0.12452488\n",
      "Iteration 487, loss = 0.12434023\n",
      "Iteration 488, loss = 0.12413584\n",
      "Iteration 489, loss = 0.12392393\n",
      "Iteration 490, loss = 0.12368633\n",
      "Iteration 491, loss = 0.12348088\n",
      "Iteration 492, loss = 0.12327332\n",
      "Iteration 493, loss = 0.12307990\n",
      "Iteration 494, loss = 0.12286925\n",
      "Iteration 495, loss = 0.12265641\n",
      "Iteration 496, loss = 0.12246630\n",
      "Iteration 497, loss = 0.12225606\n",
      "Iteration 498, loss = 0.12204336\n",
      "Iteration 499, loss = 0.12182876\n",
      "Iteration 500, loss = 0.12163715\n",
      "Iteration 501, loss = 0.12143679\n",
      "Iteration 502, loss = 0.12124325\n",
      "Iteration 503, loss = 0.12104310\n",
      "Iteration 504, loss = 0.12086801\n",
      "Iteration 505, loss = 0.12068448\n",
      "Iteration 506, loss = 0.12048040\n",
      "Iteration 507, loss = 0.12028392\n",
      "Iteration 508, loss = 0.12009829\n",
      "Iteration 509, loss = 0.11990992\n",
      "Iteration 510, loss = 0.11973331\n",
      "Iteration 511, loss = 0.11951607\n",
      "Iteration 512, loss = 0.11934534\n",
      "Iteration 513, loss = 0.11914072\n",
      "Iteration 514, loss = 0.11894149\n",
      "Iteration 515, loss = 0.11875131\n",
      "Iteration 516, loss = 0.11857820\n",
      "Iteration 517, loss = 0.11840018\n",
      "Iteration 518, loss = 0.11821512\n",
      "Iteration 519, loss = 0.11802934\n",
      "Iteration 520, loss = 0.11784538\n",
      "Iteration 521, loss = 0.11768392\n",
      "Iteration 522, loss = 0.11749240\n",
      "Iteration 523, loss = 0.11729529\n",
      "Iteration 524, loss = 0.11710493\n",
      "Iteration 525, loss = 0.11692964\n",
      "Iteration 526, loss = 0.11674471\n",
      "Iteration 527, loss = 0.11657484\n",
      "Iteration 528, loss = 0.11643071\n",
      "Iteration 529, loss = 0.11625132\n",
      "Iteration 530, loss = 0.11605565\n",
      "Iteration 531, loss = 0.11588775\n",
      "Iteration 532, loss = 0.11572725\n",
      "Iteration 533, loss = 0.11555394\n",
      "Iteration 534, loss = 0.11536985\n",
      "Iteration 535, loss = 0.11521790\n",
      "Iteration 536, loss = 0.11503590\n",
      "Iteration 537, loss = 0.11486964\n",
      "Iteration 538, loss = 0.11469561\n",
      "Iteration 539, loss = 0.11452306\n",
      "Iteration 540, loss = 0.11436795\n",
      "Iteration 541, loss = 0.11420234\n",
      "Iteration 542, loss = 0.11404128\n",
      "Iteration 543, loss = 0.11388307\n",
      "Iteration 544, loss = 0.11372449\n",
      "Iteration 545, loss = 0.11355097\n",
      "Iteration 546, loss = 0.11340981\n",
      "Iteration 547, loss = 0.11323320\n",
      "Iteration 548, loss = 0.11309817\n",
      "Iteration 549, loss = 0.11293425\n",
      "Iteration 550, loss = 0.11277027\n",
      "Iteration 551, loss = 0.11261695\n",
      "Iteration 552, loss = 0.11246217\n",
      "Iteration 553, loss = 0.11230877\n",
      "Iteration 554, loss = 0.11216313\n",
      "Iteration 555, loss = 0.11198671\n",
      "Iteration 556, loss = 0.11183368\n",
      "Iteration 557, loss = 0.11169105\n",
      "Iteration 558, loss = 0.11154502\n",
      "Iteration 559, loss = 0.11139731\n",
      "Iteration 560, loss = 0.11128493\n",
      "Iteration 561, loss = 0.11110328\n",
      "Iteration 562, loss = 0.11095320\n",
      "Iteration 563, loss = 0.11080548\n",
      "Iteration 564, loss = 0.11066129\n",
      "Iteration 565, loss = 0.11053283\n",
      "Iteration 566, loss = 0.11035052\n",
      "Iteration 567, loss = 0.11019876\n",
      "Iteration 568, loss = 0.11005290\n",
      "Iteration 569, loss = 0.10990372\n",
      "Iteration 570, loss = 0.10976541\n",
      "Iteration 571, loss = 0.10962611\n",
      "Iteration 572, loss = 0.10948079\n",
      "Iteration 573, loss = 0.10933859\n",
      "Iteration 574, loss = 0.10921245\n",
      "Iteration 575, loss = 0.10905371\n",
      "Iteration 576, loss = 0.10892945\n",
      "Iteration 577, loss = 0.10877763\n",
      "Iteration 578, loss = 0.10865150\n",
      "Iteration 579, loss = 0.10849287\n",
      "Iteration 580, loss = 0.10837172\n",
      "Iteration 581, loss = 0.10824268\n",
      "Iteration 582, loss = 0.10809424\n",
      "Iteration 583, loss = 0.10795522\n",
      "Iteration 584, loss = 0.10781969\n",
      "Iteration 585, loss = 0.10768201\n",
      "Iteration 586, loss = 0.10754425\n",
      "Iteration 587, loss = 0.10740536\n",
      "Iteration 588, loss = 0.10728700\n",
      "Iteration 589, loss = 0.10715786\n",
      "Iteration 590, loss = 0.10701199\n",
      "Iteration 591, loss = 0.10689222\n",
      "Iteration 592, loss = 0.10674793\n",
      "Iteration 593, loss = 0.10661456\n",
      "Iteration 594, loss = 0.10647644\n",
      "Iteration 595, loss = 0.10634994\n",
      "Iteration 596, loss = 0.10622196\n",
      "Iteration 597, loss = 0.10609111\n",
      "Iteration 598, loss = 0.10596951\n",
      "Iteration 599, loss = 0.10583789\n",
      "Iteration 600, loss = 0.10572973\n",
      "Iteration 601, loss = 0.10557530\n",
      "Iteration 602, loss = 0.10545820\n",
      "Iteration 603, loss = 0.10534055\n",
      "Iteration 604, loss = 0.10521683\n",
      "Iteration 605, loss = 0.10509116\n",
      "Iteration 606, loss = 0.10497291\n",
      "Iteration 607, loss = 0.10483882\n",
      "Iteration 608, loss = 0.10472281\n",
      "Iteration 609, loss = 0.10461352\n",
      "Iteration 610, loss = 0.10448846\n",
      "Iteration 611, loss = 0.10435055\n",
      "Iteration 612, loss = 0.10423353\n",
      "Iteration 613, loss = 0.10411628\n",
      "Iteration 614, loss = 0.10399332\n",
      "Iteration 615, loss = 0.10386511\n",
      "Iteration 616, loss = 0.10374781\n",
      "Iteration 617, loss = 0.10362727\n",
      "Iteration 618, loss = 0.10352773\n",
      "Iteration 619, loss = 0.10339997\n",
      "Iteration 620, loss = 0.10327044\n",
      "Iteration 621, loss = 0.10316549\n",
      "Iteration 622, loss = 0.10305077\n",
      "Iteration 623, loss = 0.10291904\n",
      "Iteration 624, loss = 0.10279801\n",
      "Iteration 625, loss = 0.10267907\n",
      "Iteration 626, loss = 0.10257075\n",
      "Iteration 627, loss = 0.10245904\n",
      "Iteration 628, loss = 0.10234540\n",
      "Iteration 629, loss = 0.10221779\n",
      "Iteration 630, loss = 0.10212552\n",
      "Iteration 631, loss = 0.10201278\n",
      "Iteration 632, loss = 0.10189546\n",
      "Iteration 633, loss = 0.10178980\n",
      "Iteration 634, loss = 0.10167248\n",
      "Iteration 635, loss = 0.10157121\n",
      "Iteration 636, loss = 0.10145720\n",
      "Iteration 637, loss = 0.10135084\n",
      "Iteration 638, loss = 0.10125781\n",
      "Iteration 639, loss = 0.10115266\n",
      "Iteration 640, loss = 0.10104604\n",
      "Iteration 641, loss = 0.10092897\n",
      "Iteration 642, loss = 0.10083541\n",
      "Iteration 643, loss = 0.10071990\n",
      "Iteration 644, loss = 0.10061518\n",
      "Iteration 645, loss = 0.10050530\n",
      "Iteration 646, loss = 0.10040157\n",
      "Iteration 647, loss = 0.10031323\n",
      "Iteration 648, loss = 0.10020169\n",
      "Iteration 649, loss = 0.10009435\n",
      "Iteration 650, loss = 0.09998472\n",
      "Iteration 651, loss = 0.09987799\n",
      "Iteration 652, loss = 0.09978376\n",
      "Iteration 653, loss = 0.09967486\n",
      "Iteration 654, loss = 0.09958322\n",
      "Iteration 655, loss = 0.09948118\n",
      "Iteration 656, loss = 0.09937359\n",
      "Iteration 657, loss = 0.09928135\n",
      "Iteration 658, loss = 0.09917199\n",
      "Iteration 659, loss = 0.09909294\n",
      "Iteration 660, loss = 0.09898170\n",
      "Iteration 661, loss = 0.09889074\n",
      "Iteration 662, loss = 0.09879818\n",
      "Iteration 663, loss = 0.09870167\n",
      "Iteration 664, loss = 0.09860022\n",
      "Iteration 665, loss = 0.09850727\n",
      "Iteration 666, loss = 0.09841111\n",
      "Iteration 667, loss = 0.09831526\n",
      "Iteration 668, loss = 0.09821440\n",
      "Iteration 669, loss = 0.09812562\n",
      "Iteration 670, loss = 0.09803165\n",
      "Iteration 671, loss = 0.09794170\n",
      "Iteration 672, loss = 0.09785347\n",
      "Iteration 673, loss = 0.09775256\n",
      "Iteration 674, loss = 0.09766114\n",
      "Iteration 675, loss = 0.09756180\n",
      "Iteration 676, loss = 0.09747868\n",
      "Iteration 677, loss = 0.09737697\n",
      "Iteration 678, loss = 0.09729182\n",
      "Iteration 679, loss = 0.09720296\n",
      "Iteration 680, loss = 0.09710703\n",
      "Iteration 681, loss = 0.09701270\n",
      "Iteration 682, loss = 0.09693127\n",
      "Iteration 683, loss = 0.09683908\n",
      "Iteration 684, loss = 0.09675218\n",
      "Iteration 685, loss = 0.09666290\n",
      "Iteration 686, loss = 0.09657974\n",
      "Iteration 687, loss = 0.09649254\n",
      "Iteration 688, loss = 0.09640634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.44109694\n",
      "Iteration 2, loss = 2.40844317\n",
      "Iteration 3, loss = 2.37092659\n",
      "Iteration 4, loss = 2.33662556\n",
      "Iteration 5, loss = 2.30848830\n",
      "Iteration 6, loss = 2.28346759\n",
      "Iteration 7, loss = 2.26095729\n",
      "Iteration 8, loss = 2.24010256\n",
      "Iteration 9, loss = 2.21971397\n",
      "Iteration 10, loss = 2.19961033\n",
      "Iteration 11, loss = 2.17951790\n",
      "Iteration 12, loss = 2.15907531\n",
      "Iteration 13, loss = 2.13823780\n",
      "Iteration 14, loss = 2.11724426\n",
      "Iteration 15, loss = 2.09600528\n",
      "Iteration 16, loss = 2.07400290\n",
      "Iteration 17, loss = 2.05149089\n",
      "Iteration 18, loss = 2.02850705\n",
      "Iteration 19, loss = 2.00503534\n",
      "Iteration 20, loss = 1.98098052\n",
      "Iteration 21, loss = 1.95628482\n",
      "Iteration 22, loss = 1.93092644\n",
      "Iteration 23, loss = 1.90521471\n",
      "Iteration 24, loss = 1.87887736\n",
      "Iteration 25, loss = 1.85226350\n",
      "Iteration 26, loss = 1.82540498\n",
      "Iteration 27, loss = 1.79818718\n",
      "Iteration 28, loss = 1.77096995\n",
      "Iteration 29, loss = 1.74321336\n",
      "Iteration 30, loss = 1.71540604\n",
      "Iteration 31, loss = 1.68730342\n",
      "Iteration 32, loss = 1.65969854\n",
      "Iteration 33, loss = 1.63162106\n",
      "Iteration 34, loss = 1.60359461\n",
      "Iteration 35, loss = 1.57563542\n",
      "Iteration 36, loss = 1.54820425\n",
      "Iteration 37, loss = 1.52079221\n",
      "Iteration 38, loss = 1.49368500\n",
      "Iteration 39, loss = 1.46678408\n",
      "Iteration 40, loss = 1.44026937\n",
      "Iteration 41, loss = 1.41399481\n",
      "Iteration 42, loss = 1.38792409\n",
      "Iteration 43, loss = 1.36242013\n",
      "Iteration 44, loss = 1.33727918\n",
      "Iteration 45, loss = 1.31258665\n",
      "Iteration 46, loss = 1.28812062\n",
      "Iteration 47, loss = 1.26400087\n",
      "Iteration 48, loss = 1.24034721\n",
      "Iteration 49, loss = 1.21731509\n",
      "Iteration 50, loss = 1.19466584\n",
      "Iteration 51, loss = 1.17253704\n",
      "Iteration 52, loss = 1.15066246\n",
      "Iteration 53, loss = 1.12964602\n",
      "Iteration 54, loss = 1.10861566\n",
      "Iteration 55, loss = 1.08806270\n",
      "Iteration 56, loss = 1.06817107\n",
      "Iteration 57, loss = 1.04874696\n",
      "Iteration 58, loss = 1.02989736\n",
      "Iteration 59, loss = 1.01121580\n",
      "Iteration 60, loss = 0.99292205\n",
      "Iteration 61, loss = 0.97507485\n",
      "Iteration 62, loss = 0.95769673\n",
      "Iteration 63, loss = 0.94065606\n",
      "Iteration 64, loss = 0.92394210\n",
      "Iteration 65, loss = 0.90767040\n",
      "Iteration 66, loss = 0.89207890\n",
      "Iteration 67, loss = 0.87673202\n",
      "Iteration 68, loss = 0.86175984\n",
      "Iteration 69, loss = 0.84713201\n",
      "Iteration 70, loss = 0.83318320\n",
      "Iteration 71, loss = 0.81931745\n",
      "Iteration 72, loss = 0.80588755\n",
      "Iteration 73, loss = 0.79286070\n",
      "Iteration 74, loss = 0.77983652\n",
      "Iteration 75, loss = 0.76737289\n",
      "Iteration 76, loss = 0.75528394\n",
      "Iteration 77, loss = 0.74347729\n",
      "Iteration 78, loss = 0.73187383\n",
      "Iteration 79, loss = 0.72066549\n",
      "Iteration 80, loss = 0.70957524\n",
      "Iteration 81, loss = 0.69895586\n",
      "Iteration 82, loss = 0.68874060\n",
      "Iteration 83, loss = 0.67855299\n",
      "Iteration 84, loss = 0.66893666\n",
      "Iteration 85, loss = 0.65936604\n",
      "Iteration 86, loss = 0.65000704\n",
      "Iteration 87, loss = 0.64092333\n",
      "Iteration 88, loss = 0.63210973\n",
      "Iteration 89, loss = 0.62355583\n",
      "Iteration 90, loss = 0.61535605\n",
      "Iteration 91, loss = 0.60713174\n",
      "Iteration 92, loss = 0.59925389\n",
      "Iteration 93, loss = 0.59152817\n",
      "Iteration 94, loss = 0.58405422\n",
      "Iteration 95, loss = 0.57673469\n",
      "Iteration 96, loss = 0.56954847\n",
      "Iteration 97, loss = 0.56251279\n",
      "Iteration 98, loss = 0.55564877\n",
      "Iteration 99, loss = 0.54900018\n",
      "Iteration 100, loss = 0.54240902\n",
      "Iteration 101, loss = 0.53603464\n",
      "Iteration 102, loss = 0.52992819\n",
      "Iteration 103, loss = 0.52386215\n",
      "Iteration 104, loss = 0.51796616\n",
      "Iteration 105, loss = 0.51218911\n",
      "Iteration 106, loss = 0.50653895\n",
      "Iteration 107, loss = 0.50098054\n",
      "Iteration 108, loss = 0.49581055\n",
      "Iteration 109, loss = 0.49049092\n",
      "Iteration 110, loss = 0.48514777\n",
      "Iteration 111, loss = 0.48003354\n",
      "Iteration 112, loss = 0.47513284\n",
      "Iteration 113, loss = 0.47041168\n",
      "Iteration 114, loss = 0.46582560\n",
      "Iteration 115, loss = 0.46115812\n",
      "Iteration 116, loss = 0.45660613\n",
      "Iteration 117, loss = 0.45203113\n",
      "Iteration 118, loss = 0.44746721\n",
      "Iteration 119, loss = 0.44330232\n",
      "Iteration 120, loss = 0.43922081\n",
      "Iteration 121, loss = 0.43514573\n",
      "Iteration 122, loss = 0.43109978\n",
      "Iteration 123, loss = 0.42717010\n",
      "Iteration 124, loss = 0.42329812\n",
      "Iteration 125, loss = 0.41940902\n",
      "Iteration 126, loss = 0.41566974\n",
      "Iteration 127, loss = 0.41194505\n",
      "Iteration 128, loss = 0.40836509\n",
      "Iteration 129, loss = 0.40486145\n",
      "Iteration 130, loss = 0.40148136\n",
      "Iteration 131, loss = 0.39813386\n",
      "Iteration 132, loss = 0.39479736\n",
      "Iteration 133, loss = 0.39159306\n",
      "Iteration 134, loss = 0.38833978\n",
      "Iteration 135, loss = 0.38516443\n",
      "Iteration 136, loss = 0.38208246\n",
      "Iteration 137, loss = 0.37915062\n",
      "Iteration 138, loss = 0.37592993\n",
      "Iteration 139, loss = 0.37306045\n",
      "Iteration 140, loss = 0.37021220\n",
      "Iteration 141, loss = 0.36730359\n",
      "Iteration 142, loss = 0.36452210\n",
      "Iteration 143, loss = 0.36179306\n",
      "Iteration 144, loss = 0.35914948\n",
      "Iteration 145, loss = 0.35634958\n",
      "Iteration 146, loss = 0.35380787\n",
      "Iteration 147, loss = 0.35123279\n",
      "Iteration 148, loss = 0.34864217\n",
      "Iteration 149, loss = 0.34614142\n",
      "Iteration 150, loss = 0.34364836\n",
      "Iteration 151, loss = 0.34123884\n",
      "Iteration 152, loss = 0.33879849\n",
      "Iteration 153, loss = 0.33645471\n",
      "Iteration 154, loss = 0.33407148\n",
      "Iteration 155, loss = 0.33171781\n",
      "Iteration 156, loss = 0.32950564\n",
      "Iteration 157, loss = 0.32730077\n",
      "Iteration 158, loss = 0.32511542\n",
      "Iteration 159, loss = 0.32296442\n",
      "Iteration 160, loss = 0.32084275\n",
      "Iteration 161, loss = 0.31867598\n",
      "Iteration 162, loss = 0.31657131\n",
      "Iteration 163, loss = 0.31452128\n",
      "Iteration 164, loss = 0.31249710\n",
      "Iteration 165, loss = 0.31045968\n",
      "Iteration 166, loss = 0.30857713\n",
      "Iteration 167, loss = 0.30668641\n",
      "Iteration 168, loss = 0.30479144\n",
      "Iteration 169, loss = 0.30292648\n",
      "Iteration 170, loss = 0.30100111\n",
      "Iteration 171, loss = 0.29904215\n",
      "Iteration 172, loss = 0.29721442\n",
      "Iteration 173, loss = 0.29536911\n",
      "Iteration 174, loss = 0.29373608\n",
      "Iteration 175, loss = 0.29187543\n",
      "Iteration 176, loss = 0.29015247\n",
      "Iteration 177, loss = 0.28845254\n",
      "Iteration 178, loss = 0.28677614\n",
      "Iteration 179, loss = 0.28513014\n",
      "Iteration 180, loss = 0.28348695\n",
      "Iteration 181, loss = 0.28193921\n",
      "Iteration 182, loss = 0.28036896\n",
      "Iteration 183, loss = 0.27870048\n",
      "Iteration 184, loss = 0.27716808\n",
      "Iteration 185, loss = 0.27563764\n",
      "Iteration 186, loss = 0.27415771\n",
      "Iteration 187, loss = 0.27261777\n",
      "Iteration 188, loss = 0.27110981\n",
      "Iteration 189, loss = 0.26962839\n",
      "Iteration 190, loss = 0.26821605\n",
      "Iteration 191, loss = 0.26675014\n",
      "Iteration 192, loss = 0.26531123\n",
      "Iteration 193, loss = 0.26399788\n",
      "Iteration 194, loss = 0.26258726\n",
      "Iteration 195, loss = 0.26116591\n",
      "Iteration 196, loss = 0.25976065\n",
      "Iteration 197, loss = 0.25846497\n",
      "Iteration 198, loss = 0.25707751\n",
      "Iteration 199, loss = 0.25578398\n",
      "Iteration 200, loss = 0.25440696\n",
      "Iteration 201, loss = 0.25317638\n",
      "Iteration 202, loss = 0.25186248\n",
      "Iteration 203, loss = 0.25069587\n",
      "Iteration 204, loss = 0.24944524\n",
      "Iteration 205, loss = 0.24822386\n",
      "Iteration 206, loss = 0.24697096\n",
      "Iteration 207, loss = 0.24577326\n",
      "Iteration 208, loss = 0.24457388\n",
      "Iteration 209, loss = 0.24340985\n",
      "Iteration 210, loss = 0.24227141\n",
      "Iteration 211, loss = 0.24107225\n",
      "Iteration 212, loss = 0.23993876\n",
      "Iteration 213, loss = 0.23877950\n",
      "Iteration 214, loss = 0.23773554\n",
      "Iteration 215, loss = 0.23659648\n",
      "Iteration 216, loss = 0.23551582\n",
      "Iteration 217, loss = 0.23448578\n",
      "Iteration 218, loss = 0.23338140\n",
      "Iteration 219, loss = 0.23226874\n",
      "Iteration 220, loss = 0.23115413\n",
      "Iteration 221, loss = 0.23020276\n",
      "Iteration 222, loss = 0.22915759\n",
      "Iteration 223, loss = 0.22816064\n",
      "Iteration 224, loss = 0.22720625\n",
      "Iteration 225, loss = 0.22608531\n",
      "Iteration 226, loss = 0.22506450\n",
      "Iteration 227, loss = 0.22409010\n",
      "Iteration 228, loss = 0.22311861\n",
      "Iteration 229, loss = 0.22212261\n",
      "Iteration 230, loss = 0.22118740\n",
      "Iteration 231, loss = 0.22031120\n",
      "Iteration 232, loss = 0.21934971\n",
      "Iteration 233, loss = 0.21845058\n",
      "Iteration 234, loss = 0.21748526\n",
      "Iteration 235, loss = 0.21653643\n",
      "Iteration 236, loss = 0.21562367\n",
      "Iteration 237, loss = 0.21472118\n",
      "Iteration 238, loss = 0.21390378\n",
      "Iteration 239, loss = 0.21293397\n",
      "Iteration 240, loss = 0.21211400\n",
      "Iteration 241, loss = 0.21125034\n",
      "Iteration 242, loss = 0.21043169\n",
      "Iteration 243, loss = 0.20956800\n",
      "Iteration 244, loss = 0.20873440\n",
      "Iteration 245, loss = 0.20789557\n",
      "Iteration 246, loss = 0.20713607\n",
      "Iteration 247, loss = 0.20627445\n",
      "Iteration 248, loss = 0.20550270\n",
      "Iteration 249, loss = 0.20467511\n",
      "Iteration 250, loss = 0.20389229\n",
      "Iteration 251, loss = 0.20308962\n",
      "Iteration 252, loss = 0.20232399\n",
      "Iteration 253, loss = 0.20151638\n",
      "Iteration 254, loss = 0.20072415\n",
      "Iteration 255, loss = 0.19995524\n",
      "Iteration 256, loss = 0.19915647\n",
      "Iteration 257, loss = 0.19848019\n",
      "Iteration 258, loss = 0.19770078\n",
      "Iteration 259, loss = 0.19694943\n",
      "Iteration 260, loss = 0.19622379\n",
      "Iteration 261, loss = 0.19547460\n",
      "Iteration 262, loss = 0.19477924\n",
      "Iteration 263, loss = 0.19405760\n",
      "Iteration 264, loss = 0.19338428\n",
      "Iteration 265, loss = 0.19273869\n",
      "Iteration 266, loss = 0.19201729\n",
      "Iteration 267, loss = 0.19129048\n",
      "Iteration 268, loss = 0.19061696\n",
      "Iteration 269, loss = 0.18986916\n",
      "Iteration 270, loss = 0.18919613\n",
      "Iteration 271, loss = 0.18852713\n",
      "Iteration 272, loss = 0.18792629\n",
      "Iteration 273, loss = 0.18727796\n",
      "Iteration 274, loss = 0.18662605\n",
      "Iteration 275, loss = 0.18597822\n",
      "Iteration 276, loss = 0.18537163\n",
      "Iteration 277, loss = 0.18465587\n",
      "Iteration 278, loss = 0.18403323\n",
      "Iteration 279, loss = 0.18342528\n",
      "Iteration 280, loss = 0.18279235\n",
      "Iteration 281, loss = 0.18214869\n",
      "Iteration 282, loss = 0.18155435\n",
      "Iteration 283, loss = 0.18093069\n",
      "Iteration 284, loss = 0.18036069\n",
      "Iteration 285, loss = 0.17974138\n",
      "Iteration 286, loss = 0.17910504\n",
      "Iteration 287, loss = 0.17849118\n",
      "Iteration 288, loss = 0.17793154\n",
      "Iteration 289, loss = 0.17733678\n",
      "Iteration 290, loss = 0.17677123\n",
      "Iteration 291, loss = 0.17617053\n",
      "Iteration 292, loss = 0.17562724\n",
      "Iteration 293, loss = 0.17505655\n",
      "Iteration 294, loss = 0.17450963\n",
      "Iteration 295, loss = 0.17391925\n",
      "Iteration 296, loss = 0.17338011\n",
      "Iteration 297, loss = 0.17285692\n",
      "Iteration 298, loss = 0.17229590\n",
      "Iteration 299, loss = 0.17174368\n",
      "Iteration 300, loss = 0.17122910\n",
      "Iteration 301, loss = 0.17068232\n",
      "Iteration 302, loss = 0.17012640\n",
      "Iteration 303, loss = 0.16963902\n",
      "Iteration 304, loss = 0.16910411\n",
      "Iteration 305, loss = 0.16859654\n",
      "Iteration 306, loss = 0.16807936\n",
      "Iteration 307, loss = 0.16757005\n",
      "Iteration 308, loss = 0.16708650\n",
      "Iteration 309, loss = 0.16660640\n",
      "Iteration 310, loss = 0.16610272\n",
      "Iteration 311, loss = 0.16563630\n",
      "Iteration 312, loss = 0.16516769\n",
      "Iteration 313, loss = 0.16463494\n",
      "Iteration 314, loss = 0.16419666\n",
      "Iteration 315, loss = 0.16372315\n",
      "Iteration 316, loss = 0.16324281\n",
      "Iteration 317, loss = 0.16274935\n",
      "Iteration 318, loss = 0.16228682\n",
      "Iteration 319, loss = 0.16184019\n",
      "Iteration 320, loss = 0.16133611\n",
      "Iteration 321, loss = 0.16089437\n",
      "Iteration 322, loss = 0.16041419\n",
      "Iteration 323, loss = 0.15994778\n",
      "Iteration 324, loss = 0.15951002\n",
      "Iteration 325, loss = 0.15906276\n",
      "Iteration 326, loss = 0.15864936\n",
      "Iteration 327, loss = 0.15818539\n",
      "Iteration 328, loss = 0.15777180\n",
      "Iteration 329, loss = 0.15734690\n",
      "Iteration 330, loss = 0.15686864\n",
      "Iteration 331, loss = 0.15647599\n",
      "Iteration 332, loss = 0.15605874\n",
      "Iteration 333, loss = 0.15562714\n",
      "Iteration 334, loss = 0.15523427\n",
      "Iteration 335, loss = 0.15480991\n",
      "Iteration 336, loss = 0.15437920\n",
      "Iteration 337, loss = 0.15395731\n",
      "Iteration 338, loss = 0.15354537\n",
      "Iteration 339, loss = 0.15317844\n",
      "Iteration 340, loss = 0.15277004\n",
      "Iteration 341, loss = 0.15239107\n",
      "Iteration 342, loss = 0.15199056\n",
      "Iteration 343, loss = 0.15159472\n",
      "Iteration 344, loss = 0.15119491\n",
      "Iteration 345, loss = 0.15080112\n",
      "Iteration 346, loss = 0.15040193\n",
      "Iteration 347, loss = 0.14997135\n",
      "Iteration 348, loss = 0.14958192\n",
      "Iteration 349, loss = 0.14918849\n",
      "Iteration 350, loss = 0.14881621\n",
      "Iteration 351, loss = 0.14843868\n",
      "Iteration 352, loss = 0.14805467\n",
      "Iteration 353, loss = 0.14767447\n",
      "Iteration 354, loss = 0.14730434\n",
      "Iteration 355, loss = 0.14696612\n",
      "Iteration 356, loss = 0.14656245\n",
      "Iteration 357, loss = 0.14621744\n",
      "Iteration 358, loss = 0.14585487\n",
      "Iteration 359, loss = 0.14548606\n",
      "Iteration 360, loss = 0.14510896\n",
      "Iteration 361, loss = 0.14475582\n",
      "Iteration 362, loss = 0.14438530\n",
      "Iteration 363, loss = 0.14404592\n",
      "Iteration 364, loss = 0.14370431\n",
      "Iteration 365, loss = 0.14334723\n",
      "Iteration 366, loss = 0.14301301\n",
      "Iteration 367, loss = 0.14266506\n",
      "Iteration 368, loss = 0.14234101\n",
      "Iteration 369, loss = 0.14197711\n",
      "Iteration 370, loss = 0.14167951\n",
      "Iteration 371, loss = 0.14134981\n",
      "Iteration 372, loss = 0.14102001\n",
      "Iteration 373, loss = 0.14066317\n",
      "Iteration 374, loss = 0.14034024\n",
      "Iteration 375, loss = 0.13999298\n",
      "Iteration 376, loss = 0.13966577\n",
      "Iteration 377, loss = 0.13935991\n",
      "Iteration 378, loss = 0.13906826\n",
      "Iteration 379, loss = 0.13873114\n",
      "Iteration 380, loss = 0.13840937\n",
      "Iteration 381, loss = 0.13809748\n",
      "Iteration 382, loss = 0.13778270\n",
      "Iteration 383, loss = 0.13745439\n",
      "Iteration 384, loss = 0.13714948\n",
      "Iteration 385, loss = 0.13684289\n",
      "Iteration 386, loss = 0.13654007\n",
      "Iteration 387, loss = 0.13623390\n",
      "Iteration 388, loss = 0.13595977\n",
      "Iteration 389, loss = 0.13561999\n",
      "Iteration 390, loss = 0.13533049\n",
      "Iteration 391, loss = 0.13504344\n",
      "Iteration 392, loss = 0.13475408\n",
      "Iteration 393, loss = 0.13446028\n",
      "Iteration 394, loss = 0.13418305\n",
      "Iteration 395, loss = 0.13389856\n",
      "Iteration 396, loss = 0.13361560\n",
      "Iteration 397, loss = 0.13331170\n",
      "Iteration 398, loss = 0.13304902\n",
      "Iteration 399, loss = 0.13274802\n",
      "Iteration 400, loss = 0.13245813\n",
      "Iteration 401, loss = 0.13218737\n",
      "Iteration 402, loss = 0.13188897\n",
      "Iteration 403, loss = 0.13162087\n",
      "Iteration 404, loss = 0.13137217\n",
      "Iteration 405, loss = 0.13109974\n",
      "Iteration 406, loss = 0.13083030\n",
      "Iteration 407, loss = 0.13055784\n",
      "Iteration 408, loss = 0.13029870\n",
      "Iteration 409, loss = 0.13004439\n",
      "Iteration 410, loss = 0.12976366\n",
      "Iteration 411, loss = 0.12949689\n",
      "Iteration 412, loss = 0.12924363\n",
      "Iteration 413, loss = 0.12896605\n",
      "Iteration 414, loss = 0.12872353\n",
      "Iteration 415, loss = 0.12843309\n",
      "Iteration 416, loss = 0.12818872\n",
      "Iteration 417, loss = 0.12793436\n",
      "Iteration 418, loss = 0.12767853\n",
      "Iteration 419, loss = 0.12743500\n",
      "Iteration 420, loss = 0.12717932\n",
      "Iteration 421, loss = 0.12693968\n",
      "Iteration 422, loss = 0.12668212\n",
      "Iteration 423, loss = 0.12643674\n",
      "Iteration 424, loss = 0.12617769\n",
      "Iteration 425, loss = 0.12593894\n",
      "Iteration 426, loss = 0.12570137\n",
      "Iteration 427, loss = 0.12545882\n",
      "Iteration 428, loss = 0.12521153\n",
      "Iteration 429, loss = 0.12496600\n",
      "Iteration 430, loss = 0.12473335\n",
      "Iteration 431, loss = 0.12448360\n",
      "Iteration 432, loss = 0.12427699\n",
      "Iteration 433, loss = 0.12403208\n",
      "Iteration 434, loss = 0.12383484\n",
      "Iteration 435, loss = 0.12356685\n",
      "Iteration 436, loss = 0.12330806\n",
      "Iteration 437, loss = 0.12307552\n",
      "Iteration 438, loss = 0.12286515\n",
      "Iteration 439, loss = 0.12265471\n",
      "Iteration 440, loss = 0.12244210\n",
      "Iteration 441, loss = 0.12218319\n",
      "Iteration 442, loss = 0.12197539\n",
      "Iteration 443, loss = 0.12175952\n",
      "Iteration 444, loss = 0.12153668\n",
      "Iteration 445, loss = 0.12134775\n",
      "Iteration 446, loss = 0.12114212\n",
      "Iteration 447, loss = 0.12091963\n",
      "Iteration 448, loss = 0.12069004\n",
      "Iteration 449, loss = 0.12045467\n",
      "Iteration 450, loss = 0.12023669\n",
      "Iteration 451, loss = 0.12002752\n",
      "Iteration 452, loss = 0.11981771\n",
      "Iteration 453, loss = 0.11959918\n",
      "Iteration 454, loss = 0.11939442\n",
      "Iteration 455, loss = 0.11919177\n",
      "Iteration 456, loss = 0.11899217\n",
      "Iteration 457, loss = 0.11878634\n",
      "Iteration 458, loss = 0.11859140\n",
      "Iteration 459, loss = 0.11836955\n",
      "Iteration 460, loss = 0.11817539\n",
      "Iteration 461, loss = 0.11797964\n",
      "Iteration 462, loss = 0.11778333\n",
      "Iteration 463, loss = 0.11757207\n",
      "Iteration 464, loss = 0.11735717\n",
      "Iteration 465, loss = 0.11718449\n",
      "Iteration 466, loss = 0.11699485\n",
      "Iteration 467, loss = 0.11679219\n",
      "Iteration 468, loss = 0.11660608\n",
      "Iteration 469, loss = 0.11642952\n",
      "Iteration 470, loss = 0.11622567\n",
      "Iteration 471, loss = 0.11602263\n",
      "Iteration 472, loss = 0.11584526\n",
      "Iteration 473, loss = 0.11565329\n",
      "Iteration 474, loss = 0.11548866\n",
      "Iteration 475, loss = 0.11528743\n",
      "Iteration 476, loss = 0.11510240\n",
      "Iteration 477, loss = 0.11491281\n",
      "Iteration 478, loss = 0.11473509\n",
      "Iteration 479, loss = 0.11455627\n",
      "Iteration 480, loss = 0.11437705\n",
      "Iteration 481, loss = 0.11418010\n",
      "Iteration 482, loss = 0.11401286\n",
      "Iteration 483, loss = 0.11382683\n",
      "Iteration 484, loss = 0.11365487\n",
      "Iteration 485, loss = 0.11348172\n",
      "Iteration 486, loss = 0.11330723\n",
      "Iteration 487, loss = 0.11314591\n",
      "Iteration 488, loss = 0.11296695\n",
      "Iteration 489, loss = 0.11280100\n",
      "Iteration 490, loss = 0.11260718\n",
      "Iteration 491, loss = 0.11243862\n",
      "Iteration 492, loss = 0.11228123\n",
      "Iteration 493, loss = 0.11211890\n",
      "Iteration 494, loss = 0.11196735\n",
      "Iteration 495, loss = 0.11178561\n",
      "Iteration 496, loss = 0.11162103\n",
      "Iteration 497, loss = 0.11142451\n",
      "Iteration 498, loss = 0.11124776\n",
      "Iteration 499, loss = 0.11108239\n",
      "Iteration 500, loss = 0.11092045\n",
      "Iteration 501, loss = 0.11075103\n",
      "Iteration 502, loss = 0.11059576\n",
      "Iteration 503, loss = 0.11045487\n",
      "Iteration 504, loss = 0.11028965\n",
      "Iteration 505, loss = 0.11012981\n",
      "Iteration 506, loss = 0.10996455\n",
      "Iteration 507, loss = 0.10980327\n",
      "Iteration 508, loss = 0.10964096\n",
      "Iteration 509, loss = 0.10947915\n",
      "Iteration 510, loss = 0.10934193\n",
      "Iteration 511, loss = 0.10917775\n",
      "Iteration 512, loss = 0.10902286\n",
      "Iteration 513, loss = 0.10885263\n",
      "Iteration 514, loss = 0.10870506\n",
      "Iteration 515, loss = 0.10852646\n",
      "Iteration 516, loss = 0.10838606\n",
      "Iteration 517, loss = 0.10824625\n",
      "Iteration 518, loss = 0.10807367\n",
      "Iteration 519, loss = 0.10792797\n",
      "Iteration 520, loss = 0.10777613\n",
      "Iteration 521, loss = 0.10763337\n",
      "Iteration 522, loss = 0.10747260\n",
      "Iteration 523, loss = 0.10732541\n",
      "Iteration 524, loss = 0.10716266\n",
      "Iteration 525, loss = 0.10702606\n",
      "Iteration 526, loss = 0.10686730\n",
      "Iteration 527, loss = 0.10672143\n",
      "Iteration 528, loss = 0.10659551\n",
      "Iteration 529, loss = 0.10644453\n",
      "Iteration 530, loss = 0.10629279\n",
      "Iteration 531, loss = 0.10614529\n",
      "Iteration 532, loss = 0.10601080\n",
      "Iteration 533, loss = 0.10585760\n",
      "Iteration 534, loss = 0.10571960\n",
      "Iteration 535, loss = 0.10560142\n",
      "Iteration 536, loss = 0.10544143\n",
      "Iteration 537, loss = 0.10530687\n",
      "Iteration 538, loss = 0.10517722\n",
      "Iteration 539, loss = 0.10503539\n",
      "Iteration 540, loss = 0.10489840\n",
      "Iteration 541, loss = 0.10475468\n",
      "Iteration 542, loss = 0.10461305\n",
      "Iteration 543, loss = 0.10448553\n",
      "Iteration 544, loss = 0.10435555\n",
      "Iteration 545, loss = 0.10421507\n",
      "Iteration 546, loss = 0.10409066\n",
      "Iteration 547, loss = 0.10395071\n",
      "Iteration 548, loss = 0.10382063\n",
      "Iteration 549, loss = 0.10370748\n",
      "Iteration 550, loss = 0.10356077\n",
      "Iteration 551, loss = 0.10343494\n",
      "Iteration 552, loss = 0.10331364\n",
      "Iteration 553, loss = 0.10318009\n",
      "Iteration 554, loss = 0.10305456\n",
      "Iteration 555, loss = 0.10291609\n",
      "Iteration 556, loss = 0.10278909\n",
      "Iteration 557, loss = 0.10266141\n",
      "Iteration 558, loss = 0.10253429\n",
      "Iteration 559, loss = 0.10241742\n",
      "Iteration 560, loss = 0.10230597\n",
      "Iteration 561, loss = 0.10218863\n",
      "Iteration 562, loss = 0.10204731\n",
      "Iteration 563, loss = 0.10192979\n",
      "Iteration 564, loss = 0.10181307\n",
      "Iteration 565, loss = 0.10171403\n",
      "Iteration 566, loss = 0.10156384\n",
      "Iteration 567, loss = 0.10143746\n",
      "Iteration 568, loss = 0.10131842\n",
      "Iteration 569, loss = 0.10118942\n",
      "Iteration 570, loss = 0.10108627\n",
      "Iteration 571, loss = 0.10096708\n",
      "Iteration 572, loss = 0.10085119\n",
      "Iteration 573, loss = 0.10073319\n",
      "Iteration 574, loss = 0.10059822\n",
      "Iteration 575, loss = 0.10048764\n",
      "Iteration 576, loss = 0.10037753\n",
      "Iteration 577, loss = 0.10025684\n",
      "Iteration 578, loss = 0.10014683\n",
      "Iteration 579, loss = 0.10002380\n",
      "Iteration 580, loss = 0.09990798\n",
      "Iteration 581, loss = 0.09980395\n",
      "Iteration 582, loss = 0.09968983\n",
      "Iteration 583, loss = 0.09958710\n",
      "Iteration 584, loss = 0.09946882\n",
      "Iteration 585, loss = 0.09934778\n",
      "Iteration 586, loss = 0.09923949\n",
      "Iteration 587, loss = 0.09912330\n",
      "Iteration 588, loss = 0.09901607\n",
      "Iteration 589, loss = 0.09890762\n",
      "Iteration 590, loss = 0.09881845\n",
      "Iteration 591, loss = 0.09869300\n",
      "Iteration 592, loss = 0.09857169\n",
      "Iteration 593, loss = 0.09846085\n",
      "Iteration 594, loss = 0.09835206\n",
      "Iteration 595, loss = 0.09824124\n",
      "Iteration 596, loss = 0.09814637\n",
      "Iteration 597, loss = 0.09803773\n",
      "Iteration 598, loss = 0.09792918\n",
      "Iteration 599, loss = 0.09782292\n",
      "Iteration 600, loss = 0.09772843\n",
      "Iteration 601, loss = 0.09761685\n",
      "Iteration 602, loss = 0.09751853\n",
      "Iteration 603, loss = 0.09740744\n",
      "Iteration 604, loss = 0.09731078\n",
      "Iteration 605, loss = 0.09719869\n",
      "Iteration 606, loss = 0.09710995\n",
      "Iteration 607, loss = 0.09697964\n",
      "Iteration 608, loss = 0.09688071\n",
      "Iteration 609, loss = 0.09678748\n",
      "Iteration 610, loss = 0.09668656\n",
      "Iteration 611, loss = 0.09657559\n",
      "Iteration 612, loss = 0.09647756\n",
      "Iteration 613, loss = 0.09638633\n",
      "Iteration 614, loss = 0.09627188\n",
      "Iteration 615, loss = 0.09616902\n",
      "Iteration 616, loss = 0.09608176\n",
      "Iteration 617, loss = 0.09597424\n",
      "Iteration 618, loss = 0.09587261\n",
      "Iteration 619, loss = 0.09577334\n",
      "Iteration 620, loss = 0.09567393\n",
      "Iteration 621, loss = 0.09558325\n",
      "Iteration 622, loss = 0.09548657\n",
      "Iteration 623, loss = 0.09539301\n",
      "Iteration 624, loss = 0.09528905\n",
      "Iteration 625, loss = 0.09519533\n",
      "Iteration 626, loss = 0.09510477\n",
      "Iteration 627, loss = 0.09500791\n",
      "Iteration 628, loss = 0.09490827\n",
      "Iteration 629, loss = 0.09481478\n",
      "Iteration 630, loss = 0.09472780\n",
      "Iteration 631, loss = 0.09463513\n",
      "Iteration 632, loss = 0.09454158\n",
      "Iteration 633, loss = 0.09445329\n",
      "Iteration 634, loss = 0.09436134\n",
      "Iteration 635, loss = 0.09427447\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.11720531\n",
      "Iteration 2, loss = 2.46984067\n",
      "Iteration 3, loss = 1.74873962\n",
      "Iteration 4, loss = 1.25909337\n",
      "Iteration 5, loss = 0.91623707\n",
      "Iteration 6, loss = 0.67325378\n",
      "Iteration 7, loss = 0.52951735\n",
      "Iteration 8, loss = 0.42296543\n",
      "Iteration 9, loss = 0.38276293\n",
      "Iteration 10, loss = 0.35373673\n",
      "Iteration 11, loss = 0.32020086\n",
      "Iteration 12, loss = 0.29267756\n",
      "Iteration 13, loss = 0.26708786\n",
      "Iteration 14, loss = 0.24542298\n",
      "Iteration 15, loss = 0.25336501\n",
      "Iteration 16, loss = 0.26451252\n",
      "Iteration 17, loss = 0.25607918\n",
      "Iteration 18, loss = 0.26141946\n",
      "Iteration 19, loss = 0.35763629\n",
      "Iteration 20, loss = 0.48019643\n",
      "Iteration 21, loss = 0.54434358\n",
      "Iteration 22, loss = 0.55476659\n",
      "Iteration 23, loss = 0.47315651\n",
      "Iteration 24, loss = 0.38753117\n",
      "Iteration 25, loss = 0.38111814\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.53260031\n",
      "Iteration 2, loss = 2.41185659\n",
      "Iteration 3, loss = 1.68430165\n",
      "Iteration 4, loss = 1.24109473\n",
      "Iteration 5, loss = 0.88675341\n",
      "Iteration 6, loss = 0.66442294\n",
      "Iteration 7, loss = 0.52011334\n",
      "Iteration 8, loss = 0.41515480\n",
      "Iteration 9, loss = 0.35691213\n",
      "Iteration 10, loss = 0.32276665\n",
      "Iteration 11, loss = 0.30787667\n",
      "Iteration 12, loss = 0.33256337\n",
      "Iteration 13, loss = 0.35789748\n",
      "Iteration 14, loss = 0.34417726\n",
      "Iteration 15, loss = 0.36483856\n",
      "Iteration 16, loss = 0.44457671\n",
      "Iteration 17, loss = 0.47480729\n",
      "Iteration 18, loss = 0.51384176\n",
      "Iteration 19, loss = 0.47490057\n",
      "Iteration 20, loss = 0.46768921\n",
      "Iteration 21, loss = 0.43722976\n",
      "Iteration 22, loss = 0.39688610\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.14090838\n",
      "Iteration 2, loss = 2.04829107\n",
      "Iteration 3, loss = 1.53930430\n",
      "Iteration 4, loss = 1.12515302\n",
      "Iteration 5, loss = 0.80633757\n",
      "Iteration 6, loss = 0.57358489\n",
      "Iteration 7, loss = 0.40684713\n",
      "Iteration 8, loss = 0.29996990\n",
      "Iteration 9, loss = 0.23658989\n",
      "Iteration 10, loss = 0.20095110\n",
      "Iteration 11, loss = 0.17355334\n",
      "Iteration 12, loss = 0.15240660\n",
      "Iteration 13, loss = 0.15463504\n",
      "Iteration 14, loss = 0.38736665\n",
      "Iteration 15, loss = 0.55675927\n",
      "Iteration 16, loss = 0.68423540\n",
      "Iteration 17, loss = 0.67839485\n",
      "Iteration 18, loss = 0.60539297\n",
      "Iteration 19, loss = 0.50743880\n",
      "Iteration 20, loss = 0.39632834\n",
      "Iteration 21, loss = 0.30888260\n",
      "Iteration 22, loss = 0.24788353\n",
      "Iteration 23, loss = 0.21618114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.61424037\n",
      "Iteration 2, loss = 2.39504082\n",
      "Iteration 3, loss = 2.09569293\n",
      "Iteration 4, loss = 1.61757474\n",
      "Iteration 5, loss = 1.15879827\n",
      "Iteration 6, loss = 0.86153190\n",
      "Iteration 7, loss = 0.65054420\n",
      "Iteration 8, loss = 0.49941414\n",
      "Iteration 9, loss = 0.40193605\n",
      "Iteration 10, loss = 0.33823806\n",
      "Iteration 11, loss = 0.30107715\n",
      "Iteration 12, loss = 0.27420525\n",
      "Iteration 13, loss = 0.23014295\n",
      "Iteration 14, loss = 0.20459527\n",
      "Iteration 15, loss = 0.18219436\n",
      "Iteration 16, loss = 0.16180325\n",
      "Iteration 17, loss = 0.15361660\n",
      "Iteration 18, loss = 0.15201395\n",
      "Iteration 19, loss = 0.23021870\n",
      "Iteration 20, loss = 0.28500618\n",
      "Iteration 21, loss = 0.37708882\n",
      "Iteration 22, loss = 0.41214379\n",
      "Iteration 23, loss = 0.40818321\n",
      "Iteration 24, loss = 0.36494014\n",
      "Iteration 25, loss = 0.32721423\n",
      "Iteration 26, loss = 0.28971982\n",
      "Iteration 27, loss = 0.27853614\n",
      "Iteration 28, loss = 0.27833398\n",
      "Iteration 29, loss = 0.26680738\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.26412868\n",
      "Iteration 2, loss = 2.56262670\n",
      "Iteration 3, loss = 2.03641824\n",
      "Iteration 4, loss = 1.49791403\n",
      "Iteration 5, loss = 1.06084173\n",
      "Iteration 6, loss = 0.76266192\n",
      "Iteration 7, loss = 0.57183997\n",
      "Iteration 8, loss = 0.47080835\n",
      "Iteration 9, loss = 0.41150933\n",
      "Iteration 10, loss = 0.37630977\n",
      "Iteration 11, loss = 0.33094653\n",
      "Iteration 12, loss = 0.33580930\n",
      "Iteration 13, loss = 0.30111201\n",
      "Iteration 14, loss = 0.29582548\n",
      "Iteration 15, loss = 0.27180134\n",
      "Iteration 16, loss = 0.30228465\n",
      "Iteration 17, loss = 0.29718948\n",
      "Iteration 18, loss = 0.31915374\n",
      "Iteration 19, loss = 0.37106812\n",
      "Iteration 20, loss = 0.37407441\n",
      "Iteration 21, loss = 0.38178295\n",
      "Iteration 22, loss = 0.36877028\n",
      "Iteration 23, loss = 0.40850512\n",
      "Iteration 24, loss = 0.41635005\n",
      "Iteration 25, loss = 0.42531264\n",
      "Iteration 26, loss = 0.44052783\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.16144351\n",
      "Iteration 2, loss = 1.03368721\n",
      "Iteration 3, loss = 0.39893286\n",
      "Iteration 4, loss = 0.23948850\n",
      "Iteration 5, loss = 0.17126602\n",
      "Iteration 6, loss = 0.13250856\n",
      "Iteration 7, loss = 0.11621331\n",
      "Iteration 8, loss = 0.10174427\n",
      "Iteration 9, loss = 0.09413892\n",
      "Iteration 10, loss = 0.08829712\n",
      "Iteration 11, loss = 0.08511366\n",
      "Iteration 12, loss = 0.08279555\n",
      "Iteration 13, loss = 0.08072969\n",
      "Iteration 14, loss = 0.07925327\n",
      "Iteration 15, loss = 0.07814143\n",
      "Iteration 16, loss = 0.07707799\n",
      "Iteration 17, loss = 0.07624071\n",
      "Iteration 18, loss = 0.07546551\n",
      "Iteration 19, loss = 0.07473449\n",
      "Iteration 20, loss = 0.07397614\n",
      "Iteration 21, loss = 0.07339703\n",
      "Iteration 22, loss = 0.07285050\n",
      "Iteration 23, loss = 0.07218287\n",
      "Iteration 24, loss = 0.07152333\n",
      "Iteration 25, loss = 0.07104950\n",
      "Iteration 26, loss = 0.07040678\n",
      "Iteration 27, loss = 0.07001314\n",
      "Iteration 28, loss = 0.06939187\n",
      "Iteration 29, loss = 0.06895796\n",
      "Iteration 30, loss = 0.06844683\n",
      "Iteration 31, loss = 0.06822539\n",
      "Iteration 32, loss = 0.06765908\n",
      "Iteration 33, loss = 0.06717125\n",
      "Iteration 34, loss = 0.06670695\n",
      "Iteration 35, loss = 0.06628915\n",
      "Iteration 36, loss = 0.06583525\n",
      "Iteration 37, loss = 0.06544517\n",
      "Iteration 38, loss = 0.06502136\n",
      "Iteration 39, loss = 0.06463958\n",
      "Iteration 40, loss = 0.06425162\n",
      "Iteration 41, loss = 0.06391365\n",
      "Iteration 42, loss = 0.06352319\n",
      "Iteration 43, loss = 0.06322222\n",
      "Iteration 44, loss = 0.06280755\n",
      "Iteration 45, loss = 0.06248340\n",
      "Iteration 46, loss = 0.06211747\n",
      "Iteration 47, loss = 0.06175306\n",
      "Iteration 48, loss = 0.06143841\n",
      "Iteration 49, loss = 0.06110322\n",
      "Iteration 50, loss = 0.06078282\n",
      "Iteration 51, loss = 0.06049307\n",
      "Iteration 52, loss = 0.06016379\n",
      "Iteration 53, loss = 0.05983245\n",
      "Iteration 54, loss = 0.05953064\n",
      "Iteration 55, loss = 0.05924099\n",
      "Iteration 56, loss = 0.05891825\n",
      "Iteration 57, loss = 0.05863523\n",
      "Iteration 58, loss = 0.05834256\n",
      "Iteration 59, loss = 0.05807985\n",
      "Iteration 60, loss = 0.05780488\n",
      "Iteration 61, loss = 0.05748027\n",
      "Iteration 62, loss = 0.05722715\n",
      "Iteration 63, loss = 0.05698069\n",
      "Iteration 64, loss = 0.05666753\n",
      "Iteration 65, loss = 0.05643724\n",
      "Iteration 66, loss = 0.05616341\n",
      "Iteration 67, loss = 0.05592816\n",
      "Iteration 68, loss = 0.05568957\n",
      "Iteration 69, loss = 0.05541818\n",
      "Iteration 70, loss = 0.05523675\n",
      "Iteration 71, loss = 0.05497764\n",
      "Iteration 72, loss = 0.05467494\n",
      "Iteration 73, loss = 0.05444960\n",
      "Iteration 74, loss = 0.05424301\n",
      "Iteration 75, loss = 0.05399548\n",
      "Iteration 76, loss = 0.05383548\n",
      "Iteration 77, loss = 0.05361064\n",
      "Iteration 78, loss = 0.05340864\n",
      "Iteration 79, loss = 0.05313328\n",
      "Iteration 80, loss = 0.05295950\n",
      "Iteration 81, loss = 0.05269486\n",
      "Iteration 82, loss = 0.05248511\n",
      "Iteration 83, loss = 0.05230121\n",
      "Iteration 84, loss = 0.05210021\n",
      "Iteration 85, loss = 0.05190068\n",
      "Iteration 86, loss = 0.05173411\n",
      "Iteration 87, loss = 0.05153078\n",
      "Iteration 88, loss = 0.05128292\n",
      "Iteration 89, loss = 0.05112710\n",
      "Iteration 90, loss = 0.05094409\n",
      "Iteration 91, loss = 0.05079263\n",
      "Iteration 92, loss = 0.05056423\n",
      "Iteration 93, loss = 0.05041585\n",
      "Iteration 94, loss = 0.05020582\n",
      "Iteration 95, loss = 0.05004386\n",
      "Iteration 96, loss = 0.04988795\n",
      "Iteration 97, loss = 0.04972328\n",
      "Iteration 98, loss = 0.04951472\n",
      "Iteration 99, loss = 0.04938909\n",
      "Iteration 100, loss = 0.04920346\n",
      "Iteration 101, loss = 0.04906237\n",
      "Iteration 102, loss = 0.04889696\n",
      "Iteration 103, loss = 0.04874908\n",
      "Iteration 104, loss = 0.04860013\n",
      "Iteration 105, loss = 0.04841147\n",
      "Iteration 106, loss = 0.04828606\n",
      "Iteration 107, loss = 0.04812495\n",
      "Iteration 108, loss = 0.04797276\n",
      "Iteration 109, loss = 0.04787666\n",
      "Iteration 110, loss = 0.04776742\n",
      "Iteration 111, loss = 0.04756495\n",
      "Iteration 112, loss = 0.04740335\n",
      "Iteration 113, loss = 0.04729986\n",
      "Iteration 114, loss = 0.04720627\n",
      "Iteration 115, loss = 0.04705106\n",
      "Iteration 116, loss = 0.04690175\n",
      "Iteration 117, loss = 0.04675371\n",
      "Iteration 118, loss = 0.04661443\n",
      "Iteration 119, loss = 0.04650680\n",
      "Iteration 120, loss = 0.04642391\n",
      "Iteration 121, loss = 0.04625871\n",
      "Iteration 122, loss = 0.04612847\n",
      "Iteration 123, loss = 0.04601939\n",
      "Iteration 124, loss = 0.04590352\n",
      "Iteration 125, loss = 0.04577628\n",
      "Iteration 126, loss = 0.04564874\n",
      "Iteration 127, loss = 0.04552858\n",
      "Iteration 128, loss = 0.04540094\n",
      "Iteration 129, loss = 0.04529907\n",
      "Iteration 130, loss = 0.04518155\n",
      "Iteration 131, loss = 0.04507350\n",
      "Iteration 132, loss = 0.04498338\n",
      "Iteration 133, loss = 0.04484329\n",
      "Iteration 134, loss = 0.04475897\n",
      "Iteration 135, loss = 0.04475552\n",
      "Iteration 136, loss = 0.04454310\n",
      "Iteration 137, loss = 0.04446737\n",
      "Iteration 138, loss = 0.04439126\n",
      "Iteration 139, loss = 0.04425659\n",
      "Iteration 140, loss = 0.04418328\n",
      "Iteration 141, loss = 0.04405465\n",
      "Iteration 142, loss = 0.04400212\n",
      "Iteration 143, loss = 0.04386314\n",
      "Iteration 144, loss = 0.04374765\n",
      "Iteration 145, loss = 0.04369865\n",
      "Iteration 146, loss = 0.04360164\n",
      "Iteration 147, loss = 0.04350628\n",
      "Iteration 148, loss = 0.04341371\n",
      "Iteration 149, loss = 0.04335061\n",
      "Iteration 150, loss = 0.04326771\n",
      "Iteration 151, loss = 0.04316860\n",
      "Iteration 152, loss = 0.04307674\n",
      "Iteration 153, loss = 0.04300002\n",
      "Iteration 154, loss = 0.04291004\n",
      "Iteration 155, loss = 0.04281577\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15112662\n",
      "Iteration 2, loss = 0.98959005\n",
      "Iteration 3, loss = 0.40809818\n",
      "Iteration 4, loss = 0.23627087\n",
      "Iteration 5, loss = 0.17189622\n",
      "Iteration 6, loss = 0.13853074\n",
      "Iteration 7, loss = 0.11987012\n",
      "Iteration 8, loss = 0.10401033\n",
      "Iteration 9, loss = 0.09633517\n",
      "Iteration 10, loss = 0.08924898\n",
      "Iteration 11, loss = 0.08540992\n",
      "Iteration 12, loss = 0.08236605\n",
      "Iteration 13, loss = 0.08038413\n",
      "Iteration 14, loss = 0.07876474\n",
      "Iteration 15, loss = 0.07758585\n",
      "Iteration 16, loss = 0.07666952\n",
      "Iteration 17, loss = 0.07564585\n",
      "Iteration 18, loss = 0.07497564\n",
      "Iteration 19, loss = 0.07405290\n",
      "Iteration 20, loss = 0.07335732\n",
      "Iteration 21, loss = 0.07278207\n",
      "Iteration 22, loss = 0.07214155\n",
      "Iteration 23, loss = 0.07154700\n",
      "Iteration 24, loss = 0.07092884\n",
      "Iteration 25, loss = 0.07050884\n",
      "Iteration 26, loss = 0.06987697\n",
      "Iteration 27, loss = 0.06934906\n",
      "Iteration 28, loss = 0.06881404\n",
      "Iteration 29, loss = 0.06835325\n",
      "Iteration 30, loss = 0.06783759\n",
      "Iteration 31, loss = 0.06758486\n",
      "Iteration 32, loss = 0.06698177\n",
      "Iteration 33, loss = 0.06649268\n",
      "Iteration 34, loss = 0.06603970\n",
      "Iteration 35, loss = 0.06565137\n",
      "Iteration 36, loss = 0.06525509\n",
      "Iteration 37, loss = 0.06483147\n",
      "Iteration 38, loss = 0.06440495\n",
      "Iteration 39, loss = 0.06402001\n",
      "Iteration 40, loss = 0.06368319\n",
      "Iteration 41, loss = 0.06331608\n",
      "Iteration 42, loss = 0.06292055\n",
      "Iteration 43, loss = 0.06261238\n",
      "Iteration 44, loss = 0.06216642\n",
      "Iteration 45, loss = 0.06181403\n",
      "Iteration 46, loss = 0.06147400\n",
      "Iteration 47, loss = 0.06114706\n",
      "Iteration 48, loss = 0.06082060\n",
      "Iteration 49, loss = 0.06046048\n",
      "Iteration 50, loss = 0.06017600\n",
      "Iteration 51, loss = 0.05985509\n",
      "Iteration 52, loss = 0.05953349\n",
      "Iteration 53, loss = 0.05921279\n",
      "Iteration 54, loss = 0.05889638\n",
      "Iteration 55, loss = 0.05862441\n",
      "Iteration 56, loss = 0.05830378\n",
      "Iteration 57, loss = 0.05801243\n",
      "Iteration 58, loss = 0.05774711\n",
      "Iteration 59, loss = 0.05747875\n",
      "Iteration 60, loss = 0.05718221\n",
      "Iteration 61, loss = 0.05690972\n",
      "Iteration 62, loss = 0.05661848\n",
      "Iteration 63, loss = 0.05640999\n",
      "Iteration 64, loss = 0.05611217\n",
      "Iteration 65, loss = 0.05584990\n",
      "Iteration 66, loss = 0.05556595\n",
      "Iteration 67, loss = 0.05534301\n",
      "Iteration 68, loss = 0.05506067\n",
      "Iteration 69, loss = 0.05485166\n",
      "Iteration 70, loss = 0.05460025\n",
      "Iteration 71, loss = 0.05435647\n",
      "Iteration 72, loss = 0.05411631\n",
      "Iteration 73, loss = 0.05389790\n",
      "Iteration 74, loss = 0.05369357\n",
      "Iteration 75, loss = 0.05341828\n",
      "Iteration 76, loss = 0.05330221\n",
      "Iteration 77, loss = 0.05297596\n",
      "Iteration 78, loss = 0.05280530\n",
      "Iteration 79, loss = 0.05253614\n",
      "Iteration 80, loss = 0.05234358\n",
      "Iteration 81, loss = 0.05210685\n",
      "Iteration 82, loss = 0.05189345\n",
      "Iteration 83, loss = 0.05173612\n",
      "Iteration 84, loss = 0.05154859\n",
      "Iteration 85, loss = 0.05133040\n",
      "Iteration 86, loss = 0.05110531\n",
      "Iteration 87, loss = 0.05092994\n",
      "Iteration 88, loss = 0.05069448\n",
      "Iteration 89, loss = 0.05055374\n",
      "Iteration 90, loss = 0.05033425\n",
      "Iteration 91, loss = 0.05021631\n",
      "Iteration 92, loss = 0.04999607\n",
      "Iteration 93, loss = 0.04982403\n",
      "Iteration 94, loss = 0.04964556\n",
      "Iteration 95, loss = 0.04946816\n",
      "Iteration 96, loss = 0.04927821\n",
      "Iteration 97, loss = 0.04913130\n",
      "Iteration 98, loss = 0.04893907\n",
      "Iteration 99, loss = 0.04879494\n",
      "Iteration 100, loss = 0.04860312\n",
      "Iteration 101, loss = 0.04846454\n",
      "Iteration 102, loss = 0.04831193\n",
      "Iteration 103, loss = 0.04814105\n",
      "Iteration 104, loss = 0.04799732\n",
      "Iteration 105, loss = 0.04785747\n",
      "Iteration 106, loss = 0.04769814\n",
      "Iteration 107, loss = 0.04754623\n",
      "Iteration 108, loss = 0.04738490\n",
      "Iteration 109, loss = 0.04726523\n",
      "Iteration 110, loss = 0.04713883\n",
      "Iteration 111, loss = 0.04699174\n",
      "Iteration 112, loss = 0.04683964\n",
      "Iteration 113, loss = 0.04673379\n",
      "Iteration 114, loss = 0.04663812\n",
      "Iteration 115, loss = 0.04648181\n",
      "Iteration 116, loss = 0.04630516\n",
      "Iteration 117, loss = 0.04618526\n",
      "Iteration 118, loss = 0.04605681\n",
      "Iteration 119, loss = 0.04592076\n",
      "Iteration 120, loss = 0.04582208\n",
      "Iteration 121, loss = 0.04566956\n",
      "Iteration 122, loss = 0.04553484\n",
      "Iteration 123, loss = 0.04541701\n",
      "Iteration 124, loss = 0.04530051\n",
      "Iteration 125, loss = 0.04518917\n",
      "Iteration 126, loss = 0.04507467\n",
      "Iteration 127, loss = 0.04493655\n",
      "Iteration 128, loss = 0.04484726\n",
      "Iteration 129, loss = 0.04472023\n",
      "Iteration 130, loss = 0.04461398\n",
      "Iteration 131, loss = 0.04450759\n",
      "Iteration 132, loss = 0.04444703\n",
      "Iteration 133, loss = 0.04430496\n",
      "Iteration 134, loss = 0.04421124\n",
      "Iteration 135, loss = 0.04415756\n",
      "Iteration 136, loss = 0.04399062\n",
      "Iteration 137, loss = 0.04388613\n",
      "Iteration 138, loss = 0.04379750\n",
      "Iteration 139, loss = 0.04368710\n",
      "Iteration 140, loss = 0.04360174\n",
      "Iteration 141, loss = 0.04348244\n",
      "Iteration 142, loss = 0.04342155\n",
      "Iteration 143, loss = 0.04332444\n",
      "Iteration 144, loss = 0.04318905\n",
      "Iteration 145, loss = 0.04313600\n",
      "Iteration 146, loss = 0.04303774\n",
      "Iteration 147, loss = 0.04296135\n",
      "Iteration 148, loss = 0.04286097\n",
      "Iteration 149, loss = 0.04277165\n",
      "Iteration 150, loss = 0.04270670\n",
      "Iteration 151, loss = 0.04260458\n",
      "Iteration 152, loss = 0.04251305\n",
      "Iteration 153, loss = 0.04241591\n",
      "Iteration 154, loss = 0.04235480\n",
      "Iteration 155, loss = 0.04226608\n",
      "Iteration 156, loss = 0.04216981\n",
      "Iteration 157, loss = 0.04213425\n",
      "Iteration 158, loss = 0.04205804\n",
      "Iteration 159, loss = 0.04200420\n",
      "Iteration 160, loss = 0.04188214\n",
      "Iteration 161, loss = 0.04190039\n",
      "Iteration 162, loss = 0.04173744\n",
      "Iteration 163, loss = 0.04166406\n",
      "Iteration 164, loss = 0.04158691\n",
      "Iteration 165, loss = 0.04152622\n",
      "Iteration 166, loss = 0.04145558\n",
      "Iteration 167, loss = 0.04141291\n",
      "Iteration 168, loss = 0.04131453\n",
      "Iteration 169, loss = 0.04126386\n",
      "Iteration 170, loss = 0.04119294\n",
      "Iteration 171, loss = 0.04111193\n",
      "Iteration 172, loss = 0.04106691\n",
      "Iteration 173, loss = 0.04098244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.16740491\n",
      "Iteration 2, loss = 1.04725472\n",
      "Iteration 3, loss = 0.40718387\n",
      "Iteration 4, loss = 0.25348963\n",
      "Iteration 5, loss = 0.17856677\n",
      "Iteration 6, loss = 0.14911643\n",
      "Iteration 7, loss = 0.12042160\n",
      "Iteration 8, loss = 0.10526667\n",
      "Iteration 9, loss = 0.09718878\n",
      "Iteration 10, loss = 0.09126562\n",
      "Iteration 11, loss = 0.08747494\n",
      "Iteration 12, loss = 0.08356320\n",
      "Iteration 13, loss = 0.08214803\n",
      "Iteration 14, loss = 0.08020799\n",
      "Iteration 15, loss = 0.07931254\n",
      "Iteration 16, loss = 0.07761710\n",
      "Iteration 17, loss = 0.07685994\n",
      "Iteration 18, loss = 0.07589002\n",
      "Iteration 19, loss = 0.07504726\n",
      "Iteration 20, loss = 0.07421021\n",
      "Iteration 21, loss = 0.07357863\n",
      "Iteration 22, loss = 0.07293181\n",
      "Iteration 23, loss = 0.07230019\n",
      "Iteration 24, loss = 0.07172464\n",
      "Iteration 25, loss = 0.07114158\n",
      "Iteration 26, loss = 0.07074935\n",
      "Iteration 27, loss = 0.07013289\n",
      "Iteration 28, loss = 0.06966578\n",
      "Iteration 29, loss = 0.06916489\n",
      "Iteration 30, loss = 0.06872197\n",
      "Iteration 31, loss = 0.06829407\n",
      "Iteration 32, loss = 0.06782520\n",
      "Iteration 33, loss = 0.06731299\n",
      "Iteration 34, loss = 0.06683715\n",
      "Iteration 35, loss = 0.06642300\n",
      "Iteration 36, loss = 0.06603851\n",
      "Iteration 37, loss = 0.06564707\n",
      "Iteration 38, loss = 0.06521571\n",
      "Iteration 39, loss = 0.06481447\n",
      "Iteration 40, loss = 0.06451298\n",
      "Iteration 41, loss = 0.06410251\n",
      "Iteration 42, loss = 0.06370405\n",
      "Iteration 43, loss = 0.06334212\n",
      "Iteration 44, loss = 0.06296665\n",
      "Iteration 45, loss = 0.06265560\n",
      "Iteration 46, loss = 0.06235473\n",
      "Iteration 47, loss = 0.06191755\n",
      "Iteration 48, loss = 0.06161578\n",
      "Iteration 49, loss = 0.06129946\n",
      "Iteration 50, loss = 0.06094832\n",
      "Iteration 51, loss = 0.06068777\n",
      "Iteration 52, loss = 0.06033147\n",
      "Iteration 53, loss = 0.06002666\n",
      "Iteration 54, loss = 0.05969742\n",
      "Iteration 55, loss = 0.05941260\n",
      "Iteration 56, loss = 0.05911927\n",
      "Iteration 57, loss = 0.05885520\n",
      "Iteration 58, loss = 0.05857350\n",
      "Iteration 59, loss = 0.05826588\n",
      "Iteration 60, loss = 0.05798235\n",
      "Iteration 61, loss = 0.05770884\n",
      "Iteration 62, loss = 0.05745171\n",
      "Iteration 63, loss = 0.05720792\n",
      "Iteration 64, loss = 0.05690656\n",
      "Iteration 65, loss = 0.05663661\n",
      "Iteration 66, loss = 0.05638674\n",
      "Iteration 67, loss = 0.05614628\n",
      "Iteration 68, loss = 0.05587912\n",
      "Iteration 69, loss = 0.05565646\n",
      "Iteration 70, loss = 0.05539593\n",
      "Iteration 71, loss = 0.05516856\n",
      "Iteration 72, loss = 0.05494732\n",
      "Iteration 73, loss = 0.05469757\n",
      "Iteration 74, loss = 0.05446023\n",
      "Iteration 75, loss = 0.05426665\n",
      "Iteration 76, loss = 0.05402616\n",
      "Iteration 77, loss = 0.05384944\n",
      "Iteration 78, loss = 0.05357340\n",
      "Iteration 79, loss = 0.05337054\n",
      "Iteration 80, loss = 0.05316980\n",
      "Iteration 81, loss = 0.05293339\n",
      "Iteration 82, loss = 0.05274292\n",
      "Iteration 83, loss = 0.05249949\n",
      "Iteration 84, loss = 0.05231571\n",
      "Iteration 85, loss = 0.05212017\n",
      "Iteration 86, loss = 0.05192478\n",
      "Iteration 87, loss = 0.05172448\n",
      "Iteration 88, loss = 0.05152793\n",
      "Iteration 89, loss = 0.05134662\n",
      "Iteration 90, loss = 0.05123090\n",
      "Iteration 91, loss = 0.05099658\n",
      "Iteration 92, loss = 0.05080674\n",
      "Iteration 93, loss = 0.05066568\n",
      "Iteration 94, loss = 0.05046327\n",
      "Iteration 95, loss = 0.05028765\n",
      "Iteration 96, loss = 0.05012468\n",
      "Iteration 97, loss = 0.04995875\n",
      "Iteration 98, loss = 0.04977122\n",
      "Iteration 99, loss = 0.04960140\n",
      "Iteration 100, loss = 0.04947854\n",
      "Iteration 101, loss = 0.04928319\n",
      "Iteration 102, loss = 0.04912739\n",
      "Iteration 103, loss = 0.04897339\n",
      "Iteration 104, loss = 0.04881082\n",
      "Iteration 105, loss = 0.04868419\n",
      "Iteration 106, loss = 0.04852325\n",
      "Iteration 107, loss = 0.04838977\n",
      "Iteration 108, loss = 0.04823768\n",
      "Iteration 109, loss = 0.04806451\n",
      "Iteration 110, loss = 0.04792855\n",
      "Iteration 111, loss = 0.04777572\n",
      "Iteration 112, loss = 0.04766208\n",
      "Iteration 113, loss = 0.04753374\n",
      "Iteration 114, loss = 0.04737807\n",
      "Iteration 115, loss = 0.04726618\n",
      "Iteration 116, loss = 0.04711498\n",
      "Iteration 117, loss = 0.04698983\n",
      "Iteration 118, loss = 0.04685960\n",
      "Iteration 119, loss = 0.04673531\n",
      "Iteration 120, loss = 0.04666651\n",
      "Iteration 121, loss = 0.04647523\n",
      "Iteration 122, loss = 0.04636511\n",
      "Iteration 123, loss = 0.04624806\n",
      "Iteration 124, loss = 0.04613368\n",
      "Iteration 125, loss = 0.04599021\n",
      "Iteration 126, loss = 0.04587723\n",
      "Iteration 127, loss = 0.04574641\n",
      "Iteration 128, loss = 0.04565810\n",
      "Iteration 129, loss = 0.04553492\n",
      "Iteration 130, loss = 0.04542120\n",
      "Iteration 131, loss = 0.04530599\n",
      "Iteration 132, loss = 0.04521139\n",
      "Iteration 133, loss = 0.04508955\n",
      "Iteration 134, loss = 0.04500847\n",
      "Iteration 135, loss = 0.04493047\n",
      "Iteration 136, loss = 0.04480721\n",
      "Iteration 137, loss = 0.04469401\n",
      "Iteration 138, loss = 0.04457949\n",
      "Iteration 139, loss = 0.04450494\n",
      "Iteration 140, loss = 0.04439427\n",
      "Iteration 141, loss = 0.04430466\n",
      "Iteration 142, loss = 0.04418623\n",
      "Iteration 143, loss = 0.04410843\n",
      "Iteration 144, loss = 0.04401503\n",
      "Iteration 145, loss = 0.04392966\n",
      "Iteration 146, loss = 0.04382712\n",
      "Iteration 147, loss = 0.04373573\n",
      "Iteration 148, loss = 0.04365014\n",
      "Iteration 149, loss = 0.04356491\n",
      "Iteration 150, loss = 0.04350144\n",
      "Iteration 151, loss = 0.04340577\n",
      "Iteration 152, loss = 0.04332695\n",
      "Iteration 153, loss = 0.04326360\n",
      "Iteration 154, loss = 0.04313529\n",
      "Iteration 155, loss = 0.04308061\n",
      "Iteration 156, loss = 0.04299661\n",
      "Iteration 157, loss = 0.04292157\n",
      "Iteration 158, loss = 0.04283706\n",
      "Iteration 159, loss = 0.04279609\n",
      "Iteration 160, loss = 0.04269611\n",
      "Iteration 161, loss = 0.04259693\n",
      "Iteration 162, loss = 0.04254084\n",
      "Iteration 163, loss = 0.04245729\n",
      "Iteration 164, loss = 0.04239918\n",
      "Iteration 165, loss = 0.04234185\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.16116213\n",
      "Iteration 2, loss = 1.04649449\n",
      "Iteration 3, loss = 0.41742923\n",
      "Iteration 4, loss = 0.26527993\n",
      "Iteration 5, loss = 0.18396887\n",
      "Iteration 6, loss = 0.14997095\n",
      "Iteration 7, loss = 0.12203838\n",
      "Iteration 8, loss = 0.10606356\n",
      "Iteration 9, loss = 0.09713010\n",
      "Iteration 10, loss = 0.09205342\n",
      "Iteration 11, loss = 0.08776437\n",
      "Iteration 12, loss = 0.08449178\n",
      "Iteration 13, loss = 0.08279715\n",
      "Iteration 14, loss = 0.08102025\n",
      "Iteration 15, loss = 0.08007176\n",
      "Iteration 16, loss = 0.07852285\n",
      "Iteration 17, loss = 0.07771862\n",
      "Iteration 18, loss = 0.07673059\n",
      "Iteration 19, loss = 0.07583893\n",
      "Iteration 20, loss = 0.07505124\n",
      "Iteration 21, loss = 0.07437059\n",
      "Iteration 22, loss = 0.07375361\n",
      "Iteration 23, loss = 0.07314988\n",
      "Iteration 24, loss = 0.07251737\n",
      "Iteration 25, loss = 0.07195708\n",
      "Iteration 26, loss = 0.07157526\n",
      "Iteration 27, loss = 0.07096480\n",
      "Iteration 28, loss = 0.07051010\n",
      "Iteration 29, loss = 0.06993682\n",
      "Iteration 30, loss = 0.06953163\n",
      "Iteration 31, loss = 0.06906301\n",
      "Iteration 32, loss = 0.06866396\n",
      "Iteration 33, loss = 0.06815132\n",
      "Iteration 34, loss = 0.06763886\n",
      "Iteration 35, loss = 0.06722212\n",
      "Iteration 36, loss = 0.06679560\n",
      "Iteration 37, loss = 0.06646019\n",
      "Iteration 38, loss = 0.06602290\n",
      "Iteration 39, loss = 0.06561003\n",
      "Iteration 40, loss = 0.06527806\n",
      "Iteration 41, loss = 0.06490604\n",
      "Iteration 42, loss = 0.06448447\n",
      "Iteration 43, loss = 0.06415870\n",
      "Iteration 44, loss = 0.06376351\n",
      "Iteration 45, loss = 0.06342757\n",
      "Iteration 46, loss = 0.06313439\n",
      "Iteration 47, loss = 0.06277251\n",
      "Iteration 48, loss = 0.06241443\n",
      "Iteration 49, loss = 0.06206913\n",
      "Iteration 50, loss = 0.06174536\n",
      "Iteration 51, loss = 0.06149831\n",
      "Iteration 52, loss = 0.06109110\n",
      "Iteration 53, loss = 0.06079104\n",
      "Iteration 54, loss = 0.06048395\n",
      "Iteration 55, loss = 0.06018940\n",
      "Iteration 56, loss = 0.05990442\n",
      "Iteration 57, loss = 0.05962563\n",
      "Iteration 58, loss = 0.05932603\n",
      "Iteration 59, loss = 0.05904331\n",
      "Iteration 60, loss = 0.05874978\n",
      "Iteration 61, loss = 0.05849463\n",
      "Iteration 62, loss = 0.05823049\n",
      "Iteration 63, loss = 0.05794739\n",
      "Iteration 64, loss = 0.05766280\n",
      "Iteration 65, loss = 0.05743354\n",
      "Iteration 66, loss = 0.05716360\n",
      "Iteration 67, loss = 0.05690787\n",
      "Iteration 68, loss = 0.05664754\n",
      "Iteration 69, loss = 0.05640161\n",
      "Iteration 70, loss = 0.05616336\n",
      "Iteration 71, loss = 0.05593213\n",
      "Iteration 72, loss = 0.05568715\n",
      "Iteration 73, loss = 0.05547772\n",
      "Iteration 74, loss = 0.05522980\n",
      "Iteration 75, loss = 0.05504803\n",
      "Iteration 76, loss = 0.05477874\n",
      "Iteration 77, loss = 0.05461564\n",
      "Iteration 78, loss = 0.05435707\n",
      "Iteration 79, loss = 0.05415284\n",
      "Iteration 80, loss = 0.05391554\n",
      "Iteration 81, loss = 0.05368241\n",
      "Iteration 82, loss = 0.05348461\n",
      "Iteration 83, loss = 0.05328236\n",
      "Iteration 84, loss = 0.05308364\n",
      "Iteration 85, loss = 0.05290532\n",
      "Iteration 86, loss = 0.05269701\n",
      "Iteration 87, loss = 0.05248026\n",
      "Iteration 88, loss = 0.05230199\n",
      "Iteration 89, loss = 0.05209861\n",
      "Iteration 90, loss = 0.05199170\n",
      "Iteration 91, loss = 0.05174590\n",
      "Iteration 92, loss = 0.05157202\n",
      "Iteration 93, loss = 0.05142199\n",
      "Iteration 94, loss = 0.05123402\n",
      "Iteration 95, loss = 0.05106436\n",
      "Iteration 96, loss = 0.05088920\n",
      "Iteration 97, loss = 0.05070063\n",
      "Iteration 98, loss = 0.05052055\n",
      "Iteration 99, loss = 0.05035371\n",
      "Iteration 100, loss = 0.05022387\n",
      "Iteration 101, loss = 0.05001931\n",
      "Iteration 102, loss = 0.04987887\n",
      "Iteration 103, loss = 0.04975053\n",
      "Iteration 104, loss = 0.04954657\n",
      "Iteration 105, loss = 0.04945313\n",
      "Iteration 106, loss = 0.04930017\n",
      "Iteration 107, loss = 0.04914348\n",
      "Iteration 108, loss = 0.04898057\n",
      "Iteration 109, loss = 0.04884955\n",
      "Iteration 110, loss = 0.04870982\n",
      "Iteration 111, loss = 0.04857313\n",
      "Iteration 112, loss = 0.04839969\n",
      "Iteration 113, loss = 0.04826578\n",
      "Iteration 114, loss = 0.04811377\n",
      "Iteration 115, loss = 0.04800079\n",
      "Iteration 116, loss = 0.04786177\n",
      "Iteration 117, loss = 0.04771682\n",
      "Iteration 118, loss = 0.04759205\n",
      "Iteration 119, loss = 0.04747710\n",
      "Iteration 120, loss = 0.04737100\n",
      "Iteration 121, loss = 0.04722042\n",
      "Iteration 122, loss = 0.04709393\n",
      "Iteration 123, loss = 0.04697473\n",
      "Iteration 124, loss = 0.04685713\n",
      "Iteration 125, loss = 0.04674228\n",
      "Iteration 126, loss = 0.04664174\n",
      "Iteration 127, loss = 0.04651991\n",
      "Iteration 128, loss = 0.04638950\n",
      "Iteration 129, loss = 0.04626642\n",
      "Iteration 130, loss = 0.04615277\n",
      "Iteration 131, loss = 0.04605480\n",
      "Iteration 132, loss = 0.04595153\n",
      "Iteration 133, loss = 0.04581835\n",
      "Iteration 134, loss = 0.04575381\n",
      "Iteration 135, loss = 0.04566013\n",
      "Iteration 136, loss = 0.04554239\n",
      "Iteration 137, loss = 0.04543339\n",
      "Iteration 138, loss = 0.04532619\n",
      "Iteration 139, loss = 0.04524361\n",
      "Iteration 140, loss = 0.04513378\n",
      "Iteration 141, loss = 0.04503899\n",
      "Iteration 142, loss = 0.04493891\n",
      "Iteration 143, loss = 0.04487018\n",
      "Iteration 144, loss = 0.04476743\n",
      "Iteration 145, loss = 0.04466920\n",
      "Iteration 146, loss = 0.04457446\n",
      "Iteration 147, loss = 0.04449411\n",
      "Iteration 148, loss = 0.04441671\n",
      "Iteration 149, loss = 0.04430430\n",
      "Iteration 150, loss = 0.04423867\n",
      "Iteration 151, loss = 0.04414103\n",
      "Iteration 152, loss = 0.04407555\n",
      "Iteration 153, loss = 0.04398588\n",
      "Iteration 154, loss = 0.04388548\n",
      "Iteration 155, loss = 0.04382327\n",
      "Iteration 156, loss = 0.04373174\n",
      "Iteration 157, loss = 0.04365416\n",
      "Iteration 158, loss = 0.04357932\n",
      "Iteration 159, loss = 0.04352828\n",
      "Iteration 160, loss = 0.04343501\n",
      "Iteration 161, loss = 0.04333282\n",
      "Iteration 162, loss = 0.04329161\n",
      "Iteration 163, loss = 0.04318286\n",
      "Iteration 164, loss = 0.04312117\n",
      "Iteration 165, loss = 0.04303911\n",
      "Iteration 166, loss = 0.04299386\n",
      "Iteration 167, loss = 0.04293739\n",
      "Iteration 168, loss = 0.04284896\n",
      "Iteration 169, loss = 0.04277432\n",
      "Iteration 170, loss = 0.04274367\n",
      "Iteration 171, loss = 0.04264317\n",
      "Iteration 172, loss = 0.04257596\n",
      "Iteration 173, loss = 0.04250454\n",
      "Iteration 174, loss = 0.04243620\n",
      "Iteration 175, loss = 0.04236911\n",
      "Iteration 176, loss = 0.04231970\n",
      "Iteration 177, loss = 0.04224737\n",
      "Iteration 178, loss = 0.04222577\n",
      "Iteration 179, loss = 0.04214395\n",
      "Iteration 180, loss = 0.04207783\n",
      "Iteration 181, loss = 0.04201248\n",
      "Iteration 182, loss = 0.04194919\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15347899\n",
      "Iteration 2, loss = 1.00147397\n",
      "Iteration 3, loss = 0.36556918\n",
      "Iteration 4, loss = 0.22186811\n",
      "Iteration 5, loss = 0.15906595\n",
      "Iteration 6, loss = 0.12130630\n",
      "Iteration 7, loss = 0.10807486\n",
      "Iteration 8, loss = 0.09453697\n",
      "Iteration 9, loss = 0.08901934\n",
      "Iteration 10, loss = 0.08536714\n",
      "Iteration 11, loss = 0.08319567\n",
      "Iteration 12, loss = 0.08080471\n",
      "Iteration 13, loss = 0.07915716\n",
      "Iteration 14, loss = 0.07793764\n",
      "Iteration 15, loss = 0.07745481\n",
      "Iteration 16, loss = 0.07592428\n",
      "Iteration 17, loss = 0.07513555\n",
      "Iteration 18, loss = 0.07435591\n",
      "Iteration 19, loss = 0.07360435\n",
      "Iteration 20, loss = 0.07291718\n",
      "Iteration 21, loss = 0.07237500\n",
      "Iteration 22, loss = 0.07162780\n",
      "Iteration 23, loss = 0.07107974\n",
      "Iteration 24, loss = 0.07052447\n",
      "Iteration 25, loss = 0.07001213\n",
      "Iteration 26, loss = 0.06948434\n",
      "Iteration 27, loss = 0.06901636\n",
      "Iteration 28, loss = 0.06850057\n",
      "Iteration 29, loss = 0.06803301\n",
      "Iteration 30, loss = 0.06756699\n",
      "Iteration 31, loss = 0.06713571\n",
      "Iteration 32, loss = 0.06663407\n",
      "Iteration 33, loss = 0.06616762\n",
      "Iteration 34, loss = 0.06569549\n",
      "Iteration 35, loss = 0.06531566\n",
      "Iteration 36, loss = 0.06489949\n",
      "Iteration 37, loss = 0.06453660\n",
      "Iteration 38, loss = 0.06410747\n",
      "Iteration 39, loss = 0.06370847\n",
      "Iteration 40, loss = 0.06342781\n",
      "Iteration 41, loss = 0.06298447\n",
      "Iteration 42, loss = 0.06260590\n",
      "Iteration 43, loss = 0.06221789\n",
      "Iteration 44, loss = 0.06188606\n",
      "Iteration 45, loss = 0.06152711\n",
      "Iteration 46, loss = 0.06118783\n",
      "Iteration 47, loss = 0.06094490\n",
      "Iteration 48, loss = 0.06055610\n",
      "Iteration 49, loss = 0.06019920\n",
      "Iteration 50, loss = 0.05984438\n",
      "Iteration 51, loss = 0.05953514\n",
      "Iteration 52, loss = 0.05924623\n",
      "Iteration 53, loss = 0.05888958\n",
      "Iteration 54, loss = 0.05860637\n",
      "Iteration 55, loss = 0.05829181\n",
      "Iteration 56, loss = 0.05798349\n",
      "Iteration 57, loss = 0.05771809\n",
      "Iteration 58, loss = 0.05745624\n",
      "Iteration 59, loss = 0.05715875\n",
      "Iteration 60, loss = 0.05683805\n",
      "Iteration 61, loss = 0.05660037\n",
      "Iteration 62, loss = 0.05630302\n",
      "Iteration 63, loss = 0.05602374\n",
      "Iteration 64, loss = 0.05578920\n",
      "Iteration 65, loss = 0.05552908\n",
      "Iteration 66, loss = 0.05527875\n",
      "Iteration 67, loss = 0.05499324\n",
      "Iteration 68, loss = 0.05476750\n",
      "Iteration 69, loss = 0.05447732\n",
      "Iteration 70, loss = 0.05427155\n",
      "Iteration 71, loss = 0.05401344\n",
      "Iteration 72, loss = 0.05379692\n",
      "Iteration 73, loss = 0.05354206\n",
      "Iteration 74, loss = 0.05329873\n",
      "Iteration 75, loss = 0.05315527\n",
      "Iteration 76, loss = 0.05289662\n",
      "Iteration 77, loss = 0.05265903\n",
      "Iteration 78, loss = 0.05246027\n",
      "Iteration 79, loss = 0.05226609\n",
      "Iteration 80, loss = 0.05199974\n",
      "Iteration 81, loss = 0.05176750\n",
      "Iteration 82, loss = 0.05157832\n",
      "Iteration 83, loss = 0.05137187\n",
      "Iteration 84, loss = 0.05116008\n",
      "Iteration 85, loss = 0.05097475\n",
      "Iteration 86, loss = 0.05075966\n",
      "Iteration 87, loss = 0.05056778\n",
      "Iteration 88, loss = 0.05038959\n",
      "Iteration 89, loss = 0.05018812\n",
      "Iteration 90, loss = 0.05001054\n",
      "Iteration 91, loss = 0.04980883\n",
      "Iteration 92, loss = 0.04965762\n",
      "Iteration 93, loss = 0.04948898\n",
      "Iteration 94, loss = 0.04931565\n",
      "Iteration 95, loss = 0.04912441\n",
      "Iteration 96, loss = 0.04897129\n",
      "Iteration 97, loss = 0.04875600\n",
      "Iteration 98, loss = 0.04860007\n",
      "Iteration 99, loss = 0.04843028\n",
      "Iteration 100, loss = 0.04828167\n",
      "Iteration 101, loss = 0.04810880\n",
      "Iteration 102, loss = 0.04795233\n",
      "Iteration 103, loss = 0.04779851\n",
      "Iteration 104, loss = 0.04763626\n",
      "Iteration 105, loss = 0.04750826\n",
      "Iteration 106, loss = 0.04734271\n",
      "Iteration 107, loss = 0.04718148\n",
      "Iteration 108, loss = 0.04702637\n",
      "Iteration 109, loss = 0.04689113\n",
      "Iteration 110, loss = 0.04674245\n",
      "Iteration 111, loss = 0.04661067\n",
      "Iteration 112, loss = 0.04646967\n",
      "Iteration 113, loss = 0.04632631\n",
      "Iteration 114, loss = 0.04616579\n",
      "Iteration 115, loss = 0.04606469\n",
      "Iteration 116, loss = 0.04591303\n",
      "Iteration 117, loss = 0.04580009\n",
      "Iteration 118, loss = 0.04565109\n",
      "Iteration 119, loss = 0.04552548\n",
      "Iteration 120, loss = 0.04541230\n",
      "Iteration 121, loss = 0.04530001\n",
      "Iteration 122, loss = 0.04515392\n",
      "Iteration 123, loss = 0.04503876\n",
      "Iteration 124, loss = 0.04491958\n",
      "Iteration 125, loss = 0.04482253\n",
      "Iteration 126, loss = 0.04470715\n",
      "Iteration 127, loss = 0.04456409\n",
      "Iteration 128, loss = 0.04448326\n",
      "Iteration 129, loss = 0.04433127\n",
      "Iteration 130, loss = 0.04420454\n",
      "Iteration 131, loss = 0.04410393\n",
      "Iteration 132, loss = 0.04402044\n",
      "Iteration 133, loss = 0.04389036\n",
      "Iteration 134, loss = 0.04378997\n",
      "Iteration 135, loss = 0.04368654\n",
      "Iteration 136, loss = 0.04359303\n",
      "Iteration 137, loss = 0.04348006\n",
      "Iteration 138, loss = 0.04338892\n",
      "Iteration 139, loss = 0.04330334\n",
      "Iteration 140, loss = 0.04317003\n",
      "Iteration 141, loss = 0.04309552\n",
      "Iteration 142, loss = 0.04298856\n",
      "Iteration 143, loss = 0.04294687\n",
      "Iteration 144, loss = 0.04283550\n",
      "Iteration 145, loss = 0.04273091\n",
      "Iteration 146, loss = 0.04263770\n",
      "Iteration 147, loss = 0.04252795\n",
      "Iteration 148, loss = 0.04245963\n",
      "Iteration 149, loss = 0.04235696\n",
      "Iteration 150, loss = 0.04229290\n",
      "Iteration 151, loss = 0.04217961\n",
      "Iteration 152, loss = 0.04211884\n",
      "Iteration 153, loss = 0.04201917\n",
      "Iteration 154, loss = 0.04191231\n",
      "Iteration 155, loss = 0.04190205\n",
      "Iteration 156, loss = 0.04176870\n",
      "Iteration 157, loss = 0.04171078\n",
      "Iteration 158, loss = 0.04164246\n",
      "Iteration 159, loss = 0.04154678\n",
      "Iteration 160, loss = 0.04147822\n",
      "Iteration 161, loss = 0.04140361\n",
      "Iteration 162, loss = 0.04135068\n",
      "Iteration 163, loss = 0.04123426\n",
      "Iteration 164, loss = 0.04118314\n",
      "Iteration 165, loss = 0.04110536\n",
      "Iteration 166, loss = 0.04102673\n",
      "Iteration 167, loss = 0.04097484\n",
      "Iteration 168, loss = 0.04092923\n",
      "Iteration 169, loss = 0.04082668\n",
      "Iteration 170, loss = 0.04075400\n",
      "Iteration 171, loss = 0.04070877\n",
      "Iteration 172, loss = 0.04066578\n",
      "Iteration 173, loss = 0.04056419\n",
      "Iteration 174, loss = 0.04049746\n",
      "Iteration 175, loss = 0.04044251\n",
      "Iteration 176, loss = 0.04037323\n",
      "Iteration 177, loss = 0.04031655\n",
      "Iteration 178, loss = 0.04026705\n",
      "Iteration 179, loss = 0.04022161\n",
      "Iteration 180, loss = 0.04014892\n",
      "Iteration 181, loss = 0.04007484\n",
      "Iteration 182, loss = 0.04000505\n",
      "Iteration 183, loss = 0.03993803\n",
      "Iteration 184, loss = 0.03990256\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.46982012\n",
      "Iteration 2, loss = 0.25584333\n",
      "Iteration 3, loss = 0.14534666\n",
      "Iteration 4, loss = 0.12465026\n",
      "Iteration 5, loss = 0.11349513\n",
      "Iteration 6, loss = 0.10255529\n",
      "Iteration 7, loss = 0.09127371\n",
      "Iteration 8, loss = 0.08107622\n",
      "Iteration 9, loss = 0.07249343\n",
      "Iteration 10, loss = 0.06580002\n",
      "Iteration 11, loss = 0.06116935\n",
      "Iteration 12, loss = 0.05779603\n",
      "Iteration 13, loss = 0.05472823\n",
      "Iteration 14, loss = 0.05257660\n",
      "Iteration 15, loss = 0.05016379\n",
      "Iteration 16, loss = 0.04858271\n",
      "Iteration 17, loss = 0.04731852\n",
      "Iteration 18, loss = 0.04611937\n",
      "Iteration 19, loss = 0.04510348\n",
      "Iteration 20, loss = 0.04425216\n",
      "Iteration 21, loss = 0.04353270\n",
      "Iteration 22, loss = 0.04428334\n",
      "Iteration 23, loss = 0.04373959\n",
      "Iteration 24, loss = 0.04324958\n",
      "Iteration 25, loss = 0.04408482\n",
      "Iteration 26, loss = 0.04305718\n",
      "Iteration 27, loss = 0.04178409\n",
      "Iteration 28, loss = 0.04103076\n",
      "Iteration 29, loss = 0.04071589\n",
      "Iteration 30, loss = 0.04075028\n",
      "Iteration 31, loss = 0.04272538\n",
      "Iteration 32, loss = 0.04500254\n",
      "Iteration 33, loss = 0.04613304\n",
      "Iteration 34, loss = 0.04508696\n",
      "Iteration 35, loss = 0.04462015\n",
      "Iteration 36, loss = 0.04347723\n",
      "Iteration 37, loss = 0.04245821\n",
      "Iteration 38, loss = 0.04155680\n",
      "Iteration 39, loss = 0.04072070\n",
      "Iteration 40, loss = 0.03987179\n",
      "Iteration 41, loss = 0.03971251\n",
      "Iteration 42, loss = 0.03980604\n",
      "Iteration 43, loss = 0.03959388\n",
      "Iteration 44, loss = 0.03957850\n",
      "Iteration 45, loss = 0.03985029\n",
      "Iteration 46, loss = 0.04033520\n",
      "Iteration 47, loss = 0.03954349\n",
      "Iteration 48, loss = 0.03973720\n",
      "Iteration 49, loss = 0.03920461\n",
      "Iteration 50, loss = 0.03911673\n",
      "Iteration 51, loss = 0.03936882\n",
      "Iteration 52, loss = 0.03941149\n",
      "Iteration 53, loss = 0.03958310\n",
      "Iteration 54, loss = 0.04136015\n",
      "Iteration 55, loss = 0.04210318\n",
      "Iteration 56, loss = 0.04279408\n",
      "Iteration 57, loss = 0.04739539\n",
      "Iteration 58, loss = 0.05719630\n",
      "Iteration 59, loss = 0.08617208\n",
      "Iteration 60, loss = 0.11714597\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.51794577\n",
      "Iteration 2, loss = 0.27435911\n",
      "Iteration 3, loss = 0.15949979\n",
      "Iteration 4, loss = 0.12390529\n",
      "Iteration 5, loss = 0.11189544\n",
      "Iteration 6, loss = 0.10114585\n",
      "Iteration 7, loss = 0.08997123\n",
      "Iteration 8, loss = 0.07979320\n",
      "Iteration 9, loss = 0.07172280\n",
      "Iteration 10, loss = 0.06560808\n",
      "Iteration 11, loss = 0.06085122\n",
      "Iteration 12, loss = 0.05745892\n",
      "Iteration 13, loss = 0.05471918\n",
      "Iteration 14, loss = 0.05232571\n",
      "Iteration 15, loss = 0.05007390\n",
      "Iteration 16, loss = 0.04883739\n",
      "Iteration 17, loss = 0.04738925\n",
      "Iteration 18, loss = 0.04629604\n",
      "Iteration 19, loss = 0.04513282\n",
      "Iteration 20, loss = 0.04407959\n",
      "Iteration 21, loss = 0.04347334\n",
      "Iteration 22, loss = 0.04307582\n",
      "Iteration 23, loss = 0.04259759\n",
      "Iteration 24, loss = 0.04204554\n",
      "Iteration 25, loss = 0.04276327\n",
      "Iteration 26, loss = 0.04218131\n",
      "Iteration 27, loss = 0.04140737\n",
      "Iteration 28, loss = 0.04054970\n",
      "Iteration 29, loss = 0.04021851\n",
      "Iteration 30, loss = 0.04045965\n",
      "Iteration 31, loss = 0.04312821\n",
      "Iteration 32, loss = 0.04490019\n",
      "Iteration 33, loss = 0.04316662\n",
      "Iteration 34, loss = 0.04150527\n",
      "Iteration 35, loss = 0.04067272\n",
      "Iteration 36, loss = 0.04075883\n",
      "Iteration 37, loss = 0.04113866\n",
      "Iteration 38, loss = 0.04001171\n",
      "Iteration 39, loss = 0.03926370\n",
      "Iteration 40, loss = 0.03997288\n",
      "Iteration 41, loss = 0.04252330\n",
      "Iteration 42, loss = 0.04097030\n",
      "Iteration 43, loss = 0.04010904\n",
      "Iteration 44, loss = 0.03925201\n",
      "Iteration 45, loss = 0.03978890\n",
      "Iteration 46, loss = 0.04022176\n",
      "Iteration 47, loss = 0.03964083\n",
      "Iteration 48, loss = 0.03960062\n",
      "Iteration 49, loss = 0.03889079\n",
      "Iteration 50, loss = 0.03928139\n",
      "Iteration 51, loss = 0.03914021\n",
      "Iteration 52, loss = 0.03982149\n",
      "Iteration 53, loss = 0.03960275\n",
      "Iteration 54, loss = 0.03922345\n",
      "Iteration 55, loss = 0.03881195\n",
      "Iteration 56, loss = 0.04012414\n",
      "Iteration 57, loss = 0.04236222\n",
      "Iteration 58, loss = 0.04304985\n",
      "Iteration 59, loss = 0.04596951\n",
      "Iteration 60, loss = 0.05024320\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.52382033\n",
      "Iteration 2, loss = 0.32984631\n",
      "Iteration 3, loss = 0.17347712\n",
      "Iteration 4, loss = 0.13592081\n",
      "Iteration 5, loss = 0.12101730\n",
      "Iteration 6, loss = 0.10893435\n",
      "Iteration 7, loss = 0.09748097\n",
      "Iteration 8, loss = 0.08663200\n",
      "Iteration 9, loss = 0.07783002\n",
      "Iteration 10, loss = 0.07084174\n",
      "Iteration 11, loss = 0.06545049\n",
      "Iteration 12, loss = 0.06117546\n",
      "Iteration 13, loss = 0.05783294\n",
      "Iteration 14, loss = 0.05536653\n",
      "Iteration 15, loss = 0.05353213\n",
      "Iteration 16, loss = 0.05159967\n",
      "Iteration 17, loss = 0.05016025\n",
      "Iteration 18, loss = 0.04883102\n",
      "Iteration 19, loss = 0.04758253\n",
      "Iteration 20, loss = 0.04626296\n",
      "Iteration 21, loss = 0.04545611\n",
      "Iteration 22, loss = 0.04467766\n",
      "Iteration 23, loss = 0.04410234\n",
      "Iteration 24, loss = 0.04357728\n",
      "Iteration 25, loss = 0.04295996\n",
      "Iteration 26, loss = 0.04263573\n",
      "Iteration 27, loss = 0.04229262\n",
      "Iteration 28, loss = 0.04221819\n",
      "Iteration 29, loss = 0.04162715\n",
      "Iteration 30, loss = 0.04124834\n",
      "Iteration 31, loss = 0.04113964\n",
      "Iteration 32, loss = 0.04115128\n",
      "Iteration 33, loss = 0.04147278\n",
      "Iteration 34, loss = 0.04086366\n",
      "Iteration 35, loss = 0.04060836\n",
      "Iteration 36, loss = 0.04033261\n",
      "Iteration 37, loss = 0.04035535\n",
      "Iteration 38, loss = 0.04051188\n",
      "Iteration 39, loss = 0.04018990\n",
      "Iteration 40, loss = 0.04041816\n",
      "Iteration 41, loss = 0.04107727\n",
      "Iteration 42, loss = 0.04122743\n",
      "Iteration 43, loss = 0.04059777\n",
      "Iteration 44, loss = 0.04061945\n",
      "Iteration 45, loss = 0.04101605\n",
      "Iteration 46, loss = 0.04040281\n",
      "Iteration 47, loss = 0.04090291\n",
      "Iteration 48, loss = 0.04092719\n",
      "Iteration 49, loss = 0.04118299\n",
      "Iteration 50, loss = 0.04096821\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48482924\n",
      "Iteration 2, loss = 0.33780915\n",
      "Iteration 3, loss = 0.17791863\n",
      "Iteration 4, loss = 0.14045120\n",
      "Iteration 5, loss = 0.11994745\n",
      "Iteration 6, loss = 0.10805671\n",
      "Iteration 7, loss = 0.09791505\n",
      "Iteration 8, loss = 0.08741354\n",
      "Iteration 9, loss = 0.07893407\n",
      "Iteration 10, loss = 0.07233317\n",
      "Iteration 11, loss = 0.06710937\n",
      "Iteration 12, loss = 0.06295795\n",
      "Iteration 13, loss = 0.05976011\n",
      "Iteration 14, loss = 0.05728832\n",
      "Iteration 15, loss = 0.05534260\n",
      "Iteration 16, loss = 0.05348303\n",
      "Iteration 17, loss = 0.05205939\n",
      "Iteration 18, loss = 0.05040304\n",
      "Iteration 19, loss = 0.04909523\n",
      "Iteration 20, loss = 0.04785522\n",
      "Iteration 21, loss = 0.04687065\n",
      "Iteration 22, loss = 0.04614188\n",
      "Iteration 23, loss = 0.04558046\n",
      "Iteration 24, loss = 0.04507192\n",
      "Iteration 25, loss = 0.04445541\n",
      "Iteration 26, loss = 0.04428666\n",
      "Iteration 27, loss = 0.04420502\n",
      "Iteration 28, loss = 0.04368867\n",
      "Iteration 29, loss = 0.04311792\n",
      "Iteration 30, loss = 0.04291632\n",
      "Iteration 31, loss = 0.04235544\n",
      "Iteration 32, loss = 0.04234620\n",
      "Iteration 33, loss = 0.04257404\n",
      "Iteration 34, loss = 0.04216829\n",
      "Iteration 35, loss = 0.04188033\n",
      "Iteration 36, loss = 0.04137946\n",
      "Iteration 37, loss = 0.04179079\n",
      "Iteration 38, loss = 0.04133127\n",
      "Iteration 39, loss = 0.04104128\n",
      "Iteration 40, loss = 0.04190304\n",
      "Iteration 41, loss = 0.04282596\n",
      "Iteration 42, loss = 0.04210905\n",
      "Iteration 43, loss = 0.04166523\n",
      "Iteration 44, loss = 0.04119361\n",
      "Iteration 45, loss = 0.04075124\n",
      "Iteration 46, loss = 0.04145920\n",
      "Iteration 47, loss = 0.04277068\n",
      "Iteration 48, loss = 0.04660912\n",
      "Iteration 49, loss = 0.04938392\n",
      "Iteration 50, loss = 0.05259177\n",
      "Iteration 51, loss = 0.05161458\n",
      "Iteration 52, loss = 0.05416121\n",
      "Iteration 53, loss = 0.05600473\n",
      "Iteration 54, loss = 0.05911401\n",
      "Iteration 55, loss = 0.05590929\n",
      "Iteration 56, loss = 0.05330987\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48950620\n",
      "Iteration 2, loss = 0.29695410\n",
      "Iteration 3, loss = 0.16229116\n",
      "Iteration 4, loss = 0.13630873\n",
      "Iteration 5, loss = 0.11871309\n",
      "Iteration 6, loss = 0.10718807\n",
      "Iteration 7, loss = 0.09481507\n",
      "Iteration 8, loss = 0.08478069\n",
      "Iteration 9, loss = 0.07603884\n",
      "Iteration 10, loss = 0.06897942\n",
      "Iteration 11, loss = 0.06351744\n",
      "Iteration 12, loss = 0.05938539\n",
      "Iteration 13, loss = 0.05621988\n",
      "Iteration 14, loss = 0.05360133\n",
      "Iteration 15, loss = 0.05164310\n",
      "Iteration 16, loss = 0.04977011\n",
      "Iteration 17, loss = 0.04813730\n",
      "Iteration 18, loss = 0.04679524\n",
      "Iteration 19, loss = 0.04560394\n",
      "Iteration 20, loss = 0.04484506\n",
      "Iteration 21, loss = 0.04429918\n",
      "Iteration 22, loss = 0.04333287\n",
      "Iteration 23, loss = 0.04260535\n",
      "Iteration 24, loss = 0.04206080\n",
      "Iteration 25, loss = 0.04167768\n",
      "Iteration 26, loss = 0.04132289\n",
      "Iteration 27, loss = 0.04106585\n",
      "Iteration 28, loss = 0.04058207\n",
      "Iteration 29, loss = 0.04034679\n",
      "Iteration 30, loss = 0.03993038\n",
      "Iteration 31, loss = 0.03986352\n",
      "Iteration 32, loss = 0.03960391\n",
      "Iteration 33, loss = 0.03948096\n",
      "Iteration 34, loss = 0.03940714\n",
      "Iteration 35, loss = 0.03918024\n",
      "Iteration 36, loss = 0.03882877\n",
      "Iteration 37, loss = 0.03912195\n",
      "Iteration 38, loss = 0.03939825\n",
      "Iteration 39, loss = 0.03981315\n",
      "Iteration 40, loss = 0.04056222\n",
      "Iteration 41, loss = 0.04083491\n",
      "Iteration 42, loss = 0.03979610\n",
      "Iteration 43, loss = 0.03896334\n",
      "Iteration 44, loss = 0.03882014\n",
      "Iteration 45, loss = 0.03861741\n",
      "Iteration 46, loss = 0.03893122\n",
      "Iteration 47, loss = 0.03871728\n",
      "Iteration 48, loss = 0.03937837\n",
      "Iteration 49, loss = 0.03833587\n",
      "Iteration 50, loss = 0.03862758\n",
      "Iteration 51, loss = 0.03845614\n",
      "Iteration 52, loss = 0.03825667\n",
      "Iteration 53, loss = 0.03838855\n",
      "Iteration 54, loss = 0.03811926\n",
      "Iteration 55, loss = 0.03821339\n",
      "Iteration 56, loss = 0.03784202\n",
      "Iteration 57, loss = 0.03824900\n",
      "Iteration 58, loss = 0.03892040\n",
      "Iteration 59, loss = 0.04063725\n",
      "Iteration 60, loss = 0.04066968\n",
      "Iteration 61, loss = 0.03932572\n",
      "Iteration 62, loss = 0.03865688\n",
      "Iteration 63, loss = 0.03815097\n",
      "Iteration 64, loss = 0.03770320\n",
      "Iteration 65, loss = 0.03773436\n",
      "Iteration 66, loss = 0.03851799\n",
      "Iteration 67, loss = 0.03956849\n",
      "Iteration 68, loss = 0.04133825\n",
      "Iteration 69, loss = 0.04162508\n",
      "Iteration 70, loss = 0.04013102\n",
      "Iteration 71, loss = 0.03953565\n",
      "Iteration 72, loss = 0.03916548\n",
      "Iteration 73, loss = 0.03844507\n",
      "Iteration 74, loss = 0.03847138\n",
      "Iteration 75, loss = 0.03909982\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38885241\n",
      "Iteration 2, loss = 2.24119468\n",
      "Iteration 3, loss = 2.06075422\n",
      "Iteration 4, loss = 1.84245015\n",
      "Iteration 5, loss = 1.59885557\n",
      "Iteration 6, loss = 1.34944582\n",
      "Iteration 7, loss = 1.12245960\n",
      "Iteration 8, loss = 0.93799970\n",
      "Iteration 9, loss = 0.78874857\n",
      "Iteration 10, loss = 0.67755036\n",
      "Iteration 11, loss = 0.59085375\n",
      "Iteration 12, loss = 0.52900998\n",
      "Iteration 13, loss = 0.47825316\n",
      "Iteration 14, loss = 0.43432622\n",
      "Iteration 15, loss = 0.40200731\n",
      "Iteration 16, loss = 0.37293166\n",
      "Iteration 17, loss = 0.34778379\n",
      "Iteration 18, loss = 0.32668203\n",
      "Iteration 19, loss = 0.30825211\n",
      "Iteration 20, loss = 0.29148111\n",
      "Iteration 21, loss = 0.27781065\n",
      "Iteration 22, loss = 0.26533898\n",
      "Iteration 23, loss = 0.25331699\n",
      "Iteration 24, loss = 0.24221105\n",
      "Iteration 25, loss = 0.23311480\n",
      "Iteration 26, loss = 0.22360922\n",
      "Iteration 27, loss = 0.21641319\n",
      "Iteration 28, loss = 0.20788422\n",
      "Iteration 29, loss = 0.20138309\n",
      "Iteration 30, loss = 0.19437045\n",
      "Iteration 31, loss = 0.18998256\n",
      "Iteration 32, loss = 0.18411120\n",
      "Iteration 33, loss = 0.17827831\n",
      "Iteration 34, loss = 0.17307524\n",
      "Iteration 35, loss = 0.16847968\n",
      "Iteration 36, loss = 0.16427563\n",
      "Iteration 37, loss = 0.16023626\n",
      "Iteration 38, loss = 0.15638159\n",
      "Iteration 39, loss = 0.15290942\n",
      "Iteration 40, loss = 0.14982561\n",
      "Iteration 41, loss = 0.14654982\n",
      "Iteration 42, loss = 0.14364820\n",
      "Iteration 43, loss = 0.14128386\n",
      "Iteration 44, loss = 0.13834698\n",
      "Iteration 45, loss = 0.13604345\n",
      "Iteration 46, loss = 0.13364983\n",
      "Iteration 47, loss = 0.13138352\n",
      "Iteration 48, loss = 0.12933900\n",
      "Iteration 49, loss = 0.12731453\n",
      "Iteration 50, loss = 0.12561725\n",
      "Iteration 51, loss = 0.12373986\n",
      "Iteration 52, loss = 0.12209850\n",
      "Iteration 53, loss = 0.12040503\n",
      "Iteration 54, loss = 0.11888952\n",
      "Iteration 55, loss = 0.11729497\n",
      "Iteration 56, loss = 0.11590954\n",
      "Iteration 57, loss = 0.11453671\n",
      "Iteration 58, loss = 0.11315252\n",
      "Iteration 59, loss = 0.11178685\n",
      "Iteration 60, loss = 0.11070227\n",
      "Iteration 61, loss = 0.10923682\n",
      "Iteration 62, loss = 0.10823390\n",
      "Iteration 63, loss = 0.10710857\n",
      "Iteration 64, loss = 0.10606440\n",
      "Iteration 65, loss = 0.10517826\n",
      "Iteration 66, loss = 0.10424182\n",
      "Iteration 67, loss = 0.10346368\n",
      "Iteration 68, loss = 0.10260946\n",
      "Iteration 69, loss = 0.10168655\n",
      "Iteration 70, loss = 0.10105816\n",
      "Iteration 71, loss = 0.10015584\n",
      "Iteration 72, loss = 0.09939610\n",
      "Iteration 73, loss = 0.09870927\n",
      "Iteration 74, loss = 0.09811266\n",
      "Iteration 75, loss = 0.09736400\n",
      "Iteration 76, loss = 0.09684375\n",
      "Iteration 77, loss = 0.09625133\n",
      "Iteration 78, loss = 0.09557762\n",
      "Iteration 79, loss = 0.09502208\n",
      "Iteration 80, loss = 0.09440303\n",
      "Iteration 81, loss = 0.09386158\n",
      "Iteration 82, loss = 0.09326329\n",
      "Iteration 83, loss = 0.09285479\n",
      "Iteration 84, loss = 0.09230619\n",
      "Iteration 85, loss = 0.09174651\n",
      "Iteration 86, loss = 0.09137765\n",
      "Iteration 87, loss = 0.09097459\n",
      "Iteration 88, loss = 0.09041342\n",
      "Iteration 89, loss = 0.09000531\n",
      "Iteration 90, loss = 0.08962278\n",
      "Iteration 91, loss = 0.08925515\n",
      "Iteration 92, loss = 0.08884609\n",
      "Iteration 93, loss = 0.08852065\n",
      "Iteration 94, loss = 0.08809305\n",
      "Iteration 95, loss = 0.08766904\n",
      "Iteration 96, loss = 0.08739976\n",
      "Iteration 97, loss = 0.08708971\n",
      "Iteration 98, loss = 0.08669004\n",
      "Iteration 99, loss = 0.08641748\n",
      "Iteration 100, loss = 0.08608037\n",
      "Iteration 101, loss = 0.08580317\n",
      "Iteration 102, loss = 0.08548715\n",
      "Iteration 103, loss = 0.08520967\n",
      "Iteration 104, loss = 0.08493394\n",
      "Iteration 105, loss = 0.08468118\n",
      "Iteration 106, loss = 0.08440392\n",
      "Iteration 107, loss = 0.08412940\n",
      "Iteration 108, loss = 0.08386896\n",
      "Iteration 109, loss = 0.08367224\n",
      "Iteration 110, loss = 0.08342443\n",
      "Iteration 111, loss = 0.08319903\n",
      "Iteration 112, loss = 0.08292318\n",
      "Iteration 113, loss = 0.08271097\n",
      "Iteration 114, loss = 0.08253052\n",
      "Iteration 115, loss = 0.08235099\n",
      "Iteration 116, loss = 0.08209307\n",
      "Iteration 117, loss = 0.08179492\n",
      "Iteration 118, loss = 0.08161308\n",
      "Iteration 119, loss = 0.08141380\n",
      "Iteration 120, loss = 0.08123080\n",
      "Iteration 121, loss = 0.08105222\n",
      "Iteration 122, loss = 0.08082976\n",
      "Iteration 123, loss = 0.08064663\n",
      "Iteration 124, loss = 0.08048919\n",
      "Iteration 125, loss = 0.08029365\n",
      "Iteration 126, loss = 0.08008218\n",
      "Iteration 127, loss = 0.07991762\n",
      "Iteration 128, loss = 0.07975118\n",
      "Iteration 129, loss = 0.07958001\n",
      "Iteration 130, loss = 0.07940826\n",
      "Iteration 131, loss = 0.07924616\n",
      "Iteration 132, loss = 0.07911619\n",
      "Iteration 133, loss = 0.07892285\n",
      "Iteration 134, loss = 0.07877286\n",
      "Iteration 135, loss = 0.07868209\n",
      "Iteration 136, loss = 0.07848612\n",
      "Iteration 137, loss = 0.07832648\n",
      "Iteration 138, loss = 0.07817873\n",
      "Iteration 139, loss = 0.07804706\n",
      "Iteration 140, loss = 0.07791852\n",
      "Iteration 141, loss = 0.07776818\n",
      "Iteration 142, loss = 0.07763973\n",
      "Iteration 143, loss = 0.07747096\n",
      "Iteration 144, loss = 0.07733477\n",
      "Iteration 145, loss = 0.07722261\n",
      "Iteration 146, loss = 0.07710497\n",
      "Iteration 147, loss = 0.07697658\n",
      "Iteration 148, loss = 0.07685024\n",
      "Iteration 149, loss = 0.07673481\n",
      "Iteration 150, loss = 0.07660715\n",
      "Iteration 151, loss = 0.07650193\n",
      "Iteration 152, loss = 0.07638777\n",
      "Iteration 153, loss = 0.07624378\n",
      "Iteration 154, loss = 0.07613541\n",
      "Iteration 155, loss = 0.07601389\n",
      "Iteration 156, loss = 0.07591078\n",
      "Iteration 157, loss = 0.07581829\n",
      "Iteration 158, loss = 0.07570869\n",
      "Iteration 159, loss = 0.07559212\n",
      "Iteration 160, loss = 0.07548422\n",
      "Iteration 161, loss = 0.07540391\n",
      "Iteration 162, loss = 0.07531846\n",
      "Iteration 163, loss = 0.07519276\n",
      "Iteration 164, loss = 0.07508487\n",
      "Iteration 165, loss = 0.07498815\n",
      "Iteration 166, loss = 0.07489549\n",
      "Iteration 167, loss = 0.07479816\n",
      "Iteration 168, loss = 0.07469223\n",
      "Iteration 169, loss = 0.07459614\n",
      "Iteration 170, loss = 0.07449980\n",
      "Iteration 171, loss = 0.07439795\n",
      "Iteration 172, loss = 0.07431590\n",
      "Iteration 173, loss = 0.07423291\n",
      "Iteration 174, loss = 0.07415435\n",
      "Iteration 175, loss = 0.07404640\n",
      "Iteration 176, loss = 0.07395782\n",
      "Iteration 177, loss = 0.07386586\n",
      "Iteration 178, loss = 0.07378307\n",
      "Iteration 179, loss = 0.07368899\n",
      "Iteration 180, loss = 0.07360819\n",
      "Iteration 181, loss = 0.07353728\n",
      "Iteration 182, loss = 0.07344266\n",
      "Iteration 183, loss = 0.07335815\n",
      "Iteration 184, loss = 0.07328244\n",
      "Iteration 185, loss = 0.07319815\n",
      "Iteration 186, loss = 0.07311975\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38495522\n",
      "Iteration 2, loss = 2.23393095\n",
      "Iteration 3, loss = 2.04725479\n",
      "Iteration 4, loss = 1.81697456\n",
      "Iteration 5, loss = 1.55889438\n",
      "Iteration 6, loss = 1.30485434\n",
      "Iteration 7, loss = 1.07492064\n",
      "Iteration 8, loss = 0.88946518\n",
      "Iteration 9, loss = 0.74410422\n",
      "Iteration 10, loss = 0.63904702\n",
      "Iteration 11, loss = 0.55919461\n",
      "Iteration 12, loss = 0.50102612\n",
      "Iteration 13, loss = 0.45207093\n",
      "Iteration 14, loss = 0.41300034\n",
      "Iteration 15, loss = 0.38126007\n",
      "Iteration 16, loss = 0.35682334\n",
      "Iteration 17, loss = 0.33218787\n",
      "Iteration 18, loss = 0.31342142\n",
      "Iteration 19, loss = 0.29557415\n",
      "Iteration 20, loss = 0.28074062\n",
      "Iteration 21, loss = 0.26804890\n",
      "Iteration 22, loss = 0.25586288\n",
      "Iteration 23, loss = 0.24499277\n",
      "Iteration 24, loss = 0.23540709\n",
      "Iteration 25, loss = 0.22677584\n",
      "Iteration 26, loss = 0.21854379\n",
      "Iteration 27, loss = 0.21068035\n",
      "Iteration 28, loss = 0.20342271\n",
      "Iteration 29, loss = 0.19708001\n",
      "Iteration 30, loss = 0.19102288\n",
      "Iteration 31, loss = 0.18670420\n",
      "Iteration 32, loss = 0.18111205\n",
      "Iteration 33, loss = 0.17539686\n",
      "Iteration 34, loss = 0.17097076\n",
      "Iteration 35, loss = 0.16667481\n",
      "Iteration 36, loss = 0.16323701\n",
      "Iteration 37, loss = 0.15929285\n",
      "Iteration 38, loss = 0.15537917\n",
      "Iteration 39, loss = 0.15225156\n",
      "Iteration 40, loss = 0.14957663\n",
      "Iteration 41, loss = 0.14635691\n",
      "Iteration 42, loss = 0.14334191\n",
      "Iteration 43, loss = 0.14101386\n",
      "Iteration 44, loss = 0.13794442\n",
      "Iteration 45, loss = 0.13550585\n",
      "Iteration 46, loss = 0.13327820\n",
      "Iteration 47, loss = 0.13126849\n",
      "Iteration 48, loss = 0.12921311\n",
      "Iteration 49, loss = 0.12720420\n",
      "Iteration 50, loss = 0.12550828\n",
      "Iteration 51, loss = 0.12381030\n",
      "Iteration 52, loss = 0.12195912\n",
      "Iteration 53, loss = 0.12048040\n",
      "Iteration 54, loss = 0.11886206\n",
      "Iteration 55, loss = 0.11748178\n",
      "Iteration 56, loss = 0.11596077\n",
      "Iteration 57, loss = 0.11462100\n",
      "Iteration 58, loss = 0.11325836\n",
      "Iteration 59, loss = 0.11185618\n",
      "Iteration 60, loss = 0.11073259\n",
      "Iteration 61, loss = 0.10934919\n",
      "Iteration 62, loss = 0.10830850\n",
      "Iteration 63, loss = 0.10732166\n",
      "Iteration 64, loss = 0.10621702\n",
      "Iteration 65, loss = 0.10527762\n",
      "Iteration 66, loss = 0.10441618\n",
      "Iteration 67, loss = 0.10350938\n",
      "Iteration 68, loss = 0.10255428\n",
      "Iteration 69, loss = 0.10176234\n",
      "Iteration 70, loss = 0.10103331\n",
      "Iteration 71, loss = 0.10009688\n",
      "Iteration 72, loss = 0.09939811\n",
      "Iteration 73, loss = 0.09872811\n",
      "Iteration 74, loss = 0.09807528\n",
      "Iteration 75, loss = 0.09725972\n",
      "Iteration 76, loss = 0.09695957\n",
      "Iteration 77, loss = 0.09608899\n",
      "Iteration 78, loss = 0.09539091\n",
      "Iteration 79, loss = 0.09482243\n",
      "Iteration 80, loss = 0.09419898\n",
      "Iteration 81, loss = 0.09365919\n",
      "Iteration 82, loss = 0.09305175\n",
      "Iteration 83, loss = 0.09264716\n",
      "Iteration 84, loss = 0.09207803\n",
      "Iteration 85, loss = 0.09152691\n",
      "Iteration 86, loss = 0.09107237\n",
      "Iteration 87, loss = 0.09064187\n",
      "Iteration 88, loss = 0.09010108\n",
      "Iteration 89, loss = 0.08965947\n",
      "Iteration 90, loss = 0.08927888\n",
      "Iteration 91, loss = 0.08886620\n",
      "Iteration 92, loss = 0.08856093\n",
      "Iteration 93, loss = 0.08819491\n",
      "Iteration 94, loss = 0.08773556\n",
      "Iteration 95, loss = 0.08731775\n",
      "Iteration 96, loss = 0.08703768\n",
      "Iteration 97, loss = 0.08666028\n",
      "Iteration 98, loss = 0.08629158\n",
      "Iteration 99, loss = 0.08603061\n",
      "Iteration 100, loss = 0.08566542\n",
      "Iteration 101, loss = 0.08539681\n",
      "Iteration 102, loss = 0.08507310\n",
      "Iteration 103, loss = 0.08477547\n",
      "Iteration 104, loss = 0.08449302\n",
      "Iteration 105, loss = 0.08423950\n",
      "Iteration 106, loss = 0.08394568\n",
      "Iteration 107, loss = 0.08368087\n",
      "Iteration 108, loss = 0.08340798\n",
      "Iteration 109, loss = 0.08318839\n",
      "Iteration 110, loss = 0.08295943\n",
      "Iteration 111, loss = 0.08274565\n",
      "Iteration 112, loss = 0.08247491\n",
      "Iteration 113, loss = 0.08225128\n",
      "Iteration 114, loss = 0.08208576\n",
      "Iteration 115, loss = 0.08186931\n",
      "Iteration 116, loss = 0.08161934\n",
      "Iteration 117, loss = 0.08136655\n",
      "Iteration 118, loss = 0.08116955\n",
      "Iteration 119, loss = 0.08093763\n",
      "Iteration 120, loss = 0.08071292\n",
      "Iteration 121, loss = 0.08053449\n",
      "Iteration 122, loss = 0.08032452\n",
      "Iteration 123, loss = 0.08014406\n",
      "Iteration 124, loss = 0.07993217\n",
      "Iteration 125, loss = 0.07976567\n",
      "Iteration 126, loss = 0.07956488\n",
      "Iteration 127, loss = 0.07939538\n",
      "Iteration 128, loss = 0.07925286\n",
      "Iteration 129, loss = 0.07909190\n",
      "Iteration 130, loss = 0.07889774\n",
      "Iteration 131, loss = 0.07873301\n",
      "Iteration 132, loss = 0.07861623\n",
      "Iteration 133, loss = 0.07842927\n",
      "Iteration 134, loss = 0.07826254\n",
      "Iteration 135, loss = 0.07812438\n",
      "Iteration 136, loss = 0.07796597\n",
      "Iteration 137, loss = 0.07780241\n",
      "Iteration 138, loss = 0.07766295\n",
      "Iteration 139, loss = 0.07752536\n",
      "Iteration 140, loss = 0.07738581\n",
      "Iteration 141, loss = 0.07723428\n",
      "Iteration 142, loss = 0.07711446\n",
      "Iteration 143, loss = 0.07696483\n",
      "Iteration 144, loss = 0.07682313\n",
      "Iteration 145, loss = 0.07670404\n",
      "Iteration 146, loss = 0.07658549\n",
      "Iteration 147, loss = 0.07645894\n",
      "Iteration 148, loss = 0.07632082\n",
      "Iteration 149, loss = 0.07621425\n",
      "Iteration 150, loss = 0.07606630\n",
      "Iteration 151, loss = 0.07595592\n",
      "Iteration 152, loss = 0.07585286\n",
      "Iteration 153, loss = 0.07570715\n",
      "Iteration 154, loss = 0.07560903\n",
      "Iteration 155, loss = 0.07548077\n",
      "Iteration 156, loss = 0.07536919\n",
      "Iteration 157, loss = 0.07527198\n",
      "Iteration 158, loss = 0.07515557\n",
      "Iteration 159, loss = 0.07506259\n",
      "Iteration 160, loss = 0.07493926\n",
      "Iteration 161, loss = 0.07487236\n",
      "Iteration 162, loss = 0.07477343\n",
      "Iteration 163, loss = 0.07465063\n",
      "Iteration 164, loss = 0.07454757\n",
      "Iteration 165, loss = 0.07443403\n",
      "Iteration 166, loss = 0.07434126\n",
      "Iteration 167, loss = 0.07424282\n",
      "Iteration 168, loss = 0.07413830\n",
      "Iteration 169, loss = 0.07404411\n",
      "Iteration 170, loss = 0.07395456\n",
      "Iteration 171, loss = 0.07385099\n",
      "Iteration 172, loss = 0.07376488\n",
      "Iteration 173, loss = 0.07367456\n",
      "Iteration 174, loss = 0.07360168\n",
      "Iteration 175, loss = 0.07349761\n",
      "Iteration 176, loss = 0.07340869\n",
      "Iteration 177, loss = 0.07332022\n",
      "Iteration 178, loss = 0.07322970\n",
      "Iteration 179, loss = 0.07314005\n",
      "Iteration 180, loss = 0.07304718\n",
      "Iteration 181, loss = 0.07297353\n",
      "Iteration 182, loss = 0.07288119\n",
      "Iteration 183, loss = 0.07279676\n",
      "Iteration 184, loss = 0.07271637\n",
      "Iteration 185, loss = 0.07263080\n",
      "Iteration 186, loss = 0.07254445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39008048\n",
      "Iteration 2, loss = 2.24394060\n",
      "Iteration 3, loss = 2.06393940\n",
      "Iteration 4, loss = 1.84087110\n",
      "Iteration 5, loss = 1.59125155\n",
      "Iteration 6, loss = 1.33796324\n",
      "Iteration 7, loss = 1.10954730\n",
      "Iteration 8, loss = 0.92336798\n",
      "Iteration 9, loss = 0.77699835\n",
      "Iteration 10, loss = 0.66686307\n",
      "Iteration 11, loss = 0.58589547\n",
      "Iteration 12, loss = 0.52239251\n",
      "Iteration 13, loss = 0.47332382\n",
      "Iteration 14, loss = 0.43386967\n",
      "Iteration 15, loss = 0.40264618\n",
      "Iteration 16, loss = 0.37292465\n",
      "Iteration 17, loss = 0.35008829\n",
      "Iteration 18, loss = 0.32802435\n",
      "Iteration 19, loss = 0.31054452\n",
      "Iteration 20, loss = 0.29464911\n",
      "Iteration 21, loss = 0.28012865\n",
      "Iteration 22, loss = 0.26755437\n",
      "Iteration 23, loss = 0.25573296\n",
      "Iteration 24, loss = 0.24575682\n",
      "Iteration 25, loss = 0.23620812\n",
      "Iteration 26, loss = 0.22814110\n",
      "Iteration 27, loss = 0.21940689\n",
      "Iteration 28, loss = 0.21269301\n",
      "Iteration 29, loss = 0.20580615\n",
      "Iteration 30, loss = 0.19967426\n",
      "Iteration 31, loss = 0.19340402\n",
      "Iteration 32, loss = 0.18754905\n",
      "Iteration 33, loss = 0.18241109\n",
      "Iteration 34, loss = 0.17732208\n",
      "Iteration 35, loss = 0.17290329\n",
      "Iteration 36, loss = 0.16916040\n",
      "Iteration 37, loss = 0.16497895\n",
      "Iteration 38, loss = 0.16098317\n",
      "Iteration 39, loss = 0.15746511\n",
      "Iteration 40, loss = 0.15433974\n",
      "Iteration 41, loss = 0.15139584\n",
      "Iteration 42, loss = 0.14822595\n",
      "Iteration 43, loss = 0.14534579\n",
      "Iteration 44, loss = 0.14247089\n",
      "Iteration 45, loss = 0.14015666\n",
      "Iteration 46, loss = 0.13792944\n",
      "Iteration 47, loss = 0.13519029\n",
      "Iteration 48, loss = 0.13297747\n",
      "Iteration 49, loss = 0.13105312\n",
      "Iteration 50, loss = 0.12892357\n",
      "Iteration 51, loss = 0.12710991\n",
      "Iteration 52, loss = 0.12532234\n",
      "Iteration 53, loss = 0.12341449\n",
      "Iteration 54, loss = 0.12177831\n",
      "Iteration 55, loss = 0.12026255\n",
      "Iteration 56, loss = 0.11875478\n",
      "Iteration 57, loss = 0.11745731\n",
      "Iteration 58, loss = 0.11603142\n",
      "Iteration 59, loss = 0.11459145\n",
      "Iteration 60, loss = 0.11337663\n",
      "Iteration 61, loss = 0.11211171\n",
      "Iteration 62, loss = 0.11109882\n",
      "Iteration 63, loss = 0.11007263\n",
      "Iteration 64, loss = 0.10884566\n",
      "Iteration 65, loss = 0.10765312\n",
      "Iteration 66, loss = 0.10667368\n",
      "Iteration 67, loss = 0.10579169\n",
      "Iteration 68, loss = 0.10480806\n",
      "Iteration 69, loss = 0.10396138\n",
      "Iteration 70, loss = 0.10307783\n",
      "Iteration 71, loss = 0.10220895\n",
      "Iteration 72, loss = 0.10149887\n",
      "Iteration 73, loss = 0.10060083\n",
      "Iteration 74, loss = 0.09989375\n",
      "Iteration 75, loss = 0.09925222\n",
      "Iteration 76, loss = 0.09843487\n",
      "Iteration 77, loss = 0.09797866\n",
      "Iteration 78, loss = 0.09720173\n",
      "Iteration 79, loss = 0.09636293\n",
      "Iteration 80, loss = 0.09590027\n",
      "Iteration 81, loss = 0.09519281\n",
      "Iteration 82, loss = 0.09458801\n",
      "Iteration 83, loss = 0.09403221\n",
      "Iteration 84, loss = 0.09350555\n",
      "Iteration 85, loss = 0.09301092\n",
      "Iteration 86, loss = 0.09261050\n",
      "Iteration 87, loss = 0.09208451\n",
      "Iteration 88, loss = 0.09157151\n",
      "Iteration 89, loss = 0.09113289\n",
      "Iteration 90, loss = 0.09084316\n",
      "Iteration 91, loss = 0.09032812\n",
      "Iteration 92, loss = 0.08985994\n",
      "Iteration 93, loss = 0.08949181\n",
      "Iteration 94, loss = 0.08912413\n",
      "Iteration 95, loss = 0.08870899\n",
      "Iteration 96, loss = 0.08835923\n",
      "Iteration 97, loss = 0.08802086\n",
      "Iteration 98, loss = 0.08767174\n",
      "Iteration 99, loss = 0.08729550\n",
      "Iteration 100, loss = 0.08695845\n",
      "Iteration 101, loss = 0.08665516\n",
      "Iteration 102, loss = 0.08631986\n",
      "Iteration 103, loss = 0.08600405\n",
      "Iteration 104, loss = 0.08570270\n",
      "Iteration 105, loss = 0.08541403\n",
      "Iteration 106, loss = 0.08514331\n",
      "Iteration 107, loss = 0.08489492\n",
      "Iteration 108, loss = 0.08461283\n",
      "Iteration 109, loss = 0.08432183\n",
      "Iteration 110, loss = 0.08406020\n",
      "Iteration 111, loss = 0.08381157\n",
      "Iteration 112, loss = 0.08361632\n",
      "Iteration 113, loss = 0.08337052\n",
      "Iteration 114, loss = 0.08312386\n",
      "Iteration 115, loss = 0.08289093\n",
      "Iteration 116, loss = 0.08265789\n",
      "Iteration 117, loss = 0.08242961\n",
      "Iteration 118, loss = 0.08224368\n",
      "Iteration 119, loss = 0.08202648\n",
      "Iteration 120, loss = 0.08184247\n",
      "Iteration 121, loss = 0.08161737\n",
      "Iteration 122, loss = 0.08140351\n",
      "Iteration 123, loss = 0.08122423\n",
      "Iteration 124, loss = 0.08103803\n",
      "Iteration 125, loss = 0.08082817\n",
      "Iteration 126, loss = 0.08064014\n",
      "Iteration 127, loss = 0.08045240\n",
      "Iteration 128, loss = 0.08029627\n",
      "Iteration 129, loss = 0.08012832\n",
      "Iteration 130, loss = 0.07994567\n",
      "Iteration 131, loss = 0.07977446\n",
      "Iteration 132, loss = 0.07960038\n",
      "Iteration 133, loss = 0.07944520\n",
      "Iteration 134, loss = 0.07928188\n",
      "Iteration 135, loss = 0.07913815\n",
      "Iteration 136, loss = 0.07898765\n",
      "Iteration 137, loss = 0.07884081\n",
      "Iteration 138, loss = 0.07865152\n",
      "Iteration 139, loss = 0.07852765\n",
      "Iteration 140, loss = 0.07836631\n",
      "Iteration 141, loss = 0.07822729\n",
      "Iteration 142, loss = 0.07808410\n",
      "Iteration 143, loss = 0.07792945\n",
      "Iteration 144, loss = 0.07781967\n",
      "Iteration 145, loss = 0.07768605\n",
      "Iteration 146, loss = 0.07754194\n",
      "Iteration 147, loss = 0.07740453\n",
      "Iteration 148, loss = 0.07727812\n",
      "Iteration 149, loss = 0.07715957\n",
      "Iteration 150, loss = 0.07703749\n",
      "Iteration 151, loss = 0.07692379\n",
      "Iteration 152, loss = 0.07681197\n",
      "Iteration 153, loss = 0.07669898\n",
      "Iteration 154, loss = 0.07656429\n",
      "Iteration 155, loss = 0.07646732\n",
      "Iteration 156, loss = 0.07633612\n",
      "Iteration 157, loss = 0.07621155\n",
      "Iteration 158, loss = 0.07610637\n",
      "Iteration 159, loss = 0.07600057\n",
      "Iteration 160, loss = 0.07588195\n",
      "Iteration 161, loss = 0.07577449\n",
      "Iteration 162, loss = 0.07567807\n",
      "Iteration 163, loss = 0.07555388\n",
      "Iteration 164, loss = 0.07545243\n",
      "Iteration 165, loss = 0.07535124\n",
      "Iteration 166, loss = 0.07523270\n",
      "Iteration 167, loss = 0.07514779\n",
      "Iteration 168, loss = 0.07509483\n",
      "Iteration 169, loss = 0.07496908\n",
      "Iteration 170, loss = 0.07485778\n",
      "Iteration 171, loss = 0.07475600\n",
      "Iteration 172, loss = 0.07466901\n",
      "Iteration 173, loss = 0.07456216\n",
      "Iteration 174, loss = 0.07446169\n",
      "Iteration 175, loss = 0.07436876\n",
      "Iteration 176, loss = 0.07428843\n",
      "Iteration 177, loss = 0.07418320\n",
      "Iteration 178, loss = 0.07411325\n",
      "Iteration 179, loss = 0.07401290\n",
      "Iteration 180, loss = 0.07392067\n",
      "Iteration 181, loss = 0.07383878\n",
      "Iteration 182, loss = 0.07375139\n",
      "Iteration 183, loss = 0.07365977\n",
      "Iteration 184, loss = 0.07356887\n",
      "Iteration 185, loss = 0.07349690\n",
      "Iteration 186, loss = 0.07341494\n",
      "Iteration 187, loss = 0.07331813\n",
      "Iteration 188, loss = 0.07323960\n",
      "Iteration 189, loss = 0.07317434\n",
      "Iteration 190, loss = 0.07309754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39014224\n",
      "Iteration 2, loss = 2.24267857\n",
      "Iteration 3, loss = 2.06297698\n",
      "Iteration 4, loss = 1.84395518\n",
      "Iteration 5, loss = 1.60181903\n",
      "Iteration 6, loss = 1.35325789\n",
      "Iteration 7, loss = 1.12864015\n",
      "Iteration 8, loss = 0.94077789\n",
      "Iteration 9, loss = 0.79790548\n",
      "Iteration 10, loss = 0.68816099\n",
      "Iteration 11, loss = 0.60626354\n",
      "Iteration 12, loss = 0.54213631\n",
      "Iteration 13, loss = 0.49242245\n",
      "Iteration 14, loss = 0.45064456\n",
      "Iteration 15, loss = 0.41834901\n",
      "Iteration 16, loss = 0.38714915\n",
      "Iteration 17, loss = 0.36279227\n",
      "Iteration 18, loss = 0.34094826\n",
      "Iteration 19, loss = 0.32184160\n",
      "Iteration 20, loss = 0.30532916\n",
      "Iteration 21, loss = 0.28998383\n",
      "Iteration 22, loss = 0.27705605\n",
      "Iteration 23, loss = 0.26487863\n",
      "Iteration 24, loss = 0.25361027\n",
      "Iteration 25, loss = 0.24379152\n",
      "Iteration 26, loss = 0.23565417\n",
      "Iteration 27, loss = 0.22616469\n",
      "Iteration 28, loss = 0.21940311\n",
      "Iteration 29, loss = 0.21138308\n",
      "Iteration 30, loss = 0.20514611\n",
      "Iteration 31, loss = 0.19865457\n",
      "Iteration 32, loss = 0.19311468\n",
      "Iteration 33, loss = 0.18769198\n",
      "Iteration 34, loss = 0.18211042\n",
      "Iteration 35, loss = 0.17757379\n",
      "Iteration 36, loss = 0.17337414\n",
      "Iteration 37, loss = 0.16920718\n",
      "Iteration 38, loss = 0.16522071\n",
      "Iteration 39, loss = 0.16133150\n",
      "Iteration 40, loss = 0.15806310\n",
      "Iteration 41, loss = 0.15508922\n",
      "Iteration 42, loss = 0.15169884\n",
      "Iteration 43, loss = 0.14863842\n",
      "Iteration 44, loss = 0.14565100\n",
      "Iteration 45, loss = 0.14309685\n",
      "Iteration 46, loss = 0.14080720\n",
      "Iteration 47, loss = 0.13817209\n",
      "Iteration 48, loss = 0.13573130\n",
      "Iteration 49, loss = 0.13351055\n",
      "Iteration 50, loss = 0.13129835\n",
      "Iteration 51, loss = 0.12970679\n",
      "Iteration 52, loss = 0.12766758\n",
      "Iteration 53, loss = 0.12552797\n",
      "Iteration 54, loss = 0.12402781\n",
      "Iteration 55, loss = 0.12232913\n",
      "Iteration 56, loss = 0.12086399\n",
      "Iteration 57, loss = 0.11943255\n",
      "Iteration 58, loss = 0.11790813\n",
      "Iteration 59, loss = 0.11651609\n",
      "Iteration 60, loss = 0.11523526\n",
      "Iteration 61, loss = 0.11396013\n",
      "Iteration 62, loss = 0.11294061\n",
      "Iteration 63, loss = 0.11168377\n",
      "Iteration 64, loss = 0.11049311\n",
      "Iteration 65, loss = 0.10939582\n",
      "Iteration 66, loss = 0.10833949\n",
      "Iteration 67, loss = 0.10743442\n",
      "Iteration 68, loss = 0.10639911\n",
      "Iteration 69, loss = 0.10552774\n",
      "Iteration 70, loss = 0.10461606\n",
      "Iteration 71, loss = 0.10378193\n",
      "Iteration 72, loss = 0.10298006\n",
      "Iteration 73, loss = 0.10220409\n",
      "Iteration 74, loss = 0.10139480\n",
      "Iteration 75, loss = 0.10083838\n",
      "Iteration 76, loss = 0.09996732\n",
      "Iteration 77, loss = 0.09933321\n",
      "Iteration 78, loss = 0.09866951\n",
      "Iteration 79, loss = 0.09787847\n",
      "Iteration 80, loss = 0.09726803\n",
      "Iteration 81, loss = 0.09660129\n",
      "Iteration 82, loss = 0.09597219\n",
      "Iteration 83, loss = 0.09547648\n",
      "Iteration 84, loss = 0.09492140\n",
      "Iteration 85, loss = 0.09443018\n",
      "Iteration 86, loss = 0.09396271\n",
      "Iteration 87, loss = 0.09341462\n",
      "Iteration 88, loss = 0.09296190\n",
      "Iteration 89, loss = 0.09248408\n",
      "Iteration 90, loss = 0.09214062\n",
      "Iteration 91, loss = 0.09158990\n",
      "Iteration 92, loss = 0.09114894\n",
      "Iteration 93, loss = 0.09079629\n",
      "Iteration 94, loss = 0.09039442\n",
      "Iteration 95, loss = 0.09000580\n",
      "Iteration 96, loss = 0.08966298\n",
      "Iteration 97, loss = 0.08927942\n",
      "Iteration 98, loss = 0.08892946\n",
      "Iteration 99, loss = 0.08852470\n",
      "Iteration 100, loss = 0.08815576\n",
      "Iteration 101, loss = 0.08787404\n",
      "Iteration 102, loss = 0.08753822\n",
      "Iteration 103, loss = 0.08725734\n",
      "Iteration 104, loss = 0.08692190\n",
      "Iteration 105, loss = 0.08661813\n",
      "Iteration 106, loss = 0.08636854\n",
      "Iteration 107, loss = 0.08608846\n",
      "Iteration 108, loss = 0.08576964\n",
      "Iteration 109, loss = 0.08550636\n",
      "Iteration 110, loss = 0.08524455\n",
      "Iteration 111, loss = 0.08500844\n",
      "Iteration 112, loss = 0.08477433\n",
      "Iteration 113, loss = 0.08452655\n",
      "Iteration 114, loss = 0.08425350\n",
      "Iteration 115, loss = 0.08401774\n",
      "Iteration 116, loss = 0.08378506\n",
      "Iteration 117, loss = 0.08355140\n",
      "Iteration 118, loss = 0.08331961\n",
      "Iteration 119, loss = 0.08311286\n",
      "Iteration 120, loss = 0.08292827\n",
      "Iteration 121, loss = 0.08269677\n",
      "Iteration 122, loss = 0.08246007\n",
      "Iteration 123, loss = 0.08228230\n",
      "Iteration 124, loss = 0.08207801\n",
      "Iteration 125, loss = 0.08188522\n",
      "Iteration 126, loss = 0.08170416\n",
      "Iteration 127, loss = 0.08150806\n",
      "Iteration 128, loss = 0.08134335\n",
      "Iteration 129, loss = 0.08116222\n",
      "Iteration 130, loss = 0.08096030\n",
      "Iteration 131, loss = 0.08079626\n",
      "Iteration 132, loss = 0.08063255\n",
      "Iteration 133, loss = 0.08045425\n",
      "Iteration 134, loss = 0.08030701\n",
      "Iteration 135, loss = 0.08014732\n",
      "Iteration 136, loss = 0.08000531\n",
      "Iteration 137, loss = 0.07984274\n",
      "Iteration 138, loss = 0.07966288\n",
      "Iteration 139, loss = 0.07954383\n",
      "Iteration 140, loss = 0.07937586\n",
      "Iteration 141, loss = 0.07922586\n",
      "Iteration 142, loss = 0.07908788\n",
      "Iteration 143, loss = 0.07892032\n",
      "Iteration 144, loss = 0.07881532\n",
      "Iteration 145, loss = 0.07867452\n",
      "Iteration 146, loss = 0.07854872\n",
      "Iteration 147, loss = 0.07839783\n",
      "Iteration 148, loss = 0.07826299\n",
      "Iteration 149, loss = 0.07814148\n",
      "Iteration 150, loss = 0.07800247\n",
      "Iteration 151, loss = 0.07788284\n",
      "Iteration 152, loss = 0.07778318\n",
      "Iteration 153, loss = 0.07766409\n",
      "Iteration 154, loss = 0.07751049\n",
      "Iteration 155, loss = 0.07742211\n",
      "Iteration 156, loss = 0.07729094\n",
      "Iteration 157, loss = 0.07716021\n",
      "Iteration 158, loss = 0.07705539\n",
      "Iteration 159, loss = 0.07694133\n",
      "Iteration 160, loss = 0.07683139\n",
      "Iteration 161, loss = 0.07671484\n",
      "Iteration 162, loss = 0.07661964\n",
      "Iteration 163, loss = 0.07648177\n",
      "Iteration 164, loss = 0.07638193\n",
      "Iteration 165, loss = 0.07627218\n",
      "Iteration 166, loss = 0.07615693\n",
      "Iteration 167, loss = 0.07606003\n",
      "Iteration 168, loss = 0.07600742\n",
      "Iteration 169, loss = 0.07587990\n",
      "Iteration 170, loss = 0.07577999\n",
      "Iteration 171, loss = 0.07567062\n",
      "Iteration 172, loss = 0.07556472\n",
      "Iteration 173, loss = 0.07545196\n",
      "Iteration 174, loss = 0.07535220\n",
      "Iteration 175, loss = 0.07526837\n",
      "Iteration 176, loss = 0.07518626\n",
      "Iteration 177, loss = 0.07507206\n",
      "Iteration 178, loss = 0.07501339\n",
      "Iteration 179, loss = 0.07490535\n",
      "Iteration 180, loss = 0.07481367\n",
      "Iteration 181, loss = 0.07472991\n",
      "Iteration 182, loss = 0.07462953\n",
      "Iteration 183, loss = 0.07453792\n",
      "Iteration 184, loss = 0.07445229\n",
      "Iteration 185, loss = 0.07436589\n",
      "Iteration 186, loss = 0.07427736\n",
      "Iteration 187, loss = 0.07419025\n",
      "Iteration 188, loss = 0.07411259\n",
      "Iteration 189, loss = 0.07405684\n",
      "Iteration 190, loss = 0.07396197\n",
      "Iteration 191, loss = 0.07388788\n",
      "Iteration 192, loss = 0.07380043\n",
      "Iteration 193, loss = 0.07371742\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38901243\n",
      "Iteration 2, loss = 2.23971020\n",
      "Iteration 3, loss = 2.05679789\n",
      "Iteration 4, loss = 1.82901627\n",
      "Iteration 5, loss = 1.57312350\n",
      "Iteration 6, loss = 1.31734635\n",
      "Iteration 7, loss = 1.08785577\n",
      "Iteration 8, loss = 0.89286590\n",
      "Iteration 9, loss = 0.74821338\n",
      "Iteration 10, loss = 0.63837852\n",
      "Iteration 11, loss = 0.55593408\n",
      "Iteration 12, loss = 0.49340145\n",
      "Iteration 13, loss = 0.44480418\n",
      "Iteration 14, loss = 0.40410965\n",
      "Iteration 15, loss = 0.37344713\n",
      "Iteration 16, loss = 0.34556456\n",
      "Iteration 17, loss = 0.32193791\n",
      "Iteration 18, loss = 0.30280839\n",
      "Iteration 19, loss = 0.28561811\n",
      "Iteration 20, loss = 0.27047279\n",
      "Iteration 21, loss = 0.25722937\n",
      "Iteration 22, loss = 0.24417148\n",
      "Iteration 23, loss = 0.23373661\n",
      "Iteration 24, loss = 0.22366690\n",
      "Iteration 25, loss = 0.21519140\n",
      "Iteration 26, loss = 0.20711980\n",
      "Iteration 27, loss = 0.19983567\n",
      "Iteration 28, loss = 0.19304564\n",
      "Iteration 29, loss = 0.18694889\n",
      "Iteration 30, loss = 0.18082915\n",
      "Iteration 31, loss = 0.17524574\n",
      "Iteration 32, loss = 0.17037929\n",
      "Iteration 33, loss = 0.16558730\n",
      "Iteration 34, loss = 0.16100437\n",
      "Iteration 35, loss = 0.15710327\n",
      "Iteration 36, loss = 0.15356262\n",
      "Iteration 37, loss = 0.15007059\n",
      "Iteration 38, loss = 0.14671720\n",
      "Iteration 39, loss = 0.14336820\n",
      "Iteration 40, loss = 0.14087621\n",
      "Iteration 41, loss = 0.13776700\n",
      "Iteration 42, loss = 0.13485494\n",
      "Iteration 43, loss = 0.13233062\n",
      "Iteration 44, loss = 0.13001579\n",
      "Iteration 45, loss = 0.12788673\n",
      "Iteration 46, loss = 0.12576139\n",
      "Iteration 47, loss = 0.12424407\n",
      "Iteration 48, loss = 0.12217791\n",
      "Iteration 49, loss = 0.12020617\n",
      "Iteration 50, loss = 0.11838151\n",
      "Iteration 51, loss = 0.11686674\n",
      "Iteration 52, loss = 0.11547128\n",
      "Iteration 53, loss = 0.11376375\n",
      "Iteration 54, loss = 0.11255994\n",
      "Iteration 55, loss = 0.11115912\n",
      "Iteration 56, loss = 0.10994624\n",
      "Iteration 57, loss = 0.10870327\n",
      "Iteration 58, loss = 0.10763401\n",
      "Iteration 59, loss = 0.10650510\n",
      "Iteration 60, loss = 0.10533043\n",
      "Iteration 61, loss = 0.10438612\n",
      "Iteration 62, loss = 0.10349711\n",
      "Iteration 63, loss = 0.10251165\n",
      "Iteration 64, loss = 0.10170344\n",
      "Iteration 65, loss = 0.10091553\n",
      "Iteration 66, loss = 0.10006315\n",
      "Iteration 67, loss = 0.09926700\n",
      "Iteration 68, loss = 0.09853497\n",
      "Iteration 69, loss = 0.09779589\n",
      "Iteration 70, loss = 0.09701534\n",
      "Iteration 71, loss = 0.09644889\n",
      "Iteration 72, loss = 0.09575698\n",
      "Iteration 73, loss = 0.09507394\n",
      "Iteration 74, loss = 0.09444284\n",
      "Iteration 75, loss = 0.09409056\n",
      "Iteration 76, loss = 0.09340599\n",
      "Iteration 77, loss = 0.09271822\n",
      "Iteration 78, loss = 0.09221295\n",
      "Iteration 79, loss = 0.09160185\n",
      "Iteration 80, loss = 0.09105202\n",
      "Iteration 81, loss = 0.09056329\n",
      "Iteration 82, loss = 0.09010733\n",
      "Iteration 83, loss = 0.08966793\n",
      "Iteration 84, loss = 0.08920342\n",
      "Iteration 85, loss = 0.08882527\n",
      "Iteration 86, loss = 0.08840154\n",
      "Iteration 87, loss = 0.08801180\n",
      "Iteration 88, loss = 0.08766268\n",
      "Iteration 89, loss = 0.08728447\n",
      "Iteration 90, loss = 0.08694504\n",
      "Iteration 91, loss = 0.08656235\n",
      "Iteration 92, loss = 0.08624219\n",
      "Iteration 93, loss = 0.08592729\n",
      "Iteration 94, loss = 0.08560420\n",
      "Iteration 95, loss = 0.08530303\n",
      "Iteration 96, loss = 0.08497011\n",
      "Iteration 97, loss = 0.08462178\n",
      "Iteration 98, loss = 0.08435632\n",
      "Iteration 99, loss = 0.08405995\n",
      "Iteration 100, loss = 0.08380346\n",
      "Iteration 101, loss = 0.08352083\n",
      "Iteration 102, loss = 0.08325558\n",
      "Iteration 103, loss = 0.08302213\n",
      "Iteration 104, loss = 0.08276382\n",
      "Iteration 105, loss = 0.08251954\n",
      "Iteration 106, loss = 0.08224595\n",
      "Iteration 107, loss = 0.08203382\n",
      "Iteration 108, loss = 0.08178871\n",
      "Iteration 109, loss = 0.08157549\n",
      "Iteration 110, loss = 0.08133481\n",
      "Iteration 111, loss = 0.08114986\n",
      "Iteration 112, loss = 0.08095326\n",
      "Iteration 113, loss = 0.08075149\n",
      "Iteration 114, loss = 0.08051393\n",
      "Iteration 115, loss = 0.08031929\n",
      "Iteration 116, loss = 0.08012899\n",
      "Iteration 117, loss = 0.07991557\n",
      "Iteration 118, loss = 0.07973493\n",
      "Iteration 119, loss = 0.07955370\n",
      "Iteration 120, loss = 0.07939332\n",
      "Iteration 121, loss = 0.07923891\n",
      "Iteration 122, loss = 0.07904388\n",
      "Iteration 123, loss = 0.07884040\n",
      "Iteration 124, loss = 0.07866694\n",
      "Iteration 125, loss = 0.07853796\n",
      "Iteration 126, loss = 0.07840036\n",
      "Iteration 127, loss = 0.07820022\n",
      "Iteration 128, loss = 0.07808712\n",
      "Iteration 129, loss = 0.07793373\n",
      "Iteration 130, loss = 0.07773187\n",
      "Iteration 131, loss = 0.07757623\n",
      "Iteration 132, loss = 0.07743364\n",
      "Iteration 133, loss = 0.07727853\n",
      "Iteration 134, loss = 0.07715674\n",
      "Iteration 135, loss = 0.07700285\n",
      "Iteration 136, loss = 0.07685594\n",
      "Iteration 137, loss = 0.07673522\n",
      "Iteration 138, loss = 0.07659625\n",
      "Iteration 139, loss = 0.07647875\n",
      "Iteration 140, loss = 0.07633990\n",
      "Iteration 141, loss = 0.07621122\n",
      "Iteration 142, loss = 0.07609403\n",
      "Iteration 143, loss = 0.07597023\n",
      "Iteration 144, loss = 0.07585790\n",
      "Iteration 145, loss = 0.07572051\n",
      "Iteration 146, loss = 0.07563358\n",
      "Iteration 147, loss = 0.07548061\n",
      "Iteration 148, loss = 0.07536456\n",
      "Iteration 149, loss = 0.07524957\n",
      "Iteration 150, loss = 0.07514452\n",
      "Iteration 151, loss = 0.07502660\n",
      "Iteration 152, loss = 0.07493287\n",
      "Iteration 153, loss = 0.07481910\n",
      "Iteration 154, loss = 0.07470168\n",
      "Iteration 155, loss = 0.07460857\n",
      "Iteration 156, loss = 0.07452079\n",
      "Iteration 157, loss = 0.07440450\n",
      "Iteration 158, loss = 0.07429171\n",
      "Iteration 159, loss = 0.07419388\n",
      "Iteration 160, loss = 0.07408934\n",
      "Iteration 161, loss = 0.07400212\n",
      "Iteration 162, loss = 0.07391698\n",
      "Iteration 163, loss = 0.07379907\n",
      "Iteration 164, loss = 0.07371044\n",
      "Iteration 165, loss = 0.07363019\n",
      "Iteration 166, loss = 0.07352226\n",
      "Iteration 167, loss = 0.07343428\n",
      "Iteration 168, loss = 0.07336643\n",
      "Iteration 169, loss = 0.07326617\n",
      "Iteration 170, loss = 0.07316763\n",
      "Iteration 171, loss = 0.07307156\n",
      "Iteration 172, loss = 0.07298892\n",
      "Iteration 173, loss = 0.07289585\n",
      "Iteration 174, loss = 0.07280382\n",
      "Iteration 175, loss = 0.07272391\n",
      "Iteration 176, loss = 0.07264556\n",
      "Iteration 177, loss = 0.07255118\n",
      "Iteration 178, loss = 0.07247822\n",
      "Iteration 179, loss = 0.07239468\n",
      "Iteration 180, loss = 0.07231900\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29329998\n",
      "Iteration 2, loss = 1.92006193\n",
      "Iteration 3, loss = 1.53768263\n",
      "Iteration 4, loss = 1.15611834\n",
      "Iteration 5, loss = 0.84662238\n",
      "Iteration 6, loss = 0.62685252\n",
      "Iteration 7, loss = 0.48104123\n",
      "Iteration 8, loss = 0.38396019\n",
      "Iteration 9, loss = 0.31742087\n",
      "Iteration 10, loss = 0.27045819\n",
      "Iteration 11, loss = 0.23335906\n",
      "Iteration 12, loss = 0.20811568\n",
      "Iteration 13, loss = 0.18643354\n",
      "Iteration 14, loss = 0.16984643\n",
      "Iteration 15, loss = 0.15612406\n",
      "Iteration 16, loss = 0.14552892\n",
      "Iteration 17, loss = 0.13541634\n",
      "Iteration 18, loss = 0.12786809\n",
      "Iteration 19, loss = 0.12136592\n",
      "Iteration 20, loss = 0.11546829\n",
      "Iteration 21, loss = 0.11090343\n",
      "Iteration 22, loss = 0.10644653\n",
      "Iteration 23, loss = 0.10296547\n",
      "Iteration 24, loss = 0.09941413\n",
      "Iteration 25, loss = 0.09661305\n",
      "Iteration 26, loss = 0.09379809\n",
      "Iteration 27, loss = 0.09157976\n",
      "Iteration 28, loss = 0.08937685\n",
      "Iteration 29, loss = 0.08749071\n",
      "Iteration 30, loss = 0.08568506\n",
      "Iteration 31, loss = 0.08416431\n",
      "Iteration 32, loss = 0.08266720\n",
      "Iteration 33, loss = 0.08122427\n",
      "Iteration 34, loss = 0.07982721\n",
      "Iteration 35, loss = 0.07860156\n",
      "Iteration 36, loss = 0.07743685\n",
      "Iteration 37, loss = 0.07637338\n",
      "Iteration 38, loss = 0.07532185\n",
      "Iteration 39, loss = 0.07434590\n",
      "Iteration 40, loss = 0.07344025\n",
      "Iteration 41, loss = 0.07254019\n",
      "Iteration 42, loss = 0.07171479\n",
      "Iteration 43, loss = 0.07097016\n",
      "Iteration 44, loss = 0.07015168\n",
      "Iteration 45, loss = 0.06940119\n",
      "Iteration 46, loss = 0.06869910\n",
      "Iteration 47, loss = 0.06799964\n",
      "Iteration 48, loss = 0.06733342\n",
      "Iteration 49, loss = 0.06668806\n",
      "Iteration 50, loss = 0.06607171\n",
      "Iteration 51, loss = 0.06546645\n",
      "Iteration 52, loss = 0.06487831\n",
      "Iteration 53, loss = 0.06429641\n",
      "Iteration 54, loss = 0.06371504\n",
      "Iteration 55, loss = 0.06315857\n",
      "Iteration 56, loss = 0.06263104\n",
      "Iteration 57, loss = 0.06209608\n",
      "Iteration 58, loss = 0.06157479\n",
      "Iteration 59, loss = 0.06109309\n",
      "Iteration 60, loss = 0.06062248\n",
      "Iteration 61, loss = 0.06012096\n",
      "Iteration 62, loss = 0.05965576\n",
      "Iteration 63, loss = 0.05919992\n",
      "Iteration 64, loss = 0.05873850\n",
      "Iteration 65, loss = 0.05832008\n",
      "Iteration 66, loss = 0.05789189\n",
      "Iteration 67, loss = 0.05750422\n",
      "Iteration 68, loss = 0.05706831\n",
      "Iteration 69, loss = 0.05670789\n",
      "Iteration 70, loss = 0.05635530\n",
      "Iteration 71, loss = 0.05592255\n",
      "Iteration 72, loss = 0.05552698\n",
      "Iteration 73, loss = 0.05516658\n",
      "Iteration 74, loss = 0.05482664\n",
      "Iteration 75, loss = 0.05445725\n",
      "Iteration 76, loss = 0.05416375\n",
      "Iteration 77, loss = 0.05382581\n",
      "Iteration 78, loss = 0.05349025\n",
      "Iteration 79, loss = 0.05314541\n",
      "Iteration 80, loss = 0.05284272\n",
      "Iteration 81, loss = 0.05252296\n",
      "Iteration 82, loss = 0.05217167\n",
      "Iteration 83, loss = 0.05191008\n",
      "Iteration 84, loss = 0.05160320\n",
      "Iteration 85, loss = 0.05128166\n",
      "Iteration 86, loss = 0.05106997\n",
      "Iteration 87, loss = 0.05078877\n",
      "Iteration 88, loss = 0.05045611\n",
      "Iteration 89, loss = 0.05020540\n",
      "Iteration 90, loss = 0.04994482\n",
      "Iteration 91, loss = 0.04971636\n",
      "Iteration 92, loss = 0.04943470\n",
      "Iteration 93, loss = 0.04921994\n",
      "Iteration 94, loss = 0.04895566\n",
      "Iteration 95, loss = 0.04871419\n",
      "Iteration 96, loss = 0.04849913\n",
      "Iteration 97, loss = 0.04826232\n",
      "Iteration 98, loss = 0.04802834\n",
      "Iteration 99, loss = 0.04783520\n",
      "Iteration 100, loss = 0.04760525\n",
      "Iteration 101, loss = 0.04742132\n",
      "Iteration 102, loss = 0.04720263\n",
      "Iteration 103, loss = 0.04700769\n",
      "Iteration 104, loss = 0.04681697\n",
      "Iteration 105, loss = 0.04661843\n",
      "Iteration 106, loss = 0.04642989\n",
      "Iteration 107, loss = 0.04624167\n",
      "Iteration 108, loss = 0.04604994\n",
      "Iteration 109, loss = 0.04593491\n",
      "Iteration 110, loss = 0.04576403\n",
      "Iteration 111, loss = 0.04557714\n",
      "Iteration 112, loss = 0.04537188\n",
      "Iteration 113, loss = 0.04523179\n",
      "Iteration 114, loss = 0.04510375\n",
      "Iteration 115, loss = 0.04496681\n",
      "Iteration 116, loss = 0.04480192\n",
      "Iteration 117, loss = 0.04460694\n",
      "Iteration 118, loss = 0.04446301\n",
      "Iteration 119, loss = 0.04431199\n",
      "Iteration 120, loss = 0.04420199\n",
      "Iteration 121, loss = 0.04405485\n",
      "Iteration 122, loss = 0.04388100\n",
      "Iteration 123, loss = 0.04377687\n",
      "Iteration 124, loss = 0.04365276\n",
      "Iteration 125, loss = 0.04351602\n",
      "Iteration 126, loss = 0.04335480\n",
      "Iteration 127, loss = 0.04325535\n",
      "Iteration 128, loss = 0.04311848\n",
      "Iteration 129, loss = 0.04299626\n",
      "Iteration 130, loss = 0.04288832\n",
      "Iteration 131, loss = 0.04276629\n",
      "Iteration 132, loss = 0.04267075\n",
      "Iteration 133, loss = 0.04253493\n",
      "Iteration 134, loss = 0.04244594\n",
      "Iteration 135, loss = 0.04242580\n",
      "Iteration 136, loss = 0.04223507\n",
      "Iteration 137, loss = 0.04217676\n",
      "Iteration 138, loss = 0.04206797\n",
      "Iteration 139, loss = 0.04197218\n",
      "Iteration 140, loss = 0.04191852\n",
      "Iteration 141, loss = 0.04177930\n",
      "Iteration 142, loss = 0.04173746\n",
      "Iteration 143, loss = 0.04157242\n",
      "Iteration 144, loss = 0.04147244\n",
      "Iteration 145, loss = 0.04142473\n",
      "Iteration 146, loss = 0.04134456\n",
      "Iteration 147, loss = 0.04125014\n",
      "Iteration 148, loss = 0.04115450\n",
      "Iteration 149, loss = 0.04109834\n",
      "Iteration 150, loss = 0.04103121\n",
      "Iteration 151, loss = 0.04096603\n",
      "Iteration 152, loss = 0.04086706\n",
      "Iteration 153, loss = 0.04080051\n",
      "Iteration 154, loss = 0.04073330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29062746\n",
      "Iteration 2, loss = 1.90839075\n",
      "Iteration 3, loss = 1.51306769\n",
      "Iteration 4, loss = 1.13012914\n",
      "Iteration 5, loss = 0.82469007\n",
      "Iteration 6, loss = 0.61102687\n",
      "Iteration 7, loss = 0.47149531\n",
      "Iteration 8, loss = 0.37904552\n",
      "Iteration 9, loss = 0.31394430\n",
      "Iteration 10, loss = 0.26655150\n",
      "Iteration 11, loss = 0.23097480\n",
      "Iteration 12, loss = 0.20638103\n",
      "Iteration 13, loss = 0.18432918\n",
      "Iteration 14, loss = 0.16789804\n",
      "Iteration 15, loss = 0.15394273\n",
      "Iteration 16, loss = 0.14371356\n",
      "Iteration 17, loss = 0.13349661\n",
      "Iteration 18, loss = 0.12586951\n",
      "Iteration 19, loss = 0.11938900\n",
      "Iteration 20, loss = 0.11355917\n",
      "Iteration 21, loss = 0.10887465\n",
      "Iteration 22, loss = 0.10471929\n",
      "Iteration 23, loss = 0.10101656\n",
      "Iteration 24, loss = 0.09783047\n",
      "Iteration 25, loss = 0.09504708\n",
      "Iteration 26, loss = 0.09242609\n",
      "Iteration 27, loss = 0.09007533\n",
      "Iteration 28, loss = 0.08803602\n",
      "Iteration 29, loss = 0.08617716\n",
      "Iteration 30, loss = 0.08444721\n",
      "Iteration 31, loss = 0.08301322\n",
      "Iteration 32, loss = 0.08157385\n",
      "Iteration 33, loss = 0.08008927\n",
      "Iteration 34, loss = 0.07881146\n",
      "Iteration 35, loss = 0.07764687\n",
      "Iteration 36, loss = 0.07655435\n",
      "Iteration 37, loss = 0.07551636\n",
      "Iteration 38, loss = 0.07448170\n",
      "Iteration 39, loss = 0.07354592\n",
      "Iteration 40, loss = 0.07268582\n",
      "Iteration 41, loss = 0.07181443\n",
      "Iteration 42, loss = 0.07097812\n",
      "Iteration 43, loss = 0.07018380\n",
      "Iteration 44, loss = 0.06943161\n",
      "Iteration 45, loss = 0.06865517\n",
      "Iteration 46, loss = 0.06797209\n",
      "Iteration 47, loss = 0.06731140\n",
      "Iteration 48, loss = 0.06664059\n",
      "Iteration 49, loss = 0.06599964\n",
      "Iteration 50, loss = 0.06539436\n",
      "Iteration 51, loss = 0.06481034\n",
      "Iteration 52, loss = 0.06420650\n",
      "Iteration 53, loss = 0.06366355\n",
      "Iteration 54, loss = 0.06307739\n",
      "Iteration 55, loss = 0.06255593\n",
      "Iteration 56, loss = 0.06201144\n",
      "Iteration 57, loss = 0.06148575\n",
      "Iteration 58, loss = 0.06097507\n",
      "Iteration 59, loss = 0.06047498\n",
      "Iteration 60, loss = 0.06002328\n",
      "Iteration 61, loss = 0.05951702\n",
      "Iteration 62, loss = 0.05906484\n",
      "Iteration 63, loss = 0.05864242\n",
      "Iteration 64, loss = 0.05817860\n",
      "Iteration 65, loss = 0.05774925\n",
      "Iteration 66, loss = 0.05734974\n",
      "Iteration 67, loss = 0.05693351\n",
      "Iteration 68, loss = 0.05650691\n",
      "Iteration 69, loss = 0.05614512\n",
      "Iteration 70, loss = 0.05576040\n",
      "Iteration 71, loss = 0.05534966\n",
      "Iteration 72, loss = 0.05499296\n",
      "Iteration 73, loss = 0.05463948\n",
      "Iteration 74, loss = 0.05430238\n",
      "Iteration 75, loss = 0.05390936\n",
      "Iteration 76, loss = 0.05368179\n",
      "Iteration 77, loss = 0.05326878\n",
      "Iteration 78, loss = 0.05291146\n",
      "Iteration 79, loss = 0.05262193\n",
      "Iteration 80, loss = 0.05229817\n",
      "Iteration 81, loss = 0.05199829\n",
      "Iteration 82, loss = 0.05165731\n",
      "Iteration 83, loss = 0.05139803\n",
      "Iteration 84, loss = 0.05109528\n",
      "Iteration 85, loss = 0.05078552\n",
      "Iteration 86, loss = 0.05052491\n",
      "Iteration 87, loss = 0.05026237\n",
      "Iteration 88, loss = 0.04994820\n",
      "Iteration 89, loss = 0.04970813\n",
      "Iteration 90, loss = 0.04944909\n",
      "Iteration 91, loss = 0.04920306\n",
      "Iteration 92, loss = 0.04898020\n",
      "Iteration 93, loss = 0.04870925\n",
      "Iteration 94, loss = 0.04846740\n",
      "Iteration 95, loss = 0.04822948\n",
      "Iteration 96, loss = 0.04799270\n",
      "Iteration 97, loss = 0.04776215\n",
      "Iteration 98, loss = 0.04754382\n",
      "Iteration 99, loss = 0.04735536\n",
      "Iteration 100, loss = 0.04710915\n",
      "Iteration 101, loss = 0.04693120\n",
      "Iteration 102, loss = 0.04672708\n",
      "Iteration 103, loss = 0.04651071\n",
      "Iteration 104, loss = 0.04632560\n",
      "Iteration 105, loss = 0.04615708\n",
      "Iteration 106, loss = 0.04594973\n",
      "Iteration 107, loss = 0.04576743\n",
      "Iteration 108, loss = 0.04557560\n",
      "Iteration 109, loss = 0.04542768\n",
      "Iteration 110, loss = 0.04524737\n",
      "Iteration 111, loss = 0.04510168\n",
      "Iteration 112, loss = 0.04491699\n",
      "Iteration 113, loss = 0.04477554\n",
      "Iteration 114, loss = 0.04464696\n",
      "Iteration 115, loss = 0.04449489\n",
      "Iteration 116, loss = 0.04431322\n",
      "Iteration 117, loss = 0.04414800\n",
      "Iteration 118, loss = 0.04402462\n",
      "Iteration 119, loss = 0.04385214\n",
      "Iteration 120, loss = 0.04370301\n",
      "Iteration 121, loss = 0.04357468\n",
      "Iteration 122, loss = 0.04342184\n",
      "Iteration 123, loss = 0.04329650\n",
      "Iteration 124, loss = 0.04315822\n",
      "Iteration 125, loss = 0.04304677\n",
      "Iteration 126, loss = 0.04290987\n",
      "Iteration 127, loss = 0.04279482\n",
      "Iteration 128, loss = 0.04268109\n",
      "Iteration 129, loss = 0.04255147\n",
      "Iteration 130, loss = 0.04243204\n",
      "Iteration 131, loss = 0.04231623\n",
      "Iteration 132, loss = 0.04226380\n",
      "Iteration 133, loss = 0.04212303\n",
      "Iteration 134, loss = 0.04201703\n",
      "Iteration 135, loss = 0.04197981\n",
      "Iteration 136, loss = 0.04182193\n",
      "Iteration 137, loss = 0.04172338\n",
      "Iteration 138, loss = 0.04161314\n",
      "Iteration 139, loss = 0.04152533\n",
      "Iteration 140, loss = 0.04143314\n",
      "Iteration 141, loss = 0.04132281\n",
      "Iteration 142, loss = 0.04126205\n",
      "Iteration 143, loss = 0.04114719\n",
      "Iteration 144, loss = 0.04102042\n",
      "Iteration 145, loss = 0.04099034\n",
      "Iteration 146, loss = 0.04090123\n",
      "Iteration 147, loss = 0.04081710\n",
      "Iteration 148, loss = 0.04072844\n",
      "Iteration 149, loss = 0.04063514\n",
      "Iteration 150, loss = 0.04058938\n",
      "Iteration 151, loss = 0.04051404\n",
      "Iteration 152, loss = 0.04043410\n",
      "Iteration 153, loss = 0.04033109\n",
      "Iteration 154, loss = 0.04028005\n",
      "Iteration 155, loss = 0.04019345\n",
      "Iteration 156, loss = 0.04010863\n",
      "Iteration 157, loss = 0.04008873\n",
      "Iteration 158, loss = 0.04000851\n",
      "Iteration 159, loss = 0.03997703\n",
      "Iteration 160, loss = 0.03987291\n",
      "Iteration 161, loss = 0.03988736\n",
      "Iteration 162, loss = 0.03977661\n",
      "Iteration 163, loss = 0.03971138\n",
      "Iteration 164, loss = 0.03963265\n",
      "Iteration 165, loss = 0.03958396\n",
      "Iteration 166, loss = 0.03953657\n",
      "Iteration 167, loss = 0.03949670\n",
      "Iteration 168, loss = 0.03941929\n",
      "Iteration 169, loss = 0.03937359\n",
      "Iteration 170, loss = 0.03933517\n",
      "Iteration 171, loss = 0.03924916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29621805\n",
      "Iteration 2, loss = 1.92692367\n",
      "Iteration 3, loss = 1.54313024\n",
      "Iteration 4, loss = 1.15991532\n",
      "Iteration 5, loss = 0.85193658\n",
      "Iteration 6, loss = 0.63440477\n",
      "Iteration 7, loss = 0.49132665\n",
      "Iteration 8, loss = 0.39326852\n",
      "Iteration 9, loss = 0.32623423\n",
      "Iteration 10, loss = 0.27671726\n",
      "Iteration 11, loss = 0.24100809\n",
      "Iteration 12, loss = 0.21285938\n",
      "Iteration 13, loss = 0.19054533\n",
      "Iteration 14, loss = 0.17333144\n",
      "Iteration 15, loss = 0.15989690\n",
      "Iteration 16, loss = 0.14766621\n",
      "Iteration 17, loss = 0.13812269\n",
      "Iteration 18, loss = 0.12957146\n",
      "Iteration 19, loss = 0.12287275\n",
      "Iteration 20, loss = 0.11702907\n",
      "Iteration 21, loss = 0.11190964\n",
      "Iteration 22, loss = 0.10759132\n",
      "Iteration 23, loss = 0.10383330\n",
      "Iteration 24, loss = 0.10039185\n",
      "Iteration 25, loss = 0.09747175\n",
      "Iteration 26, loss = 0.09497657\n",
      "Iteration 27, loss = 0.09251329\n",
      "Iteration 28, loss = 0.09040461\n",
      "Iteration 29, loss = 0.08847321\n",
      "Iteration 30, loss = 0.08665025\n",
      "Iteration 31, loss = 0.08498220\n",
      "Iteration 32, loss = 0.08347247\n",
      "Iteration 33, loss = 0.08203109\n",
      "Iteration 34, loss = 0.08068710\n",
      "Iteration 35, loss = 0.07946480\n",
      "Iteration 36, loss = 0.07838695\n",
      "Iteration 37, loss = 0.07725223\n",
      "Iteration 38, loss = 0.07620168\n",
      "Iteration 39, loss = 0.07521899\n",
      "Iteration 40, loss = 0.07431076\n",
      "Iteration 41, loss = 0.07344354\n",
      "Iteration 42, loss = 0.07256390\n",
      "Iteration 43, loss = 0.07172141\n",
      "Iteration 44, loss = 0.07093290\n",
      "Iteration 45, loss = 0.07021199\n",
      "Iteration 46, loss = 0.06952368\n",
      "Iteration 47, loss = 0.06875073\n",
      "Iteration 48, loss = 0.06809473\n",
      "Iteration 49, loss = 0.06745552\n",
      "Iteration 50, loss = 0.06677853\n",
      "Iteration 51, loss = 0.06616914\n",
      "Iteration 52, loss = 0.06558166\n",
      "Iteration 53, loss = 0.06495149\n",
      "Iteration 54, loss = 0.06441487\n",
      "Iteration 55, loss = 0.06385007\n",
      "Iteration 56, loss = 0.06330925\n",
      "Iteration 57, loss = 0.06279930\n",
      "Iteration 58, loss = 0.06229196\n",
      "Iteration 59, loss = 0.06177602\n",
      "Iteration 60, loss = 0.06129524\n",
      "Iteration 61, loss = 0.06080061\n",
      "Iteration 62, loss = 0.06037430\n",
      "Iteration 63, loss = 0.05994214\n",
      "Iteration 64, loss = 0.05945695\n",
      "Iteration 65, loss = 0.05898270\n",
      "Iteration 66, loss = 0.05855316\n",
      "Iteration 67, loss = 0.05814895\n",
      "Iteration 68, loss = 0.05772560\n",
      "Iteration 69, loss = 0.05732577\n",
      "Iteration 70, loss = 0.05693863\n",
      "Iteration 71, loss = 0.05653974\n",
      "Iteration 72, loss = 0.05618271\n",
      "Iteration 73, loss = 0.05577667\n",
      "Iteration 74, loss = 0.05543041\n",
      "Iteration 75, loss = 0.05509485\n",
      "Iteration 76, loss = 0.05473151\n",
      "Iteration 77, loss = 0.05442645\n",
      "Iteration 78, loss = 0.05408987\n",
      "Iteration 79, loss = 0.05373830\n",
      "Iteration 80, loss = 0.05345428\n",
      "Iteration 81, loss = 0.05310926\n",
      "Iteration 82, loss = 0.05279070\n",
      "Iteration 83, loss = 0.05248093\n",
      "Iteration 84, loss = 0.05218206\n",
      "Iteration 85, loss = 0.05188838\n",
      "Iteration 86, loss = 0.05163765\n",
      "Iteration 87, loss = 0.05133666\n",
      "Iteration 88, loss = 0.05104961\n",
      "Iteration 89, loss = 0.05079856\n",
      "Iteration 90, loss = 0.05056289\n",
      "Iteration 91, loss = 0.05028376\n",
      "Iteration 92, loss = 0.05001441\n",
      "Iteration 93, loss = 0.04979048\n",
      "Iteration 94, loss = 0.04953381\n",
      "Iteration 95, loss = 0.04929598\n",
      "Iteration 96, loss = 0.04907866\n",
      "Iteration 97, loss = 0.04883812\n",
      "Iteration 98, loss = 0.04861735\n",
      "Iteration 99, loss = 0.04838851\n",
      "Iteration 100, loss = 0.04819299\n",
      "Iteration 101, loss = 0.04798463\n",
      "Iteration 102, loss = 0.04774771\n",
      "Iteration 103, loss = 0.04755182\n",
      "Iteration 104, loss = 0.04735480\n",
      "Iteration 105, loss = 0.04718739\n",
      "Iteration 106, loss = 0.04699065\n",
      "Iteration 107, loss = 0.04681083\n",
      "Iteration 108, loss = 0.04663275\n",
      "Iteration 109, loss = 0.04642950\n",
      "Iteration 110, loss = 0.04624552\n",
      "Iteration 111, loss = 0.04606724\n",
      "Iteration 112, loss = 0.04593578\n",
      "Iteration 113, loss = 0.04576645\n",
      "Iteration 114, loss = 0.04558776\n",
      "Iteration 115, loss = 0.04542893\n",
      "Iteration 116, loss = 0.04526236\n",
      "Iteration 117, loss = 0.04510760\n",
      "Iteration 118, loss = 0.04499710\n",
      "Iteration 119, loss = 0.04481443\n",
      "Iteration 120, loss = 0.04472970\n",
      "Iteration 121, loss = 0.04453795\n",
      "Iteration 122, loss = 0.04439809\n",
      "Iteration 123, loss = 0.04427453\n",
      "Iteration 124, loss = 0.04414724\n",
      "Iteration 125, loss = 0.04399302\n",
      "Iteration 126, loss = 0.04387742\n",
      "Iteration 127, loss = 0.04373897\n",
      "Iteration 128, loss = 0.04364008\n",
      "Iteration 129, loss = 0.04350918\n",
      "Iteration 130, loss = 0.04338674\n",
      "Iteration 131, loss = 0.04327506\n",
      "Iteration 132, loss = 0.04315465\n",
      "Iteration 133, loss = 0.04304408\n",
      "Iteration 134, loss = 0.04296895\n",
      "Iteration 135, loss = 0.04287459\n",
      "Iteration 136, loss = 0.04276846\n",
      "Iteration 137, loss = 0.04265519\n",
      "Iteration 138, loss = 0.04251272\n",
      "Iteration 139, loss = 0.04245094\n",
      "Iteration 140, loss = 0.04232059\n",
      "Iteration 141, loss = 0.04223559\n",
      "Iteration 142, loss = 0.04212107\n",
      "Iteration 143, loss = 0.04205028\n",
      "Iteration 144, loss = 0.04197439\n",
      "Iteration 145, loss = 0.04189072\n",
      "Iteration 146, loss = 0.04179365\n",
      "Iteration 147, loss = 0.04170735\n",
      "Iteration 148, loss = 0.04162893\n",
      "Iteration 149, loss = 0.04154765\n",
      "Iteration 150, loss = 0.04148200\n",
      "Iteration 151, loss = 0.04140588\n",
      "Iteration 152, loss = 0.04133476\n",
      "Iteration 153, loss = 0.04127283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29275933\n",
      "Iteration 2, loss = 1.92025297\n",
      "Iteration 3, loss = 1.53820943\n",
      "Iteration 4, loss = 1.15568771\n",
      "Iteration 5, loss = 0.85210609\n",
      "Iteration 6, loss = 0.64088057\n",
      "Iteration 7, loss = 0.50108752\n",
      "Iteration 8, loss = 0.40191707\n",
      "Iteration 9, loss = 0.33572580\n",
      "Iteration 10, loss = 0.28584295\n",
      "Iteration 11, loss = 0.24890799\n",
      "Iteration 12, loss = 0.22088461\n",
      "Iteration 13, loss = 0.19812911\n",
      "Iteration 14, loss = 0.17981532\n",
      "Iteration 15, loss = 0.16645930\n",
      "Iteration 16, loss = 0.15316036\n",
      "Iteration 17, loss = 0.14312451\n",
      "Iteration 18, loss = 0.13430769\n",
      "Iteration 19, loss = 0.12703522\n",
      "Iteration 20, loss = 0.12091106\n",
      "Iteration 21, loss = 0.11535766\n",
      "Iteration 22, loss = 0.11081633\n",
      "Iteration 23, loss = 0.10675921\n",
      "Iteration 24, loss = 0.10311883\n",
      "Iteration 25, loss = 0.09995958\n",
      "Iteration 26, loss = 0.09742384\n",
      "Iteration 27, loss = 0.09464577\n",
      "Iteration 28, loss = 0.09250306\n",
      "Iteration 29, loss = 0.09039241\n",
      "Iteration 30, loss = 0.08853654\n",
      "Iteration 31, loss = 0.08675231\n",
      "Iteration 32, loss = 0.08526219\n",
      "Iteration 33, loss = 0.08376065\n",
      "Iteration 34, loss = 0.08226168\n",
      "Iteration 35, loss = 0.08102358\n",
      "Iteration 36, loss = 0.07982460\n",
      "Iteration 37, loss = 0.07870479\n",
      "Iteration 38, loss = 0.07763563\n",
      "Iteration 39, loss = 0.07660080\n",
      "Iteration 40, loss = 0.07565194\n",
      "Iteration 41, loss = 0.07475041\n",
      "Iteration 42, loss = 0.07385685\n",
      "Iteration 43, loss = 0.07297479\n",
      "Iteration 44, loss = 0.07217645\n",
      "Iteration 45, loss = 0.07143399\n",
      "Iteration 46, loss = 0.07069612\n",
      "Iteration 47, loss = 0.06995517\n",
      "Iteration 48, loss = 0.06927678\n",
      "Iteration 49, loss = 0.06860378\n",
      "Iteration 50, loss = 0.06792662\n",
      "Iteration 51, loss = 0.06731204\n",
      "Iteration 52, loss = 0.06673923\n",
      "Iteration 53, loss = 0.06607248\n",
      "Iteration 54, loss = 0.06556395\n",
      "Iteration 55, loss = 0.06496737\n",
      "Iteration 56, loss = 0.06443529\n",
      "Iteration 57, loss = 0.06390339\n",
      "Iteration 58, loss = 0.06337979\n",
      "Iteration 59, loss = 0.06286634\n",
      "Iteration 60, loss = 0.06238199\n",
      "Iteration 61, loss = 0.06189741\n",
      "Iteration 62, loss = 0.06145156\n",
      "Iteration 63, loss = 0.06100110\n",
      "Iteration 64, loss = 0.06053470\n",
      "Iteration 65, loss = 0.06005872\n",
      "Iteration 66, loss = 0.05961933\n",
      "Iteration 67, loss = 0.05920415\n",
      "Iteration 68, loss = 0.05877791\n",
      "Iteration 69, loss = 0.05837333\n",
      "Iteration 70, loss = 0.05797812\n",
      "Iteration 71, loss = 0.05758582\n",
      "Iteration 72, loss = 0.05720827\n",
      "Iteration 73, loss = 0.05683463\n",
      "Iteration 74, loss = 0.05647790\n",
      "Iteration 75, loss = 0.05613728\n",
      "Iteration 76, loss = 0.05576778\n",
      "Iteration 77, loss = 0.05545098\n",
      "Iteration 78, loss = 0.05511655\n",
      "Iteration 79, loss = 0.05476511\n",
      "Iteration 80, loss = 0.05446049\n",
      "Iteration 81, loss = 0.05412165\n",
      "Iteration 82, loss = 0.05377573\n",
      "Iteration 83, loss = 0.05349371\n",
      "Iteration 84, loss = 0.05319436\n",
      "Iteration 85, loss = 0.05291009\n",
      "Iteration 86, loss = 0.05263297\n",
      "Iteration 87, loss = 0.05231745\n",
      "Iteration 88, loss = 0.05205644\n",
      "Iteration 89, loss = 0.05179639\n",
      "Iteration 90, loss = 0.05156154\n",
      "Iteration 91, loss = 0.05125730\n",
      "Iteration 92, loss = 0.05101519\n",
      "Iteration 93, loss = 0.05077169\n",
      "Iteration 94, loss = 0.05051134\n",
      "Iteration 95, loss = 0.05028796\n",
      "Iteration 96, loss = 0.05006810\n",
      "Iteration 97, loss = 0.04980492\n",
      "Iteration 98, loss = 0.04959127\n",
      "Iteration 99, loss = 0.04934242\n",
      "Iteration 100, loss = 0.04913286\n",
      "Iteration 101, loss = 0.04892193\n",
      "Iteration 102, loss = 0.04870182\n",
      "Iteration 103, loss = 0.04852309\n",
      "Iteration 104, loss = 0.04830150\n",
      "Iteration 105, loss = 0.04812467\n",
      "Iteration 106, loss = 0.04793892\n",
      "Iteration 107, loss = 0.04776323\n",
      "Iteration 108, loss = 0.04755524\n",
      "Iteration 109, loss = 0.04738492\n",
      "Iteration 110, loss = 0.04719770\n",
      "Iteration 111, loss = 0.04702690\n",
      "Iteration 112, loss = 0.04686132\n",
      "Iteration 113, loss = 0.04667591\n",
      "Iteration 114, loss = 0.04649309\n",
      "Iteration 115, loss = 0.04633910\n",
      "Iteration 116, loss = 0.04617865\n",
      "Iteration 117, loss = 0.04601562\n",
      "Iteration 118, loss = 0.04588069\n",
      "Iteration 119, loss = 0.04572692\n",
      "Iteration 120, loss = 0.04561473\n",
      "Iteration 121, loss = 0.04541380\n",
      "Iteration 122, loss = 0.04528701\n",
      "Iteration 123, loss = 0.04515667\n",
      "Iteration 124, loss = 0.04502352\n",
      "Iteration 125, loss = 0.04489169\n",
      "Iteration 126, loss = 0.04478297\n",
      "Iteration 127, loss = 0.04465297\n",
      "Iteration 128, loss = 0.04451988\n",
      "Iteration 129, loss = 0.04438129\n",
      "Iteration 130, loss = 0.04425164\n",
      "Iteration 131, loss = 0.04414665\n",
      "Iteration 132, loss = 0.04404732\n",
      "Iteration 133, loss = 0.04390252\n",
      "Iteration 134, loss = 0.04382574\n",
      "Iteration 135, loss = 0.04372659\n",
      "Iteration 136, loss = 0.04363101\n",
      "Iteration 137, loss = 0.04349814\n",
      "Iteration 138, loss = 0.04339014\n",
      "Iteration 139, loss = 0.04330610\n",
      "Iteration 140, loss = 0.04317277\n",
      "Iteration 141, loss = 0.04308881\n",
      "Iteration 142, loss = 0.04298795\n",
      "Iteration 143, loss = 0.04289988\n",
      "Iteration 144, loss = 0.04284271\n",
      "Iteration 145, loss = 0.04273655\n",
      "Iteration 146, loss = 0.04264594\n",
      "Iteration 147, loss = 0.04256719\n",
      "Iteration 148, loss = 0.04248612\n",
      "Iteration 149, loss = 0.04240533\n",
      "Iteration 150, loss = 0.04229659\n",
      "Iteration 151, loss = 0.04223030\n",
      "Iteration 152, loss = 0.04215819\n",
      "Iteration 153, loss = 0.04207654\n",
      "Iteration 154, loss = 0.04199106\n",
      "Iteration 155, loss = 0.04196367\n",
      "Iteration 156, loss = 0.04185872\n",
      "Iteration 157, loss = 0.04178828\n",
      "Iteration 158, loss = 0.04173705\n",
      "Iteration 159, loss = 0.04166678\n",
      "Iteration 160, loss = 0.04161776\n",
      "Iteration 161, loss = 0.04150561\n",
      "Iteration 162, loss = 0.04147396\n",
      "Iteration 163, loss = 0.04137176\n",
      "Iteration 164, loss = 0.04131262\n",
      "Iteration 165, loss = 0.04124800\n",
      "Iteration 166, loss = 0.04120716\n",
      "Iteration 167, loss = 0.04115394\n",
      "Iteration 168, loss = 0.04108021\n",
      "Iteration 169, loss = 0.04102706\n",
      "Iteration 170, loss = 0.04101413\n",
      "Iteration 171, loss = 0.04091430\n",
      "Iteration 172, loss = 0.04086880\n",
      "Iteration 173, loss = 0.04082139\n",
      "Iteration 174, loss = 0.04072998\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29309471\n",
      "Iteration 2, loss = 1.91396137\n",
      "Iteration 3, loss = 1.51792705\n",
      "Iteration 4, loss = 1.12677577\n",
      "Iteration 5, loss = 0.81816781\n",
      "Iteration 6, loss = 0.60339146\n",
      "Iteration 7, loss = 0.46546078\n",
      "Iteration 8, loss = 0.36742102\n",
      "Iteration 9, loss = 0.30330678\n",
      "Iteration 10, loss = 0.25819265\n",
      "Iteration 11, loss = 0.22329866\n",
      "Iteration 12, loss = 0.19855966\n",
      "Iteration 13, loss = 0.17831552\n",
      "Iteration 14, loss = 0.16199924\n",
      "Iteration 15, loss = 0.15041712\n",
      "Iteration 16, loss = 0.13919272\n",
      "Iteration 17, loss = 0.13071669\n",
      "Iteration 18, loss = 0.12320520\n",
      "Iteration 19, loss = 0.11730719\n",
      "Iteration 20, loss = 0.11187805\n",
      "Iteration 21, loss = 0.10738472\n",
      "Iteration 22, loss = 0.10326458\n",
      "Iteration 23, loss = 0.09972484\n",
      "Iteration 24, loss = 0.09659507\n",
      "Iteration 25, loss = 0.09379787\n",
      "Iteration 26, loss = 0.09137286\n",
      "Iteration 27, loss = 0.08916297\n",
      "Iteration 28, loss = 0.08710643\n",
      "Iteration 29, loss = 0.08523295\n",
      "Iteration 30, loss = 0.08352119\n",
      "Iteration 31, loss = 0.08192444\n",
      "Iteration 32, loss = 0.08051354\n",
      "Iteration 33, loss = 0.07914684\n",
      "Iteration 34, loss = 0.07784600\n",
      "Iteration 35, loss = 0.07668014\n",
      "Iteration 36, loss = 0.07561969\n",
      "Iteration 37, loss = 0.07460680\n",
      "Iteration 38, loss = 0.07360776\n",
      "Iteration 39, loss = 0.07261404\n",
      "Iteration 40, loss = 0.07180304\n",
      "Iteration 41, loss = 0.07091514\n",
      "Iteration 42, loss = 0.07004719\n",
      "Iteration 43, loss = 0.06925111\n",
      "Iteration 44, loss = 0.06849465\n",
      "Iteration 45, loss = 0.06776557\n",
      "Iteration 46, loss = 0.06707015\n",
      "Iteration 47, loss = 0.06642652\n",
      "Iteration 48, loss = 0.06574096\n",
      "Iteration 49, loss = 0.06511106\n",
      "Iteration 50, loss = 0.06445368\n",
      "Iteration 51, loss = 0.06385744\n",
      "Iteration 52, loss = 0.06330471\n",
      "Iteration 53, loss = 0.06269354\n",
      "Iteration 54, loss = 0.06217207\n",
      "Iteration 55, loss = 0.06160973\n",
      "Iteration 56, loss = 0.06109968\n",
      "Iteration 57, loss = 0.06057014\n",
      "Iteration 58, loss = 0.06006994\n",
      "Iteration 59, loss = 0.05959104\n",
      "Iteration 60, loss = 0.05910035\n",
      "Iteration 61, loss = 0.05863556\n",
      "Iteration 62, loss = 0.05818686\n",
      "Iteration 63, loss = 0.05774135\n",
      "Iteration 64, loss = 0.05731733\n",
      "Iteration 65, loss = 0.05688176\n",
      "Iteration 66, loss = 0.05645679\n",
      "Iteration 67, loss = 0.05603896\n",
      "Iteration 68, loss = 0.05563379\n",
      "Iteration 69, loss = 0.05523324\n",
      "Iteration 70, loss = 0.05485619\n",
      "Iteration 71, loss = 0.05449139\n",
      "Iteration 72, loss = 0.05411370\n",
      "Iteration 73, loss = 0.05373732\n",
      "Iteration 74, loss = 0.05338888\n",
      "Iteration 75, loss = 0.05306566\n",
      "Iteration 76, loss = 0.05272997\n",
      "Iteration 77, loss = 0.05237609\n",
      "Iteration 78, loss = 0.05207259\n",
      "Iteration 79, loss = 0.05173310\n",
      "Iteration 80, loss = 0.05142554\n",
      "Iteration 81, loss = 0.05110169\n",
      "Iteration 82, loss = 0.05078760\n",
      "Iteration 83, loss = 0.05049806\n",
      "Iteration 84, loss = 0.05019375\n",
      "Iteration 85, loss = 0.04992221\n",
      "Iteration 86, loss = 0.04964154\n",
      "Iteration 87, loss = 0.04936457\n",
      "Iteration 88, loss = 0.04911057\n",
      "Iteration 89, loss = 0.04885074\n",
      "Iteration 90, loss = 0.04860211\n",
      "Iteration 91, loss = 0.04833724\n",
      "Iteration 92, loss = 0.04811444\n",
      "Iteration 93, loss = 0.04787526\n",
      "Iteration 94, loss = 0.04762847\n",
      "Iteration 95, loss = 0.04740937\n",
      "Iteration 96, loss = 0.04718919\n",
      "Iteration 97, loss = 0.04694204\n",
      "Iteration 98, loss = 0.04673592\n",
      "Iteration 99, loss = 0.04651568\n",
      "Iteration 100, loss = 0.04631725\n",
      "Iteration 101, loss = 0.04608840\n",
      "Iteration 102, loss = 0.04589681\n",
      "Iteration 103, loss = 0.04571597\n",
      "Iteration 104, loss = 0.04551007\n",
      "Iteration 105, loss = 0.04533034\n",
      "Iteration 106, loss = 0.04512875\n",
      "Iteration 107, loss = 0.04495565\n",
      "Iteration 108, loss = 0.04478147\n",
      "Iteration 109, loss = 0.04461731\n",
      "Iteration 110, loss = 0.04443375\n",
      "Iteration 111, loss = 0.04428735\n",
      "Iteration 112, loss = 0.04413081\n",
      "Iteration 113, loss = 0.04395324\n",
      "Iteration 114, loss = 0.04377072\n",
      "Iteration 115, loss = 0.04364295\n",
      "Iteration 116, loss = 0.04349201\n",
      "Iteration 117, loss = 0.04334559\n",
      "Iteration 118, loss = 0.04319491\n",
      "Iteration 119, loss = 0.04304260\n",
      "Iteration 120, loss = 0.04292868\n",
      "Iteration 121, loss = 0.04279086\n",
      "Iteration 122, loss = 0.04264702\n",
      "Iteration 123, loss = 0.04252173\n",
      "Iteration 124, loss = 0.04239164\n",
      "Iteration 125, loss = 0.04228147\n",
      "Iteration 126, loss = 0.04217240\n",
      "Iteration 127, loss = 0.04202272\n",
      "Iteration 128, loss = 0.04194355\n",
      "Iteration 129, loss = 0.04181087\n",
      "Iteration 130, loss = 0.04165751\n",
      "Iteration 131, loss = 0.04156392\n",
      "Iteration 132, loss = 0.04147987\n",
      "Iteration 133, loss = 0.04135196\n",
      "Iteration 134, loss = 0.04125444\n",
      "Iteration 135, loss = 0.04114546\n",
      "Iteration 136, loss = 0.04107597\n",
      "Iteration 137, loss = 0.04096162\n",
      "Iteration 138, loss = 0.04087287\n",
      "Iteration 139, loss = 0.04078689\n",
      "Iteration 140, loss = 0.04066327\n",
      "Iteration 141, loss = 0.04058634\n",
      "Iteration 142, loss = 0.04050622\n",
      "Iteration 143, loss = 0.04044389\n",
      "Iteration 144, loss = 0.04036575\n",
      "Iteration 145, loss = 0.04027459\n",
      "Iteration 146, loss = 0.04019345\n",
      "Iteration 147, loss = 0.04009055\n",
      "Iteration 148, loss = 0.04002363\n",
      "Iteration 149, loss = 0.03992616\n",
      "Iteration 150, loss = 0.03986139\n",
      "Iteration 151, loss = 0.03977325\n",
      "Iteration 152, loss = 0.03971583\n",
      "Iteration 153, loss = 0.03964239\n",
      "Iteration 154, loss = 0.03954932\n",
      "Iteration 155, loss = 0.03956113\n",
      "Iteration 156, loss = 0.03943585\n",
      "Iteration 157, loss = 0.03938890\n",
      "Iteration 158, loss = 0.03934299\n",
      "Iteration 159, loss = 0.03925125\n",
      "Iteration 160, loss = 0.03919243\n",
      "Iteration 161, loss = 0.03914116\n",
      "Iteration 162, loss = 0.03910221\n",
      "Iteration 163, loss = 0.03899348\n",
      "Iteration 164, loss = 0.03896981\n",
      "Iteration 165, loss = 0.03890653\n",
      "Iteration 166, loss = 0.03884881\n",
      "Iteration 167, loss = 0.03879635\n",
      "Iteration 168, loss = 0.03877706\n",
      "Iteration 169, loss = 0.03869120\n",
      "Iteration 170, loss = 0.03864715\n",
      "Iteration 171, loss = 0.03860336\n",
      "Iteration 172, loss = 0.03859513\n",
      "Iteration 173, loss = 0.03852090\n",
      "Iteration 174, loss = 0.03843605\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.42013180\n",
      "Iteration 2, loss = 2.40135469\n",
      "Iteration 3, loss = 2.37636933\n",
      "Iteration 4, loss = 2.35011972\n",
      "Iteration 5, loss = 2.32428606\n",
      "Iteration 6, loss = 2.29933098\n",
      "Iteration 7, loss = 2.27520091\n",
      "Iteration 8, loss = 2.25212411\n",
      "Iteration 9, loss = 2.22852099\n",
      "Iteration 10, loss = 2.20505924\n",
      "Iteration 11, loss = 2.18147583\n",
      "Iteration 12, loss = 2.15775779\n",
      "Iteration 13, loss = 2.13376350\n",
      "Iteration 14, loss = 2.10967719\n",
      "Iteration 15, loss = 2.08491446\n",
      "Iteration 16, loss = 2.06002935\n",
      "Iteration 17, loss = 2.03424571\n",
      "Iteration 18, loss = 2.00799192\n",
      "Iteration 19, loss = 1.98145587\n",
      "Iteration 20, loss = 1.95408483\n",
      "Iteration 21, loss = 1.92669607\n",
      "Iteration 22, loss = 1.89899944\n",
      "Iteration 23, loss = 1.87094662\n",
      "Iteration 24, loss = 1.84267565\n",
      "Iteration 25, loss = 1.81431876\n",
      "Iteration 26, loss = 1.78541423\n",
      "Iteration 27, loss = 1.75672510\n",
      "Iteration 28, loss = 1.72772867\n",
      "Iteration 29, loss = 1.69873578\n",
      "Iteration 30, loss = 1.66965539\n",
      "Iteration 31, loss = 1.64112984\n",
      "Iteration 32, loss = 1.61241913\n",
      "Iteration 33, loss = 1.58381798\n",
      "Iteration 34, loss = 1.55533022\n",
      "Iteration 35, loss = 1.52680447\n",
      "Iteration 36, loss = 1.49829962\n",
      "Iteration 37, loss = 1.47032975\n",
      "Iteration 38, loss = 1.44301110\n",
      "Iteration 39, loss = 1.41583223\n",
      "Iteration 40, loss = 1.38924291\n",
      "Iteration 41, loss = 1.36291497\n",
      "Iteration 42, loss = 1.33694581\n",
      "Iteration 43, loss = 1.31155093\n",
      "Iteration 44, loss = 1.28647192\n",
      "Iteration 45, loss = 1.26180612\n",
      "Iteration 46, loss = 1.23783509\n",
      "Iteration 47, loss = 1.21416025\n",
      "Iteration 48, loss = 1.19101228\n",
      "Iteration 49, loss = 1.16850345\n",
      "Iteration 50, loss = 1.14643614\n",
      "Iteration 51, loss = 1.12518864\n",
      "Iteration 52, loss = 1.10430742\n",
      "Iteration 53, loss = 1.08374875\n",
      "Iteration 54, loss = 1.06379564\n",
      "Iteration 55, loss = 1.04447635\n",
      "Iteration 56, loss = 1.02547836\n",
      "Iteration 57, loss = 1.00699018\n",
      "Iteration 58, loss = 0.98926211\n",
      "Iteration 59, loss = 0.97200502\n",
      "Iteration 60, loss = 0.95516146\n",
      "Iteration 61, loss = 0.93875812\n",
      "Iteration 62, loss = 0.92279253\n",
      "Iteration 63, loss = 0.90712210\n",
      "Iteration 64, loss = 0.89188139\n",
      "Iteration 65, loss = 0.87721907\n",
      "Iteration 66, loss = 0.86296468\n",
      "Iteration 67, loss = 0.84901703\n",
      "Iteration 68, loss = 0.83537396\n",
      "Iteration 69, loss = 0.82233052\n",
      "Iteration 70, loss = 0.80970819\n",
      "Iteration 71, loss = 0.79714626\n",
      "Iteration 72, loss = 0.78513959\n",
      "Iteration 73, loss = 0.77333197\n",
      "Iteration 74, loss = 0.76186839\n",
      "Iteration 75, loss = 0.75069163\n",
      "Iteration 76, loss = 0.74018647\n",
      "Iteration 77, loss = 0.72976448\n",
      "Iteration 78, loss = 0.71922999\n",
      "Iteration 79, loss = 0.70922358\n",
      "Iteration 80, loss = 0.69945326\n",
      "Iteration 81, loss = 0.69007992\n",
      "Iteration 82, loss = 0.68077169\n",
      "Iteration 83, loss = 0.67187641\n",
      "Iteration 84, loss = 0.66301879\n",
      "Iteration 85, loss = 0.65441171\n",
      "Iteration 86, loss = 0.64641164\n",
      "Iteration 87, loss = 0.63832416\n",
      "Iteration 88, loss = 0.63043402\n",
      "Iteration 89, loss = 0.62253098\n",
      "Iteration 90, loss = 0.61497021\n",
      "Iteration 91, loss = 0.60769929\n",
      "Iteration 92, loss = 0.60045408\n",
      "Iteration 93, loss = 0.59357102\n",
      "Iteration 94, loss = 0.58668254\n",
      "Iteration 95, loss = 0.57995517\n",
      "Iteration 96, loss = 0.57344224\n",
      "Iteration 97, loss = 0.56697839\n",
      "Iteration 98, loss = 0.56067102\n",
      "Iteration 99, loss = 0.55458795\n",
      "Iteration 100, loss = 0.54854541\n",
      "Iteration 101, loss = 0.54284767\n",
      "Iteration 102, loss = 0.53702075\n",
      "Iteration 103, loss = 0.53149960\n",
      "Iteration 104, loss = 0.52603323\n",
      "Iteration 105, loss = 0.52063187\n",
      "Iteration 106, loss = 0.51535978\n",
      "Iteration 107, loss = 0.51016146\n",
      "Iteration 108, loss = 0.50513371\n",
      "Iteration 109, loss = 0.50027079\n",
      "Iteration 110, loss = 0.49542910\n",
      "Iteration 111, loss = 0.49080686\n",
      "Iteration 112, loss = 0.48619240\n",
      "Iteration 113, loss = 0.48154335\n",
      "Iteration 114, loss = 0.47709148\n",
      "Iteration 115, loss = 0.47281288\n",
      "Iteration 116, loss = 0.46855404\n",
      "Iteration 117, loss = 0.46421228\n",
      "Iteration 118, loss = 0.46012908\n",
      "Iteration 119, loss = 0.45608521\n",
      "Iteration 120, loss = 0.45211532\n",
      "Iteration 121, loss = 0.44830664\n",
      "Iteration 122, loss = 0.44435838\n",
      "Iteration 123, loss = 0.44055272\n",
      "Iteration 124, loss = 0.43685643\n",
      "Iteration 125, loss = 0.43319961\n",
      "Iteration 126, loss = 0.42961796\n",
      "Iteration 127, loss = 0.42601160\n",
      "Iteration 128, loss = 0.42255462\n",
      "Iteration 129, loss = 0.41924781\n",
      "Iteration 130, loss = 0.41591283\n",
      "Iteration 131, loss = 0.41271789\n",
      "Iteration 132, loss = 0.40967545\n",
      "Iteration 133, loss = 0.40622451\n",
      "Iteration 134, loss = 0.40308212\n",
      "Iteration 135, loss = 0.40023255\n",
      "Iteration 136, loss = 0.39704634\n",
      "Iteration 137, loss = 0.39417726\n",
      "Iteration 138, loss = 0.39110078\n",
      "Iteration 139, loss = 0.38826855\n",
      "Iteration 140, loss = 0.38555324\n",
      "Iteration 141, loss = 0.38276331\n",
      "Iteration 142, loss = 0.38005377\n",
      "Iteration 143, loss = 0.37719335\n",
      "Iteration 144, loss = 0.37443682\n",
      "Iteration 145, loss = 0.37182954\n",
      "Iteration 146, loss = 0.36926567\n",
      "Iteration 147, loss = 0.36681456\n",
      "Iteration 148, loss = 0.36422045\n",
      "Iteration 149, loss = 0.36177218\n",
      "Iteration 150, loss = 0.35931856\n",
      "Iteration 151, loss = 0.35693142\n",
      "Iteration 152, loss = 0.35458960\n",
      "Iteration 153, loss = 0.35211833\n",
      "Iteration 154, loss = 0.34981363\n",
      "Iteration 155, loss = 0.34752675\n",
      "Iteration 156, loss = 0.34534244\n",
      "Iteration 157, loss = 0.34321631\n",
      "Iteration 158, loss = 0.34100416\n",
      "Iteration 159, loss = 0.33885374\n",
      "Iteration 160, loss = 0.33671227\n",
      "Iteration 161, loss = 0.33466414\n",
      "Iteration 162, loss = 0.33267160\n",
      "Iteration 163, loss = 0.33064611\n",
      "Iteration 164, loss = 0.32863351\n",
      "Iteration 165, loss = 0.32672587\n",
      "Iteration 166, loss = 0.32474551\n",
      "Iteration 167, loss = 0.32275350\n",
      "Iteration 168, loss = 0.32077731\n",
      "Iteration 169, loss = 0.31887660\n",
      "Iteration 170, loss = 0.31702551\n",
      "Iteration 171, loss = 0.31516140\n",
      "Iteration 172, loss = 0.31337863\n",
      "Iteration 173, loss = 0.31162347\n",
      "Iteration 174, loss = 0.30991143\n",
      "Iteration 175, loss = 0.30804687\n",
      "Iteration 176, loss = 0.30630265\n",
      "Iteration 177, loss = 0.30457898\n",
      "Iteration 178, loss = 0.30294591\n",
      "Iteration 179, loss = 0.30125297\n",
      "Iteration 180, loss = 0.29961412\n",
      "Iteration 181, loss = 0.29808129\n",
      "Iteration 182, loss = 0.29639704\n",
      "Iteration 183, loss = 0.29478666\n",
      "Iteration 184, loss = 0.29325133\n",
      "Iteration 185, loss = 0.29173519\n",
      "Iteration 186, loss = 0.29018155\n",
      "Iteration 187, loss = 0.28862134\n",
      "Iteration 188, loss = 0.28713702\n",
      "Iteration 189, loss = 0.28576104\n",
      "Iteration 190, loss = 0.28432535\n",
      "Iteration 191, loss = 0.28286133\n",
      "Iteration 192, loss = 0.28133521\n",
      "Iteration 193, loss = 0.27988846\n",
      "Iteration 194, loss = 0.27856689\n",
      "Iteration 195, loss = 0.27714635\n",
      "Iteration 196, loss = 0.27576050\n",
      "Iteration 197, loss = 0.27436409\n",
      "Iteration 198, loss = 0.27299001\n",
      "Iteration 199, loss = 0.27174051\n",
      "Iteration 200, loss = 0.27045779\n",
      "Iteration 201, loss = 0.26903545\n",
      "Iteration 202, loss = 0.26782516\n",
      "Iteration 203, loss = 0.26649433\n",
      "Iteration 204, loss = 0.26522844\n",
      "Iteration 205, loss = 0.26410201\n",
      "Iteration 206, loss = 0.26278589\n",
      "Iteration 207, loss = 0.26152225\n",
      "Iteration 208, loss = 0.26029305\n",
      "Iteration 209, loss = 0.25913720\n",
      "Iteration 210, loss = 0.25789372\n",
      "Iteration 211, loss = 0.25671126\n",
      "Iteration 212, loss = 0.25557561\n",
      "Iteration 213, loss = 0.25441515\n",
      "Iteration 214, loss = 0.25331893\n",
      "Iteration 215, loss = 0.25218096\n",
      "Iteration 216, loss = 0.25107361\n",
      "Iteration 217, loss = 0.24999798\n",
      "Iteration 218, loss = 0.24883906\n",
      "Iteration 219, loss = 0.24780995\n",
      "Iteration 220, loss = 0.24669217\n",
      "Iteration 221, loss = 0.24561455\n",
      "Iteration 222, loss = 0.24457223\n",
      "Iteration 223, loss = 0.24354740\n",
      "Iteration 224, loss = 0.24249474\n",
      "Iteration 225, loss = 0.24146315\n",
      "Iteration 226, loss = 0.24044685\n",
      "Iteration 227, loss = 0.23939415\n",
      "Iteration 228, loss = 0.23844217\n",
      "Iteration 229, loss = 0.23740688\n",
      "Iteration 230, loss = 0.23641644\n",
      "Iteration 231, loss = 0.23549851\n",
      "Iteration 232, loss = 0.23456791\n",
      "Iteration 233, loss = 0.23354955\n",
      "Iteration 234, loss = 0.23262714\n",
      "Iteration 235, loss = 0.23168625\n",
      "Iteration 236, loss = 0.23075339\n",
      "Iteration 237, loss = 0.22987727\n",
      "Iteration 238, loss = 0.22898493\n",
      "Iteration 239, loss = 0.22817089\n",
      "Iteration 240, loss = 0.22722526\n",
      "Iteration 241, loss = 0.22633337\n",
      "Iteration 242, loss = 0.22545807\n",
      "Iteration 243, loss = 0.22456212\n",
      "Iteration 244, loss = 0.22373429\n",
      "Iteration 245, loss = 0.22290635\n",
      "Iteration 246, loss = 0.22208072\n",
      "Iteration 247, loss = 0.22123870\n",
      "Iteration 248, loss = 0.22037808\n",
      "Iteration 249, loss = 0.21961620\n",
      "Iteration 250, loss = 0.21876433\n",
      "Iteration 251, loss = 0.21791537\n",
      "Iteration 252, loss = 0.21711067\n",
      "Iteration 253, loss = 0.21636129\n",
      "Iteration 254, loss = 0.21554964\n",
      "Iteration 255, loss = 0.21475352\n",
      "Iteration 256, loss = 0.21396565\n",
      "Iteration 257, loss = 0.21320290\n",
      "Iteration 258, loss = 0.21244440\n",
      "Iteration 259, loss = 0.21169676\n",
      "Iteration 260, loss = 0.21098362\n",
      "Iteration 261, loss = 0.21023094\n",
      "Iteration 262, loss = 0.20948517\n",
      "Iteration 263, loss = 0.20876101\n",
      "Iteration 264, loss = 0.20802993\n",
      "Iteration 265, loss = 0.20727807\n",
      "Iteration 266, loss = 0.20658934\n",
      "Iteration 267, loss = 0.20593316\n",
      "Iteration 268, loss = 0.20518299\n",
      "Iteration 269, loss = 0.20451666\n",
      "Iteration 270, loss = 0.20378689\n",
      "Iteration 271, loss = 0.20307801\n",
      "Iteration 272, loss = 0.20241211\n",
      "Iteration 273, loss = 0.20177776\n",
      "Iteration 274, loss = 0.20115008\n",
      "Iteration 275, loss = 0.20044064\n",
      "Iteration 276, loss = 0.19974748\n",
      "Iteration 277, loss = 0.19909183\n",
      "Iteration 278, loss = 0.19843911\n",
      "Iteration 279, loss = 0.19781079\n",
      "Iteration 280, loss = 0.19716532\n",
      "Iteration 281, loss = 0.19652821\n",
      "Iteration 282, loss = 0.19586757\n",
      "Iteration 283, loss = 0.19525548\n",
      "Iteration 284, loss = 0.19466423\n",
      "Iteration 285, loss = 0.19403764\n",
      "Iteration 286, loss = 0.19340247\n",
      "Iteration 287, loss = 0.19280222\n",
      "Iteration 288, loss = 0.19217807\n",
      "Iteration 289, loss = 0.19160186\n",
      "Iteration 290, loss = 0.19099779\n",
      "Iteration 291, loss = 0.19043370\n",
      "Iteration 292, loss = 0.18983470\n",
      "Iteration 293, loss = 0.18923807\n",
      "Iteration 294, loss = 0.18867670\n",
      "Iteration 295, loss = 0.18808107\n",
      "Iteration 296, loss = 0.18749128\n",
      "Iteration 297, loss = 0.18697063\n",
      "Iteration 298, loss = 0.18646641\n",
      "Iteration 299, loss = 0.18591052\n",
      "Iteration 300, loss = 0.18535052\n",
      "Iteration 301, loss = 0.18478354\n",
      "Iteration 302, loss = 0.18417555\n",
      "Iteration 303, loss = 0.18361989\n",
      "Iteration 304, loss = 0.18309939\n",
      "Iteration 305, loss = 0.18255701\n",
      "Iteration 306, loss = 0.18201552\n",
      "Iteration 307, loss = 0.18144700\n",
      "Iteration 308, loss = 0.18098535\n",
      "Iteration 309, loss = 0.18042135\n",
      "Iteration 310, loss = 0.17988363\n",
      "Iteration 311, loss = 0.17940573\n",
      "Iteration 312, loss = 0.17891543\n",
      "Iteration 313, loss = 0.17841713\n",
      "Iteration 314, loss = 0.17791751\n",
      "Iteration 315, loss = 0.17741280\n",
      "Iteration 316, loss = 0.17694195\n",
      "Iteration 317, loss = 0.17645172\n",
      "Iteration 318, loss = 0.17598865\n",
      "Iteration 319, loss = 0.17547644\n",
      "Iteration 320, loss = 0.17501723\n",
      "Iteration 321, loss = 0.17456980\n",
      "Iteration 322, loss = 0.17403340\n",
      "Iteration 323, loss = 0.17355773\n",
      "Iteration 324, loss = 0.17308017\n",
      "Iteration 325, loss = 0.17262820\n",
      "Iteration 326, loss = 0.17219016\n",
      "Iteration 327, loss = 0.17168863\n",
      "Iteration 328, loss = 0.17123471\n",
      "Iteration 329, loss = 0.17079532\n",
      "Iteration 330, loss = 0.17035007\n",
      "Iteration 331, loss = 0.16989052\n",
      "Iteration 332, loss = 0.16944978\n",
      "Iteration 333, loss = 0.16899904\n",
      "Iteration 334, loss = 0.16856224\n",
      "Iteration 335, loss = 0.16813751\n",
      "Iteration 336, loss = 0.16774280\n",
      "Iteration 337, loss = 0.16728187\n",
      "Iteration 338, loss = 0.16686191\n",
      "Iteration 339, loss = 0.16643110\n",
      "Iteration 340, loss = 0.16601198\n",
      "Iteration 341, loss = 0.16559819\n",
      "Iteration 342, loss = 0.16518690\n",
      "Iteration 343, loss = 0.16477962\n",
      "Iteration 344, loss = 0.16434534\n",
      "Iteration 345, loss = 0.16394004\n",
      "Iteration 346, loss = 0.16353501\n",
      "Iteration 347, loss = 0.16314348\n",
      "Iteration 348, loss = 0.16273853\n",
      "Iteration 349, loss = 0.16233734\n",
      "Iteration 350, loss = 0.16197838\n",
      "Iteration 351, loss = 0.16157655\n",
      "Iteration 352, loss = 0.16115460\n",
      "Iteration 353, loss = 0.16077940\n",
      "Iteration 354, loss = 0.16037614\n",
      "Iteration 355, loss = 0.15998218\n",
      "Iteration 356, loss = 0.15965647\n",
      "Iteration 357, loss = 0.15923850\n",
      "Iteration 358, loss = 0.15884774\n",
      "Iteration 359, loss = 0.15848752\n",
      "Iteration 360, loss = 0.15811828\n",
      "Iteration 361, loss = 0.15777364\n",
      "Iteration 362, loss = 0.15741492\n",
      "Iteration 363, loss = 0.15702672\n",
      "Iteration 364, loss = 0.15669502\n",
      "Iteration 365, loss = 0.15634343\n",
      "Iteration 366, loss = 0.15597536\n",
      "Iteration 367, loss = 0.15562729\n",
      "Iteration 368, loss = 0.15524549\n",
      "Iteration 369, loss = 0.15492114\n",
      "Iteration 370, loss = 0.15455373\n",
      "Iteration 371, loss = 0.15419464\n",
      "Iteration 372, loss = 0.15389181\n",
      "Iteration 373, loss = 0.15355976\n",
      "Iteration 374, loss = 0.15321173\n",
      "Iteration 375, loss = 0.15284505\n",
      "Iteration 376, loss = 0.15252109\n",
      "Iteration 377, loss = 0.15215970\n",
      "Iteration 378, loss = 0.15186778\n",
      "Iteration 379, loss = 0.15153305\n",
      "Iteration 380, loss = 0.15119824\n",
      "Iteration 381, loss = 0.15086292\n",
      "Iteration 382, loss = 0.15053442\n",
      "Iteration 383, loss = 0.15018640\n",
      "Iteration 384, loss = 0.14988598\n",
      "Iteration 385, loss = 0.14956921\n",
      "Iteration 386, loss = 0.14923281\n",
      "Iteration 387, loss = 0.14894245\n",
      "Iteration 388, loss = 0.14862245\n",
      "Iteration 389, loss = 0.14833712\n",
      "Iteration 390, loss = 0.14799779\n",
      "Iteration 391, loss = 0.14769526\n",
      "Iteration 392, loss = 0.14736814\n",
      "Iteration 393, loss = 0.14704645\n",
      "Iteration 394, loss = 0.14671895\n",
      "Iteration 395, loss = 0.14641845\n",
      "Iteration 396, loss = 0.14611466\n",
      "Iteration 397, loss = 0.14580118\n",
      "Iteration 398, loss = 0.14549798\n",
      "Iteration 399, loss = 0.14517785\n",
      "Iteration 400, loss = 0.14493575\n",
      "Iteration 401, loss = 0.14462151\n",
      "Iteration 402, loss = 0.14433874\n",
      "Iteration 403, loss = 0.14404547\n",
      "Iteration 404, loss = 0.14376020\n",
      "Iteration 405, loss = 0.14344313\n",
      "Iteration 406, loss = 0.14316125\n",
      "Iteration 407, loss = 0.14287744\n",
      "Iteration 408, loss = 0.14259047\n",
      "Iteration 409, loss = 0.14229932\n",
      "Iteration 410, loss = 0.14202453\n",
      "Iteration 411, loss = 0.14177782\n",
      "Iteration 412, loss = 0.14148877\n",
      "Iteration 413, loss = 0.14122065\n",
      "Iteration 414, loss = 0.14093241\n",
      "Iteration 415, loss = 0.14067697\n",
      "Iteration 416, loss = 0.14039657\n",
      "Iteration 417, loss = 0.14012238\n",
      "Iteration 418, loss = 0.13986291\n",
      "Iteration 419, loss = 0.13961654\n",
      "Iteration 420, loss = 0.13933679\n",
      "Iteration 421, loss = 0.13907947\n",
      "Iteration 422, loss = 0.13880159\n",
      "Iteration 423, loss = 0.13855147\n",
      "Iteration 424, loss = 0.13832557\n",
      "Iteration 425, loss = 0.13801744\n",
      "Iteration 426, loss = 0.13780093\n",
      "Iteration 427, loss = 0.13756452\n",
      "Iteration 428, loss = 0.13729953\n",
      "Iteration 429, loss = 0.13703750\n",
      "Iteration 430, loss = 0.13677949\n",
      "Iteration 431, loss = 0.13654255\n",
      "Iteration 432, loss = 0.13628002\n",
      "Iteration 433, loss = 0.13600781\n",
      "Iteration 434, loss = 0.13582071\n",
      "Iteration 435, loss = 0.13555824\n",
      "Iteration 436, loss = 0.13532715\n",
      "Iteration 437, loss = 0.13506583\n",
      "Iteration 438, loss = 0.13486033\n",
      "Iteration 439, loss = 0.13462406\n",
      "Iteration 440, loss = 0.13437096\n",
      "Iteration 441, loss = 0.13414979\n",
      "Iteration 442, loss = 0.13388940\n",
      "Iteration 443, loss = 0.13366907\n",
      "Iteration 444, loss = 0.13345249\n",
      "Iteration 445, loss = 0.13320369\n",
      "Iteration 446, loss = 0.13297618\n",
      "Iteration 447, loss = 0.13273385\n",
      "Iteration 448, loss = 0.13252824\n",
      "Iteration 449, loss = 0.13229502\n",
      "Iteration 450, loss = 0.13208234\n",
      "Iteration 451, loss = 0.13183450\n",
      "Iteration 452, loss = 0.13162851\n",
      "Iteration 453, loss = 0.13140074\n",
      "Iteration 454, loss = 0.13117700\n",
      "Iteration 455, loss = 0.13095173\n",
      "Iteration 456, loss = 0.13073376\n",
      "Iteration 457, loss = 0.13051691\n",
      "Iteration 458, loss = 0.13030364\n",
      "Iteration 459, loss = 0.13008557\n",
      "Iteration 460, loss = 0.12986884\n",
      "Iteration 461, loss = 0.12966648\n",
      "Iteration 462, loss = 0.12945247\n",
      "Iteration 463, loss = 0.12925117\n",
      "Iteration 464, loss = 0.12904081\n",
      "Iteration 465, loss = 0.12884271\n",
      "Iteration 466, loss = 0.12864860\n",
      "Iteration 467, loss = 0.12844160\n",
      "Iteration 468, loss = 0.12823296\n",
      "Iteration 469, loss = 0.12805122\n",
      "Iteration 470, loss = 0.12782656\n",
      "Iteration 471, loss = 0.12764408\n",
      "Iteration 472, loss = 0.12741699\n",
      "Iteration 473, loss = 0.12721315\n",
      "Iteration 474, loss = 0.12700549\n",
      "Iteration 475, loss = 0.12683856\n",
      "Iteration 476, loss = 0.12660282\n",
      "Iteration 477, loss = 0.12640366\n",
      "Iteration 478, loss = 0.12620784\n",
      "Iteration 479, loss = 0.12601954\n",
      "Iteration 480, loss = 0.12583848\n",
      "Iteration 481, loss = 0.12564549\n",
      "Iteration 482, loss = 0.12548362\n",
      "Iteration 483, loss = 0.12527558\n",
      "Iteration 484, loss = 0.12506921\n",
      "Iteration 485, loss = 0.12487067\n",
      "Iteration 486, loss = 0.12469622\n",
      "Iteration 487, loss = 0.12450261\n",
      "Iteration 488, loss = 0.12433195\n",
      "Iteration 489, loss = 0.12414268\n",
      "Iteration 490, loss = 0.12394117\n",
      "Iteration 491, loss = 0.12377344\n",
      "Iteration 492, loss = 0.12359105\n",
      "Iteration 493, loss = 0.12338985\n",
      "Iteration 494, loss = 0.12322462\n",
      "Iteration 495, loss = 0.12303592\n",
      "Iteration 496, loss = 0.12284220\n",
      "Iteration 497, loss = 0.12269040\n",
      "Iteration 498, loss = 0.12250631\n",
      "Iteration 499, loss = 0.12232168\n",
      "Iteration 500, loss = 0.12213646\n",
      "Iteration 501, loss = 0.12197455\n",
      "Iteration 502, loss = 0.12179508\n",
      "Iteration 503, loss = 0.12162494\n",
      "Iteration 504, loss = 0.12144971\n",
      "Iteration 505, loss = 0.12129390\n",
      "Iteration 506, loss = 0.12111497\n",
      "Iteration 507, loss = 0.12094687\n",
      "Iteration 508, loss = 0.12078154\n",
      "Iteration 509, loss = 0.12060474\n",
      "Iteration 510, loss = 0.12043804\n",
      "Iteration 511, loss = 0.12027835\n",
      "Iteration 512, loss = 0.12010722\n",
      "Iteration 513, loss = 0.11995374\n",
      "Iteration 514, loss = 0.11978552\n",
      "Iteration 515, loss = 0.11963280\n",
      "Iteration 516, loss = 0.11947619\n",
      "Iteration 517, loss = 0.11933618\n",
      "Iteration 518, loss = 0.11917960\n",
      "Iteration 519, loss = 0.11901168\n",
      "Iteration 520, loss = 0.11884100\n",
      "Iteration 521, loss = 0.11869388\n",
      "Iteration 522, loss = 0.11853498\n",
      "Iteration 523, loss = 0.11839118\n",
      "Iteration 524, loss = 0.11823948\n",
      "Iteration 525, loss = 0.11809103\n",
      "Iteration 526, loss = 0.11792145\n",
      "Iteration 527, loss = 0.11778448\n",
      "Iteration 528, loss = 0.11764818\n",
      "Iteration 529, loss = 0.11748598\n",
      "Iteration 530, loss = 0.11733780\n",
      "Iteration 531, loss = 0.11718576\n",
      "Iteration 532, loss = 0.11705909\n",
      "Iteration 533, loss = 0.11690486\n",
      "Iteration 534, loss = 0.11677484\n",
      "Iteration 535, loss = 0.11664138\n",
      "Iteration 536, loss = 0.11647105\n",
      "Iteration 537, loss = 0.11632995\n",
      "Iteration 538, loss = 0.11618815\n",
      "Iteration 539, loss = 0.11603619\n",
      "Iteration 540, loss = 0.11589540\n",
      "Iteration 541, loss = 0.11574156\n",
      "Iteration 542, loss = 0.11559598\n",
      "Iteration 543, loss = 0.11545945\n",
      "Iteration 544, loss = 0.11530877\n",
      "Iteration 545, loss = 0.11517960\n",
      "Iteration 546, loss = 0.11504455\n",
      "Iteration 547, loss = 0.11490152\n",
      "Iteration 548, loss = 0.11477011\n",
      "Iteration 549, loss = 0.11462833\n",
      "Iteration 550, loss = 0.11448896\n",
      "Iteration 551, loss = 0.11437569\n",
      "Iteration 552, loss = 0.11421884\n",
      "Iteration 553, loss = 0.11407638\n",
      "Iteration 554, loss = 0.11394629\n",
      "Iteration 555, loss = 0.11380825\n",
      "Iteration 556, loss = 0.11367269\n",
      "Iteration 557, loss = 0.11355539\n",
      "Iteration 558, loss = 0.11343103\n",
      "Iteration 559, loss = 0.11329203\n",
      "Iteration 560, loss = 0.11316693\n",
      "Iteration 561, loss = 0.11304112\n",
      "Iteration 562, loss = 0.11291627\n",
      "Iteration 563, loss = 0.11276824\n",
      "Iteration 564, loss = 0.11264596\n",
      "Iteration 565, loss = 0.11251946\n",
      "Iteration 566, loss = 0.11239389\n",
      "Iteration 567, loss = 0.11226906\n",
      "Iteration 568, loss = 0.11213958\n",
      "Iteration 569, loss = 0.11201379\n",
      "Iteration 570, loss = 0.11188275\n",
      "Iteration 571, loss = 0.11177419\n",
      "Iteration 572, loss = 0.11164361\n",
      "Iteration 573, loss = 0.11151241\n",
      "Iteration 574, loss = 0.11140860\n",
      "Iteration 575, loss = 0.11127882\n",
      "Iteration 576, loss = 0.11116101\n",
      "Iteration 577, loss = 0.11104026\n",
      "Iteration 578, loss = 0.11091724\n",
      "Iteration 579, loss = 0.11080651\n",
      "Iteration 580, loss = 0.11067382\n",
      "Iteration 581, loss = 0.11056429\n",
      "Iteration 582, loss = 0.11043816\n",
      "Iteration 583, loss = 0.11032067\n",
      "Iteration 584, loss = 0.11019560\n",
      "Iteration 585, loss = 0.11008109\n",
      "Iteration 586, loss = 0.10996413\n",
      "Iteration 587, loss = 0.10987053\n",
      "Iteration 588, loss = 0.10973886\n",
      "Iteration 589, loss = 0.10962158\n",
      "Iteration 590, loss = 0.10950155\n",
      "Iteration 591, loss = 0.10939661\n",
      "Iteration 592, loss = 0.10926481\n",
      "Iteration 593, loss = 0.10915569\n",
      "Iteration 594, loss = 0.10903085\n",
      "Iteration 595, loss = 0.10893938\n",
      "Iteration 596, loss = 0.10882325\n",
      "Iteration 597, loss = 0.10869264\n",
      "Iteration 598, loss = 0.10858788\n",
      "Iteration 599, loss = 0.10848736\n",
      "Iteration 600, loss = 0.10836992\n",
      "Iteration 601, loss = 0.10827244\n",
      "Iteration 602, loss = 0.10815652\n",
      "Iteration 603, loss = 0.10805522\n",
      "Iteration 604, loss = 0.10794948\n",
      "Iteration 605, loss = 0.10785074\n",
      "Iteration 606, loss = 0.10771831\n",
      "Iteration 607, loss = 0.10762872\n",
      "Iteration 608, loss = 0.10751165\n",
      "Iteration 609, loss = 0.10740521\n",
      "Iteration 610, loss = 0.10730352\n",
      "Iteration 611, loss = 0.10719581\n",
      "Iteration 612, loss = 0.10711536\n",
      "Iteration 613, loss = 0.10702698\n",
      "Iteration 614, loss = 0.10689196\n",
      "Iteration 615, loss = 0.10678818\n",
      "Iteration 616, loss = 0.10667218\n",
      "Iteration 617, loss = 0.10657871\n",
      "Iteration 618, loss = 0.10647516\n",
      "Iteration 619, loss = 0.10637679\n",
      "Iteration 620, loss = 0.10627041\n",
      "Iteration 621, loss = 0.10616067\n",
      "Iteration 622, loss = 0.10606384\n",
      "Iteration 623, loss = 0.10596071\n",
      "Iteration 624, loss = 0.10586405\n",
      "Iteration 625, loss = 0.10576151\n",
      "Iteration 626, loss = 0.10566174\n",
      "Iteration 627, loss = 0.10556593\n",
      "Iteration 628, loss = 0.10547187\n",
      "Iteration 629, loss = 0.10536733\n",
      "Iteration 630, loss = 0.10528184\n",
      "Iteration 631, loss = 0.10517603\n",
      "Iteration 632, loss = 0.10507994\n",
      "Iteration 633, loss = 0.10498145\n",
      "Iteration 634, loss = 0.10489729\n",
      "Iteration 635, loss = 0.10480502\n",
      "Iteration 636, loss = 0.10470186\n",
      "Iteration 637, loss = 0.10461161\n",
      "Iteration 638, loss = 0.10451926\n",
      "Iteration 639, loss = 0.10441952\n",
      "Iteration 640, loss = 0.10433259\n",
      "Iteration 641, loss = 0.10423420\n",
      "Iteration 642, loss = 0.10415871\n",
      "Iteration 643, loss = 0.10404095\n",
      "Iteration 644, loss = 0.10395615\n",
      "Iteration 645, loss = 0.10385674\n",
      "Iteration 646, loss = 0.10376484\n",
      "Iteration 647, loss = 0.10368010\n",
      "Iteration 648, loss = 0.10359230\n",
      "Iteration 649, loss = 0.10350656\n",
      "Iteration 650, loss = 0.10341184\n",
      "Iteration 651, loss = 0.10332237\n",
      "Iteration 652, loss = 0.10322025\n",
      "Iteration 653, loss = 0.10314561\n",
      "Iteration 654, loss = 0.10304428\n",
      "Iteration 655, loss = 0.10296814\n",
      "Iteration 656, loss = 0.10287412\n",
      "Iteration 657, loss = 0.10278698\n",
      "Iteration 658, loss = 0.10270368\n",
      "Iteration 659, loss = 0.10262211\n",
      "Iteration 660, loss = 0.10253807\n",
      "Iteration 661, loss = 0.10245398\n",
      "Iteration 662, loss = 0.10236773\n",
      "Iteration 663, loss = 0.10228524\n",
      "Iteration 664, loss = 0.10219886\n",
      "Iteration 665, loss = 0.10211226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.41653624\n",
      "Iteration 2, loss = 2.39730206\n",
      "Iteration 3, loss = 2.37214772\n",
      "Iteration 4, loss = 2.34556585\n",
      "Iteration 5, loss = 2.31951121\n",
      "Iteration 6, loss = 2.29430234\n",
      "Iteration 7, loss = 2.26967560\n",
      "Iteration 8, loss = 2.24563683\n",
      "Iteration 9, loss = 2.22130317\n",
      "Iteration 10, loss = 2.19706479\n",
      "Iteration 11, loss = 2.17269035\n",
      "Iteration 12, loss = 2.14791726\n",
      "Iteration 13, loss = 2.12290229\n",
      "Iteration 14, loss = 2.09749340\n",
      "Iteration 15, loss = 2.07145560\n",
      "Iteration 16, loss = 2.04519531\n",
      "Iteration 17, loss = 2.01789040\n",
      "Iteration 18, loss = 1.99010757\n",
      "Iteration 19, loss = 1.96163614\n",
      "Iteration 20, loss = 1.93276463\n",
      "Iteration 21, loss = 1.90354358\n",
      "Iteration 22, loss = 1.87416496\n",
      "Iteration 23, loss = 1.84460305\n",
      "Iteration 24, loss = 1.81478014\n",
      "Iteration 25, loss = 1.78491482\n",
      "Iteration 26, loss = 1.75465318\n",
      "Iteration 27, loss = 1.72440434\n",
      "Iteration 28, loss = 1.69427531\n",
      "Iteration 29, loss = 1.66393802\n",
      "Iteration 30, loss = 1.63375212\n",
      "Iteration 31, loss = 1.60374200\n",
      "Iteration 32, loss = 1.57396799\n",
      "Iteration 33, loss = 1.54411805\n",
      "Iteration 34, loss = 1.51464250\n",
      "Iteration 35, loss = 1.48574233\n",
      "Iteration 36, loss = 1.45655596\n",
      "Iteration 37, loss = 1.42799040\n",
      "Iteration 38, loss = 1.39990450\n",
      "Iteration 39, loss = 1.37234715\n",
      "Iteration 40, loss = 1.34530405\n",
      "Iteration 41, loss = 1.31850781\n",
      "Iteration 42, loss = 1.29182044\n",
      "Iteration 43, loss = 1.26590056\n",
      "Iteration 44, loss = 1.24037942\n",
      "Iteration 45, loss = 1.21537258\n",
      "Iteration 46, loss = 1.19086837\n",
      "Iteration 47, loss = 1.16708476\n",
      "Iteration 48, loss = 1.14368748\n",
      "Iteration 49, loss = 1.12077147\n",
      "Iteration 50, loss = 1.09859118\n",
      "Iteration 51, loss = 1.07733173\n",
      "Iteration 52, loss = 1.05629190\n",
      "Iteration 53, loss = 1.03583205\n",
      "Iteration 54, loss = 1.01598505\n",
      "Iteration 55, loss = 0.99678362\n",
      "Iteration 56, loss = 0.97812359\n",
      "Iteration 57, loss = 0.95984384\n",
      "Iteration 58, loss = 0.94234421\n",
      "Iteration 59, loss = 0.92547241\n",
      "Iteration 60, loss = 0.90903633\n",
      "Iteration 61, loss = 0.89282091\n",
      "Iteration 62, loss = 0.87709548\n",
      "Iteration 63, loss = 0.86182551\n",
      "Iteration 64, loss = 0.84696503\n",
      "Iteration 65, loss = 0.83254858\n",
      "Iteration 66, loss = 0.81891443\n",
      "Iteration 67, loss = 0.80546413\n",
      "Iteration 68, loss = 0.79207881\n",
      "Iteration 69, loss = 0.77951766\n",
      "Iteration 70, loss = 0.76732686\n",
      "Iteration 71, loss = 0.75513924\n",
      "Iteration 72, loss = 0.74350892\n",
      "Iteration 73, loss = 0.73212835\n",
      "Iteration 74, loss = 0.72124501\n",
      "Iteration 75, loss = 0.71042524\n",
      "Iteration 76, loss = 0.70039935\n",
      "Iteration 77, loss = 0.69024502\n",
      "Iteration 78, loss = 0.68021697\n",
      "Iteration 79, loss = 0.67090942\n",
      "Iteration 80, loss = 0.66163978\n",
      "Iteration 81, loss = 0.65275350\n",
      "Iteration 82, loss = 0.64392737\n",
      "Iteration 83, loss = 0.63564143\n",
      "Iteration 84, loss = 0.62724086\n",
      "Iteration 85, loss = 0.61925194\n",
      "Iteration 86, loss = 0.61149885\n",
      "Iteration 87, loss = 0.60393624\n",
      "Iteration 88, loss = 0.59650502\n",
      "Iteration 89, loss = 0.58920894\n",
      "Iteration 90, loss = 0.58216288\n",
      "Iteration 91, loss = 0.57524172\n",
      "Iteration 92, loss = 0.56861978\n",
      "Iteration 93, loss = 0.56219217\n",
      "Iteration 94, loss = 0.55574425\n",
      "Iteration 95, loss = 0.54941606\n",
      "Iteration 96, loss = 0.54326552\n",
      "Iteration 97, loss = 0.53723595\n",
      "Iteration 98, loss = 0.53146828\n",
      "Iteration 99, loss = 0.52582235\n",
      "Iteration 100, loss = 0.52028836\n",
      "Iteration 101, loss = 0.51499309\n",
      "Iteration 102, loss = 0.50950837\n",
      "Iteration 103, loss = 0.50423933\n",
      "Iteration 104, loss = 0.49927113\n",
      "Iteration 105, loss = 0.49437747\n",
      "Iteration 106, loss = 0.48944485\n",
      "Iteration 107, loss = 0.48456498\n",
      "Iteration 108, loss = 0.47982793\n",
      "Iteration 109, loss = 0.47530672\n",
      "Iteration 110, loss = 0.47084013\n",
      "Iteration 111, loss = 0.46655748\n",
      "Iteration 112, loss = 0.46222652\n",
      "Iteration 113, loss = 0.45799259\n",
      "Iteration 114, loss = 0.45399438\n",
      "Iteration 115, loss = 0.44988899\n",
      "Iteration 116, loss = 0.44580899\n",
      "Iteration 117, loss = 0.44193143\n",
      "Iteration 118, loss = 0.43810500\n",
      "Iteration 119, loss = 0.43421473\n",
      "Iteration 120, loss = 0.43067116\n",
      "Iteration 121, loss = 0.42712144\n",
      "Iteration 122, loss = 0.42346694\n",
      "Iteration 123, loss = 0.41993112\n",
      "Iteration 124, loss = 0.41642997\n",
      "Iteration 125, loss = 0.41303222\n",
      "Iteration 126, loss = 0.40975744\n",
      "Iteration 127, loss = 0.40637265\n",
      "Iteration 128, loss = 0.40321890\n",
      "Iteration 129, loss = 0.40025384\n",
      "Iteration 130, loss = 0.39711168\n",
      "Iteration 131, loss = 0.39408107\n",
      "Iteration 132, loss = 0.39123310\n",
      "Iteration 133, loss = 0.38820134\n",
      "Iteration 134, loss = 0.38528472\n",
      "Iteration 135, loss = 0.38252149\n",
      "Iteration 136, loss = 0.37961660\n",
      "Iteration 137, loss = 0.37681999\n",
      "Iteration 138, loss = 0.37409241\n",
      "Iteration 139, loss = 0.37143039\n",
      "Iteration 140, loss = 0.36884986\n",
      "Iteration 141, loss = 0.36628781\n",
      "Iteration 142, loss = 0.36374468\n",
      "Iteration 143, loss = 0.36120014\n",
      "Iteration 144, loss = 0.35865401\n",
      "Iteration 145, loss = 0.35626025\n",
      "Iteration 146, loss = 0.35385177\n",
      "Iteration 147, loss = 0.35156810\n",
      "Iteration 148, loss = 0.34914819\n",
      "Iteration 149, loss = 0.34696404\n",
      "Iteration 150, loss = 0.34452504\n",
      "Iteration 151, loss = 0.34230163\n",
      "Iteration 152, loss = 0.34020284\n",
      "Iteration 153, loss = 0.33794464\n",
      "Iteration 154, loss = 0.33582014\n",
      "Iteration 155, loss = 0.33368395\n",
      "Iteration 156, loss = 0.33157777\n",
      "Iteration 157, loss = 0.32962577\n",
      "Iteration 158, loss = 0.32750702\n",
      "Iteration 159, loss = 0.32556275\n",
      "Iteration 160, loss = 0.32356841\n",
      "Iteration 161, loss = 0.32175948\n",
      "Iteration 162, loss = 0.31986505\n",
      "Iteration 163, loss = 0.31791002\n",
      "Iteration 164, loss = 0.31596986\n",
      "Iteration 165, loss = 0.31411509\n",
      "Iteration 166, loss = 0.31230365\n",
      "Iteration 167, loss = 0.31045214\n",
      "Iteration 168, loss = 0.30862014\n",
      "Iteration 169, loss = 0.30687767\n",
      "Iteration 170, loss = 0.30519181\n",
      "Iteration 171, loss = 0.30347880\n",
      "Iteration 172, loss = 0.30176362\n",
      "Iteration 173, loss = 0.30013581\n",
      "Iteration 174, loss = 0.29858426\n",
      "Iteration 175, loss = 0.29686520\n",
      "Iteration 176, loss = 0.29530654\n",
      "Iteration 177, loss = 0.29374724\n",
      "Iteration 178, loss = 0.29215446\n",
      "Iteration 179, loss = 0.29063526\n",
      "Iteration 180, loss = 0.28902401\n",
      "Iteration 181, loss = 0.28759485\n",
      "Iteration 182, loss = 0.28607452\n",
      "Iteration 183, loss = 0.28456326\n",
      "Iteration 184, loss = 0.28309196\n",
      "Iteration 185, loss = 0.28163628\n",
      "Iteration 186, loss = 0.28019301\n",
      "Iteration 187, loss = 0.27875318\n",
      "Iteration 188, loss = 0.27737752\n",
      "Iteration 189, loss = 0.27607841\n",
      "Iteration 190, loss = 0.27466933\n",
      "Iteration 191, loss = 0.27338201\n",
      "Iteration 192, loss = 0.27202485\n",
      "Iteration 193, loss = 0.27064232\n",
      "Iteration 194, loss = 0.26937217\n",
      "Iteration 195, loss = 0.26805805\n",
      "Iteration 196, loss = 0.26687725\n",
      "Iteration 197, loss = 0.26557010\n",
      "Iteration 198, loss = 0.26430464\n",
      "Iteration 199, loss = 0.26302111\n",
      "Iteration 200, loss = 0.26173546\n",
      "Iteration 201, loss = 0.26050195\n",
      "Iteration 202, loss = 0.25933794\n",
      "Iteration 203, loss = 0.25816844\n",
      "Iteration 204, loss = 0.25696943\n",
      "Iteration 205, loss = 0.25587863\n",
      "Iteration 206, loss = 0.25468367\n",
      "Iteration 207, loss = 0.25355608\n",
      "Iteration 208, loss = 0.25238686\n",
      "Iteration 209, loss = 0.25128275\n",
      "Iteration 210, loss = 0.25024459\n",
      "Iteration 211, loss = 0.24906965\n",
      "Iteration 212, loss = 0.24794311\n",
      "Iteration 213, loss = 0.24696256\n",
      "Iteration 214, loss = 0.24594533\n",
      "Iteration 215, loss = 0.24489811\n",
      "Iteration 216, loss = 0.24383543\n",
      "Iteration 217, loss = 0.24278247\n",
      "Iteration 218, loss = 0.24173297\n",
      "Iteration 219, loss = 0.24075418\n",
      "Iteration 220, loss = 0.23971625\n",
      "Iteration 221, loss = 0.23873437\n",
      "Iteration 222, loss = 0.23771509\n",
      "Iteration 223, loss = 0.23674810\n",
      "Iteration 224, loss = 0.23583032\n",
      "Iteration 225, loss = 0.23487186\n",
      "Iteration 226, loss = 0.23391127\n",
      "Iteration 227, loss = 0.23295822\n",
      "Iteration 228, loss = 0.23202348\n",
      "Iteration 229, loss = 0.23107342\n",
      "Iteration 230, loss = 0.23019575\n",
      "Iteration 231, loss = 0.22930566\n",
      "Iteration 232, loss = 0.22846814\n",
      "Iteration 233, loss = 0.22756555\n",
      "Iteration 234, loss = 0.22669230\n",
      "Iteration 235, loss = 0.22591167\n",
      "Iteration 236, loss = 0.22497704\n",
      "Iteration 237, loss = 0.22414021\n",
      "Iteration 238, loss = 0.22326253\n",
      "Iteration 239, loss = 0.22250627\n",
      "Iteration 240, loss = 0.22160090\n",
      "Iteration 241, loss = 0.22073382\n",
      "Iteration 242, loss = 0.21995788\n",
      "Iteration 243, loss = 0.21910476\n",
      "Iteration 244, loss = 0.21836591\n",
      "Iteration 245, loss = 0.21757139\n",
      "Iteration 246, loss = 0.21679462\n",
      "Iteration 247, loss = 0.21600385\n",
      "Iteration 248, loss = 0.21522170\n",
      "Iteration 249, loss = 0.21447170\n",
      "Iteration 250, loss = 0.21367694\n",
      "Iteration 251, loss = 0.21288495\n",
      "Iteration 252, loss = 0.21215014\n",
      "Iteration 253, loss = 0.21144435\n",
      "Iteration 254, loss = 0.21067351\n",
      "Iteration 255, loss = 0.20993878\n",
      "Iteration 256, loss = 0.20923335\n",
      "Iteration 257, loss = 0.20845801\n",
      "Iteration 258, loss = 0.20774688\n",
      "Iteration 259, loss = 0.20705953\n",
      "Iteration 260, loss = 0.20639054\n",
      "Iteration 261, loss = 0.20569114\n",
      "Iteration 262, loss = 0.20498466\n",
      "Iteration 263, loss = 0.20426783\n",
      "Iteration 264, loss = 0.20357339\n",
      "Iteration 265, loss = 0.20292079\n",
      "Iteration 266, loss = 0.20224478\n",
      "Iteration 267, loss = 0.20160966\n",
      "Iteration 268, loss = 0.20097112\n",
      "Iteration 269, loss = 0.20033328\n",
      "Iteration 270, loss = 0.19966164\n",
      "Iteration 271, loss = 0.19899416\n",
      "Iteration 272, loss = 0.19836304\n",
      "Iteration 273, loss = 0.19771383\n",
      "Iteration 274, loss = 0.19711500\n",
      "Iteration 275, loss = 0.19649208\n",
      "Iteration 276, loss = 0.19585699\n",
      "Iteration 277, loss = 0.19523680\n",
      "Iteration 278, loss = 0.19466099\n",
      "Iteration 279, loss = 0.19406207\n",
      "Iteration 280, loss = 0.19347839\n",
      "Iteration 281, loss = 0.19291025\n",
      "Iteration 282, loss = 0.19227442\n",
      "Iteration 283, loss = 0.19170849\n",
      "Iteration 284, loss = 0.19111324\n",
      "Iteration 285, loss = 0.19051264\n",
      "Iteration 286, loss = 0.18990970\n",
      "Iteration 287, loss = 0.18934256\n",
      "Iteration 288, loss = 0.18875987\n",
      "Iteration 289, loss = 0.18822009\n",
      "Iteration 290, loss = 0.18768344\n",
      "Iteration 291, loss = 0.18713287\n",
      "Iteration 292, loss = 0.18658706\n",
      "Iteration 293, loss = 0.18598291\n",
      "Iteration 294, loss = 0.18549691\n",
      "Iteration 295, loss = 0.18491472\n",
      "Iteration 296, loss = 0.18439284\n",
      "Iteration 297, loss = 0.18391226\n",
      "Iteration 298, loss = 0.18342677\n",
      "Iteration 299, loss = 0.18290878\n",
      "Iteration 300, loss = 0.18235590\n",
      "Iteration 301, loss = 0.18178358\n",
      "Iteration 302, loss = 0.18123070\n",
      "Iteration 303, loss = 0.18072196\n",
      "Iteration 304, loss = 0.18024128\n",
      "Iteration 305, loss = 0.17973287\n",
      "Iteration 306, loss = 0.17921729\n",
      "Iteration 307, loss = 0.17869725\n",
      "Iteration 308, loss = 0.17824731\n",
      "Iteration 309, loss = 0.17771305\n",
      "Iteration 310, loss = 0.17725059\n",
      "Iteration 311, loss = 0.17676970\n",
      "Iteration 312, loss = 0.17632477\n",
      "Iteration 313, loss = 0.17587924\n",
      "Iteration 314, loss = 0.17536011\n",
      "Iteration 315, loss = 0.17488743\n",
      "Iteration 316, loss = 0.17442914\n",
      "Iteration 317, loss = 0.17403125\n",
      "Iteration 318, loss = 0.17353576\n",
      "Iteration 319, loss = 0.17306592\n",
      "Iteration 320, loss = 0.17262797\n",
      "Iteration 321, loss = 0.17221747\n",
      "Iteration 322, loss = 0.17175553\n",
      "Iteration 323, loss = 0.17130043\n",
      "Iteration 324, loss = 0.17087675\n",
      "Iteration 325, loss = 0.17041987\n",
      "Iteration 326, loss = 0.16994666\n",
      "Iteration 327, loss = 0.16951350\n",
      "Iteration 328, loss = 0.16907674\n",
      "Iteration 329, loss = 0.16863577\n",
      "Iteration 330, loss = 0.16821383\n",
      "Iteration 331, loss = 0.16779445\n",
      "Iteration 332, loss = 0.16737965\n",
      "Iteration 333, loss = 0.16697468\n",
      "Iteration 334, loss = 0.16655827\n",
      "Iteration 335, loss = 0.16614825\n",
      "Iteration 336, loss = 0.16577312\n",
      "Iteration 337, loss = 0.16531721\n",
      "Iteration 338, loss = 0.16493985\n",
      "Iteration 339, loss = 0.16454227\n",
      "Iteration 340, loss = 0.16413698\n",
      "Iteration 341, loss = 0.16375596\n",
      "Iteration 342, loss = 0.16335362\n",
      "Iteration 343, loss = 0.16297576\n",
      "Iteration 344, loss = 0.16258688\n",
      "Iteration 345, loss = 0.16217785\n",
      "Iteration 346, loss = 0.16178263\n",
      "Iteration 347, loss = 0.16142918\n",
      "Iteration 348, loss = 0.16102439\n",
      "Iteration 349, loss = 0.16065202\n",
      "Iteration 350, loss = 0.16029883\n",
      "Iteration 351, loss = 0.15992376\n",
      "Iteration 352, loss = 0.15954425\n",
      "Iteration 353, loss = 0.15918279\n",
      "Iteration 354, loss = 0.15879754\n",
      "Iteration 355, loss = 0.15844051\n",
      "Iteration 356, loss = 0.15811881\n",
      "Iteration 357, loss = 0.15775353\n",
      "Iteration 358, loss = 0.15734587\n",
      "Iteration 359, loss = 0.15700807\n",
      "Iteration 360, loss = 0.15665754\n",
      "Iteration 361, loss = 0.15632213\n",
      "Iteration 362, loss = 0.15597857\n",
      "Iteration 363, loss = 0.15562802\n",
      "Iteration 364, loss = 0.15527577\n",
      "Iteration 365, loss = 0.15496001\n",
      "Iteration 366, loss = 0.15460316\n",
      "Iteration 367, loss = 0.15425642\n",
      "Iteration 368, loss = 0.15389980\n",
      "Iteration 369, loss = 0.15359134\n",
      "Iteration 370, loss = 0.15325290\n",
      "Iteration 371, loss = 0.15290709\n",
      "Iteration 372, loss = 0.15261211\n",
      "Iteration 373, loss = 0.15228493\n",
      "Iteration 374, loss = 0.15195526\n",
      "Iteration 375, loss = 0.15160853\n",
      "Iteration 376, loss = 0.15129863\n",
      "Iteration 377, loss = 0.15095227\n",
      "Iteration 378, loss = 0.15066766\n",
      "Iteration 379, loss = 0.15038424\n",
      "Iteration 380, loss = 0.15005815\n",
      "Iteration 381, loss = 0.14971374\n",
      "Iteration 382, loss = 0.14938258\n",
      "Iteration 383, loss = 0.14906646\n",
      "Iteration 384, loss = 0.14876576\n",
      "Iteration 385, loss = 0.14846047\n",
      "Iteration 386, loss = 0.14814314\n",
      "Iteration 387, loss = 0.14786037\n",
      "Iteration 388, loss = 0.14756064\n",
      "Iteration 389, loss = 0.14731010\n",
      "Iteration 390, loss = 0.14699469\n",
      "Iteration 391, loss = 0.14672847\n",
      "Iteration 392, loss = 0.14641074\n",
      "Iteration 393, loss = 0.14609557\n",
      "Iteration 394, loss = 0.14578148\n",
      "Iteration 395, loss = 0.14548886\n",
      "Iteration 396, loss = 0.14518279\n",
      "Iteration 397, loss = 0.14489293\n",
      "Iteration 398, loss = 0.14459176\n",
      "Iteration 399, loss = 0.14429638\n",
      "Iteration 400, loss = 0.14407248\n",
      "Iteration 401, loss = 0.14377282\n",
      "Iteration 402, loss = 0.14350116\n",
      "Iteration 403, loss = 0.14324145\n",
      "Iteration 404, loss = 0.14296377\n",
      "Iteration 405, loss = 0.14265441\n",
      "Iteration 406, loss = 0.14237551\n",
      "Iteration 407, loss = 0.14208904\n",
      "Iteration 408, loss = 0.14181965\n",
      "Iteration 409, loss = 0.14154140\n",
      "Iteration 410, loss = 0.14128077\n",
      "Iteration 411, loss = 0.14105080\n",
      "Iteration 412, loss = 0.14076680\n",
      "Iteration 413, loss = 0.14049146\n",
      "Iteration 414, loss = 0.14022837\n",
      "Iteration 415, loss = 0.13998879\n",
      "Iteration 416, loss = 0.13972572\n",
      "Iteration 417, loss = 0.13948742\n",
      "Iteration 418, loss = 0.13923036\n",
      "Iteration 419, loss = 0.13896590\n",
      "Iteration 420, loss = 0.13869879\n",
      "Iteration 421, loss = 0.13846699\n",
      "Iteration 422, loss = 0.13820010\n",
      "Iteration 423, loss = 0.13796118\n",
      "Iteration 424, loss = 0.13773854\n",
      "Iteration 425, loss = 0.13744608\n",
      "Iteration 426, loss = 0.13723839\n",
      "Iteration 427, loss = 0.13699852\n",
      "Iteration 428, loss = 0.13675727\n",
      "Iteration 429, loss = 0.13650175\n",
      "Iteration 430, loss = 0.13624893\n",
      "Iteration 431, loss = 0.13601711\n",
      "Iteration 432, loss = 0.13575596\n",
      "Iteration 433, loss = 0.13552350\n",
      "Iteration 434, loss = 0.13532070\n",
      "Iteration 435, loss = 0.13508038\n",
      "Iteration 436, loss = 0.13486229\n",
      "Iteration 437, loss = 0.13460761\n",
      "Iteration 438, loss = 0.13439453\n",
      "Iteration 439, loss = 0.13417706\n",
      "Iteration 440, loss = 0.13392567\n",
      "Iteration 441, loss = 0.13369590\n",
      "Iteration 442, loss = 0.13346602\n",
      "Iteration 443, loss = 0.13324990\n",
      "Iteration 444, loss = 0.13303954\n",
      "Iteration 445, loss = 0.13279258\n",
      "Iteration 446, loss = 0.13256288\n",
      "Iteration 447, loss = 0.13234620\n",
      "Iteration 448, loss = 0.13214444\n",
      "Iteration 449, loss = 0.13191589\n",
      "Iteration 450, loss = 0.13166724\n",
      "Iteration 451, loss = 0.13146078\n",
      "Iteration 452, loss = 0.13126179\n",
      "Iteration 453, loss = 0.13104503\n",
      "Iteration 454, loss = 0.13082778\n",
      "Iteration 455, loss = 0.13059835\n",
      "Iteration 456, loss = 0.13038701\n",
      "Iteration 457, loss = 0.13019814\n",
      "Iteration 458, loss = 0.12996463\n",
      "Iteration 459, loss = 0.12978641\n",
      "Iteration 460, loss = 0.12957158\n",
      "Iteration 461, loss = 0.12935524\n",
      "Iteration 462, loss = 0.12914740\n",
      "Iteration 463, loss = 0.12895610\n",
      "Iteration 464, loss = 0.12874194\n",
      "Iteration 465, loss = 0.12855402\n",
      "Iteration 466, loss = 0.12836401\n",
      "Iteration 467, loss = 0.12815320\n",
      "Iteration 468, loss = 0.12796262\n",
      "Iteration 469, loss = 0.12776462\n",
      "Iteration 470, loss = 0.12756711\n",
      "Iteration 471, loss = 0.12739660\n",
      "Iteration 472, loss = 0.12717732\n",
      "Iteration 473, loss = 0.12699584\n",
      "Iteration 474, loss = 0.12678126\n",
      "Iteration 475, loss = 0.12660375\n",
      "Iteration 476, loss = 0.12639320\n",
      "Iteration 477, loss = 0.12618452\n",
      "Iteration 478, loss = 0.12599707\n",
      "Iteration 479, loss = 0.12581649\n",
      "Iteration 480, loss = 0.12563011\n",
      "Iteration 481, loss = 0.12544571\n",
      "Iteration 482, loss = 0.12529515\n",
      "Iteration 483, loss = 0.12510544\n",
      "Iteration 484, loss = 0.12488946\n",
      "Iteration 485, loss = 0.12469141\n",
      "Iteration 486, loss = 0.12450319\n",
      "Iteration 487, loss = 0.12432665\n",
      "Iteration 488, loss = 0.12415675\n",
      "Iteration 489, loss = 0.12398420\n",
      "Iteration 490, loss = 0.12376861\n",
      "Iteration 491, loss = 0.12361216\n",
      "Iteration 492, loss = 0.12343104\n",
      "Iteration 493, loss = 0.12324127\n",
      "Iteration 494, loss = 0.12307444\n",
      "Iteration 495, loss = 0.12290321\n",
      "Iteration 496, loss = 0.12271754\n",
      "Iteration 497, loss = 0.12254253\n",
      "Iteration 498, loss = 0.12236548\n",
      "Iteration 499, loss = 0.12218781\n",
      "Iteration 500, loss = 0.12200079\n",
      "Iteration 501, loss = 0.12183575\n",
      "Iteration 502, loss = 0.12166192\n",
      "Iteration 503, loss = 0.12150537\n",
      "Iteration 504, loss = 0.12133136\n",
      "Iteration 505, loss = 0.12115601\n",
      "Iteration 506, loss = 0.12098513\n",
      "Iteration 507, loss = 0.12081866\n",
      "Iteration 508, loss = 0.12067188\n",
      "Iteration 509, loss = 0.12049671\n",
      "Iteration 510, loss = 0.12031816\n",
      "Iteration 511, loss = 0.12015586\n",
      "Iteration 512, loss = 0.12000373\n",
      "Iteration 513, loss = 0.11985641\n",
      "Iteration 514, loss = 0.11967896\n",
      "Iteration 515, loss = 0.11954708\n",
      "Iteration 516, loss = 0.11940473\n",
      "Iteration 517, loss = 0.11922779\n",
      "Iteration 518, loss = 0.11908909\n",
      "Iteration 519, loss = 0.11891305\n",
      "Iteration 520, loss = 0.11875697\n",
      "Iteration 521, loss = 0.11860790\n",
      "Iteration 522, loss = 0.11843744\n",
      "Iteration 523, loss = 0.11829656\n",
      "Iteration 524, loss = 0.11814046\n",
      "Iteration 525, loss = 0.11798113\n",
      "Iteration 526, loss = 0.11782137\n",
      "Iteration 527, loss = 0.11768934\n",
      "Iteration 528, loss = 0.11754192\n",
      "Iteration 529, loss = 0.11740003\n",
      "Iteration 530, loss = 0.11724585\n",
      "Iteration 531, loss = 0.11709649\n",
      "Iteration 532, loss = 0.11695209\n",
      "Iteration 533, loss = 0.11679193\n",
      "Iteration 534, loss = 0.11667506\n",
      "Iteration 535, loss = 0.11655392\n",
      "Iteration 536, loss = 0.11637649\n",
      "Iteration 537, loss = 0.11622466\n",
      "Iteration 538, loss = 0.11608981\n",
      "Iteration 539, loss = 0.11593732\n",
      "Iteration 540, loss = 0.11579934\n",
      "Iteration 541, loss = 0.11564520\n",
      "Iteration 542, loss = 0.11550759\n",
      "Iteration 543, loss = 0.11536959\n",
      "Iteration 544, loss = 0.11522537\n",
      "Iteration 545, loss = 0.11508755\n",
      "Iteration 546, loss = 0.11496132\n",
      "Iteration 547, loss = 0.11481393\n",
      "Iteration 548, loss = 0.11467912\n",
      "Iteration 549, loss = 0.11455688\n",
      "Iteration 550, loss = 0.11442270\n",
      "Iteration 551, loss = 0.11430385\n",
      "Iteration 552, loss = 0.11415629\n",
      "Iteration 553, loss = 0.11400877\n",
      "Iteration 554, loss = 0.11388256\n",
      "Iteration 555, loss = 0.11373375\n",
      "Iteration 556, loss = 0.11360443\n",
      "Iteration 557, loss = 0.11347364\n",
      "Iteration 558, loss = 0.11335170\n",
      "Iteration 559, loss = 0.11321963\n",
      "Iteration 560, loss = 0.11309163\n",
      "Iteration 561, loss = 0.11296569\n",
      "Iteration 562, loss = 0.11283688\n",
      "Iteration 563, loss = 0.11270871\n",
      "Iteration 564, loss = 0.11258388\n",
      "Iteration 565, loss = 0.11245527\n",
      "Iteration 566, loss = 0.11232920\n",
      "Iteration 567, loss = 0.11220452\n",
      "Iteration 568, loss = 0.11206261\n",
      "Iteration 569, loss = 0.11193669\n",
      "Iteration 570, loss = 0.11180063\n",
      "Iteration 571, loss = 0.11168871\n",
      "Iteration 572, loss = 0.11155527\n",
      "Iteration 573, loss = 0.11143006\n",
      "Iteration 574, loss = 0.11132062\n",
      "Iteration 575, loss = 0.11119679\n",
      "Iteration 576, loss = 0.11107492\n",
      "Iteration 577, loss = 0.11094873\n",
      "Iteration 578, loss = 0.11083065\n",
      "Iteration 579, loss = 0.11071301\n",
      "Iteration 580, loss = 0.11058661\n",
      "Iteration 581, loss = 0.11047941\n",
      "Iteration 582, loss = 0.11036877\n",
      "Iteration 583, loss = 0.11024180\n",
      "Iteration 584, loss = 0.11010086\n",
      "Iteration 585, loss = 0.10999542\n",
      "Iteration 586, loss = 0.10987408\n",
      "Iteration 587, loss = 0.10977255\n",
      "Iteration 588, loss = 0.10964446\n",
      "Iteration 589, loss = 0.10953016\n",
      "Iteration 590, loss = 0.10941315\n",
      "Iteration 591, loss = 0.10930221\n",
      "Iteration 592, loss = 0.10917562\n",
      "Iteration 593, loss = 0.10906679\n",
      "Iteration 594, loss = 0.10893955\n",
      "Iteration 595, loss = 0.10883183\n",
      "Iteration 596, loss = 0.10871521\n",
      "Iteration 597, loss = 0.10858144\n",
      "Iteration 598, loss = 0.10847668\n",
      "Iteration 599, loss = 0.10837595\n",
      "Iteration 600, loss = 0.10825531\n",
      "Iteration 601, loss = 0.10815382\n",
      "Iteration 602, loss = 0.10804517\n",
      "Iteration 603, loss = 0.10794652\n",
      "Iteration 604, loss = 0.10782129\n",
      "Iteration 605, loss = 0.10771667\n",
      "Iteration 606, loss = 0.10759859\n",
      "Iteration 607, loss = 0.10749688\n",
      "Iteration 608, loss = 0.10738553\n",
      "Iteration 609, loss = 0.10729319\n",
      "Iteration 610, loss = 0.10718188\n",
      "Iteration 611, loss = 0.10707637\n",
      "Iteration 612, loss = 0.10697385\n",
      "Iteration 613, loss = 0.10688513\n",
      "Iteration 614, loss = 0.10676665\n",
      "Iteration 615, loss = 0.10666101\n",
      "Iteration 616, loss = 0.10654512\n",
      "Iteration 617, loss = 0.10644605\n",
      "Iteration 618, loss = 0.10635054\n",
      "Iteration 619, loss = 0.10623919\n",
      "Iteration 620, loss = 0.10614236\n",
      "Iteration 621, loss = 0.10603721\n",
      "Iteration 622, loss = 0.10593986\n",
      "Iteration 623, loss = 0.10583496\n",
      "Iteration 624, loss = 0.10574904\n",
      "Iteration 625, loss = 0.10563156\n",
      "Iteration 626, loss = 0.10554054\n",
      "Iteration 627, loss = 0.10544646\n",
      "Iteration 628, loss = 0.10534923\n",
      "Iteration 629, loss = 0.10523520\n",
      "Iteration 630, loss = 0.10514059\n",
      "Iteration 631, loss = 0.10503914\n",
      "Iteration 632, loss = 0.10495456\n",
      "Iteration 633, loss = 0.10484057\n",
      "Iteration 634, loss = 0.10475326\n",
      "Iteration 635, loss = 0.10466462\n",
      "Iteration 636, loss = 0.10454809\n",
      "Iteration 637, loss = 0.10446031\n",
      "Iteration 638, loss = 0.10436582\n",
      "Iteration 639, loss = 0.10427387\n",
      "Iteration 640, loss = 0.10418884\n",
      "Iteration 641, loss = 0.10408698\n",
      "Iteration 642, loss = 0.10399467\n",
      "Iteration 643, loss = 0.10388001\n",
      "Iteration 644, loss = 0.10378454\n",
      "Iteration 645, loss = 0.10368204\n",
      "Iteration 646, loss = 0.10359670\n",
      "Iteration 647, loss = 0.10350916\n",
      "Iteration 648, loss = 0.10342222\n",
      "Iteration 649, loss = 0.10332684\n",
      "Iteration 650, loss = 0.10322093\n",
      "Iteration 651, loss = 0.10314906\n",
      "Iteration 652, loss = 0.10305585\n",
      "Iteration 653, loss = 0.10295772\n",
      "Iteration 654, loss = 0.10286557\n",
      "Iteration 655, loss = 0.10276951\n",
      "Iteration 656, loss = 0.10268608\n",
      "Iteration 657, loss = 0.10259239\n",
      "Iteration 658, loss = 0.10250900\n",
      "Iteration 659, loss = 0.10241422\n",
      "Iteration 660, loss = 0.10232893\n",
      "Iteration 661, loss = 0.10224548\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.42092212\n",
      "Iteration 2, loss = 2.40208043\n",
      "Iteration 3, loss = 2.37739411\n",
      "Iteration 4, loss = 2.35105152\n",
      "Iteration 5, loss = 2.32540799\n",
      "Iteration 6, loss = 2.30074597\n",
      "Iteration 7, loss = 2.27670166\n",
      "Iteration 8, loss = 2.25343396\n",
      "Iteration 9, loss = 2.23035296\n",
      "Iteration 10, loss = 2.20700521\n",
      "Iteration 11, loss = 2.18333781\n",
      "Iteration 12, loss = 2.15980677\n",
      "Iteration 13, loss = 2.13541492\n",
      "Iteration 14, loss = 2.11063490\n",
      "Iteration 15, loss = 2.08546058\n",
      "Iteration 16, loss = 2.05990637\n",
      "Iteration 17, loss = 2.03383771\n",
      "Iteration 18, loss = 2.00702747\n",
      "Iteration 19, loss = 1.98002342\n",
      "Iteration 20, loss = 1.95272294\n",
      "Iteration 21, loss = 1.92493332\n",
      "Iteration 22, loss = 1.89655142\n",
      "Iteration 23, loss = 1.86809306\n",
      "Iteration 24, loss = 1.83934022\n",
      "Iteration 25, loss = 1.81041735\n",
      "Iteration 26, loss = 1.78140728\n",
      "Iteration 27, loss = 1.75229465\n",
      "Iteration 28, loss = 1.72285852\n",
      "Iteration 29, loss = 1.69339253\n",
      "Iteration 30, loss = 1.66398583\n",
      "Iteration 31, loss = 1.63439041\n",
      "Iteration 32, loss = 1.60529638\n",
      "Iteration 33, loss = 1.57604359\n",
      "Iteration 34, loss = 1.54698585\n",
      "Iteration 35, loss = 1.51849011\n",
      "Iteration 36, loss = 1.48991653\n",
      "Iteration 37, loss = 1.46188464\n",
      "Iteration 38, loss = 1.43399868\n",
      "Iteration 39, loss = 1.40663291\n",
      "Iteration 40, loss = 1.37945279\n",
      "Iteration 41, loss = 1.35305207\n",
      "Iteration 42, loss = 1.32686555\n",
      "Iteration 43, loss = 1.30122398\n",
      "Iteration 44, loss = 1.27575922\n",
      "Iteration 45, loss = 1.25096453\n",
      "Iteration 46, loss = 1.22682328\n",
      "Iteration 47, loss = 1.20318612\n",
      "Iteration 48, loss = 1.17992123\n",
      "Iteration 49, loss = 1.15720564\n",
      "Iteration 50, loss = 1.13510109\n",
      "Iteration 51, loss = 1.11376866\n",
      "Iteration 52, loss = 1.09261867\n",
      "Iteration 53, loss = 1.07200564\n",
      "Iteration 54, loss = 1.05189011\n",
      "Iteration 55, loss = 1.03218029\n",
      "Iteration 56, loss = 1.01338941\n",
      "Iteration 57, loss = 0.99499889\n",
      "Iteration 58, loss = 0.97701022\n",
      "Iteration 59, loss = 0.95974632\n",
      "Iteration 60, loss = 0.94300323\n",
      "Iteration 61, loss = 0.92641635\n",
      "Iteration 62, loss = 0.91081464\n",
      "Iteration 63, loss = 0.89528968\n",
      "Iteration 64, loss = 0.88000389\n",
      "Iteration 65, loss = 0.86506822\n",
      "Iteration 66, loss = 0.85083706\n",
      "Iteration 67, loss = 0.83706678\n",
      "Iteration 68, loss = 0.82364491\n",
      "Iteration 69, loss = 0.81063263\n",
      "Iteration 70, loss = 0.79800628\n",
      "Iteration 71, loss = 0.78579742\n",
      "Iteration 72, loss = 0.77380534\n",
      "Iteration 73, loss = 0.76220705\n",
      "Iteration 74, loss = 0.75096381\n",
      "Iteration 75, loss = 0.74013426\n",
      "Iteration 76, loss = 0.72940538\n",
      "Iteration 77, loss = 0.71895887\n",
      "Iteration 78, loss = 0.70900119\n",
      "Iteration 79, loss = 0.69909499\n",
      "Iteration 80, loss = 0.68969460\n",
      "Iteration 81, loss = 0.68037735\n",
      "Iteration 82, loss = 0.67139865\n",
      "Iteration 83, loss = 0.66268248\n",
      "Iteration 84, loss = 0.65403480\n",
      "Iteration 85, loss = 0.64563465\n",
      "Iteration 86, loss = 0.63754091\n",
      "Iteration 87, loss = 0.62957556\n",
      "Iteration 88, loss = 0.62171556\n",
      "Iteration 89, loss = 0.61416204\n",
      "Iteration 90, loss = 0.60710146\n",
      "Iteration 91, loss = 0.59980901\n",
      "Iteration 92, loss = 0.59272698\n",
      "Iteration 93, loss = 0.58598075\n",
      "Iteration 94, loss = 0.57924796\n",
      "Iteration 95, loss = 0.57270156\n",
      "Iteration 96, loss = 0.56650197\n",
      "Iteration 97, loss = 0.56012519\n",
      "Iteration 98, loss = 0.55402459\n",
      "Iteration 99, loss = 0.54813682\n",
      "Iteration 100, loss = 0.54241828\n",
      "Iteration 101, loss = 0.53674002\n",
      "Iteration 102, loss = 0.53117758\n",
      "Iteration 103, loss = 0.52582214\n",
      "Iteration 104, loss = 0.52050761\n",
      "Iteration 105, loss = 0.51534480\n",
      "Iteration 106, loss = 0.51035424\n",
      "Iteration 107, loss = 0.50543863\n",
      "Iteration 108, loss = 0.50060621\n",
      "Iteration 109, loss = 0.49584365\n",
      "Iteration 110, loss = 0.49117559\n",
      "Iteration 111, loss = 0.48654941\n",
      "Iteration 112, loss = 0.48218691\n",
      "Iteration 113, loss = 0.47792160\n",
      "Iteration 114, loss = 0.47350064\n",
      "Iteration 115, loss = 0.46929954\n",
      "Iteration 116, loss = 0.46519569\n",
      "Iteration 117, loss = 0.46098122\n",
      "Iteration 118, loss = 0.45701774\n",
      "Iteration 119, loss = 0.45317500\n",
      "Iteration 120, loss = 0.44939445\n",
      "Iteration 121, loss = 0.44547420\n",
      "Iteration 122, loss = 0.44172058\n",
      "Iteration 123, loss = 0.43804112\n",
      "Iteration 124, loss = 0.43453805\n",
      "Iteration 125, loss = 0.43104228\n",
      "Iteration 126, loss = 0.42744669\n",
      "Iteration 127, loss = 0.42400480\n",
      "Iteration 128, loss = 0.42058299\n",
      "Iteration 129, loss = 0.41739425\n",
      "Iteration 130, loss = 0.41424313\n",
      "Iteration 131, loss = 0.41104190\n",
      "Iteration 132, loss = 0.40797603\n",
      "Iteration 133, loss = 0.40489006\n",
      "Iteration 134, loss = 0.40189736\n",
      "Iteration 135, loss = 0.39896401\n",
      "Iteration 136, loss = 0.39601090\n",
      "Iteration 137, loss = 0.39321445\n",
      "Iteration 138, loss = 0.39033853\n",
      "Iteration 139, loss = 0.38756672\n",
      "Iteration 140, loss = 0.38472012\n",
      "Iteration 141, loss = 0.38207875\n",
      "Iteration 142, loss = 0.37940804\n",
      "Iteration 143, loss = 0.37677965\n",
      "Iteration 144, loss = 0.37425744\n",
      "Iteration 145, loss = 0.37176291\n",
      "Iteration 146, loss = 0.36936384\n",
      "Iteration 147, loss = 0.36679194\n",
      "Iteration 148, loss = 0.36432064\n",
      "Iteration 149, loss = 0.36194757\n",
      "Iteration 150, loss = 0.35964390\n",
      "Iteration 151, loss = 0.35730946\n",
      "Iteration 152, loss = 0.35505694\n",
      "Iteration 153, loss = 0.35284440\n",
      "Iteration 154, loss = 0.35056148\n",
      "Iteration 155, loss = 0.34839780\n",
      "Iteration 156, loss = 0.34610239\n",
      "Iteration 157, loss = 0.34401762\n",
      "Iteration 158, loss = 0.34197257\n",
      "Iteration 159, loss = 0.33980286\n",
      "Iteration 160, loss = 0.33770221\n",
      "Iteration 161, loss = 0.33570675\n",
      "Iteration 162, loss = 0.33386908\n",
      "Iteration 163, loss = 0.33179058\n",
      "Iteration 164, loss = 0.32988746\n",
      "Iteration 165, loss = 0.32793951\n",
      "Iteration 166, loss = 0.32594790\n",
      "Iteration 167, loss = 0.32411822\n",
      "Iteration 168, loss = 0.32240605\n",
      "Iteration 169, loss = 0.32048908\n",
      "Iteration 170, loss = 0.31872361\n",
      "Iteration 171, loss = 0.31679844\n",
      "Iteration 172, loss = 0.31504933\n",
      "Iteration 173, loss = 0.31328085\n",
      "Iteration 174, loss = 0.31161191\n",
      "Iteration 175, loss = 0.30988445\n",
      "Iteration 176, loss = 0.30817609\n",
      "Iteration 177, loss = 0.30646308\n",
      "Iteration 178, loss = 0.30494841\n",
      "Iteration 179, loss = 0.30321952\n",
      "Iteration 180, loss = 0.30161759\n",
      "Iteration 181, loss = 0.30005320\n",
      "Iteration 182, loss = 0.29853784\n",
      "Iteration 183, loss = 0.29694428\n",
      "Iteration 184, loss = 0.29531416\n",
      "Iteration 185, loss = 0.29383755\n",
      "Iteration 186, loss = 0.29226656\n",
      "Iteration 187, loss = 0.29076802\n",
      "Iteration 188, loss = 0.28929095\n",
      "Iteration 189, loss = 0.28792758\n",
      "Iteration 190, loss = 0.28648216\n",
      "Iteration 191, loss = 0.28503499\n",
      "Iteration 192, loss = 0.28361960\n",
      "Iteration 193, loss = 0.28223927\n",
      "Iteration 194, loss = 0.28086060\n",
      "Iteration 195, loss = 0.27948021\n",
      "Iteration 196, loss = 0.27812076\n",
      "Iteration 197, loss = 0.27681150\n",
      "Iteration 198, loss = 0.27549510\n",
      "Iteration 199, loss = 0.27428771\n",
      "Iteration 200, loss = 0.27293629\n",
      "Iteration 201, loss = 0.27161135\n",
      "Iteration 202, loss = 0.27043472\n",
      "Iteration 203, loss = 0.26917213\n",
      "Iteration 204, loss = 0.26799119\n",
      "Iteration 205, loss = 0.26672245\n",
      "Iteration 206, loss = 0.26545937\n",
      "Iteration 207, loss = 0.26423361\n",
      "Iteration 208, loss = 0.26301013\n",
      "Iteration 209, loss = 0.26197071\n",
      "Iteration 210, loss = 0.26080364\n",
      "Iteration 211, loss = 0.25961296\n",
      "Iteration 212, loss = 0.25849116\n",
      "Iteration 213, loss = 0.25735658\n",
      "Iteration 214, loss = 0.25624003\n",
      "Iteration 215, loss = 0.25513802\n",
      "Iteration 216, loss = 0.25405455\n",
      "Iteration 217, loss = 0.25296845\n",
      "Iteration 218, loss = 0.25198266\n",
      "Iteration 219, loss = 0.25088652\n",
      "Iteration 220, loss = 0.24988136\n",
      "Iteration 221, loss = 0.24887780\n",
      "Iteration 222, loss = 0.24777939\n",
      "Iteration 223, loss = 0.24664934\n",
      "Iteration 224, loss = 0.24567147\n",
      "Iteration 225, loss = 0.24470106\n",
      "Iteration 226, loss = 0.24359395\n",
      "Iteration 227, loss = 0.24257235\n",
      "Iteration 228, loss = 0.24161383\n",
      "Iteration 229, loss = 0.24063491\n",
      "Iteration 230, loss = 0.23968050\n",
      "Iteration 231, loss = 0.23865289\n",
      "Iteration 232, loss = 0.23771579\n",
      "Iteration 233, loss = 0.23675598\n",
      "Iteration 234, loss = 0.23581197\n",
      "Iteration 235, loss = 0.23487726\n",
      "Iteration 236, loss = 0.23397396\n",
      "Iteration 237, loss = 0.23313359\n",
      "Iteration 238, loss = 0.23218487\n",
      "Iteration 239, loss = 0.23133047\n",
      "Iteration 240, loss = 0.23040388\n",
      "Iteration 241, loss = 0.22952121\n",
      "Iteration 242, loss = 0.22862343\n",
      "Iteration 243, loss = 0.22778048\n",
      "Iteration 244, loss = 0.22700795\n",
      "Iteration 245, loss = 0.22615654\n",
      "Iteration 246, loss = 0.22528273\n",
      "Iteration 247, loss = 0.22447583\n",
      "Iteration 248, loss = 0.22364981\n",
      "Iteration 249, loss = 0.22281259\n",
      "Iteration 250, loss = 0.22205533\n",
      "Iteration 251, loss = 0.22129329\n",
      "Iteration 252, loss = 0.22049479\n",
      "Iteration 253, loss = 0.21967920\n",
      "Iteration 254, loss = 0.21891121\n",
      "Iteration 255, loss = 0.21818345\n",
      "Iteration 256, loss = 0.21744949\n",
      "Iteration 257, loss = 0.21667076\n",
      "Iteration 258, loss = 0.21596479\n",
      "Iteration 259, loss = 0.21518101\n",
      "Iteration 260, loss = 0.21447581\n",
      "Iteration 261, loss = 0.21372662\n",
      "Iteration 262, loss = 0.21300716\n",
      "Iteration 263, loss = 0.21227038\n",
      "Iteration 264, loss = 0.21146496\n",
      "Iteration 265, loss = 0.21078781\n",
      "Iteration 266, loss = 0.21004661\n",
      "Iteration 267, loss = 0.20937932\n",
      "Iteration 268, loss = 0.20867413\n",
      "Iteration 269, loss = 0.20798802\n",
      "Iteration 270, loss = 0.20732372\n",
      "Iteration 271, loss = 0.20660491\n",
      "Iteration 272, loss = 0.20593220\n",
      "Iteration 273, loss = 0.20524409\n",
      "Iteration 274, loss = 0.20456830\n",
      "Iteration 275, loss = 0.20390927\n",
      "Iteration 276, loss = 0.20328831\n",
      "Iteration 277, loss = 0.20259527\n",
      "Iteration 278, loss = 0.20194709\n",
      "Iteration 279, loss = 0.20133117\n",
      "Iteration 280, loss = 0.20067224\n",
      "Iteration 281, loss = 0.20003723\n",
      "Iteration 282, loss = 0.19937199\n",
      "Iteration 283, loss = 0.19878998\n",
      "Iteration 284, loss = 0.19815173\n",
      "Iteration 285, loss = 0.19755527\n",
      "Iteration 286, loss = 0.19695274\n",
      "Iteration 287, loss = 0.19633581\n",
      "Iteration 288, loss = 0.19575294\n",
      "Iteration 289, loss = 0.19514208\n",
      "Iteration 290, loss = 0.19456595\n",
      "Iteration 291, loss = 0.19402413\n",
      "Iteration 292, loss = 0.19341886\n",
      "Iteration 293, loss = 0.19282734\n",
      "Iteration 294, loss = 0.19218644\n",
      "Iteration 295, loss = 0.19156303\n",
      "Iteration 296, loss = 0.19102657\n",
      "Iteration 297, loss = 0.19046442\n",
      "Iteration 298, loss = 0.18989908\n",
      "Iteration 299, loss = 0.18933342\n",
      "Iteration 300, loss = 0.18876050\n",
      "Iteration 301, loss = 0.18819967\n",
      "Iteration 302, loss = 0.18764593\n",
      "Iteration 303, loss = 0.18708555\n",
      "Iteration 304, loss = 0.18652853\n",
      "Iteration 305, loss = 0.18601136\n",
      "Iteration 306, loss = 0.18551238\n",
      "Iteration 307, loss = 0.18498082\n",
      "Iteration 308, loss = 0.18445880\n",
      "Iteration 309, loss = 0.18394894\n",
      "Iteration 310, loss = 0.18345505\n",
      "Iteration 311, loss = 0.18293396\n",
      "Iteration 312, loss = 0.18244392\n",
      "Iteration 313, loss = 0.18194595\n",
      "Iteration 314, loss = 0.18142219\n",
      "Iteration 315, loss = 0.18093612\n",
      "Iteration 316, loss = 0.18044003\n",
      "Iteration 317, loss = 0.17993467\n",
      "Iteration 318, loss = 0.17943160\n",
      "Iteration 319, loss = 0.17895360\n",
      "Iteration 320, loss = 0.17844573\n",
      "Iteration 321, loss = 0.17799144\n",
      "Iteration 322, loss = 0.17753073\n",
      "Iteration 323, loss = 0.17707867\n",
      "Iteration 324, loss = 0.17665409\n",
      "Iteration 325, loss = 0.17616990\n",
      "Iteration 326, loss = 0.17568970\n",
      "Iteration 327, loss = 0.17525479\n",
      "Iteration 328, loss = 0.17473694\n",
      "Iteration 329, loss = 0.17430753\n",
      "Iteration 330, loss = 0.17382551\n",
      "Iteration 331, loss = 0.17341693\n",
      "Iteration 332, loss = 0.17293278\n",
      "Iteration 333, loss = 0.17249268\n",
      "Iteration 334, loss = 0.17204087\n",
      "Iteration 335, loss = 0.17158507\n",
      "Iteration 336, loss = 0.17114591\n",
      "Iteration 337, loss = 0.17072207\n",
      "Iteration 338, loss = 0.17030361\n",
      "Iteration 339, loss = 0.16985226\n",
      "Iteration 340, loss = 0.16944577\n",
      "Iteration 341, loss = 0.16903931\n",
      "Iteration 342, loss = 0.16863903\n",
      "Iteration 343, loss = 0.16815182\n",
      "Iteration 344, loss = 0.16773844\n",
      "Iteration 345, loss = 0.16735000\n",
      "Iteration 346, loss = 0.16694582\n",
      "Iteration 347, loss = 0.16657469\n",
      "Iteration 348, loss = 0.16616106\n",
      "Iteration 349, loss = 0.16576600\n",
      "Iteration 350, loss = 0.16537769\n",
      "Iteration 351, loss = 0.16499238\n",
      "Iteration 352, loss = 0.16458437\n",
      "Iteration 353, loss = 0.16420233\n",
      "Iteration 354, loss = 0.16377914\n",
      "Iteration 355, loss = 0.16341877\n",
      "Iteration 356, loss = 0.16302693\n",
      "Iteration 357, loss = 0.16264071\n",
      "Iteration 358, loss = 0.16225738\n",
      "Iteration 359, loss = 0.16186726\n",
      "Iteration 360, loss = 0.16151526\n",
      "Iteration 361, loss = 0.16112824\n",
      "Iteration 362, loss = 0.16077642\n",
      "Iteration 363, loss = 0.16038817\n",
      "Iteration 364, loss = 0.16004472\n",
      "Iteration 365, loss = 0.15969063\n",
      "Iteration 366, loss = 0.15931717\n",
      "Iteration 367, loss = 0.15898973\n",
      "Iteration 368, loss = 0.15862926\n",
      "Iteration 369, loss = 0.15823393\n",
      "Iteration 370, loss = 0.15788295\n",
      "Iteration 371, loss = 0.15754173\n",
      "Iteration 372, loss = 0.15720440\n",
      "Iteration 373, loss = 0.15681426\n",
      "Iteration 374, loss = 0.15651627\n",
      "Iteration 375, loss = 0.15616785\n",
      "Iteration 376, loss = 0.15586834\n",
      "Iteration 377, loss = 0.15550823\n",
      "Iteration 378, loss = 0.15515764\n",
      "Iteration 379, loss = 0.15481288\n",
      "Iteration 380, loss = 0.15451445\n",
      "Iteration 381, loss = 0.15416049\n",
      "Iteration 382, loss = 0.15381467\n",
      "Iteration 383, loss = 0.15348354\n",
      "Iteration 384, loss = 0.15316608\n",
      "Iteration 385, loss = 0.15290255\n",
      "Iteration 386, loss = 0.15252045\n",
      "Iteration 387, loss = 0.15221252\n",
      "Iteration 388, loss = 0.15186598\n",
      "Iteration 389, loss = 0.15151781\n",
      "Iteration 390, loss = 0.15121616\n",
      "Iteration 391, loss = 0.15090546\n",
      "Iteration 392, loss = 0.15060026\n",
      "Iteration 393, loss = 0.15030914\n",
      "Iteration 394, loss = 0.14999982\n",
      "Iteration 395, loss = 0.14967398\n",
      "Iteration 396, loss = 0.14941091\n",
      "Iteration 397, loss = 0.14906787\n",
      "Iteration 398, loss = 0.14877465\n",
      "Iteration 399, loss = 0.14849432\n",
      "Iteration 400, loss = 0.14821105\n",
      "Iteration 401, loss = 0.14790652\n",
      "Iteration 402, loss = 0.14765161\n",
      "Iteration 403, loss = 0.14732456\n",
      "Iteration 404, loss = 0.14705706\n",
      "Iteration 405, loss = 0.14675161\n",
      "Iteration 406, loss = 0.14647734\n",
      "Iteration 407, loss = 0.14619725\n",
      "Iteration 408, loss = 0.14594813\n",
      "Iteration 409, loss = 0.14562478\n",
      "Iteration 410, loss = 0.14534236\n",
      "Iteration 411, loss = 0.14506380\n",
      "Iteration 412, loss = 0.14477059\n",
      "Iteration 413, loss = 0.14448575\n",
      "Iteration 414, loss = 0.14423031\n",
      "Iteration 415, loss = 0.14392012\n",
      "Iteration 416, loss = 0.14367685\n",
      "Iteration 417, loss = 0.14337763\n",
      "Iteration 418, loss = 0.14312613\n",
      "Iteration 419, loss = 0.14285553\n",
      "Iteration 420, loss = 0.14258716\n",
      "Iteration 421, loss = 0.14232231\n",
      "Iteration 422, loss = 0.14204325\n",
      "Iteration 423, loss = 0.14180742\n",
      "Iteration 424, loss = 0.14153010\n",
      "Iteration 425, loss = 0.14127002\n",
      "Iteration 426, loss = 0.14102172\n",
      "Iteration 427, loss = 0.14076557\n",
      "Iteration 428, loss = 0.14049904\n",
      "Iteration 429, loss = 0.14025503\n",
      "Iteration 430, loss = 0.13999930\n",
      "Iteration 431, loss = 0.13975716\n",
      "Iteration 432, loss = 0.13951726\n",
      "Iteration 433, loss = 0.13925618\n",
      "Iteration 434, loss = 0.13900862\n",
      "Iteration 435, loss = 0.13877107\n",
      "Iteration 436, loss = 0.13851943\n",
      "Iteration 437, loss = 0.13826095\n",
      "Iteration 438, loss = 0.13802999\n",
      "Iteration 439, loss = 0.13776961\n",
      "Iteration 440, loss = 0.13752011\n",
      "Iteration 441, loss = 0.13726879\n",
      "Iteration 442, loss = 0.13701893\n",
      "Iteration 443, loss = 0.13681113\n",
      "Iteration 444, loss = 0.13657310\n",
      "Iteration 445, loss = 0.13632188\n",
      "Iteration 446, loss = 0.13609237\n",
      "Iteration 447, loss = 0.13586409\n",
      "Iteration 448, loss = 0.13563548\n",
      "Iteration 449, loss = 0.13540691\n",
      "Iteration 450, loss = 0.13518563\n",
      "Iteration 451, loss = 0.13496140\n",
      "Iteration 452, loss = 0.13474026\n",
      "Iteration 453, loss = 0.13451929\n",
      "Iteration 454, loss = 0.13428549\n",
      "Iteration 455, loss = 0.13403968\n",
      "Iteration 456, loss = 0.13381841\n",
      "Iteration 457, loss = 0.13359964\n",
      "Iteration 458, loss = 0.13337473\n",
      "Iteration 459, loss = 0.13315615\n",
      "Iteration 460, loss = 0.13294396\n",
      "Iteration 461, loss = 0.13271353\n",
      "Iteration 462, loss = 0.13249283\n",
      "Iteration 463, loss = 0.13228044\n",
      "Iteration 464, loss = 0.13206812\n",
      "Iteration 465, loss = 0.13186770\n",
      "Iteration 466, loss = 0.13165347\n",
      "Iteration 467, loss = 0.13146135\n",
      "Iteration 468, loss = 0.13126202\n",
      "Iteration 469, loss = 0.13102574\n",
      "Iteration 470, loss = 0.13081671\n",
      "Iteration 471, loss = 0.13062154\n",
      "Iteration 472, loss = 0.13041249\n",
      "Iteration 473, loss = 0.13023239\n",
      "Iteration 474, loss = 0.13003105\n",
      "Iteration 475, loss = 0.12984448\n",
      "Iteration 476, loss = 0.12962719\n",
      "Iteration 477, loss = 0.12941200\n",
      "Iteration 478, loss = 0.12923387\n",
      "Iteration 479, loss = 0.12902671\n",
      "Iteration 480, loss = 0.12882954\n",
      "Iteration 481, loss = 0.12861840\n",
      "Iteration 482, loss = 0.12843726\n",
      "Iteration 483, loss = 0.12820823\n",
      "Iteration 484, loss = 0.12802765\n",
      "Iteration 485, loss = 0.12784170\n",
      "Iteration 486, loss = 0.12764019\n",
      "Iteration 487, loss = 0.12746002\n",
      "Iteration 488, loss = 0.12727166\n",
      "Iteration 489, loss = 0.12709425\n",
      "Iteration 490, loss = 0.12688886\n",
      "Iteration 491, loss = 0.12670762\n",
      "Iteration 492, loss = 0.12653253\n",
      "Iteration 493, loss = 0.12634092\n",
      "Iteration 494, loss = 0.12615833\n",
      "Iteration 495, loss = 0.12597804\n",
      "Iteration 496, loss = 0.12578323\n",
      "Iteration 497, loss = 0.12561381\n",
      "Iteration 498, loss = 0.12539220\n",
      "Iteration 499, loss = 0.12523955\n",
      "Iteration 500, loss = 0.12507878\n",
      "Iteration 501, loss = 0.12488693\n",
      "Iteration 502, loss = 0.12470290\n",
      "Iteration 503, loss = 0.12452128\n",
      "Iteration 504, loss = 0.12437593\n",
      "Iteration 505, loss = 0.12417910\n",
      "Iteration 506, loss = 0.12398462\n",
      "Iteration 507, loss = 0.12381440\n",
      "Iteration 508, loss = 0.12362205\n",
      "Iteration 509, loss = 0.12344589\n",
      "Iteration 510, loss = 0.12326978\n",
      "Iteration 511, loss = 0.12310251\n",
      "Iteration 512, loss = 0.12293434\n",
      "Iteration 513, loss = 0.12275662\n",
      "Iteration 514, loss = 0.12259370\n",
      "Iteration 515, loss = 0.12243265\n",
      "Iteration 516, loss = 0.12228923\n",
      "Iteration 517, loss = 0.12209605\n",
      "Iteration 518, loss = 0.12194066\n",
      "Iteration 519, loss = 0.12176940\n",
      "Iteration 520, loss = 0.12158755\n",
      "Iteration 521, loss = 0.12144730\n",
      "Iteration 522, loss = 0.12128710\n",
      "Iteration 523, loss = 0.12111473\n",
      "Iteration 524, loss = 0.12095308\n",
      "Iteration 525, loss = 0.12080657\n",
      "Iteration 526, loss = 0.12063911\n",
      "Iteration 527, loss = 0.12048883\n",
      "Iteration 528, loss = 0.12032377\n",
      "Iteration 529, loss = 0.12016747\n",
      "Iteration 530, loss = 0.12004305\n",
      "Iteration 531, loss = 0.11987033\n",
      "Iteration 532, loss = 0.11970858\n",
      "Iteration 533, loss = 0.11955801\n",
      "Iteration 534, loss = 0.11940334\n",
      "Iteration 535, loss = 0.11925368\n",
      "Iteration 536, loss = 0.11910244\n",
      "Iteration 537, loss = 0.11895084\n",
      "Iteration 538, loss = 0.11879087\n",
      "Iteration 539, loss = 0.11865662\n",
      "Iteration 540, loss = 0.11849634\n",
      "Iteration 541, loss = 0.11834562\n",
      "Iteration 542, loss = 0.11820054\n",
      "Iteration 543, loss = 0.11805571\n",
      "Iteration 544, loss = 0.11791842\n",
      "Iteration 545, loss = 0.11776011\n",
      "Iteration 546, loss = 0.11761541\n",
      "Iteration 547, loss = 0.11748557\n",
      "Iteration 548, loss = 0.11733086\n",
      "Iteration 549, loss = 0.11718603\n",
      "Iteration 550, loss = 0.11704763\n",
      "Iteration 551, loss = 0.11690309\n",
      "Iteration 552, loss = 0.11677470\n",
      "Iteration 553, loss = 0.11664269\n",
      "Iteration 554, loss = 0.11649133\n",
      "Iteration 555, loss = 0.11635569\n",
      "Iteration 556, loss = 0.11620759\n",
      "Iteration 557, loss = 0.11606972\n",
      "Iteration 558, loss = 0.11593615\n",
      "Iteration 559, loss = 0.11578102\n",
      "Iteration 560, loss = 0.11565603\n",
      "Iteration 561, loss = 0.11552520\n",
      "Iteration 562, loss = 0.11540302\n",
      "Iteration 563, loss = 0.11524828\n",
      "Iteration 564, loss = 0.11511140\n",
      "Iteration 565, loss = 0.11499842\n",
      "Iteration 566, loss = 0.11486462\n",
      "Iteration 567, loss = 0.11473723\n",
      "Iteration 568, loss = 0.11459964\n",
      "Iteration 569, loss = 0.11445996\n",
      "Iteration 570, loss = 0.11433500\n",
      "Iteration 571, loss = 0.11419004\n",
      "Iteration 572, loss = 0.11406463\n",
      "Iteration 573, loss = 0.11392953\n",
      "Iteration 574, loss = 0.11378582\n",
      "Iteration 575, loss = 0.11364444\n",
      "Iteration 576, loss = 0.11351731\n",
      "Iteration 577, loss = 0.11337232\n",
      "Iteration 578, loss = 0.11325242\n",
      "Iteration 579, loss = 0.11314311\n",
      "Iteration 580, loss = 0.11300988\n",
      "Iteration 581, loss = 0.11287598\n",
      "Iteration 582, loss = 0.11274355\n",
      "Iteration 583, loss = 0.11263515\n",
      "Iteration 584, loss = 0.11251558\n",
      "Iteration 585, loss = 0.11238369\n",
      "Iteration 586, loss = 0.11225349\n",
      "Iteration 587, loss = 0.11213608\n",
      "Iteration 588, loss = 0.11200383\n",
      "Iteration 589, loss = 0.11186364\n",
      "Iteration 590, loss = 0.11178117\n",
      "Iteration 591, loss = 0.11164965\n",
      "Iteration 592, loss = 0.11151511\n",
      "Iteration 593, loss = 0.11138095\n",
      "Iteration 594, loss = 0.11126028\n",
      "Iteration 595, loss = 0.11113435\n",
      "Iteration 596, loss = 0.11103129\n",
      "Iteration 597, loss = 0.11090670\n",
      "Iteration 598, loss = 0.11078195\n",
      "Iteration 599, loss = 0.11068005\n",
      "Iteration 600, loss = 0.11057062\n",
      "Iteration 601, loss = 0.11045390\n",
      "Iteration 602, loss = 0.11033716\n",
      "Iteration 603, loss = 0.11022343\n",
      "Iteration 604, loss = 0.11011922\n",
      "Iteration 605, loss = 0.11002021\n",
      "Iteration 606, loss = 0.10990637\n",
      "Iteration 607, loss = 0.10980132\n",
      "Iteration 608, loss = 0.10969929\n",
      "Iteration 609, loss = 0.10959902\n",
      "Iteration 610, loss = 0.10946625\n",
      "Iteration 611, loss = 0.10935317\n",
      "Iteration 612, loss = 0.10923295\n",
      "Iteration 613, loss = 0.10912667\n",
      "Iteration 614, loss = 0.10901754\n",
      "Iteration 615, loss = 0.10891506\n",
      "Iteration 616, loss = 0.10879427\n",
      "Iteration 617, loss = 0.10869525\n",
      "Iteration 618, loss = 0.10858561\n",
      "Iteration 619, loss = 0.10847996\n",
      "Iteration 620, loss = 0.10838721\n",
      "Iteration 621, loss = 0.10827694\n",
      "Iteration 622, loss = 0.10815773\n",
      "Iteration 623, loss = 0.10805079\n",
      "Iteration 624, loss = 0.10794991\n",
      "Iteration 625, loss = 0.10783654\n",
      "Iteration 626, loss = 0.10773592\n",
      "Iteration 627, loss = 0.10762599\n",
      "Iteration 628, loss = 0.10752994\n",
      "Iteration 629, loss = 0.10742820\n",
      "Iteration 630, loss = 0.10732264\n",
      "Iteration 631, loss = 0.10722201\n",
      "Iteration 632, loss = 0.10711814\n",
      "Iteration 633, loss = 0.10701722\n",
      "Iteration 634, loss = 0.10692260\n",
      "Iteration 635, loss = 0.10681630\n",
      "Iteration 636, loss = 0.10671819\n",
      "Iteration 637, loss = 0.10662864\n",
      "Iteration 638, loss = 0.10651587\n",
      "Iteration 639, loss = 0.10641573\n",
      "Iteration 640, loss = 0.10631097\n",
      "Iteration 641, loss = 0.10621042\n",
      "Iteration 642, loss = 0.10611612\n",
      "Iteration 643, loss = 0.10600455\n",
      "Iteration 644, loss = 0.10591506\n",
      "Iteration 645, loss = 0.10581017\n",
      "Iteration 646, loss = 0.10572135\n",
      "Iteration 647, loss = 0.10563185\n",
      "Iteration 648, loss = 0.10553198\n",
      "Iteration 649, loss = 0.10543843\n",
      "Iteration 650, loss = 0.10534951\n",
      "Iteration 651, loss = 0.10524489\n",
      "Iteration 652, loss = 0.10515599\n",
      "Iteration 653, loss = 0.10505556\n",
      "Iteration 654, loss = 0.10496688\n",
      "Iteration 655, loss = 0.10487562\n",
      "Iteration 656, loss = 0.10478221\n",
      "Iteration 657, loss = 0.10468251\n",
      "Iteration 658, loss = 0.10460034\n",
      "Iteration 659, loss = 0.10450463\n",
      "Iteration 660, loss = 0.10441783\n",
      "Iteration 661, loss = 0.10433342\n",
      "Iteration 662, loss = 0.10422887\n",
      "Iteration 663, loss = 0.10413596\n",
      "Iteration 664, loss = 0.10405077\n",
      "Iteration 665, loss = 0.10396072\n",
      "Iteration 666, loss = 0.10386697\n",
      "Iteration 667, loss = 0.10377684\n",
      "Iteration 668, loss = 0.10369142\n",
      "Iteration 669, loss = 0.10360276\n",
      "Iteration 670, loss = 0.10352022\n",
      "Iteration 671, loss = 0.10343313\n",
      "Iteration 672, loss = 0.10334856\n",
      "Iteration 673, loss = 0.10326030\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.42155163\n",
      "Iteration 2, loss = 2.40253691\n",
      "Iteration 3, loss = 2.37749491\n",
      "Iteration 4, loss = 2.35069761\n",
      "Iteration 5, loss = 2.32490291\n",
      "Iteration 6, loss = 2.30000032\n",
      "Iteration 7, loss = 2.27565953\n",
      "Iteration 8, loss = 2.25210045\n",
      "Iteration 9, loss = 2.22881652\n",
      "Iteration 10, loss = 2.20542462\n",
      "Iteration 11, loss = 2.18182400\n",
      "Iteration 12, loss = 2.15825348\n",
      "Iteration 13, loss = 2.13442762\n",
      "Iteration 14, loss = 2.11000339\n",
      "Iteration 15, loss = 2.08525703\n",
      "Iteration 16, loss = 2.05998205\n",
      "Iteration 17, loss = 2.03441201\n",
      "Iteration 18, loss = 2.00820942\n",
      "Iteration 19, loss = 1.98150561\n",
      "Iteration 20, loss = 1.95434749\n",
      "Iteration 21, loss = 1.92671793\n",
      "Iteration 22, loss = 1.89866174\n",
      "Iteration 23, loss = 1.87088862\n",
      "Iteration 24, loss = 1.84279061\n",
      "Iteration 25, loss = 1.81444457\n",
      "Iteration 26, loss = 1.78598061\n",
      "Iteration 27, loss = 1.75741716\n",
      "Iteration 28, loss = 1.72879590\n",
      "Iteration 29, loss = 1.70010934\n",
      "Iteration 30, loss = 1.67145706\n",
      "Iteration 31, loss = 1.64268532\n",
      "Iteration 32, loss = 1.61407265\n",
      "Iteration 33, loss = 1.58558067\n",
      "Iteration 34, loss = 1.55681539\n",
      "Iteration 35, loss = 1.52865620\n",
      "Iteration 36, loss = 1.50048654\n",
      "Iteration 37, loss = 1.47277903\n",
      "Iteration 38, loss = 1.44522035\n",
      "Iteration 39, loss = 1.41825152\n",
      "Iteration 40, loss = 1.39125435\n",
      "Iteration 41, loss = 1.36509611\n",
      "Iteration 42, loss = 1.33916871\n",
      "Iteration 43, loss = 1.31358543\n",
      "Iteration 44, loss = 1.28827308\n",
      "Iteration 45, loss = 1.26355468\n",
      "Iteration 46, loss = 1.23960329\n",
      "Iteration 47, loss = 1.21605837\n",
      "Iteration 48, loss = 1.19310506\n",
      "Iteration 49, loss = 1.17075590\n",
      "Iteration 50, loss = 1.14871639\n",
      "Iteration 51, loss = 1.12735487\n",
      "Iteration 52, loss = 1.10648917\n",
      "Iteration 53, loss = 1.08607410\n",
      "Iteration 54, loss = 1.06619951\n",
      "Iteration 55, loss = 1.04685440\n",
      "Iteration 56, loss = 1.02800394\n",
      "Iteration 57, loss = 1.00981276\n",
      "Iteration 58, loss = 0.99209483\n",
      "Iteration 59, loss = 0.97489295\n",
      "Iteration 60, loss = 0.95800170\n",
      "Iteration 61, loss = 0.94182717\n",
      "Iteration 62, loss = 0.92617675\n",
      "Iteration 63, loss = 0.91082010\n",
      "Iteration 64, loss = 0.89580941\n",
      "Iteration 65, loss = 0.88123958\n",
      "Iteration 66, loss = 0.86704365\n",
      "Iteration 67, loss = 0.85342327\n",
      "Iteration 68, loss = 0.84013284\n",
      "Iteration 69, loss = 0.82705406\n",
      "Iteration 70, loss = 0.81447951\n",
      "Iteration 71, loss = 0.80234999\n",
      "Iteration 72, loss = 0.79058712\n",
      "Iteration 73, loss = 0.77910739\n",
      "Iteration 74, loss = 0.76806015\n",
      "Iteration 75, loss = 0.75735616\n",
      "Iteration 76, loss = 0.74662231\n",
      "Iteration 77, loss = 0.73620896\n",
      "Iteration 78, loss = 0.72644911\n",
      "Iteration 79, loss = 0.71666130\n",
      "Iteration 80, loss = 0.70721342\n",
      "Iteration 81, loss = 0.69792706\n",
      "Iteration 82, loss = 0.68892831\n",
      "Iteration 83, loss = 0.68012186\n",
      "Iteration 84, loss = 0.67157764\n",
      "Iteration 85, loss = 0.66328862\n",
      "Iteration 86, loss = 0.65515758\n",
      "Iteration 87, loss = 0.64717880\n",
      "Iteration 88, loss = 0.63943652\n",
      "Iteration 89, loss = 0.63193751\n",
      "Iteration 90, loss = 0.62473103\n",
      "Iteration 91, loss = 0.61735499\n",
      "Iteration 92, loss = 0.61027995\n",
      "Iteration 93, loss = 0.60339554\n",
      "Iteration 94, loss = 0.59675286\n",
      "Iteration 95, loss = 0.59015036\n",
      "Iteration 96, loss = 0.58382436\n",
      "Iteration 97, loss = 0.57733934\n",
      "Iteration 98, loss = 0.57120577\n",
      "Iteration 99, loss = 0.56519646\n",
      "Iteration 100, loss = 0.55935616\n",
      "Iteration 101, loss = 0.55370774\n",
      "Iteration 102, loss = 0.54807107\n",
      "Iteration 103, loss = 0.54269621\n",
      "Iteration 104, loss = 0.53725218\n",
      "Iteration 105, loss = 0.53204338\n",
      "Iteration 106, loss = 0.52697878\n",
      "Iteration 107, loss = 0.52192378\n",
      "Iteration 108, loss = 0.51690525\n",
      "Iteration 109, loss = 0.51212045\n",
      "Iteration 110, loss = 0.50732043\n",
      "Iteration 111, loss = 0.50265808\n",
      "Iteration 112, loss = 0.49820251\n",
      "Iteration 113, loss = 0.49374499\n",
      "Iteration 114, loss = 0.48928590\n",
      "Iteration 115, loss = 0.48490272\n",
      "Iteration 116, loss = 0.48069199\n",
      "Iteration 117, loss = 0.47642055\n",
      "Iteration 118, loss = 0.47236422\n",
      "Iteration 119, loss = 0.46843159\n",
      "Iteration 120, loss = 0.46453919\n",
      "Iteration 121, loss = 0.46055648\n",
      "Iteration 122, loss = 0.45664808\n",
      "Iteration 123, loss = 0.45290873\n",
      "Iteration 124, loss = 0.44922984\n",
      "Iteration 125, loss = 0.44578694\n",
      "Iteration 126, loss = 0.44229053\n",
      "Iteration 127, loss = 0.43876382\n",
      "Iteration 128, loss = 0.43520025\n",
      "Iteration 129, loss = 0.43190204\n",
      "Iteration 130, loss = 0.42864383\n",
      "Iteration 131, loss = 0.42533999\n",
      "Iteration 132, loss = 0.42221103\n",
      "Iteration 133, loss = 0.41895655\n",
      "Iteration 134, loss = 0.41596683\n",
      "Iteration 135, loss = 0.41285203\n",
      "Iteration 136, loss = 0.40993304\n",
      "Iteration 137, loss = 0.40698320\n",
      "Iteration 138, loss = 0.40404210\n",
      "Iteration 139, loss = 0.40112210\n",
      "Iteration 140, loss = 0.39823883\n",
      "Iteration 141, loss = 0.39547713\n",
      "Iteration 142, loss = 0.39276754\n",
      "Iteration 143, loss = 0.38998365\n",
      "Iteration 144, loss = 0.38738711\n",
      "Iteration 145, loss = 0.38474988\n",
      "Iteration 146, loss = 0.38232184\n",
      "Iteration 147, loss = 0.37967006\n",
      "Iteration 148, loss = 0.37710376\n",
      "Iteration 149, loss = 0.37465140\n",
      "Iteration 150, loss = 0.37219743\n",
      "Iteration 151, loss = 0.36975625\n",
      "Iteration 152, loss = 0.36748193\n",
      "Iteration 153, loss = 0.36513862\n",
      "Iteration 154, loss = 0.36273393\n",
      "Iteration 155, loss = 0.36041638\n",
      "Iteration 156, loss = 0.35805315\n",
      "Iteration 157, loss = 0.35587841\n",
      "Iteration 158, loss = 0.35379001\n",
      "Iteration 159, loss = 0.35152452\n",
      "Iteration 160, loss = 0.34933018\n",
      "Iteration 161, loss = 0.34727677\n",
      "Iteration 162, loss = 0.34534102\n",
      "Iteration 163, loss = 0.34309094\n",
      "Iteration 164, loss = 0.34114277\n",
      "Iteration 165, loss = 0.33915707\n",
      "Iteration 166, loss = 0.33707370\n",
      "Iteration 167, loss = 0.33515281\n",
      "Iteration 168, loss = 0.33335174\n",
      "Iteration 169, loss = 0.33133773\n",
      "Iteration 170, loss = 0.32947793\n",
      "Iteration 171, loss = 0.32748435\n",
      "Iteration 172, loss = 0.32568196\n",
      "Iteration 173, loss = 0.32379323\n",
      "Iteration 174, loss = 0.32197254\n",
      "Iteration 175, loss = 0.32019734\n",
      "Iteration 176, loss = 0.31845294\n",
      "Iteration 177, loss = 0.31662053\n",
      "Iteration 178, loss = 0.31511454\n",
      "Iteration 179, loss = 0.31328057\n",
      "Iteration 180, loss = 0.31159190\n",
      "Iteration 181, loss = 0.30996380\n",
      "Iteration 182, loss = 0.30829959\n",
      "Iteration 183, loss = 0.30669215\n",
      "Iteration 184, loss = 0.30503377\n",
      "Iteration 185, loss = 0.30345982\n",
      "Iteration 186, loss = 0.30188470\n",
      "Iteration 187, loss = 0.30030616\n",
      "Iteration 188, loss = 0.29879198\n",
      "Iteration 189, loss = 0.29744159\n",
      "Iteration 190, loss = 0.29584542\n",
      "Iteration 191, loss = 0.29449564\n",
      "Iteration 192, loss = 0.29296529\n",
      "Iteration 193, loss = 0.29149397\n",
      "Iteration 194, loss = 0.29001583\n",
      "Iteration 195, loss = 0.28861383\n",
      "Iteration 196, loss = 0.28718639\n",
      "Iteration 197, loss = 0.28581988\n",
      "Iteration 198, loss = 0.28443331\n",
      "Iteration 199, loss = 0.28312859\n",
      "Iteration 200, loss = 0.28173810\n",
      "Iteration 201, loss = 0.28036358\n",
      "Iteration 202, loss = 0.27907651\n",
      "Iteration 203, loss = 0.27773321\n",
      "Iteration 204, loss = 0.27651353\n",
      "Iteration 205, loss = 0.27529666\n",
      "Iteration 206, loss = 0.27393942\n",
      "Iteration 207, loss = 0.27267970\n",
      "Iteration 208, loss = 0.27141199\n",
      "Iteration 209, loss = 0.27031857\n",
      "Iteration 210, loss = 0.26914459\n",
      "Iteration 211, loss = 0.26788627\n",
      "Iteration 212, loss = 0.26669318\n",
      "Iteration 213, loss = 0.26550791\n",
      "Iteration 214, loss = 0.26436181\n",
      "Iteration 215, loss = 0.26322995\n",
      "Iteration 216, loss = 0.26210040\n",
      "Iteration 217, loss = 0.26095125\n",
      "Iteration 218, loss = 0.25995502\n",
      "Iteration 219, loss = 0.25881426\n",
      "Iteration 220, loss = 0.25782080\n",
      "Iteration 221, loss = 0.25666574\n",
      "Iteration 222, loss = 0.25555025\n",
      "Iteration 223, loss = 0.25443040\n",
      "Iteration 224, loss = 0.25344024\n",
      "Iteration 225, loss = 0.25245554\n",
      "Iteration 226, loss = 0.25131482\n",
      "Iteration 227, loss = 0.25024698\n",
      "Iteration 228, loss = 0.24920646\n",
      "Iteration 229, loss = 0.24817543\n",
      "Iteration 230, loss = 0.24715223\n",
      "Iteration 231, loss = 0.24615193\n",
      "Iteration 232, loss = 0.24514511\n",
      "Iteration 233, loss = 0.24419198\n",
      "Iteration 234, loss = 0.24312345\n",
      "Iteration 235, loss = 0.24216300\n",
      "Iteration 236, loss = 0.24117591\n",
      "Iteration 237, loss = 0.24038006\n",
      "Iteration 238, loss = 0.23934659\n",
      "Iteration 239, loss = 0.23848757\n",
      "Iteration 240, loss = 0.23755886\n",
      "Iteration 241, loss = 0.23663032\n",
      "Iteration 242, loss = 0.23563145\n",
      "Iteration 243, loss = 0.23479256\n",
      "Iteration 244, loss = 0.23397236\n",
      "Iteration 245, loss = 0.23306704\n",
      "Iteration 246, loss = 0.23216462\n",
      "Iteration 247, loss = 0.23130962\n",
      "Iteration 248, loss = 0.23049180\n",
      "Iteration 249, loss = 0.22965932\n",
      "Iteration 250, loss = 0.22885290\n",
      "Iteration 251, loss = 0.22802610\n",
      "Iteration 252, loss = 0.22719436\n",
      "Iteration 253, loss = 0.22635835\n",
      "Iteration 254, loss = 0.22554781\n",
      "Iteration 255, loss = 0.22478334\n",
      "Iteration 256, loss = 0.22403435\n",
      "Iteration 257, loss = 0.22326976\n",
      "Iteration 258, loss = 0.22247432\n",
      "Iteration 259, loss = 0.22162385\n",
      "Iteration 260, loss = 0.22087669\n",
      "Iteration 261, loss = 0.22015231\n",
      "Iteration 262, loss = 0.21936292\n",
      "Iteration 263, loss = 0.21855163\n",
      "Iteration 264, loss = 0.21779659\n",
      "Iteration 265, loss = 0.21702896\n",
      "Iteration 266, loss = 0.21628010\n",
      "Iteration 267, loss = 0.21553607\n",
      "Iteration 268, loss = 0.21481167\n",
      "Iteration 269, loss = 0.21410670\n",
      "Iteration 270, loss = 0.21341345\n",
      "Iteration 271, loss = 0.21269774\n",
      "Iteration 272, loss = 0.21197161\n",
      "Iteration 273, loss = 0.21125657\n",
      "Iteration 274, loss = 0.21052750\n",
      "Iteration 275, loss = 0.20984558\n",
      "Iteration 276, loss = 0.20921086\n",
      "Iteration 277, loss = 0.20848894\n",
      "Iteration 278, loss = 0.20777762\n",
      "Iteration 279, loss = 0.20715214\n",
      "Iteration 280, loss = 0.20649453\n",
      "Iteration 281, loss = 0.20584968\n",
      "Iteration 282, loss = 0.20512383\n",
      "Iteration 283, loss = 0.20454059\n",
      "Iteration 284, loss = 0.20382087\n",
      "Iteration 285, loss = 0.20320056\n",
      "Iteration 286, loss = 0.20257932\n",
      "Iteration 287, loss = 0.20193981\n",
      "Iteration 288, loss = 0.20131277\n",
      "Iteration 289, loss = 0.20071271\n",
      "Iteration 290, loss = 0.20012225\n",
      "Iteration 291, loss = 0.19954815\n",
      "Iteration 292, loss = 0.19888804\n",
      "Iteration 293, loss = 0.19824813\n",
      "Iteration 294, loss = 0.19765055\n",
      "Iteration 295, loss = 0.19700963\n",
      "Iteration 296, loss = 0.19642090\n",
      "Iteration 297, loss = 0.19582599\n",
      "Iteration 298, loss = 0.19526868\n",
      "Iteration 299, loss = 0.19470656\n",
      "Iteration 300, loss = 0.19408884\n",
      "Iteration 301, loss = 0.19349559\n",
      "Iteration 302, loss = 0.19294720\n",
      "Iteration 303, loss = 0.19232906\n",
      "Iteration 304, loss = 0.19177478\n",
      "Iteration 305, loss = 0.19121031\n",
      "Iteration 306, loss = 0.19071230\n",
      "Iteration 307, loss = 0.19013685\n",
      "Iteration 308, loss = 0.18960728\n",
      "Iteration 309, loss = 0.18907805\n",
      "Iteration 310, loss = 0.18853712\n",
      "Iteration 311, loss = 0.18802727\n",
      "Iteration 312, loss = 0.18748049\n",
      "Iteration 313, loss = 0.18695471\n",
      "Iteration 314, loss = 0.18641605\n",
      "Iteration 315, loss = 0.18593644\n",
      "Iteration 316, loss = 0.18540892\n",
      "Iteration 317, loss = 0.18486414\n",
      "Iteration 318, loss = 0.18434504\n",
      "Iteration 319, loss = 0.18384475\n",
      "Iteration 320, loss = 0.18334038\n",
      "Iteration 321, loss = 0.18285818\n",
      "Iteration 322, loss = 0.18238397\n",
      "Iteration 323, loss = 0.18190838\n",
      "Iteration 324, loss = 0.18142498\n",
      "Iteration 325, loss = 0.18089090\n",
      "Iteration 326, loss = 0.18045012\n",
      "Iteration 327, loss = 0.17999747\n",
      "Iteration 328, loss = 0.17946389\n",
      "Iteration 329, loss = 0.17894193\n",
      "Iteration 330, loss = 0.17844505\n",
      "Iteration 331, loss = 0.17799990\n",
      "Iteration 332, loss = 0.17751591\n",
      "Iteration 333, loss = 0.17709926\n",
      "Iteration 334, loss = 0.17662119\n",
      "Iteration 335, loss = 0.17614751\n",
      "Iteration 336, loss = 0.17570886\n",
      "Iteration 337, loss = 0.17523748\n",
      "Iteration 338, loss = 0.17481548\n",
      "Iteration 339, loss = 0.17431989\n",
      "Iteration 340, loss = 0.17390763\n",
      "Iteration 341, loss = 0.17348040\n",
      "Iteration 342, loss = 0.17306684\n",
      "Iteration 343, loss = 0.17258798\n",
      "Iteration 344, loss = 0.17216763\n",
      "Iteration 345, loss = 0.17176415\n",
      "Iteration 346, loss = 0.17135088\n",
      "Iteration 347, loss = 0.17093455\n",
      "Iteration 348, loss = 0.17048124\n",
      "Iteration 349, loss = 0.17006582\n",
      "Iteration 350, loss = 0.16966855\n",
      "Iteration 351, loss = 0.16923470\n",
      "Iteration 352, loss = 0.16885754\n",
      "Iteration 353, loss = 0.16845068\n",
      "Iteration 354, loss = 0.16802114\n",
      "Iteration 355, loss = 0.16764747\n",
      "Iteration 356, loss = 0.16720412\n",
      "Iteration 357, loss = 0.16678450\n",
      "Iteration 358, loss = 0.16641327\n",
      "Iteration 359, loss = 0.16599384\n",
      "Iteration 360, loss = 0.16562250\n",
      "Iteration 361, loss = 0.16522135\n",
      "Iteration 362, loss = 0.16487247\n",
      "Iteration 363, loss = 0.16444920\n",
      "Iteration 364, loss = 0.16409206\n",
      "Iteration 365, loss = 0.16371144\n",
      "Iteration 366, loss = 0.16333850\n",
      "Iteration 367, loss = 0.16296037\n",
      "Iteration 368, loss = 0.16255746\n",
      "Iteration 369, loss = 0.16217000\n",
      "Iteration 370, loss = 0.16181735\n",
      "Iteration 371, loss = 0.16144903\n",
      "Iteration 372, loss = 0.16108848\n",
      "Iteration 373, loss = 0.16071474\n",
      "Iteration 374, loss = 0.16037428\n",
      "Iteration 375, loss = 0.16000785\n",
      "Iteration 376, loss = 0.15967640\n",
      "Iteration 377, loss = 0.15931103\n",
      "Iteration 378, loss = 0.15894149\n",
      "Iteration 379, loss = 0.15861657\n",
      "Iteration 380, loss = 0.15827460\n",
      "Iteration 381, loss = 0.15792661\n",
      "Iteration 382, loss = 0.15758290\n",
      "Iteration 383, loss = 0.15725027\n",
      "Iteration 384, loss = 0.15691932\n",
      "Iteration 385, loss = 0.15665999\n",
      "Iteration 386, loss = 0.15626231\n",
      "Iteration 387, loss = 0.15590530\n",
      "Iteration 388, loss = 0.15554147\n",
      "Iteration 389, loss = 0.15519963\n",
      "Iteration 390, loss = 0.15486444\n",
      "Iteration 391, loss = 0.15455989\n",
      "Iteration 392, loss = 0.15422610\n",
      "Iteration 393, loss = 0.15389885\n",
      "Iteration 394, loss = 0.15359514\n",
      "Iteration 395, loss = 0.15327682\n",
      "Iteration 396, loss = 0.15294056\n",
      "Iteration 397, loss = 0.15261826\n",
      "Iteration 398, loss = 0.15232315\n",
      "Iteration 399, loss = 0.15201786\n",
      "Iteration 400, loss = 0.15172740\n",
      "Iteration 401, loss = 0.15139770\n",
      "Iteration 402, loss = 0.15112106\n",
      "Iteration 403, loss = 0.15080034\n",
      "Iteration 404, loss = 0.15052018\n",
      "Iteration 405, loss = 0.15019934\n",
      "Iteration 406, loss = 0.14991192\n",
      "Iteration 407, loss = 0.14959750\n",
      "Iteration 408, loss = 0.14931990\n",
      "Iteration 409, loss = 0.14900036\n",
      "Iteration 410, loss = 0.14872724\n",
      "Iteration 411, loss = 0.14841888\n",
      "Iteration 412, loss = 0.14812757\n",
      "Iteration 413, loss = 0.14783124\n",
      "Iteration 414, loss = 0.14756071\n",
      "Iteration 415, loss = 0.14726389\n",
      "Iteration 416, loss = 0.14698430\n",
      "Iteration 417, loss = 0.14669286\n",
      "Iteration 418, loss = 0.14641215\n",
      "Iteration 419, loss = 0.14613289\n",
      "Iteration 420, loss = 0.14586447\n",
      "Iteration 421, loss = 0.14557777\n",
      "Iteration 422, loss = 0.14528977\n",
      "Iteration 423, loss = 0.14504998\n",
      "Iteration 424, loss = 0.14475920\n",
      "Iteration 425, loss = 0.14450113\n",
      "Iteration 426, loss = 0.14422327\n",
      "Iteration 427, loss = 0.14397387\n",
      "Iteration 428, loss = 0.14369271\n",
      "Iteration 429, loss = 0.14343650\n",
      "Iteration 430, loss = 0.14315372\n",
      "Iteration 431, loss = 0.14290835\n",
      "Iteration 432, loss = 0.14264824\n",
      "Iteration 433, loss = 0.14237315\n",
      "Iteration 434, loss = 0.14213859\n",
      "Iteration 435, loss = 0.14188043\n",
      "Iteration 436, loss = 0.14161299\n",
      "Iteration 437, loss = 0.14135535\n",
      "Iteration 438, loss = 0.14108888\n",
      "Iteration 439, loss = 0.14081818\n",
      "Iteration 440, loss = 0.14057543\n",
      "Iteration 441, loss = 0.14032632\n",
      "Iteration 442, loss = 0.14007246\n",
      "Iteration 443, loss = 0.13984778\n",
      "Iteration 444, loss = 0.13961568\n",
      "Iteration 445, loss = 0.13935291\n",
      "Iteration 446, loss = 0.13911607\n",
      "Iteration 447, loss = 0.13886611\n",
      "Iteration 448, loss = 0.13861321\n",
      "Iteration 449, loss = 0.13838552\n",
      "Iteration 450, loss = 0.13815773\n",
      "Iteration 451, loss = 0.13791955\n",
      "Iteration 452, loss = 0.13768340\n",
      "Iteration 453, loss = 0.13746151\n",
      "Iteration 454, loss = 0.13723198\n",
      "Iteration 455, loss = 0.13697765\n",
      "Iteration 456, loss = 0.13675280\n",
      "Iteration 457, loss = 0.13650020\n",
      "Iteration 458, loss = 0.13627306\n",
      "Iteration 459, loss = 0.13605142\n",
      "Iteration 460, loss = 0.13583174\n",
      "Iteration 461, loss = 0.13559690\n",
      "Iteration 462, loss = 0.13537631\n",
      "Iteration 463, loss = 0.13514114\n",
      "Iteration 464, loss = 0.13491426\n",
      "Iteration 465, loss = 0.13472468\n",
      "Iteration 466, loss = 0.13450914\n",
      "Iteration 467, loss = 0.13429683\n",
      "Iteration 468, loss = 0.13408291\n",
      "Iteration 469, loss = 0.13385051\n",
      "Iteration 470, loss = 0.13362800\n",
      "Iteration 471, loss = 0.13341651\n",
      "Iteration 472, loss = 0.13320392\n",
      "Iteration 473, loss = 0.13303563\n",
      "Iteration 474, loss = 0.13281132\n",
      "Iteration 475, loss = 0.13262482\n",
      "Iteration 476, loss = 0.13240847\n",
      "Iteration 477, loss = 0.13217978\n",
      "Iteration 478, loss = 0.13198391\n",
      "Iteration 479, loss = 0.13176462\n",
      "Iteration 480, loss = 0.13154661\n",
      "Iteration 481, loss = 0.13133428\n",
      "Iteration 482, loss = 0.13114119\n",
      "Iteration 483, loss = 0.13089919\n",
      "Iteration 484, loss = 0.13071663\n",
      "Iteration 485, loss = 0.13051919\n",
      "Iteration 486, loss = 0.13030531\n",
      "Iteration 487, loss = 0.13011324\n",
      "Iteration 488, loss = 0.12992133\n",
      "Iteration 489, loss = 0.12974209\n",
      "Iteration 490, loss = 0.12952938\n",
      "Iteration 491, loss = 0.12933799\n",
      "Iteration 492, loss = 0.12916394\n",
      "Iteration 493, loss = 0.12896254\n",
      "Iteration 494, loss = 0.12878377\n",
      "Iteration 495, loss = 0.12859125\n",
      "Iteration 496, loss = 0.12839124\n",
      "Iteration 497, loss = 0.12821310\n",
      "Iteration 498, loss = 0.12798434\n",
      "Iteration 499, loss = 0.12781956\n",
      "Iteration 500, loss = 0.12763769\n",
      "Iteration 501, loss = 0.12744320\n",
      "Iteration 502, loss = 0.12726131\n",
      "Iteration 503, loss = 0.12706557\n",
      "Iteration 504, loss = 0.12691484\n",
      "Iteration 505, loss = 0.12670219\n",
      "Iteration 506, loss = 0.12649892\n",
      "Iteration 507, loss = 0.12631679\n",
      "Iteration 508, loss = 0.12612858\n",
      "Iteration 509, loss = 0.12595133\n",
      "Iteration 510, loss = 0.12576428\n",
      "Iteration 511, loss = 0.12558910\n",
      "Iteration 512, loss = 0.12541937\n",
      "Iteration 513, loss = 0.12523474\n",
      "Iteration 514, loss = 0.12505271\n",
      "Iteration 515, loss = 0.12489316\n",
      "Iteration 516, loss = 0.12474701\n",
      "Iteration 517, loss = 0.12454435\n",
      "Iteration 518, loss = 0.12437880\n",
      "Iteration 519, loss = 0.12420962\n",
      "Iteration 520, loss = 0.12402623\n",
      "Iteration 521, loss = 0.12387068\n",
      "Iteration 522, loss = 0.12371203\n",
      "Iteration 523, loss = 0.12353622\n",
      "Iteration 524, loss = 0.12336615\n",
      "Iteration 525, loss = 0.12319990\n",
      "Iteration 526, loss = 0.12304487\n",
      "Iteration 527, loss = 0.12287527\n",
      "Iteration 528, loss = 0.12271463\n",
      "Iteration 529, loss = 0.12255143\n",
      "Iteration 530, loss = 0.12240989\n",
      "Iteration 531, loss = 0.12222667\n",
      "Iteration 532, loss = 0.12206575\n",
      "Iteration 533, loss = 0.12193177\n",
      "Iteration 534, loss = 0.12176218\n",
      "Iteration 535, loss = 0.12159558\n",
      "Iteration 536, loss = 0.12143571\n",
      "Iteration 537, loss = 0.12127594\n",
      "Iteration 538, loss = 0.12112251\n",
      "Iteration 539, loss = 0.12098432\n",
      "Iteration 540, loss = 0.12081813\n",
      "Iteration 541, loss = 0.12066347\n",
      "Iteration 542, loss = 0.12050907\n",
      "Iteration 543, loss = 0.12035652\n",
      "Iteration 544, loss = 0.12021519\n",
      "Iteration 545, loss = 0.12005356\n",
      "Iteration 546, loss = 0.11991080\n",
      "Iteration 547, loss = 0.11977440\n",
      "Iteration 548, loss = 0.11960532\n",
      "Iteration 549, loss = 0.11946792\n",
      "Iteration 550, loss = 0.11931341\n",
      "Iteration 551, loss = 0.11916410\n",
      "Iteration 552, loss = 0.11901834\n",
      "Iteration 553, loss = 0.11889652\n",
      "Iteration 554, loss = 0.11874598\n",
      "Iteration 555, loss = 0.11859185\n",
      "Iteration 556, loss = 0.11843948\n",
      "Iteration 557, loss = 0.11829169\n",
      "Iteration 558, loss = 0.11815033\n",
      "Iteration 559, loss = 0.11800347\n",
      "Iteration 560, loss = 0.11787267\n",
      "Iteration 561, loss = 0.11774316\n",
      "Iteration 562, loss = 0.11760320\n",
      "Iteration 563, loss = 0.11745158\n",
      "Iteration 564, loss = 0.11730477\n",
      "Iteration 565, loss = 0.11719199\n",
      "Iteration 566, loss = 0.11704082\n",
      "Iteration 567, loss = 0.11691346\n",
      "Iteration 568, loss = 0.11677226\n",
      "Iteration 569, loss = 0.11662526\n",
      "Iteration 570, loss = 0.11650026\n",
      "Iteration 571, loss = 0.11635127\n",
      "Iteration 572, loss = 0.11621432\n",
      "Iteration 573, loss = 0.11607299\n",
      "Iteration 574, loss = 0.11592819\n",
      "Iteration 575, loss = 0.11578359\n",
      "Iteration 576, loss = 0.11565319\n",
      "Iteration 577, loss = 0.11551072\n",
      "Iteration 578, loss = 0.11537997\n",
      "Iteration 579, loss = 0.11526990\n",
      "Iteration 580, loss = 0.11513281\n",
      "Iteration 581, loss = 0.11500038\n",
      "Iteration 582, loss = 0.11486335\n",
      "Iteration 583, loss = 0.11474383\n",
      "Iteration 584, loss = 0.11461524\n",
      "Iteration 585, loss = 0.11447754\n",
      "Iteration 586, loss = 0.11434686\n",
      "Iteration 587, loss = 0.11421771\n",
      "Iteration 588, loss = 0.11408659\n",
      "Iteration 589, loss = 0.11394047\n",
      "Iteration 590, loss = 0.11383402\n",
      "Iteration 591, loss = 0.11368825\n",
      "Iteration 592, loss = 0.11354943\n",
      "Iteration 593, loss = 0.11341848\n",
      "Iteration 594, loss = 0.11330071\n",
      "Iteration 595, loss = 0.11317915\n",
      "Iteration 596, loss = 0.11306474\n",
      "Iteration 597, loss = 0.11294382\n",
      "Iteration 598, loss = 0.11280100\n",
      "Iteration 599, loss = 0.11270688\n",
      "Iteration 600, loss = 0.11258475\n",
      "Iteration 601, loss = 0.11244862\n",
      "Iteration 602, loss = 0.11233715\n",
      "Iteration 603, loss = 0.11221579\n",
      "Iteration 604, loss = 0.11210868\n",
      "Iteration 605, loss = 0.11199507\n",
      "Iteration 606, loss = 0.11188491\n",
      "Iteration 607, loss = 0.11176471\n",
      "Iteration 608, loss = 0.11166061\n",
      "Iteration 609, loss = 0.11155657\n",
      "Iteration 610, loss = 0.11142757\n",
      "Iteration 611, loss = 0.11130797\n",
      "Iteration 612, loss = 0.11118161\n",
      "Iteration 613, loss = 0.11106432\n",
      "Iteration 614, loss = 0.11095500\n",
      "Iteration 615, loss = 0.11086766\n",
      "Iteration 616, loss = 0.11074864\n",
      "Iteration 617, loss = 0.11064590\n",
      "Iteration 618, loss = 0.11053203\n",
      "Iteration 619, loss = 0.11041655\n",
      "Iteration 620, loss = 0.11030239\n",
      "Iteration 621, loss = 0.11020080\n",
      "Iteration 622, loss = 0.11007901\n",
      "Iteration 623, loss = 0.10996656\n",
      "Iteration 624, loss = 0.10986326\n",
      "Iteration 625, loss = 0.10974084\n",
      "Iteration 626, loss = 0.10963977\n",
      "Iteration 627, loss = 0.10952393\n",
      "Iteration 628, loss = 0.10941948\n",
      "Iteration 629, loss = 0.10930944\n",
      "Iteration 630, loss = 0.10920595\n",
      "Iteration 631, loss = 0.10910598\n",
      "Iteration 632, loss = 0.10899959\n",
      "Iteration 633, loss = 0.10889446\n",
      "Iteration 634, loss = 0.10879353\n",
      "Iteration 635, loss = 0.10869332\n",
      "Iteration 636, loss = 0.10858408\n",
      "Iteration 637, loss = 0.10848843\n",
      "Iteration 638, loss = 0.10837570\n",
      "Iteration 639, loss = 0.10826899\n",
      "Iteration 640, loss = 0.10816848\n",
      "Iteration 641, loss = 0.10805472\n",
      "Iteration 642, loss = 0.10795362\n",
      "Iteration 643, loss = 0.10784575\n",
      "Iteration 644, loss = 0.10775700\n",
      "Iteration 645, loss = 0.10765464\n",
      "Iteration 646, loss = 0.10755587\n",
      "Iteration 647, loss = 0.10746705\n",
      "Iteration 648, loss = 0.10736074\n",
      "Iteration 649, loss = 0.10725801\n",
      "Iteration 650, loss = 0.10717412\n",
      "Iteration 651, loss = 0.10706726\n",
      "Iteration 652, loss = 0.10697730\n",
      "Iteration 653, loss = 0.10687145\n",
      "Iteration 654, loss = 0.10677881\n",
      "Iteration 655, loss = 0.10669013\n",
      "Iteration 656, loss = 0.10659321\n",
      "Iteration 657, loss = 0.10649986\n",
      "Iteration 658, loss = 0.10641403\n",
      "Iteration 659, loss = 0.10631497\n",
      "Iteration 660, loss = 0.10622918\n",
      "Iteration 661, loss = 0.10613367\n",
      "Iteration 662, loss = 0.10603112\n",
      "Iteration 663, loss = 0.10593958\n",
      "Iteration 664, loss = 0.10584012\n",
      "Iteration 665, loss = 0.10575205\n",
      "Iteration 666, loss = 0.10565898\n",
      "Iteration 667, loss = 0.10556348\n",
      "Iteration 668, loss = 0.10547228\n",
      "Iteration 669, loss = 0.10538188\n",
      "Iteration 670, loss = 0.10529280\n",
      "Iteration 671, loss = 0.10520597\n",
      "Iteration 672, loss = 0.10511599\n",
      "Iteration 673, loss = 0.10502914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.42101667\n",
      "Iteration 2, loss = 2.40156913\n",
      "Iteration 3, loss = 2.37594989\n",
      "Iteration 4, loss = 2.34898835\n",
      "Iteration 5, loss = 2.32293328\n",
      "Iteration 6, loss = 2.29785826\n",
      "Iteration 7, loss = 2.27339298\n",
      "Iteration 8, loss = 2.24970047\n",
      "Iteration 9, loss = 2.22616290\n",
      "Iteration 10, loss = 2.20237620\n",
      "Iteration 11, loss = 2.17820185\n",
      "Iteration 12, loss = 2.15383459\n",
      "Iteration 13, loss = 2.12914043\n",
      "Iteration 14, loss = 2.10381318\n",
      "Iteration 15, loss = 2.07829631\n",
      "Iteration 16, loss = 2.05191161\n",
      "Iteration 17, loss = 2.02502487\n",
      "Iteration 18, loss = 1.99775097\n",
      "Iteration 19, loss = 1.96979947\n",
      "Iteration 20, loss = 1.94149382\n",
      "Iteration 21, loss = 1.91265127\n",
      "Iteration 22, loss = 1.88346342\n",
      "Iteration 23, loss = 1.85414006\n",
      "Iteration 24, loss = 1.82468492\n",
      "Iteration 25, loss = 1.79487374\n",
      "Iteration 26, loss = 1.76494720\n",
      "Iteration 27, loss = 1.73502474\n",
      "Iteration 28, loss = 1.70492168\n",
      "Iteration 29, loss = 1.67484565\n",
      "Iteration 30, loss = 1.64469330\n",
      "Iteration 31, loss = 1.61443444\n",
      "Iteration 32, loss = 1.58464048\n",
      "Iteration 33, loss = 1.55491100\n",
      "Iteration 34, loss = 1.52519446\n",
      "Iteration 35, loss = 1.49582442\n",
      "Iteration 36, loss = 1.46687173\n",
      "Iteration 37, loss = 1.43810949\n",
      "Iteration 38, loss = 1.40990102\n",
      "Iteration 39, loss = 1.38182122\n",
      "Iteration 40, loss = 1.35437472\n",
      "Iteration 41, loss = 1.32734817\n",
      "Iteration 42, loss = 1.30049070\n",
      "Iteration 43, loss = 1.27418186\n",
      "Iteration 44, loss = 1.24834839\n",
      "Iteration 45, loss = 1.22298095\n",
      "Iteration 46, loss = 1.19847836\n",
      "Iteration 47, loss = 1.17452903\n",
      "Iteration 48, loss = 1.15065036\n",
      "Iteration 49, loss = 1.12774857\n",
      "Iteration 50, loss = 1.10506101\n",
      "Iteration 51, loss = 1.08310158\n",
      "Iteration 52, loss = 1.06199335\n",
      "Iteration 53, loss = 1.04122856\n",
      "Iteration 54, loss = 1.02113398\n",
      "Iteration 55, loss = 1.00135709\n",
      "Iteration 56, loss = 0.98237689\n",
      "Iteration 57, loss = 0.96371091\n",
      "Iteration 58, loss = 0.94581973\n",
      "Iteration 59, loss = 0.92844227\n",
      "Iteration 60, loss = 0.91130344\n",
      "Iteration 61, loss = 0.89516776\n",
      "Iteration 62, loss = 0.87901416\n",
      "Iteration 63, loss = 0.86353359\n",
      "Iteration 64, loss = 0.84848106\n",
      "Iteration 65, loss = 0.83390768\n",
      "Iteration 66, loss = 0.81970462\n",
      "Iteration 67, loss = 0.80598786\n",
      "Iteration 68, loss = 0.79254259\n",
      "Iteration 69, loss = 0.77947796\n",
      "Iteration 70, loss = 0.76690194\n",
      "Iteration 71, loss = 0.75475195\n",
      "Iteration 72, loss = 0.74287273\n",
      "Iteration 73, loss = 0.73138924\n",
      "Iteration 74, loss = 0.72034174\n",
      "Iteration 75, loss = 0.70966218\n",
      "Iteration 76, loss = 0.69922374\n",
      "Iteration 77, loss = 0.68862248\n",
      "Iteration 78, loss = 0.67883319\n",
      "Iteration 79, loss = 0.66919004\n",
      "Iteration 80, loss = 0.65972210\n",
      "Iteration 81, loss = 0.65047391\n",
      "Iteration 82, loss = 0.64146957\n",
      "Iteration 83, loss = 0.63273730\n",
      "Iteration 84, loss = 0.62411915\n",
      "Iteration 85, loss = 0.61586510\n",
      "Iteration 86, loss = 0.60764874\n",
      "Iteration 87, loss = 0.59976389\n",
      "Iteration 88, loss = 0.59210701\n",
      "Iteration 89, loss = 0.58452340\n",
      "Iteration 90, loss = 0.57733676\n",
      "Iteration 91, loss = 0.57010927\n",
      "Iteration 92, loss = 0.56326669\n",
      "Iteration 93, loss = 0.55653689\n",
      "Iteration 94, loss = 0.54994304\n",
      "Iteration 95, loss = 0.54348132\n",
      "Iteration 96, loss = 0.53719733\n",
      "Iteration 97, loss = 0.53092815\n",
      "Iteration 98, loss = 0.52492674\n",
      "Iteration 99, loss = 0.51903944\n",
      "Iteration 100, loss = 0.51344021\n",
      "Iteration 101, loss = 0.50782537\n",
      "Iteration 102, loss = 0.50239720\n",
      "Iteration 103, loss = 0.49716668\n",
      "Iteration 104, loss = 0.49192186\n",
      "Iteration 105, loss = 0.48673830\n",
      "Iteration 106, loss = 0.48178274\n",
      "Iteration 107, loss = 0.47683696\n",
      "Iteration 108, loss = 0.47199895\n",
      "Iteration 109, loss = 0.46740637\n",
      "Iteration 110, loss = 0.46268791\n",
      "Iteration 111, loss = 0.45824811\n",
      "Iteration 112, loss = 0.45400443\n",
      "Iteration 113, loss = 0.44957869\n",
      "Iteration 114, loss = 0.44529986\n",
      "Iteration 115, loss = 0.44114918\n",
      "Iteration 116, loss = 0.43711964\n",
      "Iteration 117, loss = 0.43295840\n",
      "Iteration 118, loss = 0.42905234\n",
      "Iteration 119, loss = 0.42517272\n",
      "Iteration 120, loss = 0.42144799\n",
      "Iteration 121, loss = 0.41783727\n",
      "Iteration 122, loss = 0.41408868\n",
      "Iteration 123, loss = 0.41044706\n",
      "Iteration 124, loss = 0.40681466\n",
      "Iteration 125, loss = 0.40361209\n",
      "Iteration 126, loss = 0.40026287\n",
      "Iteration 127, loss = 0.39681448\n",
      "Iteration 128, loss = 0.39354515\n",
      "Iteration 129, loss = 0.39039631\n",
      "Iteration 130, loss = 0.38720122\n",
      "Iteration 131, loss = 0.38408390\n",
      "Iteration 132, loss = 0.38110781\n",
      "Iteration 133, loss = 0.37804382\n",
      "Iteration 134, loss = 0.37518138\n",
      "Iteration 135, loss = 0.37221149\n",
      "Iteration 136, loss = 0.36936272\n",
      "Iteration 137, loss = 0.36661220\n",
      "Iteration 138, loss = 0.36391071\n",
      "Iteration 139, loss = 0.36109827\n",
      "Iteration 140, loss = 0.35842426\n",
      "Iteration 141, loss = 0.35585700\n",
      "Iteration 142, loss = 0.35330705\n",
      "Iteration 143, loss = 0.35075895\n",
      "Iteration 144, loss = 0.34829431\n",
      "Iteration 145, loss = 0.34569961\n",
      "Iteration 146, loss = 0.34340796\n",
      "Iteration 147, loss = 0.34090472\n",
      "Iteration 148, loss = 0.33857690\n",
      "Iteration 149, loss = 0.33621710\n",
      "Iteration 150, loss = 0.33397731\n",
      "Iteration 151, loss = 0.33163968\n",
      "Iteration 152, loss = 0.32950221\n",
      "Iteration 153, loss = 0.32731257\n",
      "Iteration 154, loss = 0.32517792\n",
      "Iteration 155, loss = 0.32297320\n",
      "Iteration 156, loss = 0.32092575\n",
      "Iteration 157, loss = 0.31890929\n",
      "Iteration 158, loss = 0.31681995\n",
      "Iteration 159, loss = 0.31477266\n",
      "Iteration 160, loss = 0.31266293\n",
      "Iteration 161, loss = 0.31080982\n",
      "Iteration 162, loss = 0.30900274\n",
      "Iteration 163, loss = 0.30693087\n",
      "Iteration 164, loss = 0.30507743\n",
      "Iteration 165, loss = 0.30327622\n",
      "Iteration 166, loss = 0.30139028\n",
      "Iteration 167, loss = 0.29965000\n",
      "Iteration 168, loss = 0.29795049\n",
      "Iteration 169, loss = 0.29610539\n",
      "Iteration 170, loss = 0.29439602\n",
      "Iteration 171, loss = 0.29262179\n",
      "Iteration 172, loss = 0.29100808\n",
      "Iteration 173, loss = 0.28923809\n",
      "Iteration 174, loss = 0.28757571\n",
      "Iteration 175, loss = 0.28594488\n",
      "Iteration 176, loss = 0.28431805\n",
      "Iteration 177, loss = 0.28272624\n",
      "Iteration 178, loss = 0.28123299\n",
      "Iteration 179, loss = 0.27959537\n",
      "Iteration 180, loss = 0.27808948\n",
      "Iteration 181, loss = 0.27654841\n",
      "Iteration 182, loss = 0.27503730\n",
      "Iteration 183, loss = 0.27350241\n",
      "Iteration 184, loss = 0.27200214\n",
      "Iteration 185, loss = 0.27057740\n",
      "Iteration 186, loss = 0.26915933\n",
      "Iteration 187, loss = 0.26773604\n",
      "Iteration 188, loss = 0.26636508\n",
      "Iteration 189, loss = 0.26511933\n",
      "Iteration 190, loss = 0.26360909\n",
      "Iteration 191, loss = 0.26229062\n",
      "Iteration 192, loss = 0.26087557\n",
      "Iteration 193, loss = 0.25951693\n",
      "Iteration 194, loss = 0.25826588\n",
      "Iteration 195, loss = 0.25697099\n",
      "Iteration 196, loss = 0.25570121\n",
      "Iteration 197, loss = 0.25448095\n",
      "Iteration 198, loss = 0.25318435\n",
      "Iteration 199, loss = 0.25196678\n",
      "Iteration 200, loss = 0.25073431\n",
      "Iteration 201, loss = 0.24958610\n",
      "Iteration 202, loss = 0.24840251\n",
      "Iteration 203, loss = 0.24717735\n",
      "Iteration 204, loss = 0.24607275\n",
      "Iteration 205, loss = 0.24485807\n",
      "Iteration 206, loss = 0.24372016\n",
      "Iteration 207, loss = 0.24261495\n",
      "Iteration 208, loss = 0.24146598\n",
      "Iteration 209, loss = 0.24043123\n",
      "Iteration 210, loss = 0.23931880\n",
      "Iteration 211, loss = 0.23817118\n",
      "Iteration 212, loss = 0.23710265\n",
      "Iteration 213, loss = 0.23604327\n",
      "Iteration 214, loss = 0.23499265\n",
      "Iteration 215, loss = 0.23395932\n",
      "Iteration 216, loss = 0.23295621\n",
      "Iteration 217, loss = 0.23198177\n",
      "Iteration 218, loss = 0.23099429\n",
      "Iteration 219, loss = 0.22999126\n",
      "Iteration 220, loss = 0.22900709\n",
      "Iteration 221, loss = 0.22798539\n",
      "Iteration 222, loss = 0.22700171\n",
      "Iteration 223, loss = 0.22597868\n",
      "Iteration 224, loss = 0.22503418\n",
      "Iteration 225, loss = 0.22410354\n",
      "Iteration 226, loss = 0.22314006\n",
      "Iteration 227, loss = 0.22219752\n",
      "Iteration 228, loss = 0.22134496\n",
      "Iteration 229, loss = 0.22041051\n",
      "Iteration 230, loss = 0.21950070\n",
      "Iteration 231, loss = 0.21864318\n",
      "Iteration 232, loss = 0.21774843\n",
      "Iteration 233, loss = 0.21686465\n",
      "Iteration 234, loss = 0.21592197\n",
      "Iteration 235, loss = 0.21501649\n",
      "Iteration 236, loss = 0.21416831\n",
      "Iteration 237, loss = 0.21336106\n",
      "Iteration 238, loss = 0.21243373\n",
      "Iteration 239, loss = 0.21164216\n",
      "Iteration 240, loss = 0.21082322\n",
      "Iteration 241, loss = 0.21001703\n",
      "Iteration 242, loss = 0.20920838\n",
      "Iteration 243, loss = 0.20840709\n",
      "Iteration 244, loss = 0.20767672\n",
      "Iteration 245, loss = 0.20686041\n",
      "Iteration 246, loss = 0.20607674\n",
      "Iteration 247, loss = 0.20530023\n",
      "Iteration 248, loss = 0.20458701\n",
      "Iteration 249, loss = 0.20381695\n",
      "Iteration 250, loss = 0.20306109\n",
      "Iteration 251, loss = 0.20235136\n",
      "Iteration 252, loss = 0.20163209\n",
      "Iteration 253, loss = 0.20088437\n",
      "Iteration 254, loss = 0.20017519\n",
      "Iteration 255, loss = 0.19947249\n",
      "Iteration 256, loss = 0.19887162\n",
      "Iteration 257, loss = 0.19810489\n",
      "Iteration 258, loss = 0.19739149\n",
      "Iteration 259, loss = 0.19664831\n",
      "Iteration 260, loss = 0.19596323\n",
      "Iteration 261, loss = 0.19527325\n",
      "Iteration 262, loss = 0.19459945\n",
      "Iteration 263, loss = 0.19388552\n",
      "Iteration 264, loss = 0.19321232\n",
      "Iteration 265, loss = 0.19252655\n",
      "Iteration 266, loss = 0.19188434\n",
      "Iteration 267, loss = 0.19123512\n",
      "Iteration 268, loss = 0.19057549\n",
      "Iteration 269, loss = 0.18992563\n",
      "Iteration 270, loss = 0.18932699\n",
      "Iteration 271, loss = 0.18870172\n",
      "Iteration 272, loss = 0.18807549\n",
      "Iteration 273, loss = 0.18743993\n",
      "Iteration 274, loss = 0.18682360\n",
      "Iteration 275, loss = 0.18624879\n",
      "Iteration 276, loss = 0.18561107\n",
      "Iteration 277, loss = 0.18498822\n",
      "Iteration 278, loss = 0.18441576\n",
      "Iteration 279, loss = 0.18381981\n",
      "Iteration 280, loss = 0.18324051\n",
      "Iteration 281, loss = 0.18265898\n",
      "Iteration 282, loss = 0.18206240\n",
      "Iteration 283, loss = 0.18153180\n",
      "Iteration 284, loss = 0.18094699\n",
      "Iteration 285, loss = 0.18036128\n",
      "Iteration 286, loss = 0.17982112\n",
      "Iteration 287, loss = 0.17926610\n",
      "Iteration 288, loss = 0.17871849\n",
      "Iteration 289, loss = 0.17819520\n",
      "Iteration 290, loss = 0.17767599\n",
      "Iteration 291, loss = 0.17714258\n",
      "Iteration 292, loss = 0.17665390\n",
      "Iteration 293, loss = 0.17607738\n",
      "Iteration 294, loss = 0.17552896\n",
      "Iteration 295, loss = 0.17499038\n",
      "Iteration 296, loss = 0.17451647\n",
      "Iteration 297, loss = 0.17400168\n",
      "Iteration 298, loss = 0.17348500\n",
      "Iteration 299, loss = 0.17295105\n",
      "Iteration 300, loss = 0.17247395\n",
      "Iteration 301, loss = 0.17193930\n",
      "Iteration 302, loss = 0.17146986\n",
      "Iteration 303, loss = 0.17095936\n",
      "Iteration 304, loss = 0.17050705\n",
      "Iteration 305, loss = 0.17001581\n",
      "Iteration 306, loss = 0.16952162\n",
      "Iteration 307, loss = 0.16904486\n",
      "Iteration 308, loss = 0.16856607\n",
      "Iteration 309, loss = 0.16809136\n",
      "Iteration 310, loss = 0.16760734\n",
      "Iteration 311, loss = 0.16715691\n",
      "Iteration 312, loss = 0.16667172\n",
      "Iteration 313, loss = 0.16625041\n",
      "Iteration 314, loss = 0.16575736\n",
      "Iteration 315, loss = 0.16535151\n",
      "Iteration 316, loss = 0.16489511\n",
      "Iteration 317, loss = 0.16445850\n",
      "Iteration 318, loss = 0.16401106\n",
      "Iteration 319, loss = 0.16356275\n",
      "Iteration 320, loss = 0.16313632\n",
      "Iteration 321, loss = 0.16271676\n",
      "Iteration 322, loss = 0.16232354\n",
      "Iteration 323, loss = 0.16188843\n",
      "Iteration 324, loss = 0.16151207\n",
      "Iteration 325, loss = 0.16100750\n",
      "Iteration 326, loss = 0.16060065\n",
      "Iteration 327, loss = 0.16022075\n",
      "Iteration 328, loss = 0.15980021\n",
      "Iteration 329, loss = 0.15935304\n",
      "Iteration 330, loss = 0.15891698\n",
      "Iteration 331, loss = 0.15854144\n",
      "Iteration 332, loss = 0.15811942\n",
      "Iteration 333, loss = 0.15772819\n",
      "Iteration 334, loss = 0.15732330\n",
      "Iteration 335, loss = 0.15691523\n",
      "Iteration 336, loss = 0.15651561\n",
      "Iteration 337, loss = 0.15613630\n",
      "Iteration 338, loss = 0.15573158\n",
      "Iteration 339, loss = 0.15535097\n",
      "Iteration 340, loss = 0.15496515\n",
      "Iteration 341, loss = 0.15461849\n",
      "Iteration 342, loss = 0.15425968\n",
      "Iteration 343, loss = 0.15386471\n",
      "Iteration 344, loss = 0.15349629\n",
      "Iteration 345, loss = 0.15313712\n",
      "Iteration 346, loss = 0.15276541\n",
      "Iteration 347, loss = 0.15239765\n",
      "Iteration 348, loss = 0.15200667\n",
      "Iteration 349, loss = 0.15165541\n",
      "Iteration 350, loss = 0.15129815\n",
      "Iteration 351, loss = 0.15095280\n",
      "Iteration 352, loss = 0.15059876\n",
      "Iteration 353, loss = 0.15026194\n",
      "Iteration 354, loss = 0.14989545\n",
      "Iteration 355, loss = 0.14955564\n",
      "Iteration 356, loss = 0.14920731\n",
      "Iteration 357, loss = 0.14883359\n",
      "Iteration 358, loss = 0.14848773\n",
      "Iteration 359, loss = 0.14815321\n",
      "Iteration 360, loss = 0.14784733\n",
      "Iteration 361, loss = 0.14753092\n",
      "Iteration 362, loss = 0.14720115\n",
      "Iteration 363, loss = 0.14687973\n",
      "Iteration 364, loss = 0.14653660\n",
      "Iteration 365, loss = 0.14619418\n",
      "Iteration 366, loss = 0.14588124\n",
      "Iteration 367, loss = 0.14554862\n",
      "Iteration 368, loss = 0.14524353\n",
      "Iteration 369, loss = 0.14490921\n",
      "Iteration 370, loss = 0.14461445\n",
      "Iteration 371, loss = 0.14428515\n",
      "Iteration 372, loss = 0.14399294\n",
      "Iteration 373, loss = 0.14367592\n",
      "Iteration 374, loss = 0.14337797\n",
      "Iteration 375, loss = 0.14308502\n",
      "Iteration 376, loss = 0.14280012\n",
      "Iteration 377, loss = 0.14249361\n",
      "Iteration 378, loss = 0.14220319\n",
      "Iteration 379, loss = 0.14192835\n",
      "Iteration 380, loss = 0.14162653\n",
      "Iteration 381, loss = 0.14134011\n",
      "Iteration 382, loss = 0.14104728\n",
      "Iteration 383, loss = 0.14075828\n",
      "Iteration 384, loss = 0.14047256\n",
      "Iteration 385, loss = 0.14022563\n",
      "Iteration 386, loss = 0.13995161\n",
      "Iteration 387, loss = 0.13964953\n",
      "Iteration 388, loss = 0.13936859\n",
      "Iteration 389, loss = 0.13909456\n",
      "Iteration 390, loss = 0.13881608\n",
      "Iteration 391, loss = 0.13854680\n",
      "Iteration 392, loss = 0.13826257\n",
      "Iteration 393, loss = 0.13798988\n",
      "Iteration 394, loss = 0.13772041\n",
      "Iteration 395, loss = 0.13747453\n",
      "Iteration 396, loss = 0.13719315\n",
      "Iteration 397, loss = 0.13692159\n",
      "Iteration 398, loss = 0.13667693\n",
      "Iteration 399, loss = 0.13643090\n",
      "Iteration 400, loss = 0.13615607\n",
      "Iteration 401, loss = 0.13589454\n",
      "Iteration 402, loss = 0.13563174\n",
      "Iteration 403, loss = 0.13538019\n",
      "Iteration 404, loss = 0.13514162\n",
      "Iteration 405, loss = 0.13489520\n",
      "Iteration 406, loss = 0.13464484\n",
      "Iteration 407, loss = 0.13437939\n",
      "Iteration 408, loss = 0.13413527\n",
      "Iteration 409, loss = 0.13388126\n",
      "Iteration 410, loss = 0.13363829\n",
      "Iteration 411, loss = 0.13338962\n",
      "Iteration 412, loss = 0.13313274\n",
      "Iteration 413, loss = 0.13291231\n",
      "Iteration 414, loss = 0.13266432\n",
      "Iteration 415, loss = 0.13242854\n",
      "Iteration 416, loss = 0.13219442\n",
      "Iteration 417, loss = 0.13194389\n",
      "Iteration 418, loss = 0.13173869\n",
      "Iteration 419, loss = 0.13148074\n",
      "Iteration 420, loss = 0.13125333\n",
      "Iteration 421, loss = 0.13100446\n",
      "Iteration 422, loss = 0.13077723\n",
      "Iteration 423, loss = 0.13053885\n",
      "Iteration 424, loss = 0.13029526\n",
      "Iteration 425, loss = 0.13007336\n",
      "Iteration 426, loss = 0.12986951\n",
      "Iteration 427, loss = 0.12965104\n",
      "Iteration 428, loss = 0.12941834\n",
      "Iteration 429, loss = 0.12919993\n",
      "Iteration 430, loss = 0.12896338\n",
      "Iteration 431, loss = 0.12875291\n",
      "Iteration 432, loss = 0.12853486\n",
      "Iteration 433, loss = 0.12830770\n",
      "Iteration 434, loss = 0.12809990\n",
      "Iteration 435, loss = 0.12789004\n",
      "Iteration 436, loss = 0.12766423\n",
      "Iteration 437, loss = 0.12742659\n",
      "Iteration 438, loss = 0.12721642\n",
      "Iteration 439, loss = 0.12699932\n",
      "Iteration 440, loss = 0.12679180\n",
      "Iteration 441, loss = 0.12657836\n",
      "Iteration 442, loss = 0.12637737\n",
      "Iteration 443, loss = 0.12617660\n",
      "Iteration 444, loss = 0.12597804\n",
      "Iteration 445, loss = 0.12577963\n",
      "Iteration 446, loss = 0.12557656\n",
      "Iteration 447, loss = 0.12537723\n",
      "Iteration 448, loss = 0.12517870\n",
      "Iteration 449, loss = 0.12497995\n",
      "Iteration 450, loss = 0.12478756\n",
      "Iteration 451, loss = 0.12458975\n",
      "Iteration 452, loss = 0.12440132\n",
      "Iteration 453, loss = 0.12421147\n",
      "Iteration 454, loss = 0.12402092\n",
      "Iteration 455, loss = 0.12382733\n",
      "Iteration 456, loss = 0.12363573\n",
      "Iteration 457, loss = 0.12343711\n",
      "Iteration 458, loss = 0.12327328\n",
      "Iteration 459, loss = 0.12308744\n",
      "Iteration 460, loss = 0.12289749\n",
      "Iteration 461, loss = 0.12271504\n",
      "Iteration 462, loss = 0.12253045\n",
      "Iteration 463, loss = 0.12234608\n",
      "Iteration 464, loss = 0.12215388\n",
      "Iteration 465, loss = 0.12198047\n",
      "Iteration 466, loss = 0.12180822\n",
      "Iteration 467, loss = 0.12162870\n",
      "Iteration 468, loss = 0.12144903\n",
      "Iteration 469, loss = 0.12126868\n",
      "Iteration 470, loss = 0.12108709\n",
      "Iteration 471, loss = 0.12090425\n",
      "Iteration 472, loss = 0.12075101\n",
      "Iteration 473, loss = 0.12058071\n",
      "Iteration 474, loss = 0.12038650\n",
      "Iteration 475, loss = 0.12021052\n",
      "Iteration 476, loss = 0.12003429\n",
      "Iteration 477, loss = 0.11986309\n",
      "Iteration 478, loss = 0.11970323\n",
      "Iteration 479, loss = 0.11953080\n",
      "Iteration 480, loss = 0.11935204\n",
      "Iteration 481, loss = 0.11917709\n",
      "Iteration 482, loss = 0.11902361\n",
      "Iteration 483, loss = 0.11884689\n",
      "Iteration 484, loss = 0.11868079\n",
      "Iteration 485, loss = 0.11850305\n",
      "Iteration 486, loss = 0.11832276\n",
      "Iteration 487, loss = 0.11816003\n",
      "Iteration 488, loss = 0.11801426\n",
      "Iteration 489, loss = 0.11785719\n",
      "Iteration 490, loss = 0.11769446\n",
      "Iteration 491, loss = 0.11751937\n",
      "Iteration 492, loss = 0.11737493\n",
      "Iteration 493, loss = 0.11722315\n",
      "Iteration 494, loss = 0.11705062\n",
      "Iteration 495, loss = 0.11689316\n",
      "Iteration 496, loss = 0.11673299\n",
      "Iteration 497, loss = 0.11658878\n",
      "Iteration 498, loss = 0.11641571\n",
      "Iteration 499, loss = 0.11627475\n",
      "Iteration 500, loss = 0.11611828\n",
      "Iteration 501, loss = 0.11595400\n",
      "Iteration 502, loss = 0.11582425\n",
      "Iteration 503, loss = 0.11566588\n",
      "Iteration 504, loss = 0.11552256\n",
      "Iteration 505, loss = 0.11535843\n",
      "Iteration 506, loss = 0.11519504\n",
      "Iteration 507, loss = 0.11505241\n",
      "Iteration 508, loss = 0.11490074\n",
      "Iteration 509, loss = 0.11477610\n",
      "Iteration 510, loss = 0.11461191\n",
      "Iteration 511, loss = 0.11447211\n",
      "Iteration 512, loss = 0.11435108\n",
      "Iteration 513, loss = 0.11418965\n",
      "Iteration 514, loss = 0.11404013\n",
      "Iteration 515, loss = 0.11389929\n",
      "Iteration 516, loss = 0.11377998\n",
      "Iteration 517, loss = 0.11360709\n",
      "Iteration 518, loss = 0.11347689\n",
      "Iteration 519, loss = 0.11333983\n",
      "Iteration 520, loss = 0.11319141\n",
      "Iteration 521, loss = 0.11305674\n",
      "Iteration 522, loss = 0.11292454\n",
      "Iteration 523, loss = 0.11278910\n",
      "Iteration 524, loss = 0.11266256\n",
      "Iteration 525, loss = 0.11252283\n",
      "Iteration 526, loss = 0.11239008\n",
      "Iteration 527, loss = 0.11225110\n",
      "Iteration 528, loss = 0.11212294\n",
      "Iteration 529, loss = 0.11199437\n",
      "Iteration 530, loss = 0.11186328\n",
      "Iteration 531, loss = 0.11173501\n",
      "Iteration 532, loss = 0.11160367\n",
      "Iteration 533, loss = 0.11147985\n",
      "Iteration 534, loss = 0.11135849\n",
      "Iteration 535, loss = 0.11121996\n",
      "Iteration 536, loss = 0.11109543\n",
      "Iteration 537, loss = 0.11096766\n",
      "Iteration 538, loss = 0.11083935\n",
      "Iteration 539, loss = 0.11072001\n",
      "Iteration 540, loss = 0.11058359\n",
      "Iteration 541, loss = 0.11048112\n",
      "Iteration 542, loss = 0.11035050\n",
      "Iteration 543, loss = 0.11022523\n",
      "Iteration 544, loss = 0.11010385\n",
      "Iteration 545, loss = 0.10998253\n",
      "Iteration 546, loss = 0.10985532\n",
      "Iteration 547, loss = 0.10973570\n",
      "Iteration 548, loss = 0.10959746\n",
      "Iteration 549, loss = 0.10947258\n",
      "Iteration 550, loss = 0.10934919\n",
      "Iteration 551, loss = 0.10922921\n",
      "Iteration 552, loss = 0.10910720\n",
      "Iteration 553, loss = 0.10899858\n",
      "Iteration 554, loss = 0.10887933\n",
      "Iteration 555, loss = 0.10876032\n",
      "Iteration 556, loss = 0.10864431\n",
      "Iteration 557, loss = 0.10852901\n",
      "Iteration 558, loss = 0.10841262\n",
      "Iteration 559, loss = 0.10829611\n",
      "Iteration 560, loss = 0.10817967\n",
      "Iteration 561, loss = 0.10806059\n",
      "Iteration 562, loss = 0.10795343\n",
      "Iteration 563, loss = 0.10785196\n",
      "Iteration 564, loss = 0.10772805\n",
      "Iteration 565, loss = 0.10761775\n",
      "Iteration 566, loss = 0.10750242\n",
      "Iteration 567, loss = 0.10739159\n",
      "Iteration 568, loss = 0.10727376\n",
      "Iteration 569, loss = 0.10715661\n",
      "Iteration 570, loss = 0.10705405\n",
      "Iteration 571, loss = 0.10694018\n",
      "Iteration 572, loss = 0.10682799\n",
      "Iteration 573, loss = 0.10671842\n",
      "Iteration 574, loss = 0.10660883\n",
      "Iteration 575, loss = 0.10649007\n",
      "Iteration 576, loss = 0.10637983\n",
      "Iteration 577, loss = 0.10627458\n",
      "Iteration 578, loss = 0.10616831\n",
      "Iteration 579, loss = 0.10605787\n",
      "Iteration 580, loss = 0.10595616\n",
      "Iteration 581, loss = 0.10583236\n",
      "Iteration 582, loss = 0.10573088\n",
      "Iteration 583, loss = 0.10564410\n",
      "Iteration 584, loss = 0.10552523\n",
      "Iteration 585, loss = 0.10542070\n",
      "Iteration 586, loss = 0.10532519\n",
      "Iteration 587, loss = 0.10521852\n",
      "Iteration 588, loss = 0.10512316\n",
      "Iteration 589, loss = 0.10501330\n",
      "Iteration 590, loss = 0.10492365\n",
      "Iteration 591, loss = 0.10480319\n",
      "Iteration 592, loss = 0.10468514\n",
      "Iteration 593, loss = 0.10458028\n",
      "Iteration 594, loss = 0.10447720\n",
      "Iteration 595, loss = 0.10437810\n",
      "Iteration 596, loss = 0.10428746\n",
      "Iteration 597, loss = 0.10418424\n",
      "Iteration 598, loss = 0.10407505\n",
      "Iteration 599, loss = 0.10398782\n",
      "Iteration 600, loss = 0.10388808\n",
      "Iteration 601, loss = 0.10378614\n",
      "Iteration 602, loss = 0.10369616\n",
      "Iteration 603, loss = 0.10359824\n",
      "Iteration 604, loss = 0.10350483\n",
      "Iteration 605, loss = 0.10340804\n",
      "Iteration 606, loss = 0.10331623\n",
      "Iteration 607, loss = 0.10322029\n",
      "Iteration 608, loss = 0.10312629\n",
      "Iteration 609, loss = 0.10303516\n",
      "Iteration 610, loss = 0.10294985\n",
      "Iteration 611, loss = 0.10285528\n",
      "Iteration 612, loss = 0.10276182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.67824209\n",
      "Iteration 2, loss = 3.14029594\n",
      "Iteration 3, loss = 2.48309140\n",
      "Iteration 4, loss = 1.66612375\n",
      "Iteration 5, loss = 1.17751406\n",
      "Iteration 6, loss = 0.87378753\n",
      "Iteration 7, loss = 0.65729893\n",
      "Iteration 8, loss = 0.52013594\n",
      "Iteration 9, loss = 0.45673638\n",
      "Iteration 10, loss = 0.45558924\n",
      "Iteration 11, loss = 0.42803845\n",
      "Iteration 12, loss = 0.42177251\n",
      "Iteration 13, loss = 0.39251770\n",
      "Iteration 14, loss = 0.39045088\n",
      "Iteration 15, loss = 0.35510954\n",
      "Iteration 16, loss = 0.37744887\n",
      "Iteration 17, loss = 0.38977512\n",
      "Iteration 18, loss = 0.45288443\n",
      "Iteration 19, loss = 0.40375903\n",
      "Iteration 20, loss = 0.43070250\n",
      "Iteration 21, loss = 0.38548195\n",
      "Iteration 22, loss = 0.36265732\n",
      "Iteration 23, loss = 0.37032906\n",
      "Iteration 24, loss = 0.37651871\n",
      "Iteration 25, loss = 0.40574623\n",
      "Iteration 26, loss = 0.38697155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.22987473\n",
      "Iteration 2, loss = 2.71889665\n",
      "Iteration 3, loss = 2.02218078\n",
      "Iteration 4, loss = 1.43694753\n",
      "Iteration 5, loss = 1.03251168\n",
      "Iteration 6, loss = 0.74347874\n",
      "Iteration 7, loss = 0.56297267\n",
      "Iteration 8, loss = 0.47129592\n",
      "Iteration 9, loss = 0.41606966\n",
      "Iteration 10, loss = 0.40749953\n",
      "Iteration 11, loss = 0.37385984\n",
      "Iteration 12, loss = 0.36016917\n",
      "Iteration 13, loss = 0.33649005\n",
      "Iteration 14, loss = 0.31753617\n",
      "Iteration 15, loss = 0.39224536\n",
      "Iteration 16, loss = 0.48957129\n",
      "Iteration 17, loss = 0.53649797\n",
      "Iteration 18, loss = 0.54319042\n",
      "Iteration 19, loss = 0.53162618\n",
      "Iteration 20, loss = 0.46069532\n",
      "Iteration 21, loss = 0.43036714\n",
      "Iteration 22, loss = 0.40084859\n",
      "Iteration 23, loss = 0.34815530\n",
      "Iteration 24, loss = 0.34360917\n",
      "Iteration 25, loss = 0.35193175\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.74505625\n",
      "Iteration 2, loss = 2.86301206\n",
      "Iteration 3, loss = 1.90546658\n",
      "Iteration 4, loss = 1.27631369\n",
      "Iteration 5, loss = 0.93410133\n",
      "Iteration 6, loss = 0.68589720\n",
      "Iteration 7, loss = 0.51807118\n",
      "Iteration 8, loss = 0.39354774\n",
      "Iteration 9, loss = 0.30309590\n",
      "Iteration 10, loss = 0.24095942\n",
      "Iteration 11, loss = 0.20918530\n",
      "Iteration 12, loss = 0.18874280\n",
      "Iteration 13, loss = 0.19841627\n",
      "Iteration 14, loss = 0.27330161\n",
      "Iteration 15, loss = 0.34350985\n",
      "Iteration 16, loss = 0.36736250\n",
      "Iteration 17, loss = 0.36365313\n",
      "Iteration 18, loss = 0.39162831\n",
      "Iteration 19, loss = 0.40845373\n",
      "Iteration 20, loss = 0.45407633\n",
      "Iteration 21, loss = 0.44774038\n",
      "Iteration 22, loss = 0.43379819\n",
      "Iteration 23, loss = 0.47438249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.40102215\n",
      "Iteration 2, loss = 2.41148441\n",
      "Iteration 3, loss = 1.80163691\n",
      "Iteration 4, loss = 1.29451913\n",
      "Iteration 5, loss = 0.95030780\n",
      "Iteration 6, loss = 0.68147118\n",
      "Iteration 7, loss = 0.50969591\n",
      "Iteration 8, loss = 0.37898332\n",
      "Iteration 9, loss = 0.30201868\n",
      "Iteration 10, loss = 0.25176563\n",
      "Iteration 11, loss = 0.22278116\n",
      "Iteration 12, loss = 0.21422660\n",
      "Iteration 13, loss = 0.28835203\n",
      "Iteration 14, loss = 0.45510493\n",
      "Iteration 15, loss = 0.51614556\n",
      "Iteration 16, loss = 0.57114097\n",
      "Iteration 17, loss = 0.50544032\n",
      "Iteration 18, loss = 0.51061916\n",
      "Iteration 19, loss = 0.42022774\n",
      "Iteration 20, loss = 0.37703994\n",
      "Iteration 21, loss = 0.32263788\n",
      "Iteration 22, loss = 0.27630984\n",
      "Iteration 23, loss = 0.24349423\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.30639650\n",
      "Iteration 2, loss = 2.66786484\n",
      "Iteration 3, loss = 1.94238669\n",
      "Iteration 4, loss = 1.40701998\n",
      "Iteration 5, loss = 0.96213692\n",
      "Iteration 6, loss = 0.71151701\n",
      "Iteration 7, loss = 0.52614640\n",
      "Iteration 8, loss = 0.39527583\n",
      "Iteration 9, loss = 0.30714442\n",
      "Iteration 10, loss = 0.27246297\n",
      "Iteration 11, loss = 0.27016477\n",
      "Iteration 12, loss = 0.25873788\n",
      "Iteration 13, loss = 0.25116659\n",
      "Iteration 14, loss = 0.26023029\n",
      "Iteration 15, loss = 0.31001368\n",
      "Iteration 16, loss = 0.36830671\n",
      "Iteration 17, loss = 0.44960324\n",
      "Iteration 18, loss = 0.61113174\n",
      "Iteration 19, loss = 0.63803087\n",
      "Iteration 20, loss = 0.60975139\n",
      "Iteration 21, loss = 0.52995734\n",
      "Iteration 22, loss = 0.44492196\n",
      "Iteration 23, loss = 0.36046406\n",
      "Iteration 24, loss = 0.30518535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14174639\n",
      "Iteration 2, loss = 1.00707957\n",
      "Iteration 3, loss = 0.41545283\n",
      "Iteration 4, loss = 0.24690174\n",
      "Iteration 5, loss = 0.17792746\n",
      "Iteration 6, loss = 0.14478526\n",
      "Iteration 7, loss = 0.12257162\n",
      "Iteration 8, loss = 0.10982449\n",
      "Iteration 9, loss = 0.10184064\n",
      "Iteration 10, loss = 0.09951860\n",
      "Iteration 11, loss = 0.09365231\n",
      "Iteration 12, loss = 0.09114853\n",
      "Iteration 13, loss = 0.08883616\n",
      "Iteration 14, loss = 0.08713044\n",
      "Iteration 15, loss = 0.08608240\n",
      "Iteration 16, loss = 0.08487929\n",
      "Iteration 17, loss = 0.08390506\n",
      "Iteration 18, loss = 0.08312451\n",
      "Iteration 19, loss = 0.08221316\n",
      "Iteration 20, loss = 0.08150649\n",
      "Iteration 21, loss = 0.08077064\n",
      "Iteration 22, loss = 0.08002337\n",
      "Iteration 23, loss = 0.07933422\n",
      "Iteration 24, loss = 0.07868625\n",
      "Iteration 25, loss = 0.07812233\n",
      "Iteration 26, loss = 0.07739381\n",
      "Iteration 27, loss = 0.07678919\n",
      "Iteration 28, loss = 0.07619331\n",
      "Iteration 29, loss = 0.07563642\n",
      "Iteration 30, loss = 0.07511098\n",
      "Iteration 31, loss = 0.07454209\n",
      "Iteration 32, loss = 0.07400577\n",
      "Iteration 33, loss = 0.07353260\n",
      "Iteration 34, loss = 0.07298853\n",
      "Iteration 35, loss = 0.07247845\n",
      "Iteration 36, loss = 0.07198904\n",
      "Iteration 37, loss = 0.07150165\n",
      "Iteration 38, loss = 0.07101407\n",
      "Iteration 39, loss = 0.07054964\n",
      "Iteration 40, loss = 0.07011632\n",
      "Iteration 41, loss = 0.06967101\n",
      "Iteration 42, loss = 0.06922347\n",
      "Iteration 43, loss = 0.06878416\n",
      "Iteration 44, loss = 0.06837281\n",
      "Iteration 45, loss = 0.06795935\n",
      "Iteration 46, loss = 0.06754690\n",
      "Iteration 47, loss = 0.06715032\n",
      "Iteration 48, loss = 0.06676067\n",
      "Iteration 49, loss = 0.06630840\n",
      "Iteration 50, loss = 0.06598375\n",
      "Iteration 51, loss = 0.06554178\n",
      "Iteration 52, loss = 0.06518468\n",
      "Iteration 53, loss = 0.06485148\n",
      "Iteration 54, loss = 0.06442543\n",
      "Iteration 55, loss = 0.06406242\n",
      "Iteration 56, loss = 0.06369970\n",
      "Iteration 57, loss = 0.06336709\n",
      "Iteration 58, loss = 0.06301635\n",
      "Iteration 59, loss = 0.06264970\n",
      "Iteration 60, loss = 0.06234372\n",
      "Iteration 61, loss = 0.06203568\n",
      "Iteration 62, loss = 0.06165036\n",
      "Iteration 63, loss = 0.06132747\n",
      "Iteration 64, loss = 0.06105450\n",
      "Iteration 65, loss = 0.06073000\n",
      "Iteration 66, loss = 0.06039950\n",
      "Iteration 67, loss = 0.06009580\n",
      "Iteration 68, loss = 0.05976900\n",
      "Iteration 69, loss = 0.05949134\n",
      "Iteration 70, loss = 0.05920446\n",
      "Iteration 71, loss = 0.05888447\n",
      "Iteration 72, loss = 0.05863977\n",
      "Iteration 73, loss = 0.05832028\n",
      "Iteration 74, loss = 0.05808805\n",
      "Iteration 75, loss = 0.05778860\n",
      "Iteration 76, loss = 0.05748429\n",
      "Iteration 77, loss = 0.05725733\n",
      "Iteration 78, loss = 0.05693384\n",
      "Iteration 79, loss = 0.05672238\n",
      "Iteration 80, loss = 0.05646790\n",
      "Iteration 81, loss = 0.05619620\n",
      "Iteration 82, loss = 0.05594666\n",
      "Iteration 83, loss = 0.05568506\n",
      "Iteration 84, loss = 0.05544770\n",
      "Iteration 85, loss = 0.05520077\n",
      "Iteration 86, loss = 0.05496783\n",
      "Iteration 87, loss = 0.05473501\n",
      "Iteration 88, loss = 0.05450647\n",
      "Iteration 89, loss = 0.05429395\n",
      "Iteration 90, loss = 0.05406075\n",
      "Iteration 91, loss = 0.05385636\n",
      "Iteration 92, loss = 0.05363077\n",
      "Iteration 93, loss = 0.05345884\n",
      "Iteration 94, loss = 0.05318627\n",
      "Iteration 95, loss = 0.05298853\n",
      "Iteration 96, loss = 0.05278566\n",
      "Iteration 97, loss = 0.05258564\n",
      "Iteration 98, loss = 0.05236852\n",
      "Iteration 99, loss = 0.05215648\n",
      "Iteration 100, loss = 0.05195498\n",
      "Iteration 101, loss = 0.05174854\n",
      "Iteration 102, loss = 0.05157998\n",
      "Iteration 103, loss = 0.05136557\n",
      "Iteration 104, loss = 0.05119932\n",
      "Iteration 105, loss = 0.05098151\n",
      "Iteration 106, loss = 0.05083099\n",
      "Iteration 107, loss = 0.05062845\n",
      "Iteration 108, loss = 0.05045164\n",
      "Iteration 109, loss = 0.05028779\n",
      "Iteration 110, loss = 0.05013946\n",
      "Iteration 111, loss = 0.04995783\n",
      "Iteration 112, loss = 0.04979259\n",
      "Iteration 113, loss = 0.04960260\n",
      "Iteration 114, loss = 0.04943658\n",
      "Iteration 115, loss = 0.04926390\n",
      "Iteration 116, loss = 0.04910363\n",
      "Iteration 117, loss = 0.04899576\n",
      "Iteration 118, loss = 0.04878299\n",
      "Iteration 119, loss = 0.04864170\n",
      "Iteration 120, loss = 0.04849298\n",
      "Iteration 121, loss = 0.04835761\n",
      "Iteration 122, loss = 0.04819069\n",
      "Iteration 123, loss = 0.04805094\n",
      "Iteration 124, loss = 0.04791217\n",
      "Iteration 125, loss = 0.04777944\n",
      "Iteration 126, loss = 0.04762180\n",
      "Iteration 127, loss = 0.04747059\n",
      "Iteration 128, loss = 0.04734297\n",
      "Iteration 129, loss = 0.04725217\n",
      "Iteration 130, loss = 0.04706867\n",
      "Iteration 131, loss = 0.04696299\n",
      "Iteration 132, loss = 0.04682521\n",
      "Iteration 133, loss = 0.04668761\n",
      "Iteration 134, loss = 0.04654350\n",
      "Iteration 135, loss = 0.04641822\n",
      "Iteration 136, loss = 0.04629508\n",
      "Iteration 137, loss = 0.04618999\n",
      "Iteration 138, loss = 0.04607634\n",
      "Iteration 139, loss = 0.04593569\n",
      "Iteration 140, loss = 0.04582072\n",
      "Iteration 141, loss = 0.04570858\n",
      "Iteration 142, loss = 0.04560198\n",
      "Iteration 143, loss = 0.04547459\n",
      "Iteration 144, loss = 0.04534946\n",
      "Iteration 145, loss = 0.04529573\n",
      "Iteration 146, loss = 0.04515449\n",
      "Iteration 147, loss = 0.04505256\n",
      "Iteration 148, loss = 0.04492838\n",
      "Iteration 149, loss = 0.04484137\n",
      "Iteration 150, loss = 0.04472037\n",
      "Iteration 151, loss = 0.04464320\n",
      "Iteration 152, loss = 0.04453852\n",
      "Iteration 153, loss = 0.04445615\n",
      "Iteration 154, loss = 0.04432851\n",
      "Iteration 155, loss = 0.04422859\n",
      "Iteration 156, loss = 0.04413876\n",
      "Iteration 157, loss = 0.04405990\n",
      "Iteration 158, loss = 0.04396212\n",
      "Iteration 159, loss = 0.04390298\n",
      "Iteration 160, loss = 0.04377665\n",
      "Iteration 161, loss = 0.04367662\n",
      "Iteration 162, loss = 0.04358427\n",
      "Iteration 163, loss = 0.04348394\n",
      "Iteration 164, loss = 0.04339815\n",
      "Iteration 165, loss = 0.04331221\n",
      "Iteration 166, loss = 0.04325483\n",
      "Iteration 167, loss = 0.04317355\n",
      "Iteration 168, loss = 0.04305927\n",
      "Iteration 169, loss = 0.04301923\n",
      "Iteration 170, loss = 0.04290404\n",
      "Iteration 171, loss = 0.04284450\n",
      "Iteration 172, loss = 0.04276685\n",
      "Iteration 173, loss = 0.04268910\n",
      "Iteration 174, loss = 0.04259978\n",
      "Iteration 175, loss = 0.04252742\n",
      "Iteration 176, loss = 0.04245386\n",
      "Iteration 177, loss = 0.04239791\n",
      "Iteration 178, loss = 0.04230129\n",
      "Iteration 179, loss = 0.04225240\n",
      "Iteration 180, loss = 0.04216562\n",
      "Iteration 181, loss = 0.04210923\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14444420\n",
      "Iteration 2, loss = 0.98765191\n",
      "Iteration 3, loss = 0.39424127\n",
      "Iteration 4, loss = 0.25668283\n",
      "Iteration 5, loss = 0.19464428\n",
      "Iteration 6, loss = 0.14605353\n",
      "Iteration 7, loss = 0.12758701\n",
      "Iteration 8, loss = 0.11374153\n",
      "Iteration 9, loss = 0.10360095\n",
      "Iteration 10, loss = 0.10219652\n",
      "Iteration 11, loss = 0.09392411\n",
      "Iteration 12, loss = 0.09102136\n",
      "Iteration 13, loss = 0.08866162\n",
      "Iteration 14, loss = 0.08685559\n",
      "Iteration 15, loss = 0.08577447\n",
      "Iteration 16, loss = 0.08449046\n",
      "Iteration 17, loss = 0.08340045\n",
      "Iteration 18, loss = 0.08256436\n",
      "Iteration 19, loss = 0.08173808\n",
      "Iteration 20, loss = 0.08099512\n",
      "Iteration 21, loss = 0.08018199\n",
      "Iteration 22, loss = 0.07968813\n",
      "Iteration 23, loss = 0.07887536\n",
      "Iteration 24, loss = 0.07809087\n",
      "Iteration 25, loss = 0.07749904\n",
      "Iteration 26, loss = 0.07680062\n",
      "Iteration 27, loss = 0.07620363\n",
      "Iteration 28, loss = 0.07561653\n",
      "Iteration 29, loss = 0.07513096\n",
      "Iteration 30, loss = 0.07457808\n",
      "Iteration 31, loss = 0.07398079\n",
      "Iteration 32, loss = 0.07344579\n",
      "Iteration 33, loss = 0.07296994\n",
      "Iteration 34, loss = 0.07242293\n",
      "Iteration 35, loss = 0.07193983\n",
      "Iteration 36, loss = 0.07147200\n",
      "Iteration 37, loss = 0.07094823\n",
      "Iteration 38, loss = 0.07049923\n",
      "Iteration 39, loss = 0.06999956\n",
      "Iteration 40, loss = 0.06959811\n",
      "Iteration 41, loss = 0.06911108\n",
      "Iteration 42, loss = 0.06866953\n",
      "Iteration 43, loss = 0.06829924\n",
      "Iteration 44, loss = 0.06780246\n",
      "Iteration 45, loss = 0.06743651\n",
      "Iteration 46, loss = 0.06699035\n",
      "Iteration 47, loss = 0.06658437\n",
      "Iteration 48, loss = 0.06620548\n",
      "Iteration 49, loss = 0.06578255\n",
      "Iteration 50, loss = 0.06540463\n",
      "Iteration 51, loss = 0.06499530\n",
      "Iteration 52, loss = 0.06466161\n",
      "Iteration 53, loss = 0.06425730\n",
      "Iteration 54, loss = 0.06391398\n",
      "Iteration 55, loss = 0.06353132\n",
      "Iteration 56, loss = 0.06312810\n",
      "Iteration 57, loss = 0.06279961\n",
      "Iteration 58, loss = 0.06246309\n",
      "Iteration 59, loss = 0.06209298\n",
      "Iteration 60, loss = 0.06179455\n",
      "Iteration 61, loss = 0.06146011\n",
      "Iteration 62, loss = 0.06117189\n",
      "Iteration 63, loss = 0.06079771\n",
      "Iteration 64, loss = 0.06047851\n",
      "Iteration 65, loss = 0.06014821\n",
      "Iteration 66, loss = 0.05983072\n",
      "Iteration 67, loss = 0.05953534\n",
      "Iteration 68, loss = 0.05923702\n",
      "Iteration 69, loss = 0.05891880\n",
      "Iteration 70, loss = 0.05862299\n",
      "Iteration 71, loss = 0.05833349\n",
      "Iteration 72, loss = 0.05811947\n",
      "Iteration 73, loss = 0.05781599\n",
      "Iteration 74, loss = 0.05756162\n",
      "Iteration 75, loss = 0.05721106\n",
      "Iteration 76, loss = 0.05698369\n",
      "Iteration 77, loss = 0.05666629\n",
      "Iteration 78, loss = 0.05640794\n",
      "Iteration 79, loss = 0.05611540\n",
      "Iteration 80, loss = 0.05591418\n",
      "Iteration 81, loss = 0.05563787\n",
      "Iteration 82, loss = 0.05541831\n",
      "Iteration 83, loss = 0.05511833\n",
      "Iteration 84, loss = 0.05486874\n",
      "Iteration 85, loss = 0.05463654\n",
      "Iteration 86, loss = 0.05441383\n",
      "Iteration 87, loss = 0.05416670\n",
      "Iteration 88, loss = 0.05394509\n",
      "Iteration 89, loss = 0.05373285\n",
      "Iteration 90, loss = 0.05350060\n",
      "Iteration 91, loss = 0.05327193\n",
      "Iteration 92, loss = 0.05306108\n",
      "Iteration 93, loss = 0.05288080\n",
      "Iteration 94, loss = 0.05260876\n",
      "Iteration 95, loss = 0.05243058\n",
      "Iteration 96, loss = 0.05225025\n",
      "Iteration 97, loss = 0.05205240\n",
      "Iteration 98, loss = 0.05180458\n",
      "Iteration 99, loss = 0.05158571\n",
      "Iteration 100, loss = 0.05139216\n",
      "Iteration 101, loss = 0.05120471\n",
      "Iteration 102, loss = 0.05102215\n",
      "Iteration 103, loss = 0.05080354\n",
      "Iteration 104, loss = 0.05068330\n",
      "Iteration 105, loss = 0.05043300\n",
      "Iteration 106, loss = 0.05025453\n",
      "Iteration 107, loss = 0.05007985\n",
      "Iteration 108, loss = 0.04990545\n",
      "Iteration 109, loss = 0.04971675\n",
      "Iteration 110, loss = 0.04957692\n",
      "Iteration 111, loss = 0.04939460\n",
      "Iteration 112, loss = 0.04922429\n",
      "Iteration 113, loss = 0.04904446\n",
      "Iteration 114, loss = 0.04886729\n",
      "Iteration 115, loss = 0.04872189\n",
      "Iteration 116, loss = 0.04856106\n",
      "Iteration 117, loss = 0.04840533\n",
      "Iteration 118, loss = 0.04823099\n",
      "Iteration 119, loss = 0.04807950\n",
      "Iteration 120, loss = 0.04792753\n",
      "Iteration 121, loss = 0.04781464\n",
      "Iteration 122, loss = 0.04763337\n",
      "Iteration 123, loss = 0.04750553\n",
      "Iteration 124, loss = 0.04735877\n",
      "Iteration 125, loss = 0.04722012\n",
      "Iteration 126, loss = 0.04706931\n",
      "Iteration 127, loss = 0.04694544\n",
      "Iteration 128, loss = 0.04678480\n",
      "Iteration 129, loss = 0.04667049\n",
      "Iteration 130, loss = 0.04653024\n",
      "Iteration 131, loss = 0.04640838\n",
      "Iteration 132, loss = 0.04626026\n",
      "Iteration 133, loss = 0.04613109\n",
      "Iteration 134, loss = 0.04598713\n",
      "Iteration 135, loss = 0.04589581\n",
      "Iteration 136, loss = 0.04575435\n",
      "Iteration 137, loss = 0.04561462\n",
      "Iteration 138, loss = 0.04550238\n",
      "Iteration 139, loss = 0.04535650\n",
      "Iteration 140, loss = 0.04526002\n",
      "Iteration 141, loss = 0.04512426\n",
      "Iteration 142, loss = 0.04501988\n",
      "Iteration 143, loss = 0.04490451\n",
      "Iteration 144, loss = 0.04480294\n",
      "Iteration 145, loss = 0.04474629\n",
      "Iteration 146, loss = 0.04458141\n",
      "Iteration 147, loss = 0.04448442\n",
      "Iteration 148, loss = 0.04436398\n",
      "Iteration 149, loss = 0.04425970\n",
      "Iteration 150, loss = 0.04414783\n",
      "Iteration 151, loss = 0.04404915\n",
      "Iteration 152, loss = 0.04395250\n",
      "Iteration 153, loss = 0.04386357\n",
      "Iteration 154, loss = 0.04373934\n",
      "Iteration 155, loss = 0.04364408\n",
      "Iteration 156, loss = 0.04354324\n",
      "Iteration 157, loss = 0.04345024\n",
      "Iteration 158, loss = 0.04339070\n",
      "Iteration 159, loss = 0.04329810\n",
      "Iteration 160, loss = 0.04322048\n",
      "Iteration 161, loss = 0.04310736\n",
      "Iteration 162, loss = 0.04301836\n",
      "Iteration 163, loss = 0.04289344\n",
      "Iteration 164, loss = 0.04280930\n",
      "Iteration 165, loss = 0.04272779\n",
      "Iteration 166, loss = 0.04265574\n",
      "Iteration 167, loss = 0.04255114\n",
      "Iteration 168, loss = 0.04245814\n",
      "Iteration 169, loss = 0.04241435\n",
      "Iteration 170, loss = 0.04232412\n",
      "Iteration 171, loss = 0.04225136\n",
      "Iteration 172, loss = 0.04215085\n",
      "Iteration 173, loss = 0.04209571\n",
      "Iteration 174, loss = 0.04202045\n",
      "Iteration 175, loss = 0.04193705\n",
      "Iteration 176, loss = 0.04185776\n",
      "Iteration 177, loss = 0.04179945\n",
      "Iteration 178, loss = 0.04171917\n",
      "Iteration 179, loss = 0.04166479\n",
      "Iteration 180, loss = 0.04153988\n",
      "Iteration 181, loss = 0.04150904\n",
      "Iteration 182, loss = 0.04143268\n",
      "Iteration 183, loss = 0.04138283\n",
      "Iteration 184, loss = 0.04130164\n",
      "Iteration 185, loss = 0.04124028\n",
      "Iteration 186, loss = 0.04117020\n",
      "Iteration 187, loss = 0.04108220\n",
      "Iteration 188, loss = 0.04104164\n",
      "Iteration 189, loss = 0.04093886\n",
      "Iteration 190, loss = 0.04089551\n",
      "Iteration 191, loss = 0.04083561\n",
      "Iteration 192, loss = 0.04078102\n",
      "Iteration 193, loss = 0.04072656\n",
      "Iteration 194, loss = 0.04066857\n",
      "Iteration 195, loss = 0.04060692\n",
      "Iteration 196, loss = 0.04054603\n",
      "Iteration 197, loss = 0.04047508\n",
      "Iteration 198, loss = 0.04043209\n",
      "Iteration 199, loss = 0.04037599\n",
      "Iteration 200, loss = 0.04033397\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15620262\n",
      "Iteration 2, loss = 1.03016595\n",
      "Iteration 3, loss = 0.41649462\n",
      "Iteration 4, loss = 0.25433228\n",
      "Iteration 5, loss = 0.18567878\n",
      "Iteration 6, loss = 0.15477256\n",
      "Iteration 7, loss = 0.12746175\n",
      "Iteration 8, loss = 0.11346735\n",
      "Iteration 9, loss = 0.10745494\n",
      "Iteration 10, loss = 0.09815875\n",
      "Iteration 11, loss = 0.09548751\n",
      "Iteration 12, loss = 0.09212556\n",
      "Iteration 13, loss = 0.08989028\n",
      "Iteration 14, loss = 0.08815150\n",
      "Iteration 15, loss = 0.08664514\n",
      "Iteration 16, loss = 0.08563092\n",
      "Iteration 17, loss = 0.08452930\n",
      "Iteration 18, loss = 0.08359202\n",
      "Iteration 19, loss = 0.08261444\n",
      "Iteration 20, loss = 0.08186504\n",
      "Iteration 21, loss = 0.08105795\n",
      "Iteration 22, loss = 0.08040185\n",
      "Iteration 23, loss = 0.07965155\n",
      "Iteration 24, loss = 0.07893334\n",
      "Iteration 25, loss = 0.07831882\n",
      "Iteration 26, loss = 0.07773741\n",
      "Iteration 27, loss = 0.07709074\n",
      "Iteration 28, loss = 0.07654071\n",
      "Iteration 29, loss = 0.07600798\n",
      "Iteration 30, loss = 0.07538979\n",
      "Iteration 31, loss = 0.07488315\n",
      "Iteration 32, loss = 0.07433686\n",
      "Iteration 33, loss = 0.07381376\n",
      "Iteration 34, loss = 0.07334253\n",
      "Iteration 35, loss = 0.07281571\n",
      "Iteration 36, loss = 0.07228702\n",
      "Iteration 37, loss = 0.07186158\n",
      "Iteration 38, loss = 0.07132332\n",
      "Iteration 39, loss = 0.07085567\n",
      "Iteration 40, loss = 0.07041284\n",
      "Iteration 41, loss = 0.06999196\n",
      "Iteration 42, loss = 0.06950844\n",
      "Iteration 43, loss = 0.06910671\n",
      "Iteration 44, loss = 0.06866025\n",
      "Iteration 45, loss = 0.06829865\n",
      "Iteration 46, loss = 0.06786362\n",
      "Iteration 47, loss = 0.06741036\n",
      "Iteration 48, loss = 0.06701490\n",
      "Iteration 49, loss = 0.06659408\n",
      "Iteration 50, loss = 0.06621973\n",
      "Iteration 51, loss = 0.06581199\n",
      "Iteration 52, loss = 0.06544517\n",
      "Iteration 53, loss = 0.06506766\n",
      "Iteration 54, loss = 0.06471845\n",
      "Iteration 55, loss = 0.06439741\n",
      "Iteration 56, loss = 0.06403686\n",
      "Iteration 57, loss = 0.06364869\n",
      "Iteration 58, loss = 0.06332595\n",
      "Iteration 59, loss = 0.06294266\n",
      "Iteration 60, loss = 0.06259425\n",
      "Iteration 61, loss = 0.06224966\n",
      "Iteration 62, loss = 0.06192313\n",
      "Iteration 63, loss = 0.06159580\n",
      "Iteration 64, loss = 0.06136116\n",
      "Iteration 65, loss = 0.06098011\n",
      "Iteration 66, loss = 0.06068895\n",
      "Iteration 67, loss = 0.06036011\n",
      "Iteration 68, loss = 0.06010611\n",
      "Iteration 69, loss = 0.05975504\n",
      "Iteration 70, loss = 0.05946392\n",
      "Iteration 71, loss = 0.05918866\n",
      "Iteration 72, loss = 0.05889102\n",
      "Iteration 73, loss = 0.05862091\n",
      "Iteration 74, loss = 0.05833488\n",
      "Iteration 75, loss = 0.05806147\n",
      "Iteration 76, loss = 0.05777902\n",
      "Iteration 77, loss = 0.05752567\n",
      "Iteration 78, loss = 0.05724636\n",
      "Iteration 79, loss = 0.05699639\n",
      "Iteration 80, loss = 0.05673240\n",
      "Iteration 81, loss = 0.05652885\n",
      "Iteration 82, loss = 0.05625113\n",
      "Iteration 83, loss = 0.05598649\n",
      "Iteration 84, loss = 0.05575674\n",
      "Iteration 85, loss = 0.05554292\n",
      "Iteration 86, loss = 0.05529079\n",
      "Iteration 87, loss = 0.05503303\n",
      "Iteration 88, loss = 0.05480811\n",
      "Iteration 89, loss = 0.05460705\n",
      "Iteration 90, loss = 0.05438462\n",
      "Iteration 91, loss = 0.05415681\n",
      "Iteration 92, loss = 0.05392438\n",
      "Iteration 93, loss = 0.05371707\n",
      "Iteration 94, loss = 0.05349276\n",
      "Iteration 95, loss = 0.05328312\n",
      "Iteration 96, loss = 0.05305915\n",
      "Iteration 97, loss = 0.05285434\n",
      "Iteration 98, loss = 0.05265019\n",
      "Iteration 99, loss = 0.05244242\n",
      "Iteration 100, loss = 0.05225131\n",
      "Iteration 101, loss = 0.05207066\n",
      "Iteration 102, loss = 0.05186280\n",
      "Iteration 103, loss = 0.05170594\n",
      "Iteration 104, loss = 0.05152362\n",
      "Iteration 105, loss = 0.05130814\n",
      "Iteration 106, loss = 0.05113526\n",
      "Iteration 107, loss = 0.05095893\n",
      "Iteration 108, loss = 0.05077862\n",
      "Iteration 109, loss = 0.05059816\n",
      "Iteration 110, loss = 0.05042662\n",
      "Iteration 111, loss = 0.05026634\n",
      "Iteration 112, loss = 0.05007895\n",
      "Iteration 113, loss = 0.04991234\n",
      "Iteration 114, loss = 0.04973552\n",
      "Iteration 115, loss = 0.04957722\n",
      "Iteration 116, loss = 0.04942764\n",
      "Iteration 117, loss = 0.04924153\n",
      "Iteration 118, loss = 0.04910025\n",
      "Iteration 119, loss = 0.04895003\n",
      "Iteration 120, loss = 0.04882637\n",
      "Iteration 121, loss = 0.04865294\n",
      "Iteration 122, loss = 0.04848071\n",
      "Iteration 123, loss = 0.04834805\n",
      "Iteration 124, loss = 0.04818169\n",
      "Iteration 125, loss = 0.04804448\n",
      "Iteration 126, loss = 0.04789156\n",
      "Iteration 127, loss = 0.04776092\n",
      "Iteration 128, loss = 0.04763902\n",
      "Iteration 129, loss = 0.04749199\n",
      "Iteration 130, loss = 0.04735228\n",
      "Iteration 131, loss = 0.04720972\n",
      "Iteration 132, loss = 0.04711468\n",
      "Iteration 133, loss = 0.04698801\n",
      "Iteration 134, loss = 0.04683014\n",
      "Iteration 135, loss = 0.04670838\n",
      "Iteration 136, loss = 0.04658463\n",
      "Iteration 137, loss = 0.04645379\n",
      "Iteration 138, loss = 0.04635036\n",
      "Iteration 139, loss = 0.04621306\n",
      "Iteration 140, loss = 0.04608035\n",
      "Iteration 141, loss = 0.04600032\n",
      "Iteration 142, loss = 0.04585421\n",
      "Iteration 143, loss = 0.04573562\n",
      "Iteration 144, loss = 0.04563436\n",
      "Iteration 145, loss = 0.04551536\n",
      "Iteration 146, loss = 0.04542055\n",
      "Iteration 147, loss = 0.04533742\n",
      "Iteration 148, loss = 0.04518875\n",
      "Iteration 149, loss = 0.04508974\n",
      "Iteration 150, loss = 0.04501785\n",
      "Iteration 151, loss = 0.04488093\n",
      "Iteration 152, loss = 0.04477484\n",
      "Iteration 153, loss = 0.04467322\n",
      "Iteration 154, loss = 0.04457828\n",
      "Iteration 155, loss = 0.04448325\n",
      "Iteration 156, loss = 0.04437818\n",
      "Iteration 157, loss = 0.04428825\n",
      "Iteration 158, loss = 0.04419756\n",
      "Iteration 159, loss = 0.04410754\n",
      "Iteration 160, loss = 0.04400233\n",
      "Iteration 161, loss = 0.04392690\n",
      "Iteration 162, loss = 0.04385743\n",
      "Iteration 163, loss = 0.04373275\n",
      "Iteration 164, loss = 0.04367090\n",
      "Iteration 165, loss = 0.04356552\n",
      "Iteration 166, loss = 0.04347070\n",
      "Iteration 167, loss = 0.04338437\n",
      "Iteration 168, loss = 0.04331609\n",
      "Iteration 169, loss = 0.04322676\n",
      "Iteration 170, loss = 0.04315338\n",
      "Iteration 171, loss = 0.04307123\n",
      "Iteration 172, loss = 0.04299267\n",
      "Iteration 173, loss = 0.04294787\n",
      "Iteration 174, loss = 0.04282160\n",
      "Iteration 175, loss = 0.04276507\n",
      "Iteration 176, loss = 0.04267684\n",
      "Iteration 177, loss = 0.04259957\n",
      "Iteration 178, loss = 0.04252968\n",
      "Iteration 179, loss = 0.04244644\n",
      "Iteration 180, loss = 0.04237524\n",
      "Iteration 181, loss = 0.04229677\n",
      "Iteration 182, loss = 0.04223500\n",
      "Iteration 183, loss = 0.04215831\n",
      "Iteration 184, loss = 0.04211087\n",
      "Iteration 185, loss = 0.04204845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14626067\n",
      "Iteration 2, loss = 1.01398616\n",
      "Iteration 3, loss = 0.41492507\n",
      "Iteration 4, loss = 0.26528892\n",
      "Iteration 5, loss = 0.18925825\n",
      "Iteration 6, loss = 0.15543109\n",
      "Iteration 7, loss = 0.12904728\n",
      "Iteration 8, loss = 0.11340823\n",
      "Iteration 9, loss = 0.10696673\n",
      "Iteration 10, loss = 0.09964620\n",
      "Iteration 11, loss = 0.09636353\n",
      "Iteration 12, loss = 0.09281690\n",
      "Iteration 13, loss = 0.09064900\n",
      "Iteration 14, loss = 0.08933384\n",
      "Iteration 15, loss = 0.08763200\n",
      "Iteration 16, loss = 0.08657710\n",
      "Iteration 17, loss = 0.08528126\n",
      "Iteration 18, loss = 0.08447269\n",
      "Iteration 19, loss = 0.08344233\n",
      "Iteration 20, loss = 0.08262647\n",
      "Iteration 21, loss = 0.08189065\n",
      "Iteration 22, loss = 0.08116805\n",
      "Iteration 23, loss = 0.08047968\n",
      "Iteration 24, loss = 0.07974033\n",
      "Iteration 25, loss = 0.07916379\n",
      "Iteration 26, loss = 0.07855440\n",
      "Iteration 27, loss = 0.07787845\n",
      "Iteration 28, loss = 0.07731494\n",
      "Iteration 29, loss = 0.07675008\n",
      "Iteration 30, loss = 0.07618040\n",
      "Iteration 31, loss = 0.07563323\n",
      "Iteration 32, loss = 0.07505307\n",
      "Iteration 33, loss = 0.07457206\n",
      "Iteration 34, loss = 0.07406434\n",
      "Iteration 35, loss = 0.07353135\n",
      "Iteration 36, loss = 0.07305775\n",
      "Iteration 37, loss = 0.07258261\n",
      "Iteration 38, loss = 0.07207803\n",
      "Iteration 39, loss = 0.07161670\n",
      "Iteration 40, loss = 0.07117340\n",
      "Iteration 41, loss = 0.07072754\n",
      "Iteration 42, loss = 0.07025331\n",
      "Iteration 43, loss = 0.06983696\n",
      "Iteration 44, loss = 0.06939022\n",
      "Iteration 45, loss = 0.06905329\n",
      "Iteration 46, loss = 0.06859385\n",
      "Iteration 47, loss = 0.06817477\n",
      "Iteration 48, loss = 0.06780540\n",
      "Iteration 49, loss = 0.06734925\n",
      "Iteration 50, loss = 0.06695847\n",
      "Iteration 51, loss = 0.06658536\n",
      "Iteration 52, loss = 0.06621730\n",
      "Iteration 53, loss = 0.06583283\n",
      "Iteration 54, loss = 0.06544966\n",
      "Iteration 55, loss = 0.06514871\n",
      "Iteration 56, loss = 0.06475347\n",
      "Iteration 57, loss = 0.06442005\n",
      "Iteration 58, loss = 0.06404404\n",
      "Iteration 59, loss = 0.06369785\n",
      "Iteration 60, loss = 0.06335351\n",
      "Iteration 61, loss = 0.06302984\n",
      "Iteration 62, loss = 0.06269274\n",
      "Iteration 63, loss = 0.06237246\n",
      "Iteration 64, loss = 0.06210884\n",
      "Iteration 65, loss = 0.06175290\n",
      "Iteration 66, loss = 0.06143996\n",
      "Iteration 67, loss = 0.06112508\n",
      "Iteration 68, loss = 0.06082851\n",
      "Iteration 69, loss = 0.06051605\n",
      "Iteration 70, loss = 0.06022169\n",
      "Iteration 71, loss = 0.05993248\n",
      "Iteration 72, loss = 0.05969357\n",
      "Iteration 73, loss = 0.05937357\n",
      "Iteration 74, loss = 0.05908553\n",
      "Iteration 75, loss = 0.05880918\n",
      "Iteration 76, loss = 0.05853718\n",
      "Iteration 77, loss = 0.05826984\n",
      "Iteration 78, loss = 0.05800002\n",
      "Iteration 79, loss = 0.05777124\n",
      "Iteration 80, loss = 0.05750650\n",
      "Iteration 81, loss = 0.05727978\n",
      "Iteration 82, loss = 0.05701261\n",
      "Iteration 83, loss = 0.05673289\n",
      "Iteration 84, loss = 0.05652108\n",
      "Iteration 85, loss = 0.05626254\n",
      "Iteration 86, loss = 0.05601327\n",
      "Iteration 87, loss = 0.05579048\n",
      "Iteration 88, loss = 0.05555668\n",
      "Iteration 89, loss = 0.05534722\n",
      "Iteration 90, loss = 0.05513011\n",
      "Iteration 91, loss = 0.05488990\n",
      "Iteration 92, loss = 0.05466156\n",
      "Iteration 93, loss = 0.05442937\n",
      "Iteration 94, loss = 0.05423426\n",
      "Iteration 95, loss = 0.05403206\n",
      "Iteration 96, loss = 0.05379731\n",
      "Iteration 97, loss = 0.05360614\n",
      "Iteration 98, loss = 0.05339776\n",
      "Iteration 99, loss = 0.05319970\n",
      "Iteration 100, loss = 0.05299488\n",
      "Iteration 101, loss = 0.05283655\n",
      "Iteration 102, loss = 0.05261891\n",
      "Iteration 103, loss = 0.05244424\n",
      "Iteration 104, loss = 0.05224958\n",
      "Iteration 105, loss = 0.05206602\n",
      "Iteration 106, loss = 0.05188545\n",
      "Iteration 107, loss = 0.05170520\n",
      "Iteration 108, loss = 0.05148865\n",
      "Iteration 109, loss = 0.05133230\n",
      "Iteration 110, loss = 0.05118248\n",
      "Iteration 111, loss = 0.05104387\n",
      "Iteration 112, loss = 0.05082282\n",
      "Iteration 113, loss = 0.05064527\n",
      "Iteration 114, loss = 0.05046235\n",
      "Iteration 115, loss = 0.05031240\n",
      "Iteration 116, loss = 0.05016500\n",
      "Iteration 117, loss = 0.04997072\n",
      "Iteration 118, loss = 0.04983943\n",
      "Iteration 119, loss = 0.04970923\n",
      "Iteration 120, loss = 0.04955144\n",
      "Iteration 121, loss = 0.04941065\n",
      "Iteration 122, loss = 0.04921964\n",
      "Iteration 123, loss = 0.04909995\n",
      "Iteration 124, loss = 0.04893580\n",
      "Iteration 125, loss = 0.04878775\n",
      "Iteration 126, loss = 0.04865508\n",
      "Iteration 127, loss = 0.04851243\n",
      "Iteration 128, loss = 0.04839002\n",
      "Iteration 129, loss = 0.04824322\n",
      "Iteration 130, loss = 0.04808335\n",
      "Iteration 131, loss = 0.04794545\n",
      "Iteration 132, loss = 0.04783293\n",
      "Iteration 133, loss = 0.04771107\n",
      "Iteration 134, loss = 0.04759585\n",
      "Iteration 135, loss = 0.04743803\n",
      "Iteration 136, loss = 0.04731503\n",
      "Iteration 137, loss = 0.04720687\n",
      "Iteration 138, loss = 0.04711205\n",
      "Iteration 139, loss = 0.04698375\n",
      "Iteration 140, loss = 0.04681738\n",
      "Iteration 141, loss = 0.04675111\n",
      "Iteration 142, loss = 0.04658974\n",
      "Iteration 143, loss = 0.04647617\n",
      "Iteration 144, loss = 0.04638391\n",
      "Iteration 145, loss = 0.04626010\n",
      "Iteration 146, loss = 0.04616886\n",
      "Iteration 147, loss = 0.04606920\n",
      "Iteration 148, loss = 0.04591951\n",
      "Iteration 149, loss = 0.04584754\n",
      "Iteration 150, loss = 0.04574362\n",
      "Iteration 151, loss = 0.04560489\n",
      "Iteration 152, loss = 0.04551589\n",
      "Iteration 153, loss = 0.04539835\n",
      "Iteration 154, loss = 0.04530459\n",
      "Iteration 155, loss = 0.04521831\n",
      "Iteration 156, loss = 0.04511513\n",
      "Iteration 157, loss = 0.04502082\n",
      "Iteration 158, loss = 0.04493541\n",
      "Iteration 159, loss = 0.04484028\n",
      "Iteration 160, loss = 0.04475307\n",
      "Iteration 161, loss = 0.04465920\n",
      "Iteration 162, loss = 0.04459384\n",
      "Iteration 163, loss = 0.04447068\n",
      "Iteration 164, loss = 0.04440848\n",
      "Iteration 165, loss = 0.04429270\n",
      "Iteration 166, loss = 0.04420510\n",
      "Iteration 167, loss = 0.04411694\n",
      "Iteration 168, loss = 0.04405383\n",
      "Iteration 169, loss = 0.04395574\n",
      "Iteration 170, loss = 0.04386546\n",
      "Iteration 171, loss = 0.04379726\n",
      "Iteration 172, loss = 0.04372216\n",
      "Iteration 173, loss = 0.04365969\n",
      "Iteration 174, loss = 0.04355198\n",
      "Iteration 175, loss = 0.04349916\n",
      "Iteration 176, loss = 0.04340798\n",
      "Iteration 177, loss = 0.04333091\n",
      "Iteration 178, loss = 0.04327897\n",
      "Iteration 179, loss = 0.04318854\n",
      "Iteration 180, loss = 0.04310854\n",
      "Iteration 181, loss = 0.04303766\n",
      "Iteration 182, loss = 0.04296789\n",
      "Iteration 183, loss = 0.04287646\n",
      "Iteration 184, loss = 0.04282220\n",
      "Iteration 185, loss = 0.04277823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14266308\n",
      "Iteration 2, loss = 0.99201186\n",
      "Iteration 3, loss = 0.36234909\n",
      "Iteration 4, loss = 0.21538915\n",
      "Iteration 5, loss = 0.16000879\n",
      "Iteration 6, loss = 0.13237208\n",
      "Iteration 7, loss = 0.11396698\n",
      "Iteration 8, loss = 0.10591137\n",
      "Iteration 9, loss = 0.10538493\n",
      "Iteration 10, loss = 0.09325614\n",
      "Iteration 11, loss = 0.09095095\n",
      "Iteration 12, loss = 0.08856069\n",
      "Iteration 13, loss = 0.08697423\n",
      "Iteration 14, loss = 0.08591741\n",
      "Iteration 15, loss = 0.08484296\n",
      "Iteration 16, loss = 0.08367039\n",
      "Iteration 17, loss = 0.08263032\n",
      "Iteration 18, loss = 0.08167789\n",
      "Iteration 19, loss = 0.08096971\n",
      "Iteration 20, loss = 0.08023631\n",
      "Iteration 21, loss = 0.07951013\n",
      "Iteration 22, loss = 0.07887632\n",
      "Iteration 23, loss = 0.07820506\n",
      "Iteration 24, loss = 0.07752547\n",
      "Iteration 25, loss = 0.07694070\n",
      "Iteration 26, loss = 0.07629025\n",
      "Iteration 27, loss = 0.07571212\n",
      "Iteration 28, loss = 0.07516282\n",
      "Iteration 29, loss = 0.07463935\n",
      "Iteration 30, loss = 0.07403469\n",
      "Iteration 31, loss = 0.07347600\n",
      "Iteration 32, loss = 0.07297260\n",
      "Iteration 33, loss = 0.07245771\n",
      "Iteration 34, loss = 0.07195106\n",
      "Iteration 35, loss = 0.07145242\n",
      "Iteration 36, loss = 0.07098501\n",
      "Iteration 37, loss = 0.07049668\n",
      "Iteration 38, loss = 0.07000933\n",
      "Iteration 39, loss = 0.06956049\n",
      "Iteration 40, loss = 0.06914360\n",
      "Iteration 41, loss = 0.06865673\n",
      "Iteration 42, loss = 0.06823331\n",
      "Iteration 43, loss = 0.06781375\n",
      "Iteration 44, loss = 0.06738355\n",
      "Iteration 45, loss = 0.06698692\n",
      "Iteration 46, loss = 0.06655887\n",
      "Iteration 47, loss = 0.06616624\n",
      "Iteration 48, loss = 0.06576519\n",
      "Iteration 49, loss = 0.06530645\n",
      "Iteration 50, loss = 0.06494976\n",
      "Iteration 51, loss = 0.06454400\n",
      "Iteration 52, loss = 0.06419438\n",
      "Iteration 53, loss = 0.06381106\n",
      "Iteration 54, loss = 0.06344978\n",
      "Iteration 55, loss = 0.06308442\n",
      "Iteration 56, loss = 0.06273939\n",
      "Iteration 57, loss = 0.06237578\n",
      "Iteration 58, loss = 0.06205692\n",
      "Iteration 59, loss = 0.06166801\n",
      "Iteration 60, loss = 0.06137246\n",
      "Iteration 61, loss = 0.06104287\n",
      "Iteration 62, loss = 0.06070155\n",
      "Iteration 63, loss = 0.06038230\n",
      "Iteration 64, loss = 0.06011804\n",
      "Iteration 65, loss = 0.05975152\n",
      "Iteration 66, loss = 0.05944702\n",
      "Iteration 67, loss = 0.05912815\n",
      "Iteration 68, loss = 0.05886216\n",
      "Iteration 69, loss = 0.05853763\n",
      "Iteration 70, loss = 0.05825261\n",
      "Iteration 71, loss = 0.05794180\n",
      "Iteration 72, loss = 0.05767928\n",
      "Iteration 73, loss = 0.05738007\n",
      "Iteration 74, loss = 0.05712238\n",
      "Iteration 75, loss = 0.05681923\n",
      "Iteration 76, loss = 0.05657421\n",
      "Iteration 77, loss = 0.05628659\n",
      "Iteration 78, loss = 0.05601792\n",
      "Iteration 79, loss = 0.05577848\n",
      "Iteration 80, loss = 0.05553527\n",
      "Iteration 81, loss = 0.05528001\n",
      "Iteration 82, loss = 0.05502502\n",
      "Iteration 83, loss = 0.05475996\n",
      "Iteration 84, loss = 0.05454082\n",
      "Iteration 85, loss = 0.05426847\n",
      "Iteration 86, loss = 0.05402418\n",
      "Iteration 87, loss = 0.05382201\n",
      "Iteration 88, loss = 0.05355021\n",
      "Iteration 89, loss = 0.05333423\n",
      "Iteration 90, loss = 0.05312154\n",
      "Iteration 91, loss = 0.05288790\n",
      "Iteration 92, loss = 0.05266213\n",
      "Iteration 93, loss = 0.05245579\n",
      "Iteration 94, loss = 0.05222689\n",
      "Iteration 95, loss = 0.05203123\n",
      "Iteration 96, loss = 0.05182083\n",
      "Iteration 97, loss = 0.05161377\n",
      "Iteration 98, loss = 0.05139694\n",
      "Iteration 99, loss = 0.05122358\n",
      "Iteration 100, loss = 0.05102077\n",
      "Iteration 101, loss = 0.05084519\n",
      "Iteration 102, loss = 0.05061987\n",
      "Iteration 103, loss = 0.05044383\n",
      "Iteration 104, loss = 0.05024003\n",
      "Iteration 105, loss = 0.05004540\n",
      "Iteration 106, loss = 0.04987427\n",
      "Iteration 107, loss = 0.04969572\n",
      "Iteration 108, loss = 0.04952665\n",
      "Iteration 109, loss = 0.04936813\n",
      "Iteration 110, loss = 0.04919871\n",
      "Iteration 111, loss = 0.04900612\n",
      "Iteration 112, loss = 0.04880514\n",
      "Iteration 113, loss = 0.04865196\n",
      "Iteration 114, loss = 0.04847585\n",
      "Iteration 115, loss = 0.04830762\n",
      "Iteration 116, loss = 0.04816042\n",
      "Iteration 117, loss = 0.04799543\n",
      "Iteration 118, loss = 0.04781641\n",
      "Iteration 119, loss = 0.04769507\n",
      "Iteration 120, loss = 0.04753854\n",
      "Iteration 121, loss = 0.04738546\n",
      "Iteration 122, loss = 0.04723200\n",
      "Iteration 123, loss = 0.04707256\n",
      "Iteration 124, loss = 0.04692733\n",
      "Iteration 125, loss = 0.04679018\n",
      "Iteration 126, loss = 0.04665425\n",
      "Iteration 127, loss = 0.04653659\n",
      "Iteration 128, loss = 0.04637553\n",
      "Iteration 129, loss = 0.04623503\n",
      "Iteration 130, loss = 0.04608030\n",
      "Iteration 131, loss = 0.04594802\n",
      "Iteration 132, loss = 0.04582490\n",
      "Iteration 133, loss = 0.04569875\n",
      "Iteration 134, loss = 0.04554933\n",
      "Iteration 135, loss = 0.04541859\n",
      "Iteration 136, loss = 0.04531884\n",
      "Iteration 137, loss = 0.04518887\n",
      "Iteration 138, loss = 0.04507733\n",
      "Iteration 139, loss = 0.04494777\n",
      "Iteration 140, loss = 0.04483154\n",
      "Iteration 141, loss = 0.04470264\n",
      "Iteration 142, loss = 0.04459614\n",
      "Iteration 143, loss = 0.04448447\n",
      "Iteration 144, loss = 0.04438299\n",
      "Iteration 145, loss = 0.04426065\n",
      "Iteration 146, loss = 0.04415783\n",
      "Iteration 147, loss = 0.04407427\n",
      "Iteration 148, loss = 0.04393780\n",
      "Iteration 149, loss = 0.04383423\n",
      "Iteration 150, loss = 0.04373557\n",
      "Iteration 151, loss = 0.04362472\n",
      "Iteration 152, loss = 0.04353310\n",
      "Iteration 153, loss = 0.04342415\n",
      "Iteration 154, loss = 0.04333269\n",
      "Iteration 155, loss = 0.04323499\n",
      "Iteration 156, loss = 0.04312612\n",
      "Iteration 157, loss = 0.04302656\n",
      "Iteration 158, loss = 0.04292370\n",
      "Iteration 159, loss = 0.04283635\n",
      "Iteration 160, loss = 0.04274574\n",
      "Iteration 161, loss = 0.04268260\n",
      "Iteration 162, loss = 0.04259346\n",
      "Iteration 163, loss = 0.04249539\n",
      "Iteration 164, loss = 0.04239595\n",
      "Iteration 165, loss = 0.04230434\n",
      "Iteration 166, loss = 0.04222173\n",
      "Iteration 167, loss = 0.04213780\n",
      "Iteration 168, loss = 0.04205846\n",
      "Iteration 169, loss = 0.04197297\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.44864283\n",
      "Iteration 2, loss = 0.26102061\n",
      "Iteration 3, loss = 0.16441531\n",
      "Iteration 4, loss = 0.13481298\n",
      "Iteration 5, loss = 0.12307943\n",
      "Iteration 6, loss = 0.10925817\n",
      "Iteration 7, loss = 0.09576028\n",
      "Iteration 8, loss = 0.08367977\n",
      "Iteration 9, loss = 0.07400999\n",
      "Iteration 10, loss = 0.06679399\n",
      "Iteration 11, loss = 0.06190251\n",
      "Iteration 12, loss = 0.05798073\n",
      "Iteration 13, loss = 0.05492908\n",
      "Iteration 14, loss = 0.05216820\n",
      "Iteration 15, loss = 0.05032108\n",
      "Iteration 16, loss = 0.04873659\n",
      "Iteration 17, loss = 0.04715503\n",
      "Iteration 18, loss = 0.04602015\n",
      "Iteration 19, loss = 0.04585071\n",
      "Iteration 20, loss = 0.04528469\n",
      "Iteration 21, loss = 0.04502963\n",
      "Iteration 22, loss = 0.04406353\n",
      "Iteration 23, loss = 0.04332965\n",
      "Iteration 24, loss = 0.04307263\n",
      "Iteration 25, loss = 0.04289180\n",
      "Iteration 26, loss = 0.04199253\n",
      "Iteration 27, loss = 0.04124924\n",
      "Iteration 28, loss = 0.04067162\n",
      "Iteration 29, loss = 0.04062926\n",
      "Iteration 30, loss = 0.04059376\n",
      "Iteration 31, loss = 0.04099816\n",
      "Iteration 32, loss = 0.04059813\n",
      "Iteration 33, loss = 0.04011026\n",
      "Iteration 34, loss = 0.04002114\n",
      "Iteration 35, loss = 0.04000477\n",
      "Iteration 36, loss = 0.04075107\n",
      "Iteration 37, loss = 0.04179495\n",
      "Iteration 38, loss = 0.04228795\n",
      "Iteration 39, loss = 0.04163748\n",
      "Iteration 40, loss = 0.04119502\n",
      "Iteration 41, loss = 0.04026637\n",
      "Iteration 42, loss = 0.04021123\n",
      "Iteration 43, loss = 0.04033458\n",
      "Iteration 44, loss = 0.04153469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47367539\n",
      "Iteration 2, loss = 0.27727439\n",
      "Iteration 3, loss = 0.17284182\n",
      "Iteration 4, loss = 0.13954024\n",
      "Iteration 5, loss = 0.12319723\n",
      "Iteration 6, loss = 0.10930553\n",
      "Iteration 7, loss = 0.09614620\n",
      "Iteration 8, loss = 0.08416605\n",
      "Iteration 9, loss = 0.07447662\n",
      "Iteration 10, loss = 0.06736198\n",
      "Iteration 11, loss = 0.06203497\n",
      "Iteration 12, loss = 0.05806807\n",
      "Iteration 13, loss = 0.05501038\n",
      "Iteration 14, loss = 0.05242522\n",
      "Iteration 15, loss = 0.05087927\n",
      "Iteration 16, loss = 0.04915451\n",
      "Iteration 17, loss = 0.04748213\n",
      "Iteration 18, loss = 0.04662832\n",
      "Iteration 19, loss = 0.04584321\n",
      "Iteration 20, loss = 0.04559318\n",
      "Iteration 21, loss = 0.04462493\n",
      "Iteration 22, loss = 0.04428484\n",
      "Iteration 23, loss = 0.04414745\n",
      "Iteration 24, loss = 0.04292527\n",
      "Iteration 25, loss = 0.04246172\n",
      "Iteration 26, loss = 0.04203789\n",
      "Iteration 27, loss = 0.04099230\n",
      "Iteration 28, loss = 0.04060894\n",
      "Iteration 29, loss = 0.04056672\n",
      "Iteration 30, loss = 0.04079490\n",
      "Iteration 31, loss = 0.04131638\n",
      "Iteration 32, loss = 0.04021702\n",
      "Iteration 33, loss = 0.03997477\n",
      "Iteration 34, loss = 0.03966391\n",
      "Iteration 35, loss = 0.03966444\n",
      "Iteration 36, loss = 0.04021334\n",
      "Iteration 37, loss = 0.04062282\n",
      "Iteration 38, loss = 0.04093670\n",
      "Iteration 39, loss = 0.04102331\n",
      "Iteration 40, loss = 0.03950926\n",
      "Iteration 41, loss = 0.03987032\n",
      "Iteration 42, loss = 0.03926281\n",
      "Iteration 43, loss = 0.03929078\n",
      "Iteration 44, loss = 0.03948832\n",
      "Iteration 45, loss = 0.03927003\n",
      "Iteration 46, loss = 0.04410706\n",
      "Iteration 47, loss = 0.04703702\n",
      "Iteration 48, loss = 0.04765317\n",
      "Iteration 49, loss = 0.04870624\n",
      "Iteration 50, loss = 0.04927733\n",
      "Iteration 51, loss = 0.04984828\n",
      "Iteration 52, loss = 0.05157527\n",
      "Iteration 53, loss = 0.05059718\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.52451673\n",
      "Iteration 2, loss = 0.28696102\n",
      "Iteration 3, loss = 0.15907034\n",
      "Iteration 4, loss = 0.13188439\n",
      "Iteration 5, loss = 0.11911360\n",
      "Iteration 6, loss = 0.10671040\n",
      "Iteration 7, loss = 0.09382789\n",
      "Iteration 8, loss = 0.08259477\n",
      "Iteration 9, loss = 0.07357851\n",
      "Iteration 10, loss = 0.06704792\n",
      "Iteration 11, loss = 0.06261170\n",
      "Iteration 12, loss = 0.05881499\n",
      "Iteration 13, loss = 0.05582733\n",
      "Iteration 14, loss = 0.05322954\n",
      "Iteration 15, loss = 0.05146614\n",
      "Iteration 16, loss = 0.04957040\n",
      "Iteration 17, loss = 0.04787623\n",
      "Iteration 18, loss = 0.04680017\n",
      "Iteration 19, loss = 0.04572251\n",
      "Iteration 20, loss = 0.04472966\n",
      "Iteration 21, loss = 0.04412287\n",
      "Iteration 22, loss = 0.04322901\n",
      "Iteration 23, loss = 0.04320086\n",
      "Iteration 24, loss = 0.04295974\n",
      "Iteration 25, loss = 0.04283378\n",
      "Iteration 26, loss = 0.04286233\n",
      "Iteration 27, loss = 0.04205771\n",
      "Iteration 28, loss = 0.04161188\n",
      "Iteration 29, loss = 0.04177284\n",
      "Iteration 30, loss = 0.04147898\n",
      "Iteration 31, loss = 0.04141447\n",
      "Iteration 32, loss = 0.04101049\n",
      "Iteration 33, loss = 0.04156597\n",
      "Iteration 34, loss = 0.04334740\n",
      "Iteration 35, loss = 0.04260000\n",
      "Iteration 36, loss = 0.04254831\n",
      "Iteration 37, loss = 0.04193557\n",
      "Iteration 38, loss = 0.04164516\n",
      "Iteration 39, loss = 0.04156096\n",
      "Iteration 40, loss = 0.04095172\n",
      "Iteration 41, loss = 0.04053304\n",
      "Iteration 42, loss = 0.04058165\n",
      "Iteration 43, loss = 0.04056272\n",
      "Iteration 44, loss = 0.04048911\n",
      "Iteration 45, loss = 0.04160012\n",
      "Iteration 46, loss = 0.04374423\n",
      "Iteration 47, loss = 0.04418785\n",
      "Iteration 48, loss = 0.04648632\n",
      "Iteration 49, loss = 0.04426267\n",
      "Iteration 50, loss = 0.04416898\n",
      "Iteration 51, loss = 0.04539822\n",
      "Iteration 52, loss = 0.04541578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48729970\n",
      "Iteration 2, loss = 0.28353260\n",
      "Iteration 3, loss = 0.17448360\n",
      "Iteration 4, loss = 0.13808405\n",
      "Iteration 5, loss = 0.12323765\n",
      "Iteration 6, loss = 0.11069360\n",
      "Iteration 7, loss = 0.09854630\n",
      "Iteration 8, loss = 0.08676179\n",
      "Iteration 9, loss = 0.07756878\n",
      "Iteration 10, loss = 0.07001354\n",
      "Iteration 11, loss = 0.06502659\n",
      "Iteration 12, loss = 0.06076733\n",
      "Iteration 13, loss = 0.05749225\n",
      "Iteration 14, loss = 0.05488147\n",
      "Iteration 15, loss = 0.05316057\n",
      "Iteration 16, loss = 0.05128223\n",
      "Iteration 17, loss = 0.04968480\n",
      "Iteration 18, loss = 0.04796313\n",
      "Iteration 19, loss = 0.04675648\n",
      "Iteration 20, loss = 0.04585122\n",
      "Iteration 21, loss = 0.04496445\n",
      "Iteration 22, loss = 0.04454335\n",
      "Iteration 23, loss = 0.04496952\n",
      "Iteration 24, loss = 0.04467874\n",
      "Iteration 25, loss = 0.04442130\n",
      "Iteration 26, loss = 0.04392299\n",
      "Iteration 27, loss = 0.04318686\n",
      "Iteration 28, loss = 0.04264730\n",
      "Iteration 29, loss = 0.04265833\n",
      "Iteration 30, loss = 0.04283902\n",
      "Iteration 31, loss = 0.04269623\n",
      "Iteration 32, loss = 0.04221131\n",
      "Iteration 33, loss = 0.04183406\n",
      "Iteration 34, loss = 0.04207097\n",
      "Iteration 35, loss = 0.04153438\n",
      "Iteration 36, loss = 0.04094052\n",
      "Iteration 37, loss = 0.04070525\n",
      "Iteration 38, loss = 0.04111778\n",
      "Iteration 39, loss = 0.04116439\n",
      "Iteration 40, loss = 0.04146784\n",
      "Iteration 41, loss = 0.04167370\n",
      "Iteration 42, loss = 0.04201223\n",
      "Iteration 43, loss = 0.04191581\n",
      "Iteration 44, loss = 0.04168337\n",
      "Iteration 45, loss = 0.04398467\n",
      "Iteration 46, loss = 0.04552758\n",
      "Iteration 47, loss = 0.04605538\n",
      "Iteration 48, loss = 0.04620177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50395965\n",
      "Iteration 2, loss = 0.27162077\n",
      "Iteration 3, loss = 0.15935949\n",
      "Iteration 4, loss = 0.13204554\n",
      "Iteration 5, loss = 0.11699464\n",
      "Iteration 6, loss = 0.10551449\n",
      "Iteration 7, loss = 0.09289793\n",
      "Iteration 8, loss = 0.08183426\n",
      "Iteration 9, loss = 0.07300263\n",
      "Iteration 10, loss = 0.06651273\n",
      "Iteration 11, loss = 0.06178483\n",
      "Iteration 12, loss = 0.05793620\n",
      "Iteration 13, loss = 0.05449594\n",
      "Iteration 14, loss = 0.05227736\n",
      "Iteration 15, loss = 0.05044342\n",
      "Iteration 16, loss = 0.04925219\n",
      "Iteration 17, loss = 0.04760020\n",
      "Iteration 18, loss = 0.04624130\n",
      "Iteration 19, loss = 0.04514792\n",
      "Iteration 20, loss = 0.04378172\n",
      "Iteration 21, loss = 0.04319149\n",
      "Iteration 22, loss = 0.04272487\n",
      "Iteration 23, loss = 0.04314642\n",
      "Iteration 24, loss = 0.04207979\n",
      "Iteration 25, loss = 0.04122447\n",
      "Iteration 26, loss = 0.04095392\n",
      "Iteration 27, loss = 0.04048432\n",
      "Iteration 28, loss = 0.04085481\n",
      "Iteration 29, loss = 0.04086200\n",
      "Iteration 30, loss = 0.04074027\n",
      "Iteration 31, loss = 0.03979213\n",
      "Iteration 32, loss = 0.04024973\n",
      "Iteration 33, loss = 0.04037230\n",
      "Iteration 34, loss = 0.04101926\n",
      "Iteration 35, loss = 0.04093506\n",
      "Iteration 36, loss = 0.04124634\n",
      "Iteration 37, loss = 0.03993189\n",
      "Iteration 38, loss = 0.03932211\n",
      "Iteration 39, loss = 0.03889188\n",
      "Iteration 40, loss = 0.03914233\n",
      "Iteration 41, loss = 0.03902242\n",
      "Iteration 42, loss = 0.03945198\n",
      "Iteration 43, loss = 0.03886171\n",
      "Iteration 44, loss = 0.03958967\n",
      "Iteration 45, loss = 0.04255403\n",
      "Iteration 46, loss = 0.04582996\n",
      "Iteration 47, loss = 0.04833486\n",
      "Iteration 48, loss = 0.04835670\n",
      "Iteration 49, loss = 0.04779257\n",
      "Iteration 50, loss = 0.04451622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38235260\n",
      "Iteration 2, loss = 2.22219956\n",
      "Iteration 3, loss = 2.03982620\n",
      "Iteration 4, loss = 1.81538264\n",
      "Iteration 5, loss = 1.57466660\n",
      "Iteration 6, loss = 1.32781005\n",
      "Iteration 7, loss = 1.10726222\n",
      "Iteration 8, loss = 0.92623285\n",
      "Iteration 9, loss = 0.78378671\n",
      "Iteration 10, loss = 0.67860813\n",
      "Iteration 11, loss = 0.59617285\n",
      "Iteration 12, loss = 0.53330143\n",
      "Iteration 13, loss = 0.48145105\n",
      "Iteration 14, loss = 0.44172918\n",
      "Iteration 15, loss = 0.40868421\n",
      "Iteration 16, loss = 0.37771710\n",
      "Iteration 17, loss = 0.35287177\n",
      "Iteration 18, loss = 0.33213188\n",
      "Iteration 19, loss = 0.31319894\n",
      "Iteration 20, loss = 0.29749875\n",
      "Iteration 21, loss = 0.28248605\n",
      "Iteration 22, loss = 0.26903779\n",
      "Iteration 23, loss = 0.25726224\n",
      "Iteration 24, loss = 0.24667708\n",
      "Iteration 25, loss = 0.23690508\n",
      "Iteration 26, loss = 0.22807608\n",
      "Iteration 27, loss = 0.21989688\n",
      "Iteration 28, loss = 0.21265172\n",
      "Iteration 29, loss = 0.20617269\n",
      "Iteration 30, loss = 0.20018150\n",
      "Iteration 31, loss = 0.19428931\n",
      "Iteration 32, loss = 0.18887942\n",
      "Iteration 33, loss = 0.18390534\n",
      "Iteration 34, loss = 0.17920229\n",
      "Iteration 35, loss = 0.17487382\n",
      "Iteration 36, loss = 0.17052894\n",
      "Iteration 37, loss = 0.16654342\n",
      "Iteration 38, loss = 0.16285966\n",
      "Iteration 39, loss = 0.15970604\n",
      "Iteration 40, loss = 0.15659591\n",
      "Iteration 41, loss = 0.15366906\n",
      "Iteration 42, loss = 0.15082182\n",
      "Iteration 43, loss = 0.14819066\n",
      "Iteration 44, loss = 0.14594941\n",
      "Iteration 45, loss = 0.14343113\n",
      "Iteration 46, loss = 0.14114261\n",
      "Iteration 47, loss = 0.13914803\n",
      "Iteration 48, loss = 0.13711594\n",
      "Iteration 49, loss = 0.13502023\n",
      "Iteration 50, loss = 0.13324763\n",
      "Iteration 51, loss = 0.13136110\n",
      "Iteration 52, loss = 0.12955978\n",
      "Iteration 53, loss = 0.12807819\n",
      "Iteration 54, loss = 0.12625085\n",
      "Iteration 55, loss = 0.12476259\n",
      "Iteration 56, loss = 0.12337822\n",
      "Iteration 57, loss = 0.12203802\n",
      "Iteration 58, loss = 0.12077411\n",
      "Iteration 59, loss = 0.11950178\n",
      "Iteration 60, loss = 0.11859001\n",
      "Iteration 61, loss = 0.11728887\n",
      "Iteration 62, loss = 0.11622243\n",
      "Iteration 63, loss = 0.11514378\n",
      "Iteration 64, loss = 0.11407725\n",
      "Iteration 65, loss = 0.11323168\n",
      "Iteration 66, loss = 0.11233310\n",
      "Iteration 67, loss = 0.11146255\n",
      "Iteration 68, loss = 0.11043431\n",
      "Iteration 69, loss = 0.10961605\n",
      "Iteration 70, loss = 0.10881414\n",
      "Iteration 71, loss = 0.10809319\n",
      "Iteration 72, loss = 0.10746254\n",
      "Iteration 73, loss = 0.10659498\n",
      "Iteration 74, loss = 0.10595920\n",
      "Iteration 75, loss = 0.10532241\n",
      "Iteration 76, loss = 0.10465767\n",
      "Iteration 77, loss = 0.10397621\n",
      "Iteration 78, loss = 0.10334855\n",
      "Iteration 79, loss = 0.10285096\n",
      "Iteration 80, loss = 0.10229189\n",
      "Iteration 81, loss = 0.10165299\n",
      "Iteration 82, loss = 0.10111570\n",
      "Iteration 83, loss = 0.10064378\n",
      "Iteration 84, loss = 0.10016407\n",
      "Iteration 85, loss = 0.09967627\n",
      "Iteration 86, loss = 0.09927526\n",
      "Iteration 87, loss = 0.09884554\n",
      "Iteration 88, loss = 0.09843462\n",
      "Iteration 89, loss = 0.09803535\n",
      "Iteration 90, loss = 0.09762752\n",
      "Iteration 91, loss = 0.09723068\n",
      "Iteration 92, loss = 0.09687896\n",
      "Iteration 93, loss = 0.09659071\n",
      "Iteration 94, loss = 0.09615630\n",
      "Iteration 95, loss = 0.09579119\n",
      "Iteration 96, loss = 0.09544775\n",
      "Iteration 97, loss = 0.09513145\n",
      "Iteration 98, loss = 0.09480240\n",
      "Iteration 99, loss = 0.09447578\n",
      "Iteration 100, loss = 0.09412997\n",
      "Iteration 101, loss = 0.09380902\n",
      "Iteration 102, loss = 0.09355413\n",
      "Iteration 103, loss = 0.09324032\n",
      "Iteration 104, loss = 0.09295608\n",
      "Iteration 105, loss = 0.09267234\n",
      "Iteration 106, loss = 0.09242719\n",
      "Iteration 107, loss = 0.09213026\n",
      "Iteration 108, loss = 0.09188762\n",
      "Iteration 109, loss = 0.09163512\n",
      "Iteration 110, loss = 0.09138912\n",
      "Iteration 111, loss = 0.09116359\n",
      "Iteration 112, loss = 0.09091784\n",
      "Iteration 113, loss = 0.09067406\n",
      "Iteration 114, loss = 0.09045793\n",
      "Iteration 115, loss = 0.09023920\n",
      "Iteration 116, loss = 0.09001665\n",
      "Iteration 117, loss = 0.08982948\n",
      "Iteration 118, loss = 0.08957162\n",
      "Iteration 119, loss = 0.08937989\n",
      "Iteration 120, loss = 0.08917415\n",
      "Iteration 121, loss = 0.08899692\n",
      "Iteration 122, loss = 0.08879159\n",
      "Iteration 123, loss = 0.08857347\n",
      "Iteration 124, loss = 0.08839324\n",
      "Iteration 125, loss = 0.08823210\n",
      "Iteration 126, loss = 0.08804609\n",
      "Iteration 127, loss = 0.08785957\n",
      "Iteration 128, loss = 0.08769443\n",
      "Iteration 129, loss = 0.08752516\n",
      "Iteration 130, loss = 0.08734959\n",
      "Iteration 131, loss = 0.08717842\n",
      "Iteration 132, loss = 0.08704318\n",
      "Iteration 133, loss = 0.08686544\n",
      "Iteration 134, loss = 0.08668083\n",
      "Iteration 135, loss = 0.08652852\n",
      "Iteration 136, loss = 0.08636345\n",
      "Iteration 137, loss = 0.08621576\n",
      "Iteration 138, loss = 0.08607617\n",
      "Iteration 139, loss = 0.08591241\n",
      "Iteration 140, loss = 0.08575305\n",
      "Iteration 141, loss = 0.08560843\n",
      "Iteration 142, loss = 0.08547996\n",
      "Iteration 143, loss = 0.08534143\n",
      "Iteration 144, loss = 0.08521021\n",
      "Iteration 145, loss = 0.08507956\n",
      "Iteration 146, loss = 0.08496108\n",
      "Iteration 147, loss = 0.08480535\n",
      "Iteration 148, loss = 0.08465174\n",
      "Iteration 149, loss = 0.08452849\n",
      "Iteration 150, loss = 0.08439703\n",
      "Iteration 151, loss = 0.08428208\n",
      "Iteration 152, loss = 0.08415210\n",
      "Iteration 153, loss = 0.08403486\n",
      "Iteration 154, loss = 0.08390165\n",
      "Iteration 155, loss = 0.08378461\n",
      "Iteration 156, loss = 0.08366629\n",
      "Iteration 157, loss = 0.08356791\n",
      "Iteration 158, loss = 0.08343482\n",
      "Iteration 159, loss = 0.08334819\n",
      "Iteration 160, loss = 0.08321302\n",
      "Iteration 161, loss = 0.08310234\n",
      "Iteration 162, loss = 0.08299342\n",
      "Iteration 163, loss = 0.08287660\n",
      "Iteration 164, loss = 0.08276712\n",
      "Iteration 165, loss = 0.08265243\n",
      "Iteration 166, loss = 0.08254613\n",
      "Iteration 167, loss = 0.08243698\n",
      "Iteration 168, loss = 0.08232188\n",
      "Iteration 169, loss = 0.08223562\n",
      "Iteration 170, loss = 0.08212600\n",
      "Iteration 171, loss = 0.08202482\n",
      "Iteration 172, loss = 0.08193402\n",
      "Iteration 173, loss = 0.08183735\n",
      "Iteration 174, loss = 0.08172627\n",
      "Iteration 175, loss = 0.08162362\n",
      "Iteration 176, loss = 0.08152149\n",
      "Iteration 177, loss = 0.08143079\n",
      "Iteration 178, loss = 0.08134425\n",
      "Iteration 179, loss = 0.08124981\n",
      "Iteration 180, loss = 0.08114389\n",
      "Iteration 181, loss = 0.08104966\n",
      "Iteration 182, loss = 0.08096480\n",
      "Iteration 183, loss = 0.08086246\n",
      "Iteration 184, loss = 0.08078129\n",
      "Iteration 185, loss = 0.08070101\n",
      "Iteration 186, loss = 0.08061537\n",
      "Iteration 187, loss = 0.08050969\n",
      "Iteration 188, loss = 0.08041628\n",
      "Iteration 189, loss = 0.08033203\n",
      "Iteration 190, loss = 0.08024464\n",
      "Iteration 191, loss = 0.08015297\n",
      "Iteration 192, loss = 0.08006467\n",
      "Iteration 193, loss = 0.07998495\n",
      "Iteration 194, loss = 0.07989736\n",
      "Iteration 195, loss = 0.07981296\n",
      "Iteration 196, loss = 0.07973176\n",
      "Iteration 197, loss = 0.07964859\n",
      "Iteration 198, loss = 0.07956797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38698007\n",
      "Iteration 2, loss = 2.21964158\n",
      "Iteration 3, loss = 2.03449199\n",
      "Iteration 4, loss = 1.80630829\n",
      "Iteration 5, loss = 1.55617283\n",
      "Iteration 6, loss = 1.30384721\n",
      "Iteration 7, loss = 1.07867019\n",
      "Iteration 8, loss = 0.89752087\n",
      "Iteration 9, loss = 0.75726363\n",
      "Iteration 10, loss = 0.65381990\n",
      "Iteration 11, loss = 0.57429648\n",
      "Iteration 12, loss = 0.51360714\n",
      "Iteration 13, loss = 0.46422261\n",
      "Iteration 14, loss = 0.42495089\n",
      "Iteration 15, loss = 0.39537790\n",
      "Iteration 16, loss = 0.36677900\n",
      "Iteration 17, loss = 0.34272976\n",
      "Iteration 18, loss = 0.32334822\n",
      "Iteration 19, loss = 0.30621936\n",
      "Iteration 20, loss = 0.29092224\n",
      "Iteration 21, loss = 0.27711507\n",
      "Iteration 22, loss = 0.26624214\n",
      "Iteration 23, loss = 0.25341604\n",
      "Iteration 24, loss = 0.24398221\n",
      "Iteration 25, loss = 0.23477988\n",
      "Iteration 26, loss = 0.22650442\n",
      "Iteration 27, loss = 0.21917126\n",
      "Iteration 28, loss = 0.21203997\n",
      "Iteration 29, loss = 0.20641144\n",
      "Iteration 30, loss = 0.20064718\n",
      "Iteration 31, loss = 0.19465974\n",
      "Iteration 32, loss = 0.18951971\n",
      "Iteration 33, loss = 0.18508015\n",
      "Iteration 34, loss = 0.18026296\n",
      "Iteration 35, loss = 0.17642052\n",
      "Iteration 36, loss = 0.17262997\n",
      "Iteration 37, loss = 0.16864701\n",
      "Iteration 38, loss = 0.16496439\n",
      "Iteration 39, loss = 0.16194930\n",
      "Iteration 40, loss = 0.15908021\n",
      "Iteration 41, loss = 0.15606469\n",
      "Iteration 42, loss = 0.15326532\n",
      "Iteration 43, loss = 0.15076700\n",
      "Iteration 44, loss = 0.14837688\n",
      "Iteration 45, loss = 0.14588700\n",
      "Iteration 46, loss = 0.14354247\n",
      "Iteration 47, loss = 0.14136726\n",
      "Iteration 48, loss = 0.13967031\n",
      "Iteration 49, loss = 0.13732535\n",
      "Iteration 50, loss = 0.13551952\n",
      "Iteration 51, loss = 0.13357977\n",
      "Iteration 52, loss = 0.13184802\n",
      "Iteration 53, loss = 0.13024687\n",
      "Iteration 54, loss = 0.12858149\n",
      "Iteration 55, loss = 0.12693246\n",
      "Iteration 56, loss = 0.12548518\n",
      "Iteration 57, loss = 0.12414378\n",
      "Iteration 58, loss = 0.12286378\n",
      "Iteration 59, loss = 0.12135235\n",
      "Iteration 60, loss = 0.12020892\n",
      "Iteration 61, loss = 0.11907455\n",
      "Iteration 62, loss = 0.11817793\n",
      "Iteration 63, loss = 0.11691334\n",
      "Iteration 64, loss = 0.11551861\n",
      "Iteration 65, loss = 0.11463338\n",
      "Iteration 66, loss = 0.11371069\n",
      "Iteration 67, loss = 0.11269374\n",
      "Iteration 68, loss = 0.11185228\n",
      "Iteration 69, loss = 0.11081068\n",
      "Iteration 70, loss = 0.10997049\n",
      "Iteration 71, loss = 0.10925781\n",
      "Iteration 72, loss = 0.10858224\n",
      "Iteration 73, loss = 0.10775159\n",
      "Iteration 74, loss = 0.10698474\n",
      "Iteration 75, loss = 0.10615760\n",
      "Iteration 76, loss = 0.10554887\n",
      "Iteration 77, loss = 0.10475806\n",
      "Iteration 78, loss = 0.10416532\n",
      "Iteration 79, loss = 0.10346419\n",
      "Iteration 80, loss = 0.10294301\n",
      "Iteration 81, loss = 0.10227946\n",
      "Iteration 82, loss = 0.10171112\n",
      "Iteration 83, loss = 0.10113305\n",
      "Iteration 84, loss = 0.10063833\n",
      "Iteration 85, loss = 0.10016244\n",
      "Iteration 86, loss = 0.09971097\n",
      "Iteration 87, loss = 0.09925937\n",
      "Iteration 88, loss = 0.09885381\n",
      "Iteration 89, loss = 0.09843264\n",
      "Iteration 90, loss = 0.09803358\n",
      "Iteration 91, loss = 0.09753766\n",
      "Iteration 92, loss = 0.09716562\n",
      "Iteration 93, loss = 0.09682404\n",
      "Iteration 94, loss = 0.09639787\n",
      "Iteration 95, loss = 0.09601031\n",
      "Iteration 96, loss = 0.09561311\n",
      "Iteration 97, loss = 0.09532991\n",
      "Iteration 98, loss = 0.09495977\n",
      "Iteration 99, loss = 0.09458578\n",
      "Iteration 100, loss = 0.09424527\n",
      "Iteration 101, loss = 0.09392084\n",
      "Iteration 102, loss = 0.09364735\n",
      "Iteration 103, loss = 0.09332027\n",
      "Iteration 104, loss = 0.09308222\n",
      "Iteration 105, loss = 0.09272193\n",
      "Iteration 106, loss = 0.09243784\n",
      "Iteration 107, loss = 0.09213769\n",
      "Iteration 108, loss = 0.09188967\n",
      "Iteration 109, loss = 0.09161592\n",
      "Iteration 110, loss = 0.09136574\n",
      "Iteration 111, loss = 0.09112151\n",
      "Iteration 112, loss = 0.09086485\n",
      "Iteration 113, loss = 0.09063213\n",
      "Iteration 114, loss = 0.09039138\n",
      "Iteration 115, loss = 0.09016907\n",
      "Iteration 116, loss = 0.08993078\n",
      "Iteration 117, loss = 0.08970534\n",
      "Iteration 118, loss = 0.08949133\n",
      "Iteration 119, loss = 0.08927796\n",
      "Iteration 120, loss = 0.08907213\n",
      "Iteration 121, loss = 0.08889012\n",
      "Iteration 122, loss = 0.08867211\n",
      "Iteration 123, loss = 0.08847131\n",
      "Iteration 124, loss = 0.08828384\n",
      "Iteration 125, loss = 0.08812236\n",
      "Iteration 126, loss = 0.08791382\n",
      "Iteration 127, loss = 0.08770368\n",
      "Iteration 128, loss = 0.08752573\n",
      "Iteration 129, loss = 0.08735375\n",
      "Iteration 130, loss = 0.08719683\n",
      "Iteration 131, loss = 0.08701645\n",
      "Iteration 132, loss = 0.08684485\n",
      "Iteration 133, loss = 0.08667488\n",
      "Iteration 134, loss = 0.08649988\n",
      "Iteration 135, loss = 0.08634197\n",
      "Iteration 136, loss = 0.08616851\n",
      "Iteration 137, loss = 0.08601040\n",
      "Iteration 138, loss = 0.08586189\n",
      "Iteration 139, loss = 0.08568838\n",
      "Iteration 140, loss = 0.08552228\n",
      "Iteration 141, loss = 0.08537663\n",
      "Iteration 142, loss = 0.08524048\n",
      "Iteration 143, loss = 0.08509930\n",
      "Iteration 144, loss = 0.08495793\n",
      "Iteration 145, loss = 0.08481944\n",
      "Iteration 146, loss = 0.08468950\n",
      "Iteration 147, loss = 0.08454431\n",
      "Iteration 148, loss = 0.08438498\n",
      "Iteration 149, loss = 0.08424979\n",
      "Iteration 150, loss = 0.08412581\n",
      "Iteration 151, loss = 0.08401445\n",
      "Iteration 152, loss = 0.08387170\n",
      "Iteration 153, loss = 0.08373668\n",
      "Iteration 154, loss = 0.08360677\n",
      "Iteration 155, loss = 0.08348967\n",
      "Iteration 156, loss = 0.08336974\n",
      "Iteration 157, loss = 0.08325378\n",
      "Iteration 158, loss = 0.08312488\n",
      "Iteration 159, loss = 0.08302824\n",
      "Iteration 160, loss = 0.08289704\n",
      "Iteration 161, loss = 0.08277640\n",
      "Iteration 162, loss = 0.08267928\n",
      "Iteration 163, loss = 0.08255964\n",
      "Iteration 164, loss = 0.08243638\n",
      "Iteration 165, loss = 0.08231084\n",
      "Iteration 166, loss = 0.08220859\n",
      "Iteration 167, loss = 0.08208702\n",
      "Iteration 168, loss = 0.08197785\n",
      "Iteration 169, loss = 0.08187881\n",
      "Iteration 170, loss = 0.08176005\n",
      "Iteration 171, loss = 0.08166441\n",
      "Iteration 172, loss = 0.08157112\n",
      "Iteration 173, loss = 0.08147543\n",
      "Iteration 174, loss = 0.08136164\n",
      "Iteration 175, loss = 0.08126455\n",
      "Iteration 176, loss = 0.08116356\n",
      "Iteration 177, loss = 0.08106227\n",
      "Iteration 178, loss = 0.08097922\n",
      "Iteration 179, loss = 0.08088213\n",
      "Iteration 180, loss = 0.08077388\n",
      "Iteration 181, loss = 0.08067841\n",
      "Iteration 182, loss = 0.08059031\n",
      "Iteration 183, loss = 0.08048979\n",
      "Iteration 184, loss = 0.08040232\n",
      "Iteration 185, loss = 0.08031505\n",
      "Iteration 186, loss = 0.08023538\n",
      "Iteration 187, loss = 0.08013049\n",
      "Iteration 188, loss = 0.08003658\n",
      "Iteration 189, loss = 0.07994012\n",
      "Iteration 190, loss = 0.07985574\n",
      "Iteration 191, loss = 0.07976703\n",
      "Iteration 192, loss = 0.07968461\n",
      "Iteration 193, loss = 0.07959224\n",
      "Iteration 194, loss = 0.07950689\n",
      "Iteration 195, loss = 0.07941947\n",
      "Iteration 196, loss = 0.07933335\n",
      "Iteration 197, loss = 0.07924656\n",
      "Iteration 198, loss = 0.07916822\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38605584\n",
      "Iteration 2, loss = 2.22179863\n",
      "Iteration 3, loss = 2.04098413\n",
      "Iteration 4, loss = 1.81646693\n",
      "Iteration 5, loss = 1.57189393\n",
      "Iteration 6, loss = 1.32575903\n",
      "Iteration 7, loss = 1.10308779\n",
      "Iteration 8, loss = 0.92253341\n",
      "Iteration 9, loss = 0.78219115\n",
      "Iteration 10, loss = 0.67527689\n",
      "Iteration 11, loss = 0.59490184\n",
      "Iteration 12, loss = 0.53209375\n",
      "Iteration 13, loss = 0.48113670\n",
      "Iteration 14, loss = 0.44186197\n",
      "Iteration 15, loss = 0.40688004\n",
      "Iteration 16, loss = 0.38013512\n",
      "Iteration 17, loss = 0.35494399\n",
      "Iteration 18, loss = 0.33415824\n",
      "Iteration 19, loss = 0.31555686\n",
      "Iteration 20, loss = 0.30061436\n",
      "Iteration 21, loss = 0.28538966\n",
      "Iteration 22, loss = 0.27415896\n",
      "Iteration 23, loss = 0.26208727\n",
      "Iteration 24, loss = 0.25143720\n",
      "Iteration 25, loss = 0.24191435\n",
      "Iteration 26, loss = 0.23344144\n",
      "Iteration 27, loss = 0.22605453\n",
      "Iteration 28, loss = 0.21884272\n",
      "Iteration 29, loss = 0.21205881\n",
      "Iteration 30, loss = 0.20556488\n",
      "Iteration 31, loss = 0.19971978\n",
      "Iteration 32, loss = 0.19462303\n",
      "Iteration 33, loss = 0.18965928\n",
      "Iteration 34, loss = 0.18473863\n",
      "Iteration 35, loss = 0.18072169\n",
      "Iteration 36, loss = 0.17622125\n",
      "Iteration 37, loss = 0.17250082\n",
      "Iteration 38, loss = 0.16872013\n",
      "Iteration 39, loss = 0.16493475\n",
      "Iteration 40, loss = 0.16192628\n",
      "Iteration 41, loss = 0.15898282\n",
      "Iteration 42, loss = 0.15582648\n",
      "Iteration 43, loss = 0.15305152\n",
      "Iteration 44, loss = 0.15063161\n",
      "Iteration 45, loss = 0.14820387\n",
      "Iteration 46, loss = 0.14575944\n",
      "Iteration 47, loss = 0.14291981\n",
      "Iteration 48, loss = 0.14045052\n",
      "Iteration 49, loss = 0.13853144\n",
      "Iteration 50, loss = 0.13633949\n",
      "Iteration 51, loss = 0.13459906\n",
      "Iteration 52, loss = 0.13280449\n",
      "Iteration 53, loss = 0.13086078\n",
      "Iteration 54, loss = 0.12935308\n",
      "Iteration 55, loss = 0.12771541\n",
      "Iteration 56, loss = 0.12639506\n",
      "Iteration 57, loss = 0.12479959\n",
      "Iteration 58, loss = 0.12329637\n",
      "Iteration 59, loss = 0.12204812\n",
      "Iteration 60, loss = 0.12066987\n",
      "Iteration 61, loss = 0.11937466\n",
      "Iteration 62, loss = 0.11831814\n",
      "Iteration 63, loss = 0.11722854\n",
      "Iteration 64, loss = 0.11637101\n",
      "Iteration 65, loss = 0.11509131\n",
      "Iteration 66, loss = 0.11429871\n",
      "Iteration 67, loss = 0.11317262\n",
      "Iteration 68, loss = 0.11237711\n",
      "Iteration 69, loss = 0.11142617\n",
      "Iteration 70, loss = 0.11045529\n",
      "Iteration 71, loss = 0.10964320\n",
      "Iteration 72, loss = 0.10898978\n",
      "Iteration 73, loss = 0.10816388\n",
      "Iteration 74, loss = 0.10735656\n",
      "Iteration 75, loss = 0.10664477\n",
      "Iteration 76, loss = 0.10593291\n",
      "Iteration 77, loss = 0.10526342\n",
      "Iteration 78, loss = 0.10465765\n",
      "Iteration 79, loss = 0.10401645\n",
      "Iteration 80, loss = 0.10351297\n",
      "Iteration 81, loss = 0.10303374\n",
      "Iteration 82, loss = 0.10243301\n",
      "Iteration 83, loss = 0.10187060\n",
      "Iteration 84, loss = 0.10137744\n",
      "Iteration 85, loss = 0.10084510\n",
      "Iteration 86, loss = 0.10035883\n",
      "Iteration 87, loss = 0.09984914\n",
      "Iteration 88, loss = 0.09939246\n",
      "Iteration 89, loss = 0.09899371\n",
      "Iteration 90, loss = 0.09856171\n",
      "Iteration 91, loss = 0.09815946\n",
      "Iteration 92, loss = 0.09771014\n",
      "Iteration 93, loss = 0.09732371\n",
      "Iteration 94, loss = 0.09696849\n",
      "Iteration 95, loss = 0.09658434\n",
      "Iteration 96, loss = 0.09616869\n",
      "Iteration 97, loss = 0.09579685\n",
      "Iteration 98, loss = 0.09544303\n",
      "Iteration 99, loss = 0.09510075\n",
      "Iteration 100, loss = 0.09479084\n",
      "Iteration 101, loss = 0.09449993\n",
      "Iteration 102, loss = 0.09418542\n",
      "Iteration 103, loss = 0.09388245\n",
      "Iteration 104, loss = 0.09364288\n",
      "Iteration 105, loss = 0.09331866\n",
      "Iteration 106, loss = 0.09301883\n",
      "Iteration 107, loss = 0.09277529\n",
      "Iteration 108, loss = 0.09249323\n",
      "Iteration 109, loss = 0.09224206\n",
      "Iteration 110, loss = 0.09197519\n",
      "Iteration 111, loss = 0.09172590\n",
      "Iteration 112, loss = 0.09147066\n",
      "Iteration 113, loss = 0.09124371\n",
      "Iteration 114, loss = 0.09100533\n",
      "Iteration 115, loss = 0.09075782\n",
      "Iteration 116, loss = 0.09053344\n",
      "Iteration 117, loss = 0.09031767\n",
      "Iteration 118, loss = 0.09011739\n",
      "Iteration 119, loss = 0.08988997\n",
      "Iteration 120, loss = 0.08971186\n",
      "Iteration 121, loss = 0.08949862\n",
      "Iteration 122, loss = 0.08927807\n",
      "Iteration 123, loss = 0.08908351\n",
      "Iteration 124, loss = 0.08886455\n",
      "Iteration 125, loss = 0.08867945\n",
      "Iteration 126, loss = 0.08848813\n",
      "Iteration 127, loss = 0.08831495\n",
      "Iteration 128, loss = 0.08812862\n",
      "Iteration 129, loss = 0.08796566\n",
      "Iteration 130, loss = 0.08777586\n",
      "Iteration 131, loss = 0.08760010\n",
      "Iteration 132, loss = 0.08744116\n",
      "Iteration 133, loss = 0.08727011\n",
      "Iteration 134, loss = 0.08709513\n",
      "Iteration 135, loss = 0.08691244\n",
      "Iteration 136, loss = 0.08675940\n",
      "Iteration 137, loss = 0.08661601\n",
      "Iteration 138, loss = 0.08647880\n",
      "Iteration 139, loss = 0.08630385\n",
      "Iteration 140, loss = 0.08614460\n",
      "Iteration 141, loss = 0.08601852\n",
      "Iteration 142, loss = 0.08585233\n",
      "Iteration 143, loss = 0.08571017\n",
      "Iteration 144, loss = 0.08557460\n",
      "Iteration 145, loss = 0.08543392\n",
      "Iteration 146, loss = 0.08529462\n",
      "Iteration 147, loss = 0.08517355\n",
      "Iteration 148, loss = 0.08505997\n",
      "Iteration 149, loss = 0.08490853\n",
      "Iteration 150, loss = 0.08477658\n",
      "Iteration 151, loss = 0.08463361\n",
      "Iteration 152, loss = 0.08449329\n",
      "Iteration 153, loss = 0.08437750\n",
      "Iteration 154, loss = 0.08425436\n",
      "Iteration 155, loss = 0.08413375\n",
      "Iteration 156, loss = 0.08400972\n",
      "Iteration 157, loss = 0.08388392\n",
      "Iteration 158, loss = 0.08377872\n",
      "Iteration 159, loss = 0.08367341\n",
      "Iteration 160, loss = 0.08355064\n",
      "Iteration 161, loss = 0.08343668\n",
      "Iteration 162, loss = 0.08332725\n",
      "Iteration 163, loss = 0.08320696\n",
      "Iteration 164, loss = 0.08310638\n",
      "Iteration 165, loss = 0.08298134\n",
      "Iteration 166, loss = 0.08287277\n",
      "Iteration 167, loss = 0.08276967\n",
      "Iteration 168, loss = 0.08267083\n",
      "Iteration 169, loss = 0.08255284\n",
      "Iteration 170, loss = 0.08245454\n",
      "Iteration 171, loss = 0.08235630\n",
      "Iteration 172, loss = 0.08224232\n",
      "Iteration 173, loss = 0.08215791\n",
      "Iteration 174, loss = 0.08206072\n",
      "Iteration 175, loss = 0.08195204\n",
      "Iteration 176, loss = 0.08183663\n",
      "Iteration 177, loss = 0.08173395\n",
      "Iteration 178, loss = 0.08163541\n",
      "Iteration 179, loss = 0.08153271\n",
      "Iteration 180, loss = 0.08144255\n",
      "Iteration 181, loss = 0.08134224\n",
      "Iteration 182, loss = 0.08124811\n",
      "Iteration 183, loss = 0.08115736\n",
      "Iteration 184, loss = 0.08106972\n",
      "Iteration 185, loss = 0.08098765\n",
      "Iteration 186, loss = 0.08089740\n",
      "Iteration 187, loss = 0.08080974\n",
      "Iteration 188, loss = 0.08071095\n",
      "Iteration 189, loss = 0.08061608\n",
      "Iteration 190, loss = 0.08052828\n",
      "Iteration 191, loss = 0.08044815\n",
      "Iteration 192, loss = 0.08036085\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38524905\n",
      "Iteration 2, loss = 2.22764385\n",
      "Iteration 3, loss = 2.04711088\n",
      "Iteration 4, loss = 1.83019948\n",
      "Iteration 5, loss = 1.58439610\n",
      "Iteration 6, loss = 1.33650200\n",
      "Iteration 7, loss = 1.11381636\n",
      "Iteration 8, loss = 0.93357038\n",
      "Iteration 9, loss = 0.79445292\n",
      "Iteration 10, loss = 0.68716053\n",
      "Iteration 11, loss = 0.60690594\n",
      "Iteration 12, loss = 0.54368082\n",
      "Iteration 13, loss = 0.49180811\n",
      "Iteration 14, loss = 0.45185845\n",
      "Iteration 15, loss = 0.41732460\n",
      "Iteration 16, loss = 0.38861796\n",
      "Iteration 17, loss = 0.36319305\n",
      "Iteration 18, loss = 0.34303963\n",
      "Iteration 19, loss = 0.32322684\n",
      "Iteration 20, loss = 0.30733123\n",
      "Iteration 21, loss = 0.29236369\n",
      "Iteration 22, loss = 0.27985542\n",
      "Iteration 23, loss = 0.26839037\n",
      "Iteration 24, loss = 0.25704995\n",
      "Iteration 25, loss = 0.24761218\n",
      "Iteration 26, loss = 0.23901689\n",
      "Iteration 27, loss = 0.23049368\n",
      "Iteration 28, loss = 0.22311477\n",
      "Iteration 29, loss = 0.21626066\n",
      "Iteration 30, loss = 0.20977289\n",
      "Iteration 31, loss = 0.20370712\n",
      "Iteration 32, loss = 0.19812233\n",
      "Iteration 33, loss = 0.19337248\n",
      "Iteration 34, loss = 0.18803329\n",
      "Iteration 35, loss = 0.18367983\n",
      "Iteration 36, loss = 0.17939582\n",
      "Iteration 37, loss = 0.17535117\n",
      "Iteration 38, loss = 0.17158059\n",
      "Iteration 39, loss = 0.16787110\n",
      "Iteration 40, loss = 0.16477380\n",
      "Iteration 41, loss = 0.16181831\n",
      "Iteration 42, loss = 0.15831007\n",
      "Iteration 43, loss = 0.15568880\n",
      "Iteration 44, loss = 0.15302777\n",
      "Iteration 45, loss = 0.15090040\n",
      "Iteration 46, loss = 0.14807928\n",
      "Iteration 47, loss = 0.14533750\n",
      "Iteration 48, loss = 0.14312372\n",
      "Iteration 49, loss = 0.14083926\n",
      "Iteration 50, loss = 0.13865281\n",
      "Iteration 51, loss = 0.13694499\n",
      "Iteration 52, loss = 0.13525859\n",
      "Iteration 53, loss = 0.13317374\n",
      "Iteration 54, loss = 0.13147181\n",
      "Iteration 55, loss = 0.12985942\n",
      "Iteration 56, loss = 0.12849695\n",
      "Iteration 57, loss = 0.12679269\n",
      "Iteration 58, loss = 0.12539973\n",
      "Iteration 59, loss = 0.12412352\n",
      "Iteration 60, loss = 0.12275531\n",
      "Iteration 61, loss = 0.12161989\n",
      "Iteration 62, loss = 0.12031455\n",
      "Iteration 63, loss = 0.11925895\n",
      "Iteration 64, loss = 0.11829379\n",
      "Iteration 65, loss = 0.11705804\n",
      "Iteration 66, loss = 0.11613373\n",
      "Iteration 67, loss = 0.11505873\n",
      "Iteration 68, loss = 0.11419717\n",
      "Iteration 69, loss = 0.11316808\n",
      "Iteration 70, loss = 0.11225136\n",
      "Iteration 71, loss = 0.11139408\n",
      "Iteration 72, loss = 0.11079710\n",
      "Iteration 73, loss = 0.10986246\n",
      "Iteration 74, loss = 0.10904236\n",
      "Iteration 75, loss = 0.10831389\n",
      "Iteration 76, loss = 0.10761881\n",
      "Iteration 77, loss = 0.10693948\n",
      "Iteration 78, loss = 0.10622771\n",
      "Iteration 79, loss = 0.10563601\n",
      "Iteration 80, loss = 0.10514992\n",
      "Iteration 81, loss = 0.10460448\n",
      "Iteration 82, loss = 0.10399783\n",
      "Iteration 83, loss = 0.10339756\n",
      "Iteration 84, loss = 0.10286367\n",
      "Iteration 85, loss = 0.10230539\n",
      "Iteration 86, loss = 0.10178142\n",
      "Iteration 87, loss = 0.10126935\n",
      "Iteration 88, loss = 0.10080520\n",
      "Iteration 89, loss = 0.10040086\n",
      "Iteration 90, loss = 0.09996269\n",
      "Iteration 91, loss = 0.09955071\n",
      "Iteration 92, loss = 0.09907625\n",
      "Iteration 93, loss = 0.09864963\n",
      "Iteration 94, loss = 0.09828456\n",
      "Iteration 95, loss = 0.09793017\n",
      "Iteration 96, loss = 0.09749519\n",
      "Iteration 97, loss = 0.09713105\n",
      "Iteration 98, loss = 0.09677678\n",
      "Iteration 99, loss = 0.09641670\n",
      "Iteration 100, loss = 0.09608294\n",
      "Iteration 101, loss = 0.09579359\n",
      "Iteration 102, loss = 0.09545776\n",
      "Iteration 103, loss = 0.09514450\n",
      "Iteration 104, loss = 0.09489637\n",
      "Iteration 105, loss = 0.09455765\n",
      "Iteration 106, loss = 0.09424905\n",
      "Iteration 107, loss = 0.09401394\n",
      "Iteration 108, loss = 0.09370842\n",
      "Iteration 109, loss = 0.09342744\n",
      "Iteration 110, loss = 0.09317193\n",
      "Iteration 111, loss = 0.09292132\n",
      "Iteration 112, loss = 0.09262317\n",
      "Iteration 113, loss = 0.09237714\n",
      "Iteration 114, loss = 0.09214341\n",
      "Iteration 115, loss = 0.09189372\n",
      "Iteration 116, loss = 0.09165032\n",
      "Iteration 117, loss = 0.09142056\n",
      "Iteration 118, loss = 0.09123021\n",
      "Iteration 119, loss = 0.09099587\n",
      "Iteration 120, loss = 0.09080531\n",
      "Iteration 121, loss = 0.09059877\n",
      "Iteration 122, loss = 0.09037809\n",
      "Iteration 123, loss = 0.09017608\n",
      "Iteration 124, loss = 0.08994512\n",
      "Iteration 125, loss = 0.08974438\n",
      "Iteration 126, loss = 0.08956777\n",
      "Iteration 127, loss = 0.08938410\n",
      "Iteration 128, loss = 0.08921087\n",
      "Iteration 129, loss = 0.08900546\n",
      "Iteration 130, loss = 0.08883101\n",
      "Iteration 131, loss = 0.08863922\n",
      "Iteration 132, loss = 0.08847280\n",
      "Iteration 133, loss = 0.08828305\n",
      "Iteration 134, loss = 0.08814070\n",
      "Iteration 135, loss = 0.08793463\n",
      "Iteration 136, loss = 0.08776351\n",
      "Iteration 137, loss = 0.08762073\n",
      "Iteration 138, loss = 0.08747331\n",
      "Iteration 139, loss = 0.08732989\n",
      "Iteration 140, loss = 0.08715926\n",
      "Iteration 141, loss = 0.08702512\n",
      "Iteration 142, loss = 0.08684743\n",
      "Iteration 143, loss = 0.08668885\n",
      "Iteration 144, loss = 0.08655927\n",
      "Iteration 145, loss = 0.08641150\n",
      "Iteration 146, loss = 0.08627626\n",
      "Iteration 147, loss = 0.08614708\n",
      "Iteration 148, loss = 0.08600242\n",
      "Iteration 149, loss = 0.08587564\n",
      "Iteration 150, loss = 0.08572808\n",
      "Iteration 151, loss = 0.08558333\n",
      "Iteration 152, loss = 0.08543356\n",
      "Iteration 153, loss = 0.08531810\n",
      "Iteration 154, loss = 0.08520148\n",
      "Iteration 155, loss = 0.08507287\n",
      "Iteration 156, loss = 0.08494580\n",
      "Iteration 157, loss = 0.08482098\n",
      "Iteration 158, loss = 0.08471200\n",
      "Iteration 159, loss = 0.08460031\n",
      "Iteration 160, loss = 0.08447968\n",
      "Iteration 161, loss = 0.08435692\n",
      "Iteration 162, loss = 0.08425400\n",
      "Iteration 163, loss = 0.08413205\n",
      "Iteration 164, loss = 0.08401697\n",
      "Iteration 165, loss = 0.08390017\n",
      "Iteration 166, loss = 0.08379032\n",
      "Iteration 167, loss = 0.08367685\n",
      "Iteration 168, loss = 0.08356661\n",
      "Iteration 169, loss = 0.08345581\n",
      "Iteration 170, loss = 0.08334667\n",
      "Iteration 171, loss = 0.08324490\n",
      "Iteration 172, loss = 0.08313030\n",
      "Iteration 173, loss = 0.08304253\n",
      "Iteration 174, loss = 0.08292799\n",
      "Iteration 175, loss = 0.08283467\n",
      "Iteration 176, loss = 0.08272728\n",
      "Iteration 177, loss = 0.08261872\n",
      "Iteration 178, loss = 0.08252228\n",
      "Iteration 179, loss = 0.08240852\n",
      "Iteration 180, loss = 0.08232446\n",
      "Iteration 181, loss = 0.08222309\n",
      "Iteration 182, loss = 0.08212287\n",
      "Iteration 183, loss = 0.08203246\n",
      "Iteration 184, loss = 0.08194414\n",
      "Iteration 185, loss = 0.08184860\n",
      "Iteration 186, loss = 0.08175641\n",
      "Iteration 187, loss = 0.08167312\n",
      "Iteration 188, loss = 0.08157699\n",
      "Iteration 189, loss = 0.08148258\n",
      "Iteration 190, loss = 0.08139841\n",
      "Iteration 191, loss = 0.08131054\n",
      "Iteration 192, loss = 0.08121849\n",
      "Iteration 193, loss = 0.08112929\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38360589\n",
      "Iteration 2, loss = 2.22193175\n",
      "Iteration 3, loss = 2.03564778\n",
      "Iteration 4, loss = 1.81019774\n",
      "Iteration 5, loss = 1.56031065\n",
      "Iteration 6, loss = 1.31165983\n",
      "Iteration 7, loss = 1.08666845\n",
      "Iteration 8, loss = 0.90438644\n",
      "Iteration 9, loss = 0.76319397\n",
      "Iteration 10, loss = 0.65112743\n",
      "Iteration 11, loss = 0.56871040\n",
      "Iteration 12, loss = 0.50509495\n",
      "Iteration 13, loss = 0.45434915\n",
      "Iteration 14, loss = 0.41455490\n",
      "Iteration 15, loss = 0.38179106\n",
      "Iteration 16, loss = 0.35303973\n",
      "Iteration 17, loss = 0.32996068\n",
      "Iteration 18, loss = 0.30957727\n",
      "Iteration 19, loss = 0.29221890\n",
      "Iteration 20, loss = 0.27701937\n",
      "Iteration 21, loss = 0.26403002\n",
      "Iteration 22, loss = 0.25250781\n",
      "Iteration 23, loss = 0.24156467\n",
      "Iteration 24, loss = 0.23106839\n",
      "Iteration 25, loss = 0.22235644\n",
      "Iteration 26, loss = 0.21433490\n",
      "Iteration 27, loss = 0.20726344\n",
      "Iteration 28, loss = 0.20030147\n",
      "Iteration 29, loss = 0.19423293\n",
      "Iteration 30, loss = 0.18786496\n",
      "Iteration 31, loss = 0.18259331\n",
      "Iteration 32, loss = 0.17782155\n",
      "Iteration 33, loss = 0.17365941\n",
      "Iteration 34, loss = 0.16880146\n",
      "Iteration 35, loss = 0.16501425\n",
      "Iteration 36, loss = 0.16098238\n",
      "Iteration 37, loss = 0.15739038\n",
      "Iteration 38, loss = 0.15427340\n",
      "Iteration 39, loss = 0.15122071\n",
      "Iteration 40, loss = 0.14855881\n",
      "Iteration 41, loss = 0.14560652\n",
      "Iteration 42, loss = 0.14288534\n",
      "Iteration 43, loss = 0.14084928\n",
      "Iteration 44, loss = 0.13828002\n",
      "Iteration 45, loss = 0.13654819\n",
      "Iteration 46, loss = 0.13423509\n",
      "Iteration 47, loss = 0.13224585\n",
      "Iteration 48, loss = 0.13024907\n",
      "Iteration 49, loss = 0.12843688\n",
      "Iteration 50, loss = 0.12658546\n",
      "Iteration 51, loss = 0.12499948\n",
      "Iteration 52, loss = 0.12362733\n",
      "Iteration 53, loss = 0.12203600\n",
      "Iteration 54, loss = 0.12067313\n",
      "Iteration 55, loss = 0.11938206\n",
      "Iteration 56, loss = 0.11820076\n",
      "Iteration 57, loss = 0.11688948\n",
      "Iteration 58, loss = 0.11587767\n",
      "Iteration 59, loss = 0.11470703\n",
      "Iteration 60, loss = 0.11359928\n",
      "Iteration 61, loss = 0.11282097\n",
      "Iteration 62, loss = 0.11174814\n",
      "Iteration 63, loss = 0.11072602\n",
      "Iteration 64, loss = 0.10996798\n",
      "Iteration 65, loss = 0.10906552\n",
      "Iteration 66, loss = 0.10824387\n",
      "Iteration 67, loss = 0.10750115\n",
      "Iteration 68, loss = 0.10674989\n",
      "Iteration 69, loss = 0.10599428\n",
      "Iteration 70, loss = 0.10527959\n",
      "Iteration 71, loss = 0.10453613\n",
      "Iteration 72, loss = 0.10403694\n",
      "Iteration 73, loss = 0.10333360\n",
      "Iteration 74, loss = 0.10269429\n",
      "Iteration 75, loss = 0.10218216\n",
      "Iteration 76, loss = 0.10158364\n",
      "Iteration 77, loss = 0.10095989\n",
      "Iteration 78, loss = 0.10042025\n",
      "Iteration 79, loss = 0.09994480\n",
      "Iteration 80, loss = 0.09950099\n",
      "Iteration 81, loss = 0.09900346\n",
      "Iteration 82, loss = 0.09856363\n",
      "Iteration 83, loss = 0.09809344\n",
      "Iteration 84, loss = 0.09766003\n",
      "Iteration 85, loss = 0.09718967\n",
      "Iteration 86, loss = 0.09680064\n",
      "Iteration 87, loss = 0.09635711\n",
      "Iteration 88, loss = 0.09594302\n",
      "Iteration 89, loss = 0.09556453\n",
      "Iteration 90, loss = 0.09519575\n",
      "Iteration 91, loss = 0.09484437\n",
      "Iteration 92, loss = 0.09451867\n",
      "Iteration 93, loss = 0.09418028\n",
      "Iteration 94, loss = 0.09383814\n",
      "Iteration 95, loss = 0.09352679\n",
      "Iteration 96, loss = 0.09323841\n",
      "Iteration 97, loss = 0.09292704\n",
      "Iteration 98, loss = 0.09257652\n",
      "Iteration 99, loss = 0.09231159\n",
      "Iteration 100, loss = 0.09201233\n",
      "Iteration 101, loss = 0.09179150\n",
      "Iteration 102, loss = 0.09145935\n",
      "Iteration 103, loss = 0.09121504\n",
      "Iteration 104, loss = 0.09096719\n",
      "Iteration 105, loss = 0.09069161\n",
      "Iteration 106, loss = 0.09044319\n",
      "Iteration 107, loss = 0.09022295\n",
      "Iteration 108, loss = 0.08999363\n",
      "Iteration 109, loss = 0.08974568\n",
      "Iteration 110, loss = 0.08952321\n",
      "Iteration 111, loss = 0.08929057\n",
      "Iteration 112, loss = 0.08903720\n",
      "Iteration 113, loss = 0.08883553\n",
      "Iteration 114, loss = 0.08863817\n",
      "Iteration 115, loss = 0.08840415\n",
      "Iteration 116, loss = 0.08818155\n",
      "Iteration 117, loss = 0.08802733\n",
      "Iteration 118, loss = 0.08782655\n",
      "Iteration 119, loss = 0.08762930\n",
      "Iteration 120, loss = 0.08745124\n",
      "Iteration 121, loss = 0.08727206\n",
      "Iteration 122, loss = 0.08708448\n",
      "Iteration 123, loss = 0.08690796\n",
      "Iteration 124, loss = 0.08670268\n",
      "Iteration 125, loss = 0.08653247\n",
      "Iteration 126, loss = 0.08638456\n",
      "Iteration 127, loss = 0.08626252\n",
      "Iteration 128, loss = 0.08603352\n",
      "Iteration 129, loss = 0.08585608\n",
      "Iteration 130, loss = 0.08570755\n",
      "Iteration 131, loss = 0.08555835\n",
      "Iteration 132, loss = 0.08539684\n",
      "Iteration 133, loss = 0.08524768\n",
      "Iteration 134, loss = 0.08509227\n",
      "Iteration 135, loss = 0.08493996\n",
      "Iteration 136, loss = 0.08478854\n",
      "Iteration 137, loss = 0.08465163\n",
      "Iteration 138, loss = 0.08454087\n",
      "Iteration 139, loss = 0.08438443\n",
      "Iteration 140, loss = 0.08423829\n",
      "Iteration 141, loss = 0.08410544\n",
      "Iteration 142, loss = 0.08398056\n",
      "Iteration 143, loss = 0.08383673\n",
      "Iteration 144, loss = 0.08371100\n",
      "Iteration 145, loss = 0.08357980\n",
      "Iteration 146, loss = 0.08345265\n",
      "Iteration 147, loss = 0.08332585\n",
      "Iteration 148, loss = 0.08320304\n",
      "Iteration 149, loss = 0.08308106\n",
      "Iteration 150, loss = 0.08295803\n",
      "Iteration 151, loss = 0.08283725\n",
      "Iteration 152, loss = 0.08271989\n",
      "Iteration 153, loss = 0.08261030\n",
      "Iteration 154, loss = 0.08248942\n",
      "Iteration 155, loss = 0.08237743\n",
      "Iteration 156, loss = 0.08225708\n",
      "Iteration 157, loss = 0.08214095\n",
      "Iteration 158, loss = 0.08203122\n",
      "Iteration 159, loss = 0.08192787\n",
      "Iteration 160, loss = 0.08182800\n",
      "Iteration 161, loss = 0.08171307\n",
      "Iteration 162, loss = 0.08162447\n",
      "Iteration 163, loss = 0.08152765\n",
      "Iteration 164, loss = 0.08142328\n",
      "Iteration 165, loss = 0.08130325\n",
      "Iteration 166, loss = 0.08120000\n",
      "Iteration 167, loss = 0.08109361\n",
      "Iteration 168, loss = 0.08099225\n",
      "Iteration 169, loss = 0.08090491\n",
      "Iteration 170, loss = 0.08079852\n",
      "Iteration 171, loss = 0.08068855\n",
      "Iteration 172, loss = 0.08059252\n",
      "Iteration 173, loss = 0.08050074\n",
      "Iteration 174, loss = 0.08040623\n",
      "Iteration 175, loss = 0.08032322\n",
      "Iteration 176, loss = 0.08022045\n",
      "Iteration 177, loss = 0.08012969\n",
      "Iteration 178, loss = 0.08003006\n",
      "Iteration 179, loss = 0.07992951\n",
      "Iteration 180, loss = 0.07984451\n",
      "Iteration 181, loss = 0.07975953\n",
      "Iteration 182, loss = 0.07967094\n",
      "Iteration 183, loss = 0.07957004\n",
      "Iteration 184, loss = 0.07949169\n",
      "Iteration 185, loss = 0.07940536\n",
      "Iteration 186, loss = 0.07931431\n",
      "Iteration 187, loss = 0.07922737\n",
      "Iteration 188, loss = 0.07913699\n",
      "Iteration 189, loss = 0.07904902\n",
      "Iteration 190, loss = 0.07897375\n",
      "Iteration 191, loss = 0.07889244\n",
      "Iteration 192, loss = 0.07880519\n",
      "Iteration 193, loss = 0.07872455\n",
      "Iteration 194, loss = 0.07863683\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27588163\n",
      "Iteration 2, loss = 1.85528260\n",
      "Iteration 3, loss = 1.42869829\n",
      "Iteration 4, loss = 1.03911399\n",
      "Iteration 5, loss = 0.74343934\n",
      "Iteration 6, loss = 0.54799921\n",
      "Iteration 7, loss = 0.42062789\n",
      "Iteration 8, loss = 0.33650128\n",
      "Iteration 9, loss = 0.27800201\n",
      "Iteration 10, loss = 0.23721510\n",
      "Iteration 11, loss = 0.20836609\n",
      "Iteration 12, loss = 0.18526414\n",
      "Iteration 13, loss = 0.16767947\n",
      "Iteration 14, loss = 0.15368208\n",
      "Iteration 15, loss = 0.14270525\n",
      "Iteration 16, loss = 0.13280286\n",
      "Iteration 17, loss = 0.12519781\n",
      "Iteration 18, loss = 0.11900882\n",
      "Iteration 19, loss = 0.11348877\n",
      "Iteration 20, loss = 0.10898797\n",
      "Iteration 21, loss = 0.10502646\n",
      "Iteration 22, loss = 0.10153143\n",
      "Iteration 23, loss = 0.09842903\n",
      "Iteration 24, loss = 0.09579376\n",
      "Iteration 25, loss = 0.09343436\n",
      "Iteration 26, loss = 0.09127094\n",
      "Iteration 27, loss = 0.08920457\n",
      "Iteration 28, loss = 0.08739712\n",
      "Iteration 29, loss = 0.08579755\n",
      "Iteration 30, loss = 0.08428130\n",
      "Iteration 31, loss = 0.08280728\n",
      "Iteration 32, loss = 0.08146366\n",
      "Iteration 33, loss = 0.08025637\n",
      "Iteration 34, loss = 0.07904905\n",
      "Iteration 35, loss = 0.07793712\n",
      "Iteration 36, loss = 0.07688362\n",
      "Iteration 37, loss = 0.07585527\n",
      "Iteration 38, loss = 0.07486525\n",
      "Iteration 39, loss = 0.07394497\n",
      "Iteration 40, loss = 0.07305034\n",
      "Iteration 41, loss = 0.07218080\n",
      "Iteration 42, loss = 0.07131876\n",
      "Iteration 43, loss = 0.07050882\n",
      "Iteration 44, loss = 0.06975164\n",
      "Iteration 45, loss = 0.06896961\n",
      "Iteration 46, loss = 0.06822941\n",
      "Iteration 47, loss = 0.06752527\n",
      "Iteration 48, loss = 0.06683559\n",
      "Iteration 49, loss = 0.06612459\n",
      "Iteration 50, loss = 0.06548642\n",
      "Iteration 51, loss = 0.06482747\n",
      "Iteration 52, loss = 0.06419015\n",
      "Iteration 53, loss = 0.06359357\n",
      "Iteration 54, loss = 0.06297973\n",
      "Iteration 55, loss = 0.06238454\n",
      "Iteration 56, loss = 0.06182309\n",
      "Iteration 57, loss = 0.06129096\n",
      "Iteration 58, loss = 0.06073263\n",
      "Iteration 59, loss = 0.06020769\n",
      "Iteration 60, loss = 0.05972817\n",
      "Iteration 61, loss = 0.05920792\n",
      "Iteration 62, loss = 0.05870080\n",
      "Iteration 63, loss = 0.05821539\n",
      "Iteration 64, loss = 0.05775494\n",
      "Iteration 65, loss = 0.05731790\n",
      "Iteration 66, loss = 0.05685704\n",
      "Iteration 67, loss = 0.05642508\n",
      "Iteration 68, loss = 0.05597729\n",
      "Iteration 69, loss = 0.05557931\n",
      "Iteration 70, loss = 0.05516133\n",
      "Iteration 71, loss = 0.05475244\n",
      "Iteration 72, loss = 0.05438444\n",
      "Iteration 73, loss = 0.05397948\n",
      "Iteration 74, loss = 0.05363197\n",
      "Iteration 75, loss = 0.05327634\n",
      "Iteration 76, loss = 0.05289324\n",
      "Iteration 77, loss = 0.05255154\n",
      "Iteration 78, loss = 0.05217672\n",
      "Iteration 79, loss = 0.05187433\n",
      "Iteration 80, loss = 0.05153971\n",
      "Iteration 81, loss = 0.05119657\n",
      "Iteration 82, loss = 0.05091075\n",
      "Iteration 83, loss = 0.05059124\n",
      "Iteration 84, loss = 0.05029945\n",
      "Iteration 85, loss = 0.05000061\n",
      "Iteration 86, loss = 0.04971678\n",
      "Iteration 87, loss = 0.04944765\n",
      "Iteration 88, loss = 0.04917833\n",
      "Iteration 89, loss = 0.04891422\n",
      "Iteration 90, loss = 0.04864903\n",
      "Iteration 91, loss = 0.04840919\n",
      "Iteration 92, loss = 0.04816215\n",
      "Iteration 93, loss = 0.04795899\n",
      "Iteration 94, loss = 0.04768506\n",
      "Iteration 95, loss = 0.04745871\n",
      "Iteration 96, loss = 0.04723863\n",
      "Iteration 97, loss = 0.04703473\n",
      "Iteration 98, loss = 0.04679215\n",
      "Iteration 99, loss = 0.04658527\n",
      "Iteration 100, loss = 0.04634410\n",
      "Iteration 101, loss = 0.04614651\n",
      "Iteration 102, loss = 0.04596157\n",
      "Iteration 103, loss = 0.04576464\n",
      "Iteration 104, loss = 0.04557831\n",
      "Iteration 105, loss = 0.04538332\n",
      "Iteration 106, loss = 0.04522557\n",
      "Iteration 107, loss = 0.04503291\n",
      "Iteration 108, loss = 0.04484819\n",
      "Iteration 109, loss = 0.04469699\n",
      "Iteration 110, loss = 0.04456185\n",
      "Iteration 111, loss = 0.04439766\n",
      "Iteration 112, loss = 0.04424083\n",
      "Iteration 113, loss = 0.04406147\n",
      "Iteration 114, loss = 0.04392801\n",
      "Iteration 115, loss = 0.04376357\n",
      "Iteration 116, loss = 0.04361993\n",
      "Iteration 117, loss = 0.04352618\n",
      "Iteration 118, loss = 0.04334578\n",
      "Iteration 119, loss = 0.04321536\n",
      "Iteration 120, loss = 0.04309612\n",
      "Iteration 121, loss = 0.04297483\n",
      "Iteration 122, loss = 0.04283444\n",
      "Iteration 123, loss = 0.04270819\n",
      "Iteration 124, loss = 0.04260675\n",
      "Iteration 125, loss = 0.04250479\n",
      "Iteration 126, loss = 0.04237527\n",
      "Iteration 127, loss = 0.04225759\n",
      "Iteration 128, loss = 0.04216968\n",
      "Iteration 129, loss = 0.04209200\n",
      "Iteration 130, loss = 0.04194952\n",
      "Iteration 131, loss = 0.04189892\n",
      "Iteration 132, loss = 0.04176455\n",
      "Iteration 133, loss = 0.04166767\n",
      "Iteration 134, loss = 0.04155829\n",
      "Iteration 135, loss = 0.04146161\n",
      "Iteration 136, loss = 0.04136283\n",
      "Iteration 137, loss = 0.04128659\n",
      "Iteration 138, loss = 0.04120964\n",
      "Iteration 139, loss = 0.04110648\n",
      "Iteration 140, loss = 0.04104939\n",
      "Iteration 141, loss = 0.04095128\n",
      "Iteration 142, loss = 0.04088761\n",
      "Iteration 143, loss = 0.04080168\n",
      "Iteration 144, loss = 0.04069740\n",
      "Iteration 145, loss = 0.04069410\n",
      "Iteration 146, loss = 0.04057594\n",
      "Iteration 147, loss = 0.04052491\n",
      "Iteration 148, loss = 0.04043920\n",
      "Iteration 149, loss = 0.04038662\n",
      "Iteration 150, loss = 0.04030293\n",
      "Iteration 151, loss = 0.04025627\n",
      "Iteration 152, loss = 0.04018535\n",
      "Iteration 153, loss = 0.04014443\n",
      "Iteration 154, loss = 0.04003153\n",
      "Iteration 155, loss = 0.04000732\n",
      "Iteration 156, loss = 0.03993786\n",
      "Iteration 157, loss = 0.03991241\n",
      "Iteration 158, loss = 0.03984959\n",
      "Iteration 159, loss = 0.03986381\n",
      "Iteration 160, loss = 0.03975858\n",
      "Iteration 161, loss = 0.03968474\n",
      "Iteration 162, loss = 0.03964217\n",
      "Iteration 163, loss = 0.03956198\n",
      "Iteration 164, loss = 0.03951663\n",
      "Iteration 165, loss = 0.03947129\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28078213\n",
      "Iteration 2, loss = 1.85557759\n",
      "Iteration 3, loss = 1.42716082\n",
      "Iteration 4, loss = 1.03446466\n",
      "Iteration 5, loss = 0.73600103\n",
      "Iteration 6, loss = 0.54003064\n",
      "Iteration 7, loss = 0.41592689\n",
      "Iteration 8, loss = 0.33353504\n",
      "Iteration 9, loss = 0.27733319\n",
      "Iteration 10, loss = 0.23634070\n",
      "Iteration 11, loss = 0.20790338\n",
      "Iteration 12, loss = 0.18466669\n",
      "Iteration 13, loss = 0.16729147\n",
      "Iteration 14, loss = 0.15281240\n",
      "Iteration 15, loss = 0.14240560\n",
      "Iteration 16, loss = 0.13265428\n",
      "Iteration 17, loss = 0.12470518\n",
      "Iteration 18, loss = 0.11810586\n",
      "Iteration 19, loss = 0.11275178\n",
      "Iteration 20, loss = 0.10799313\n",
      "Iteration 21, loss = 0.10409850\n",
      "Iteration 22, loss = 0.10074335\n",
      "Iteration 23, loss = 0.09757664\n",
      "Iteration 24, loss = 0.09487757\n",
      "Iteration 25, loss = 0.09243459\n",
      "Iteration 26, loss = 0.09027443\n",
      "Iteration 27, loss = 0.08822331\n",
      "Iteration 28, loss = 0.08638984\n",
      "Iteration 29, loss = 0.08492351\n",
      "Iteration 30, loss = 0.08338876\n",
      "Iteration 31, loss = 0.08187980\n",
      "Iteration 32, loss = 0.08052463\n",
      "Iteration 33, loss = 0.07934379\n",
      "Iteration 34, loss = 0.07813294\n",
      "Iteration 35, loss = 0.07703442\n",
      "Iteration 36, loss = 0.07599015\n",
      "Iteration 37, loss = 0.07496071\n",
      "Iteration 38, loss = 0.07400940\n",
      "Iteration 39, loss = 0.07307861\n",
      "Iteration 40, loss = 0.07220049\n",
      "Iteration 41, loss = 0.07133133\n",
      "Iteration 42, loss = 0.07048187\n",
      "Iteration 43, loss = 0.06968300\n",
      "Iteration 44, loss = 0.06891781\n",
      "Iteration 45, loss = 0.06816158\n",
      "Iteration 46, loss = 0.06739950\n",
      "Iteration 47, loss = 0.06668643\n",
      "Iteration 48, loss = 0.06601999\n",
      "Iteration 49, loss = 0.06533227\n",
      "Iteration 50, loss = 0.06469129\n",
      "Iteration 51, loss = 0.06404315\n",
      "Iteration 52, loss = 0.06342596\n",
      "Iteration 53, loss = 0.06283593\n",
      "Iteration 54, loss = 0.06220350\n",
      "Iteration 55, loss = 0.06163434\n",
      "Iteration 56, loss = 0.06106746\n",
      "Iteration 57, loss = 0.06052280\n",
      "Iteration 58, loss = 0.06002194\n",
      "Iteration 59, loss = 0.05946197\n",
      "Iteration 60, loss = 0.05897829\n",
      "Iteration 61, loss = 0.05847853\n",
      "Iteration 62, loss = 0.05803191\n",
      "Iteration 63, loss = 0.05754084\n",
      "Iteration 64, loss = 0.05702931\n",
      "Iteration 65, loss = 0.05660762\n",
      "Iteration 66, loss = 0.05614624\n",
      "Iteration 67, loss = 0.05570423\n",
      "Iteration 68, loss = 0.05530335\n",
      "Iteration 69, loss = 0.05485998\n",
      "Iteration 70, loss = 0.05444863\n",
      "Iteration 71, loss = 0.05406136\n",
      "Iteration 72, loss = 0.05371639\n",
      "Iteration 73, loss = 0.05332040\n",
      "Iteration 74, loss = 0.05297545\n",
      "Iteration 75, loss = 0.05258053\n",
      "Iteration 76, loss = 0.05225083\n",
      "Iteration 77, loss = 0.05185888\n",
      "Iteration 78, loss = 0.05152742\n",
      "Iteration 79, loss = 0.05116162\n",
      "Iteration 80, loss = 0.05087764\n",
      "Iteration 81, loss = 0.05054430\n",
      "Iteration 82, loss = 0.05025296\n",
      "Iteration 83, loss = 0.04991780\n",
      "Iteration 84, loss = 0.04961308\n",
      "Iteration 85, loss = 0.04933832\n",
      "Iteration 86, loss = 0.04905759\n",
      "Iteration 87, loss = 0.04878321\n",
      "Iteration 88, loss = 0.04852316\n",
      "Iteration 89, loss = 0.04826695\n",
      "Iteration 90, loss = 0.04802085\n",
      "Iteration 91, loss = 0.04775570\n",
      "Iteration 92, loss = 0.04752906\n",
      "Iteration 93, loss = 0.04730263\n",
      "Iteration 94, loss = 0.04705029\n",
      "Iteration 95, loss = 0.04683417\n",
      "Iteration 96, loss = 0.04660692\n",
      "Iteration 97, loss = 0.04643678\n",
      "Iteration 98, loss = 0.04617312\n",
      "Iteration 99, loss = 0.04594681\n",
      "Iteration 100, loss = 0.04573579\n",
      "Iteration 101, loss = 0.04554093\n",
      "Iteration 102, loss = 0.04535233\n",
      "Iteration 103, loss = 0.04515624\n",
      "Iteration 104, loss = 0.04499406\n",
      "Iteration 105, loss = 0.04478409\n",
      "Iteration 106, loss = 0.04462203\n",
      "Iteration 107, loss = 0.04443359\n",
      "Iteration 108, loss = 0.04426976\n",
      "Iteration 109, loss = 0.04408900\n",
      "Iteration 110, loss = 0.04397084\n",
      "Iteration 111, loss = 0.04380808\n",
      "Iteration 112, loss = 0.04363050\n",
      "Iteration 113, loss = 0.04347888\n",
      "Iteration 114, loss = 0.04332330\n",
      "Iteration 115, loss = 0.04318780\n",
      "Iteration 116, loss = 0.04305423\n",
      "Iteration 117, loss = 0.04290985\n",
      "Iteration 118, loss = 0.04277126\n",
      "Iteration 119, loss = 0.04263305\n",
      "Iteration 120, loss = 0.04250845\n",
      "Iteration 121, loss = 0.04239692\n",
      "Iteration 122, loss = 0.04225982\n",
      "Iteration 123, loss = 0.04214539\n",
      "Iteration 124, loss = 0.04205339\n",
      "Iteration 125, loss = 0.04192826\n",
      "Iteration 126, loss = 0.04182234\n",
      "Iteration 127, loss = 0.04171211\n",
      "Iteration 128, loss = 0.04159510\n",
      "Iteration 129, loss = 0.04150948\n",
      "Iteration 130, loss = 0.04139281\n",
      "Iteration 131, loss = 0.04131172\n",
      "Iteration 132, loss = 0.04120880\n",
      "Iteration 133, loss = 0.04110123\n",
      "Iteration 134, loss = 0.04100920\n",
      "Iteration 135, loss = 0.04093389\n",
      "Iteration 136, loss = 0.04080057\n",
      "Iteration 137, loss = 0.04073842\n",
      "Iteration 138, loss = 0.04063413\n",
      "Iteration 139, loss = 0.04053809\n",
      "Iteration 140, loss = 0.04047182\n",
      "Iteration 141, loss = 0.04037911\n",
      "Iteration 142, loss = 0.04030742\n",
      "Iteration 143, loss = 0.04021513\n",
      "Iteration 144, loss = 0.04015590\n",
      "Iteration 145, loss = 0.04013756\n",
      "Iteration 146, loss = 0.04003425\n",
      "Iteration 147, loss = 0.03998588\n",
      "Iteration 148, loss = 0.03988475\n",
      "Iteration 149, loss = 0.03982902\n",
      "Iteration 150, loss = 0.03973276\n",
      "Iteration 151, loss = 0.03967790\n",
      "Iteration 152, loss = 0.03960933\n",
      "Iteration 153, loss = 0.03956465\n",
      "Iteration 154, loss = 0.03947439\n",
      "Iteration 155, loss = 0.03943381\n",
      "Iteration 156, loss = 0.03937074\n",
      "Iteration 157, loss = 0.03933944\n",
      "Iteration 158, loss = 0.03929666\n",
      "Iteration 159, loss = 0.03925672\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28582317\n",
      "Iteration 2, loss = 1.87953803\n",
      "Iteration 3, loss = 1.46571832\n",
      "Iteration 4, loss = 1.07389773\n",
      "Iteration 5, loss = 0.76787433\n",
      "Iteration 6, loss = 0.56571638\n",
      "Iteration 7, loss = 0.43358879\n",
      "Iteration 8, loss = 0.34713568\n",
      "Iteration 9, loss = 0.28781557\n",
      "Iteration 10, loss = 0.24649197\n",
      "Iteration 11, loss = 0.21515869\n",
      "Iteration 12, loss = 0.19172960\n",
      "Iteration 13, loss = 0.17258436\n",
      "Iteration 14, loss = 0.15788083\n",
      "Iteration 15, loss = 0.14533296\n",
      "Iteration 16, loss = 0.13610466\n",
      "Iteration 17, loss = 0.12770807\n",
      "Iteration 18, loss = 0.12087907\n",
      "Iteration 19, loss = 0.11522971\n",
      "Iteration 20, loss = 0.11056739\n",
      "Iteration 21, loss = 0.10637179\n",
      "Iteration 22, loss = 0.10289390\n",
      "Iteration 23, loss = 0.09984274\n",
      "Iteration 24, loss = 0.09702832\n",
      "Iteration 25, loss = 0.09451650\n",
      "Iteration 26, loss = 0.09231353\n",
      "Iteration 27, loss = 0.09037481\n",
      "Iteration 28, loss = 0.08850022\n",
      "Iteration 29, loss = 0.08681191\n",
      "Iteration 30, loss = 0.08520426\n",
      "Iteration 31, loss = 0.08369553\n",
      "Iteration 32, loss = 0.08236523\n",
      "Iteration 33, loss = 0.08112612\n",
      "Iteration 34, loss = 0.07986514\n",
      "Iteration 35, loss = 0.07879419\n",
      "Iteration 36, loss = 0.07763557\n",
      "Iteration 37, loss = 0.07660726\n",
      "Iteration 38, loss = 0.07561748\n",
      "Iteration 39, loss = 0.07464834\n",
      "Iteration 40, loss = 0.07372813\n",
      "Iteration 41, loss = 0.07288226\n",
      "Iteration 42, loss = 0.07200686\n",
      "Iteration 43, loss = 0.07119214\n",
      "Iteration 44, loss = 0.07040501\n",
      "Iteration 45, loss = 0.06965539\n",
      "Iteration 46, loss = 0.06886532\n",
      "Iteration 47, loss = 0.06812491\n",
      "Iteration 48, loss = 0.06741343\n",
      "Iteration 49, loss = 0.06671986\n",
      "Iteration 50, loss = 0.06608881\n",
      "Iteration 51, loss = 0.06542784\n",
      "Iteration 52, loss = 0.06481263\n",
      "Iteration 53, loss = 0.06418825\n",
      "Iteration 54, loss = 0.06361357\n",
      "Iteration 55, loss = 0.06302287\n",
      "Iteration 56, loss = 0.06249157\n",
      "Iteration 57, loss = 0.06189693\n",
      "Iteration 58, loss = 0.06135810\n",
      "Iteration 59, loss = 0.06083478\n",
      "Iteration 60, loss = 0.06029223\n",
      "Iteration 61, loss = 0.05978515\n",
      "Iteration 62, loss = 0.05931045\n",
      "Iteration 63, loss = 0.05882349\n",
      "Iteration 64, loss = 0.05838302\n",
      "Iteration 65, loss = 0.05789899\n",
      "Iteration 66, loss = 0.05749017\n",
      "Iteration 67, loss = 0.05701255\n",
      "Iteration 68, loss = 0.05659123\n",
      "Iteration 69, loss = 0.05617773\n",
      "Iteration 70, loss = 0.05574420\n",
      "Iteration 71, loss = 0.05534163\n",
      "Iteration 72, loss = 0.05495874\n",
      "Iteration 73, loss = 0.05456431\n",
      "Iteration 74, loss = 0.05421202\n",
      "Iteration 75, loss = 0.05381406\n",
      "Iteration 76, loss = 0.05346370\n",
      "Iteration 77, loss = 0.05309695\n",
      "Iteration 78, loss = 0.05275768\n",
      "Iteration 79, loss = 0.05241021\n",
      "Iteration 80, loss = 0.05210348\n",
      "Iteration 81, loss = 0.05180751\n",
      "Iteration 82, loss = 0.05148134\n",
      "Iteration 83, loss = 0.05115598\n",
      "Iteration 84, loss = 0.05086026\n",
      "Iteration 85, loss = 0.05059544\n",
      "Iteration 86, loss = 0.05030392\n",
      "Iteration 87, loss = 0.04999923\n",
      "Iteration 88, loss = 0.04972806\n",
      "Iteration 89, loss = 0.04948508\n",
      "Iteration 90, loss = 0.04921133\n",
      "Iteration 91, loss = 0.04896552\n",
      "Iteration 92, loss = 0.04867599\n",
      "Iteration 93, loss = 0.04845461\n",
      "Iteration 94, loss = 0.04821126\n",
      "Iteration 95, loss = 0.04796403\n",
      "Iteration 96, loss = 0.04773648\n",
      "Iteration 97, loss = 0.04750478\n",
      "Iteration 98, loss = 0.04728547\n",
      "Iteration 99, loss = 0.04706818\n",
      "Iteration 100, loss = 0.04685027\n",
      "Iteration 101, loss = 0.04668272\n",
      "Iteration 102, loss = 0.04645791\n",
      "Iteration 103, loss = 0.04628973\n",
      "Iteration 104, loss = 0.04609274\n",
      "Iteration 105, loss = 0.04588954\n",
      "Iteration 106, loss = 0.04571117\n",
      "Iteration 107, loss = 0.04553931\n",
      "Iteration 108, loss = 0.04537185\n",
      "Iteration 109, loss = 0.04519764\n",
      "Iteration 110, loss = 0.04502622\n",
      "Iteration 111, loss = 0.04487214\n",
      "Iteration 112, loss = 0.04468626\n",
      "Iteration 113, loss = 0.04453744\n",
      "Iteration 114, loss = 0.04436881\n",
      "Iteration 115, loss = 0.04422755\n",
      "Iteration 116, loss = 0.04409231\n",
      "Iteration 117, loss = 0.04394097\n",
      "Iteration 118, loss = 0.04380324\n",
      "Iteration 119, loss = 0.04365010\n",
      "Iteration 120, loss = 0.04356937\n",
      "Iteration 121, loss = 0.04342072\n",
      "Iteration 122, loss = 0.04326439\n",
      "Iteration 123, loss = 0.04315550\n",
      "Iteration 124, loss = 0.04300868\n",
      "Iteration 125, loss = 0.04289371\n",
      "Iteration 126, loss = 0.04277601\n",
      "Iteration 127, loss = 0.04267678\n",
      "Iteration 128, loss = 0.04256742\n",
      "Iteration 129, loss = 0.04244702\n",
      "Iteration 130, loss = 0.04232017\n",
      "Iteration 131, loss = 0.04222088\n",
      "Iteration 132, loss = 0.04214403\n",
      "Iteration 133, loss = 0.04203558\n",
      "Iteration 134, loss = 0.04192176\n",
      "Iteration 135, loss = 0.04182267\n",
      "Iteration 136, loss = 0.04176050\n",
      "Iteration 137, loss = 0.04164893\n",
      "Iteration 138, loss = 0.04157882\n",
      "Iteration 139, loss = 0.04146110\n",
      "Iteration 140, loss = 0.04137783\n",
      "Iteration 141, loss = 0.04131660\n",
      "Iteration 142, loss = 0.04123209\n",
      "Iteration 143, loss = 0.04111947\n",
      "Iteration 144, loss = 0.04105321\n",
      "Iteration 145, loss = 0.04098434\n",
      "Iteration 146, loss = 0.04090708\n",
      "Iteration 147, loss = 0.04087165\n",
      "Iteration 148, loss = 0.04075256\n",
      "Iteration 149, loss = 0.04070275\n",
      "Iteration 150, loss = 0.04067239\n",
      "Iteration 151, loss = 0.04056950\n",
      "Iteration 152, loss = 0.04048776\n",
      "Iteration 153, loss = 0.04042092\n",
      "Iteration 154, loss = 0.04036462\n",
      "Iteration 155, loss = 0.04032143\n",
      "Iteration 156, loss = 0.04024720\n",
      "Iteration 157, loss = 0.04019545\n",
      "Iteration 158, loss = 0.04013335\n",
      "Iteration 159, loss = 0.04007496\n",
      "Iteration 160, loss = 0.04000690\n",
      "Iteration 161, loss = 0.03997484\n",
      "Iteration 162, loss = 0.03995293\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27926704\n",
      "Iteration 2, loss = 1.86590904\n",
      "Iteration 3, loss = 1.44593780\n",
      "Iteration 4, loss = 1.06254942\n",
      "Iteration 5, loss = 0.76756276\n",
      "Iteration 6, loss = 0.57160735\n",
      "Iteration 7, loss = 0.44147123\n",
      "Iteration 8, loss = 0.35418861\n",
      "Iteration 9, loss = 0.29430445\n",
      "Iteration 10, loss = 0.25078473\n",
      "Iteration 11, loss = 0.21842831\n",
      "Iteration 12, loss = 0.19385793\n",
      "Iteration 13, loss = 0.17471300\n",
      "Iteration 14, loss = 0.15951085\n",
      "Iteration 15, loss = 0.14706342\n",
      "Iteration 16, loss = 0.13727435\n",
      "Iteration 17, loss = 0.12861186\n",
      "Iteration 18, loss = 0.12191891\n",
      "Iteration 19, loss = 0.11587872\n",
      "Iteration 20, loss = 0.11096585\n",
      "Iteration 21, loss = 0.10681072\n",
      "Iteration 22, loss = 0.10320927\n",
      "Iteration 23, loss = 0.10007974\n",
      "Iteration 24, loss = 0.09712966\n",
      "Iteration 25, loss = 0.09468822\n",
      "Iteration 26, loss = 0.09242311\n",
      "Iteration 27, loss = 0.09034626\n",
      "Iteration 28, loss = 0.08850551\n",
      "Iteration 29, loss = 0.08679762\n",
      "Iteration 30, loss = 0.08521457\n",
      "Iteration 31, loss = 0.08370964\n",
      "Iteration 32, loss = 0.08234627\n",
      "Iteration 33, loss = 0.08105842\n",
      "Iteration 34, loss = 0.07980769\n",
      "Iteration 35, loss = 0.07870320\n",
      "Iteration 36, loss = 0.07752821\n",
      "Iteration 37, loss = 0.07651467\n",
      "Iteration 38, loss = 0.07553602\n",
      "Iteration 39, loss = 0.07456320\n",
      "Iteration 40, loss = 0.07367050\n",
      "Iteration 41, loss = 0.07278899\n",
      "Iteration 42, loss = 0.07190233\n",
      "Iteration 43, loss = 0.07109577\n",
      "Iteration 44, loss = 0.07032475\n",
      "Iteration 45, loss = 0.06959180\n",
      "Iteration 46, loss = 0.06879170\n",
      "Iteration 47, loss = 0.06804328\n",
      "Iteration 48, loss = 0.06737320\n",
      "Iteration 49, loss = 0.06666095\n",
      "Iteration 50, loss = 0.06602021\n",
      "Iteration 51, loss = 0.06537016\n",
      "Iteration 52, loss = 0.06476970\n",
      "Iteration 53, loss = 0.06414040\n",
      "Iteration 54, loss = 0.06354496\n",
      "Iteration 55, loss = 0.06298338\n",
      "Iteration 56, loss = 0.06245075\n",
      "Iteration 57, loss = 0.06185991\n",
      "Iteration 58, loss = 0.06132323\n",
      "Iteration 59, loss = 0.06082543\n",
      "Iteration 60, loss = 0.06027258\n",
      "Iteration 61, loss = 0.05981486\n",
      "Iteration 62, loss = 0.05929944\n",
      "Iteration 63, loss = 0.05883677\n",
      "Iteration 64, loss = 0.05837853\n",
      "Iteration 65, loss = 0.05790847\n",
      "Iteration 66, loss = 0.05747702\n",
      "Iteration 67, loss = 0.05702179\n",
      "Iteration 68, loss = 0.05661174\n",
      "Iteration 69, loss = 0.05618424\n",
      "Iteration 70, loss = 0.05577736\n",
      "Iteration 71, loss = 0.05537894\n",
      "Iteration 72, loss = 0.05503416\n",
      "Iteration 73, loss = 0.05460762\n",
      "Iteration 74, loss = 0.05426927\n",
      "Iteration 75, loss = 0.05388056\n",
      "Iteration 76, loss = 0.05353991\n",
      "Iteration 77, loss = 0.05318299\n",
      "Iteration 78, loss = 0.05284738\n",
      "Iteration 79, loss = 0.05251670\n",
      "Iteration 80, loss = 0.05223551\n",
      "Iteration 81, loss = 0.05192397\n",
      "Iteration 82, loss = 0.05160795\n",
      "Iteration 83, loss = 0.05125686\n",
      "Iteration 84, loss = 0.05099075\n",
      "Iteration 85, loss = 0.05069815\n",
      "Iteration 86, loss = 0.05039792\n",
      "Iteration 87, loss = 0.05014015\n",
      "Iteration 88, loss = 0.04986719\n",
      "Iteration 89, loss = 0.04962306\n",
      "Iteration 90, loss = 0.04937043\n",
      "Iteration 91, loss = 0.04912703\n",
      "Iteration 92, loss = 0.04882234\n",
      "Iteration 93, loss = 0.04861527\n",
      "Iteration 94, loss = 0.04838272\n",
      "Iteration 95, loss = 0.04814505\n",
      "Iteration 96, loss = 0.04791015\n",
      "Iteration 97, loss = 0.04770941\n",
      "Iteration 98, loss = 0.04748341\n",
      "Iteration 99, loss = 0.04729060\n",
      "Iteration 100, loss = 0.04705719\n",
      "Iteration 101, loss = 0.04691247\n",
      "Iteration 102, loss = 0.04667996\n",
      "Iteration 103, loss = 0.04652874\n",
      "Iteration 104, loss = 0.04631343\n",
      "Iteration 105, loss = 0.04614261\n",
      "Iteration 106, loss = 0.04597450\n",
      "Iteration 107, loss = 0.04579988\n",
      "Iteration 108, loss = 0.04559468\n",
      "Iteration 109, loss = 0.04543423\n",
      "Iteration 110, loss = 0.04530070\n",
      "Iteration 111, loss = 0.04516604\n",
      "Iteration 112, loss = 0.04495952\n",
      "Iteration 113, loss = 0.04483311\n",
      "Iteration 114, loss = 0.04464708\n",
      "Iteration 115, loss = 0.04450598\n",
      "Iteration 116, loss = 0.04437667\n",
      "Iteration 117, loss = 0.04421498\n",
      "Iteration 118, loss = 0.04410436\n",
      "Iteration 119, loss = 0.04397287\n",
      "Iteration 120, loss = 0.04387170\n",
      "Iteration 121, loss = 0.04374323\n",
      "Iteration 122, loss = 0.04357966\n",
      "Iteration 123, loss = 0.04351829\n",
      "Iteration 124, loss = 0.04333722\n",
      "Iteration 125, loss = 0.04323479\n",
      "Iteration 126, loss = 0.04313267\n",
      "Iteration 127, loss = 0.04303083\n",
      "Iteration 128, loss = 0.04292627\n",
      "Iteration 129, loss = 0.04281735\n",
      "Iteration 130, loss = 0.04267203\n",
      "Iteration 131, loss = 0.04257517\n",
      "Iteration 132, loss = 0.04249546\n",
      "Iteration 133, loss = 0.04240967\n",
      "Iteration 134, loss = 0.04233019\n",
      "Iteration 135, loss = 0.04219254\n",
      "Iteration 136, loss = 0.04215107\n",
      "Iteration 137, loss = 0.04206950\n",
      "Iteration 138, loss = 0.04199703\n",
      "Iteration 139, loss = 0.04189733\n",
      "Iteration 140, loss = 0.04176740\n",
      "Iteration 141, loss = 0.04176611\n",
      "Iteration 142, loss = 0.04162701\n",
      "Iteration 143, loss = 0.04155344\n",
      "Iteration 144, loss = 0.04147224\n",
      "Iteration 145, loss = 0.04142081\n",
      "Iteration 146, loss = 0.04134175\n",
      "Iteration 147, loss = 0.04130451\n",
      "Iteration 148, loss = 0.04117363\n",
      "Iteration 149, loss = 0.04115283\n",
      "Iteration 150, loss = 0.04109644\n",
      "Iteration 151, loss = 0.04097547\n",
      "Iteration 152, loss = 0.04094862\n",
      "Iteration 153, loss = 0.04085446\n",
      "Iteration 154, loss = 0.04078312\n",
      "Iteration 155, loss = 0.04077423\n",
      "Iteration 156, loss = 0.04068258\n",
      "Iteration 157, loss = 0.04063866\n",
      "Iteration 158, loss = 0.04058842\n",
      "Iteration 159, loss = 0.04053163\n",
      "Iteration 160, loss = 0.04049198\n",
      "Iteration 161, loss = 0.04045194\n",
      "Iteration 162, loss = 0.04042285\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27771618\n",
      "Iteration 2, loss = 1.86124373\n",
      "Iteration 3, loss = 1.43144628\n",
      "Iteration 4, loss = 1.03786325\n",
      "Iteration 5, loss = 0.73095993\n",
      "Iteration 6, loss = 0.53491625\n",
      "Iteration 7, loss = 0.40513494\n",
      "Iteration 8, loss = 0.32191717\n",
      "Iteration 9, loss = 0.26738170\n",
      "Iteration 10, loss = 0.22742013\n",
      "Iteration 11, loss = 0.19748394\n",
      "Iteration 12, loss = 0.17609224\n",
      "Iteration 13, loss = 0.15935690\n",
      "Iteration 14, loss = 0.14622030\n",
      "Iteration 15, loss = 0.13555198\n",
      "Iteration 16, loss = 0.12691813\n",
      "Iteration 17, loss = 0.11980354\n",
      "Iteration 18, loss = 0.11385896\n",
      "Iteration 19, loss = 0.10868217\n",
      "Iteration 20, loss = 0.10456503\n",
      "Iteration 21, loss = 0.10104332\n",
      "Iteration 22, loss = 0.09794322\n",
      "Iteration 23, loss = 0.09510220\n",
      "Iteration 24, loss = 0.09250903\n",
      "Iteration 25, loss = 0.09029194\n",
      "Iteration 26, loss = 0.08823540\n",
      "Iteration 27, loss = 0.08636732\n",
      "Iteration 28, loss = 0.08468142\n",
      "Iteration 29, loss = 0.08314082\n",
      "Iteration 30, loss = 0.08163357\n",
      "Iteration 31, loss = 0.08023865\n",
      "Iteration 32, loss = 0.07899811\n",
      "Iteration 33, loss = 0.07782470\n",
      "Iteration 34, loss = 0.07664858\n",
      "Iteration 35, loss = 0.07558570\n",
      "Iteration 36, loss = 0.07450353\n",
      "Iteration 37, loss = 0.07351639\n",
      "Iteration 38, loss = 0.07260217\n",
      "Iteration 39, loss = 0.07167814\n",
      "Iteration 40, loss = 0.07087394\n",
      "Iteration 41, loss = 0.06997698\n",
      "Iteration 42, loss = 0.06912744\n",
      "Iteration 43, loss = 0.06837773\n",
      "Iteration 44, loss = 0.06759875\n",
      "Iteration 45, loss = 0.06687480\n",
      "Iteration 46, loss = 0.06616557\n",
      "Iteration 47, loss = 0.06545030\n",
      "Iteration 48, loss = 0.06477685\n",
      "Iteration 49, loss = 0.06406050\n",
      "Iteration 50, loss = 0.06344555\n",
      "Iteration 51, loss = 0.06279271\n",
      "Iteration 52, loss = 0.06220003\n",
      "Iteration 53, loss = 0.06161465\n",
      "Iteration 54, loss = 0.06102555\n",
      "Iteration 55, loss = 0.06047813\n",
      "Iteration 56, loss = 0.05992748\n",
      "Iteration 57, loss = 0.05936885\n",
      "Iteration 58, loss = 0.05886215\n",
      "Iteration 59, loss = 0.05833793\n",
      "Iteration 60, loss = 0.05783505\n",
      "Iteration 61, loss = 0.05740225\n",
      "Iteration 62, loss = 0.05689126\n",
      "Iteration 63, loss = 0.05643469\n",
      "Iteration 64, loss = 0.05599442\n",
      "Iteration 65, loss = 0.05552734\n",
      "Iteration 66, loss = 0.05509876\n",
      "Iteration 67, loss = 0.05465962\n",
      "Iteration 68, loss = 0.05427286\n",
      "Iteration 69, loss = 0.05385263\n",
      "Iteration 70, loss = 0.05343975\n",
      "Iteration 71, loss = 0.05304156\n",
      "Iteration 72, loss = 0.05269888\n",
      "Iteration 73, loss = 0.05230183\n",
      "Iteration 74, loss = 0.05195456\n",
      "Iteration 75, loss = 0.05156181\n",
      "Iteration 76, loss = 0.05124838\n",
      "Iteration 77, loss = 0.05089401\n",
      "Iteration 78, loss = 0.05057311\n",
      "Iteration 79, loss = 0.05025092\n",
      "Iteration 80, loss = 0.04995719\n",
      "Iteration 81, loss = 0.04963656\n",
      "Iteration 82, loss = 0.04934151\n",
      "Iteration 83, loss = 0.04902224\n",
      "Iteration 84, loss = 0.04875417\n",
      "Iteration 85, loss = 0.04847573\n",
      "Iteration 86, loss = 0.04817296\n",
      "Iteration 87, loss = 0.04791797\n",
      "Iteration 88, loss = 0.04764238\n",
      "Iteration 89, loss = 0.04737655\n",
      "Iteration 90, loss = 0.04712657\n",
      "Iteration 91, loss = 0.04688772\n",
      "Iteration 92, loss = 0.04662582\n",
      "Iteration 93, loss = 0.04640277\n",
      "Iteration 94, loss = 0.04615608\n",
      "Iteration 95, loss = 0.04594256\n",
      "Iteration 96, loss = 0.04572848\n",
      "Iteration 97, loss = 0.04552516\n",
      "Iteration 98, loss = 0.04529907\n",
      "Iteration 99, loss = 0.04511812\n",
      "Iteration 100, loss = 0.04490173\n",
      "Iteration 101, loss = 0.04474949\n",
      "Iteration 102, loss = 0.04450854\n",
      "Iteration 103, loss = 0.04435860\n",
      "Iteration 104, loss = 0.04415429\n",
      "Iteration 105, loss = 0.04397023\n",
      "Iteration 106, loss = 0.04380256\n",
      "Iteration 107, loss = 0.04363771\n",
      "Iteration 108, loss = 0.04345755\n",
      "Iteration 109, loss = 0.04333154\n",
      "Iteration 110, loss = 0.04318523\n",
      "Iteration 111, loss = 0.04302230\n",
      "Iteration 112, loss = 0.04282072\n",
      "Iteration 113, loss = 0.04269542\n",
      "Iteration 114, loss = 0.04252582\n",
      "Iteration 115, loss = 0.04238176\n",
      "Iteration 116, loss = 0.04224543\n",
      "Iteration 117, loss = 0.04213280\n",
      "Iteration 118, loss = 0.04197402\n",
      "Iteration 119, loss = 0.04186081\n",
      "Iteration 120, loss = 0.04175032\n",
      "Iteration 121, loss = 0.04162778\n",
      "Iteration 122, loss = 0.04150622\n",
      "Iteration 123, loss = 0.04137993\n",
      "Iteration 124, loss = 0.04123208\n",
      "Iteration 125, loss = 0.04115180\n",
      "Iteration 126, loss = 0.04105607\n",
      "Iteration 127, loss = 0.04098571\n",
      "Iteration 128, loss = 0.04083550\n",
      "Iteration 129, loss = 0.04073668\n",
      "Iteration 130, loss = 0.04059987\n",
      "Iteration 131, loss = 0.04050895\n",
      "Iteration 132, loss = 0.04042234\n",
      "Iteration 133, loss = 0.04032959\n",
      "Iteration 134, loss = 0.04023435\n",
      "Iteration 135, loss = 0.04012323\n",
      "Iteration 136, loss = 0.04008632\n",
      "Iteration 137, loss = 0.03998307\n",
      "Iteration 138, loss = 0.03989363\n",
      "Iteration 139, loss = 0.03981514\n",
      "Iteration 140, loss = 0.03973772\n",
      "Iteration 141, loss = 0.03964130\n",
      "Iteration 142, loss = 0.03957574\n",
      "Iteration 143, loss = 0.03951308\n",
      "Iteration 144, loss = 0.03944383\n",
      "Iteration 145, loss = 0.03938878\n",
      "Iteration 146, loss = 0.03931652\n",
      "Iteration 147, loss = 0.03927566\n",
      "Iteration 148, loss = 0.03917774\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.42496932\n",
      "Iteration 2, loss = 2.39900496\n",
      "Iteration 3, loss = 2.36699935\n",
      "Iteration 4, loss = 2.33604710\n",
      "Iteration 5, loss = 2.30744460\n",
      "Iteration 6, loss = 2.28103751\n",
      "Iteration 7, loss = 2.25569366\n",
      "Iteration 8, loss = 2.23164407\n",
      "Iteration 9, loss = 2.20821049\n",
      "Iteration 10, loss = 2.18429350\n",
      "Iteration 11, loss = 2.16019487\n",
      "Iteration 12, loss = 2.13594364\n",
      "Iteration 13, loss = 2.11150639\n",
      "Iteration 14, loss = 2.08659589\n",
      "Iteration 15, loss = 2.06164657\n",
      "Iteration 16, loss = 2.03643887\n",
      "Iteration 17, loss = 2.01082170\n",
      "Iteration 18, loss = 1.98492324\n",
      "Iteration 19, loss = 1.95860245\n",
      "Iteration 20, loss = 1.93200483\n",
      "Iteration 21, loss = 1.90503272\n",
      "Iteration 22, loss = 1.87772447\n",
      "Iteration 23, loss = 1.85058670\n",
      "Iteration 24, loss = 1.82305311\n",
      "Iteration 25, loss = 1.79536838\n",
      "Iteration 26, loss = 1.76730421\n",
      "Iteration 27, loss = 1.73916606\n",
      "Iteration 28, loss = 1.71086292\n",
      "Iteration 29, loss = 1.68261888\n",
      "Iteration 30, loss = 1.65435064\n",
      "Iteration 31, loss = 1.62601777\n",
      "Iteration 32, loss = 1.59794570\n",
      "Iteration 33, loss = 1.56985028\n",
      "Iteration 34, loss = 1.54187928\n",
      "Iteration 35, loss = 1.51421289\n",
      "Iteration 36, loss = 1.48690572\n",
      "Iteration 37, loss = 1.45965299\n",
      "Iteration 38, loss = 1.43305124\n",
      "Iteration 39, loss = 1.40650459\n",
      "Iteration 40, loss = 1.38055529\n",
      "Iteration 41, loss = 1.35484840\n",
      "Iteration 42, loss = 1.32963807\n",
      "Iteration 43, loss = 1.30471716\n",
      "Iteration 44, loss = 1.28004334\n",
      "Iteration 45, loss = 1.25597931\n",
      "Iteration 46, loss = 1.23263864\n",
      "Iteration 47, loss = 1.20968033\n",
      "Iteration 48, loss = 1.18704201\n",
      "Iteration 49, loss = 1.16490431\n",
      "Iteration 50, loss = 1.14394556\n",
      "Iteration 51, loss = 1.12277622\n",
      "Iteration 52, loss = 1.10239601\n",
      "Iteration 53, loss = 1.08227570\n",
      "Iteration 54, loss = 1.06288603\n",
      "Iteration 55, loss = 1.04387985\n",
      "Iteration 56, loss = 1.02542515\n",
      "Iteration 57, loss = 1.00724557\n",
      "Iteration 58, loss = 0.98974922\n",
      "Iteration 59, loss = 0.97264057\n",
      "Iteration 60, loss = 0.95621495\n",
      "Iteration 61, loss = 0.93985801\n",
      "Iteration 62, loss = 0.92416791\n",
      "Iteration 63, loss = 0.90891031\n",
      "Iteration 64, loss = 0.89431114\n",
      "Iteration 65, loss = 0.87974001\n",
      "Iteration 66, loss = 0.86564322\n",
      "Iteration 67, loss = 0.85216210\n",
      "Iteration 68, loss = 0.83872180\n",
      "Iteration 69, loss = 0.82570974\n",
      "Iteration 70, loss = 0.81321423\n",
      "Iteration 71, loss = 0.80081642\n",
      "Iteration 72, loss = 0.78902282\n",
      "Iteration 73, loss = 0.77735396\n",
      "Iteration 74, loss = 0.76618369\n",
      "Iteration 75, loss = 0.75523739\n",
      "Iteration 76, loss = 0.74460583\n",
      "Iteration 77, loss = 0.73418982\n",
      "Iteration 78, loss = 0.72413302\n",
      "Iteration 79, loss = 0.71432911\n",
      "Iteration 80, loss = 0.70474456\n",
      "Iteration 81, loss = 0.69536762\n",
      "Iteration 82, loss = 0.68616751\n",
      "Iteration 83, loss = 0.67737345\n",
      "Iteration 84, loss = 0.66848294\n",
      "Iteration 85, loss = 0.65995113\n",
      "Iteration 86, loss = 0.65161687\n",
      "Iteration 87, loss = 0.64364067\n",
      "Iteration 88, loss = 0.63578073\n",
      "Iteration 89, loss = 0.62815734\n",
      "Iteration 90, loss = 0.62066887\n",
      "Iteration 91, loss = 0.61332877\n",
      "Iteration 92, loss = 0.60617374\n",
      "Iteration 93, loss = 0.59940747\n",
      "Iteration 94, loss = 0.59241480\n",
      "Iteration 95, loss = 0.58579156\n",
      "Iteration 96, loss = 0.57921065\n",
      "Iteration 97, loss = 0.57302515\n",
      "Iteration 98, loss = 0.56664102\n",
      "Iteration 99, loss = 0.56051494\n",
      "Iteration 100, loss = 0.55454962\n",
      "Iteration 101, loss = 0.54859456\n",
      "Iteration 102, loss = 0.54296683\n",
      "Iteration 103, loss = 0.53745880\n",
      "Iteration 104, loss = 0.53215971\n",
      "Iteration 105, loss = 0.52661714\n",
      "Iteration 106, loss = 0.52146549\n",
      "Iteration 107, loss = 0.51620829\n",
      "Iteration 108, loss = 0.51111270\n",
      "Iteration 109, loss = 0.50622648\n",
      "Iteration 110, loss = 0.50131265\n",
      "Iteration 111, loss = 0.49665967\n",
      "Iteration 112, loss = 0.49220657\n",
      "Iteration 113, loss = 0.48771190\n",
      "Iteration 114, loss = 0.48328554\n",
      "Iteration 115, loss = 0.47884655\n",
      "Iteration 116, loss = 0.47447889\n",
      "Iteration 117, loss = 0.47031349\n",
      "Iteration 118, loss = 0.46607260\n",
      "Iteration 119, loss = 0.46200907\n",
      "Iteration 120, loss = 0.45794369\n",
      "Iteration 121, loss = 0.45411488\n",
      "Iteration 122, loss = 0.45028297\n",
      "Iteration 123, loss = 0.44647665\n",
      "Iteration 124, loss = 0.44264584\n",
      "Iteration 125, loss = 0.43902113\n",
      "Iteration 126, loss = 0.43542089\n",
      "Iteration 127, loss = 0.43190345\n",
      "Iteration 128, loss = 0.42850615\n",
      "Iteration 129, loss = 0.42509294\n",
      "Iteration 130, loss = 0.42174866\n",
      "Iteration 131, loss = 0.41851525\n",
      "Iteration 132, loss = 0.41520602\n",
      "Iteration 133, loss = 0.41208283\n",
      "Iteration 134, loss = 0.40884395\n",
      "Iteration 135, loss = 0.40566910\n",
      "Iteration 136, loss = 0.40259271\n",
      "Iteration 137, loss = 0.39973077\n",
      "Iteration 138, loss = 0.39673008\n",
      "Iteration 139, loss = 0.39379669\n",
      "Iteration 140, loss = 0.39093260\n",
      "Iteration 141, loss = 0.38812318\n",
      "Iteration 142, loss = 0.38538749\n",
      "Iteration 143, loss = 0.38270435\n",
      "Iteration 144, loss = 0.38003337\n",
      "Iteration 145, loss = 0.37732685\n",
      "Iteration 146, loss = 0.37480047\n",
      "Iteration 147, loss = 0.37230384\n",
      "Iteration 148, loss = 0.36970350\n",
      "Iteration 149, loss = 0.36724592\n",
      "Iteration 150, loss = 0.36475900\n",
      "Iteration 151, loss = 0.36236765\n",
      "Iteration 152, loss = 0.35995338\n",
      "Iteration 153, loss = 0.35764303\n",
      "Iteration 154, loss = 0.35527678\n",
      "Iteration 155, loss = 0.35303120\n",
      "Iteration 156, loss = 0.35077475\n",
      "Iteration 157, loss = 0.34865266\n",
      "Iteration 158, loss = 0.34642208\n",
      "Iteration 159, loss = 0.34443330\n",
      "Iteration 160, loss = 0.34214331\n",
      "Iteration 161, loss = 0.34015308\n",
      "Iteration 162, loss = 0.33808941\n",
      "Iteration 163, loss = 0.33604619\n",
      "Iteration 164, loss = 0.33407473\n",
      "Iteration 165, loss = 0.33209105\n",
      "Iteration 166, loss = 0.33010500\n",
      "Iteration 167, loss = 0.32819458\n",
      "Iteration 168, loss = 0.32613865\n",
      "Iteration 169, loss = 0.32432905\n",
      "Iteration 170, loss = 0.32237237\n",
      "Iteration 171, loss = 0.32052277\n",
      "Iteration 172, loss = 0.31872661\n",
      "Iteration 173, loss = 0.31696599\n",
      "Iteration 174, loss = 0.31522550\n",
      "Iteration 175, loss = 0.31346957\n",
      "Iteration 176, loss = 0.31169339\n",
      "Iteration 177, loss = 0.31000382\n",
      "Iteration 178, loss = 0.30833659\n",
      "Iteration 179, loss = 0.30670959\n",
      "Iteration 180, loss = 0.30499703\n",
      "Iteration 181, loss = 0.30339321\n",
      "Iteration 182, loss = 0.30183325\n",
      "Iteration 183, loss = 0.30013543\n",
      "Iteration 184, loss = 0.29860952\n",
      "Iteration 185, loss = 0.29711989\n",
      "Iteration 186, loss = 0.29564527\n",
      "Iteration 187, loss = 0.29409299\n",
      "Iteration 188, loss = 0.29252941\n",
      "Iteration 189, loss = 0.29095953\n",
      "Iteration 190, loss = 0.28945168\n",
      "Iteration 191, loss = 0.28802309\n",
      "Iteration 192, loss = 0.28658034\n",
      "Iteration 193, loss = 0.28522804\n",
      "Iteration 194, loss = 0.28378732\n",
      "Iteration 195, loss = 0.28240694\n",
      "Iteration 196, loss = 0.28111463\n",
      "Iteration 197, loss = 0.27970093\n",
      "Iteration 198, loss = 0.27834042\n",
      "Iteration 199, loss = 0.27705468\n",
      "Iteration 200, loss = 0.27572924\n",
      "Iteration 201, loss = 0.27451588\n",
      "Iteration 202, loss = 0.27326123\n",
      "Iteration 203, loss = 0.27193409\n",
      "Iteration 204, loss = 0.27063525\n",
      "Iteration 205, loss = 0.26937739\n",
      "Iteration 206, loss = 0.26815329\n",
      "Iteration 207, loss = 0.26689740\n",
      "Iteration 208, loss = 0.26576223\n",
      "Iteration 209, loss = 0.26462837\n",
      "Iteration 210, loss = 0.26336038\n",
      "Iteration 211, loss = 0.26221796\n",
      "Iteration 212, loss = 0.26107400\n",
      "Iteration 213, loss = 0.25998719\n",
      "Iteration 214, loss = 0.25889955\n",
      "Iteration 215, loss = 0.25778994\n",
      "Iteration 216, loss = 0.25675449\n",
      "Iteration 217, loss = 0.25562355\n",
      "Iteration 218, loss = 0.25452280\n",
      "Iteration 219, loss = 0.25342522\n",
      "Iteration 220, loss = 0.25238317\n",
      "Iteration 221, loss = 0.25128441\n",
      "Iteration 222, loss = 0.25027278\n",
      "Iteration 223, loss = 0.24925511\n",
      "Iteration 224, loss = 0.24826298\n",
      "Iteration 225, loss = 0.24718918\n",
      "Iteration 226, loss = 0.24621015\n",
      "Iteration 227, loss = 0.24524758\n",
      "Iteration 228, loss = 0.24421974\n",
      "Iteration 229, loss = 0.24322098\n",
      "Iteration 230, loss = 0.24228004\n",
      "Iteration 231, loss = 0.24133717\n",
      "Iteration 232, loss = 0.24038134\n",
      "Iteration 233, loss = 0.23944271\n",
      "Iteration 234, loss = 0.23857393\n",
      "Iteration 235, loss = 0.23763369\n",
      "Iteration 236, loss = 0.23679222\n",
      "Iteration 237, loss = 0.23586629\n",
      "Iteration 238, loss = 0.23495775\n",
      "Iteration 239, loss = 0.23411347\n",
      "Iteration 240, loss = 0.23320432\n",
      "Iteration 241, loss = 0.23231614\n",
      "Iteration 242, loss = 0.23145823\n",
      "Iteration 243, loss = 0.23059704\n",
      "Iteration 244, loss = 0.22969193\n",
      "Iteration 245, loss = 0.22886768\n",
      "Iteration 246, loss = 0.22807729\n",
      "Iteration 247, loss = 0.22719057\n",
      "Iteration 248, loss = 0.22634447\n",
      "Iteration 249, loss = 0.22550410\n",
      "Iteration 250, loss = 0.22473578\n",
      "Iteration 251, loss = 0.22395695\n",
      "Iteration 252, loss = 0.22315000\n",
      "Iteration 253, loss = 0.22235444\n",
      "Iteration 254, loss = 0.22157062\n",
      "Iteration 255, loss = 0.22078732\n",
      "Iteration 256, loss = 0.22007683\n",
      "Iteration 257, loss = 0.21925575\n",
      "Iteration 258, loss = 0.21848071\n",
      "Iteration 259, loss = 0.21780491\n",
      "Iteration 260, loss = 0.21706866\n",
      "Iteration 261, loss = 0.21632804\n",
      "Iteration 262, loss = 0.21558277\n",
      "Iteration 263, loss = 0.21484263\n",
      "Iteration 264, loss = 0.21413281\n",
      "Iteration 265, loss = 0.21340921\n",
      "Iteration 266, loss = 0.21273364\n",
      "Iteration 267, loss = 0.21207834\n",
      "Iteration 268, loss = 0.21142643\n",
      "Iteration 269, loss = 0.21070379\n",
      "Iteration 270, loss = 0.21005871\n",
      "Iteration 271, loss = 0.20932640\n",
      "Iteration 272, loss = 0.20865086\n",
      "Iteration 273, loss = 0.20798930\n",
      "Iteration 274, loss = 0.20731134\n",
      "Iteration 275, loss = 0.20661688\n",
      "Iteration 276, loss = 0.20598413\n",
      "Iteration 277, loss = 0.20535701\n",
      "Iteration 278, loss = 0.20467033\n",
      "Iteration 279, loss = 0.20405233\n",
      "Iteration 280, loss = 0.20339736\n",
      "Iteration 281, loss = 0.20273274\n",
      "Iteration 282, loss = 0.20215723\n",
      "Iteration 283, loss = 0.20152863\n",
      "Iteration 284, loss = 0.20091308\n",
      "Iteration 285, loss = 0.20028831\n",
      "Iteration 286, loss = 0.19972927\n",
      "Iteration 287, loss = 0.19913618\n",
      "Iteration 288, loss = 0.19851571\n",
      "Iteration 289, loss = 0.19796916\n",
      "Iteration 290, loss = 0.19735930\n",
      "Iteration 291, loss = 0.19681443\n",
      "Iteration 292, loss = 0.19621631\n",
      "Iteration 293, loss = 0.19562259\n",
      "Iteration 294, loss = 0.19502658\n",
      "Iteration 295, loss = 0.19448222\n",
      "Iteration 296, loss = 0.19389300\n",
      "Iteration 297, loss = 0.19332957\n",
      "Iteration 298, loss = 0.19274330\n",
      "Iteration 299, loss = 0.19222075\n",
      "Iteration 300, loss = 0.19170821\n",
      "Iteration 301, loss = 0.19113727\n",
      "Iteration 302, loss = 0.19059990\n",
      "Iteration 303, loss = 0.19009735\n",
      "Iteration 304, loss = 0.18957217\n",
      "Iteration 305, loss = 0.18901639\n",
      "Iteration 306, loss = 0.18848203\n",
      "Iteration 307, loss = 0.18797479\n",
      "Iteration 308, loss = 0.18747904\n",
      "Iteration 309, loss = 0.18696221\n",
      "Iteration 310, loss = 0.18647155\n",
      "Iteration 311, loss = 0.18595648\n",
      "Iteration 312, loss = 0.18547205\n",
      "Iteration 313, loss = 0.18497520\n",
      "Iteration 314, loss = 0.18449510\n",
      "Iteration 315, loss = 0.18401037\n",
      "Iteration 316, loss = 0.18352458\n",
      "Iteration 317, loss = 0.18301608\n",
      "Iteration 318, loss = 0.18255683\n",
      "Iteration 319, loss = 0.18207855\n",
      "Iteration 320, loss = 0.18164189\n",
      "Iteration 321, loss = 0.18112561\n",
      "Iteration 322, loss = 0.18068806\n",
      "Iteration 323, loss = 0.18024553\n",
      "Iteration 324, loss = 0.17979564\n",
      "Iteration 325, loss = 0.17929970\n",
      "Iteration 326, loss = 0.17887505\n",
      "Iteration 327, loss = 0.17845689\n",
      "Iteration 328, loss = 0.17800799\n",
      "Iteration 329, loss = 0.17755962\n",
      "Iteration 330, loss = 0.17713180\n",
      "Iteration 331, loss = 0.17669914\n",
      "Iteration 332, loss = 0.17626206\n",
      "Iteration 333, loss = 0.17583197\n",
      "Iteration 334, loss = 0.17540848\n",
      "Iteration 335, loss = 0.17501757\n",
      "Iteration 336, loss = 0.17456529\n",
      "Iteration 337, loss = 0.17413810\n",
      "Iteration 338, loss = 0.17373791\n",
      "Iteration 339, loss = 0.17331712\n",
      "Iteration 340, loss = 0.17290213\n",
      "Iteration 341, loss = 0.17246271\n",
      "Iteration 342, loss = 0.17206031\n",
      "Iteration 343, loss = 0.17162849\n",
      "Iteration 344, loss = 0.17126778\n",
      "Iteration 345, loss = 0.17086536\n",
      "Iteration 346, loss = 0.17045768\n",
      "Iteration 347, loss = 0.17006007\n",
      "Iteration 348, loss = 0.16968574\n",
      "Iteration 349, loss = 0.16929536\n",
      "Iteration 350, loss = 0.16891775\n",
      "Iteration 351, loss = 0.16852677\n",
      "Iteration 352, loss = 0.16812564\n",
      "Iteration 353, loss = 0.16772263\n",
      "Iteration 354, loss = 0.16732004\n",
      "Iteration 355, loss = 0.16693427\n",
      "Iteration 356, loss = 0.16656896\n",
      "Iteration 357, loss = 0.16621072\n",
      "Iteration 358, loss = 0.16583379\n",
      "Iteration 359, loss = 0.16546716\n",
      "Iteration 360, loss = 0.16509426\n",
      "Iteration 361, loss = 0.16473061\n",
      "Iteration 362, loss = 0.16439182\n",
      "Iteration 363, loss = 0.16404725\n",
      "Iteration 364, loss = 0.16366714\n",
      "Iteration 365, loss = 0.16330097\n",
      "Iteration 366, loss = 0.16296822\n",
      "Iteration 367, loss = 0.16264831\n",
      "Iteration 368, loss = 0.16232229\n",
      "Iteration 369, loss = 0.16200796\n",
      "Iteration 370, loss = 0.16160067\n",
      "Iteration 371, loss = 0.16128650\n",
      "Iteration 372, loss = 0.16095022\n",
      "Iteration 373, loss = 0.16059476\n",
      "Iteration 374, loss = 0.16026360\n",
      "Iteration 375, loss = 0.15993985\n",
      "Iteration 376, loss = 0.15961230\n",
      "Iteration 377, loss = 0.15928745\n",
      "Iteration 378, loss = 0.15897492\n",
      "Iteration 379, loss = 0.15865365\n",
      "Iteration 380, loss = 0.15833408\n",
      "Iteration 381, loss = 0.15802066\n",
      "Iteration 382, loss = 0.15769585\n",
      "Iteration 383, loss = 0.15737666\n",
      "Iteration 384, loss = 0.15704588\n",
      "Iteration 385, loss = 0.15675780\n",
      "Iteration 386, loss = 0.15645634\n",
      "Iteration 387, loss = 0.15615307\n",
      "Iteration 388, loss = 0.15586520\n",
      "Iteration 389, loss = 0.15553313\n",
      "Iteration 390, loss = 0.15519789\n",
      "Iteration 391, loss = 0.15492822\n",
      "Iteration 392, loss = 0.15462456\n",
      "Iteration 393, loss = 0.15433442\n",
      "Iteration 394, loss = 0.15404152\n",
      "Iteration 395, loss = 0.15375028\n",
      "Iteration 396, loss = 0.15348877\n",
      "Iteration 397, loss = 0.15315760\n",
      "Iteration 398, loss = 0.15286243\n",
      "Iteration 399, loss = 0.15259541\n",
      "Iteration 400, loss = 0.15228402\n",
      "Iteration 401, loss = 0.15200582\n",
      "Iteration 402, loss = 0.15175029\n",
      "Iteration 403, loss = 0.15147124\n",
      "Iteration 404, loss = 0.15121564\n",
      "Iteration 405, loss = 0.15091827\n",
      "Iteration 406, loss = 0.15061818\n",
      "Iteration 407, loss = 0.15038186\n",
      "Iteration 408, loss = 0.15007003\n",
      "Iteration 409, loss = 0.14981272\n",
      "Iteration 410, loss = 0.14954886\n",
      "Iteration 411, loss = 0.14927955\n",
      "Iteration 412, loss = 0.14901335\n",
      "Iteration 413, loss = 0.14872427\n",
      "Iteration 414, loss = 0.14847926\n",
      "Iteration 415, loss = 0.14820523\n",
      "Iteration 416, loss = 0.14796898\n",
      "Iteration 417, loss = 0.14770084\n",
      "Iteration 418, loss = 0.14744520\n",
      "Iteration 419, loss = 0.14717989\n",
      "Iteration 420, loss = 0.14694366\n",
      "Iteration 421, loss = 0.14669280\n",
      "Iteration 422, loss = 0.14643012\n",
      "Iteration 423, loss = 0.14618829\n",
      "Iteration 424, loss = 0.14596795\n",
      "Iteration 425, loss = 0.14569608\n",
      "Iteration 426, loss = 0.14544826\n",
      "Iteration 427, loss = 0.14516220\n",
      "Iteration 428, loss = 0.14495161\n",
      "Iteration 429, loss = 0.14466561\n",
      "Iteration 430, loss = 0.14441896\n",
      "Iteration 431, loss = 0.14418661\n",
      "Iteration 432, loss = 0.14396558\n",
      "Iteration 433, loss = 0.14369097\n",
      "Iteration 434, loss = 0.14345950\n",
      "Iteration 435, loss = 0.14322282\n",
      "Iteration 436, loss = 0.14298235\n",
      "Iteration 437, loss = 0.14276021\n",
      "Iteration 438, loss = 0.14252833\n",
      "Iteration 439, loss = 0.14229931\n",
      "Iteration 440, loss = 0.14208462\n",
      "Iteration 441, loss = 0.14185067\n",
      "Iteration 442, loss = 0.14158206\n",
      "Iteration 443, loss = 0.14136114\n",
      "Iteration 444, loss = 0.14114024\n",
      "Iteration 445, loss = 0.14091078\n",
      "Iteration 446, loss = 0.14069062\n",
      "Iteration 447, loss = 0.14047508\n",
      "Iteration 448, loss = 0.14023208\n",
      "Iteration 449, loss = 0.14002999\n",
      "Iteration 450, loss = 0.13979822\n",
      "Iteration 451, loss = 0.13958243\n",
      "Iteration 452, loss = 0.13936930\n",
      "Iteration 453, loss = 0.13915781\n",
      "Iteration 454, loss = 0.13896130\n",
      "Iteration 455, loss = 0.13873981\n",
      "Iteration 456, loss = 0.13852245\n",
      "Iteration 457, loss = 0.13831813\n",
      "Iteration 458, loss = 0.13811999\n",
      "Iteration 459, loss = 0.13790536\n",
      "Iteration 460, loss = 0.13772212\n",
      "Iteration 461, loss = 0.13750895\n",
      "Iteration 462, loss = 0.13731577\n",
      "Iteration 463, loss = 0.13710023\n",
      "Iteration 464, loss = 0.13688719\n",
      "Iteration 465, loss = 0.13667464\n",
      "Iteration 466, loss = 0.13648206\n",
      "Iteration 467, loss = 0.13631873\n",
      "Iteration 468, loss = 0.13612077\n",
      "Iteration 469, loss = 0.13591162\n",
      "Iteration 470, loss = 0.13569209\n",
      "Iteration 471, loss = 0.13550474\n",
      "Iteration 472, loss = 0.13531426\n",
      "Iteration 473, loss = 0.13509951\n",
      "Iteration 474, loss = 0.13489739\n",
      "Iteration 475, loss = 0.13472230\n",
      "Iteration 476, loss = 0.13452164\n",
      "Iteration 477, loss = 0.13434735\n",
      "Iteration 478, loss = 0.13414774\n",
      "Iteration 479, loss = 0.13397314\n",
      "Iteration 480, loss = 0.13374995\n",
      "Iteration 481, loss = 0.13358202\n",
      "Iteration 482, loss = 0.13342276\n",
      "Iteration 483, loss = 0.13323327\n",
      "Iteration 484, loss = 0.13303220\n",
      "Iteration 485, loss = 0.13283265\n",
      "Iteration 486, loss = 0.13264919\n",
      "Iteration 487, loss = 0.13247805\n",
      "Iteration 488, loss = 0.13228017\n",
      "Iteration 489, loss = 0.13209480\n",
      "Iteration 490, loss = 0.13193244\n",
      "Iteration 491, loss = 0.13173328\n",
      "Iteration 492, loss = 0.13157870\n",
      "Iteration 493, loss = 0.13139735\n",
      "Iteration 494, loss = 0.13124033\n",
      "Iteration 495, loss = 0.13105207\n",
      "Iteration 496, loss = 0.13087590\n",
      "Iteration 497, loss = 0.13071264\n",
      "Iteration 498, loss = 0.13053321\n",
      "Iteration 499, loss = 0.13036210\n",
      "Iteration 500, loss = 0.13020562\n",
      "Iteration 501, loss = 0.13001656\n",
      "Iteration 502, loss = 0.12985806\n",
      "Iteration 503, loss = 0.12970312\n",
      "Iteration 504, loss = 0.12953239\n",
      "Iteration 505, loss = 0.12936679\n",
      "Iteration 506, loss = 0.12921568\n",
      "Iteration 507, loss = 0.12905211\n",
      "Iteration 508, loss = 0.12886763\n",
      "Iteration 509, loss = 0.12870646\n",
      "Iteration 510, loss = 0.12854913\n",
      "Iteration 511, loss = 0.12839420\n",
      "Iteration 512, loss = 0.12824780\n",
      "Iteration 513, loss = 0.12807522\n",
      "Iteration 514, loss = 0.12791460\n",
      "Iteration 515, loss = 0.12774942\n",
      "Iteration 516, loss = 0.12759980\n",
      "Iteration 517, loss = 0.12743560\n",
      "Iteration 518, loss = 0.12729095\n",
      "Iteration 519, loss = 0.12715383\n",
      "Iteration 520, loss = 0.12699266\n",
      "Iteration 521, loss = 0.12684978\n",
      "Iteration 522, loss = 0.12669962\n",
      "Iteration 523, loss = 0.12655545\n",
      "Iteration 524, loss = 0.12639218\n",
      "Iteration 525, loss = 0.12623619\n",
      "Iteration 526, loss = 0.12609747\n",
      "Iteration 527, loss = 0.12594383\n",
      "Iteration 528, loss = 0.12579303\n",
      "Iteration 529, loss = 0.12566149\n",
      "Iteration 530, loss = 0.12549775\n",
      "Iteration 531, loss = 0.12535127\n",
      "Iteration 532, loss = 0.12521191\n",
      "Iteration 533, loss = 0.12505232\n",
      "Iteration 534, loss = 0.12491127\n",
      "Iteration 535, loss = 0.12476283\n",
      "Iteration 536, loss = 0.12462884\n",
      "Iteration 537, loss = 0.12449548\n",
      "Iteration 538, loss = 0.12434463\n",
      "Iteration 539, loss = 0.12419521\n",
      "Iteration 540, loss = 0.12405941\n",
      "Iteration 541, loss = 0.12390570\n",
      "Iteration 542, loss = 0.12376286\n",
      "Iteration 543, loss = 0.12362851\n",
      "Iteration 544, loss = 0.12350429\n",
      "Iteration 545, loss = 0.12337983\n",
      "Iteration 546, loss = 0.12323561\n",
      "Iteration 547, loss = 0.12309038\n",
      "Iteration 548, loss = 0.12296988\n",
      "Iteration 549, loss = 0.12284367\n",
      "Iteration 550, loss = 0.12270601\n",
      "Iteration 551, loss = 0.12259262\n",
      "Iteration 552, loss = 0.12245177\n",
      "Iteration 553, loss = 0.12230731\n",
      "Iteration 554, loss = 0.12216773\n",
      "Iteration 555, loss = 0.12202941\n",
      "Iteration 556, loss = 0.12189340\n",
      "Iteration 557, loss = 0.12175920\n",
      "Iteration 558, loss = 0.12163643\n",
      "Iteration 559, loss = 0.12149905\n",
      "Iteration 560, loss = 0.12137899\n",
      "Iteration 561, loss = 0.12125463\n",
      "Iteration 562, loss = 0.12110788\n",
      "Iteration 563, loss = 0.12098703\n",
      "Iteration 564, loss = 0.12086340\n",
      "Iteration 565, loss = 0.12073819\n",
      "Iteration 566, loss = 0.12061456\n",
      "Iteration 567, loss = 0.12048932\n",
      "Iteration 568, loss = 0.12035822\n",
      "Iteration 569, loss = 0.12023851\n",
      "Iteration 570, loss = 0.12013220\n",
      "Iteration 571, loss = 0.12002099\n",
      "Iteration 572, loss = 0.11989733\n",
      "Iteration 573, loss = 0.11976965\n",
      "Iteration 574, loss = 0.11964273\n",
      "Iteration 575, loss = 0.11953661\n",
      "Iteration 576, loss = 0.11939540\n",
      "Iteration 577, loss = 0.11927051\n",
      "Iteration 578, loss = 0.11916781\n",
      "Iteration 579, loss = 0.11903520\n",
      "Iteration 580, loss = 0.11891499\n",
      "Iteration 581, loss = 0.11879205\n",
      "Iteration 582, loss = 0.11868429\n",
      "Iteration 583, loss = 0.11856018\n",
      "Iteration 584, loss = 0.11845939\n",
      "Iteration 585, loss = 0.11832812\n",
      "Iteration 586, loss = 0.11820816\n",
      "Iteration 587, loss = 0.11807698\n",
      "Iteration 588, loss = 0.11795645\n",
      "Iteration 589, loss = 0.11784272\n",
      "Iteration 590, loss = 0.11773049\n",
      "Iteration 591, loss = 0.11761714\n",
      "Iteration 592, loss = 0.11751067\n",
      "Iteration 593, loss = 0.11739948\n",
      "Iteration 594, loss = 0.11728369\n",
      "Iteration 595, loss = 0.11717500\n",
      "Iteration 596, loss = 0.11706680\n",
      "Iteration 597, loss = 0.11695017\n",
      "Iteration 598, loss = 0.11684854\n",
      "Iteration 599, loss = 0.11675293\n",
      "Iteration 600, loss = 0.11663616\n",
      "Iteration 601, loss = 0.11653390\n",
      "Iteration 602, loss = 0.11642095\n",
      "Iteration 603, loss = 0.11632645\n",
      "Iteration 604, loss = 0.11621352\n",
      "Iteration 605, loss = 0.11610435\n",
      "Iteration 606, loss = 0.11599344\n",
      "Iteration 607, loss = 0.11587901\n",
      "Iteration 608, loss = 0.11578197\n",
      "Iteration 609, loss = 0.11566651\n",
      "Iteration 610, loss = 0.11555752\n",
      "Iteration 611, loss = 0.11545807\n",
      "Iteration 612, loss = 0.11535685\n",
      "Iteration 613, loss = 0.11525809\n",
      "Iteration 614, loss = 0.11515573\n",
      "Iteration 615, loss = 0.11505507\n",
      "Iteration 616, loss = 0.11495869\n",
      "Iteration 617, loss = 0.11485222\n",
      "Iteration 618, loss = 0.11475053\n",
      "Iteration 619, loss = 0.11465136\n",
      "Iteration 620, loss = 0.11453569\n",
      "Iteration 621, loss = 0.11443787\n",
      "Iteration 622, loss = 0.11433921\n",
      "Iteration 623, loss = 0.11425001\n",
      "Iteration 624, loss = 0.11415250\n",
      "Iteration 625, loss = 0.11405717\n",
      "Iteration 626, loss = 0.11395468\n",
      "Iteration 627, loss = 0.11386325\n",
      "Iteration 628, loss = 0.11376401\n",
      "Iteration 629, loss = 0.11366693\n",
      "Iteration 630, loss = 0.11357078\n",
      "Iteration 631, loss = 0.11347371\n",
      "Iteration 632, loss = 0.11338832\n",
      "Iteration 633, loss = 0.11329118\n",
      "Iteration 634, loss = 0.11320209\n",
      "Iteration 635, loss = 0.11310964\n",
      "Iteration 636, loss = 0.11301669\n",
      "Iteration 637, loss = 0.11292734\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.43053553\n",
      "Iteration 2, loss = 2.40377799\n",
      "Iteration 3, loss = 2.37084333\n",
      "Iteration 4, loss = 2.33826705\n",
      "Iteration 5, loss = 2.30856704\n",
      "Iteration 6, loss = 2.28158815\n",
      "Iteration 7, loss = 2.25549183\n",
      "Iteration 8, loss = 2.23049459\n",
      "Iteration 9, loss = 2.20625704\n",
      "Iteration 10, loss = 2.18155344\n",
      "Iteration 11, loss = 2.15670247\n",
      "Iteration 12, loss = 2.13167691\n",
      "Iteration 13, loss = 2.10644039\n",
      "Iteration 14, loss = 2.08065277\n",
      "Iteration 15, loss = 2.05495437\n",
      "Iteration 16, loss = 2.02889871\n",
      "Iteration 17, loss = 2.00254437\n",
      "Iteration 18, loss = 1.97571770\n",
      "Iteration 19, loss = 1.94848422\n",
      "Iteration 20, loss = 1.92094859\n",
      "Iteration 21, loss = 1.89299065\n",
      "Iteration 22, loss = 1.86454133\n",
      "Iteration 23, loss = 1.83622643\n",
      "Iteration 24, loss = 1.80769921\n",
      "Iteration 25, loss = 1.77884881\n",
      "Iteration 26, loss = 1.74979629\n",
      "Iteration 27, loss = 1.72085123\n",
      "Iteration 28, loss = 1.69149963\n",
      "Iteration 29, loss = 1.66222116\n",
      "Iteration 30, loss = 1.63296579\n",
      "Iteration 31, loss = 1.60370646\n",
      "Iteration 32, loss = 1.57487645\n",
      "Iteration 33, loss = 1.54593037\n",
      "Iteration 34, loss = 1.51710732\n",
      "Iteration 35, loss = 1.48857377\n",
      "Iteration 36, loss = 1.46058644\n",
      "Iteration 37, loss = 1.43261505\n",
      "Iteration 38, loss = 1.40533861\n",
      "Iteration 39, loss = 1.37804498\n",
      "Iteration 40, loss = 1.35159128\n",
      "Iteration 41, loss = 1.32562472\n",
      "Iteration 42, loss = 1.30006462\n",
      "Iteration 43, loss = 1.27466520\n",
      "Iteration 44, loss = 1.24990924\n",
      "Iteration 45, loss = 1.22564827\n",
      "Iteration 46, loss = 1.20180269\n",
      "Iteration 47, loss = 1.17865676\n",
      "Iteration 48, loss = 1.15599374\n",
      "Iteration 49, loss = 1.13391992\n",
      "Iteration 50, loss = 1.11242095\n",
      "Iteration 51, loss = 1.09124935\n",
      "Iteration 52, loss = 1.07081226\n",
      "Iteration 53, loss = 1.05093473\n",
      "Iteration 54, loss = 1.03128769\n",
      "Iteration 55, loss = 1.01239673\n",
      "Iteration 56, loss = 0.99382096\n",
      "Iteration 57, loss = 0.97578614\n",
      "Iteration 58, loss = 0.95835028\n",
      "Iteration 59, loss = 0.94134605\n",
      "Iteration 60, loss = 0.92498551\n",
      "Iteration 61, loss = 0.90894963\n",
      "Iteration 62, loss = 0.89334544\n",
      "Iteration 63, loss = 0.87840852\n",
      "Iteration 64, loss = 0.86393452\n",
      "Iteration 65, loss = 0.84961836\n",
      "Iteration 66, loss = 0.83559811\n",
      "Iteration 67, loss = 0.82232228\n",
      "Iteration 68, loss = 0.80927041\n",
      "Iteration 69, loss = 0.79658070\n",
      "Iteration 70, loss = 0.78434322\n",
      "Iteration 71, loss = 0.77233723\n",
      "Iteration 72, loss = 0.76087677\n",
      "Iteration 73, loss = 0.74960605\n",
      "Iteration 74, loss = 0.73865447\n",
      "Iteration 75, loss = 0.72786345\n",
      "Iteration 76, loss = 0.71761245\n",
      "Iteration 77, loss = 0.70731226\n",
      "Iteration 78, loss = 0.69760673\n",
      "Iteration 79, loss = 0.68783543\n",
      "Iteration 80, loss = 0.67864609\n",
      "Iteration 81, loss = 0.66946509\n",
      "Iteration 82, loss = 0.66059029\n",
      "Iteration 83, loss = 0.65212569\n",
      "Iteration 84, loss = 0.64339809\n",
      "Iteration 85, loss = 0.63518166\n",
      "Iteration 86, loss = 0.62712215\n",
      "Iteration 87, loss = 0.61934181\n",
      "Iteration 88, loss = 0.61185905\n",
      "Iteration 89, loss = 0.60450777\n",
      "Iteration 90, loss = 0.59745369\n",
      "Iteration 91, loss = 0.59037720\n",
      "Iteration 92, loss = 0.58340831\n",
      "Iteration 93, loss = 0.57687622\n",
      "Iteration 94, loss = 0.57031530\n",
      "Iteration 95, loss = 0.56389722\n",
      "Iteration 96, loss = 0.55759183\n",
      "Iteration 97, loss = 0.55170233\n",
      "Iteration 98, loss = 0.54564094\n",
      "Iteration 99, loss = 0.53972682\n",
      "Iteration 100, loss = 0.53405874\n",
      "Iteration 101, loss = 0.52848311\n",
      "Iteration 102, loss = 0.52311822\n",
      "Iteration 103, loss = 0.51780825\n",
      "Iteration 104, loss = 0.51279799\n",
      "Iteration 105, loss = 0.50765545\n",
      "Iteration 106, loss = 0.50274130\n",
      "Iteration 107, loss = 0.49771038\n",
      "Iteration 108, loss = 0.49290844\n",
      "Iteration 109, loss = 0.48830098\n",
      "Iteration 110, loss = 0.48363828\n",
      "Iteration 111, loss = 0.47923372\n",
      "Iteration 112, loss = 0.47481527\n",
      "Iteration 113, loss = 0.47062240\n",
      "Iteration 114, loss = 0.46640749\n",
      "Iteration 115, loss = 0.46216853\n",
      "Iteration 116, loss = 0.45806852\n",
      "Iteration 117, loss = 0.45406286\n",
      "Iteration 118, loss = 0.45016430\n",
      "Iteration 119, loss = 0.44627570\n",
      "Iteration 120, loss = 0.44240099\n",
      "Iteration 121, loss = 0.43883164\n",
      "Iteration 122, loss = 0.43510303\n",
      "Iteration 123, loss = 0.43157006\n",
      "Iteration 124, loss = 0.42806271\n",
      "Iteration 125, loss = 0.42469575\n",
      "Iteration 126, loss = 0.42128658\n",
      "Iteration 127, loss = 0.41791904\n",
      "Iteration 128, loss = 0.41460439\n",
      "Iteration 129, loss = 0.41139856\n",
      "Iteration 130, loss = 0.40835627\n",
      "Iteration 131, loss = 0.40532299\n",
      "Iteration 132, loss = 0.40217943\n",
      "Iteration 133, loss = 0.39928564\n",
      "Iteration 134, loss = 0.39616241\n",
      "Iteration 135, loss = 0.39328504\n",
      "Iteration 136, loss = 0.39041197\n",
      "Iteration 137, loss = 0.38768657\n",
      "Iteration 138, loss = 0.38504741\n",
      "Iteration 139, loss = 0.38229629\n",
      "Iteration 140, loss = 0.37952959\n",
      "Iteration 141, loss = 0.37679910\n",
      "Iteration 142, loss = 0.37427410\n",
      "Iteration 143, loss = 0.37173993\n",
      "Iteration 144, loss = 0.36924060\n",
      "Iteration 145, loss = 0.36673314\n",
      "Iteration 146, loss = 0.36434985\n",
      "Iteration 147, loss = 0.36203508\n",
      "Iteration 148, loss = 0.35955952\n",
      "Iteration 149, loss = 0.35722357\n",
      "Iteration 150, loss = 0.35487913\n",
      "Iteration 151, loss = 0.35272554\n",
      "Iteration 152, loss = 0.35039458\n",
      "Iteration 153, loss = 0.34816096\n",
      "Iteration 154, loss = 0.34589434\n",
      "Iteration 155, loss = 0.34383281\n",
      "Iteration 156, loss = 0.34177412\n",
      "Iteration 157, loss = 0.33973442\n",
      "Iteration 158, loss = 0.33762942\n",
      "Iteration 159, loss = 0.33570658\n",
      "Iteration 160, loss = 0.33359549\n",
      "Iteration 161, loss = 0.33169145\n",
      "Iteration 162, loss = 0.32979282\n",
      "Iteration 163, loss = 0.32786252\n",
      "Iteration 164, loss = 0.32601913\n",
      "Iteration 165, loss = 0.32417681\n",
      "Iteration 166, loss = 0.32228995\n",
      "Iteration 167, loss = 0.32046108\n",
      "Iteration 168, loss = 0.31858800\n",
      "Iteration 169, loss = 0.31687099\n",
      "Iteration 170, loss = 0.31502487\n",
      "Iteration 171, loss = 0.31334370\n",
      "Iteration 172, loss = 0.31167692\n",
      "Iteration 173, loss = 0.31007238\n",
      "Iteration 174, loss = 0.30833106\n",
      "Iteration 175, loss = 0.30670666\n",
      "Iteration 176, loss = 0.30509037\n",
      "Iteration 177, loss = 0.30347684\n",
      "Iteration 178, loss = 0.30197587\n",
      "Iteration 179, loss = 0.30042821\n",
      "Iteration 180, loss = 0.29885745\n",
      "Iteration 181, loss = 0.29737716\n",
      "Iteration 182, loss = 0.29591802\n",
      "Iteration 183, loss = 0.29434153\n",
      "Iteration 184, loss = 0.29293107\n",
      "Iteration 185, loss = 0.29148541\n",
      "Iteration 186, loss = 0.29011899\n",
      "Iteration 187, loss = 0.28866417\n",
      "Iteration 188, loss = 0.28721381\n",
      "Iteration 189, loss = 0.28576268\n",
      "Iteration 190, loss = 0.28439336\n",
      "Iteration 191, loss = 0.28304515\n",
      "Iteration 192, loss = 0.28172740\n",
      "Iteration 193, loss = 0.28041681\n",
      "Iteration 194, loss = 0.27910807\n",
      "Iteration 195, loss = 0.27778650\n",
      "Iteration 196, loss = 0.27654684\n",
      "Iteration 197, loss = 0.27525059\n",
      "Iteration 198, loss = 0.27403513\n",
      "Iteration 199, loss = 0.27273267\n",
      "Iteration 200, loss = 0.27156370\n",
      "Iteration 201, loss = 0.27035989\n",
      "Iteration 202, loss = 0.26919970\n",
      "Iteration 203, loss = 0.26793997\n",
      "Iteration 204, loss = 0.26683591\n",
      "Iteration 205, loss = 0.26560602\n",
      "Iteration 206, loss = 0.26448173\n",
      "Iteration 207, loss = 0.26332685\n",
      "Iteration 208, loss = 0.26228434\n",
      "Iteration 209, loss = 0.26118808\n",
      "Iteration 210, loss = 0.26001524\n",
      "Iteration 211, loss = 0.25896958\n",
      "Iteration 212, loss = 0.25791001\n",
      "Iteration 213, loss = 0.25686685\n",
      "Iteration 214, loss = 0.25588754\n",
      "Iteration 215, loss = 0.25478161\n",
      "Iteration 216, loss = 0.25377012\n",
      "Iteration 217, loss = 0.25276867\n",
      "Iteration 218, loss = 0.25171547\n",
      "Iteration 219, loss = 0.25069272\n",
      "Iteration 220, loss = 0.24973166\n",
      "Iteration 221, loss = 0.24867676\n",
      "Iteration 222, loss = 0.24775705\n",
      "Iteration 223, loss = 0.24683279\n",
      "Iteration 224, loss = 0.24589003\n",
      "Iteration 225, loss = 0.24491463\n",
      "Iteration 226, loss = 0.24402276\n",
      "Iteration 227, loss = 0.24300068\n",
      "Iteration 228, loss = 0.24210907\n",
      "Iteration 229, loss = 0.24116562\n",
      "Iteration 230, loss = 0.24025185\n",
      "Iteration 231, loss = 0.23939846\n",
      "Iteration 232, loss = 0.23844614\n",
      "Iteration 233, loss = 0.23760312\n",
      "Iteration 234, loss = 0.23683276\n",
      "Iteration 235, loss = 0.23584623\n",
      "Iteration 236, loss = 0.23507739\n",
      "Iteration 237, loss = 0.23417378\n",
      "Iteration 238, loss = 0.23335396\n",
      "Iteration 239, loss = 0.23250947\n",
      "Iteration 240, loss = 0.23166170\n",
      "Iteration 241, loss = 0.23084185\n",
      "Iteration 242, loss = 0.23000055\n",
      "Iteration 243, loss = 0.22923227\n",
      "Iteration 244, loss = 0.22836756\n",
      "Iteration 245, loss = 0.22759888\n",
      "Iteration 246, loss = 0.22685664\n",
      "Iteration 247, loss = 0.22605157\n",
      "Iteration 248, loss = 0.22516967\n",
      "Iteration 249, loss = 0.22443225\n",
      "Iteration 250, loss = 0.22365110\n",
      "Iteration 251, loss = 0.22291800\n",
      "Iteration 252, loss = 0.22209342\n",
      "Iteration 253, loss = 0.22137160\n",
      "Iteration 254, loss = 0.22063081\n",
      "Iteration 255, loss = 0.21991100\n",
      "Iteration 256, loss = 0.21919031\n",
      "Iteration 257, loss = 0.21848011\n",
      "Iteration 258, loss = 0.21773748\n",
      "Iteration 259, loss = 0.21703978\n",
      "Iteration 260, loss = 0.21634788\n",
      "Iteration 261, loss = 0.21562829\n",
      "Iteration 262, loss = 0.21498499\n",
      "Iteration 263, loss = 0.21426181\n",
      "Iteration 264, loss = 0.21360872\n",
      "Iteration 265, loss = 0.21289191\n",
      "Iteration 266, loss = 0.21224722\n",
      "Iteration 267, loss = 0.21159981\n",
      "Iteration 268, loss = 0.21094732\n",
      "Iteration 269, loss = 0.21029802\n",
      "Iteration 270, loss = 0.20972869\n",
      "Iteration 271, loss = 0.20897612\n",
      "Iteration 272, loss = 0.20829110\n",
      "Iteration 273, loss = 0.20766586\n",
      "Iteration 274, loss = 0.20704617\n",
      "Iteration 275, loss = 0.20645812\n",
      "Iteration 276, loss = 0.20578131\n",
      "Iteration 277, loss = 0.20517688\n",
      "Iteration 278, loss = 0.20455460\n",
      "Iteration 279, loss = 0.20399334\n",
      "Iteration 280, loss = 0.20334543\n",
      "Iteration 281, loss = 0.20272853\n",
      "Iteration 282, loss = 0.20215515\n",
      "Iteration 283, loss = 0.20159719\n",
      "Iteration 284, loss = 0.20101029\n",
      "Iteration 285, loss = 0.20041589\n",
      "Iteration 286, loss = 0.19995140\n",
      "Iteration 287, loss = 0.19935517\n",
      "Iteration 288, loss = 0.19875329\n",
      "Iteration 289, loss = 0.19819797\n",
      "Iteration 290, loss = 0.19759859\n",
      "Iteration 291, loss = 0.19706170\n",
      "Iteration 292, loss = 0.19649685\n",
      "Iteration 293, loss = 0.19594575\n",
      "Iteration 294, loss = 0.19540035\n",
      "Iteration 295, loss = 0.19484551\n",
      "Iteration 296, loss = 0.19431943\n",
      "Iteration 297, loss = 0.19376607\n",
      "Iteration 298, loss = 0.19320069\n",
      "Iteration 299, loss = 0.19272726\n",
      "Iteration 300, loss = 0.19220431\n",
      "Iteration 301, loss = 0.19168389\n",
      "Iteration 302, loss = 0.19119430\n",
      "Iteration 303, loss = 0.19065913\n",
      "Iteration 304, loss = 0.19012313\n",
      "Iteration 305, loss = 0.18960613\n",
      "Iteration 306, loss = 0.18914373\n",
      "Iteration 307, loss = 0.18867671\n",
      "Iteration 308, loss = 0.18818736\n",
      "Iteration 309, loss = 0.18770421\n",
      "Iteration 310, loss = 0.18723019\n",
      "Iteration 311, loss = 0.18671923\n",
      "Iteration 312, loss = 0.18628456\n",
      "Iteration 313, loss = 0.18579960\n",
      "Iteration 314, loss = 0.18531071\n",
      "Iteration 315, loss = 0.18484867\n",
      "Iteration 316, loss = 0.18435838\n",
      "Iteration 317, loss = 0.18387850\n",
      "Iteration 318, loss = 0.18342873\n",
      "Iteration 319, loss = 0.18298294\n",
      "Iteration 320, loss = 0.18260839\n",
      "Iteration 321, loss = 0.18211505\n",
      "Iteration 322, loss = 0.18169587\n",
      "Iteration 323, loss = 0.18127038\n",
      "Iteration 324, loss = 0.18085130\n",
      "Iteration 325, loss = 0.18040832\n",
      "Iteration 326, loss = 0.17997754\n",
      "Iteration 327, loss = 0.17951257\n",
      "Iteration 328, loss = 0.17910199\n",
      "Iteration 329, loss = 0.17863867\n",
      "Iteration 330, loss = 0.17821770\n",
      "Iteration 331, loss = 0.17780568\n",
      "Iteration 332, loss = 0.17737701\n",
      "Iteration 333, loss = 0.17700759\n",
      "Iteration 334, loss = 0.17657293\n",
      "Iteration 335, loss = 0.17617188\n",
      "Iteration 336, loss = 0.17574706\n",
      "Iteration 337, loss = 0.17535270\n",
      "Iteration 338, loss = 0.17496684\n",
      "Iteration 339, loss = 0.17458566\n",
      "Iteration 340, loss = 0.17416857\n",
      "Iteration 341, loss = 0.17373335\n",
      "Iteration 342, loss = 0.17332859\n",
      "Iteration 343, loss = 0.17291897\n",
      "Iteration 344, loss = 0.17257151\n",
      "Iteration 345, loss = 0.17216939\n",
      "Iteration 346, loss = 0.17177269\n",
      "Iteration 347, loss = 0.17138119\n",
      "Iteration 348, loss = 0.17100693\n",
      "Iteration 349, loss = 0.17063115\n",
      "Iteration 350, loss = 0.17025323\n",
      "Iteration 351, loss = 0.16989531\n",
      "Iteration 352, loss = 0.16951655\n",
      "Iteration 353, loss = 0.16912694\n",
      "Iteration 354, loss = 0.16874932\n",
      "Iteration 355, loss = 0.16840089\n",
      "Iteration 356, loss = 0.16802436\n",
      "Iteration 357, loss = 0.16766863\n",
      "Iteration 358, loss = 0.16728056\n",
      "Iteration 359, loss = 0.16693683\n",
      "Iteration 360, loss = 0.16657488\n",
      "Iteration 361, loss = 0.16621798\n",
      "Iteration 362, loss = 0.16587869\n",
      "Iteration 363, loss = 0.16553882\n",
      "Iteration 364, loss = 0.16521380\n",
      "Iteration 365, loss = 0.16481202\n",
      "Iteration 366, loss = 0.16449801\n",
      "Iteration 367, loss = 0.16416581\n",
      "Iteration 368, loss = 0.16385140\n",
      "Iteration 369, loss = 0.16352131\n",
      "Iteration 370, loss = 0.16313934\n",
      "Iteration 371, loss = 0.16281514\n",
      "Iteration 372, loss = 0.16251082\n",
      "Iteration 373, loss = 0.16216478\n",
      "Iteration 374, loss = 0.16186185\n",
      "Iteration 375, loss = 0.16154144\n",
      "Iteration 376, loss = 0.16120286\n",
      "Iteration 377, loss = 0.16088912\n",
      "Iteration 378, loss = 0.16056978\n",
      "Iteration 379, loss = 0.16024085\n",
      "Iteration 380, loss = 0.15994191\n",
      "Iteration 381, loss = 0.15962332\n",
      "Iteration 382, loss = 0.15928656\n",
      "Iteration 383, loss = 0.15898329\n",
      "Iteration 384, loss = 0.15868208\n",
      "Iteration 385, loss = 0.15837893\n",
      "Iteration 386, loss = 0.15805616\n",
      "Iteration 387, loss = 0.15776533\n",
      "Iteration 388, loss = 0.15747157\n",
      "Iteration 389, loss = 0.15718087\n",
      "Iteration 390, loss = 0.15683857\n",
      "Iteration 391, loss = 0.15656933\n",
      "Iteration 392, loss = 0.15630327\n",
      "Iteration 393, loss = 0.15599684\n",
      "Iteration 394, loss = 0.15572246\n",
      "Iteration 395, loss = 0.15543738\n",
      "Iteration 396, loss = 0.15516828\n",
      "Iteration 397, loss = 0.15485717\n",
      "Iteration 398, loss = 0.15457402\n",
      "Iteration 399, loss = 0.15429283\n",
      "Iteration 400, loss = 0.15400595\n",
      "Iteration 401, loss = 0.15373597\n",
      "Iteration 402, loss = 0.15346256\n",
      "Iteration 403, loss = 0.15317633\n",
      "Iteration 404, loss = 0.15293743\n",
      "Iteration 405, loss = 0.15263309\n",
      "Iteration 406, loss = 0.15234558\n",
      "Iteration 407, loss = 0.15206899\n",
      "Iteration 408, loss = 0.15178727\n",
      "Iteration 409, loss = 0.15152765\n",
      "Iteration 410, loss = 0.15128562\n",
      "Iteration 411, loss = 0.15101372\n",
      "Iteration 412, loss = 0.15072285\n",
      "Iteration 413, loss = 0.15045847\n",
      "Iteration 414, loss = 0.15022061\n",
      "Iteration 415, loss = 0.14993719\n",
      "Iteration 416, loss = 0.14968125\n",
      "Iteration 417, loss = 0.14943327\n",
      "Iteration 418, loss = 0.14915120\n",
      "Iteration 419, loss = 0.14889765\n",
      "Iteration 420, loss = 0.14865254\n",
      "Iteration 421, loss = 0.14838725\n",
      "Iteration 422, loss = 0.14815389\n",
      "Iteration 423, loss = 0.14789331\n",
      "Iteration 424, loss = 0.14765811\n",
      "Iteration 425, loss = 0.14739381\n",
      "Iteration 426, loss = 0.14713220\n",
      "Iteration 427, loss = 0.14684899\n",
      "Iteration 428, loss = 0.14664484\n",
      "Iteration 429, loss = 0.14635820\n",
      "Iteration 430, loss = 0.14610861\n",
      "Iteration 431, loss = 0.14587732\n",
      "Iteration 432, loss = 0.14566349\n",
      "Iteration 433, loss = 0.14540986\n",
      "Iteration 434, loss = 0.14519200\n",
      "Iteration 435, loss = 0.14494739\n",
      "Iteration 436, loss = 0.14470573\n",
      "Iteration 437, loss = 0.14448241\n",
      "Iteration 438, loss = 0.14423724\n",
      "Iteration 439, loss = 0.14402115\n",
      "Iteration 440, loss = 0.14377678\n",
      "Iteration 441, loss = 0.14356057\n",
      "Iteration 442, loss = 0.14329535\n",
      "Iteration 443, loss = 0.14309721\n",
      "Iteration 444, loss = 0.14283709\n",
      "Iteration 445, loss = 0.14259387\n",
      "Iteration 446, loss = 0.14238231\n",
      "Iteration 447, loss = 0.14214632\n",
      "Iteration 448, loss = 0.14193433\n",
      "Iteration 449, loss = 0.14170542\n",
      "Iteration 450, loss = 0.14149798\n",
      "Iteration 451, loss = 0.14128251\n",
      "Iteration 452, loss = 0.14106473\n",
      "Iteration 453, loss = 0.14085006\n",
      "Iteration 454, loss = 0.14067237\n",
      "Iteration 455, loss = 0.14044757\n",
      "Iteration 456, loss = 0.14021488\n",
      "Iteration 457, loss = 0.14003188\n",
      "Iteration 458, loss = 0.13981467\n",
      "Iteration 459, loss = 0.13960354\n",
      "Iteration 460, loss = 0.13943217\n",
      "Iteration 461, loss = 0.13920652\n",
      "Iteration 462, loss = 0.13900815\n",
      "Iteration 463, loss = 0.13877152\n",
      "Iteration 464, loss = 0.13855992\n",
      "Iteration 465, loss = 0.13832744\n",
      "Iteration 466, loss = 0.13812794\n",
      "Iteration 467, loss = 0.13795529\n",
      "Iteration 468, loss = 0.13775382\n",
      "Iteration 469, loss = 0.13754595\n",
      "Iteration 470, loss = 0.13735055\n",
      "Iteration 471, loss = 0.13716333\n",
      "Iteration 472, loss = 0.13696407\n",
      "Iteration 473, loss = 0.13676193\n",
      "Iteration 474, loss = 0.13654860\n",
      "Iteration 475, loss = 0.13632987\n",
      "Iteration 476, loss = 0.13611327\n",
      "Iteration 477, loss = 0.13599518\n",
      "Iteration 478, loss = 0.13575653\n",
      "Iteration 479, loss = 0.13557243\n",
      "Iteration 480, loss = 0.13538327\n",
      "Iteration 481, loss = 0.13517658\n",
      "Iteration 482, loss = 0.13499265\n",
      "Iteration 483, loss = 0.13480552\n",
      "Iteration 484, loss = 0.13461875\n",
      "Iteration 485, loss = 0.13441865\n",
      "Iteration 486, loss = 0.13422199\n",
      "Iteration 487, loss = 0.13403931\n",
      "Iteration 488, loss = 0.13385207\n",
      "Iteration 489, loss = 0.13366344\n",
      "Iteration 490, loss = 0.13349848\n",
      "Iteration 491, loss = 0.13330333\n",
      "Iteration 492, loss = 0.13315650\n",
      "Iteration 493, loss = 0.13296065\n",
      "Iteration 494, loss = 0.13279774\n",
      "Iteration 495, loss = 0.13261776\n",
      "Iteration 496, loss = 0.13243780\n",
      "Iteration 497, loss = 0.13227964\n",
      "Iteration 498, loss = 0.13207806\n",
      "Iteration 499, loss = 0.13190953\n",
      "Iteration 500, loss = 0.13177333\n",
      "Iteration 501, loss = 0.13157457\n",
      "Iteration 502, loss = 0.13141131\n",
      "Iteration 503, loss = 0.13123245\n",
      "Iteration 504, loss = 0.13107013\n",
      "Iteration 505, loss = 0.13089233\n",
      "Iteration 506, loss = 0.13073920\n",
      "Iteration 507, loss = 0.13058126\n",
      "Iteration 508, loss = 0.13038683\n",
      "Iteration 509, loss = 0.13022634\n",
      "Iteration 510, loss = 0.13005841\n",
      "Iteration 511, loss = 0.12989934\n",
      "Iteration 512, loss = 0.12975613\n",
      "Iteration 513, loss = 0.12959531\n",
      "Iteration 514, loss = 0.12941685\n",
      "Iteration 515, loss = 0.12925134\n",
      "Iteration 516, loss = 0.12909827\n",
      "Iteration 517, loss = 0.12892822\n",
      "Iteration 518, loss = 0.12878288\n",
      "Iteration 519, loss = 0.12860631\n",
      "Iteration 520, loss = 0.12847506\n",
      "Iteration 521, loss = 0.12832368\n",
      "Iteration 522, loss = 0.12816895\n",
      "Iteration 523, loss = 0.12800989\n",
      "Iteration 524, loss = 0.12783863\n",
      "Iteration 525, loss = 0.12767576\n",
      "Iteration 526, loss = 0.12753775\n",
      "Iteration 527, loss = 0.12737105\n",
      "Iteration 528, loss = 0.12722648\n",
      "Iteration 529, loss = 0.12710585\n",
      "Iteration 530, loss = 0.12692710\n",
      "Iteration 531, loss = 0.12678027\n",
      "Iteration 532, loss = 0.12665424\n",
      "Iteration 533, loss = 0.12648653\n",
      "Iteration 534, loss = 0.12633848\n",
      "Iteration 535, loss = 0.12618262\n",
      "Iteration 536, loss = 0.12604677\n",
      "Iteration 537, loss = 0.12588944\n",
      "Iteration 538, loss = 0.12574043\n",
      "Iteration 539, loss = 0.12558298\n",
      "Iteration 540, loss = 0.12546095\n",
      "Iteration 541, loss = 0.12530283\n",
      "Iteration 542, loss = 0.12516181\n",
      "Iteration 543, loss = 0.12502537\n",
      "Iteration 544, loss = 0.12488941\n",
      "Iteration 545, loss = 0.12474939\n",
      "Iteration 546, loss = 0.12460869\n",
      "Iteration 547, loss = 0.12448043\n",
      "Iteration 548, loss = 0.12434373\n",
      "Iteration 549, loss = 0.12421427\n",
      "Iteration 550, loss = 0.12406215\n",
      "Iteration 551, loss = 0.12394053\n",
      "Iteration 552, loss = 0.12379471\n",
      "Iteration 553, loss = 0.12363433\n",
      "Iteration 554, loss = 0.12349872\n",
      "Iteration 555, loss = 0.12335149\n",
      "Iteration 556, loss = 0.12321509\n",
      "Iteration 557, loss = 0.12307412\n",
      "Iteration 558, loss = 0.12294790\n",
      "Iteration 559, loss = 0.12280765\n",
      "Iteration 560, loss = 0.12267045\n",
      "Iteration 561, loss = 0.12254761\n",
      "Iteration 562, loss = 0.12239978\n",
      "Iteration 563, loss = 0.12225953\n",
      "Iteration 564, loss = 0.12213845\n",
      "Iteration 565, loss = 0.12200259\n",
      "Iteration 566, loss = 0.12187733\n",
      "Iteration 567, loss = 0.12174394\n",
      "Iteration 568, loss = 0.12162085\n",
      "Iteration 569, loss = 0.12151638\n",
      "Iteration 570, loss = 0.12138925\n",
      "Iteration 571, loss = 0.12128015\n",
      "Iteration 572, loss = 0.12113871\n",
      "Iteration 573, loss = 0.12100427\n",
      "Iteration 574, loss = 0.12087280\n",
      "Iteration 575, loss = 0.12079047\n",
      "Iteration 576, loss = 0.12065036\n",
      "Iteration 577, loss = 0.12050976\n",
      "Iteration 578, loss = 0.12038125\n",
      "Iteration 579, loss = 0.12024614\n",
      "Iteration 580, loss = 0.12012363\n",
      "Iteration 581, loss = 0.12000046\n",
      "Iteration 582, loss = 0.11989409\n",
      "Iteration 583, loss = 0.11976006\n",
      "Iteration 584, loss = 0.11966255\n",
      "Iteration 585, loss = 0.11952704\n",
      "Iteration 586, loss = 0.11940893\n",
      "Iteration 587, loss = 0.11928071\n",
      "Iteration 588, loss = 0.11915198\n",
      "Iteration 589, loss = 0.11902758\n",
      "Iteration 590, loss = 0.11891035\n",
      "Iteration 591, loss = 0.11879050\n",
      "Iteration 592, loss = 0.11866624\n",
      "Iteration 593, loss = 0.11854376\n",
      "Iteration 594, loss = 0.11842318\n",
      "Iteration 595, loss = 0.11830288\n",
      "Iteration 596, loss = 0.11819053\n",
      "Iteration 597, loss = 0.11806550\n",
      "Iteration 598, loss = 0.11795495\n",
      "Iteration 599, loss = 0.11785217\n",
      "Iteration 600, loss = 0.11772992\n",
      "Iteration 601, loss = 0.11762215\n",
      "Iteration 602, loss = 0.11751838\n",
      "Iteration 603, loss = 0.11741936\n",
      "Iteration 604, loss = 0.11730039\n",
      "Iteration 605, loss = 0.11717736\n",
      "Iteration 606, loss = 0.11706773\n",
      "Iteration 607, loss = 0.11695748\n",
      "Iteration 608, loss = 0.11685449\n",
      "Iteration 609, loss = 0.11673510\n",
      "Iteration 610, loss = 0.11662349\n",
      "Iteration 611, loss = 0.11651343\n",
      "Iteration 612, loss = 0.11640637\n",
      "Iteration 613, loss = 0.11630720\n",
      "Iteration 614, loss = 0.11620558\n",
      "Iteration 615, loss = 0.11610330\n",
      "Iteration 616, loss = 0.11600351\n",
      "Iteration 617, loss = 0.11589002\n",
      "Iteration 618, loss = 0.11578970\n",
      "Iteration 619, loss = 0.11567941\n",
      "Iteration 620, loss = 0.11557344\n",
      "Iteration 621, loss = 0.11546749\n",
      "Iteration 622, loss = 0.11537070\n",
      "Iteration 623, loss = 0.11527850\n",
      "Iteration 624, loss = 0.11517530\n",
      "Iteration 625, loss = 0.11506797\n",
      "Iteration 626, loss = 0.11499448\n",
      "Iteration 627, loss = 0.11488127\n",
      "Iteration 628, loss = 0.11475861\n",
      "Iteration 629, loss = 0.11465636\n",
      "Iteration 630, loss = 0.11454996\n",
      "Iteration 631, loss = 0.11446397\n",
      "Iteration 632, loss = 0.11436384\n",
      "Iteration 633, loss = 0.11426224\n",
      "Iteration 634, loss = 0.11416470\n",
      "Iteration 635, loss = 0.11407608\n",
      "Iteration 636, loss = 0.11397900\n",
      "Iteration 637, loss = 0.11387741\n",
      "Iteration 638, loss = 0.11378994\n",
      "Iteration 639, loss = 0.11369353\n",
      "Iteration 640, loss = 0.11357583\n",
      "Iteration 641, loss = 0.11349130\n",
      "Iteration 642, loss = 0.11338146\n",
      "Iteration 643, loss = 0.11329290\n",
      "Iteration 644, loss = 0.11318613\n",
      "Iteration 645, loss = 0.11309920\n",
      "Iteration 646, loss = 0.11299332\n",
      "Iteration 647, loss = 0.11290270\n",
      "Iteration 648, loss = 0.11280324\n",
      "Iteration 649, loss = 0.11272299\n",
      "Iteration 650, loss = 0.11262169\n",
      "Iteration 651, loss = 0.11251101\n",
      "Iteration 652, loss = 0.11242751\n",
      "Iteration 653, loss = 0.11232473\n",
      "Iteration 654, loss = 0.11224978\n",
      "Iteration 655, loss = 0.11217703\n",
      "Iteration 656, loss = 0.11207964\n",
      "Iteration 657, loss = 0.11198136\n",
      "Iteration 658, loss = 0.11188129\n",
      "Iteration 659, loss = 0.11179044\n",
      "Iteration 660, loss = 0.11169790\n",
      "Iteration 661, loss = 0.11160310\n",
      "Iteration 662, loss = 0.11151208\n",
      "Iteration 663, loss = 0.11141789\n",
      "Iteration 664, loss = 0.11133158\n",
      "Iteration 665, loss = 0.11124834\n",
      "Iteration 666, loss = 0.11116437\n",
      "Iteration 667, loss = 0.11108057\n",
      "Iteration 668, loss = 0.11099004\n",
      "Iteration 669, loss = 0.11091630\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.42845475\n",
      "Iteration 2, loss = 2.40143967\n",
      "Iteration 3, loss = 2.36911784\n",
      "Iteration 4, loss = 2.33703964\n",
      "Iteration 5, loss = 2.30874254\n",
      "Iteration 6, loss = 2.28190783\n",
      "Iteration 7, loss = 2.25620353\n",
      "Iteration 8, loss = 2.23174764\n",
      "Iteration 9, loss = 2.20774395\n",
      "Iteration 10, loss = 2.18367784\n",
      "Iteration 11, loss = 2.15961545\n",
      "Iteration 12, loss = 2.13526815\n",
      "Iteration 13, loss = 2.11084353\n",
      "Iteration 14, loss = 2.08592383\n",
      "Iteration 15, loss = 2.06075296\n",
      "Iteration 16, loss = 2.03516466\n",
      "Iteration 17, loss = 2.00920584\n",
      "Iteration 18, loss = 1.98317665\n",
      "Iteration 19, loss = 1.95668670\n",
      "Iteration 20, loss = 1.92998075\n",
      "Iteration 21, loss = 1.90281797\n",
      "Iteration 22, loss = 1.87532332\n",
      "Iteration 23, loss = 1.84754837\n",
      "Iteration 24, loss = 1.81968107\n",
      "Iteration 25, loss = 1.79152024\n",
      "Iteration 26, loss = 1.76314555\n",
      "Iteration 27, loss = 1.73442528\n",
      "Iteration 28, loss = 1.70599330\n",
      "Iteration 29, loss = 1.67752475\n",
      "Iteration 30, loss = 1.64902230\n",
      "Iteration 31, loss = 1.62041101\n",
      "Iteration 32, loss = 1.59227079\n",
      "Iteration 33, loss = 1.56402656\n",
      "Iteration 34, loss = 1.53613252\n",
      "Iteration 35, loss = 1.50839799\n",
      "Iteration 36, loss = 1.48078344\n",
      "Iteration 37, loss = 1.45380492\n",
      "Iteration 38, loss = 1.42698179\n",
      "Iteration 39, loss = 1.40044242\n",
      "Iteration 40, loss = 1.37450327\n",
      "Iteration 41, loss = 1.34895119\n",
      "Iteration 42, loss = 1.32347308\n",
      "Iteration 43, loss = 1.29875396\n",
      "Iteration 44, loss = 1.27447102\n",
      "Iteration 45, loss = 1.25051690\n",
      "Iteration 46, loss = 1.22699351\n",
      "Iteration 47, loss = 1.20424818\n",
      "Iteration 48, loss = 1.18218138\n",
      "Iteration 49, loss = 1.16015951\n",
      "Iteration 50, loss = 1.13895259\n",
      "Iteration 51, loss = 1.11786226\n",
      "Iteration 52, loss = 1.09736173\n",
      "Iteration 53, loss = 1.07759525\n",
      "Iteration 54, loss = 1.05827712\n",
      "Iteration 55, loss = 1.03940686\n",
      "Iteration 56, loss = 1.02100745\n",
      "Iteration 57, loss = 1.00302599\n",
      "Iteration 58, loss = 0.98551402\n",
      "Iteration 59, loss = 0.96865856\n",
      "Iteration 60, loss = 0.95201154\n",
      "Iteration 61, loss = 0.93569109\n",
      "Iteration 62, loss = 0.91999644\n",
      "Iteration 63, loss = 0.90476344\n",
      "Iteration 64, loss = 0.88996583\n",
      "Iteration 65, loss = 0.87539616\n",
      "Iteration 66, loss = 0.86167845\n",
      "Iteration 67, loss = 0.84794114\n",
      "Iteration 68, loss = 0.83456032\n",
      "Iteration 69, loss = 0.82187064\n",
      "Iteration 70, loss = 0.80927817\n",
      "Iteration 71, loss = 0.79703633\n",
      "Iteration 72, loss = 0.78511692\n",
      "Iteration 73, loss = 0.77359444\n",
      "Iteration 74, loss = 0.76259302\n",
      "Iteration 75, loss = 0.75177345\n",
      "Iteration 76, loss = 0.74113733\n",
      "Iteration 77, loss = 0.73080194\n",
      "Iteration 78, loss = 0.72059207\n",
      "Iteration 79, loss = 0.71080538\n",
      "Iteration 80, loss = 0.70129342\n",
      "Iteration 81, loss = 0.69208559\n",
      "Iteration 82, loss = 0.68300050\n",
      "Iteration 83, loss = 0.67416010\n",
      "Iteration 84, loss = 0.66554677\n",
      "Iteration 85, loss = 0.65705576\n",
      "Iteration 86, loss = 0.64894997\n",
      "Iteration 87, loss = 0.64092208\n",
      "Iteration 88, loss = 0.63328387\n",
      "Iteration 89, loss = 0.62552193\n",
      "Iteration 90, loss = 0.61796713\n",
      "Iteration 91, loss = 0.61078646\n",
      "Iteration 92, loss = 0.60363361\n",
      "Iteration 93, loss = 0.59678508\n",
      "Iteration 94, loss = 0.59020516\n",
      "Iteration 95, loss = 0.58363157\n",
      "Iteration 96, loss = 0.57717779\n",
      "Iteration 97, loss = 0.57095396\n",
      "Iteration 98, loss = 0.56480039\n",
      "Iteration 99, loss = 0.55886659\n",
      "Iteration 100, loss = 0.55288307\n",
      "Iteration 101, loss = 0.54726997\n",
      "Iteration 102, loss = 0.54154575\n",
      "Iteration 103, loss = 0.53603593\n",
      "Iteration 104, loss = 0.53069153\n",
      "Iteration 105, loss = 0.52534703\n",
      "Iteration 106, loss = 0.52015795\n",
      "Iteration 107, loss = 0.51516763\n",
      "Iteration 108, loss = 0.51019193\n",
      "Iteration 109, loss = 0.50537110\n",
      "Iteration 110, loss = 0.50065581\n",
      "Iteration 111, loss = 0.49601312\n",
      "Iteration 112, loss = 0.49139784\n",
      "Iteration 113, loss = 0.48699849\n",
      "Iteration 114, loss = 0.48264130\n",
      "Iteration 115, loss = 0.47829308\n",
      "Iteration 116, loss = 0.47407914\n",
      "Iteration 117, loss = 0.46997477\n",
      "Iteration 118, loss = 0.46599987\n",
      "Iteration 119, loss = 0.46202647\n",
      "Iteration 120, loss = 0.45815999\n",
      "Iteration 121, loss = 0.45436878\n",
      "Iteration 122, loss = 0.45062963\n",
      "Iteration 123, loss = 0.44692131\n",
      "Iteration 124, loss = 0.44323081\n",
      "Iteration 125, loss = 0.43969397\n",
      "Iteration 126, loss = 0.43622206\n",
      "Iteration 127, loss = 0.43278326\n",
      "Iteration 128, loss = 0.42937196\n",
      "Iteration 129, loss = 0.42618276\n",
      "Iteration 130, loss = 0.42286982\n",
      "Iteration 131, loss = 0.41966683\n",
      "Iteration 132, loss = 0.41647313\n",
      "Iteration 133, loss = 0.41337067\n",
      "Iteration 134, loss = 0.41024491\n",
      "Iteration 135, loss = 0.40720780\n",
      "Iteration 136, loss = 0.40423128\n",
      "Iteration 137, loss = 0.40126867\n",
      "Iteration 138, loss = 0.39850078\n",
      "Iteration 139, loss = 0.39565982\n",
      "Iteration 140, loss = 0.39277759\n",
      "Iteration 141, loss = 0.39010959\n",
      "Iteration 142, loss = 0.38737026\n",
      "Iteration 143, loss = 0.38476859\n",
      "Iteration 144, loss = 0.38210089\n",
      "Iteration 145, loss = 0.37950299\n",
      "Iteration 146, loss = 0.37701901\n",
      "Iteration 147, loss = 0.37455724\n",
      "Iteration 148, loss = 0.37218707\n",
      "Iteration 149, loss = 0.36976608\n",
      "Iteration 150, loss = 0.36743932\n",
      "Iteration 151, loss = 0.36499181\n",
      "Iteration 152, loss = 0.36257993\n",
      "Iteration 153, loss = 0.36036386\n",
      "Iteration 154, loss = 0.35811392\n",
      "Iteration 155, loss = 0.35587102\n",
      "Iteration 156, loss = 0.35367438\n",
      "Iteration 157, loss = 0.35144759\n",
      "Iteration 158, loss = 0.34944233\n",
      "Iteration 159, loss = 0.34729043\n",
      "Iteration 160, loss = 0.34524745\n",
      "Iteration 161, loss = 0.34318037\n",
      "Iteration 162, loss = 0.34116993\n",
      "Iteration 163, loss = 0.33915456\n",
      "Iteration 164, loss = 0.33726245\n",
      "Iteration 165, loss = 0.33519903\n",
      "Iteration 166, loss = 0.33324741\n",
      "Iteration 167, loss = 0.33142352\n",
      "Iteration 168, loss = 0.32956730\n",
      "Iteration 169, loss = 0.32773722\n",
      "Iteration 170, loss = 0.32588128\n",
      "Iteration 171, loss = 0.32410822\n",
      "Iteration 172, loss = 0.32232408\n",
      "Iteration 173, loss = 0.32061150\n",
      "Iteration 174, loss = 0.31888956\n",
      "Iteration 175, loss = 0.31719496\n",
      "Iteration 176, loss = 0.31539567\n",
      "Iteration 177, loss = 0.31372490\n",
      "Iteration 178, loss = 0.31202051\n",
      "Iteration 179, loss = 0.31035945\n",
      "Iteration 180, loss = 0.30874844\n",
      "Iteration 181, loss = 0.30716116\n",
      "Iteration 182, loss = 0.30556506\n",
      "Iteration 183, loss = 0.30402113\n",
      "Iteration 184, loss = 0.30248894\n",
      "Iteration 185, loss = 0.30102454\n",
      "Iteration 186, loss = 0.29949919\n",
      "Iteration 187, loss = 0.29808721\n",
      "Iteration 188, loss = 0.29657612\n",
      "Iteration 189, loss = 0.29511810\n",
      "Iteration 190, loss = 0.29365659\n",
      "Iteration 191, loss = 0.29222434\n",
      "Iteration 192, loss = 0.29084546\n",
      "Iteration 193, loss = 0.28953047\n",
      "Iteration 194, loss = 0.28816707\n",
      "Iteration 195, loss = 0.28675648\n",
      "Iteration 196, loss = 0.28551908\n",
      "Iteration 197, loss = 0.28414636\n",
      "Iteration 198, loss = 0.28276919\n",
      "Iteration 199, loss = 0.28147471\n",
      "Iteration 200, loss = 0.28014996\n",
      "Iteration 201, loss = 0.27894468\n",
      "Iteration 202, loss = 0.27764693\n",
      "Iteration 203, loss = 0.27645173\n",
      "Iteration 204, loss = 0.27522713\n",
      "Iteration 205, loss = 0.27393278\n",
      "Iteration 206, loss = 0.27276077\n",
      "Iteration 207, loss = 0.27149757\n",
      "Iteration 208, loss = 0.27029935\n",
      "Iteration 209, loss = 0.26914046\n",
      "Iteration 210, loss = 0.26802183\n",
      "Iteration 211, loss = 0.26679903\n",
      "Iteration 212, loss = 0.26574350\n",
      "Iteration 213, loss = 0.26464193\n",
      "Iteration 214, loss = 0.26347043\n",
      "Iteration 215, loss = 0.26236512\n",
      "Iteration 216, loss = 0.26131330\n",
      "Iteration 217, loss = 0.26022529\n",
      "Iteration 218, loss = 0.25918402\n",
      "Iteration 219, loss = 0.25803296\n",
      "Iteration 220, loss = 0.25694989\n",
      "Iteration 221, loss = 0.25588613\n",
      "Iteration 222, loss = 0.25490760\n",
      "Iteration 223, loss = 0.25385955\n",
      "Iteration 224, loss = 0.25279032\n",
      "Iteration 225, loss = 0.25183722\n",
      "Iteration 226, loss = 0.25083100\n",
      "Iteration 227, loss = 0.24991340\n",
      "Iteration 228, loss = 0.24893878\n",
      "Iteration 229, loss = 0.24795537\n",
      "Iteration 230, loss = 0.24701484\n",
      "Iteration 231, loss = 0.24608037\n",
      "Iteration 232, loss = 0.24513578\n",
      "Iteration 233, loss = 0.24423164\n",
      "Iteration 234, loss = 0.24324963\n",
      "Iteration 235, loss = 0.24238697\n",
      "Iteration 236, loss = 0.24146209\n",
      "Iteration 237, loss = 0.24057266\n",
      "Iteration 238, loss = 0.23966605\n",
      "Iteration 239, loss = 0.23878772\n",
      "Iteration 240, loss = 0.23788264\n",
      "Iteration 241, loss = 0.23697616\n",
      "Iteration 242, loss = 0.23616064\n",
      "Iteration 243, loss = 0.23530607\n",
      "Iteration 244, loss = 0.23446682\n",
      "Iteration 245, loss = 0.23361774\n",
      "Iteration 246, loss = 0.23277507\n",
      "Iteration 247, loss = 0.23195377\n",
      "Iteration 248, loss = 0.23116349\n",
      "Iteration 249, loss = 0.23038393\n",
      "Iteration 250, loss = 0.22954530\n",
      "Iteration 251, loss = 0.22876356\n",
      "Iteration 252, loss = 0.22794931\n",
      "Iteration 253, loss = 0.22716156\n",
      "Iteration 254, loss = 0.22645231\n",
      "Iteration 255, loss = 0.22563081\n",
      "Iteration 256, loss = 0.22493632\n",
      "Iteration 257, loss = 0.22416463\n",
      "Iteration 258, loss = 0.22339001\n",
      "Iteration 259, loss = 0.22261797\n",
      "Iteration 260, loss = 0.22191009\n",
      "Iteration 261, loss = 0.22119362\n",
      "Iteration 262, loss = 0.22039710\n",
      "Iteration 263, loss = 0.21966326\n",
      "Iteration 264, loss = 0.21900936\n",
      "Iteration 265, loss = 0.21825102\n",
      "Iteration 266, loss = 0.21754032\n",
      "Iteration 267, loss = 0.21685252\n",
      "Iteration 268, loss = 0.21612583\n",
      "Iteration 269, loss = 0.21543725\n",
      "Iteration 270, loss = 0.21472700\n",
      "Iteration 271, loss = 0.21404292\n",
      "Iteration 272, loss = 0.21343089\n",
      "Iteration 273, loss = 0.21275267\n",
      "Iteration 274, loss = 0.21212650\n",
      "Iteration 275, loss = 0.21144959\n",
      "Iteration 276, loss = 0.21077404\n",
      "Iteration 277, loss = 0.21009581\n",
      "Iteration 278, loss = 0.20957833\n",
      "Iteration 279, loss = 0.20888679\n",
      "Iteration 280, loss = 0.20824526\n",
      "Iteration 281, loss = 0.20759001\n",
      "Iteration 282, loss = 0.20695122\n",
      "Iteration 283, loss = 0.20632222\n",
      "Iteration 284, loss = 0.20569956\n",
      "Iteration 285, loss = 0.20505631\n",
      "Iteration 286, loss = 0.20446394\n",
      "Iteration 287, loss = 0.20381865\n",
      "Iteration 288, loss = 0.20326511\n",
      "Iteration 289, loss = 0.20266721\n",
      "Iteration 290, loss = 0.20209447\n",
      "Iteration 291, loss = 0.20152542\n",
      "Iteration 292, loss = 0.20093620\n",
      "Iteration 293, loss = 0.20040963\n",
      "Iteration 294, loss = 0.19978804\n",
      "Iteration 295, loss = 0.19918678\n",
      "Iteration 296, loss = 0.19862920\n",
      "Iteration 297, loss = 0.19802993\n",
      "Iteration 298, loss = 0.19746419\n",
      "Iteration 299, loss = 0.19692346\n",
      "Iteration 300, loss = 0.19636561\n",
      "Iteration 301, loss = 0.19581003\n",
      "Iteration 302, loss = 0.19525997\n",
      "Iteration 303, loss = 0.19474369\n",
      "Iteration 304, loss = 0.19419457\n",
      "Iteration 305, loss = 0.19365789\n",
      "Iteration 306, loss = 0.19316487\n",
      "Iteration 307, loss = 0.19263422\n",
      "Iteration 308, loss = 0.19210796\n",
      "Iteration 309, loss = 0.19161137\n",
      "Iteration 310, loss = 0.19108760\n",
      "Iteration 311, loss = 0.19058741\n",
      "Iteration 312, loss = 0.19007787\n",
      "Iteration 313, loss = 0.18960604\n",
      "Iteration 314, loss = 0.18910718\n",
      "Iteration 315, loss = 0.18862126\n",
      "Iteration 316, loss = 0.18812778\n",
      "Iteration 317, loss = 0.18763472\n",
      "Iteration 318, loss = 0.18713515\n",
      "Iteration 319, loss = 0.18667551\n",
      "Iteration 320, loss = 0.18618629\n",
      "Iteration 321, loss = 0.18573387\n",
      "Iteration 322, loss = 0.18524656\n",
      "Iteration 323, loss = 0.18479499\n",
      "Iteration 324, loss = 0.18429267\n",
      "Iteration 325, loss = 0.18384033\n",
      "Iteration 326, loss = 0.18337524\n",
      "Iteration 327, loss = 0.18293677\n",
      "Iteration 328, loss = 0.18250511\n",
      "Iteration 329, loss = 0.18203756\n",
      "Iteration 330, loss = 0.18161111\n",
      "Iteration 331, loss = 0.18117671\n",
      "Iteration 332, loss = 0.18076701\n",
      "Iteration 333, loss = 0.18031972\n",
      "Iteration 334, loss = 0.17984842\n",
      "Iteration 335, loss = 0.17943821\n",
      "Iteration 336, loss = 0.17900740\n",
      "Iteration 337, loss = 0.17857669\n",
      "Iteration 338, loss = 0.17816092\n",
      "Iteration 339, loss = 0.17773098\n",
      "Iteration 340, loss = 0.17734128\n",
      "Iteration 341, loss = 0.17687107\n",
      "Iteration 342, loss = 0.17645895\n",
      "Iteration 343, loss = 0.17604300\n",
      "Iteration 344, loss = 0.17564121\n",
      "Iteration 345, loss = 0.17520739\n",
      "Iteration 346, loss = 0.17481220\n",
      "Iteration 347, loss = 0.17445822\n",
      "Iteration 348, loss = 0.17403267\n",
      "Iteration 349, loss = 0.17362474\n",
      "Iteration 350, loss = 0.17323472\n",
      "Iteration 351, loss = 0.17284884\n",
      "Iteration 352, loss = 0.17243943\n",
      "Iteration 353, loss = 0.17204970\n",
      "Iteration 354, loss = 0.17168028\n",
      "Iteration 355, loss = 0.17129500\n",
      "Iteration 356, loss = 0.17092330\n",
      "Iteration 357, loss = 0.17054474\n",
      "Iteration 358, loss = 0.17019561\n",
      "Iteration 359, loss = 0.16983329\n",
      "Iteration 360, loss = 0.16947898\n",
      "Iteration 361, loss = 0.16909526\n",
      "Iteration 362, loss = 0.16873614\n",
      "Iteration 363, loss = 0.16834357\n",
      "Iteration 364, loss = 0.16794244\n",
      "Iteration 365, loss = 0.16754009\n",
      "Iteration 366, loss = 0.16718720\n",
      "Iteration 367, loss = 0.16683055\n",
      "Iteration 368, loss = 0.16649413\n",
      "Iteration 369, loss = 0.16613988\n",
      "Iteration 370, loss = 0.16580125\n",
      "Iteration 371, loss = 0.16547467\n",
      "Iteration 372, loss = 0.16509275\n",
      "Iteration 373, loss = 0.16474344\n",
      "Iteration 374, loss = 0.16440372\n",
      "Iteration 375, loss = 0.16406409\n",
      "Iteration 376, loss = 0.16374995\n",
      "Iteration 377, loss = 0.16337690\n",
      "Iteration 378, loss = 0.16305070\n",
      "Iteration 379, loss = 0.16268485\n",
      "Iteration 380, loss = 0.16234348\n",
      "Iteration 381, loss = 0.16203660\n",
      "Iteration 382, loss = 0.16168214\n",
      "Iteration 383, loss = 0.16133836\n",
      "Iteration 384, loss = 0.16103370\n",
      "Iteration 385, loss = 0.16069909\n",
      "Iteration 386, loss = 0.16041915\n",
      "Iteration 387, loss = 0.16010329\n",
      "Iteration 388, loss = 0.15979416\n",
      "Iteration 389, loss = 0.15949807\n",
      "Iteration 390, loss = 0.15911900\n",
      "Iteration 391, loss = 0.15879135\n",
      "Iteration 392, loss = 0.15848430\n",
      "Iteration 393, loss = 0.15818061\n",
      "Iteration 394, loss = 0.15788920\n",
      "Iteration 395, loss = 0.15755536\n",
      "Iteration 396, loss = 0.15724518\n",
      "Iteration 397, loss = 0.15696376\n",
      "Iteration 398, loss = 0.15668091\n",
      "Iteration 399, loss = 0.15633847\n",
      "Iteration 400, loss = 0.15606218\n",
      "Iteration 401, loss = 0.15576584\n",
      "Iteration 402, loss = 0.15550673\n",
      "Iteration 403, loss = 0.15524787\n",
      "Iteration 404, loss = 0.15496303\n",
      "Iteration 405, loss = 0.15465317\n",
      "Iteration 406, loss = 0.15432716\n",
      "Iteration 407, loss = 0.15404158\n",
      "Iteration 408, loss = 0.15371787\n",
      "Iteration 409, loss = 0.15345906\n",
      "Iteration 410, loss = 0.15316284\n",
      "Iteration 411, loss = 0.15289889\n",
      "Iteration 412, loss = 0.15259932\n",
      "Iteration 413, loss = 0.15230764\n",
      "Iteration 414, loss = 0.15202510\n",
      "Iteration 415, loss = 0.15175047\n",
      "Iteration 416, loss = 0.15148324\n",
      "Iteration 417, loss = 0.15122326\n",
      "Iteration 418, loss = 0.15093335\n",
      "Iteration 419, loss = 0.15067984\n",
      "Iteration 420, loss = 0.15043615\n",
      "Iteration 421, loss = 0.15017671\n",
      "Iteration 422, loss = 0.14991108\n",
      "Iteration 423, loss = 0.14968033\n",
      "Iteration 424, loss = 0.14941461\n",
      "Iteration 425, loss = 0.14914182\n",
      "Iteration 426, loss = 0.14888539\n",
      "Iteration 427, loss = 0.14864233\n",
      "Iteration 428, loss = 0.14835840\n",
      "Iteration 429, loss = 0.14813566\n",
      "Iteration 430, loss = 0.14787036\n",
      "Iteration 431, loss = 0.14763642\n",
      "Iteration 432, loss = 0.14740664\n",
      "Iteration 433, loss = 0.14715298\n",
      "Iteration 434, loss = 0.14691855\n",
      "Iteration 435, loss = 0.14667471\n",
      "Iteration 436, loss = 0.14642489\n",
      "Iteration 437, loss = 0.14619303\n",
      "Iteration 438, loss = 0.14594210\n",
      "Iteration 439, loss = 0.14571631\n",
      "Iteration 440, loss = 0.14547776\n",
      "Iteration 441, loss = 0.14524273\n",
      "Iteration 442, loss = 0.14501641\n",
      "Iteration 443, loss = 0.14476514\n",
      "Iteration 444, loss = 0.14456183\n",
      "Iteration 445, loss = 0.14432729\n",
      "Iteration 446, loss = 0.14409406\n",
      "Iteration 447, loss = 0.14387569\n",
      "Iteration 448, loss = 0.14364490\n",
      "Iteration 449, loss = 0.14340500\n",
      "Iteration 450, loss = 0.14315858\n",
      "Iteration 451, loss = 0.14294138\n",
      "Iteration 452, loss = 0.14270944\n",
      "Iteration 453, loss = 0.14249063\n",
      "Iteration 454, loss = 0.14229387\n",
      "Iteration 455, loss = 0.14206175\n",
      "Iteration 456, loss = 0.14185461\n",
      "Iteration 457, loss = 0.14160423\n",
      "Iteration 458, loss = 0.14139404\n",
      "Iteration 459, loss = 0.14116112\n",
      "Iteration 460, loss = 0.14091939\n",
      "Iteration 461, loss = 0.14070652\n",
      "Iteration 462, loss = 0.14052612\n",
      "Iteration 463, loss = 0.14031509\n",
      "Iteration 464, loss = 0.14011850\n",
      "Iteration 465, loss = 0.13988890\n",
      "Iteration 466, loss = 0.13965400\n",
      "Iteration 467, loss = 0.13945149\n",
      "Iteration 468, loss = 0.13922770\n",
      "Iteration 469, loss = 0.13901979\n",
      "Iteration 470, loss = 0.13880439\n",
      "Iteration 471, loss = 0.13860993\n",
      "Iteration 472, loss = 0.13839042\n",
      "Iteration 473, loss = 0.13819566\n",
      "Iteration 474, loss = 0.13798323\n",
      "Iteration 475, loss = 0.13779563\n",
      "Iteration 476, loss = 0.13761085\n",
      "Iteration 477, loss = 0.13739654\n",
      "Iteration 478, loss = 0.13720980\n",
      "Iteration 479, loss = 0.13699466\n",
      "Iteration 480, loss = 0.13679763\n",
      "Iteration 481, loss = 0.13659173\n",
      "Iteration 482, loss = 0.13641368\n",
      "Iteration 483, loss = 0.13621901\n",
      "Iteration 484, loss = 0.13602265\n",
      "Iteration 485, loss = 0.13580967\n",
      "Iteration 486, loss = 0.13562664\n",
      "Iteration 487, loss = 0.13543605\n",
      "Iteration 488, loss = 0.13526540\n",
      "Iteration 489, loss = 0.13507033\n",
      "Iteration 490, loss = 0.13487312\n",
      "Iteration 491, loss = 0.13468498\n",
      "Iteration 492, loss = 0.13452513\n",
      "Iteration 493, loss = 0.13431921\n",
      "Iteration 494, loss = 0.13413122\n",
      "Iteration 495, loss = 0.13395706\n",
      "Iteration 496, loss = 0.13378663\n",
      "Iteration 497, loss = 0.13358229\n",
      "Iteration 498, loss = 0.13341913\n",
      "Iteration 499, loss = 0.13320322\n",
      "Iteration 500, loss = 0.13302593\n",
      "Iteration 501, loss = 0.13285640\n",
      "Iteration 502, loss = 0.13268368\n",
      "Iteration 503, loss = 0.13249885\n",
      "Iteration 504, loss = 0.13234135\n",
      "Iteration 505, loss = 0.13217822\n",
      "Iteration 506, loss = 0.13198546\n",
      "Iteration 507, loss = 0.13180863\n",
      "Iteration 508, loss = 0.13164532\n",
      "Iteration 509, loss = 0.13148509\n",
      "Iteration 510, loss = 0.13129693\n",
      "Iteration 511, loss = 0.13111554\n",
      "Iteration 512, loss = 0.13093262\n",
      "Iteration 513, loss = 0.13078524\n",
      "Iteration 514, loss = 0.13062605\n",
      "Iteration 515, loss = 0.13046605\n",
      "Iteration 516, loss = 0.13032115\n",
      "Iteration 517, loss = 0.13015032\n",
      "Iteration 518, loss = 0.12997202\n",
      "Iteration 519, loss = 0.12983366\n",
      "Iteration 520, loss = 0.12962843\n",
      "Iteration 521, loss = 0.12945743\n",
      "Iteration 522, loss = 0.12930097\n",
      "Iteration 523, loss = 0.12913443\n",
      "Iteration 524, loss = 0.12898187\n",
      "Iteration 525, loss = 0.12882914\n",
      "Iteration 526, loss = 0.12867395\n",
      "Iteration 527, loss = 0.12851357\n",
      "Iteration 528, loss = 0.12836601\n",
      "Iteration 529, loss = 0.12822894\n",
      "Iteration 530, loss = 0.12807756\n",
      "Iteration 531, loss = 0.12789170\n",
      "Iteration 532, loss = 0.12774395\n",
      "Iteration 533, loss = 0.12758667\n",
      "Iteration 534, loss = 0.12743095\n",
      "Iteration 535, loss = 0.12728109\n",
      "Iteration 536, loss = 0.12713626\n",
      "Iteration 537, loss = 0.12698971\n",
      "Iteration 538, loss = 0.12683006\n",
      "Iteration 539, loss = 0.12669762\n",
      "Iteration 540, loss = 0.12654726\n",
      "Iteration 541, loss = 0.12639494\n",
      "Iteration 542, loss = 0.12625818\n",
      "Iteration 543, loss = 0.12609281\n",
      "Iteration 544, loss = 0.12596042\n",
      "Iteration 545, loss = 0.12581945\n",
      "Iteration 546, loss = 0.12567539\n",
      "Iteration 547, loss = 0.12553328\n",
      "Iteration 548, loss = 0.12540131\n",
      "Iteration 549, loss = 0.12524142\n",
      "Iteration 550, loss = 0.12509973\n",
      "Iteration 551, loss = 0.12494039\n",
      "Iteration 552, loss = 0.12483939\n",
      "Iteration 553, loss = 0.12466073\n",
      "Iteration 554, loss = 0.12452269\n",
      "Iteration 555, loss = 0.12437942\n",
      "Iteration 556, loss = 0.12425400\n",
      "Iteration 557, loss = 0.12410032\n",
      "Iteration 558, loss = 0.12397678\n",
      "Iteration 559, loss = 0.12385362\n",
      "Iteration 560, loss = 0.12371941\n",
      "Iteration 561, loss = 0.12358684\n",
      "Iteration 562, loss = 0.12345020\n",
      "Iteration 563, loss = 0.12331586\n",
      "Iteration 564, loss = 0.12318210\n",
      "Iteration 565, loss = 0.12305757\n",
      "Iteration 566, loss = 0.12292740\n",
      "Iteration 567, loss = 0.12279258\n",
      "Iteration 568, loss = 0.12266316\n",
      "Iteration 569, loss = 0.12254001\n",
      "Iteration 570, loss = 0.12240767\n",
      "Iteration 571, loss = 0.12227943\n",
      "Iteration 572, loss = 0.12216093\n",
      "Iteration 573, loss = 0.12202078\n",
      "Iteration 574, loss = 0.12189864\n",
      "Iteration 575, loss = 0.12178700\n",
      "Iteration 576, loss = 0.12165127\n",
      "Iteration 577, loss = 0.12151727\n",
      "Iteration 578, loss = 0.12137965\n",
      "Iteration 579, loss = 0.12125850\n",
      "Iteration 580, loss = 0.12113525\n",
      "Iteration 581, loss = 0.12102616\n",
      "Iteration 582, loss = 0.12089947\n",
      "Iteration 583, loss = 0.12077578\n",
      "Iteration 584, loss = 0.12066218\n",
      "Iteration 585, loss = 0.12053480\n",
      "Iteration 586, loss = 0.12041403\n",
      "Iteration 587, loss = 0.12028830\n",
      "Iteration 588, loss = 0.12015998\n",
      "Iteration 589, loss = 0.12004416\n",
      "Iteration 590, loss = 0.11992458\n",
      "Iteration 591, loss = 0.11980208\n",
      "Iteration 592, loss = 0.11969036\n",
      "Iteration 593, loss = 0.11956500\n",
      "Iteration 594, loss = 0.11944689\n",
      "Iteration 595, loss = 0.11933732\n",
      "Iteration 596, loss = 0.11921968\n",
      "Iteration 597, loss = 0.11910298\n",
      "Iteration 598, loss = 0.11898826\n",
      "Iteration 599, loss = 0.11888750\n",
      "Iteration 600, loss = 0.11876475\n",
      "Iteration 601, loss = 0.11865113\n",
      "Iteration 602, loss = 0.11853918\n",
      "Iteration 603, loss = 0.11843517\n",
      "Iteration 604, loss = 0.11831952\n",
      "Iteration 605, loss = 0.11821594\n",
      "Iteration 606, loss = 0.11810663\n",
      "Iteration 607, loss = 0.11798655\n",
      "Iteration 608, loss = 0.11788334\n",
      "Iteration 609, loss = 0.11777397\n",
      "Iteration 610, loss = 0.11766852\n",
      "Iteration 611, loss = 0.11755558\n",
      "Iteration 612, loss = 0.11745780\n",
      "Iteration 613, loss = 0.11734505\n",
      "Iteration 614, loss = 0.11723877\n",
      "Iteration 615, loss = 0.11713974\n",
      "Iteration 616, loss = 0.11701345\n",
      "Iteration 617, loss = 0.11691080\n",
      "Iteration 618, loss = 0.11680178\n",
      "Iteration 619, loss = 0.11669749\n",
      "Iteration 620, loss = 0.11659182\n",
      "Iteration 621, loss = 0.11648847\n",
      "Iteration 622, loss = 0.11639395\n",
      "Iteration 623, loss = 0.11628333\n",
      "Iteration 624, loss = 0.11617739\n",
      "Iteration 625, loss = 0.11607741\n",
      "Iteration 626, loss = 0.11598119\n",
      "Iteration 627, loss = 0.11587175\n",
      "Iteration 628, loss = 0.11576600\n",
      "Iteration 629, loss = 0.11566220\n",
      "Iteration 630, loss = 0.11557383\n",
      "Iteration 631, loss = 0.11547000\n",
      "Iteration 632, loss = 0.11536386\n",
      "Iteration 633, loss = 0.11526647\n",
      "Iteration 634, loss = 0.11516985\n",
      "Iteration 635, loss = 0.11507737\n",
      "Iteration 636, loss = 0.11498208\n",
      "Iteration 637, loss = 0.11487733\n",
      "Iteration 638, loss = 0.11478119\n",
      "Iteration 639, loss = 0.11468998\n",
      "Iteration 640, loss = 0.11458305\n",
      "Iteration 641, loss = 0.11449789\n",
      "Iteration 642, loss = 0.11440643\n",
      "Iteration 643, loss = 0.11429416\n",
      "Iteration 644, loss = 0.11420618\n",
      "Iteration 645, loss = 0.11411405\n",
      "Iteration 646, loss = 0.11401201\n",
      "Iteration 647, loss = 0.11392092\n",
      "Iteration 648, loss = 0.11381489\n",
      "Iteration 649, loss = 0.11371021\n",
      "Iteration 650, loss = 0.11362015\n",
      "Iteration 651, loss = 0.11352061\n",
      "Iteration 652, loss = 0.11342452\n",
      "Iteration 653, loss = 0.11332195\n",
      "Iteration 654, loss = 0.11323643\n",
      "Iteration 655, loss = 0.11313492\n",
      "Iteration 656, loss = 0.11304155\n",
      "Iteration 657, loss = 0.11294485\n",
      "Iteration 658, loss = 0.11285639\n",
      "Iteration 659, loss = 0.11277294\n",
      "Iteration 660, loss = 0.11268418\n",
      "Iteration 661, loss = 0.11259147\n",
      "Iteration 662, loss = 0.11251044\n",
      "Iteration 663, loss = 0.11240666\n",
      "Iteration 664, loss = 0.11234321\n",
      "Iteration 665, loss = 0.11224010\n",
      "Iteration 666, loss = 0.11214133\n",
      "Iteration 667, loss = 0.11205681\n",
      "Iteration 668, loss = 0.11197260\n",
      "Iteration 669, loss = 0.11188144\n",
      "Iteration 670, loss = 0.11179864\n",
      "Iteration 671, loss = 0.11171708\n",
      "Iteration 672, loss = 0.11162095\n",
      "Iteration 673, loss = 0.11153844\n",
      "Iteration 674, loss = 0.11145672\n",
      "Iteration 675, loss = 0.11136802\n",
      "Iteration 676, loss = 0.11127510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.42722985\n",
      "Iteration 2, loss = 2.40181490\n",
      "Iteration 3, loss = 2.36998687\n",
      "Iteration 4, loss = 2.33858129\n",
      "Iteration 5, loss = 2.31068125\n",
      "Iteration 6, loss = 2.28394248\n",
      "Iteration 7, loss = 2.25901177\n",
      "Iteration 8, loss = 2.23524942\n",
      "Iteration 9, loss = 2.21200800\n",
      "Iteration 10, loss = 2.18847009\n",
      "Iteration 11, loss = 2.16478150\n",
      "Iteration 12, loss = 2.14084934\n",
      "Iteration 13, loss = 2.11672782\n",
      "Iteration 14, loss = 2.09215140\n",
      "Iteration 15, loss = 2.06726007\n",
      "Iteration 16, loss = 2.04198038\n",
      "Iteration 17, loss = 2.01663737\n",
      "Iteration 18, loss = 1.99108042\n",
      "Iteration 19, loss = 1.96520650\n",
      "Iteration 20, loss = 1.93864823\n",
      "Iteration 21, loss = 1.91186589\n",
      "Iteration 22, loss = 1.88471267\n",
      "Iteration 23, loss = 1.85728353\n",
      "Iteration 24, loss = 1.82964260\n",
      "Iteration 25, loss = 1.80176396\n",
      "Iteration 26, loss = 1.77371950\n",
      "Iteration 27, loss = 1.74539935\n",
      "Iteration 28, loss = 1.71718612\n",
      "Iteration 29, loss = 1.68906524\n",
      "Iteration 30, loss = 1.66079082\n",
      "Iteration 31, loss = 1.63228411\n",
      "Iteration 32, loss = 1.60411847\n",
      "Iteration 33, loss = 1.57604376\n",
      "Iteration 34, loss = 1.54806465\n",
      "Iteration 35, loss = 1.52008810\n",
      "Iteration 36, loss = 1.49211108\n",
      "Iteration 37, loss = 1.46482249\n",
      "Iteration 38, loss = 1.43775606\n",
      "Iteration 39, loss = 1.41112248\n",
      "Iteration 40, loss = 1.38514102\n",
      "Iteration 41, loss = 1.35959892\n",
      "Iteration 42, loss = 1.33418942\n",
      "Iteration 43, loss = 1.30945550\n",
      "Iteration 44, loss = 1.28531228\n",
      "Iteration 45, loss = 1.26134893\n",
      "Iteration 46, loss = 1.23792266\n",
      "Iteration 47, loss = 1.21525690\n",
      "Iteration 48, loss = 1.19317736\n",
      "Iteration 49, loss = 1.17113222\n",
      "Iteration 50, loss = 1.14994672\n",
      "Iteration 51, loss = 1.12899053\n",
      "Iteration 52, loss = 1.10864776\n",
      "Iteration 53, loss = 1.08887078\n",
      "Iteration 54, loss = 1.06958790\n",
      "Iteration 55, loss = 1.05081185\n",
      "Iteration 56, loss = 1.03234803\n",
      "Iteration 57, loss = 1.01438525\n",
      "Iteration 58, loss = 0.99699782\n",
      "Iteration 59, loss = 0.98006242\n",
      "Iteration 60, loss = 0.96360532\n",
      "Iteration 61, loss = 0.94751223\n",
      "Iteration 62, loss = 0.93191721\n",
      "Iteration 63, loss = 0.91681881\n",
      "Iteration 64, loss = 0.90206989\n",
      "Iteration 65, loss = 0.88763054\n",
      "Iteration 66, loss = 0.87374731\n",
      "Iteration 67, loss = 0.86014738\n",
      "Iteration 68, loss = 0.84692351\n",
      "Iteration 69, loss = 0.83417502\n",
      "Iteration 70, loss = 0.82175558\n",
      "Iteration 71, loss = 0.80980112\n",
      "Iteration 72, loss = 0.79799705\n",
      "Iteration 73, loss = 0.78631701\n",
      "Iteration 74, loss = 0.77525271\n",
      "Iteration 75, loss = 0.76432006\n",
      "Iteration 76, loss = 0.75375833\n",
      "Iteration 77, loss = 0.74333864\n",
      "Iteration 78, loss = 0.73302687\n",
      "Iteration 79, loss = 0.72313699\n",
      "Iteration 80, loss = 0.71369623\n",
      "Iteration 81, loss = 0.70449483\n",
      "Iteration 82, loss = 0.69542961\n",
      "Iteration 83, loss = 0.68650590\n",
      "Iteration 84, loss = 0.67774982\n",
      "Iteration 85, loss = 0.66914570\n",
      "Iteration 86, loss = 0.66098064\n",
      "Iteration 87, loss = 0.65303599\n",
      "Iteration 88, loss = 0.64524059\n",
      "Iteration 89, loss = 0.63750986\n",
      "Iteration 90, loss = 0.63016047\n",
      "Iteration 91, loss = 0.62291716\n",
      "Iteration 92, loss = 0.61563961\n",
      "Iteration 93, loss = 0.60868677\n",
      "Iteration 94, loss = 0.60204400\n",
      "Iteration 95, loss = 0.59551865\n",
      "Iteration 96, loss = 0.58896707\n",
      "Iteration 97, loss = 0.58264991\n",
      "Iteration 98, loss = 0.57649283\n",
      "Iteration 99, loss = 0.57041068\n",
      "Iteration 100, loss = 0.56463146\n",
      "Iteration 101, loss = 0.55884014\n",
      "Iteration 102, loss = 0.55300093\n",
      "Iteration 103, loss = 0.54752137\n",
      "Iteration 104, loss = 0.54195561\n",
      "Iteration 105, loss = 0.53666167\n",
      "Iteration 106, loss = 0.53133822\n",
      "Iteration 107, loss = 0.52645888\n",
      "Iteration 108, loss = 0.52140256\n",
      "Iteration 109, loss = 0.51642789\n",
      "Iteration 110, loss = 0.51159225\n",
      "Iteration 111, loss = 0.50694241\n",
      "Iteration 112, loss = 0.50215087\n",
      "Iteration 113, loss = 0.49768736\n",
      "Iteration 114, loss = 0.49322594\n",
      "Iteration 115, loss = 0.48886758\n",
      "Iteration 116, loss = 0.48446367\n",
      "Iteration 117, loss = 0.48016665\n",
      "Iteration 118, loss = 0.47608541\n",
      "Iteration 119, loss = 0.47203768\n",
      "Iteration 120, loss = 0.46806718\n",
      "Iteration 121, loss = 0.46418524\n",
      "Iteration 122, loss = 0.46041560\n",
      "Iteration 123, loss = 0.45664993\n",
      "Iteration 124, loss = 0.45286648\n",
      "Iteration 125, loss = 0.44921967\n",
      "Iteration 126, loss = 0.44579066\n",
      "Iteration 127, loss = 0.44222161\n",
      "Iteration 128, loss = 0.43882020\n",
      "Iteration 129, loss = 0.43535011\n",
      "Iteration 130, loss = 0.43205684\n",
      "Iteration 131, loss = 0.42866572\n",
      "Iteration 132, loss = 0.42551614\n",
      "Iteration 133, loss = 0.42221021\n",
      "Iteration 134, loss = 0.41903621\n",
      "Iteration 135, loss = 0.41591794\n",
      "Iteration 136, loss = 0.41291930\n",
      "Iteration 137, loss = 0.40995490\n",
      "Iteration 138, loss = 0.40706509\n",
      "Iteration 139, loss = 0.40434176\n",
      "Iteration 140, loss = 0.40143676\n",
      "Iteration 141, loss = 0.39868055\n",
      "Iteration 142, loss = 0.39585718\n",
      "Iteration 143, loss = 0.39318846\n",
      "Iteration 144, loss = 0.39053519\n",
      "Iteration 145, loss = 0.38788015\n",
      "Iteration 146, loss = 0.38536814\n",
      "Iteration 147, loss = 0.38291150\n",
      "Iteration 148, loss = 0.38033255\n",
      "Iteration 149, loss = 0.37787945\n",
      "Iteration 150, loss = 0.37537390\n",
      "Iteration 151, loss = 0.37291832\n",
      "Iteration 152, loss = 0.37038397\n",
      "Iteration 153, loss = 0.36808353\n",
      "Iteration 154, loss = 0.36590286\n",
      "Iteration 155, loss = 0.36353765\n",
      "Iteration 156, loss = 0.36128280\n",
      "Iteration 157, loss = 0.35904289\n",
      "Iteration 158, loss = 0.35699155\n",
      "Iteration 159, loss = 0.35480646\n",
      "Iteration 160, loss = 0.35272770\n",
      "Iteration 161, loss = 0.35056187\n",
      "Iteration 162, loss = 0.34858021\n",
      "Iteration 163, loss = 0.34657263\n",
      "Iteration 164, loss = 0.34455080\n",
      "Iteration 165, loss = 0.34251114\n",
      "Iteration 166, loss = 0.34055418\n",
      "Iteration 167, loss = 0.33862370\n",
      "Iteration 168, loss = 0.33674917\n",
      "Iteration 169, loss = 0.33484644\n",
      "Iteration 170, loss = 0.33297869\n",
      "Iteration 171, loss = 0.33115294\n",
      "Iteration 172, loss = 0.32926618\n",
      "Iteration 173, loss = 0.32752684\n",
      "Iteration 174, loss = 0.32575754\n",
      "Iteration 175, loss = 0.32398347\n",
      "Iteration 176, loss = 0.32219090\n",
      "Iteration 177, loss = 0.32044416\n",
      "Iteration 178, loss = 0.31871537\n",
      "Iteration 179, loss = 0.31699983\n",
      "Iteration 180, loss = 0.31542975\n",
      "Iteration 181, loss = 0.31383264\n",
      "Iteration 182, loss = 0.31214818\n",
      "Iteration 183, loss = 0.31054169\n",
      "Iteration 184, loss = 0.30898820\n",
      "Iteration 185, loss = 0.30746667\n",
      "Iteration 186, loss = 0.30596104\n",
      "Iteration 187, loss = 0.30452363\n",
      "Iteration 188, loss = 0.30295498\n",
      "Iteration 189, loss = 0.30146626\n",
      "Iteration 190, loss = 0.30003685\n",
      "Iteration 191, loss = 0.29853620\n",
      "Iteration 192, loss = 0.29713509\n",
      "Iteration 193, loss = 0.29575064\n",
      "Iteration 194, loss = 0.29431335\n",
      "Iteration 195, loss = 0.29288636\n",
      "Iteration 196, loss = 0.29160790\n",
      "Iteration 197, loss = 0.29020291\n",
      "Iteration 198, loss = 0.28880635\n",
      "Iteration 199, loss = 0.28749326\n",
      "Iteration 200, loss = 0.28613778\n",
      "Iteration 201, loss = 0.28483698\n",
      "Iteration 202, loss = 0.28356375\n",
      "Iteration 203, loss = 0.28232552\n",
      "Iteration 204, loss = 0.28107213\n",
      "Iteration 205, loss = 0.27973384\n",
      "Iteration 206, loss = 0.27847046\n",
      "Iteration 207, loss = 0.27726777\n",
      "Iteration 208, loss = 0.27606442\n",
      "Iteration 209, loss = 0.27488911\n",
      "Iteration 210, loss = 0.27373472\n",
      "Iteration 211, loss = 0.27241768\n",
      "Iteration 212, loss = 0.27134633\n",
      "Iteration 213, loss = 0.27022239\n",
      "Iteration 214, loss = 0.26898686\n",
      "Iteration 215, loss = 0.26787921\n",
      "Iteration 216, loss = 0.26673421\n",
      "Iteration 217, loss = 0.26565422\n",
      "Iteration 218, loss = 0.26459174\n",
      "Iteration 219, loss = 0.26338548\n",
      "Iteration 220, loss = 0.26228038\n",
      "Iteration 221, loss = 0.26118444\n",
      "Iteration 222, loss = 0.26013740\n",
      "Iteration 223, loss = 0.25904952\n",
      "Iteration 224, loss = 0.25800552\n",
      "Iteration 225, loss = 0.25705029\n",
      "Iteration 226, loss = 0.25599653\n",
      "Iteration 227, loss = 0.25505399\n",
      "Iteration 228, loss = 0.25403690\n",
      "Iteration 229, loss = 0.25306132\n",
      "Iteration 230, loss = 0.25206166\n",
      "Iteration 231, loss = 0.25113683\n",
      "Iteration 232, loss = 0.25014520\n",
      "Iteration 233, loss = 0.24919167\n",
      "Iteration 234, loss = 0.24823845\n",
      "Iteration 235, loss = 0.24726709\n",
      "Iteration 236, loss = 0.24634653\n",
      "Iteration 237, loss = 0.24542740\n",
      "Iteration 238, loss = 0.24451782\n",
      "Iteration 239, loss = 0.24360037\n",
      "Iteration 240, loss = 0.24272974\n",
      "Iteration 241, loss = 0.24180975\n",
      "Iteration 242, loss = 0.24085244\n",
      "Iteration 243, loss = 0.24003188\n",
      "Iteration 244, loss = 0.23916937\n",
      "Iteration 245, loss = 0.23831805\n",
      "Iteration 246, loss = 0.23745990\n",
      "Iteration 247, loss = 0.23662843\n",
      "Iteration 248, loss = 0.23576381\n",
      "Iteration 249, loss = 0.23498483\n",
      "Iteration 250, loss = 0.23414685\n",
      "Iteration 251, loss = 0.23335004\n",
      "Iteration 252, loss = 0.23252616\n",
      "Iteration 253, loss = 0.23173442\n",
      "Iteration 254, loss = 0.23094180\n",
      "Iteration 255, loss = 0.23013460\n",
      "Iteration 256, loss = 0.22945932\n",
      "Iteration 257, loss = 0.22860786\n",
      "Iteration 258, loss = 0.22786450\n",
      "Iteration 259, loss = 0.22703954\n",
      "Iteration 260, loss = 0.22629765\n",
      "Iteration 261, loss = 0.22555728\n",
      "Iteration 262, loss = 0.22474750\n",
      "Iteration 263, loss = 0.22404579\n",
      "Iteration 264, loss = 0.22334771\n",
      "Iteration 265, loss = 0.22256474\n",
      "Iteration 266, loss = 0.22185447\n",
      "Iteration 267, loss = 0.22117233\n",
      "Iteration 268, loss = 0.22047345\n",
      "Iteration 269, loss = 0.21974635\n",
      "Iteration 270, loss = 0.21900820\n",
      "Iteration 271, loss = 0.21829774\n",
      "Iteration 272, loss = 0.21764360\n",
      "Iteration 273, loss = 0.21698675\n",
      "Iteration 274, loss = 0.21629717\n",
      "Iteration 275, loss = 0.21558275\n",
      "Iteration 276, loss = 0.21492127\n",
      "Iteration 277, loss = 0.21419410\n",
      "Iteration 278, loss = 0.21361123\n",
      "Iteration 279, loss = 0.21297887\n",
      "Iteration 280, loss = 0.21232263\n",
      "Iteration 281, loss = 0.21165833\n",
      "Iteration 282, loss = 0.21097312\n",
      "Iteration 283, loss = 0.21033729\n",
      "Iteration 284, loss = 0.20972321\n",
      "Iteration 285, loss = 0.20906452\n",
      "Iteration 286, loss = 0.20843819\n",
      "Iteration 287, loss = 0.20781719\n",
      "Iteration 288, loss = 0.20725333\n",
      "Iteration 289, loss = 0.20664071\n",
      "Iteration 290, loss = 0.20604691\n",
      "Iteration 291, loss = 0.20544844\n",
      "Iteration 292, loss = 0.20489746\n",
      "Iteration 293, loss = 0.20436885\n",
      "Iteration 294, loss = 0.20371923\n",
      "Iteration 295, loss = 0.20308384\n",
      "Iteration 296, loss = 0.20251109\n",
      "Iteration 297, loss = 0.20195240\n",
      "Iteration 298, loss = 0.20138064\n",
      "Iteration 299, loss = 0.20078736\n",
      "Iteration 300, loss = 0.20021038\n",
      "Iteration 301, loss = 0.19964525\n",
      "Iteration 302, loss = 0.19908765\n",
      "Iteration 303, loss = 0.19854106\n",
      "Iteration 304, loss = 0.19795701\n",
      "Iteration 305, loss = 0.19747858\n",
      "Iteration 306, loss = 0.19695003\n",
      "Iteration 307, loss = 0.19639176\n",
      "Iteration 308, loss = 0.19586051\n",
      "Iteration 309, loss = 0.19538364\n",
      "Iteration 310, loss = 0.19481386\n",
      "Iteration 311, loss = 0.19427193\n",
      "Iteration 312, loss = 0.19378168\n",
      "Iteration 313, loss = 0.19329525\n",
      "Iteration 314, loss = 0.19285011\n",
      "Iteration 315, loss = 0.19235880\n",
      "Iteration 316, loss = 0.19184565\n",
      "Iteration 317, loss = 0.19130982\n",
      "Iteration 318, loss = 0.19079804\n",
      "Iteration 319, loss = 0.19029604\n",
      "Iteration 320, loss = 0.18978587\n",
      "Iteration 321, loss = 0.18933761\n",
      "Iteration 322, loss = 0.18884286\n",
      "Iteration 323, loss = 0.18835121\n",
      "Iteration 324, loss = 0.18789131\n",
      "Iteration 325, loss = 0.18744837\n",
      "Iteration 326, loss = 0.18695850\n",
      "Iteration 327, loss = 0.18652286\n",
      "Iteration 328, loss = 0.18607467\n",
      "Iteration 329, loss = 0.18561243\n",
      "Iteration 330, loss = 0.18515013\n",
      "Iteration 331, loss = 0.18471500\n",
      "Iteration 332, loss = 0.18427707\n",
      "Iteration 333, loss = 0.18383945\n",
      "Iteration 334, loss = 0.18337566\n",
      "Iteration 335, loss = 0.18293061\n",
      "Iteration 336, loss = 0.18247883\n",
      "Iteration 337, loss = 0.18207062\n",
      "Iteration 338, loss = 0.18164564\n",
      "Iteration 339, loss = 0.18121208\n",
      "Iteration 340, loss = 0.18080494\n",
      "Iteration 341, loss = 0.18031812\n",
      "Iteration 342, loss = 0.17993134\n",
      "Iteration 343, loss = 0.17950130\n",
      "Iteration 344, loss = 0.17906432\n",
      "Iteration 345, loss = 0.17862085\n",
      "Iteration 346, loss = 0.17822898\n",
      "Iteration 347, loss = 0.17786384\n",
      "Iteration 348, loss = 0.17743512\n",
      "Iteration 349, loss = 0.17702252\n",
      "Iteration 350, loss = 0.17664176\n",
      "Iteration 351, loss = 0.17624505\n",
      "Iteration 352, loss = 0.17582809\n",
      "Iteration 353, loss = 0.17544378\n",
      "Iteration 354, loss = 0.17506394\n",
      "Iteration 355, loss = 0.17463370\n",
      "Iteration 356, loss = 0.17424143\n",
      "Iteration 357, loss = 0.17382579\n",
      "Iteration 358, loss = 0.17346048\n",
      "Iteration 359, loss = 0.17313429\n",
      "Iteration 360, loss = 0.17277128\n",
      "Iteration 361, loss = 0.17239563\n",
      "Iteration 362, loss = 0.17201089\n",
      "Iteration 363, loss = 0.17161005\n",
      "Iteration 364, loss = 0.17118099\n",
      "Iteration 365, loss = 0.17079126\n",
      "Iteration 366, loss = 0.17041931\n",
      "Iteration 367, loss = 0.17004278\n",
      "Iteration 368, loss = 0.16969748\n",
      "Iteration 369, loss = 0.16935044\n",
      "Iteration 370, loss = 0.16898383\n",
      "Iteration 371, loss = 0.16863965\n",
      "Iteration 372, loss = 0.16825710\n",
      "Iteration 373, loss = 0.16789673\n",
      "Iteration 374, loss = 0.16755805\n",
      "Iteration 375, loss = 0.16722206\n",
      "Iteration 376, loss = 0.16692789\n",
      "Iteration 377, loss = 0.16654062\n",
      "Iteration 378, loss = 0.16618233\n",
      "Iteration 379, loss = 0.16580436\n",
      "Iteration 380, loss = 0.16546455\n",
      "Iteration 381, loss = 0.16513929\n",
      "Iteration 382, loss = 0.16477292\n",
      "Iteration 383, loss = 0.16445231\n",
      "Iteration 384, loss = 0.16413686\n",
      "Iteration 385, loss = 0.16380587\n",
      "Iteration 386, loss = 0.16349463\n",
      "Iteration 387, loss = 0.16316528\n",
      "Iteration 388, loss = 0.16287643\n",
      "Iteration 389, loss = 0.16256321\n",
      "Iteration 390, loss = 0.16218670\n",
      "Iteration 391, loss = 0.16183735\n",
      "Iteration 392, loss = 0.16153562\n",
      "Iteration 393, loss = 0.16122530\n",
      "Iteration 394, loss = 0.16093619\n",
      "Iteration 395, loss = 0.16059731\n",
      "Iteration 396, loss = 0.16027405\n",
      "Iteration 397, loss = 0.15998098\n",
      "Iteration 398, loss = 0.15970739\n",
      "Iteration 399, loss = 0.15934097\n",
      "Iteration 400, loss = 0.15906449\n",
      "Iteration 401, loss = 0.15877756\n",
      "Iteration 402, loss = 0.15849939\n",
      "Iteration 403, loss = 0.15823161\n",
      "Iteration 404, loss = 0.15795671\n",
      "Iteration 405, loss = 0.15762714\n",
      "Iteration 406, loss = 0.15727775\n",
      "Iteration 407, loss = 0.15697407\n",
      "Iteration 408, loss = 0.15666920\n",
      "Iteration 409, loss = 0.15639979\n",
      "Iteration 410, loss = 0.15609033\n",
      "Iteration 411, loss = 0.15579916\n",
      "Iteration 412, loss = 0.15551248\n",
      "Iteration 413, loss = 0.15523155\n",
      "Iteration 414, loss = 0.15493389\n",
      "Iteration 415, loss = 0.15465845\n",
      "Iteration 416, loss = 0.15437316\n",
      "Iteration 417, loss = 0.15413089\n",
      "Iteration 418, loss = 0.15383234\n",
      "Iteration 419, loss = 0.15356976\n",
      "Iteration 420, loss = 0.15329994\n",
      "Iteration 421, loss = 0.15305115\n",
      "Iteration 422, loss = 0.15278029\n",
      "Iteration 423, loss = 0.15255053\n",
      "Iteration 424, loss = 0.15228775\n",
      "Iteration 425, loss = 0.15201174\n",
      "Iteration 426, loss = 0.15174849\n",
      "Iteration 427, loss = 0.15148371\n",
      "Iteration 428, loss = 0.15122144\n",
      "Iteration 429, loss = 0.15097401\n",
      "Iteration 430, loss = 0.15070557\n",
      "Iteration 431, loss = 0.15044347\n",
      "Iteration 432, loss = 0.15019422\n",
      "Iteration 433, loss = 0.14993811\n",
      "Iteration 434, loss = 0.14969936\n",
      "Iteration 435, loss = 0.14945171\n",
      "Iteration 436, loss = 0.14919637\n",
      "Iteration 437, loss = 0.14895517\n",
      "Iteration 438, loss = 0.14871725\n",
      "Iteration 439, loss = 0.14846701\n",
      "Iteration 440, loss = 0.14822821\n",
      "Iteration 441, loss = 0.14799797\n",
      "Iteration 442, loss = 0.14776109\n",
      "Iteration 443, loss = 0.14749309\n",
      "Iteration 444, loss = 0.14727030\n",
      "Iteration 445, loss = 0.14703352\n",
      "Iteration 446, loss = 0.14680319\n",
      "Iteration 447, loss = 0.14658144\n",
      "Iteration 448, loss = 0.14633087\n",
      "Iteration 449, loss = 0.14609902\n",
      "Iteration 450, loss = 0.14585834\n",
      "Iteration 451, loss = 0.14562969\n",
      "Iteration 452, loss = 0.14540482\n",
      "Iteration 453, loss = 0.14516601\n",
      "Iteration 454, loss = 0.14495861\n",
      "Iteration 455, loss = 0.14475740\n",
      "Iteration 456, loss = 0.14453311\n",
      "Iteration 457, loss = 0.14426922\n",
      "Iteration 458, loss = 0.14405463\n",
      "Iteration 459, loss = 0.14382035\n",
      "Iteration 460, loss = 0.14357095\n",
      "Iteration 461, loss = 0.14333533\n",
      "Iteration 462, loss = 0.14315333\n",
      "Iteration 463, loss = 0.14291789\n",
      "Iteration 464, loss = 0.14273143\n",
      "Iteration 465, loss = 0.14249223\n",
      "Iteration 466, loss = 0.14226424\n",
      "Iteration 467, loss = 0.14203403\n",
      "Iteration 468, loss = 0.14180966\n",
      "Iteration 469, loss = 0.14161310\n",
      "Iteration 470, loss = 0.14139206\n",
      "Iteration 471, loss = 0.14119801\n",
      "Iteration 472, loss = 0.14096221\n",
      "Iteration 473, loss = 0.14074731\n",
      "Iteration 474, loss = 0.14053145\n",
      "Iteration 475, loss = 0.14034553\n",
      "Iteration 476, loss = 0.14014915\n",
      "Iteration 477, loss = 0.13992527\n",
      "Iteration 478, loss = 0.13975251\n",
      "Iteration 479, loss = 0.13951868\n",
      "Iteration 480, loss = 0.13930766\n",
      "Iteration 481, loss = 0.13910231\n",
      "Iteration 482, loss = 0.13891891\n",
      "Iteration 483, loss = 0.13870787\n",
      "Iteration 484, loss = 0.13852180\n",
      "Iteration 485, loss = 0.13831539\n",
      "Iteration 486, loss = 0.13812646\n",
      "Iteration 487, loss = 0.13794960\n",
      "Iteration 488, loss = 0.13776064\n",
      "Iteration 489, loss = 0.13755661\n",
      "Iteration 490, loss = 0.13736218\n",
      "Iteration 491, loss = 0.13716295\n",
      "Iteration 492, loss = 0.13699442\n",
      "Iteration 493, loss = 0.13677650\n",
      "Iteration 494, loss = 0.13657283\n",
      "Iteration 495, loss = 0.13639778\n",
      "Iteration 496, loss = 0.13621767\n",
      "Iteration 497, loss = 0.13601873\n",
      "Iteration 498, loss = 0.13584812\n",
      "Iteration 499, loss = 0.13562632\n",
      "Iteration 500, loss = 0.13545131\n",
      "Iteration 501, loss = 0.13529470\n",
      "Iteration 502, loss = 0.13509584\n",
      "Iteration 503, loss = 0.13491647\n",
      "Iteration 504, loss = 0.13474114\n",
      "Iteration 505, loss = 0.13457127\n",
      "Iteration 506, loss = 0.13437762\n",
      "Iteration 507, loss = 0.13419889\n",
      "Iteration 508, loss = 0.13402708\n",
      "Iteration 509, loss = 0.13385501\n",
      "Iteration 510, loss = 0.13368008\n",
      "Iteration 511, loss = 0.13349269\n",
      "Iteration 512, loss = 0.13331470\n",
      "Iteration 513, loss = 0.13316228\n",
      "Iteration 514, loss = 0.13299312\n",
      "Iteration 515, loss = 0.13282624\n",
      "Iteration 516, loss = 0.13267168\n",
      "Iteration 517, loss = 0.13249337\n",
      "Iteration 518, loss = 0.13231983\n",
      "Iteration 519, loss = 0.13214884\n",
      "Iteration 520, loss = 0.13196323\n",
      "Iteration 521, loss = 0.13178672\n",
      "Iteration 522, loss = 0.13162194\n",
      "Iteration 523, loss = 0.13145242\n",
      "Iteration 524, loss = 0.13130076\n",
      "Iteration 525, loss = 0.13113662\n",
      "Iteration 526, loss = 0.13098446\n",
      "Iteration 527, loss = 0.13081179\n",
      "Iteration 528, loss = 0.13066089\n",
      "Iteration 529, loss = 0.13050938\n",
      "Iteration 530, loss = 0.13034741\n",
      "Iteration 531, loss = 0.13017319\n",
      "Iteration 532, loss = 0.13002632\n",
      "Iteration 533, loss = 0.12985721\n",
      "Iteration 534, loss = 0.12970245\n",
      "Iteration 535, loss = 0.12954945\n",
      "Iteration 536, loss = 0.12940462\n",
      "Iteration 537, loss = 0.12924331\n",
      "Iteration 538, loss = 0.12908570\n",
      "Iteration 539, loss = 0.12894647\n",
      "Iteration 540, loss = 0.12879609\n",
      "Iteration 541, loss = 0.12863378\n",
      "Iteration 542, loss = 0.12849204\n",
      "Iteration 543, loss = 0.12832895\n",
      "Iteration 544, loss = 0.12817805\n",
      "Iteration 545, loss = 0.12802884\n",
      "Iteration 546, loss = 0.12788211\n",
      "Iteration 547, loss = 0.12773568\n",
      "Iteration 548, loss = 0.12759453\n",
      "Iteration 549, loss = 0.12743432\n",
      "Iteration 550, loss = 0.12728450\n",
      "Iteration 551, loss = 0.12712609\n",
      "Iteration 552, loss = 0.12702467\n",
      "Iteration 553, loss = 0.12684044\n",
      "Iteration 554, loss = 0.12669379\n",
      "Iteration 555, loss = 0.12654936\n",
      "Iteration 556, loss = 0.12641786\n",
      "Iteration 557, loss = 0.12626797\n",
      "Iteration 558, loss = 0.12613738\n",
      "Iteration 559, loss = 0.12601344\n",
      "Iteration 560, loss = 0.12586847\n",
      "Iteration 561, loss = 0.12573307\n",
      "Iteration 562, loss = 0.12559122\n",
      "Iteration 563, loss = 0.12544798\n",
      "Iteration 564, loss = 0.12531524\n",
      "Iteration 565, loss = 0.12518230\n",
      "Iteration 566, loss = 0.12504809\n",
      "Iteration 567, loss = 0.12491374\n",
      "Iteration 568, loss = 0.12477610\n",
      "Iteration 569, loss = 0.12464098\n",
      "Iteration 570, loss = 0.12451865\n",
      "Iteration 571, loss = 0.12438763\n",
      "Iteration 572, loss = 0.12427859\n",
      "Iteration 573, loss = 0.12413031\n",
      "Iteration 574, loss = 0.12400400\n",
      "Iteration 575, loss = 0.12388822\n",
      "Iteration 576, loss = 0.12374191\n",
      "Iteration 577, loss = 0.12361113\n",
      "Iteration 578, loss = 0.12347089\n",
      "Iteration 579, loss = 0.12333923\n",
      "Iteration 580, loss = 0.12321674\n",
      "Iteration 581, loss = 0.12309550\n",
      "Iteration 582, loss = 0.12297204\n",
      "Iteration 583, loss = 0.12284845\n",
      "Iteration 584, loss = 0.12272783\n",
      "Iteration 585, loss = 0.12259418\n",
      "Iteration 586, loss = 0.12246087\n",
      "Iteration 587, loss = 0.12234155\n",
      "Iteration 588, loss = 0.12220167\n",
      "Iteration 589, loss = 0.12208009\n",
      "Iteration 590, loss = 0.12196515\n",
      "Iteration 591, loss = 0.12183782\n",
      "Iteration 592, loss = 0.12171782\n",
      "Iteration 593, loss = 0.12159199\n",
      "Iteration 594, loss = 0.12148083\n",
      "Iteration 595, loss = 0.12134691\n",
      "Iteration 596, loss = 0.12123227\n",
      "Iteration 597, loss = 0.12110516\n",
      "Iteration 598, loss = 0.12098707\n",
      "Iteration 599, loss = 0.12087674\n",
      "Iteration 600, loss = 0.12075382\n",
      "Iteration 601, loss = 0.12063750\n",
      "Iteration 602, loss = 0.12052671\n",
      "Iteration 603, loss = 0.12042531\n",
      "Iteration 604, loss = 0.12028690\n",
      "Iteration 605, loss = 0.12018297\n",
      "Iteration 606, loss = 0.12006597\n",
      "Iteration 607, loss = 0.11994872\n",
      "Iteration 608, loss = 0.11984374\n",
      "Iteration 609, loss = 0.11972913\n",
      "Iteration 610, loss = 0.11961699\n",
      "Iteration 611, loss = 0.11950359\n",
      "Iteration 612, loss = 0.11940688\n",
      "Iteration 613, loss = 0.11928471\n",
      "Iteration 614, loss = 0.11917805\n",
      "Iteration 615, loss = 0.11906276\n",
      "Iteration 616, loss = 0.11893959\n",
      "Iteration 617, loss = 0.11883732\n",
      "Iteration 618, loss = 0.11872335\n",
      "Iteration 619, loss = 0.11861603\n",
      "Iteration 620, loss = 0.11851699\n",
      "Iteration 621, loss = 0.11840301\n",
      "Iteration 622, loss = 0.11830604\n",
      "Iteration 623, loss = 0.11819252\n",
      "Iteration 624, loss = 0.11808304\n",
      "Iteration 625, loss = 0.11797928\n",
      "Iteration 626, loss = 0.11788020\n",
      "Iteration 627, loss = 0.11776402\n",
      "Iteration 628, loss = 0.11765981\n",
      "Iteration 629, loss = 0.11754994\n",
      "Iteration 630, loss = 0.11745402\n",
      "Iteration 631, loss = 0.11735095\n",
      "Iteration 632, loss = 0.11723981\n",
      "Iteration 633, loss = 0.11713721\n",
      "Iteration 634, loss = 0.11705009\n",
      "Iteration 635, loss = 0.11693766\n",
      "Iteration 636, loss = 0.11683625\n",
      "Iteration 637, loss = 0.11672373\n",
      "Iteration 638, loss = 0.11662917\n",
      "Iteration 639, loss = 0.11653874\n",
      "Iteration 640, loss = 0.11642362\n",
      "Iteration 641, loss = 0.11633772\n",
      "Iteration 642, loss = 0.11623320\n",
      "Iteration 643, loss = 0.11611411\n",
      "Iteration 644, loss = 0.11602397\n",
      "Iteration 645, loss = 0.11593664\n",
      "Iteration 646, loss = 0.11582243\n",
      "Iteration 647, loss = 0.11573572\n",
      "Iteration 648, loss = 0.11561956\n",
      "Iteration 649, loss = 0.11551678\n",
      "Iteration 650, loss = 0.11542025\n",
      "Iteration 651, loss = 0.11532552\n",
      "Iteration 652, loss = 0.11523769\n",
      "Iteration 653, loss = 0.11513065\n",
      "Iteration 654, loss = 0.11503210\n",
      "Iteration 655, loss = 0.11493661\n",
      "Iteration 656, loss = 0.11484380\n",
      "Iteration 657, loss = 0.11474566\n",
      "Iteration 658, loss = 0.11465061\n",
      "Iteration 659, loss = 0.11456365\n",
      "Iteration 660, loss = 0.11446740\n",
      "Iteration 661, loss = 0.11437039\n",
      "Iteration 662, loss = 0.11428598\n",
      "Iteration 663, loss = 0.11417950\n",
      "Iteration 664, loss = 0.11411948\n",
      "Iteration 665, loss = 0.11401260\n",
      "Iteration 666, loss = 0.11390729\n",
      "Iteration 667, loss = 0.11382271\n",
      "Iteration 668, loss = 0.11373004\n",
      "Iteration 669, loss = 0.11363888\n",
      "Iteration 670, loss = 0.11355578\n",
      "Iteration 671, loss = 0.11347563\n",
      "Iteration 672, loss = 0.11337609\n",
      "Iteration 673, loss = 0.11328753\n",
      "Iteration 674, loss = 0.11319759\n",
      "Iteration 675, loss = 0.11311626\n",
      "Iteration 676, loss = 0.11302106\n",
      "Iteration 677, loss = 0.11293901\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.42601586\n",
      "Iteration 2, loss = 2.40018956\n",
      "Iteration 3, loss = 2.36801360\n",
      "Iteration 4, loss = 2.33606465\n",
      "Iteration 5, loss = 2.30685073\n",
      "Iteration 6, loss = 2.27967099\n",
      "Iteration 7, loss = 2.25399940\n",
      "Iteration 8, loss = 2.22977263\n",
      "Iteration 9, loss = 2.20594718\n",
      "Iteration 10, loss = 2.18165106\n",
      "Iteration 11, loss = 2.15750174\n",
      "Iteration 12, loss = 2.13271112\n",
      "Iteration 13, loss = 2.10779935\n",
      "Iteration 14, loss = 2.08250525\n",
      "Iteration 15, loss = 2.05689797\n",
      "Iteration 16, loss = 2.03086566\n",
      "Iteration 17, loss = 2.00453689\n",
      "Iteration 18, loss = 1.97787640\n",
      "Iteration 19, loss = 1.95088547\n",
      "Iteration 20, loss = 1.92326652\n",
      "Iteration 21, loss = 1.89553653\n",
      "Iteration 22, loss = 1.86747339\n",
      "Iteration 23, loss = 1.83924348\n",
      "Iteration 24, loss = 1.81067534\n",
      "Iteration 25, loss = 1.78196149\n",
      "Iteration 26, loss = 1.75326700\n",
      "Iteration 27, loss = 1.72418819\n",
      "Iteration 28, loss = 1.69539354\n",
      "Iteration 29, loss = 1.66666446\n",
      "Iteration 30, loss = 1.63765862\n",
      "Iteration 31, loss = 1.60842992\n",
      "Iteration 32, loss = 1.57955692\n",
      "Iteration 33, loss = 1.55110223\n",
      "Iteration 34, loss = 1.52273795\n",
      "Iteration 35, loss = 1.49438878\n",
      "Iteration 36, loss = 1.46634615\n",
      "Iteration 37, loss = 1.43887901\n",
      "Iteration 38, loss = 1.41167142\n",
      "Iteration 39, loss = 1.38484160\n",
      "Iteration 40, loss = 1.35856011\n",
      "Iteration 41, loss = 1.33265129\n",
      "Iteration 42, loss = 1.30695979\n",
      "Iteration 43, loss = 1.28200990\n",
      "Iteration 44, loss = 1.25748790\n",
      "Iteration 45, loss = 1.23340341\n",
      "Iteration 46, loss = 1.20962279\n",
      "Iteration 47, loss = 1.18669926\n",
      "Iteration 48, loss = 1.16406590\n",
      "Iteration 49, loss = 1.14166335\n",
      "Iteration 50, loss = 1.11971619\n",
      "Iteration 51, loss = 1.09855349\n",
      "Iteration 52, loss = 1.07773500\n",
      "Iteration 53, loss = 1.05760486\n",
      "Iteration 54, loss = 1.03796108\n",
      "Iteration 55, loss = 1.01860571\n",
      "Iteration 56, loss = 0.99981001\n",
      "Iteration 57, loss = 0.98170049\n",
      "Iteration 58, loss = 0.96395520\n",
      "Iteration 59, loss = 0.94669539\n",
      "Iteration 60, loss = 0.92987590\n",
      "Iteration 61, loss = 0.91349423\n",
      "Iteration 62, loss = 0.89783780\n",
      "Iteration 63, loss = 0.88227554\n",
      "Iteration 64, loss = 0.86716219\n",
      "Iteration 65, loss = 0.85252688\n",
      "Iteration 66, loss = 0.83844886\n",
      "Iteration 67, loss = 0.82455918\n",
      "Iteration 68, loss = 0.81104765\n",
      "Iteration 69, loss = 0.79813783\n",
      "Iteration 70, loss = 0.78548818\n",
      "Iteration 71, loss = 0.77341131\n",
      "Iteration 72, loss = 0.76135865\n",
      "Iteration 73, loss = 0.74949615\n",
      "Iteration 74, loss = 0.73823283\n",
      "Iteration 75, loss = 0.72695257\n",
      "Iteration 76, loss = 0.71626310\n",
      "Iteration 77, loss = 0.70566728\n",
      "Iteration 78, loss = 0.69548957\n",
      "Iteration 79, loss = 0.68549025\n",
      "Iteration 80, loss = 0.67591578\n",
      "Iteration 81, loss = 0.66647656\n",
      "Iteration 82, loss = 0.65731620\n",
      "Iteration 83, loss = 0.64842163\n",
      "Iteration 84, loss = 0.63968546\n",
      "Iteration 85, loss = 0.63111919\n",
      "Iteration 86, loss = 0.62284113\n",
      "Iteration 87, loss = 0.61485564\n",
      "Iteration 88, loss = 0.60701750\n",
      "Iteration 89, loss = 0.59928448\n",
      "Iteration 90, loss = 0.59177169\n",
      "Iteration 91, loss = 0.58454111\n",
      "Iteration 92, loss = 0.57745560\n",
      "Iteration 93, loss = 0.57056833\n",
      "Iteration 94, loss = 0.56382097\n",
      "Iteration 95, loss = 0.55726621\n",
      "Iteration 96, loss = 0.55074703\n",
      "Iteration 97, loss = 0.54452549\n",
      "Iteration 98, loss = 0.53830550\n",
      "Iteration 99, loss = 0.53230358\n",
      "Iteration 100, loss = 0.52644324\n",
      "Iteration 101, loss = 0.52074808\n",
      "Iteration 102, loss = 0.51489116\n",
      "Iteration 103, loss = 0.50942771\n",
      "Iteration 104, loss = 0.50395962\n",
      "Iteration 105, loss = 0.49868686\n",
      "Iteration 106, loss = 0.49353762\n",
      "Iteration 107, loss = 0.48865061\n",
      "Iteration 108, loss = 0.48375484\n",
      "Iteration 109, loss = 0.47883903\n",
      "Iteration 110, loss = 0.47413557\n",
      "Iteration 111, loss = 0.46961532\n",
      "Iteration 112, loss = 0.46490237\n",
      "Iteration 113, loss = 0.46045089\n",
      "Iteration 114, loss = 0.45614547\n",
      "Iteration 115, loss = 0.45189459\n",
      "Iteration 116, loss = 0.44758806\n",
      "Iteration 117, loss = 0.44351330\n",
      "Iteration 118, loss = 0.43950843\n",
      "Iteration 119, loss = 0.43563656\n",
      "Iteration 120, loss = 0.43180125\n",
      "Iteration 121, loss = 0.42802785\n",
      "Iteration 122, loss = 0.42419605\n",
      "Iteration 123, loss = 0.42060744\n",
      "Iteration 124, loss = 0.41697974\n",
      "Iteration 125, loss = 0.41345152\n",
      "Iteration 126, loss = 0.41006995\n",
      "Iteration 127, loss = 0.40674116\n",
      "Iteration 128, loss = 0.40325337\n",
      "Iteration 129, loss = 0.39995440\n",
      "Iteration 130, loss = 0.39673114\n",
      "Iteration 131, loss = 0.39355325\n",
      "Iteration 132, loss = 0.39045881\n",
      "Iteration 133, loss = 0.38738944\n",
      "Iteration 134, loss = 0.38437939\n",
      "Iteration 135, loss = 0.38137758\n",
      "Iteration 136, loss = 0.37853734\n",
      "Iteration 137, loss = 0.37568461\n",
      "Iteration 138, loss = 0.37299014\n",
      "Iteration 139, loss = 0.37024350\n",
      "Iteration 140, loss = 0.36753493\n",
      "Iteration 141, loss = 0.36491967\n",
      "Iteration 142, loss = 0.36229130\n",
      "Iteration 143, loss = 0.35969239\n",
      "Iteration 144, loss = 0.35712299\n",
      "Iteration 145, loss = 0.35460705\n",
      "Iteration 146, loss = 0.35216191\n",
      "Iteration 147, loss = 0.34974925\n",
      "Iteration 148, loss = 0.34729686\n",
      "Iteration 149, loss = 0.34494222\n",
      "Iteration 150, loss = 0.34265712\n",
      "Iteration 151, loss = 0.34040484\n",
      "Iteration 152, loss = 0.33805426\n",
      "Iteration 153, loss = 0.33586130\n",
      "Iteration 154, loss = 0.33364936\n",
      "Iteration 155, loss = 0.33145612\n",
      "Iteration 156, loss = 0.32933062\n",
      "Iteration 157, loss = 0.32719334\n",
      "Iteration 158, loss = 0.32513981\n",
      "Iteration 159, loss = 0.32314777\n",
      "Iteration 160, loss = 0.32118981\n",
      "Iteration 161, loss = 0.31917443\n",
      "Iteration 162, loss = 0.31732679\n",
      "Iteration 163, loss = 0.31548843\n",
      "Iteration 164, loss = 0.31368264\n",
      "Iteration 165, loss = 0.31170795\n",
      "Iteration 166, loss = 0.30978292\n",
      "Iteration 167, loss = 0.30787773\n",
      "Iteration 168, loss = 0.30613149\n",
      "Iteration 169, loss = 0.30435904\n",
      "Iteration 170, loss = 0.30261992\n",
      "Iteration 171, loss = 0.30086502\n",
      "Iteration 172, loss = 0.29915586\n",
      "Iteration 173, loss = 0.29746308\n",
      "Iteration 174, loss = 0.29582721\n",
      "Iteration 175, loss = 0.29421993\n",
      "Iteration 176, loss = 0.29252910\n",
      "Iteration 177, loss = 0.29093587\n",
      "Iteration 178, loss = 0.28933668\n",
      "Iteration 179, loss = 0.28769075\n",
      "Iteration 180, loss = 0.28614996\n",
      "Iteration 181, loss = 0.28467913\n",
      "Iteration 182, loss = 0.28312855\n",
      "Iteration 183, loss = 0.28155010\n",
      "Iteration 184, loss = 0.28017183\n",
      "Iteration 185, loss = 0.27870184\n",
      "Iteration 186, loss = 0.27728380\n",
      "Iteration 187, loss = 0.27589018\n",
      "Iteration 188, loss = 0.27443983\n",
      "Iteration 189, loss = 0.27302007\n",
      "Iteration 190, loss = 0.27170899\n",
      "Iteration 191, loss = 0.27032778\n",
      "Iteration 192, loss = 0.26897895\n",
      "Iteration 193, loss = 0.26768423\n",
      "Iteration 194, loss = 0.26631900\n",
      "Iteration 195, loss = 0.26507633\n",
      "Iteration 196, loss = 0.26378805\n",
      "Iteration 197, loss = 0.26249330\n",
      "Iteration 198, loss = 0.26123743\n",
      "Iteration 199, loss = 0.26001662\n",
      "Iteration 200, loss = 0.25880063\n",
      "Iteration 201, loss = 0.25757551\n",
      "Iteration 202, loss = 0.25636122\n",
      "Iteration 203, loss = 0.25518142\n",
      "Iteration 204, loss = 0.25414933\n",
      "Iteration 205, loss = 0.25295983\n",
      "Iteration 206, loss = 0.25172612\n",
      "Iteration 207, loss = 0.25060083\n",
      "Iteration 208, loss = 0.24954067\n",
      "Iteration 209, loss = 0.24843513\n",
      "Iteration 210, loss = 0.24734363\n",
      "Iteration 211, loss = 0.24618915\n",
      "Iteration 212, loss = 0.24511343\n",
      "Iteration 213, loss = 0.24409198\n",
      "Iteration 214, loss = 0.24305175\n",
      "Iteration 215, loss = 0.24199492\n",
      "Iteration 216, loss = 0.24090920\n",
      "Iteration 217, loss = 0.23990288\n",
      "Iteration 218, loss = 0.23886792\n",
      "Iteration 219, loss = 0.23785647\n",
      "Iteration 220, loss = 0.23688512\n",
      "Iteration 221, loss = 0.23596239\n",
      "Iteration 222, loss = 0.23496050\n",
      "Iteration 223, loss = 0.23397346\n",
      "Iteration 224, loss = 0.23302085\n",
      "Iteration 225, loss = 0.23204517\n",
      "Iteration 226, loss = 0.23109799\n",
      "Iteration 227, loss = 0.23016560\n",
      "Iteration 228, loss = 0.22926316\n",
      "Iteration 229, loss = 0.22835276\n",
      "Iteration 230, loss = 0.22745343\n",
      "Iteration 231, loss = 0.22658976\n",
      "Iteration 232, loss = 0.22568492\n",
      "Iteration 233, loss = 0.22482063\n",
      "Iteration 234, loss = 0.22398320\n",
      "Iteration 235, loss = 0.22314314\n",
      "Iteration 236, loss = 0.22225787\n",
      "Iteration 237, loss = 0.22142256\n",
      "Iteration 238, loss = 0.22059448\n",
      "Iteration 239, loss = 0.21976555\n",
      "Iteration 240, loss = 0.21898032\n",
      "Iteration 241, loss = 0.21815406\n",
      "Iteration 242, loss = 0.21729840\n",
      "Iteration 243, loss = 0.21648502\n",
      "Iteration 244, loss = 0.21570983\n",
      "Iteration 245, loss = 0.21490723\n",
      "Iteration 246, loss = 0.21411355\n",
      "Iteration 247, loss = 0.21336090\n",
      "Iteration 248, loss = 0.21262249\n",
      "Iteration 249, loss = 0.21188725\n",
      "Iteration 250, loss = 0.21112772\n",
      "Iteration 251, loss = 0.21037742\n",
      "Iteration 252, loss = 0.20966195\n",
      "Iteration 253, loss = 0.20894550\n",
      "Iteration 254, loss = 0.20825006\n",
      "Iteration 255, loss = 0.20749680\n",
      "Iteration 256, loss = 0.20679795\n",
      "Iteration 257, loss = 0.20612711\n",
      "Iteration 258, loss = 0.20543434\n",
      "Iteration 259, loss = 0.20473221\n",
      "Iteration 260, loss = 0.20406800\n",
      "Iteration 261, loss = 0.20340206\n",
      "Iteration 262, loss = 0.20272216\n",
      "Iteration 263, loss = 0.20207514\n",
      "Iteration 264, loss = 0.20145040\n",
      "Iteration 265, loss = 0.20075095\n",
      "Iteration 266, loss = 0.20009402\n",
      "Iteration 267, loss = 0.19941089\n",
      "Iteration 268, loss = 0.19880548\n",
      "Iteration 269, loss = 0.19817563\n",
      "Iteration 270, loss = 0.19749604\n",
      "Iteration 271, loss = 0.19688202\n",
      "Iteration 272, loss = 0.19628929\n",
      "Iteration 273, loss = 0.19571664\n",
      "Iteration 274, loss = 0.19504499\n",
      "Iteration 275, loss = 0.19443091\n",
      "Iteration 276, loss = 0.19383999\n",
      "Iteration 277, loss = 0.19325720\n",
      "Iteration 278, loss = 0.19266255\n",
      "Iteration 279, loss = 0.19204597\n",
      "Iteration 280, loss = 0.19142678\n",
      "Iteration 281, loss = 0.19085005\n",
      "Iteration 282, loss = 0.19027410\n",
      "Iteration 283, loss = 0.18966749\n",
      "Iteration 284, loss = 0.18912182\n",
      "Iteration 285, loss = 0.18853411\n",
      "Iteration 286, loss = 0.18799079\n",
      "Iteration 287, loss = 0.18745850\n",
      "Iteration 288, loss = 0.18690780\n",
      "Iteration 289, loss = 0.18635082\n",
      "Iteration 290, loss = 0.18580130\n",
      "Iteration 291, loss = 0.18526049\n",
      "Iteration 292, loss = 0.18481089\n",
      "Iteration 293, loss = 0.18428822\n",
      "Iteration 294, loss = 0.18371149\n",
      "Iteration 295, loss = 0.18315404\n",
      "Iteration 296, loss = 0.18262349\n",
      "Iteration 297, loss = 0.18212156\n",
      "Iteration 298, loss = 0.18161225\n",
      "Iteration 299, loss = 0.18112952\n",
      "Iteration 300, loss = 0.18060998\n",
      "Iteration 301, loss = 0.18013390\n",
      "Iteration 302, loss = 0.17962219\n",
      "Iteration 303, loss = 0.17912803\n",
      "Iteration 304, loss = 0.17861708\n",
      "Iteration 305, loss = 0.17815484\n",
      "Iteration 306, loss = 0.17767022\n",
      "Iteration 307, loss = 0.17719742\n",
      "Iteration 308, loss = 0.17674045\n",
      "Iteration 309, loss = 0.17628581\n",
      "Iteration 310, loss = 0.17579732\n",
      "Iteration 311, loss = 0.17534209\n",
      "Iteration 312, loss = 0.17489234\n",
      "Iteration 313, loss = 0.17444495\n",
      "Iteration 314, loss = 0.17405457\n",
      "Iteration 315, loss = 0.17361832\n",
      "Iteration 316, loss = 0.17318098\n",
      "Iteration 317, loss = 0.17273547\n",
      "Iteration 318, loss = 0.17227536\n",
      "Iteration 319, loss = 0.17183817\n",
      "Iteration 320, loss = 0.17140293\n",
      "Iteration 321, loss = 0.17098872\n",
      "Iteration 322, loss = 0.17058150\n",
      "Iteration 323, loss = 0.17013327\n",
      "Iteration 324, loss = 0.16973028\n",
      "Iteration 325, loss = 0.16932304\n",
      "Iteration 326, loss = 0.16891914\n",
      "Iteration 327, loss = 0.16854037\n",
      "Iteration 328, loss = 0.16811262\n",
      "Iteration 329, loss = 0.16769322\n",
      "Iteration 330, loss = 0.16728993\n",
      "Iteration 331, loss = 0.16688799\n",
      "Iteration 332, loss = 0.16651237\n",
      "Iteration 333, loss = 0.16611340\n",
      "Iteration 334, loss = 0.16573924\n",
      "Iteration 335, loss = 0.16533036\n",
      "Iteration 336, loss = 0.16490009\n",
      "Iteration 337, loss = 0.16455097\n",
      "Iteration 338, loss = 0.16416200\n",
      "Iteration 339, loss = 0.16377665\n",
      "Iteration 340, loss = 0.16343649\n",
      "Iteration 341, loss = 0.16303743\n",
      "Iteration 342, loss = 0.16266703\n",
      "Iteration 343, loss = 0.16229315\n",
      "Iteration 344, loss = 0.16195572\n",
      "Iteration 345, loss = 0.16157539\n",
      "Iteration 346, loss = 0.16122472\n",
      "Iteration 347, loss = 0.16086157\n",
      "Iteration 348, loss = 0.16051633\n",
      "Iteration 349, loss = 0.16015628\n",
      "Iteration 350, loss = 0.15981095\n",
      "Iteration 351, loss = 0.15945965\n",
      "Iteration 352, loss = 0.15911583\n",
      "Iteration 353, loss = 0.15877125\n",
      "Iteration 354, loss = 0.15842053\n",
      "Iteration 355, loss = 0.15809355\n",
      "Iteration 356, loss = 0.15773350\n",
      "Iteration 357, loss = 0.15739166\n",
      "Iteration 358, loss = 0.15704463\n",
      "Iteration 359, loss = 0.15674456\n",
      "Iteration 360, loss = 0.15643419\n",
      "Iteration 361, loss = 0.15609272\n",
      "Iteration 362, loss = 0.15576874\n",
      "Iteration 363, loss = 0.15542907\n",
      "Iteration 364, loss = 0.15507272\n",
      "Iteration 365, loss = 0.15476642\n",
      "Iteration 366, loss = 0.15443057\n",
      "Iteration 367, loss = 0.15411824\n",
      "Iteration 368, loss = 0.15380456\n",
      "Iteration 369, loss = 0.15350975\n",
      "Iteration 370, loss = 0.15319705\n",
      "Iteration 371, loss = 0.15289279\n",
      "Iteration 372, loss = 0.15257406\n",
      "Iteration 373, loss = 0.15226342\n",
      "Iteration 374, loss = 0.15197801\n",
      "Iteration 375, loss = 0.15169035\n",
      "Iteration 376, loss = 0.15140478\n",
      "Iteration 377, loss = 0.15108272\n",
      "Iteration 378, loss = 0.15080268\n",
      "Iteration 379, loss = 0.15046653\n",
      "Iteration 380, loss = 0.15020161\n",
      "Iteration 381, loss = 0.14990347\n",
      "Iteration 382, loss = 0.14961215\n",
      "Iteration 383, loss = 0.14934835\n",
      "Iteration 384, loss = 0.14906714\n",
      "Iteration 385, loss = 0.14877985\n",
      "Iteration 386, loss = 0.14849803\n",
      "Iteration 387, loss = 0.14823966\n",
      "Iteration 388, loss = 0.14799251\n",
      "Iteration 389, loss = 0.14773892\n",
      "Iteration 390, loss = 0.14740700\n",
      "Iteration 391, loss = 0.14712561\n",
      "Iteration 392, loss = 0.14689189\n",
      "Iteration 393, loss = 0.14660200\n",
      "Iteration 394, loss = 0.14634322\n",
      "Iteration 395, loss = 0.14606474\n",
      "Iteration 396, loss = 0.14579038\n",
      "Iteration 397, loss = 0.14551109\n",
      "Iteration 398, loss = 0.14528135\n",
      "Iteration 399, loss = 0.14499594\n",
      "Iteration 400, loss = 0.14472445\n",
      "Iteration 401, loss = 0.14448710\n",
      "Iteration 402, loss = 0.14423737\n",
      "Iteration 403, loss = 0.14400499\n",
      "Iteration 404, loss = 0.14378125\n",
      "Iteration 405, loss = 0.14350547\n",
      "Iteration 406, loss = 0.14321859\n",
      "Iteration 407, loss = 0.14294418\n",
      "Iteration 408, loss = 0.14270176\n",
      "Iteration 409, loss = 0.14246580\n",
      "Iteration 410, loss = 0.14221571\n",
      "Iteration 411, loss = 0.14197280\n",
      "Iteration 412, loss = 0.14174005\n",
      "Iteration 413, loss = 0.14149138\n",
      "Iteration 414, loss = 0.14121804\n",
      "Iteration 415, loss = 0.14099797\n",
      "Iteration 416, loss = 0.14075174\n",
      "Iteration 417, loss = 0.14053501\n",
      "Iteration 418, loss = 0.14029373\n",
      "Iteration 419, loss = 0.14006654\n",
      "Iteration 420, loss = 0.13983428\n",
      "Iteration 421, loss = 0.13962626\n",
      "Iteration 422, loss = 0.13938475\n",
      "Iteration 423, loss = 0.13917736\n",
      "Iteration 424, loss = 0.13894780\n",
      "Iteration 425, loss = 0.13869299\n",
      "Iteration 426, loss = 0.13850803\n",
      "Iteration 427, loss = 0.13825239\n",
      "Iteration 428, loss = 0.13805946\n",
      "Iteration 429, loss = 0.13782590\n",
      "Iteration 430, loss = 0.13760674\n",
      "Iteration 431, loss = 0.13740194\n",
      "Iteration 432, loss = 0.13717413\n",
      "Iteration 433, loss = 0.13694921\n",
      "Iteration 434, loss = 0.13675390\n",
      "Iteration 435, loss = 0.13653740\n",
      "Iteration 436, loss = 0.13631580\n",
      "Iteration 437, loss = 0.13609085\n",
      "Iteration 438, loss = 0.13590718\n",
      "Iteration 439, loss = 0.13570094\n",
      "Iteration 440, loss = 0.13548377\n",
      "Iteration 441, loss = 0.13527096\n",
      "Iteration 442, loss = 0.13506393\n",
      "Iteration 443, loss = 0.13485809\n",
      "Iteration 444, loss = 0.13465592\n",
      "Iteration 445, loss = 0.13446304\n",
      "Iteration 446, loss = 0.13424625\n",
      "Iteration 447, loss = 0.13405595\n",
      "Iteration 448, loss = 0.13385574\n",
      "Iteration 449, loss = 0.13367224\n",
      "Iteration 450, loss = 0.13348178\n",
      "Iteration 451, loss = 0.13329291\n",
      "Iteration 452, loss = 0.13308712\n",
      "Iteration 453, loss = 0.13288799\n",
      "Iteration 454, loss = 0.13271737\n",
      "Iteration 455, loss = 0.13253023\n",
      "Iteration 456, loss = 0.13235702\n",
      "Iteration 457, loss = 0.13211821\n",
      "Iteration 458, loss = 0.13194441\n",
      "Iteration 459, loss = 0.13175340\n",
      "Iteration 460, loss = 0.13155349\n",
      "Iteration 461, loss = 0.13136543\n",
      "Iteration 462, loss = 0.13119974\n",
      "Iteration 463, loss = 0.13100478\n",
      "Iteration 464, loss = 0.13084326\n",
      "Iteration 465, loss = 0.13067185\n",
      "Iteration 466, loss = 0.13049009\n",
      "Iteration 467, loss = 0.13029554\n",
      "Iteration 468, loss = 0.13012480\n",
      "Iteration 469, loss = 0.12994438\n",
      "Iteration 470, loss = 0.12975828\n",
      "Iteration 471, loss = 0.12958971\n",
      "Iteration 472, loss = 0.12941307\n",
      "Iteration 473, loss = 0.12923058\n",
      "Iteration 474, loss = 0.12906097\n",
      "Iteration 475, loss = 0.12889443\n",
      "Iteration 476, loss = 0.12871903\n",
      "Iteration 477, loss = 0.12855431\n",
      "Iteration 478, loss = 0.12841104\n",
      "Iteration 479, loss = 0.12819747\n",
      "Iteration 480, loss = 0.12802316\n",
      "Iteration 481, loss = 0.12786583\n",
      "Iteration 482, loss = 0.12771287\n",
      "Iteration 483, loss = 0.12751978\n",
      "Iteration 484, loss = 0.12736207\n",
      "Iteration 485, loss = 0.12719856\n",
      "Iteration 486, loss = 0.12703577\n",
      "Iteration 487, loss = 0.12687684\n",
      "Iteration 488, loss = 0.12672571\n",
      "Iteration 489, loss = 0.12656431\n",
      "Iteration 490, loss = 0.12640580\n",
      "Iteration 491, loss = 0.12625175\n",
      "Iteration 492, loss = 0.12610771\n",
      "Iteration 493, loss = 0.12593989\n",
      "Iteration 494, loss = 0.12577299\n",
      "Iteration 495, loss = 0.12563138\n",
      "Iteration 496, loss = 0.12547362\n",
      "Iteration 497, loss = 0.12529713\n",
      "Iteration 498, loss = 0.12515426\n",
      "Iteration 499, loss = 0.12498680\n",
      "Iteration 500, loss = 0.12483503\n",
      "Iteration 501, loss = 0.12470139\n",
      "Iteration 502, loss = 0.12454233\n",
      "Iteration 503, loss = 0.12438712\n",
      "Iteration 504, loss = 0.12423722\n",
      "Iteration 505, loss = 0.12408531\n",
      "Iteration 506, loss = 0.12393707\n",
      "Iteration 507, loss = 0.12378787\n",
      "Iteration 508, loss = 0.12363257\n",
      "Iteration 509, loss = 0.12350322\n",
      "Iteration 510, loss = 0.12334953\n",
      "Iteration 511, loss = 0.12319467\n",
      "Iteration 512, loss = 0.12304907\n",
      "Iteration 513, loss = 0.12291614\n",
      "Iteration 514, loss = 0.12278395\n",
      "Iteration 515, loss = 0.12264615\n",
      "Iteration 516, loss = 0.12251039\n",
      "Iteration 517, loss = 0.12236259\n",
      "Iteration 518, loss = 0.12221934\n",
      "Iteration 519, loss = 0.12208235\n",
      "Iteration 520, loss = 0.12194447\n",
      "Iteration 521, loss = 0.12179742\n",
      "Iteration 522, loss = 0.12166301\n",
      "Iteration 523, loss = 0.12152437\n",
      "Iteration 524, loss = 0.12138843\n",
      "Iteration 525, loss = 0.12125345\n",
      "Iteration 526, loss = 0.12112258\n",
      "Iteration 527, loss = 0.12098592\n",
      "Iteration 528, loss = 0.12086386\n",
      "Iteration 529, loss = 0.12075231\n",
      "Iteration 530, loss = 0.12060140\n",
      "Iteration 531, loss = 0.12046057\n",
      "Iteration 532, loss = 0.12032934\n",
      "Iteration 533, loss = 0.12019961\n",
      "Iteration 534, loss = 0.12006041\n",
      "Iteration 535, loss = 0.11994028\n",
      "Iteration 536, loss = 0.11981056\n",
      "Iteration 537, loss = 0.11967937\n",
      "Iteration 538, loss = 0.11955038\n",
      "Iteration 539, loss = 0.11943538\n",
      "Iteration 540, loss = 0.11930527\n",
      "Iteration 541, loss = 0.11916954\n",
      "Iteration 542, loss = 0.11903623\n",
      "Iteration 543, loss = 0.11893066\n",
      "Iteration 544, loss = 0.11880441\n",
      "Iteration 545, loss = 0.11867503\n",
      "Iteration 546, loss = 0.11856679\n",
      "Iteration 547, loss = 0.11843467\n",
      "Iteration 548, loss = 0.11831107\n",
      "Iteration 549, loss = 0.11817559\n",
      "Iteration 550, loss = 0.11806431\n",
      "Iteration 551, loss = 0.11792896\n",
      "Iteration 552, loss = 0.11782989\n",
      "Iteration 553, loss = 0.11769018\n",
      "Iteration 554, loss = 0.11756720\n",
      "Iteration 555, loss = 0.11745131\n",
      "Iteration 556, loss = 0.11733856\n",
      "Iteration 557, loss = 0.11721654\n",
      "Iteration 558, loss = 0.11710143\n",
      "Iteration 559, loss = 0.11700174\n",
      "Iteration 560, loss = 0.11687501\n",
      "Iteration 561, loss = 0.11676445\n",
      "Iteration 562, loss = 0.11665320\n",
      "Iteration 563, loss = 0.11653129\n",
      "Iteration 564, loss = 0.11642993\n",
      "Iteration 565, loss = 0.11630725\n",
      "Iteration 566, loss = 0.11618990\n",
      "Iteration 567, loss = 0.11608282\n",
      "Iteration 568, loss = 0.11598695\n",
      "Iteration 569, loss = 0.11586400\n",
      "Iteration 570, loss = 0.11575248\n",
      "Iteration 571, loss = 0.11564201\n",
      "Iteration 572, loss = 0.11553450\n",
      "Iteration 573, loss = 0.11542585\n",
      "Iteration 574, loss = 0.11532354\n",
      "Iteration 575, loss = 0.11521503\n",
      "Iteration 576, loss = 0.11510566\n",
      "Iteration 577, loss = 0.11499900\n",
      "Iteration 578, loss = 0.11489769\n",
      "Iteration 579, loss = 0.11478765\n",
      "Iteration 580, loss = 0.11467014\n",
      "Iteration 581, loss = 0.11457181\n",
      "Iteration 582, loss = 0.11446502\n",
      "Iteration 583, loss = 0.11436730\n",
      "Iteration 584, loss = 0.11426749\n",
      "Iteration 585, loss = 0.11417106\n",
      "Iteration 586, loss = 0.11405035\n",
      "Iteration 587, loss = 0.11395434\n",
      "Iteration 588, loss = 0.11385908\n",
      "Iteration 589, loss = 0.11375595\n",
      "Iteration 590, loss = 0.11364827\n",
      "Iteration 591, loss = 0.11354718\n",
      "Iteration 592, loss = 0.11344644\n",
      "Iteration 593, loss = 0.11334859\n",
      "Iteration 594, loss = 0.11324950\n",
      "Iteration 595, loss = 0.11314578\n",
      "Iteration 596, loss = 0.11305921\n",
      "Iteration 597, loss = 0.11294880\n",
      "Iteration 598, loss = 0.11285910\n",
      "Iteration 599, loss = 0.11275126\n",
      "Iteration 600, loss = 0.11264931\n",
      "Iteration 601, loss = 0.11255090\n",
      "Iteration 602, loss = 0.11245755\n",
      "Iteration 603, loss = 0.11236711\n",
      "Iteration 604, loss = 0.11226615\n",
      "Iteration 605, loss = 0.11218697\n",
      "Iteration 606, loss = 0.11207940\n",
      "Iteration 607, loss = 0.11198751\n",
      "Iteration 608, loss = 0.11190352\n",
      "Iteration 609, loss = 0.11180120\n",
      "Iteration 610, loss = 0.11171219\n",
      "Iteration 611, loss = 0.11162164\n",
      "Iteration 612, loss = 0.11152927\n",
      "Iteration 613, loss = 0.11143147\n",
      "Iteration 614, loss = 0.11134575\n",
      "Iteration 615, loss = 0.11124733\n",
      "Iteration 616, loss = 0.11115349\n",
      "Iteration 617, loss = 0.11105961\n",
      "Iteration 618, loss = 0.11096883\n",
      "Iteration 619, loss = 0.11088353\n",
      "Iteration 620, loss = 0.11079757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Best params:{'hidden_layer_sizes': 40, 'learning_rate_init': 0.1, 'solver': 'lbfgs'}\n",
      "Skuteczno (train): 100.0%\n",
      "Skuteczno (test): 94.72222222222221%\n",
      "[[142   0   0   0   0   0   0   0   0   0]\n",
      " [  0 146   0   0   0   0   0   0   0   0]\n",
      " [  0   0 142   0   0   0   0   0   0   0]\n",
      " [  0   0   0 146   0   0   0   0   0   0]\n",
      " [  0   0   0   0 145   0   0   0   0   0]\n",
      " [  0   0   0   0   0 145   0   0   0   0]\n",
      " [  0   0   0   0   0   0 145   0   0   0]\n",
      " [  0   0   0   0   0   0   0 143   0   0]\n",
      " [  0   0   0   0   0   0   0   0 139   0]\n",
      " [  0   0   0   0   0   0   0   0   0 144]]\n",
      "[[35  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 36  0  0  0  0  0  0  0  0]\n",
      " [ 0  1 34  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 35  0  0  0  1  1  0]\n",
      " [ 0  1  0  0 32  1  0  0  2  0]\n",
      " [ 0  0  0  0  0 37  0  0  0  0]\n",
      " [ 0  1  0  0  0  0 35  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 36  0  0]\n",
      " [ 0  1  0  1  0  0  0  1 31  1]\n",
      " [ 0  1  0  1  0  2  0  1  1 30]]\n"
     ]
    }
   ],
   "source": [
    "# Wczytanie liczb\n",
    "liczby = datasets.load_digits()\n",
    "y = liczby.target\n",
    "x = liczby.images.reshape((len(liczby.images), -1))\n",
    "# display(y)\n",
    "x = hotone.fit_transform(x).toarray()\n",
    "# display(x)\n",
    "# print(x.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0, stratify=y)\n",
    "mlp = MLPClassifier(alpha=0.1, hidden_layer_sizes=40, learning_rate_init=0.01, max_iter=1000, random_state=1,\n",
    "                    solver='sgd', verbose=10)\n",
    "mlp.out_activation = 'softmax'\n",
    "parametry = {'learning_rate_init' : (0.1,0.01, 0.001), 'hidden_layer_sizes' : [20, 40, 60, 80, 100], 'solver' : ['adam', 'lbfgs', 'sgd']}\n",
    "clf = GridSearchCV(mlp, parametry)\n",
    "clf.fit(x,y)\n",
    "print(\"Best params:\" + str(clf.best_params_))\n",
    "# zastosowanie najlepszych parametrw\n",
    "mlp = MLPClassifier (alpha=0.1, hidden_layer_sizes=clf.best_params_['hidden_layer_sizes'],\n",
    "                    learning_rate_init=clf.best_params_['learning_rate_init'], max_iter=1000, random_state=1,\n",
    "                    solver=clf.best_params_['solver'], verbose=10)\n",
    "mlp.fit(x_train, y_train)\n",
    "print(\"Skuteczno (train): \" + str(mlp.score(x_train,y_train)*100) + '%')\n",
    "print(\"Skuteczno (test): \" + str(mlp.score(x_test,y_test)*100) + '%')\n",
    "\n",
    "Y_pred_train = mlp.predict(x_train)\n",
    "print(confusion_matrix(y_train,Y_pred_train)) \n",
    "Y_pred_test = mlp.predict(x_test) \n",
    "print(confusion_matrix(y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DANE Z KAGLE\n",
    "dt = pd.read_csv('Data_Fruits.csv')#,error_bad_lines=False)\n",
    "#display(dt)\n",
    "X = dt.drop(columns=['class'])\n",
    "y = dt['class']\n",
    "y = le.fit_transform(y)\n",
    "X = hotone.fit_transform(X).toarray()\n",
    "display(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0, stratify=y)\n",
    "mlp = MLPClassifier (alpha=0.1, hidden_layer_sizes=20, learning_rate_init=0.001, max_iter=1000, random_state=1, solver='sgd') \n",
    "mlp.out_activation = 'softmax\"\n",
    "parametry {'learning_rate_init': (0.1,0.01, 0.001), 'hidden_layer_sizes' : [20, 40, 60, 80, 100], 'solver': ['adam', 'lbfgs', 'sgd']}\n",
    "clf = GridSearchCV(mlp, parametry)\n",
    "clf.fit(x,y)\n",
    "print(\"Best params: \" + str(clf.best_params_))\n",
    "#zastosowanie najlepszych parametrw\n",
    "mlp = MLPClassifier (alpha=0.1, hidden_layer_sizes=40,\n",
    "                    learning_rate_init=0.1, max_iter=1000, random_state=1,\n",
    "                    solver='lbfgs', verbose=10)\n",
    "mlp.out_activation = 'softmax'\n",
    "\n",
    "mlp.fit(x_train, y_train)\n",
    "print(\"Skuteczno (train): \" + str(mlp.score(x_train,y_train)*100) + '%') \n",
    "print(\"Skuteczno (test): \"+ str(mlp.score(x_test,y_test)*100) + '%')\n",
    "Y_pred_train = mlp.predict(x_train) \n",
    "print(confusion_matrix(y_train,Y_pred_train)) \n",
    "Y_pred_test = mlp.predict(x_test) \n",
    "print(confusion_matrix(y_test,Y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
