{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import *\n",
    "from scipy.linalg import norm, pinv, svd\n",
    "#import matplotlib.pyplot as plt\n",
    "from numpy.linalg import det\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "LAMBDA=0.0\n",
    "\n",
    "def gendist(X,Y):\n",
    "  return cdist(X,Y,metric='euclidean')\n",
    "\n",
    "class RBF:\n",
    "     \n",
    "    def __init__(self, centers, outdim, R):\n",
    "        indim = centers.shape[1]\n",
    "        numCenters = centers.shape[0]\n",
    "        self.indim = indim\n",
    "        self.outdim = outdim\n",
    "        self.numCenters = numCenters\n",
    "        self.centers = centers\n",
    "        self.R = R\n",
    "        self.W = np.random.random((self.numCenters, self.outdim))\n",
    "        self.TRAINING_X_DATA = []\n",
    "        \n",
    "    def _basisfunc(self, c, d):\n",
    "        assert len(d) == self.indim\n",
    "        return np.exp(-1/(self.R)**2 * norm(c-d)**2)\n",
    "    \n",
    "    def _basisfuncFast(self, c, d, dist_mat):\n",
    "        print(\"_basisfuncFast ---> STARTED\")\n",
    "        ret = np.exp(-1/(self.R)**2 * dist_mat**2)\n",
    "        print(\"_basisfuncFast ---> ENDED\")\n",
    "        return ret\n",
    "     \n",
    "    def _calcAct(self, X):\n",
    "        # calculate activations of RBFs\n",
    "        G = np.zeros((X.shape[0], self.numCenters), float)\n",
    "        for ci, c in enumerate(self.centers):\n",
    "            for xi, x in enumerate(X):\n",
    "                G[xi,ci] = self._basisfunc(c, x)\n",
    "        \n",
    "        if G.shape[0] == G.shape[1]:\n",
    "           print('det(G) = ', det(G))\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def _calcActFast(self, X):\n",
    "        G = np.zeros((X.shape[0], self.numCenters), float)\n",
    "\n",
    "        self.centers = np.vstack(self.centers[:,:]).astype(np.float64)\n",
    "        X = np.vstack(X[:,:]).astype(np.float64)\n",
    "\n",
    "        dist_mat = gendist(self.centers, X)\n",
    "        print(\"distance matrix ...  beeing calculated - STARTED\")\n",
    "        G = self._basisfuncFast(self.centers, X, dist_mat)\n",
    "        print(\"distance matrix ...  beeing calculated - ENDEDED\")\n",
    "\n",
    "        print('X.shape[0]=',X.shape[0], 'self.numCenters=',self.numCenters)\n",
    "       \n",
    "        if G.shape[0] == G.shape[1]:\n",
    "           print('det(G) = ', det(G))\n",
    "        \n",
    "        return G\n",
    "\n",
    "    def wypiszZbior(self, X, tekst):\n",
    "        print(tekst)\n",
    "        for i in range(X.shape[0]):\n",
    "            print(i,X[i,:])\n",
    "        \n",
    "    def train(self, X, Y):\n",
    "        \"\"\" X: matrix of dimensions n x indim \n",
    "            y: column vector of dimension n x 1 \"\"\"\n",
    "         \n",
    "        self.TRAINING_X_DATA = X\n",
    "        \n",
    "        # self.wypiszZbior(X,'ZBIOR TRENINGOWY ----------------------->')\n",
    "        G = self._calcAct(X)\n",
    "        # calculate output weights (pseudoinverse)\n",
    "        self.W = np.dot(pinv(G), Y)\n",
    "        \n",
    "    def trainRegularized(self, X, Y, _lambda=0):\n",
    "        print(\"Regularized RBF training.\")\n",
    "        self.wypiszZbior(X,'ZBIOR TRENINGOWY ----------------------->')\n",
    "        \n",
    "        self.TRAINING_X_DATA = X\n",
    "        \n",
    "        G = self._calcActFast(X)\n",
    "        U, S, VT = svd(G)\n",
    "        self.wartosci_szczegolne = S\n",
    "        \n",
    "        S_inv = S*S-_lambda\n",
    "        \n",
    "        print('VT.shape=',VT.shape)\n",
    "        print('S.shape=',S.shape)\n",
    "        print('U.shape=',U.shape)\n",
    "        \n",
    "        N = G.shape[0]\n",
    "        \n",
    "        r = N\n",
    "        for i in range(N):\n",
    "            if S[i] < _lambda:\n",
    "                r = i\n",
    "                break\n",
    "                \n",
    "        W_final = np.zeros((G.shape[0], Y.shape[1]))\n",
    "    \n",
    "        for ii in range(Y.shape[1]):\n",
    "            w_lambda = np.zeros(G.shape[0])\n",
    "            for i in range(r):\n",
    "                sigma = S[i]\n",
    "                uTi = U[:,i].transpose()\n",
    "                y = Y[:,ii]\n",
    "                vi = VT.transpose()[:,i]\n",
    "                f = sigma**2/(sigma**2+_lambda**2)\n",
    "                w_lambda += 1/sigma*f*np.dot(np.dot(uTi,y),vi)\n",
    "            W_final[:,ii] = w_lambda\n",
    "\n",
    "        print('r=',r)\n",
    "        \n",
    "        self.W = W_final\n",
    "        \n",
    "    def test(self, X, treningowy = False):\n",
    "        \"\"\" X: matrix of dimensions n x indim \"\"\"\n",
    "        G = self._calcAct(X)\n",
    "        Y = np.dot(G, self.W)\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def standardize_dataset(data):\n",
    "    \"\"\"\n",
    "    Standaryzuje dane: (wartość - średnia) / odchylenie standardowe.\n",
    "\n",
    "    :param data: dane do standaryzacji\n",
    "    :return: standaryzowane dane\n",
    "    \"\"\"\n",
    "    mean_value = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    standardized_data = [(value - mean_value) / std_dev for value in data]\n",
    "    \n",
    "    return np.array(standardized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_data(data, target_index, sequence_length):\n",
    "    \"\"\"\n",
    "    Przygotowuje dane do modelu sekwencyjnego.\n",
    "\n",
    "    :param data: dane wejściowe\n",
    "    :param target_index: indeks kolumny docelowej\n",
    "    :param sequence_length: długość sekwencji wejściowych\n",
    "    :return: przekształcone dane wejściowe i docelowe\n",
    "    \"\"\"\n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    \n",
    "    for i in range(len(data) - sequence_length - 1):\n",
    "        sequence_data = data[i:(i + sequence_length)]\n",
    "        reshaped_sequence = sequence_data.values.reshape(sequence_data.shape[0] * sequence_data.shape[1])\n",
    "\n",
    "        target_sequence = data[(i + sequence_length):(i + sequence_length + 1)][target_index]\n",
    "        \n",
    "        input_data.append(reshaped_sequence)\n",
    "        target_data.append(target_sequence)\n",
    "    \n",
    "    return np.array(input_data), np.array(target_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_max_distance(data):\n",
    "    \"\"\"\n",
    "    Oblicza najdłuższy dystans między dowolnymi dwoma punktami w zestawie danych.\n",
    "\n",
    "    :param data: zestaw danych\n",
    "    :return: najdłuższy obliczony dystans\n",
    "    \"\"\"\n",
    "    max_distance = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        for j in range(i+1, len(data)):\n",
    "            current_distance = np.linalg.norm(data[i] - data[j])\n",
    "            max_distance = max(max_distance, current_distance)\n",
    "    \n",
    "    return max_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def apply_kohonen_algorithm(data, num_iterations, initial_learning_rate, num_representatives, learning_rate_decay, measure, decay_rate1, decay_rate2, standardize):\n",
    "    \"\"\"\n",
    "    Implementacja algorytmu Kohonena do uczenia sieci neuronowych bez nauczyciela.\n",
    "\n",
    "    :param data: dane wejściowe\n",
    "    :param num_iterations: liczba iteracji do wykonania\n",
    "    :param initial_learning_rate: początkowa wartość współczynnika uczenia\n",
    "    :param num_representatives: liczba reprezentantów\n",
    "    :param learning_rate_decay: typ zmniejszania współczynnika uczenia (1: liniowe, 2: wykładnicze, 3: hiperboliczne)\n",
    "    :param measure: typ miary (1, 2, 3)\n",
    "    :param decay_rate1: parametr C1 dla wykładniczego i hiperbolicznego zmniejszania współczynnika uczenia\n",
    "    :param decay_rate2: parametr C2 dla hiperbolicznego zmniejszania współczynnika uczenia\n",
    "    :param standardize: czy dane mają być standaryzowane\n",
    "    :return: wektory reprezentantów, tablica m, dane wejściowe\n",
    "    \"\"\"\n",
    "\n",
    "    # Standaryzacja danych\n",
    "    if standardize:\n",
    "        data = standardize_dataset(data)\n",
    "\n",
    "    # Inicjalizacja i normalizacja wektorów reprezentantów\n",
    "    representative_vectors = initialize_representative_vectors(len(data), num_representatives, len(data[0]))\n",
    "\n",
    "    # Wybór miary i wykonanie iteracji\n",
    "    measure_table = [0]*len(data)\n",
    "    learning_rate = initial_learning_rate\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        chosen_representative = apply_measure(measure, data, iteration, num_representatives, representative_vectors)\n",
    "        measure_table[iteration%len(data)] = chosen_representative\n",
    "\n",
    "        # Modyfikacja wektorów reprezentantów\n",
    "        representative_vectors[chosen_representative] += learning_rate * (data[iteration%len(data)] - representative_vectors[chosen_representative])\n",
    "        representative_vectors[chosen_representative] /= np.linalg.norm(representative_vectors[chosen_representative])\n",
    "\n",
    "        # Aktualizacja współczynnika uczenia\n",
    "        learning_rate = update_learning_rate(learning_rate_decay, initial_learning_rate, num_iterations, iteration, decay_rate1, decay_rate2)\n",
    "\n",
    "    return representative_vectors, measure_table, data\n",
    "\n",
    "\n",
    "def initialize_representative_vectors(data_length, num_representatives, data_width):\n",
    "    vectors = []\n",
    "    random_generator = np.random.RandomState(0)\n",
    "\n",
    "    for _ in range(num_representatives):\n",
    "        for _ in range(data_length):\n",
    "            temp_vector = random_generator.normal(loc=0.0, scale=0.01, size=data_width)\n",
    "            vectors.append(temp_vector/np.linalg.norm(temp_vector))\n",
    "\n",
    "    return np.array(vectors)\n",
    "\n",
    "\n",
    "def apply_measure(measure, data, iteration, num_representatives, representative_vectors):\n",
    "    if measure == 1:\n",
    "        return dot_product_measure(data, iteration, num_representatives, representative_vectors)\n",
    "\n",
    "    if measure == 2:\n",
    "        return euclidean_distance_measure(data, iteration, num_representatives, representative_vectors)\n",
    "\n",
    "    if measure == 3:\n",
    "        return manhattan_distance_measure(data, iteration, num_representatives, representative_vectors)\n",
    "\n",
    "\n",
    "def update_learning_rate(learning_rate_decay, initial_learning_rate, num_iterations, current_iteration, decay_rate1, decay_rate2):\n",
    "    if learning_rate_decay == 1:  # Liniowe zmniejszanie\n",
    "        return initial_learning_rate * (num_iterations - current_iteration) / num_iterations\n",
    "\n",
    "    if learning_rate_decay == 2:  # Wykładnicze zmniejszanie\n",
    "        return initial_learning_rate * math.exp(-decay_rate1 * current_iteration)\n",
    "\n",
    "    if learning_rate_decay == 3:  # Hiperboliczne zmniejszanie\n",
    "        return decay_rate1 / (decay_rate2 + current_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def dot_product_measure(data, iteration_index, num_representatives, representative_vectors):\n",
    "    \"\"\"\n",
    "    Miara podobieństwa oparta na iloczynie skalarnym (miara 3).\n",
    "\n",
    "    :param data: dane wejściowe\n",
    "    :param iteration_index: indeks iteracji\n",
    "    :param num_representatives: liczba reprezentantów\n",
    "    :param representative_vectors: wektory reprezentantów\n",
    "    :return: indeks reprezentanta o największej miarze\n",
    "    \"\"\"\n",
    "    measurements = [np.dot(representative_vectors[i], data[iteration_index % len(data)]) for i in range(num_representatives)]\n",
    "    \n",
    "    max_index = np.argmax(measurements)\n",
    "    return max_index\n",
    "\n",
    "\n",
    "def euclidean_distance_measure(data, iteration_index, num_representatives, representative_vectors):\n",
    "    \"\"\"\n",
    "    Miara podobieństwa oparta na odległości euklidesowej (miara 4).\n",
    "\n",
    "    :param data: dane wejściowe\n",
    "    :param iteration_index: indeks iteracji\n",
    "    :param num_representatives: liczba reprezentantów\n",
    "    :param representative_vectors: wektory reprezentantów\n",
    "    :return: indeks reprezentanta o najmniejszej miarze\n",
    "    \"\"\"\n",
    "    measurements = [np.linalg.norm(representative_vectors[i] - data[iteration_index % len(data)]) for i in range(num_representatives)]\n",
    "    \n",
    "    min_index = np.argmin(measurements)\n",
    "    return min_index\n",
    "\n",
    "\n",
    "def manhattan_distance_measure(data, iteration_index, num_representatives, representative_vectors):\n",
    "    \"\"\"\n",
    "    Miara podobieństwa oparta na odległości Manhattan (miara 5).\n",
    "\n",
    "    :param data: dane wejściowe\n",
    "    :param iteration_index: indeks iteracji\n",
    "    :param num_representatives: liczba reprezentantów\n",
    "    :param representative_vectors: wektory reprezentantów\n",
    "    :return: indeks reprezentanta o najmniejszej miarze\n",
    "    \"\"\"\n",
    "    measurements = [sum(abs(representative_vectors[i][j] - data[iteration_index % len(data)][j]) for j in range(len(representative_vectors[0]))) for i in range(num_representatives)]\n",
    "    \n",
    "    min_index = np.argmin(measurements)\n",
    "    return min_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(expected, predicted_kmeans, predicted_kohonen, num_clusters, limit):\n",
    "    plt.grid()\n",
    "    plt.plot(expected, '-', label='Oczekiwane wartości', c='red')\n",
    "    plt.plot(predicted_kmeans, '-', label='Przewidywane wartości Kmeans',c= 'blue')\n",
    "    plt.plot(predicted_kohonen, '-', label='Przewidywane wartości Kohonen',c= 'green')\n",
    "    \n",
    "    plt.title(f\"Liczba Centrów Danych: {num_clusters}\\n Limit Predykcji: {limit}\", size=10)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Zapisanie wykresu z ustandaryzowaną nazwą\n",
    "    filename = f\"plot_cluster_{num_clusters}_limit_{limit}_.png\"\n",
    "    # plt.savefig(filename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# wczytanie danych\n",
    "data = pd.read_csv('all_stocks_5yr.csv')\n",
    "df = data[data['Name'] == 'AOS']\n",
    "X_df = df[['open', 'high', 'low', 'close']].head(500)\n",
    "\n",
    "# parametry\n",
    "alpha = 0.1 \n",
    "iter = 5000 \n",
    "measure = 1 \n",
    "learning_rate = 1 \n",
    "C1 = 0.5\n",
    "C2 = 0.5\n",
    "standardize_data = True \n",
    "\n",
    "cluster_values = [5]#,10,25,50,100]\n",
    "limit_values = [5]#1,5,10,15,20]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error Value for Kohonen is :  2.1501\n",
      "Mean Absolute Error Value for Kmeans is :  0.3873\n"
     ]
    }
   ],
   "source": [
    "# Otwórz plik tekstowy do zapisu\n",
    "with open(\"results1.txt\", \"w\") as file:\n",
    "    for num_clusters in cluster_values:\n",
    "        for limit in limit_values:\n",
    "            # Preparacja danych\n",
    "            data,expected_vals = prepare_data(X_df, 'close', limit)\n",
    "            standardized_data = standardize_dataset(data) \n",
    "            X_train, X_test, y_train, y_test = train_test_split(standardized_data, expected_vals, test_size=0.2, shuffle=False)\n",
    "\n",
    "            # Kohonen\n",
    "            centers, predicts, X = apply_kohonen_algorithm(X_train, iter, alpha, num_clusters, learning_rate, measure, C1, C2, standardize_data)\n",
    "            max_distance = compute_max_distance(standardized_data)\n",
    "            rbf = RBF(centers, num_clusters, max_distance)\n",
    "            rbf.train(X_train, y_train)\n",
    "            output_kohonen = rbf.test(X_test)\n",
    "            MAEValue_kohonen = round(mean_absolute_error(y_test, output_kohonen),4) \n",
    "            print('Mean Absolute Error Value for Kohonen is : ', MAEValue_kohonen)\n",
    "\n",
    "            # KMeans\n",
    "            kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "            kmeans.fit(X_train)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            centers = np.array(centers)\n",
    "            max_distance = compute_max_distance(standardized_data)\n",
    "            rbf = RBF(centers, num_clusters, max_distance)\n",
    "            rbf.train(X_train, y_train)\n",
    "            output_kmeans = rbf.test(X_test)\n",
    "            MAEValue_kmeans = round(mean_absolute_error(y_test, output_kmeans),4) \n",
    "            print('Mean Absolute Error Value for Kmeans is : ', MAEValue_kmeans)\n",
    "\n",
    "            # Rysowanie wykresu\n",
    "            plot_results(y_test, output_kmeans, output_kohonen, num_clusters, limit)\n",
    "\n",
    "            # Zapisz wyniki MAE do pliku tekstowego\n",
    "            #file.write(f\"Cluster: {num_clusters}, Limit: {limit}, MAE for Kohonen: {MAEValue_kohonen}, MAE for Kmeans: {MAEValue_kmeans}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
